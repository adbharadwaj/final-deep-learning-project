Vocabulary Size: 33447
encodedPathwayA = 8 encodedPathwayB = 53
Varying embedding size
Starting Experiment - embedding_size_256 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 256
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR embedding_size_256_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633

Start training
2017-12-10T11:17:17.371717: step 1, loss 22.1737, acc 0.703125, prec 0, recall 0
2017-12-10T11:17:17.832036: step 2, loss 0.938537, acc 0.828125, prec 0, recall 0
2017-12-10T11:17:18.267829: step 3, loss 28.0297, acc 0.84375, prec 0, recall 0
2017-12-10T11:17:18.720918: step 4, loss 8.50351, acc 0.890625, prec 0.0227273, recall 0.2
2017-12-10T11:17:19.171042: step 5, loss 0.906776, acc 0.71875, prec 0.016129, recall 0.2
2017-12-10T11:17:19.624235: step 6, loss 16.617, acc 0.6875, prec 0.0123457, recall 0.166667
2017-12-10T11:17:20.076355: step 7, loss 2.69619, acc 0.59375, prec 0.0275229, recall 0.375
2017-12-10T11:17:20.509044: step 8, loss 27.7516, acc 0.4375, prec 0.020979, recall 0.3
2017-12-10T11:17:20.948697: step 9, loss 3.92805, acc 0.390625, prec 0.0164835, recall 0.3
2017-12-10T11:17:21.382673: step 10, loss 3.32527, acc 0.40625, prec 0.0180995, recall 0.363636
2017-12-10T11:17:21.832010: step 11, loss 5.70155, acc 0.234375, prec 0.0184502, recall 0.416667
2017-12-10T11:17:22.277032: step 12, loss 7.69364, acc 0.1875, prec 0.015528, recall 0.384615
2017-12-10T11:17:22.720354: step 13, loss 14.5379, acc 0.1875, prec 0.0160428, recall 0.4
2017-12-10T11:17:23.159290: step 14, loss 6.69945, acc 0.15625, prec 0.0186047, recall 0.470588
2017-12-10T11:17:23.606121: step 15, loss 7.80636, acc 0.109375, prec 0.0204499, recall 0.526316
2017-12-10T11:17:24.045888: step 16, loss 8.51527, acc 0.09375, prec 0.020073, recall 0.55
2017-12-10T11:17:24.487779: step 17, loss 8.52864, acc 0.140625, prec 0.0198676, recall 0.571429
2017-12-10T11:17:24.928257: step 18, loss 7.67402, acc 0.140625, prec 0.0182094, recall 0.571429
2017-12-10T11:17:25.378890: step 19, loss 5.12373, acc 0.296875, prec 0.0170455, recall 0.571429
2017-12-10T11:17:25.818538: step 20, loss 5.25634, acc 0.25, prec 0.0159574, recall 0.571429
2017-12-10T11:17:26.246384: step 21, loss 4.56006, acc 0.453125, prec 0.017744, recall 0.608696
2017-12-10T11:17:26.688006: step 22, loss 3.49686, acc 0.390625, prec 0.0169082, recall 0.608696
2017-12-10T11:17:27.126687: step 23, loss 3.56931, acc 0.4375, prec 0.017341, recall 0.625
2017-12-10T11:17:27.562462: step 24, loss 1.43446, acc 0.734375, prec 0.0170068, recall 0.625
2017-12-10T11:17:27.996950: step 25, loss 3.14596, acc 0.6875, prec 0.0177384, recall 0.615385
2017-12-10T11:17:28.437963: step 26, loss 1.12423, acc 0.796875, prec 0.0174863, recall 0.615385
2017-12-10T11:17:28.898125: step 27, loss 30.1322, acc 0.796875, prec 0.0183387, recall 0.586207
2017-12-10T11:17:29.343261: step 28, loss 3.56759, acc 0.78125, prec 0.0201699, recall 0.59375
2017-12-10T11:17:29.779666: step 29, loss 15.9849, acc 0.765625, prec 0.0198745, recall 0.575758
2017-12-10T11:17:30.226206: step 30, loss 10.495, acc 0.796875, prec 0.0196281, recall 0.558824
2017-12-10T11:17:30.667030: step 31, loss 2.21227, acc 0.640625, prec 0.0191726, recall 0.558824
2017-12-10T11:17:31.117233: step 32, loss 2.77795, acc 0.5625, prec 0.0186457, recall 0.558824
2017-12-10T11:17:31.557275: step 33, loss 2.83586, acc 0.546875, prec 0.0181298, recall 0.558824
2017-12-10T11:17:32.002295: step 34, loss 3.21849, acc 0.46875, prec 0.0184672, recall 0.571429
2017-12-10T11:17:32.450397: step 35, loss 10.3985, acc 0.59375, prec 0.0180505, recall 0.555556
2017-12-10T11:17:32.897831: step 36, loss 2.86719, acc 0.421875, prec 0.0174672, recall 0.555556
2017-12-10T11:17:33.339885: step 37, loss 3.67296, acc 0.53125, prec 0.0186916, recall 0.578947
2017-12-10T11:17:33.800730: step 38, loss 4.67167, acc 0.40625, prec 0.0189145, recall 0.589744
2017-12-10T11:17:34.239983: step 39, loss 4.66401, acc 0.34375, prec 0.018283, recall 0.589744
2017-12-10T11:17:34.674939: step 40, loss 3.95143, acc 0.453125, prec 0.0185471, recall 0.6
2017-12-10T11:17:35.118944: step 41, loss 3.61181, acc 0.421875, prec 0.0202399, recall 0.627907
2017-12-10T11:17:35.576852: step 42, loss 2.90675, acc 0.546875, prec 0.0198092, recall 0.627907
2017-12-10T11:17:36.020134: step 43, loss 37.3501, acc 0.453125, prec 0.019341, recall 0.6
2017-12-10T11:17:36.468852: step 44, loss 3.53318, acc 0.515625, prec 0.0202939, recall 0.617021
2017-12-10T11:17:36.916849: step 45, loss 5.06813, acc 0.53125, prec 0.0219028, recall 0.627451
2017-12-10T11:17:37.361705: step 46, loss 3.3865, acc 0.5, prec 0.0214334, recall 0.627451
2017-12-10T11:17:37.800979: step 47, loss 3.18371, acc 0.546875, prec 0.0216678, recall 0.634615
2017-12-10T11:17:38.270316: step 48, loss 3.8407, acc 0.453125, prec 0.021181, recall 0.634615
2017-12-10T11:17:38.716051: step 49, loss 3.32634, acc 0.5, prec 0.0207547, recall 0.634615
2017-12-10T11:17:39.165775: step 50, loss 3.42996, acc 0.515625, prec 0.0203578, recall 0.634615
2017-12-10T11:17:39.599121: step 51, loss 15.1848, acc 0.578125, prec 0.0200364, recall 0.622642
2017-12-10T11:17:40.049040: step 52, loss 3.53742, acc 0.53125, prec 0.0202622, recall 0.62963
2017-12-10T11:17:40.488883: step 53, loss 9.30503, acc 0.640625, prec 0.0205761, recall 0.625
2017-12-10T11:17:40.935393: step 54, loss 2.80875, acc 0.609375, prec 0.0202781, recall 0.625
2017-12-10T11:17:41.379330: step 55, loss 9.89069, acc 0.59375, prec 0.0199886, recall 0.614035
2017-12-10T11:17:41.847500: step 56, loss 3.14321, acc 0.4375, prec 0.0201342, recall 0.62069
2017-12-10T11:17:42.293698: step 57, loss 2.06707, acc 0.65625, prec 0.0209713, recall 0.633333
2017-12-10T11:17:42.738430: step 58, loss 2.42017, acc 0.578125, prec 0.0206634, recall 0.633333
2017-12-10T11:17:43.180105: step 59, loss 3.88013, acc 0.515625, prec 0.0208445, recall 0.639344
2017-12-10T11:17:43.622107: step 60, loss 3.58656, acc 0.515625, prec 0.0210194, recall 0.645161
2017-12-10T11:17:44.067006: step 61, loss 3.95557, acc 0.484375, prec 0.0206612, recall 0.645161
2017-12-10T11:17:44.521180: step 62, loss 3.32195, acc 0.484375, prec 0.0203149, recall 0.645161
2017-12-10T11:17:44.957162: step 63, loss 3.73674, acc 0.453125, prec 0.0199601, recall 0.645161
2017-12-10T11:17:45.395460: step 64, loss 6.02554, acc 0.5, prec 0.019656, recall 0.634921
2017-12-10T11:17:45.857316: step 65, loss 1.94218, acc 0.640625, prec 0.0194363, recall 0.634921
2017-12-10T11:17:46.303567: step 66, loss 1.79407, acc 0.640625, prec 0.0192215, recall 0.634921
2017-12-10T11:17:46.762417: step 67, loss 2.68332, acc 0.671875, prec 0.019496, recall 0.640625
2017-12-10T11:17:47.228848: step 68, loss 2.58169, acc 0.71875, prec 0.0193305, recall 0.640625
2017-12-10T11:17:47.682898: step 69, loss 2.76916, acc 0.46875, prec 0.0190255, recall 0.640625
2017-12-10T11:17:48.135537: step 70, loss 1.91711, acc 0.734375, prec 0.0188766, recall 0.640625
2017-12-10T11:17:48.589795: step 71, loss 1.35554, acc 0.796875, prec 0.0187643, recall 0.640625
2017-12-10T11:17:49.022722: step 72, loss 18.4289, acc 0.703125, prec 0.0195011, recall 0.641791
2017-12-10T11:17:49.465032: step 73, loss 1.81423, acc 0.6875, prec 0.0193258, recall 0.641791
2017-12-10T11:17:49.921639: step 74, loss 1.16663, acc 0.78125, prec 0.0196429, recall 0.647059
2017-12-10T11:17:50.351340: step 75, loss 2.08048, acc 0.75, prec 0.0208057, recall 0.661972
2017-12-10T11:17:50.803365: step 76, loss 3.85306, acc 0.734375, prec 0.0206593, recall 0.652778
2017-12-10T11:17:51.239309: step 77, loss 24.0942, acc 0.71875, prec 0.0209333, recall 0.648649
2017-12-10T11:17:51.682926: step 78, loss 7.77976, acc 0.734375, prec 0.0212121, recall 0.644737
2017-12-10T11:17:52.143050: step 79, loss 2.13122, acc 0.640625, prec 0.021003, recall 0.644737
2017-12-10T11:17:52.590324: step 80, loss 2.21853, acc 0.59375, prec 0.0207715, recall 0.644737
2017-12-10T11:17:53.027976: step 81, loss 2.46097, acc 0.578125, prec 0.0205365, recall 0.644737
2017-12-10T11:17:53.467318: step 82, loss 3.20935, acc 0.46875, prec 0.0206526, recall 0.649351
2017-12-10T11:17:53.904682: step 83, loss 3.23742, acc 0.5, prec 0.0211813, recall 0.658228
2017-12-10T11:17:54.340744: step 84, loss 3.68892, acc 0.578125, prec 0.0213537, recall 0.654321
2017-12-10T11:17:54.780539: step 85, loss 2.34942, acc 0.640625, prec 0.0211577, recall 0.654321
2017-12-10T11:17:55.235612: step 86, loss 2.90336, acc 0.515625, prec 0.0208991, recall 0.654321
2017-12-10T11:17:55.662851: step 87, loss 8.90786, acc 0.515625, prec 0.0217984, recall 0.658824
2017-12-10T11:17:56.103757: step 88, loss 7.5399, acc 0.5625, prec 0.0219484, recall 0.655172
2017-12-10T11:17:56.539217: step 89, loss 4.06241, acc 0.46875, prec 0.0224079, recall 0.662921
2017-12-10T11:17:56.969422: step 90, loss 4.89999, acc 0.390625, prec 0.0220808, recall 0.662921
2017-12-10T11:17:57.409730: step 91, loss 3.17106, acc 0.53125, prec 0.0218357, recall 0.662921
2017-12-10T11:17:57.857887: step 92, loss 3.53136, acc 0.484375, prec 0.0215722, recall 0.662921
2017-12-10T11:17:58.302043: step 93, loss 3.44245, acc 0.53125, prec 0.0220455, recall 0.67033
2017-12-10T11:17:58.741032: step 94, loss 2.28667, acc 0.59375, prec 0.0218403, recall 0.67033
2017-12-10T11:17:59.183678: step 95, loss 2.76867, acc 0.546875, prec 0.0219625, recall 0.673913
2017-12-10T11:17:59.618914: step 96, loss 3.64338, acc 0.625, prec 0.0221208, recall 0.677419
2017-12-10T11:18:00.047300: step 97, loss 2.52123, acc 0.609375, prec 0.0219283, recall 0.677419
2017-12-10T11:18:00.490270: step 98, loss 18.5911, acc 0.609375, prec 0.0217466, recall 0.670213
2017-12-10T11:18:00.931116: step 99, loss 2.28997, acc 0.6875, prec 0.0219328, recall 0.673684
2017-12-10T11:18:01.369093: step 100, loss 1.9531, acc 0.703125, prec 0.0217909, recall 0.673684
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-100

2017-12-10T11:18:03.185710: step 101, loss 5.82766, acc 0.640625, prec 0.0219595, recall 0.670103
2017-12-10T11:18:03.627683: step 102, loss 2.834, acc 0.640625, prec 0.022118, recall 0.673469
2017-12-10T11:18:04.082167: step 103, loss 2.08023, acc 0.703125, prec 0.0223036, recall 0.676768
2017-12-10T11:18:04.524320: step 104, loss 2.00428, acc 0.6875, prec 0.0221561, recall 0.676768
2017-12-10T11:18:04.978809: step 105, loss 2.33717, acc 0.71875, prec 0.0223464, recall 0.68
2017-12-10T11:18:05.420584: step 106, loss 6.21954, acc 0.5, prec 0.0227568, recall 0.679612
2017-12-10T11:18:05.870644: step 107, loss 1.06882, acc 0.78125, prec 0.0229699, recall 0.682692
2017-12-10T11:18:06.314568: step 108, loss 5.82186, acc 0.671875, prec 0.0228222, recall 0.67619
2017-12-10T11:18:06.751279: step 109, loss 1.80618, acc 0.6875, prec 0.0226765, recall 0.67619
2017-12-10T11:18:07.188633: step 110, loss 1.91086, acc 0.640625, prec 0.0228209, recall 0.679245
2017-12-10T11:18:07.621710: step 111, loss 14.3771, acc 0.609375, prec 0.0226486, recall 0.672897
2017-12-10T11:18:08.067922: step 112, loss 3.08435, acc 0.5, prec 0.0224229, recall 0.672897
2017-12-10T11:18:08.512791: step 113, loss 2.05556, acc 0.71875, prec 0.0222979, recall 0.672897
2017-12-10T11:18:08.954063: step 114, loss 2.22964, acc 0.75, prec 0.0224892, recall 0.675926
2017-12-10T11:18:09.384150: step 115, loss 15.313, acc 0.640625, prec 0.0223378, recall 0.669725
2017-12-10T11:18:09.822085: step 116, loss 3.39503, acc 0.5, prec 0.0224174, recall 0.672727
2017-12-10T11:18:10.247940: step 117, loss 3.45654, acc 0.515625, prec 0.0222089, recall 0.672727
2017-12-10T11:18:10.684779: step 118, loss 5.22991, acc 0.375, prec 0.0222354, recall 0.675676
2017-12-10T11:18:11.140414: step 119, loss 3.07918, acc 0.515625, prec 0.0226072, recall 0.681416
2017-12-10T11:18:11.571399: step 120, loss 3.90008, acc 0.546875, prec 0.0227008, recall 0.684211
2017-12-10T11:18:12.024717: step 121, loss 3.28791, acc 0.546875, prec 0.0225108, recall 0.684211
2017-12-10T11:18:12.470200: step 122, loss 3.10625, acc 0.5625, prec 0.0228898, recall 0.689655
2017-12-10T11:18:12.913463: step 123, loss 2.23538, acc 0.578125, prec 0.0227144, recall 0.689655
2017-12-10T11:18:13.344529: step 124, loss 2.62935, acc 0.640625, prec 0.022567, recall 0.689655
2017-12-10T11:18:13.784560: step 125, loss 2.86734, acc 0.625, prec 0.0226891, recall 0.692308
2017-12-10T11:18:14.220968: step 126, loss 2.55125, acc 0.6875, prec 0.0231069, recall 0.697479
2017-12-10T11:18:14.665862: step 127, loss 1.54925, acc 0.71875, prec 0.0229917, recall 0.697479
2017-12-10T11:18:15.106539: step 128, loss 2.32606, acc 0.65625, prec 0.0231214, recall 0.7
2017-12-10T11:18:15.547300: step 129, loss 2.4583, acc 0.625, prec 0.0232367, recall 0.702479
2017-12-10T11:18:16.004311: step 130, loss 10.3868, acc 0.734375, prec 0.0231418, recall 0.691057
2017-12-10T11:18:16.448098: step 131, loss 1.73844, acc 0.71875, prec 0.023029, recall 0.691057
2017-12-10T11:18:16.874672: step 132, loss 6.31096, acc 0.765625, prec 0.022942, recall 0.685484
2017-12-10T11:18:17.326678: step 133, loss 2.21321, acc 0.671875, prec 0.0228127, recall 0.685484
2017-12-10T11:18:17.770671: step 134, loss 1.67078, acc 0.734375, prec 0.0227091, recall 0.685484
2017-12-10T11:18:18.202004: step 135, loss 1.62396, acc 0.703125, prec 0.0225944, recall 0.685484
2017-12-10T11:18:18.635969: step 136, loss 0.833878, acc 0.8125, prec 0.0225225, recall 0.685484
2017-12-10T11:18:19.084778: step 137, loss 2.16633, acc 0.65625, prec 0.022392, recall 0.685484
2017-12-10T11:18:19.535540: step 138, loss 1.09794, acc 0.734375, prec 0.0225485, recall 0.688
2017-12-10T11:18:19.973814: step 139, loss 2.27433, acc 0.640625, prec 0.0224133, recall 0.688
2017-12-10T11:18:20.419560: step 140, loss 3.6007, acc 0.671875, prec 0.0222971, recall 0.68254
2017-12-10T11:18:20.873241: step 141, loss 1.75407, acc 0.671875, prec 0.0226804, recall 0.6875
2017-12-10T11:18:21.321198: step 142, loss 1.8033, acc 0.75, prec 0.0225873, recall 0.6875
2017-12-10T11:18:21.775568: step 143, loss 1.28834, acc 0.765625, prec 0.0225006, recall 0.6875
2017-12-10T11:18:22.203890: step 144, loss 10.5363, acc 0.75, prec 0.0226636, recall 0.684615
2017-12-10T11:18:22.646688: step 145, loss 1.0949, acc 0.765625, prec 0.023073, recall 0.689394
2017-12-10T11:18:23.089321: step 146, loss 4.43863, acc 0.796875, prec 0.023003, recall 0.684211
2017-12-10T11:18:23.525164: step 147, loss 2.61126, acc 0.609375, prec 0.023104, recall 0.686567
2017-12-10T11:18:23.961408: step 148, loss 1.37419, acc 0.8125, prec 0.0232791, recall 0.688889
2017-12-10T11:18:24.395279: step 149, loss 1.49647, acc 0.75, prec 0.0231862, recall 0.688889
2017-12-10T11:18:24.830401: step 150, loss 1.30966, acc 0.734375, prec 0.0230884, recall 0.688889
2017-12-10T11:18:25.274771: step 151, loss 19.604, acc 0.640625, prec 0.0229686, recall 0.678832
2017-12-10T11:18:25.731259: step 152, loss 2.72289, acc 0.609375, prec 0.0228277, recall 0.678832
2017-12-10T11:18:26.173581: step 153, loss 4.7753, acc 0.578125, prec 0.0226829, recall 0.673913
2017-12-10T11:18:26.604936: step 154, loss 12.1378, acc 0.5625, prec 0.0225345, recall 0.669065
2017-12-10T11:18:27.072360: step 155, loss 2.62739, acc 0.609375, prec 0.0226342, recall 0.671429
2017-12-10T11:18:27.504013: step 156, loss 3.16657, acc 0.609375, prec 0.0224988, recall 0.671429
2017-12-10T11:18:27.939780: step 157, loss 3.62599, acc 0.53125, prec 0.0228029, recall 0.676056
2017-12-10T11:18:28.379155: step 158, loss 3.95369, acc 0.453125, prec 0.022845, recall 0.678322
2017-12-10T11:18:28.820153: step 159, loss 4.8421, acc 0.421875, prec 0.0228758, recall 0.680556
2017-12-10T11:18:29.261163: step 160, loss 5.58065, acc 0.375, prec 0.0226642, recall 0.680556
2017-12-10T11:18:29.698488: step 161, loss 3.47721, acc 0.421875, prec 0.0224719, recall 0.680556
2017-12-10T11:18:30.142686: step 162, loss 6.88206, acc 0.5625, prec 0.0225564, recall 0.678082
2017-12-10T11:18:30.583876: step 163, loss 5.00217, acc 0.484375, prec 0.0226091, recall 0.680272
2017-12-10T11:18:31.028132: step 164, loss 8.47759, acc 0.390625, prec 0.0224165, recall 0.675676
2017-12-10T11:18:31.499712: step 165, loss 3.94141, acc 0.546875, prec 0.022707, recall 0.68
2017-12-10T11:18:31.938671: step 166, loss 3.95559, acc 0.484375, prec 0.0227574, recall 0.682119
2017-12-10T11:18:32.392735: step 167, loss 4.18725, acc 0.4375, prec 0.0225778, recall 0.682119
2017-12-10T11:18:32.835957: step 168, loss 3.86852, acc 0.46875, prec 0.0226234, recall 0.684211
2017-12-10T11:18:33.271827: step 169, loss 3.66591, acc 0.515625, prec 0.0224719, recall 0.684211
2017-12-10T11:18:33.703675: step 170, loss 2.894, acc 0.609375, prec 0.0227712, recall 0.688312
2017-12-10T11:18:34.141912: step 171, loss 3.4776, acc 0.515625, prec 0.0228291, recall 0.690323
2017-12-10T11:18:34.580524: step 172, loss 2.99589, acc 0.609375, prec 0.0229153, recall 0.692308
2017-12-10T11:18:35.016380: step 173, loss 1.99588, acc 0.71875, prec 0.0228282, recall 0.692308
2017-12-10T11:18:35.457727: step 174, loss 1.77763, acc 0.71875, prec 0.0227416, recall 0.692308
2017-12-10T11:18:35.916649: step 175, loss 8.99669, acc 0.765625, prec 0.0228799, recall 0.689873
2017-12-10T11:18:36.372236: step 176, loss 1.93976, acc 0.765625, prec 0.0230126, recall 0.691824
2017-12-10T11:18:36.816693: step 177, loss 11.4232, acc 0.78125, prec 0.0229501, recall 0.6875
2017-12-10T11:18:37.254946: step 178, loss 20.4283, acc 0.75, prec 0.0228785, recall 0.68323
2017-12-10T11:18:37.699351: step 179, loss 2.05153, acc 0.75, prec 0.0228027, recall 0.68323
2017-12-10T11:18:38.131539: step 180, loss 1.89202, acc 0.6875, prec 0.0227085, recall 0.68323
2017-12-10T11:18:38.567532: step 181, loss 1.98188, acc 0.6875, prec 0.022816, recall 0.685185
2017-12-10T11:18:38.996148: step 182, loss 1.70656, acc 0.734375, prec 0.0229367, recall 0.687117
2017-12-10T11:18:39.443932: step 183, loss 1.28703, acc 0.75, prec 0.0230612, recall 0.689024
2017-12-10T11:18:39.890972: step 184, loss 1.68212, acc 0.765625, prec 0.0233882, recall 0.692771
2017-12-10T11:18:40.330159: step 185, loss 2.21554, acc 0.65625, prec 0.0232841, recall 0.692771
2017-12-10T11:18:40.772944: step 186, loss 4.3334, acc 0.765625, prec 0.0236125, recall 0.692308
2017-12-10T11:18:41.219712: step 187, loss 1.45072, acc 0.75, prec 0.0235365, recall 0.692308
2017-12-10T11:18:41.665336: step 188, loss 9.81656, acc 0.6875, prec 0.0238381, recall 0.69186
2017-12-10T11:18:42.105891: step 189, loss 1.35707, acc 0.703125, prec 0.0237478, recall 0.69186
2017-12-10T11:18:42.551877: step 190, loss 1.74538, acc 0.78125, prec 0.0238758, recall 0.693642
2017-12-10T11:18:43.014456: step 191, loss 1.50013, acc 0.6875, prec 0.0237812, recall 0.693642
2017-12-10T11:18:43.459820: step 192, loss 1.48543, acc 0.796875, prec 0.0237201, recall 0.693642
2017-12-10T11:18:43.905542: step 193, loss 17.1982, acc 0.59375, prec 0.0236035, recall 0.689655
2017-12-10T11:18:44.342525: step 194, loss 2.10751, acc 0.703125, prec 0.0238981, recall 0.693182
2017-12-10T11:18:44.785797: step 195, loss 14.0017, acc 0.609375, prec 0.0239766, recall 0.691011
2017-12-10T11:18:45.238712: step 196, loss 11.8213, acc 0.640625, prec 0.024253, recall 0.690608
2017-12-10T11:18:45.684275: step 197, loss 4.5164, acc 0.5, prec 0.0242915, recall 0.692308
2017-12-10T11:18:46.120754: step 198, loss 4.40265, acc 0.359375, prec 0.0242876, recall 0.693989
2017-12-10T11:18:46.576552: step 199, loss 4.25435, acc 0.453125, prec 0.0243115, recall 0.695652
2017-12-10T11:18:47.016335: step 200, loss 5.76274, acc 0.390625, prec 0.0243167, recall 0.697297
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-200

2017-12-10T11:18:49.007658: step 201, loss 2.86502, acc 0.484375, prec 0.0241664, recall 0.697297
2017-12-10T11:18:49.448880: step 202, loss 4.24882, acc 0.421875, prec 0.024, recall 0.697297
2017-12-10T11:18:49.894094: step 203, loss 3.95832, acc 0.453125, prec 0.0238447, recall 0.697297
2017-12-10T11:18:50.323758: step 204, loss 4.26036, acc 0.515625, prec 0.0238883, recall 0.698925
2017-12-10T11:18:50.765444: step 205, loss 3.74724, acc 0.546875, prec 0.0239401, recall 0.700535
2017-12-10T11:18:51.204656: step 206, loss 3.43695, acc 0.4375, prec 0.0237836, recall 0.700535
2017-12-10T11:18:51.636307: step 207, loss 2.58799, acc 0.578125, prec 0.0236676, recall 0.700535
2017-12-10T11:18:52.072737: step 208, loss 16.6994, acc 0.5625, prec 0.0239037, recall 0.7
2017-12-10T11:18:52.503138: step 209, loss 12.8155, acc 0.59375, prec 0.0239714, recall 0.697917
2017-12-10T11:18:52.944285: step 210, loss 1.91344, acc 0.703125, prec 0.0240642, recall 0.699482
2017-12-10T11:18:53.380873: step 211, loss 2.21133, acc 0.640625, prec 0.0241392, recall 0.701031
2017-12-10T11:18:53.841544: step 212, loss 1.96377, acc 0.71875, prec 0.0242349, recall 0.702564
2017-12-10T11:18:54.285863: step 213, loss 2.0171, acc 0.6875, prec 0.0241495, recall 0.702564
2017-12-10T11:18:54.725775: step 214, loss 3.40102, acc 0.703125, prec 0.0242446, recall 0.700508
2017-12-10T11:18:55.174388: step 215, loss 1.59031, acc 0.71875, prec 0.0241681, recall 0.700508
2017-12-10T11:18:55.610926: step 216, loss 5.96438, acc 0.65625, prec 0.0242498, recall 0.698492
2017-12-10T11:18:56.055883: step 217, loss 1.5508, acc 0.75, prec 0.0241823, recall 0.698492
2017-12-10T11:18:56.505944: step 218, loss 1.44355, acc 0.765625, prec 0.0244579, recall 0.701493
2017-12-10T11:18:56.950042: step 219, loss 2.98143, acc 0.640625, prec 0.0245293, recall 0.70297
2017-12-10T11:18:57.387926: step 220, loss 1.28114, acc 0.765625, prec 0.0246339, recall 0.704434
2017-12-10T11:18:57.818482: step 221, loss 1.56811, acc 0.796875, prec 0.0245789, recall 0.704434
2017-12-10T11:18:58.251802: step 222, loss 1.71602, acc 0.703125, prec 0.0244989, recall 0.704434
2017-12-10T11:18:58.682559: step 223, loss 1.64363, acc 0.71875, prec 0.0244236, recall 0.704434
2017-12-10T11:18:59.105245: step 224, loss 42.0227, acc 0.671875, prec 0.0245106, recall 0.699029
2017-12-10T11:18:59.559598: step 225, loss 1.62085, acc 0.65625, prec 0.0245846, recall 0.700483
2017-12-10T11:18:59.998579: step 226, loss 5.05944, acc 0.6875, prec 0.0246705, recall 0.698565
2017-12-10T11:19:00.439913: step 227, loss 2.81466, acc 0.640625, prec 0.0247391, recall 0.7
2017-12-10T11:19:00.904666: step 228, loss 2.81722, acc 0.5625, prec 0.0246231, recall 0.7
2017-12-10T11:19:01.347510: step 229, loss 2.10117, acc 0.578125, prec 0.0246749, recall 0.701422
2017-12-10T11:19:01.787154: step 230, loss 3.6666, acc 0.421875, prec 0.0245236, recall 0.701422
2017-12-10T11:19:02.231691: step 231, loss 8.68344, acc 0.40625, prec 0.024856, recall 0.702326
2017-12-10T11:19:02.684211: step 232, loss 3.38093, acc 0.5, prec 0.0247257, recall 0.702326
2017-12-10T11:19:03.120199: step 233, loss 3.60427, acc 0.515625, prec 0.0249186, recall 0.705069
2017-12-10T11:19:03.541212: step 234, loss 4.39069, acc 0.34375, prec 0.024907, recall 0.706422
2017-12-10T11:19:03.980804: step 235, loss 6.26586, acc 0.515625, prec 0.0249437, recall 0.704545
2017-12-10T11:19:04.428338: step 236, loss 3.5595, acc 0.484375, prec 0.024968, recall 0.705882
2017-12-10T11:19:04.869605: step 237, loss 4.66311, acc 0.359375, prec 0.0248052, recall 0.705882
2017-12-10T11:19:05.291547: step 238, loss 4.76997, acc 0.390625, prec 0.0248064, recall 0.707207
2017-12-10T11:19:05.722662: step 239, loss 2.43213, acc 0.59375, prec 0.024705, recall 0.707207
2017-12-10T11:19:06.164374: step 240, loss 3.48429, acc 0.578125, prec 0.0247533, recall 0.70852
2017-12-10T11:19:06.616921: step 241, loss 2.72009, acc 0.59375, prec 0.0246528, recall 0.70852
2017-12-10T11:19:07.055496: step 242, loss 9.37277, acc 0.640625, prec 0.0245685, recall 0.705357
2017-12-10T11:19:07.496265: step 243, loss 1.29347, acc 0.78125, prec 0.0248177, recall 0.707965
2017-12-10T11:19:07.930822: step 244, loss 1.6102, acc 0.625, prec 0.025027, recall 0.710526
2017-12-10T11:19:08.377803: step 245, loss 1.29406, acc 0.734375, prec 0.0249615, recall 0.710526
2017-12-10T11:19:08.810592: step 246, loss 2.03702, acc 0.703125, prec 0.0248886, recall 0.710526
2017-12-10T11:19:09.247801: step 247, loss 1.59136, acc 0.671875, prec 0.0248086, recall 0.710526
2017-12-10T11:19:09.677324: step 248, loss 0.853429, acc 0.8125, prec 0.0247631, recall 0.710526
2017-12-10T11:19:10.103007: step 249, loss 10.1537, acc 0.796875, prec 0.0247177, recall 0.707424
2017-12-10T11:19:10.554470: step 250, loss 0.818732, acc 0.796875, prec 0.0248173, recall 0.708696
2017-12-10T11:19:10.991192: step 251, loss 0.882207, acc 0.765625, prec 0.0249089, recall 0.709957
2017-12-10T11:19:11.432612: step 252, loss 1.22731, acc 0.8125, prec 0.0248636, recall 0.709957
2017-12-10T11:19:11.865904: step 253, loss 0.729308, acc 0.796875, prec 0.0248146, recall 0.709957
2017-12-10T11:19:12.302880: step 254, loss 0.787801, acc 0.796875, prec 0.0247659, recall 0.709957
2017-12-10T11:19:12.735455: step 255, loss 24.1093, acc 0.765625, prec 0.0248606, recall 0.708154
2017-12-10T11:19:13.170747: step 256, loss 0.974627, acc 0.84375, prec 0.0248232, recall 0.708154
2017-12-10T11:19:13.602716: step 257, loss 0.938613, acc 0.8125, prec 0.0247785, recall 0.708154
2017-12-10T11:19:14.024499: step 258, loss 7.67627, acc 0.703125, prec 0.0247117, recall 0.705128
2017-12-10T11:19:14.466025: step 259, loss 0.708115, acc 0.875, prec 0.0246821, recall 0.705128
2017-12-10T11:19:14.901646: step 260, loss 1.17684, acc 0.78125, prec 0.0247761, recall 0.706383
2017-12-10T11:19:15.343906: step 261, loss 5.83513, acc 0.734375, prec 0.0250074, recall 0.705882
2017-12-10T11:19:15.780926: step 262, loss 1.5577, acc 0.671875, prec 0.0249295, recall 0.705882
2017-12-10T11:19:16.219301: step 263, loss 2.5175, acc 0.609375, prec 0.0248374, recall 0.705882
2017-12-10T11:19:16.665598: step 264, loss 1.56734, acc 0.75, prec 0.0247788, recall 0.705882
2017-12-10T11:19:17.096771: step 265, loss 4.81585, acc 0.6875, prec 0.0249963, recall 0.705394
2017-12-10T11:19:17.539715: step 266, loss 2.19555, acc 0.6875, prec 0.024923, recall 0.705394
2017-12-10T11:19:17.990952: step 267, loss 10.5306, acc 0.65625, prec 0.024989, recall 0.703704
2017-12-10T11:19:18.450837: step 268, loss 3.99087, acc 0.625, prec 0.0249053, recall 0.70082
2017-12-10T11:19:18.897744: step 269, loss 3.77533, acc 0.53125, prec 0.0249384, recall 0.702041
2017-12-10T11:19:19.333785: step 270, loss 4.69179, acc 0.34375, prec 0.0247874, recall 0.702041
2017-12-10T11:19:19.771368: step 271, loss 3.53418, acc 0.5, prec 0.0246736, recall 0.702041
2017-12-10T11:19:20.228092: step 272, loss 3.78308, acc 0.40625, prec 0.024679, recall 0.703252
2017-12-10T11:19:20.670011: step 273, loss 3.42925, acc 0.484375, prec 0.0248403, recall 0.705645
2017-12-10T11:19:21.111212: step 274, loss 3.90228, acc 0.421875, prec 0.0247105, recall 0.705645
2017-12-10T11:19:21.560982: step 275, loss 21.9905, acc 0.421875, prec 0.0249965, recall 0.706349
2017-12-10T11:19:21.999134: step 276, loss 4.13171, acc 0.46875, prec 0.0254225, recall 0.710938
2017-12-10T11:19:22.435549: step 277, loss 2.81108, acc 0.53125, prec 0.0255875, recall 0.713178
2017-12-10T11:19:22.874670: step 278, loss 2.87449, acc 0.546875, prec 0.0256197, recall 0.714286
2017-12-10T11:19:23.305945: step 279, loss 2.08678, acc 0.625, prec 0.0256693, recall 0.715385
2017-12-10T11:19:23.755515: step 280, loss 3.06867, acc 0.59375, prec 0.0257115, recall 0.716475
2017-12-10T11:19:24.200700: step 281, loss 2.36562, acc 0.53125, prec 0.0257393, recall 0.717557
2017-12-10T11:19:24.633605: step 282, loss 1.51635, acc 0.703125, prec 0.0259386, recall 0.719697
2017-12-10T11:19:25.075799: step 283, loss 1.85797, acc 0.6875, prec 0.0258679, recall 0.719697
2017-12-10T11:19:25.506001: step 284, loss 1.07765, acc 0.703125, prec 0.0260657, recall 0.721804
2017-12-10T11:19:25.945158: step 285, loss 0.888545, acc 0.75, prec 0.0261411, recall 0.722846
2017-12-10T11:19:26.403487: step 286, loss 0.79372, acc 0.859375, prec 0.0261093, recall 0.722846
2017-12-10T11:19:26.832649: step 287, loss 0.671912, acc 0.84375, prec 0.0263371, recall 0.724907
2017-12-10T11:19:27.276700: step 288, loss 5.8261, acc 0.890625, prec 0.0263158, recall 0.722222
2017-12-10T11:19:27.719694: step 289, loss 15.0166, acc 0.8125, prec 0.0262768, recall 0.719557
2017-12-10T11:19:28.159620: step 290, loss 0.817555, acc 0.84375, prec 0.0263724, recall 0.720588
2017-12-10T11:19:28.601999: step 291, loss 0.631157, acc 0.796875, prec 0.0263264, recall 0.720588
2017-12-10T11:19:29.045997: step 292, loss 10.7187, acc 0.734375, prec 0.0262699, recall 0.717949
2017-12-10T11:19:29.481436: step 293, loss 0.640542, acc 0.859375, prec 0.0263686, recall 0.718978
2017-12-10T11:19:29.921334: step 294, loss 1.63835, acc 0.765625, prec 0.0264458, recall 0.72
2017-12-10T11:19:30.380506: step 295, loss 1.16039, acc 0.75, prec 0.0263894, recall 0.72
2017-12-10T11:19:30.834104: step 296, loss 7.40088, acc 0.734375, prec 0.0263333, recall 0.717391
2017-12-10T11:19:31.287778: step 297, loss 1.31808, acc 0.75, prec 0.0262774, recall 0.717391
2017-12-10T11:19:31.723473: step 298, loss 3.49956, acc 0.609375, prec 0.0263228, recall 0.715827
2017-12-10T11:19:32.171996: step 299, loss 2.17777, acc 0.59375, prec 0.0263609, recall 0.716846
2017-12-10T11:19:32.612969: step 300, loss 1.63988, acc 0.65625, prec 0.0262847, recall 0.716846
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-300

2017-12-10T11:19:34.502498: step 301, loss 14.8618, acc 0.65625, prec 0.0262123, recall 0.714286
2017-12-10T11:19:34.955239: step 302, loss 2.97758, acc 0.53125, prec 0.0261097, recall 0.714286
2017-12-10T11:19:35.399399: step 303, loss 3.39352, acc 0.59375, prec 0.026148, recall 0.715302
2017-12-10T11:19:35.849648: step 304, loss 3.63011, acc 0.421875, prec 0.0260228, recall 0.715302
2017-12-10T11:19:36.282159: step 305, loss 3.09633, acc 0.53125, prec 0.0260477, recall 0.716312
2017-12-10T11:19:36.715839: step 306, loss 2.1268, acc 0.578125, prec 0.0259573, recall 0.716312
2017-12-10T11:19:37.159872: step 307, loss 3.52697, acc 0.5625, prec 0.0258643, recall 0.716312
2017-12-10T11:19:37.622457: step 308, loss 2.97387, acc 0.53125, prec 0.0257653, recall 0.716312
2017-12-10T11:19:38.067188: step 309, loss 3.12548, acc 0.59375, prec 0.025804, recall 0.717314
2017-12-10T11:19:38.512899: step 310, loss 3.22112, acc 0.5625, prec 0.0259592, recall 0.719298
2017-12-10T11:19:38.954159: step 311, loss 5.88197, acc 0.625, prec 0.0258838, recall 0.716783
2017-12-10T11:19:39.399677: step 312, loss 3.46754, acc 0.546875, prec 0.0260345, recall 0.71875
2017-12-10T11:19:39.841906: step 313, loss 2.11688, acc 0.59375, prec 0.0259496, recall 0.71875
2017-12-10T11:19:40.286744: step 314, loss 1.99948, acc 0.59375, prec 0.0258653, recall 0.71875
2017-12-10T11:19:40.727429: step 315, loss 1.52191, acc 0.734375, prec 0.0258105, recall 0.71875
2017-12-10T11:19:41.177646: step 316, loss 10.4597, acc 0.71875, prec 0.0258771, recall 0.717241
2017-12-10T11:19:41.624716: step 317, loss 1.33254, acc 0.6875, prec 0.0258129, recall 0.717241
2017-12-10T11:19:42.063373: step 318, loss 1.94563, acc 0.703125, prec 0.0258727, recall 0.718213
2017-12-10T11:19:42.514700: step 319, loss 2.1486, acc 0.609375, prec 0.0257929, recall 0.718213
2017-12-10T11:19:42.961374: step 320, loss 6.30317, acc 0.640625, prec 0.0257231, recall 0.715753
2017-12-10T11:19:43.405078: step 321, loss 1.43452, acc 0.71875, prec 0.0256662, recall 0.715753
2017-12-10T11:19:43.843722: step 322, loss 1.56855, acc 0.6875, prec 0.0256033, recall 0.715753
2017-12-10T11:19:44.281623: step 323, loss 1.18925, acc 0.71875, prec 0.0256661, recall 0.716724
2017-12-10T11:19:44.738274: step 324, loss 5.19591, acc 0.609375, prec 0.025591, recall 0.714286
2017-12-10T11:19:45.200503: step 325, loss 2.0212, acc 0.65625, prec 0.0255226, recall 0.714286
2017-12-10T11:19:45.653125: step 326, loss 2.34366, acc 0.625, prec 0.0254484, recall 0.714286
2017-12-10T11:19:46.104650: step 327, loss 2.41967, acc 0.65625, prec 0.0254985, recall 0.715254
2017-12-10T11:19:46.547415: step 328, loss 6.32666, acc 0.703125, prec 0.0254431, recall 0.712838
2017-12-10T11:19:46.982370: step 329, loss 2.00415, acc 0.625, prec 0.0253697, recall 0.712838
2017-12-10T11:19:47.428192: step 330, loss 2.75241, acc 0.59375, prec 0.0252907, recall 0.712838
2017-12-10T11:19:47.870062: step 331, loss 2.21751, acc 0.65625, prec 0.0253407, recall 0.713805
2017-12-10T11:19:48.315196: step 332, loss 2.75609, acc 0.578125, prec 0.0253753, recall 0.714765
2017-12-10T11:19:48.756942: step 333, loss 12.9045, acc 0.65625, prec 0.0254278, recall 0.713333
2017-12-10T11:19:49.205727: step 334, loss 5.91647, acc 0.609375, prec 0.0255894, recall 0.710526
2017-12-10T11:19:49.655065: step 335, loss 1.87347, acc 0.671875, prec 0.0255259, recall 0.710526
2017-12-10T11:19:50.090368: step 336, loss 2.60385, acc 0.546875, prec 0.0254387, recall 0.710526
2017-12-10T11:19:50.545130: step 337, loss 4.13613, acc 0.453125, prec 0.0253343, recall 0.710526
2017-12-10T11:19:50.976387: step 338, loss 3.85135, acc 0.4375, prec 0.0252277, recall 0.710526
2017-12-10T11:19:51.426116: step 339, loss 3.00032, acc 0.5625, prec 0.0253724, recall 0.712418
2017-12-10T11:19:51.873738: step 340, loss 2.57157, acc 0.546875, prec 0.0255132, recall 0.714286
2017-12-10T11:19:52.304576: step 341, loss 3.63775, acc 0.5, prec 0.0255314, recall 0.71521
2017-12-10T11:19:52.747038: step 342, loss 4.89558, acc 0.546875, prec 0.0254491, recall 0.712903
2017-12-10T11:19:53.183091: step 343, loss 3.16926, acc 0.5625, prec 0.0253673, recall 0.712903
2017-12-10T11:19:53.620852: step 344, loss 4.14896, acc 0.640625, prec 0.0256381, recall 0.713376
2017-12-10T11:19:54.061325: step 345, loss 3.90743, acc 0.53125, prec 0.0258837, recall 0.716088
2017-12-10T11:19:54.505823: step 346, loss 3.15859, acc 0.5, prec 0.0260109, recall 0.717868
2017-12-10T11:19:54.968081: step 347, loss 20.7049, acc 0.515625, prec 0.0259226, recall 0.715625
2017-12-10T11:19:55.416227: step 348, loss 2.31286, acc 0.625, prec 0.0260722, recall 0.717391
2017-12-10T11:19:55.862331: step 349, loss 6.20586, acc 0.546875, prec 0.0262092, recall 0.716923
2017-12-10T11:19:56.302859: step 350, loss 2.14384, acc 0.625, prec 0.0261387, recall 0.716923
2017-12-10T11:19:56.752458: step 351, loss 3.02531, acc 0.53125, prec 0.026051, recall 0.716923
2017-12-10T11:19:57.176031: step 352, loss 2.35705, acc 0.546875, prec 0.0260753, recall 0.717791
2017-12-10T11:19:57.614400: step 353, loss 2.54504, acc 0.546875, prec 0.0260995, recall 0.718654
2017-12-10T11:19:58.054641: step 354, loss 3.11188, acc 0.5625, prec 0.0260186, recall 0.718654
2017-12-10T11:19:58.501301: step 355, loss 1.96319, acc 0.65625, prec 0.0259554, recall 0.718654
2017-12-10T11:19:58.957984: step 356, loss 2.7373, acc 0.59375, prec 0.0259883, recall 0.719512
2017-12-10T11:19:59.408900: step 357, loss 16.1604, acc 0.578125, prec 0.0260211, recall 0.718182
2017-12-10T11:19:59.855159: step 358, loss 11.6988, acc 0.578125, prec 0.025947, recall 0.716012
2017-12-10T11:20:00.302087: step 359, loss 9.15888, acc 0.578125, prec 0.0258734, recall 0.713855
2017-12-10T11:20:00.741750: step 360, loss 2.30907, acc 0.640625, prec 0.0259146, recall 0.714715
2017-12-10T11:20:01.183112: step 361, loss 7.31521, acc 0.609375, prec 0.0258471, recall 0.712575
2017-12-10T11:20:01.615308: step 362, loss 2.73404, acc 0.578125, prec 0.0257715, recall 0.712575
2017-12-10T11:20:02.063325: step 363, loss 3.06302, acc 0.53125, prec 0.0256881, recall 0.712575
2017-12-10T11:20:02.501454: step 364, loss 2.22459, acc 0.546875, prec 0.0256079, recall 0.712575
2017-12-10T11:20:02.932654: step 365, loss 3.63411, acc 0.390625, prec 0.0257097, recall 0.714286
2017-12-10T11:20:03.363363: step 366, loss 4.29579, acc 0.453125, prec 0.0256137, recall 0.714286
2017-12-10T11:20:03.798441: step 367, loss 3.26704, acc 0.4375, prec 0.0255156, recall 0.714286
2017-12-10T11:20:04.238995: step 368, loss 3.50344, acc 0.5625, prec 0.0256465, recall 0.715976
2017-12-10T11:20:04.682490: step 369, loss 2.32913, acc 0.53125, prec 0.0255652, recall 0.715976
2017-12-10T11:20:05.121606: step 370, loss 1.88352, acc 0.640625, prec 0.0257086, recall 0.717647
2017-12-10T11:20:05.556482: step 371, loss 3.11781, acc 0.53125, prec 0.0257299, recall 0.718475
2017-12-10T11:20:05.997240: step 372, loss 1.39747, acc 0.671875, prec 0.0256733, recall 0.718475
2017-12-10T11:20:06.436124: step 373, loss 1.85217, acc 0.640625, prec 0.0258152, recall 0.720117
2017-12-10T11:20:06.886760: step 374, loss 0.818983, acc 0.734375, prec 0.0257694, recall 0.720117
2017-12-10T11:20:07.325646: step 375, loss 1.44644, acc 0.765625, prec 0.0258306, recall 0.72093
2017-12-10T11:20:07.776811: step 376, loss 0.760048, acc 0.84375, prec 0.0258038, recall 0.72093
2017-12-10T11:20:08.212435: step 377, loss 1.28006, acc 0.765625, prec 0.0257636, recall 0.72093
2017-12-10T11:20:08.649569: step 378, loss 0.207771, acc 0.921875, prec 0.0258513, recall 0.721739
2017-12-10T11:20:09.097080: step 379, loss 1.9562, acc 0.890625, prec 0.0258352, recall 0.719653
2017-12-10T11:20:09.534510: step 380, loss 0.270971, acc 0.921875, prec 0.0258218, recall 0.719653
2017-12-10T11:20:09.976525: step 381, loss 0.729043, acc 0.90625, prec 0.0259067, recall 0.720461
2017-12-10T11:20:10.417122: step 382, loss 0.296131, acc 0.921875, prec 0.0258933, recall 0.720461
2017-12-10T11:20:10.872355: step 383, loss 0.579063, acc 0.875, prec 0.0259727, recall 0.721264
2017-12-10T11:20:11.307772: step 384, loss 7.41975, acc 0.890625, prec 0.0260573, recall 0.72
2017-12-10T11:20:11.754287: step 385, loss 0.298853, acc 0.921875, prec 0.0261445, recall 0.720798
2017-12-10T11:20:12.193540: step 386, loss 0.901054, acc 0.8125, prec 0.0261121, recall 0.720798
2017-12-10T11:20:12.638114: step 387, loss 0.364133, acc 0.875, prec 0.026191, recall 0.721591
2017-12-10T11:20:13.085736: step 388, loss 6.61632, acc 0.921875, prec 0.0261802, recall 0.719547
2017-12-10T11:20:13.538666: step 389, loss 0.392382, acc 0.890625, prec 0.0261613, recall 0.719547
2017-12-10T11:20:13.970502: step 390, loss 16.1489, acc 0.84375, prec 0.0261371, recall 0.717514
2017-12-10T11:20:14.418087: step 391, loss 2.59308, acc 0.765625, prec 0.0260995, recall 0.715493
2017-12-10T11:20:14.856422: step 392, loss 1.09679, acc 0.703125, prec 0.0260486, recall 0.715493
2017-12-10T11:20:15.295717: step 393, loss 9.71974, acc 0.78125, prec 0.0262134, recall 0.715084
2017-12-10T11:20:15.741599: step 394, loss 2.60182, acc 0.53125, prec 0.0261331, recall 0.715084
2017-12-10T11:20:16.178859: step 395, loss 2.61522, acc 0.578125, prec 0.0261604, recall 0.715877
2017-12-10T11:20:16.623929: step 396, loss 3.00788, acc 0.484375, prec 0.0260728, recall 0.715877
2017-12-10T11:20:17.052567: step 397, loss 3.47337, acc 0.4375, prec 0.0260764, recall 0.716667
2017-12-10T11:20:17.494141: step 398, loss 4.09904, acc 0.484375, prec 0.0259897, recall 0.716667
2017-12-10T11:20:17.957278: step 399, loss 2.02071, acc 0.546875, prec 0.0261097, recall 0.718232
2017-12-10T11:20:18.396492: step 400, loss 4.19687, acc 0.34375, prec 0.026, recall 0.718232
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-400

2017-12-10T11:20:20.228788: step 401, loss 2.78577, acc 0.453125, prec 0.0260064, recall 0.719008
2017-12-10T11:20:20.659396: step 402, loss 3.45591, acc 0.390625, prec 0.0259057, recall 0.719008
2017-12-10T11:20:21.087472: step 403, loss 3.24535, acc 0.453125, prec 0.025816, recall 0.719008
2017-12-10T11:20:21.521767: step 404, loss 2.68857, acc 0.5625, prec 0.0258408, recall 0.71978
2017-12-10T11:20:21.981178: step 405, loss 2.08535, acc 0.53125, prec 0.0257646, recall 0.71978
2017-12-10T11:20:22.410471: step 406, loss 1.18429, acc 0.734375, prec 0.0257216, recall 0.71978
2017-12-10T11:20:22.856862: step 407, loss 1.76493, acc 0.640625, prec 0.0258545, recall 0.721311
2017-12-10T11:20:23.290872: step 408, loss 3.7523, acc 0.71875, prec 0.0258115, recall 0.719346
2017-12-10T11:20:23.736961: step 409, loss 0.803677, acc 0.8125, prec 0.0257812, recall 0.719346
2017-12-10T11:20:24.181121: step 410, loss 7.75066, acc 0.765625, prec 0.0257461, recall 0.717391
2017-12-10T11:20:24.624306: step 411, loss 1.07867, acc 0.796875, prec 0.0257135, recall 0.717391
2017-12-10T11:20:25.054226: step 412, loss 13.1651, acc 0.859375, prec 0.0257882, recall 0.716216
2017-12-10T11:20:25.493185: step 413, loss 5.16913, acc 0.765625, prec 0.0257532, recall 0.714286
2017-12-10T11:20:25.947787: step 414, loss 4.76065, acc 0.78125, prec 0.0257207, recall 0.712366
2017-12-10T11:20:26.385955: step 415, loss 1.01757, acc 0.734375, prec 0.0256783, recall 0.712366
2017-12-10T11:20:26.836696: step 416, loss 1.32113, acc 0.65625, prec 0.0257179, recall 0.713137
2017-12-10T11:20:27.281151: step 417, loss 1.66199, acc 0.578125, prec 0.0257449, recall 0.713904
2017-12-10T11:20:27.739557: step 418, loss 4.65561, acc 0.59375, prec 0.0257767, recall 0.712766
2017-12-10T11:20:28.190692: step 419, loss 2.55674, acc 0.625, prec 0.0259042, recall 0.714286
2017-12-10T11:20:28.621839: step 420, loss 1.89954, acc 0.640625, prec 0.0258472, recall 0.714286
2017-12-10T11:20:29.055890: step 421, loss 3.37922, acc 0.453125, prec 0.0257609, recall 0.714286
2017-12-10T11:20:29.493664: step 422, loss 3.5082, acc 0.5625, prec 0.0256923, recall 0.714286
2017-12-10T11:20:29.931447: step 423, loss 2.26278, acc 0.609375, prec 0.0256313, recall 0.714286
2017-12-10T11:20:30.377377: step 424, loss 2.49106, acc 0.53125, prec 0.0256507, recall 0.71504
2017-12-10T11:20:30.806738: step 425, loss 2.92999, acc 0.546875, prec 0.0256725, recall 0.715789
2017-12-10T11:20:31.249731: step 426, loss 3.15533, acc 0.40625, prec 0.0255807, recall 0.715789
2017-12-10T11:20:31.686777: step 427, loss 1.61157, acc 0.671875, prec 0.0255303, recall 0.715789
2017-12-10T11:20:32.118967: step 428, loss 5.59678, acc 0.65625, prec 0.0254801, recall 0.713911
2017-12-10T11:20:32.553881: step 429, loss 1.83295, acc 0.625, prec 0.025514, recall 0.71466
2017-12-10T11:20:33.005803: step 430, loss 7.90159, acc 0.59375, prec 0.0255478, recall 0.711688
2017-12-10T11:20:33.444361: step 431, loss 4.08874, acc 0.71875, prec 0.0255981, recall 0.710594
2017-12-10T11:20:33.889300: step 432, loss 1.06453, acc 0.71875, prec 0.0255552, recall 0.710594
2017-12-10T11:20:34.325912: step 433, loss 1.75144, acc 0.6875, prec 0.0256886, recall 0.712082
2017-12-10T11:20:34.774122: step 434, loss 2.76751, acc 0.546875, prec 0.0257098, recall 0.712821
2017-12-10T11:20:35.201725: step 435, loss 2.63012, acc 0.546875, prec 0.025641, recall 0.712821
2017-12-10T11:20:35.644882: step 436, loss 2.25781, acc 0.609375, prec 0.0257613, recall 0.714286
2017-12-10T11:20:36.094165: step 437, loss 2.29797, acc 0.546875, prec 0.0256928, recall 0.714286
2017-12-10T11:20:36.545781: step 438, loss 2.41575, acc 0.609375, prec 0.025634, recall 0.714286
2017-12-10T11:20:36.987436: step 439, loss 2.07331, acc 0.671875, prec 0.0255848, recall 0.714286
2017-12-10T11:20:37.426397: step 440, loss 3.94199, acc 0.5625, prec 0.0255218, recall 0.712468
2017-12-10T11:20:37.869910: step 441, loss 2.65685, acc 0.59375, prec 0.0256387, recall 0.713924
2017-12-10T11:20:38.316334: step 442, loss 8.81942, acc 0.609375, prec 0.0257596, recall 0.713568
2017-12-10T11:20:38.750972: step 443, loss 2.64526, acc 0.5, prec 0.0256851, recall 0.713568
2017-12-10T11:20:39.195876: step 444, loss 5.9937, acc 0.5, prec 0.0256133, recall 0.711779
2017-12-10T11:20:39.617291: step 445, loss 3.12497, acc 0.5, prec 0.0256272, recall 0.7125
2017-12-10T11:20:40.055186: step 446, loss 3.61687, acc 0.375, prec 0.0256226, recall 0.713217
2017-12-10T11:20:40.494515: step 447, loss 2.58468, acc 0.53125, prec 0.025554, recall 0.713217
2017-12-10T11:20:40.926644: step 448, loss 2.70142, acc 0.5625, prec 0.0256639, recall 0.71464
2017-12-10T11:20:41.367580: step 449, loss 11.9925, acc 0.53125, prec 0.0255977, recall 0.712871
2017-12-10T11:20:41.821141: step 450, loss 2.54271, acc 0.609375, prec 0.0257138, recall 0.714286
2017-12-10T11:20:42.248160: step 451, loss 2.77329, acc 0.546875, prec 0.025734, recall 0.714988
2017-12-10T11:20:42.692089: step 452, loss 2.13998, acc 0.65625, prec 0.02577, recall 0.715686
2017-12-10T11:20:43.130914: step 453, loss 1.87956, acc 0.5625, prec 0.0257923, recall 0.716381
2017-12-10T11:20:43.591577: step 454, loss 1.62139, acc 0.671875, prec 0.0257447, recall 0.716381
2017-12-10T11:20:44.033390: step 455, loss 2.06755, acc 0.734375, prec 0.0258772, recall 0.717762
2017-12-10T11:20:44.473767: step 456, loss 1.50236, acc 0.640625, prec 0.0259104, recall 0.718447
2017-12-10T11:20:44.917868: step 457, loss 1.27057, acc 0.71875, prec 0.0259547, recall 0.719128
2017-12-10T11:20:45.355254: step 458, loss 1.82426, acc 0.671875, prec 0.0260771, recall 0.720482
2017-12-10T11:20:45.803872: step 459, loss 0.7088, acc 0.828125, prec 0.026137, recall 0.721154
2017-12-10T11:20:46.248532: step 460, loss 1.30943, acc 0.765625, prec 0.0261876, recall 0.721823
2017-12-10T11:20:46.682721: step 461, loss 4.29267, acc 0.734375, prec 0.0261534, recall 0.718377
2017-12-10T11:20:47.129542: step 462, loss 1.36317, acc 0.765625, prec 0.0261194, recall 0.718377
2017-12-10T11:20:47.559815: step 463, loss 0.905426, acc 0.8125, prec 0.0260922, recall 0.718377
2017-12-10T11:20:47.998545: step 464, loss 1.10652, acc 0.75, prec 0.0260561, recall 0.718377
2017-12-10T11:20:48.433873: step 465, loss 7.80124, acc 0.703125, prec 0.0260156, recall 0.716667
2017-12-10T11:20:48.868770: step 466, loss 0.807212, acc 0.828125, prec 0.0259908, recall 0.716667
2017-12-10T11:20:49.311036: step 467, loss 1.62926, acc 0.828125, prec 0.0259662, recall 0.716667
2017-12-10T11:20:49.764781: step 468, loss 1.24205, acc 0.78125, prec 0.0261866, recall 0.718676
2017-12-10T11:20:50.225144: step 469, loss 6.27053, acc 0.65625, prec 0.026223, recall 0.717647
2017-12-10T11:20:50.673895: step 470, loss 0.830831, acc 0.78125, prec 0.0261915, recall 0.717647
2017-12-10T11:20:51.115576: step 471, loss 3.66441, acc 0.75, prec 0.0262413, recall 0.716628
2017-12-10T11:20:51.547710: step 472, loss 1.17521, acc 0.796875, prec 0.0262955, recall 0.71729
2017-12-10T11:20:51.989364: step 473, loss 15.8094, acc 0.75, prec 0.0264283, recall 0.716937
2017-12-10T11:20:52.435086: step 474, loss 3.0052, acc 0.5625, prec 0.0263652, recall 0.716937
2017-12-10T11:20:52.880344: step 475, loss 2.06021, acc 0.625, prec 0.0263942, recall 0.717593
2017-12-10T11:20:53.330624: step 476, loss 2.16118, acc 0.59375, prec 0.0264186, recall 0.718245
2017-12-10T11:20:53.792815: step 477, loss 2.83035, acc 0.484375, prec 0.0265097, recall 0.71954
2017-12-10T11:20:54.236376: step 478, loss 2.28346, acc 0.59375, prec 0.026616, recall 0.720824
2017-12-10T11:20:54.684496: step 479, loss 2.71792, acc 0.5, prec 0.0265442, recall 0.720824
2017-12-10T11:20:55.119828: step 480, loss 8.25421, acc 0.609375, prec 0.0265725, recall 0.719818
2017-12-10T11:20:55.580015: step 481, loss 1.74876, acc 0.609375, prec 0.0265984, recall 0.720455
2017-12-10T11:20:56.021086: step 482, loss 2.21275, acc 0.5625, prec 0.0265361, recall 0.720455
2017-12-10T11:20:56.457015: step 483, loss 2.87011, acc 0.5, prec 0.0264652, recall 0.720455
2017-12-10T11:20:56.893434: step 484, loss 1.89868, acc 0.578125, prec 0.0264057, recall 0.720455
2017-12-10T11:20:57.328934: step 485, loss 2.96708, acc 0.546875, prec 0.0264229, recall 0.721088
2017-12-10T11:20:57.761741: step 486, loss 1.85701, acc 0.671875, prec 0.0264577, recall 0.721719
2017-12-10T11:20:58.199532: step 487, loss 2.55198, acc 0.578125, prec 0.0264791, recall 0.722348
2017-12-10T11:20:58.633444: step 488, loss 3.86149, acc 0.640625, prec 0.026431, recall 0.720721
2017-12-10T11:20:59.089533: step 489, loss 1.6626, acc 0.671875, prec 0.0263852, recall 0.720721
2017-12-10T11:20:59.559492: step 490, loss 6.36508, acc 0.6875, prec 0.0266645, recall 0.721604
2017-12-10T11:21:00.002968: step 491, loss 1.78612, acc 0.671875, prec 0.0266984, recall 0.722222
2017-12-10T11:21:00.448912: step 492, loss 10.7888, acc 0.6875, prec 0.0266568, recall 0.720621
2017-12-10T11:21:00.893462: step 493, loss 1.59287, acc 0.71875, prec 0.0266175, recall 0.720621
2017-12-10T11:21:01.334631: step 494, loss 2.10437, acc 0.578125, prec 0.0265588, recall 0.720621
2017-12-10T11:21:01.779436: step 495, loss 16.246, acc 0.578125, prec 0.0265025, recall 0.719027
2017-12-10T11:21:02.222029: step 496, loss 1.71831, acc 0.65625, prec 0.0265343, recall 0.719647
2017-12-10T11:21:02.626457: step 497, loss 3.47105, acc 0.470588, prec 0.0265551, recall 0.720264
2017-12-10T11:21:03.082345: step 498, loss 4.16144, acc 0.390625, prec 0.0264713, recall 0.720264
2017-12-10T11:21:03.516384: step 499, loss 2.41349, acc 0.546875, prec 0.0264093, recall 0.720264
2017-12-10T11:21:03.961284: step 500, loss 3.22396, acc 0.484375, prec 0.0263391, recall 0.720264
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-500

2017-12-10T11:21:05.782667: step 501, loss 1.97356, acc 0.53125, prec 0.0262756, recall 0.720264
2017-12-10T11:21:06.220089: step 502, loss 2.43148, acc 0.515625, prec 0.0262103, recall 0.720264
2017-12-10T11:21:06.665904: step 503, loss 3.05752, acc 0.515625, prec 0.0261454, recall 0.720264
2017-12-10T11:21:07.097716: step 504, loss 1.97202, acc 0.578125, prec 0.026089, recall 0.720264
2017-12-10T11:21:07.531106: step 505, loss 1.24066, acc 0.703125, prec 0.0260496, recall 0.720264
2017-12-10T11:21:07.982900: step 506, loss 1.00328, acc 0.78125, prec 0.0261755, recall 0.721491
2017-12-10T11:21:08.409257: step 507, loss 0.904433, acc 0.75, prec 0.0262196, recall 0.722101
2017-12-10T11:21:08.865545: step 508, loss 0.721287, acc 0.765625, prec 0.0261884, recall 0.722101
2017-12-10T11:21:09.298723: step 509, loss 1.12128, acc 0.75, prec 0.0261552, recall 0.722101
2017-12-10T11:21:09.742479: step 510, loss 0.870656, acc 0.84375, prec 0.0261345, recall 0.722101
2017-12-10T11:21:10.183597: step 511, loss 1.05605, acc 0.765625, prec 0.0261805, recall 0.722707
2017-12-10T11:21:10.625323: step 512, loss 0.449872, acc 0.921875, prec 0.0262471, recall 0.723312
2017-12-10T11:21:11.070844: step 513, loss 0.623738, acc 0.9375, prec 0.0263158, recall 0.723913
2017-12-10T11:21:11.516663: step 514, loss 0.53017, acc 0.890625, prec 0.0263012, recall 0.723913
2017-12-10T11:21:11.955760: step 515, loss 16.8854, acc 0.8125, prec 0.0262805, recall 0.720779
2017-12-10T11:21:12.411104: step 516, loss 1.31799, acc 0.890625, prec 0.026268, recall 0.719222
2017-12-10T11:21:12.859094: step 517, loss 0.512219, acc 0.875, prec 0.0262515, recall 0.719222
2017-12-10T11:21:13.316745: step 518, loss 4.70437, acc 0.78125, prec 0.0262246, recall 0.717672
2017-12-10T11:21:13.779798: step 519, loss 0.543713, acc 0.859375, prec 0.026206, recall 0.717672
2017-12-10T11:21:14.222263: step 520, loss 0.97513, acc 0.828125, prec 0.0263365, recall 0.718884
2017-12-10T11:21:14.655930: step 521, loss 1.0588, acc 0.796875, prec 0.0264625, recall 0.720085
2017-12-10T11:21:15.110508: step 522, loss 1.42488, acc 0.734375, prec 0.0264272, recall 0.720085
2017-12-10T11:21:15.539335: step 523, loss 1.13635, acc 0.75, prec 0.0264704, recall 0.720682
2017-12-10T11:21:15.981981: step 524, loss 1.01511, acc 0.765625, prec 0.0265154, recall 0.721277
2017-12-10T11:21:16.442418: step 525, loss 0.875309, acc 0.75, prec 0.0267104, recall 0.723044
2017-12-10T11:21:16.881197: step 526, loss 1.89702, acc 0.640625, prec 0.0266625, recall 0.723044
2017-12-10T11:21:17.323613: step 527, loss 2.04191, acc 0.640625, prec 0.0266905, recall 0.723629
2017-12-10T11:21:17.763623: step 528, loss 1.4036, acc 0.71875, prec 0.0267288, recall 0.724211
2017-12-10T11:21:18.201782: step 529, loss 0.994385, acc 0.71875, prec 0.0266915, recall 0.724211
2017-12-10T11:21:18.647353: step 530, loss 1.12621, acc 0.859375, prec 0.0267483, recall 0.72479
2017-12-10T11:21:19.084377: step 531, loss 0.623413, acc 0.890625, prec 0.0268092, recall 0.725367
2017-12-10T11:21:19.531594: step 532, loss 0.83984, acc 0.765625, prec 0.0267781, recall 0.725367
2017-12-10T11:21:19.987870: step 533, loss 1.02408, acc 0.765625, prec 0.0268975, recall 0.726514
2017-12-10T11:21:20.430004: step 534, loss 1.30633, acc 0.78125, prec 0.0269436, recall 0.727083
2017-12-10T11:21:20.877539: step 535, loss 0.78319, acc 0.78125, prec 0.0269145, recall 0.727083
2017-12-10T11:21:21.337333: step 536, loss 0.790946, acc 0.828125, prec 0.0269666, recall 0.727651
2017-12-10T11:21:21.778690: step 537, loss 5.61267, acc 0.828125, prec 0.0272454, recall 0.728395
2017-12-10T11:21:22.238994: step 538, loss 2.87702, acc 0.859375, prec 0.0273035, recall 0.727459
2017-12-10T11:21:22.678558: step 539, loss 0.685355, acc 0.84375, prec 0.0273573, recall 0.728016
2017-12-10T11:21:23.140343: step 540, loss 0.532323, acc 0.859375, prec 0.027413, recall 0.728571
2017-12-10T11:21:23.584174: step 541, loss 0.734531, acc 0.84375, prec 0.0274666, recall 0.729124
2017-12-10T11:21:24.041811: step 542, loss 0.927276, acc 0.78125, prec 0.0275862, recall 0.730223
2017-12-10T11:21:24.476788: step 543, loss 1.83032, acc 0.765625, prec 0.0275545, recall 0.730223
2017-12-10T11:21:24.923487: step 544, loss 1.2293, acc 0.796875, prec 0.0276015, recall 0.730769
2017-12-10T11:21:25.366316: step 545, loss 1.03472, acc 0.78125, prec 0.0276463, recall 0.731313
2017-12-10T11:21:25.802404: step 546, loss 1.02385, acc 0.78125, prec 0.0277651, recall 0.732394
2017-12-10T11:21:26.246980: step 547, loss 0.934955, acc 0.765625, prec 0.0278074, recall 0.732932
2017-12-10T11:21:26.681281: step 548, loss 0.794119, acc 0.84375, prec 0.0278602, recall 0.733467
2017-12-10T11:21:27.122531: step 549, loss 6.84072, acc 0.84375, prec 0.0278412, recall 0.732
2017-12-10T11:21:27.576738: step 550, loss 0.910971, acc 0.765625, prec 0.0278094, recall 0.732
2017-12-10T11:21:28.011372: step 551, loss 0.93431, acc 0.78125, prec 0.0277799, recall 0.732
2017-12-10T11:21:28.453973: step 552, loss 1.33642, acc 0.75, prec 0.0278936, recall 0.733068
2017-12-10T11:21:28.894771: step 553, loss 0.789744, acc 0.828125, prec 0.027944, recall 0.733598
2017-12-10T11:21:29.329089: step 554, loss 0.867161, acc 0.828125, prec 0.0279942, recall 0.734127
2017-12-10T11:21:29.777240: step 555, loss 0.653358, acc 0.875, prec 0.0279773, recall 0.734127
2017-12-10T11:21:30.217375: step 556, loss 0.948361, acc 0.796875, prec 0.0280233, recall 0.734653
2017-12-10T11:21:30.602720: step 557, loss 1.18288, acc 0.765625, prec 0.0281382, recall 0.7357
2017-12-10T11:21:30.993184: step 558, loss 1.30294, acc 0.796875, prec 0.0281839, recall 0.73622
2017-12-10T11:21:31.383705: step 559, loss 3.07186, acc 0.6875, prec 0.0281436, recall 0.734774
2017-12-10T11:21:31.772667: step 560, loss 6.02102, acc 0.75, prec 0.0281118, recall 0.733333
2017-12-10T11:21:32.174282: step 561, loss 0.696113, acc 0.796875, prec 0.0280844, recall 0.733333
2017-12-10T11:21:32.612073: step 562, loss 0.991675, acc 0.796875, prec 0.0281299, recall 0.733855
2017-12-10T11:21:33.045892: step 563, loss 3.63617, acc 0.640625, prec 0.0280836, recall 0.732422
2017-12-10T11:21:33.480480: step 564, loss 1.30559, acc 0.71875, prec 0.0280458, recall 0.732422
2017-12-10T11:21:33.920345: step 565, loss 8.71608, acc 0.578125, prec 0.0279913, recall 0.730994
2017-12-10T11:21:34.358201: step 566, loss 1.35328, acc 0.671875, prec 0.0279475, recall 0.730994
2017-12-10T11:21:34.801771: step 567, loss 1.91002, acc 0.640625, prec 0.027972, recall 0.731517
2017-12-10T11:21:35.234484: step 568, loss 2.10151, acc 0.6875, prec 0.0280749, recall 0.732558
2017-12-10T11:21:35.666045: step 569, loss 1.73691, acc 0.625, prec 0.0280249, recall 0.732558
2017-12-10T11:21:36.102317: step 570, loss 1.80236, acc 0.734375, prec 0.0280616, recall 0.733075
2017-12-10T11:21:36.539308: step 571, loss 1.60051, acc 0.578125, prec 0.0280774, recall 0.733591
2017-12-10T11:21:36.983430: step 572, loss 1.6149, acc 0.671875, prec 0.0281056, recall 0.734104
2017-12-10T11:21:37.420625: step 573, loss 1.43076, acc 0.8125, prec 0.028224, recall 0.735125
2017-12-10T11:21:37.855502: step 574, loss 1.559, acc 0.71875, prec 0.0281866, recall 0.735125
2017-12-10T11:21:38.296753: step 575, loss 1.34968, acc 0.703125, prec 0.0282187, recall 0.735632
2017-12-10T11:21:38.729300: step 576, loss 1.40022, acc 0.671875, prec 0.0281752, recall 0.735632
2017-12-10T11:21:39.175949: step 577, loss 1.29829, acc 0.703125, prec 0.0282784, recall 0.736641
2017-12-10T11:21:39.614461: step 578, loss 6.7936, acc 0.78125, prec 0.0283226, recall 0.735741
2017-12-10T11:21:40.068238: step 579, loss 0.874429, acc 0.796875, prec 0.0284378, recall 0.736742
2017-12-10T11:21:40.527304: step 580, loss 0.887117, acc 0.734375, prec 0.0284734, recall 0.73724
2017-12-10T11:21:40.965770: step 581, loss 0.512528, acc 0.78125, prec 0.0284443, recall 0.73724
2017-12-10T11:21:41.398472: step 582, loss 0.712906, acc 0.84375, prec 0.0284944, recall 0.737736
2017-12-10T11:21:41.846688: step 583, loss 0.905015, acc 0.78125, prec 0.0284653, recall 0.737736
2017-12-10T11:21:42.283633: step 584, loss 0.666445, acc 0.828125, prec 0.0285839, recall 0.738722
2017-12-10T11:21:42.720529: step 585, loss 2.85011, acc 0.8125, prec 0.0286316, recall 0.737828
2017-12-10T11:21:43.159554: step 586, loss 0.854578, acc 0.796875, prec 0.0288866, recall 0.739777
2017-12-10T11:21:43.594615: step 587, loss 7.65176, acc 0.84375, prec 0.0288678, recall 0.738404
2017-12-10T11:21:44.034625: step 588, loss 1.2237, acc 0.796875, prec 0.0289813, recall 0.739372
2017-12-10T11:21:44.467730: step 589, loss 1.22914, acc 0.703125, prec 0.0290117, recall 0.739852
2017-12-10T11:21:44.907853: step 590, loss 0.845214, acc 0.828125, prec 0.029129, recall 0.740809
2017-12-10T11:21:45.361896: step 591, loss 1.75854, acc 0.703125, prec 0.0291591, recall 0.741284
2017-12-10T11:21:45.801181: step 592, loss 1.09288, acc 0.75, prec 0.0291955, recall 0.741758
2017-12-10T11:21:46.240734: step 593, loss 0.853204, acc 0.71875, prec 0.0291577, recall 0.741758
2017-12-10T11:21:46.676567: step 594, loss 0.610556, acc 0.765625, prec 0.0292658, recall 0.742701
2017-12-10T11:21:47.118557: step 595, loss 15.5893, acc 0.828125, prec 0.0293843, recall 0.742287
2017-12-10T11:21:47.567216: step 596, loss 0.877201, acc 0.8125, prec 0.0294983, recall 0.743219
2017-12-10T11:21:48.012529: step 597, loss 0.394879, acc 0.84375, prec 0.0295468, recall 0.743682
2017-12-10T11:21:48.446345: step 598, loss 0.844335, acc 0.828125, prec 0.029593, recall 0.744144
2017-12-10T11:21:48.880541: step 599, loss 5.54655, acc 0.734375, prec 0.0295591, recall 0.742806
2017-12-10T11:21:49.314478: step 600, loss 1.37667, acc 0.703125, prec 0.029519, recall 0.742806
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-600

2017-12-10T11:21:51.452777: step 601, loss 1.36722, acc 0.65625, prec 0.0294726, recall 0.742806
2017-12-10T11:21:51.889210: step 602, loss 0.945679, acc 0.796875, prec 0.0295837, recall 0.743728
2017-12-10T11:21:52.335124: step 603, loss 1.65558, acc 0.703125, prec 0.0297509, recall 0.745098
2017-12-10T11:21:52.782263: step 604, loss 0.745476, acc 0.734375, prec 0.0297839, recall 0.745552
2017-12-10T11:21:53.230223: step 605, loss 0.8592, acc 0.75, prec 0.0297501, recall 0.745552
2017-12-10T11:21:53.666722: step 606, loss 1.97499, acc 0.671875, prec 0.0297746, recall 0.746004
2017-12-10T11:21:54.096352: step 607, loss 1.4208, acc 0.71875, prec 0.0299427, recall 0.74735
2017-12-10T11:21:54.530613: step 608, loss 1.14662, acc 0.71875, prec 0.0299731, recall 0.747795
2017-12-10T11:21:54.964752: step 609, loss 0.886739, acc 0.734375, prec 0.0299372, recall 0.747795
2017-12-10T11:21:55.406532: step 610, loss 6.96242, acc 0.796875, prec 0.0299118, recall 0.746479
2017-12-10T11:21:55.851274: step 611, loss 2.88057, acc 0.78125, prec 0.0300874, recall 0.747811
2017-12-10T11:21:56.291194: step 612, loss 1.17149, acc 0.78125, prec 0.0300577, recall 0.747811
2017-12-10T11:21:56.726387: step 613, loss 1.39916, acc 0.640625, prec 0.0300091, recall 0.747811
2017-12-10T11:21:57.160902: step 614, loss 1.11037, acc 0.765625, prec 0.0299775, recall 0.747811
2017-12-10T11:21:57.616664: step 615, loss 0.9758, acc 0.734375, prec 0.0300098, recall 0.748252
2017-12-10T11:21:58.035691: step 616, loss 1.03912, acc 0.765625, prec 0.0301142, recall 0.749129
2017-12-10T11:21:58.484650: step 617, loss 1.57811, acc 0.640625, prec 0.0300657, recall 0.749129
2017-12-10T11:21:58.931520: step 618, loss 0.634842, acc 0.78125, prec 0.0300363, recall 0.749129
2017-12-10T11:21:59.368726: step 619, loss 1.67676, acc 0.75, prec 0.0300705, recall 0.749565
2017-12-10T11:21:59.808178: step 620, loss 0.897887, acc 0.734375, prec 0.0301024, recall 0.75
2017-12-10T11:22:00.241683: step 621, loss 0.962707, acc 0.78125, prec 0.0303431, recall 0.751724
2017-12-10T11:22:00.670496: step 622, loss 1.14894, acc 0.796875, prec 0.0304505, recall 0.752577
2017-12-10T11:22:01.112174: step 623, loss 0.949507, acc 0.765625, prec 0.0304861, recall 0.753002
2017-12-10T11:22:01.552306: step 624, loss 1.18601, acc 0.8125, prec 0.0304607, recall 0.753002
2017-12-10T11:22:01.991898: step 625, loss 1.1362, acc 0.78125, prec 0.0304984, recall 0.753425
2017-12-10T11:22:02.444559: step 626, loss 0.523657, acc 0.859375, prec 0.0304794, recall 0.753425
2017-12-10T11:22:02.882758: step 627, loss 0.799649, acc 0.796875, prec 0.030519, recall 0.753846
2017-12-10T11:22:03.335768: step 628, loss 0.429515, acc 0.921875, prec 0.0305085, recall 0.753846
2017-12-10T11:22:03.774068: step 629, loss 0.227828, acc 0.921875, prec 0.0304979, recall 0.753846
2017-12-10T11:22:04.225133: step 630, loss 0.193528, acc 0.921875, prec 0.0305544, recall 0.754266
2017-12-10T11:22:04.672072: step 631, loss 0.748908, acc 0.921875, prec 0.0306778, recall 0.755102
2017-12-10T11:22:05.110212: step 632, loss 0.714323, acc 0.828125, prec 0.0307214, recall 0.755518
2017-12-10T11:22:05.564293: step 633, loss 0.10607, acc 0.953125, prec 0.0307151, recall 0.755518
2017-12-10T11:22:06.008350: step 634, loss 0.0456633, acc 1, prec 0.0307151, recall 0.755518
2017-12-10T11:22:06.446257: step 635, loss 0.298009, acc 0.96875, prec 0.0307777, recall 0.755932
2017-12-10T11:22:06.885935: step 636, loss 9.66741, acc 0.84375, prec 0.0307607, recall 0.753378
2017-12-10T11:22:07.339469: step 637, loss 0.320991, acc 0.890625, prec 0.0307459, recall 0.753378
2017-12-10T11:22:07.765530: step 638, loss 0.294675, acc 0.953125, prec 0.0308063, recall 0.753794
2017-12-10T11:22:08.218215: step 639, loss 2.15238, acc 0.921875, prec 0.0309314, recall 0.753356
2017-12-10T11:22:08.660990: step 640, loss 0.789864, acc 0.921875, prec 0.0309875, recall 0.753769
2017-12-10T11:22:09.109812: step 641, loss 0.31049, acc 0.90625, prec 0.0311081, recall 0.754591
2017-12-10T11:22:09.545939: step 642, loss 5.62977, acc 0.78125, prec 0.0310802, recall 0.753333
2017-12-10T11:22:09.982081: step 643, loss 0.551321, acc 0.8125, prec 0.0311877, recall 0.754153
2017-12-10T11:22:10.417728: step 644, loss 1.42964, acc 0.734375, prec 0.0311514, recall 0.754153
2017-12-10T11:22:10.855505: step 645, loss 1.35813, acc 0.734375, prec 0.0311815, recall 0.754561
2017-12-10T11:22:11.294051: step 646, loss 2.60818, acc 0.6875, prec 0.0312714, recall 0.755372
2017-12-10T11:22:11.731859: step 647, loss 1.37675, acc 0.734375, prec 0.031235, recall 0.755372
2017-12-10T11:22:12.186616: step 648, loss 1.32911, acc 0.6875, prec 0.0313246, recall 0.756178
2017-12-10T11:22:12.621804: step 649, loss 2.11828, acc 0.609375, prec 0.0312713, recall 0.756178
2017-12-10T11:22:13.080768: step 650, loss 2.65099, acc 0.640625, prec 0.0313541, recall 0.756979
2017-12-10T11:22:13.517408: step 651, loss 2.36041, acc 0.59375, prec 0.0312988, recall 0.756979
2017-12-10T11:22:13.967328: step 652, loss 1.96106, acc 0.671875, prec 0.0313199, recall 0.757377
2017-12-10T11:22:14.406469: step 653, loss 1.87523, acc 0.640625, prec 0.0313367, recall 0.757774
2017-12-10T11:22:14.842644: step 654, loss 2.66543, acc 0.578125, prec 0.0314104, recall 0.758564
2017-12-10T11:22:15.280339: step 655, loss 2.20195, acc 0.609375, prec 0.0314881, recall 0.75935
2017-12-10T11:22:15.729660: step 656, loss 3.40916, acc 0.640625, prec 0.0314415, recall 0.758117
2017-12-10T11:22:16.173442: step 657, loss 2.26785, acc 0.609375, prec 0.0313886, recall 0.758117
2017-12-10T11:22:16.608875: step 658, loss 1.93605, acc 0.59375, prec 0.0313989, recall 0.758509
2017-12-10T11:22:17.049388: step 659, loss 2.0717, acc 0.609375, prec 0.0313463, recall 0.758509
2017-12-10T11:22:17.492876: step 660, loss 1.38402, acc 0.75, prec 0.0313127, recall 0.758509
2017-12-10T11:22:17.930373: step 661, loss 0.794603, acc 0.8125, prec 0.0313524, recall 0.7589
2017-12-10T11:22:18.362052: step 662, loss 1.08997, acc 0.765625, prec 0.0313856, recall 0.759289
2017-12-10T11:22:18.797428: step 663, loss 0.85238, acc 0.84375, prec 0.031494, recall 0.760064
2017-12-10T11:22:19.245701: step 664, loss 1.48973, acc 0.75, prec 0.0314604, recall 0.760064
2017-12-10T11:22:19.680749: step 665, loss 1.11556, acc 0.734375, prec 0.0314248, recall 0.760064
2017-12-10T11:22:20.126225: step 666, loss 0.910423, acc 0.78125, prec 0.0314599, recall 0.76045
2017-12-10T11:22:20.556280: step 667, loss 2.62854, acc 0.828125, prec 0.0315034, recall 0.759615
2017-12-10T11:22:21.012990: step 668, loss 0.489335, acc 0.921875, prec 0.0315573, recall 0.76
2017-12-10T11:22:21.452016: step 669, loss 15.6819, acc 0.859375, prec 0.0315426, recall 0.757576
2017-12-10T11:22:21.889363: step 670, loss 9.86396, acc 0.734375, prec 0.0316376, recall 0.757143
2017-12-10T11:22:22.333780: step 671, loss 0.893662, acc 0.765625, prec 0.0316061, recall 0.757143
2017-12-10T11:22:22.778630: step 672, loss 0.760856, acc 0.78125, prec 0.0315769, recall 0.757143
2017-12-10T11:22:23.239655: step 673, loss 1.17522, acc 0.6875, prec 0.0315351, recall 0.757143
2017-12-10T11:22:23.680372: step 674, loss 0.974411, acc 0.8125, prec 0.0315101, recall 0.757143
2017-12-10T11:22:24.123923: step 675, loss 1.57706, acc 0.765625, prec 0.0315428, recall 0.757528
2017-12-10T11:22:24.570565: step 676, loss 2.56637, acc 0.6875, prec 0.031631, recall 0.757098
2017-12-10T11:22:25.018410: step 677, loss 2.60719, acc 0.484375, prec 0.0316897, recall 0.757862
2017-12-10T11:22:25.466632: step 678, loss 2.16749, acc 0.5, prec 0.0316231, recall 0.757862
2017-12-10T11:22:25.902554: step 679, loss 2.13341, acc 0.625, prec 0.0316369, recall 0.758242
2017-12-10T11:22:26.340142: step 680, loss 1.27434, acc 0.75, prec 0.0316037, recall 0.758242
2017-12-10T11:22:26.790513: step 681, loss 1.36794, acc 0.75, prec 0.031634, recall 0.758621
2017-12-10T11:22:27.240877: step 682, loss 1.96035, acc 0.65625, prec 0.0315886, recall 0.758621
2017-12-10T11:22:27.696386: step 683, loss 1.81284, acc 0.625, prec 0.0315392, recall 0.758621
2017-12-10T11:22:28.132313: step 684, loss 1.49977, acc 0.65625, prec 0.03162, recall 0.759375
2017-12-10T11:22:28.576026: step 685, loss 1.21521, acc 0.765625, prec 0.0315892, recall 0.759375
2017-12-10T11:22:29.009616: step 686, loss 1.67457, acc 0.625, prec 0.0316029, recall 0.75975
2017-12-10T11:22:29.446176: step 687, loss 1.48075, acc 0.71875, prec 0.0316288, recall 0.760125
2017-12-10T11:22:29.877439: step 688, loss 0.513569, acc 0.859375, prec 0.0316103, recall 0.760125
2017-12-10T11:22:30.312237: step 689, loss 0.844928, acc 0.796875, prec 0.0315837, recall 0.760125
2017-12-10T11:22:30.763200: step 690, loss 0.787605, acc 0.84375, prec 0.0315633, recall 0.760125
2017-12-10T11:22:31.208779: step 691, loss 2.70868, acc 0.875, prec 0.0316742, recall 0.75969
2017-12-10T11:22:31.665083: step 692, loss 0.479859, acc 0.875, prec 0.0316578, recall 0.75969
2017-12-10T11:22:32.106750: step 693, loss 0.572152, acc 0.8125, prec 0.0316958, recall 0.760062
2017-12-10T11:22:32.557215: step 694, loss 0.539815, acc 0.921875, prec 0.0316856, recall 0.760062
2017-12-10T11:22:32.991748: step 695, loss 5.49681, acc 0.890625, prec 0.0316733, recall 0.758887
2017-12-10T11:22:33.432687: step 696, loss 0.782224, acc 0.84375, prec 0.0318402, recall 0.76
2017-12-10T11:22:33.868569: step 697, loss 0.688704, acc 0.84375, prec 0.0318196, recall 0.76
2017-12-10T11:22:34.297903: step 698, loss 1.02823, acc 0.84375, prec 0.0318615, recall 0.760369
2017-12-10T11:22:34.734309: step 699, loss 0.973227, acc 0.859375, prec 0.0320298, recall 0.761468
2017-12-10T11:22:35.174892: step 700, loss 0.707848, acc 0.859375, prec 0.0320113, recall 0.761468
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-700

2017-12-10T11:22:37.070785: step 701, loss 0.283078, acc 0.875, prec 0.032057, recall 0.761832
2017-12-10T11:22:37.516768: step 702, loss 0.534034, acc 0.8125, prec 0.0320324, recall 0.761832
2017-12-10T11:22:37.951879: step 703, loss 0.516947, acc 0.859375, prec 0.032076, recall 0.762195
2017-12-10T11:22:38.384054: step 704, loss 0.606136, acc 0.859375, prec 0.0320574, recall 0.762195
2017-12-10T11:22:38.819129: step 705, loss 0.658944, acc 0.828125, prec 0.0320969, recall 0.762557
2017-12-10T11:22:39.257015: step 706, loss 0.372985, acc 0.890625, prec 0.0320825, recall 0.762557
2017-12-10T11:22:39.702879: step 707, loss 0.47613, acc 0.875, prec 0.032128, recall 0.762918
2017-12-10T11:22:40.159351: step 708, loss 0.228105, acc 0.90625, prec 0.0321157, recall 0.762918
2017-12-10T11:22:40.630523: step 709, loss 0.69078, acc 0.890625, prec 0.0321632, recall 0.763278
2017-12-10T11:22:41.083677: step 710, loss 0.556115, acc 0.90625, prec 0.0322127, recall 0.763636
2017-12-10T11:22:41.528642: step 711, loss 0.489354, acc 0.890625, prec 0.0321983, recall 0.763636
2017-12-10T11:22:41.971127: step 712, loss 0.551561, acc 0.828125, prec 0.0321757, recall 0.763636
2017-12-10T11:22:42.407788: step 713, loss 0.558715, acc 0.921875, prec 0.0322889, recall 0.76435
2017-12-10T11:22:42.845249: step 714, loss 3.42328, acc 0.90625, prec 0.0322786, recall 0.763198
2017-12-10T11:22:43.284623: step 715, loss 1.17709, acc 0.9375, prec 0.0323321, recall 0.763554
2017-12-10T11:22:43.713681: step 716, loss 1.04148, acc 0.84375, prec 0.0323732, recall 0.76391
2017-12-10T11:22:44.154999: step 717, loss 1.78757, acc 0.90625, prec 0.0324245, recall 0.763118
2017-12-10T11:22:44.592724: step 718, loss 0.273954, acc 0.90625, prec 0.0324121, recall 0.763118
2017-12-10T11:22:45.026058: step 719, loss 4.03437, acc 0.765625, prec 0.0325064, recall 0.762687
2017-12-10T11:22:45.475752: step 720, loss 0.500953, acc 0.921875, prec 0.0325575, recall 0.76304
2017-12-10T11:22:45.910753: step 721, loss 1.15058, acc 0.734375, prec 0.0325838, recall 0.763393
2017-12-10T11:22:46.352666: step 722, loss 1.94846, acc 0.765625, prec 0.0326142, recall 0.763744
2017-12-10T11:22:46.818273: step 723, loss 12.1832, acc 0.65625, prec 0.0325708, recall 0.762611
2017-12-10T11:22:47.264890: step 724, loss 1.82137, acc 0.546875, prec 0.0325111, recall 0.762611
2017-12-10T11:22:47.696701: step 725, loss 1.37211, acc 0.671875, prec 0.0325291, recall 0.762963
2017-12-10T11:22:48.127907: step 726, loss 1.84243, acc 0.59375, prec 0.0324757, recall 0.762963
2017-12-10T11:22:48.560011: step 727, loss 2.17426, acc 0.578125, prec 0.0324205, recall 0.762963
2017-12-10T11:22:49.001796: step 728, loss 2.77418, acc 0.578125, prec 0.0324263, recall 0.763314
2017-12-10T11:22:49.444749: step 729, loss 2.76793, acc 0.546875, prec 0.0325494, recall 0.764359
2017-12-10T11:22:49.885330: step 730, loss 2.50491, acc 0.484375, prec 0.0324822, recall 0.764359
2017-12-10T11:22:50.352635: step 731, loss 2.64416, acc 0.578125, prec 0.0325483, recall 0.765051
2017-12-10T11:22:50.785217: step 732, loss 2.13676, acc 0.59375, prec 0.0326162, recall 0.765739
2017-12-10T11:22:51.222903: step 733, loss 1.54145, acc 0.671875, prec 0.0326337, recall 0.766082
2017-12-10T11:22:51.668932: step 734, loss 1.62519, acc 0.640625, prec 0.0327074, recall 0.766764
2017-12-10T11:22:52.099524: step 735, loss 1.80292, acc 0.6875, prec 0.0326668, recall 0.766764
2017-12-10T11:22:52.547587: step 736, loss 1.12528, acc 0.75, prec 0.0326343, recall 0.766764
2017-12-10T11:22:53.001837: step 737, loss 0.780805, acc 0.8125, prec 0.03267, recall 0.767103
2017-12-10T11:22:53.433973: step 738, loss 0.979731, acc 0.8125, prec 0.0326457, recall 0.767103
2017-12-10T11:22:53.868684: step 739, loss 12.1788, acc 0.84375, prec 0.0326874, recall 0.766328
2017-12-10T11:22:54.315284: step 740, loss 0.647352, acc 0.8125, prec 0.0326632, recall 0.766328
2017-12-10T11:22:54.756801: step 741, loss 0.975233, acc 0.8125, prec 0.0326987, recall 0.766667
2017-12-10T11:22:55.194368: step 742, loss 0.479746, acc 0.828125, prec 0.0327363, recall 0.767004
2017-12-10T11:22:55.634406: step 743, loss 0.365406, acc 0.890625, prec 0.0327221, recall 0.767004
2017-12-10T11:22:56.080843: step 744, loss 0.582122, acc 0.859375, prec 0.0327636, recall 0.767341
2017-12-10T11:22:56.530602: step 745, loss 0.925916, acc 0.828125, prec 0.0329203, recall 0.768345
2017-12-10T11:22:56.964421: step 746, loss 4.52398, acc 0.765625, prec 0.033011, recall 0.767908
2017-12-10T11:22:57.419626: step 747, loss 10.1268, acc 0.828125, prec 0.0329907, recall 0.76681
2017-12-10T11:22:57.864013: step 748, loss 11.5466, acc 0.796875, prec 0.0329664, recall 0.765714
2017-12-10T11:22:58.316384: step 749, loss 2.07698, acc 0.8125, prec 0.0329441, recall 0.764622
2017-12-10T11:22:58.767877: step 750, loss 0.64501, acc 0.78125, prec 0.0329157, recall 0.764622
2017-12-10T11:22:59.211685: step 751, loss 1.47494, acc 0.640625, prec 0.0329286, recall 0.764957
2017-12-10T11:22:59.656106: step 752, loss 2.31878, acc 0.5625, prec 0.0329314, recall 0.765292
2017-12-10T11:23:00.110442: step 753, loss 5.81479, acc 0.609375, prec 0.0329422, recall 0.764539
2017-12-10T11:23:00.561313: step 754, loss 2.42893, acc 0.59375, prec 0.0330079, recall 0.765205
2017-12-10T11:23:01.011969: step 755, loss 2.64114, acc 0.5, prec 0.0329436, recall 0.765205
2017-12-10T11:23:01.455892: step 756, loss 2.20638, acc 0.484375, prec 0.0329951, recall 0.765867
2017-12-10T11:23:01.886971: step 757, loss 1.93822, acc 0.5625, prec 0.0329977, recall 0.766197
2017-12-10T11:23:02.328853: step 758, loss 2.42934, acc 0.5625, prec 0.0330003, recall 0.766526
2017-12-10T11:23:02.772548: step 759, loss 2.36091, acc 0.546875, prec 0.0329425, recall 0.766526
2017-12-10T11:23:03.223214: step 760, loss 2.13261, acc 0.53125, prec 0.0329412, recall 0.766854
2017-12-10T11:23:03.667683: step 761, loss 2.3075, acc 0.46875, prec 0.032932, recall 0.767181
2017-12-10T11:23:04.089134: step 762, loss 5.83523, acc 0.640625, prec 0.0329466, recall 0.766434
2017-12-10T11:23:04.541786: step 763, loss 1.52368, acc 0.5625, prec 0.0329492, recall 0.76676
2017-12-10T11:23:04.979378: step 764, loss 1.27566, acc 0.703125, prec 0.0329117, recall 0.76676
2017-12-10T11:23:05.424098: step 765, loss 2.02879, acc 0.578125, prec 0.0329164, recall 0.767085
2017-12-10T11:23:05.867129: step 766, loss 1.99605, acc 0.75, prec 0.0329427, recall 0.767409
2017-12-10T11:23:06.310852: step 767, loss 1.49526, acc 0.671875, prec 0.0329592, recall 0.767733
2017-12-10T11:23:06.759056: step 768, loss 6.25669, acc 0.75, prec 0.0329297, recall 0.766667
2017-12-10T11:23:07.196388: step 769, loss 1.09344, acc 0.671875, prec 0.0329461, recall 0.76699
2017-12-10T11:23:07.637618: step 770, loss 1.14011, acc 0.75, prec 0.0330298, recall 0.767635
2017-12-10T11:23:08.078660: step 771, loss 1.48533, acc 0.65625, prec 0.0329866, recall 0.767635
2017-12-10T11:23:08.517559: step 772, loss 2.00902, acc 0.59375, prec 0.0329357, recall 0.767635
2017-12-10T11:23:08.974343: step 773, loss 1.61256, acc 0.6875, prec 0.032954, recall 0.767956
2017-12-10T11:23:09.419161: step 774, loss 12.1449, acc 0.75, prec 0.032982, recall 0.767218
2017-12-10T11:23:09.853553: step 775, loss 4.8998, acc 0.6875, prec 0.0329449, recall 0.766162
2017-12-10T11:23:10.291586: step 776, loss 1.16122, acc 0.71875, prec 0.032967, recall 0.766484
2017-12-10T11:23:10.752641: step 777, loss 1.01282, acc 0.75, prec 0.03305, recall 0.767123
2017-12-10T11:23:11.194552: step 778, loss 2.24879, acc 0.640625, prec 0.0330052, recall 0.767123
2017-12-10T11:23:11.625060: step 779, loss 1.29369, acc 0.640625, prec 0.0329606, recall 0.767123
2017-12-10T11:23:12.060300: step 780, loss 1.73231, acc 0.625, prec 0.0329141, recall 0.767123
2017-12-10T11:23:12.492165: step 781, loss 0.824735, acc 0.796875, prec 0.0329457, recall 0.767442
2017-12-10T11:23:12.933323: step 782, loss 1.54766, acc 0.640625, prec 0.0330147, recall 0.768076
2017-12-10T11:23:13.373899: step 783, loss 2.02046, acc 0.609375, prec 0.0329664, recall 0.768076
2017-12-10T11:23:13.821989: step 784, loss 1.00337, acc 0.75, prec 0.0329355, recall 0.768076
2017-12-10T11:23:14.278928: step 785, loss 7.48063, acc 0.765625, prec 0.0330781, recall 0.767978
2017-12-10T11:23:14.739501: step 786, loss 1.06038, acc 0.671875, prec 0.033094, recall 0.768293
2017-12-10T11:23:15.179761: step 787, loss 1.68836, acc 0.625, prec 0.0331604, recall 0.768919
2017-12-10T11:23:15.637110: step 788, loss 1.33517, acc 0.75, prec 0.0331858, recall 0.769231
2017-12-10T11:23:16.074050: step 789, loss 0.97989, acc 0.765625, prec 0.0332131, recall 0.769542
2017-12-10T11:23:16.527110: step 790, loss 6.15008, acc 0.84375, prec 0.0332519, recall 0.768817
2017-12-10T11:23:16.965991: step 791, loss 0.615977, acc 0.75, prec 0.0332772, recall 0.769127
2017-12-10T11:23:17.418323: step 792, loss 1.04927, acc 0.734375, prec 0.0332444, recall 0.769127
2017-12-10T11:23:17.859039: step 793, loss 0.815466, acc 0.765625, prec 0.0332155, recall 0.769127
2017-12-10T11:23:18.307060: step 794, loss 1.10849, acc 0.703125, prec 0.0331789, recall 0.769127
2017-12-10T11:23:18.746301: step 795, loss 0.975613, acc 0.765625, prec 0.0332061, recall 0.769437
2017-12-10T11:23:19.185731: step 796, loss 1.10576, acc 0.75, prec 0.0331754, recall 0.769437
2017-12-10T11:23:19.623504: step 797, loss 1.26722, acc 0.75, prec 0.0331447, recall 0.769437
2017-12-10T11:23:20.064980: step 798, loss 18.9627, acc 0.609375, prec 0.0331565, recall 0.76769
2017-12-10T11:23:20.500444: step 799, loss 1.33562, acc 0.75, prec 0.0331259, recall 0.76769
2017-12-10T11:23:20.944804: step 800, loss 6.79637, acc 0.765625, prec 0.0330992, recall 0.766667
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-800

2017-12-10T11:23:22.956961: step 801, loss 2.06336, acc 0.625, prec 0.0331647, recall 0.767287
2017-12-10T11:23:23.407478: step 802, loss 1.56603, acc 0.6875, prec 0.0331267, recall 0.767287
2017-12-10T11:23:23.839031: step 803, loss 1.58621, acc 0.734375, prec 0.0330944, recall 0.767287
2017-12-10T11:23:24.280037: step 804, loss 2.12323, acc 0.5625, prec 0.0330413, recall 0.767287
2017-12-10T11:23:24.712105: step 805, loss 2.07031, acc 0.515625, prec 0.0329827, recall 0.767287
2017-12-10T11:23:25.164805: step 806, loss 1.53063, acc 0.6875, prec 0.0329451, recall 0.767287
2017-12-10T11:23:25.601526: step 807, loss 1.69905, acc 0.625, prec 0.0329, recall 0.767287
2017-12-10T11:23:26.052889: step 808, loss 2.2537, acc 0.59375, prec 0.0329063, recall 0.767596
2017-12-10T11:23:26.499296: step 809, loss 2.03847, acc 0.59375, prec 0.0329677, recall 0.768212
2017-12-10T11:23:26.936692: step 810, loss 1.83252, acc 0.71875, prec 0.032934, recall 0.768212
2017-12-10T11:23:27.372423: step 811, loss 1.52938, acc 0.765625, prec 0.0329608, recall 0.768519
2017-12-10T11:23:27.805932: step 812, loss 1.00117, acc 0.765625, prec 0.0330424, recall 0.769129
2017-12-10T11:23:28.240323: step 813, loss 1.04701, acc 0.765625, prec 0.0331238, recall 0.769737
2017-12-10T11:23:28.672217: step 814, loss 1.50633, acc 0.6875, prec 0.0330864, recall 0.769737
2017-12-10T11:23:29.113900: step 815, loss 1.20478, acc 0.671875, prec 0.0331017, recall 0.770039
2017-12-10T11:23:29.554065: step 816, loss 0.78143, acc 0.828125, prec 0.0330812, recall 0.770039
2017-12-10T11:23:30.020237: step 817, loss 1.29214, acc 0.890625, prec 0.0331772, recall 0.770642
2017-12-10T11:23:30.478757: step 818, loss 0.771891, acc 0.796875, prec 0.0332074, recall 0.770942
2017-12-10T11:23:30.922674: step 819, loss 0.587685, acc 0.78125, prec 0.0331812, recall 0.770942
2017-12-10T11:23:31.359124: step 820, loss 0.660462, acc 0.859375, prec 0.0332188, recall 0.771242
2017-12-10T11:23:31.808776: step 821, loss 0.603335, acc 0.828125, prec 0.0331983, recall 0.771242
2017-12-10T11:23:32.252530: step 822, loss 0.412787, acc 0.875, prec 0.0331834, recall 0.771242
2017-12-10T11:23:32.683946: step 823, loss 0.195828, acc 0.921875, prec 0.033174, recall 0.771242
2017-12-10T11:23:33.134251: step 824, loss 0.181153, acc 0.9375, prec 0.0331666, recall 0.771242
2017-12-10T11:23:33.592779: step 825, loss 0.30259, acc 0.9375, prec 0.0332678, recall 0.771838
2017-12-10T11:23:34.027924: step 826, loss 0.591676, acc 0.859375, prec 0.0333053, recall 0.772135
2017-12-10T11:23:34.466984: step 827, loss 0.375891, acc 0.90625, prec 0.033294, recall 0.772135
2017-12-10T11:23:34.895751: step 828, loss 0.345025, acc 0.9375, prec 0.0333408, recall 0.772432
2017-12-10T11:23:35.338433: step 829, loss 0.275299, acc 0.96875, prec 0.0334456, recall 0.773022
2017-12-10T11:23:35.797117: step 830, loss 0.280614, acc 0.90625, prec 0.0334343, recall 0.773022
2017-12-10T11:23:36.257155: step 831, loss 8.24135, acc 0.921875, prec 0.0334268, recall 0.772021
2017-12-10T11:23:36.702068: step 832, loss 12.6623, acc 0.90625, prec 0.0334174, recall 0.771022
2017-12-10T11:23:37.148758: step 833, loss 0.291423, acc 0.90625, prec 0.0334062, recall 0.771022
2017-12-10T11:23:37.599658: step 834, loss 0.199142, acc 0.890625, prec 0.0333931, recall 0.771022
2017-12-10T11:23:38.040344: step 835, loss 0.294392, acc 0.921875, prec 0.0334379, recall 0.771318
2017-12-10T11:23:38.484921: step 836, loss 0.47297, acc 0.890625, prec 0.0334248, recall 0.771318
2017-12-10T11:23:38.932977: step 837, loss 0.754799, acc 0.875, prec 0.0334098, recall 0.771318
2017-12-10T11:23:39.374386: step 838, loss 0.818355, acc 0.875, prec 0.033557, recall 0.772201
2017-12-10T11:23:39.814886: step 839, loss 0.431554, acc 0.875, prec 0.0335961, recall 0.772494
2017-12-10T11:23:40.258962: step 840, loss 0.714817, acc 0.875, prec 0.033581, recall 0.772494
2017-12-10T11:23:40.706842: step 841, loss 0.332871, acc 0.953125, prec 0.0335754, recall 0.772494
2017-12-10T11:23:41.148191: step 842, loss 0.776984, acc 0.859375, prec 0.0336665, recall 0.773077
2017-12-10T11:23:41.584724: step 843, loss 0.515554, acc 0.875, prec 0.0336514, recall 0.773077
2017-12-10T11:23:42.029526: step 844, loss 0.39481, acc 0.890625, prec 0.0337461, recall 0.773657
2017-12-10T11:23:42.465976: step 845, loss 2.15745, acc 0.84375, prec 0.033783, recall 0.772959
2017-12-10T11:23:42.926188: step 846, loss 0.447991, acc 0.859375, prec 0.0338199, recall 0.773248
2017-12-10T11:23:43.367130: step 847, loss 0.905946, acc 0.734375, prec 0.0337879, recall 0.773248
2017-12-10T11:23:43.816844: step 848, loss 0.412261, acc 0.890625, prec 0.0338285, recall 0.773537
2017-12-10T11:23:44.260207: step 849, loss 0.621789, acc 0.84375, prec 0.0338097, recall 0.773537
2017-12-10T11:23:44.705089: step 850, loss 1.21393, acc 0.8125, prec 0.0338945, recall 0.774112
2017-12-10T11:23:45.142607: step 851, loss 0.54836, acc 0.828125, prec 0.0338738, recall 0.774112
2017-12-10T11:23:45.580615: step 852, loss 7.07901, acc 0.765625, prec 0.0338475, recall 0.773131
2017-12-10T11:23:46.019931: step 853, loss 1.11376, acc 0.734375, prec 0.0338156, recall 0.773131
2017-12-10T11:23:46.479974: step 854, loss 2.04127, acc 0.796875, prec 0.0338467, recall 0.77244
2017-12-10T11:23:46.920682: step 855, loss 1.32307, acc 0.796875, prec 0.0338223, recall 0.77244
2017-12-10T11:23:47.368834: step 856, loss 1.21509, acc 0.734375, prec 0.0337905, recall 0.77244
2017-12-10T11:23:47.813645: step 857, loss 0.791527, acc 0.765625, prec 0.0338159, recall 0.772727
2017-12-10T11:23:48.252831: step 858, loss 1.75739, acc 0.8125, prec 0.0338468, recall 0.773014
2017-12-10T11:23:48.699688: step 859, loss 1.89055, acc 0.75, prec 0.0338703, recall 0.7733
2017-12-10T11:23:49.158699: step 860, loss 0.890427, acc 0.796875, prec 0.0339525, recall 0.773869
2017-12-10T11:23:49.590987: step 861, loss 1.21413, acc 0.75, prec 0.0339758, recall 0.774153
2017-12-10T11:23:50.036132: step 862, loss 1.43013, acc 0.75, prec 0.033999, recall 0.774436
2017-12-10T11:23:50.485626: step 863, loss 1.24176, acc 0.703125, prec 0.0340697, recall 0.775
2017-12-10T11:23:50.922788: step 864, loss 0.892697, acc 0.828125, prec 0.0340491, recall 0.775
2017-12-10T11:23:51.367032: step 865, loss 0.942798, acc 0.78125, prec 0.0340229, recall 0.775
2017-12-10T11:23:51.805907: step 866, loss 1.03607, acc 0.734375, prec 0.0340442, recall 0.775281
2017-12-10T11:23:52.236150: step 867, loss 0.492845, acc 0.90625, prec 0.0340859, recall 0.775561
2017-12-10T11:23:52.669938: step 868, loss 0.937806, acc 0.8125, prec 0.0340635, recall 0.775561
2017-12-10T11:23:53.122213: step 869, loss 0.292795, acc 0.90625, prec 0.0341052, recall 0.775841
2017-12-10T11:23:53.563989: step 870, loss 0.44362, acc 0.875, prec 0.0340903, recall 0.775841
2017-12-10T11:23:54.006918: step 871, loss 0.632142, acc 0.828125, prec 0.0340698, recall 0.775841
2017-12-10T11:23:54.459267: step 872, loss 0.835175, acc 0.859375, prec 0.0341058, recall 0.776119
2017-12-10T11:23:54.909687: step 873, loss 0.464254, acc 0.84375, prec 0.0340872, recall 0.776119
2017-12-10T11:23:55.347614: step 874, loss 16.4008, acc 0.859375, prec 0.0340723, recall 0.775155
2017-12-10T11:23:55.798177: step 875, loss 0.859308, acc 0.796875, prec 0.0340481, recall 0.775155
2017-12-10T11:23:56.243089: step 876, loss 0.778558, acc 0.796875, prec 0.034024, recall 0.775155
2017-12-10T11:23:56.699428: step 877, loss 2.12631, acc 0.90625, prec 0.0340147, recall 0.774194
2017-12-10T11:23:57.151345: step 878, loss 0.663412, acc 0.828125, prec 0.0339943, recall 0.774194
2017-12-10T11:23:57.594947: step 879, loss 1.07695, acc 0.765625, prec 0.0340717, recall 0.774752
2017-12-10T11:23:58.052408: step 880, loss 0.801312, acc 0.8125, prec 0.0341546, recall 0.775309
2017-12-10T11:23:58.485848: step 881, loss 0.588622, acc 0.859375, prec 0.0341379, recall 0.775309
2017-12-10T11:23:58.929285: step 882, loss 0.772168, acc 0.75, prec 0.0341082, recall 0.775309
2017-12-10T11:23:59.379570: step 883, loss 0.769681, acc 0.8125, prec 0.0341384, recall 0.775586
2017-12-10T11:23:59.824896: step 884, loss 9.25095, acc 0.765625, prec 0.0341125, recall 0.774631
2017-12-10T11:24:00.295268: step 885, loss 0.624032, acc 0.765625, prec 0.0340848, recall 0.774631
2017-12-10T11:24:00.741892: step 886, loss 0.8029, acc 0.796875, prec 0.0341131, recall 0.774908
2017-12-10T11:24:01.184515: step 887, loss 0.597063, acc 0.796875, prec 0.0341936, recall 0.77546
2017-12-10T11:24:01.635747: step 888, loss 1.11771, acc 0.734375, prec 0.0342144, recall 0.775735
2017-12-10T11:24:02.088160: step 889, loss 1.20295, acc 0.734375, prec 0.0342873, recall 0.776284
2017-12-10T11:24:02.531269: step 890, loss 4.79642, acc 0.75, prec 0.0343637, recall 0.775883
2017-12-10T11:24:02.962098: step 891, loss 1.42337, acc 0.703125, prec 0.0344326, recall 0.776428
2017-12-10T11:24:03.402380: step 892, loss 1.19082, acc 0.6875, prec 0.0343955, recall 0.776428
2017-12-10T11:24:03.845010: step 893, loss 1.49987, acc 0.671875, prec 0.0345124, recall 0.77724
2017-12-10T11:24:04.297063: step 894, loss 1.64682, acc 0.71875, prec 0.0344791, recall 0.77724
2017-12-10T11:24:04.747696: step 895, loss 1.59503, acc 0.8125, prec 0.0345605, recall 0.777778
2017-12-10T11:24:05.199943: step 896, loss 1.50531, acc 0.71875, prec 0.0345789, recall 0.778046
2017-12-10T11:24:05.652037: step 897, loss 1.06009, acc 0.703125, prec 0.0347505, recall 0.779112
2017-12-10T11:24:06.092129: step 898, loss 0.772194, acc 0.734375, prec 0.0347705, recall 0.779377
2017-12-10T11:24:06.544210: step 899, loss 1.26584, acc 0.734375, prec 0.0347905, recall 0.779641
2017-12-10T11:24:06.985006: step 900, loss 1.35051, acc 0.6875, prec 0.0347534, recall 0.779641
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-900

2017-12-10T11:24:08.960787: step 901, loss 0.867778, acc 0.796875, prec 0.0347808, recall 0.779904
2017-12-10T11:24:09.403203: step 902, loss 10.4109, acc 0.828125, prec 0.0348137, recall 0.779236
2017-12-10T11:24:09.871982: step 903, loss 0.533845, acc 0.859375, prec 0.034797, recall 0.779236
2017-12-10T11:24:10.319836: step 904, loss 0.974881, acc 0.796875, prec 0.0348243, recall 0.779499
2017-12-10T11:24:10.773575: step 905, loss 0.83863, acc 0.796875, prec 0.0348002, recall 0.779499
2017-12-10T11:24:11.226169: step 906, loss 1.21054, acc 0.796875, prec 0.0348788, recall 0.780024
2017-12-10T11:24:11.658392: step 907, loss 0.902278, acc 0.828125, prec 0.0348584, recall 0.780024
2017-12-10T11:24:12.094352: step 908, loss 0.47236, acc 0.84375, prec 0.0348399, recall 0.780024
2017-12-10T11:24:12.554802: step 909, loss 0.778668, acc 0.859375, prec 0.0348745, recall 0.780285
2017-12-10T11:24:12.992540: step 910, loss 0.78812, acc 0.8125, prec 0.0348523, recall 0.780285
2017-12-10T11:24:13.426250: step 911, loss 0.771527, acc 0.8125, prec 0.0349324, recall 0.780806
2017-12-10T11:24:13.867562: step 912, loss 0.683474, acc 0.828125, prec 0.0349121, recall 0.780806
2017-12-10T11:24:14.306058: step 913, loss 2.89584, acc 0.859375, prec 0.0348991, recall 0.77896
2017-12-10T11:24:14.761652: step 914, loss 0.310773, acc 0.890625, prec 0.0348862, recall 0.77896
2017-12-10T11:24:15.227312: step 915, loss 0.681606, acc 0.859375, prec 0.0348696, recall 0.77896
2017-12-10T11:24:15.664700: step 916, loss 0.690731, acc 0.828125, prec 0.0348493, recall 0.77896
2017-12-10T11:24:16.109816: step 917, loss 2.71326, acc 0.8125, prec 0.03488, recall 0.778302
2017-12-10T11:24:16.556445: step 918, loss 0.641071, acc 0.875, prec 0.0348653, recall 0.778302
2017-12-10T11:24:17.010782: step 919, loss 2.5914, acc 0.796875, prec 0.0349451, recall 0.777908
2017-12-10T11:24:17.462389: step 920, loss 1.06561, acc 0.75, prec 0.0349156, recall 0.777908
2017-12-10T11:24:17.910518: step 921, loss 0.652327, acc 0.78125, prec 0.0349916, recall 0.778429
2017-12-10T11:24:18.350882: step 922, loss 3.52061, acc 0.78125, prec 0.0349676, recall 0.777518
2017-12-10T11:24:18.793893: step 923, loss 1.16663, acc 0.765625, prec 0.0349908, recall 0.777778
2017-12-10T11:24:19.223530: step 924, loss 3.08855, acc 0.6875, prec 0.0350573, recall 0.777389
2017-12-10T11:24:19.673460: step 925, loss 1.68552, acc 0.640625, prec 0.0350656, recall 0.777648
2017-12-10T11:24:20.131723: step 926, loss 1.61942, acc 0.625, prec 0.0350215, recall 0.777648
2017-12-10T11:24:20.596132: step 927, loss 2.02139, acc 0.609375, prec 0.0350767, recall 0.778165
2017-12-10T11:24:21.037055: step 928, loss 1.26902, acc 0.609375, prec 0.0350308, recall 0.778165
2017-12-10T11:24:21.468718: step 929, loss 1.60271, acc 0.609375, prec 0.0350355, recall 0.778422
2017-12-10T11:24:21.907752: step 930, loss 1.08269, acc 0.71875, prec 0.0350529, recall 0.778679
2017-12-10T11:24:22.341240: step 931, loss 1.66086, acc 0.625, prec 0.0350091, recall 0.778679
2017-12-10T11:24:22.778956: step 932, loss 1.20107, acc 0.640625, prec 0.0350174, recall 0.778935
2017-12-10T11:24:23.238731: step 933, loss 1.4432, acc 0.625, prec 0.0349738, recall 0.778935
2017-12-10T11:24:23.685004: step 934, loss 1.62595, acc 0.671875, prec 0.0349857, recall 0.779191
2017-12-10T11:24:24.132663: step 935, loss 1.50797, acc 0.640625, prec 0.034944, recall 0.779191
2017-12-10T11:24:24.574479: step 936, loss 2.36851, acc 0.703125, prec 0.0349114, recall 0.778291
2017-12-10T11:24:25.007511: step 937, loss 0.973322, acc 0.796875, prec 0.0348879, recall 0.778291
2017-12-10T11:24:25.434652: step 938, loss 0.933234, acc 0.84375, prec 0.0348699, recall 0.778291
2017-12-10T11:24:25.887015: step 939, loss 1.28891, acc 0.703125, prec 0.0348356, recall 0.778291
2017-12-10T11:24:26.319974: step 940, loss 0.88362, acc 0.765625, prec 0.0348585, recall 0.778547
2017-12-10T11:24:26.752957: step 941, loss 0.631278, acc 0.796875, prec 0.0348351, recall 0.778547
2017-12-10T11:24:27.190512: step 942, loss 0.823031, acc 0.796875, prec 0.0348118, recall 0.778547
2017-12-10T11:24:27.634548: step 943, loss 0.770787, acc 0.828125, prec 0.0348418, recall 0.778802
2017-12-10T11:24:28.074962: step 944, loss 3.47721, acc 0.84375, prec 0.0348256, recall 0.777906
2017-12-10T11:24:28.542628: step 945, loss 13.432, acc 0.875, prec 0.0348131, recall 0.777012
2017-12-10T11:24:28.978648: step 946, loss 6.25768, acc 0.828125, prec 0.0348448, recall 0.776376
2017-12-10T11:24:29.422895: step 947, loss 0.680505, acc 0.765625, prec 0.0348676, recall 0.776632
2017-12-10T11:24:29.854421: step 948, loss 0.739666, acc 0.890625, prec 0.0349542, recall 0.777143
2017-12-10T11:24:30.290320: step 949, loss 0.728769, acc 0.859375, prec 0.0349381, recall 0.777143
2017-12-10T11:24:30.744126: step 950, loss 1.44138, acc 0.59375, prec 0.0349905, recall 0.777651
2017-12-10T11:24:31.207274: step 951, loss 2.39449, acc 0.53125, prec 0.0349367, recall 0.777651
2017-12-10T11:24:31.647133: step 952, loss 1.50992, acc 0.609375, prec 0.0349908, recall 0.778157
2017-12-10T11:24:32.094537: step 953, loss 1.6317, acc 0.640625, prec 0.0349497, recall 0.778157
2017-12-10T11:24:32.526012: step 954, loss 1.35612, acc 0.65625, prec 0.0349104, recall 0.778157
2017-12-10T11:24:32.969374: step 955, loss 1.45828, acc 0.578125, prec 0.0349608, recall 0.778661
2017-12-10T11:24:33.406504: step 956, loss 1.23377, acc 0.703125, prec 0.0349269, recall 0.778661
2017-12-10T11:24:33.861847: step 957, loss 1.90992, acc 0.6875, prec 0.0349405, recall 0.778912
2017-12-10T11:24:34.299925: step 958, loss 1.80851, acc 0.609375, prec 0.0348961, recall 0.778912
2017-12-10T11:24:34.753599: step 959, loss 6.50462, acc 0.734375, prec 0.0348678, recall 0.778029
2017-12-10T11:24:35.192041: step 960, loss 0.831059, acc 0.796875, prec 0.0348448, recall 0.778029
2017-12-10T11:24:35.626462: step 961, loss 1.42284, acc 0.609375, prec 0.0348984, recall 0.778531
2017-12-10T11:24:36.066340: step 962, loss 1.39775, acc 0.6875, prec 0.0348631, recall 0.778531
2017-12-10T11:24:36.514856: step 963, loss 1.22574, acc 0.765625, prec 0.0348855, recall 0.778781
2017-12-10T11:24:36.952722: step 964, loss 1.30651, acc 0.640625, prec 0.0348937, recall 0.77903
2017-12-10T11:24:37.398776: step 965, loss 1.17853, acc 0.765625, prec 0.0348673, recall 0.77903
2017-12-10T11:24:37.835819: step 966, loss 0.599113, acc 0.84375, prec 0.0348497, recall 0.77903
2017-12-10T11:24:38.270612: step 967, loss 1.07562, acc 0.75, prec 0.0349675, recall 0.779775
2017-12-10T11:24:38.696445: step 968, loss 0.942269, acc 0.75, prec 0.0349393, recall 0.779775
2017-12-10T11:24:39.151038: step 969, loss 0.830492, acc 0.8125, prec 0.0349668, recall 0.780022
2017-12-10T11:24:39.589485: step 970, loss 4.30567, acc 0.8125, prec 0.0349475, recall 0.779148
2017-12-10T11:24:40.029837: step 971, loss 0.664106, acc 0.84375, prec 0.0349299, recall 0.779148
2017-12-10T11:24:40.465981: step 972, loss 0.467667, acc 0.875, prec 0.0350128, recall 0.779642
2017-12-10T11:24:40.907061: step 973, loss 0.599824, acc 0.84375, prec 0.0349952, recall 0.779642
2017-12-10T11:24:41.344614: step 974, loss 0.725864, acc 0.78125, prec 0.0350675, recall 0.780134
2017-12-10T11:24:41.785715: step 975, loss 3.57265, acc 0.75, prec 0.0350429, recall 0.778396
2017-12-10T11:24:42.242270: step 976, loss 4.4456, acc 0.859375, prec 0.0350288, recall 0.777531
2017-12-10T11:24:42.695297: step 977, loss 12.8194, acc 0.796875, prec 0.0350078, recall 0.776667
2017-12-10T11:24:43.149383: step 978, loss 0.841753, acc 0.84375, prec 0.0350385, recall 0.776915
2017-12-10T11:24:43.599314: step 979, loss 1.54504, acc 0.59375, prec 0.034993, recall 0.776915
2017-12-10T11:24:44.045313: step 980, loss 1.74031, acc 0.625, prec 0.0349511, recall 0.776915
2017-12-10T11:24:44.511155: step 981, loss 1.133, acc 0.671875, prec 0.0349145, recall 0.776915
2017-12-10T11:24:44.958257: step 982, loss 1.70122, acc 0.59375, prec 0.0349173, recall 0.777162
2017-12-10T11:24:45.399637: step 983, loss 1.8762, acc 0.65625, prec 0.0348791, recall 0.777162
2017-12-10T11:24:45.842998: step 984, loss 1.5785, acc 0.5625, prec 0.0348306, recall 0.777162
2017-12-10T11:24:46.283987: step 985, loss 1.75504, acc 0.5625, prec 0.0347822, recall 0.777162
2017-12-10T11:24:46.721862: step 986, loss 1.57735, acc 0.65625, prec 0.0348399, recall 0.777655
2017-12-10T11:24:47.175874: step 987, loss 1.32812, acc 0.6875, prec 0.0348532, recall 0.777901
2017-12-10T11:24:47.632908: step 988, loss 1.19369, acc 0.671875, prec 0.0348647, recall 0.778146
2017-12-10T11:24:48.079273: step 989, loss 0.918325, acc 0.796875, prec 0.0348423, recall 0.778146
2017-12-10T11:24:48.526853: step 990, loss 1.30435, acc 0.796875, prec 0.0348676, recall 0.77839
2017-12-10T11:24:48.969510: step 991, loss 1.0541, acc 0.734375, prec 0.0348384, recall 0.77839
2017-12-10T11:24:49.413940: step 992, loss 0.663329, acc 0.765625, prec 0.0348602, recall 0.778634
2017-12-10T11:24:49.872579: step 993, loss 1.03185, acc 0.75, prec 0.0348327, recall 0.778634
2017-12-10T11:24:50.278046: step 994, loss 0.463861, acc 0.843137, prec 0.034819, recall 0.778634
2017-12-10T11:24:50.747293: step 995, loss 0.616959, acc 0.8125, prec 0.0348459, recall 0.778878
2017-12-10T11:24:51.195897: step 996, loss 0.893237, acc 0.765625, prec 0.0348202, recall 0.778878
2017-12-10T11:24:51.646265: step 997, loss 0.634545, acc 0.828125, prec 0.0348489, recall 0.779121
2017-12-10T11:24:52.091309: step 998, loss 0.202073, acc 0.953125, prec 0.0349386, recall 0.779605
2017-12-10T11:24:52.541365: step 999, loss 0.421933, acc 0.921875, prec 0.03493, recall 0.779605
2017-12-10T11:24:52.994802: step 1000, loss 0.430239, acc 0.875, prec 0.0349637, recall 0.779847
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1000

2017-12-10T11:24:54.965049: step 1001, loss 0.204725, acc 0.953125, prec 0.0349585, recall 0.779847
2017-12-10T11:24:55.419528: step 1002, loss 0.156101, acc 0.90625, prec 0.0349956, recall 0.780088
2017-12-10T11:24:55.872893: step 1003, loss 0.0742585, acc 0.96875, prec 0.0349921, recall 0.780088
2017-12-10T11:24:56.324053: step 1004, loss 1.88428, acc 0.875, prec 0.0349801, recall 0.779235
2017-12-10T11:24:56.778544: step 1005, loss 0.371066, acc 0.953125, prec 0.0350223, recall 0.779476
2017-12-10T11:24:57.223074: step 1006, loss 0.0670615, acc 0.96875, prec 0.0350189, recall 0.779476
2017-12-10T11:24:57.668450: step 1007, loss 0.10921, acc 0.953125, prec 0.0350137, recall 0.779476
2017-12-10T11:24:58.123392: step 1008, loss 4.81611, acc 0.90625, prec 0.0350069, recall 0.777778
2017-12-10T11:24:58.583156: step 1009, loss 0.342369, acc 0.890625, prec 0.0350421, recall 0.77802
2017-12-10T11:24:59.035891: step 1010, loss 0.531427, acc 0.859375, prec 0.035074, recall 0.778261
2017-12-10T11:24:59.483328: step 1011, loss 0.117015, acc 0.9375, prec 0.0351144, recall 0.778502
2017-12-10T11:24:59.930135: step 1012, loss 9.64984, acc 0.9375, prec 0.0351092, recall 0.777657
2017-12-10T11:25:00.387435: step 1013, loss 0.511508, acc 0.875, prec 0.0351427, recall 0.777898
2017-12-10T11:25:00.832313: step 1014, loss 12.5923, acc 0.859375, prec 0.0351761, recall 0.777297
2017-12-10T11:25:01.287363: step 1015, loss 0.369418, acc 0.84375, prec 0.0352061, recall 0.777538
2017-12-10T11:25:01.725994: step 1016, loss 0.622327, acc 0.84375, prec 0.035236, recall 0.777778
2017-12-10T11:25:02.167388: step 1017, loss 1.09607, acc 0.75, prec 0.0352556, recall 0.778017
2017-12-10T11:25:02.604653: step 1018, loss 0.986414, acc 0.78125, prec 0.0352786, recall 0.778256
2017-12-10T11:25:03.068707: step 1019, loss 1.26405, acc 0.6875, prec 0.0352442, recall 0.778256
2017-12-10T11:25:03.510256: step 1020, loss 1.25391, acc 0.671875, prec 0.0352082, recall 0.778256
2017-12-10T11:25:03.950754: step 1021, loss 1.41133, acc 0.71875, prec 0.0352243, recall 0.778495
2017-12-10T11:25:04.389277: step 1022, loss 1.38509, acc 0.65625, prec 0.0351866, recall 0.778495
2017-12-10T11:25:04.843608: step 1023, loss 1.78016, acc 0.625, prec 0.0351925, recall 0.778733
2017-12-10T11:25:05.289394: step 1024, loss 1.41188, acc 0.625, prec 0.0351983, recall 0.77897
2017-12-10T11:25:05.735691: step 1025, loss 1.16981, acc 0.71875, prec 0.0351676, recall 0.77897
2017-12-10T11:25:06.192702: step 1026, loss 1.24667, acc 0.671875, prec 0.0351319, recall 0.77897
2017-12-10T11:25:06.642689: step 1027, loss 0.686445, acc 0.78125, prec 0.0351081, recall 0.77897
2017-12-10T11:25:07.088090: step 1028, loss 1.56978, acc 0.671875, prec 0.0351191, recall 0.779207
2017-12-10T11:25:07.539528: step 1029, loss 0.73064, acc 0.71875, prec 0.0351351, recall 0.779443
2017-12-10T11:25:07.975266: step 1030, loss 0.778552, acc 0.84375, prec 0.0351647, recall 0.779679
2017-12-10T11:25:08.429637: step 1031, loss 1.97168, acc 0.734375, prec 0.0353684, recall 0.780851
2017-12-10T11:25:08.889348: step 1032, loss 0.668755, acc 0.828125, prec 0.0353961, recall 0.781084
2017-12-10T11:25:09.331445: step 1033, loss 1.05744, acc 0.71875, prec 0.0353654, recall 0.781084
2017-12-10T11:25:09.781614: step 1034, loss 0.709765, acc 0.828125, prec 0.0353931, recall 0.781316
2017-12-10T11:25:10.225746: step 1035, loss 0.234871, acc 0.921875, prec 0.035431, recall 0.781548
2017-12-10T11:25:10.663790: step 1036, loss 0.458067, acc 0.84375, prec 0.0354603, recall 0.78178
2017-12-10T11:25:11.123568: step 1037, loss 0.997818, acc 0.84375, prec 0.0355822, recall 0.782471
2017-12-10T11:25:11.586273: step 1038, loss 0.272153, acc 0.875, prec 0.0355686, recall 0.782471
2017-12-10T11:25:12.038125: step 1039, loss 0.551374, acc 0.84375, prec 0.0355978, recall 0.7827
2017-12-10T11:25:12.477076: step 1040, loss 0.381845, acc 0.875, prec 0.0355841, recall 0.7827
2017-12-10T11:25:12.929751: step 1041, loss 0.792723, acc 0.8125, prec 0.0356099, recall 0.782929
2017-12-10T11:25:13.360191: step 1042, loss 0.23699, acc 0.90625, prec 0.0356458, recall 0.783158
2017-12-10T11:25:13.806559: step 1043, loss 0.364027, acc 0.921875, prec 0.0356835, recall 0.783386
2017-12-10T11:25:14.262595: step 1044, loss 0.310146, acc 0.921875, prec 0.0357673, recall 0.78384
2017-12-10T11:25:14.707871: step 1045, loss 0.201814, acc 0.953125, prec 0.0358083, recall 0.784067
2017-12-10T11:25:15.157781: step 1046, loss 0.180357, acc 0.90625, prec 0.035798, recall 0.784067
2017-12-10T11:25:15.596162: step 1047, loss 6.66335, acc 0.953125, prec 0.0357963, recall 0.782427
2017-12-10T11:25:16.040100: step 1048, loss 1.54077, acc 0.890625, prec 0.035786, recall 0.781609
2017-12-10T11:25:16.476758: step 1049, loss 0.233321, acc 0.921875, prec 0.0358236, recall 0.781837
2017-12-10T11:25:16.930540: step 1050, loss 1.40914, acc 0.890625, prec 0.0359055, recall 0.781478
2017-12-10T11:25:17.383675: step 1051, loss 5.80595, acc 0.828125, prec 0.0359362, recall 0.780083
2017-12-10T11:25:17.829315: step 1052, loss 1.41345, acc 0.734375, prec 0.035999, recall 0.780538
2017-12-10T11:25:18.267287: step 1053, loss 1.49027, acc 0.65625, prec 0.0360072, recall 0.780765
2017-12-10T11:25:18.704360: step 1054, loss 0.922901, acc 0.703125, prec 0.0359747, recall 0.780765
2017-12-10T11:25:19.150787: step 1055, loss 1.54172, acc 0.65625, prec 0.035937, recall 0.780765
2017-12-10T11:25:19.604538: step 1056, loss 1.98686, acc 0.53125, prec 0.0359774, recall 0.781218
2017-12-10T11:25:20.054251: step 1057, loss 1.93805, acc 0.5625, prec 0.0359296, recall 0.781218
2017-12-10T11:25:20.485141: step 1058, loss 1.44528, acc 0.625, prec 0.0358887, recall 0.781218
2017-12-10T11:25:20.923941: step 1059, loss 2.62894, acc 0.4375, prec 0.0358275, recall 0.781218
2017-12-10T11:25:21.371635: step 1060, loss 1.99838, acc 0.53125, prec 0.0358223, recall 0.781443
2017-12-10T11:25:21.814415: step 1061, loss 1.9105, acc 0.65625, prec 0.0357851, recall 0.781443
2017-12-10T11:25:22.278526: step 1062, loss 1.4747, acc 0.578125, prec 0.035785, recall 0.781668
2017-12-10T11:25:22.714586: step 1063, loss 2.5896, acc 0.640625, prec 0.0357479, recall 0.780864
2017-12-10T11:25:23.152092: step 1064, loss 1.8984, acc 0.609375, prec 0.0357059, recall 0.780864
2017-12-10T11:25:23.582103: step 1065, loss 1.72282, acc 0.609375, prec 0.0357093, recall 0.781089
2017-12-10T11:25:24.028930: step 1066, loss 1.48605, acc 0.671875, prec 0.0356741, recall 0.781089
2017-12-10T11:25:24.479046: step 1067, loss 1.22323, acc 0.734375, prec 0.0356456, recall 0.781089
2017-12-10T11:25:24.951173: step 1068, loss 1.44885, acc 0.6875, prec 0.0356574, recall 0.781314
2017-12-10T11:25:25.399086: step 1069, loss 0.930358, acc 0.78125, prec 0.0356792, recall 0.781538
2017-12-10T11:25:25.837123: step 1070, loss 0.976198, acc 0.6875, prec 0.035736, recall 0.781986
2017-12-10T11:25:26.277550: step 1071, loss 0.818636, acc 0.75, prec 0.0357093, recall 0.781986
2017-12-10T11:25:26.710824: step 1072, loss 0.668676, acc 0.78125, prec 0.0356859, recall 0.781986
2017-12-10T11:25:27.155438: step 1073, loss 2.04341, acc 0.859375, prec 0.0356726, recall 0.781186
2017-12-10T11:25:27.618522: step 1074, loss 7.39591, acc 0.875, prec 0.0356609, recall 0.780388
2017-12-10T11:25:28.055829: step 1075, loss 0.899218, acc 0.765625, prec 0.035681, recall 0.780612
2017-12-10T11:25:28.494776: step 1076, loss 0.629904, acc 0.796875, prec 0.0357942, recall 0.781282
2017-12-10T11:25:28.939122: step 1077, loss 0.870919, acc 0.78125, prec 0.0358158, recall 0.781504
2017-12-10T11:25:29.391246: step 1078, loss 0.388657, acc 0.859375, prec 0.0358456, recall 0.781726
2017-12-10T11:25:29.840056: step 1079, loss 0.517399, acc 0.84375, prec 0.0358738, recall 0.781947
2017-12-10T11:25:30.304069: step 1080, loss 0.536532, acc 0.796875, prec 0.0358521, recall 0.781947
2017-12-10T11:25:30.743955: step 1081, loss 0.312597, acc 0.859375, prec 0.0358819, recall 0.782168
2017-12-10T11:25:31.179377: step 1082, loss 0.541293, acc 0.90625, prec 0.0359167, recall 0.782389
2017-12-10T11:25:31.616260: step 1083, loss 2.16262, acc 0.90625, prec 0.0359084, recall 0.781598
2017-12-10T11:25:32.062601: step 1084, loss 0.527612, acc 0.84375, prec 0.0358917, recall 0.781598
2017-12-10T11:25:32.511626: step 1085, loss 2.69511, acc 0.796875, prec 0.0359165, recall 0.781029
2017-12-10T11:25:32.969484: step 1086, loss 0.875268, acc 0.75, prec 0.0359345, recall 0.78125
2017-12-10T11:25:33.412279: step 1087, loss 0.719948, acc 0.8125, prec 0.0359145, recall 0.78125
2017-12-10T11:25:33.854512: step 1088, loss 0.472014, acc 0.828125, prec 0.0358962, recall 0.78125
2017-12-10T11:25:34.303976: step 1089, loss 0.448056, acc 0.8125, prec 0.0358763, recall 0.78125
2017-12-10T11:25:34.747634: step 1090, loss 0.5025, acc 0.859375, prec 0.0358614, recall 0.78125
2017-12-10T11:25:35.189089: step 1091, loss 0.776906, acc 0.75, prec 0.0358348, recall 0.78125
2017-12-10T11:25:35.656379: step 1092, loss 0.429004, acc 0.859375, prec 0.0358645, recall 0.78147
2017-12-10T11:25:36.098045: step 1093, loss 1.12124, acc 0.875, prec 0.0358958, recall 0.78169
2017-12-10T11:25:36.539113: step 1094, loss 0.374676, acc 0.890625, prec 0.0358842, recall 0.78169
2017-12-10T11:25:36.974251: step 1095, loss 0.851491, acc 0.8125, prec 0.0359088, recall 0.78191
2017-12-10T11:25:37.421177: step 1096, loss 0.413616, acc 0.84375, prec 0.0358922, recall 0.78191
2017-12-10T11:25:37.861193: step 1097, loss 3.57277, acc 0.765625, prec 0.0359135, recall 0.781344
2017-12-10T11:25:38.320480: step 1098, loss 2.20108, acc 0.8125, prec 0.0359397, recall 0.780781
2017-12-10T11:25:38.762117: step 1099, loss 0.520851, acc 0.890625, prec 0.0359725, recall 0.781
2017-12-10T11:25:39.200194: step 1100, loss 0.500879, acc 0.828125, prec 0.0359987, recall 0.781219
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1100

2017-12-10T11:25:41.084341: step 1101, loss 0.80661, acc 0.75, prec 0.0359722, recall 0.781219
2017-12-10T11:25:41.522281: step 1102, loss 0.918124, acc 0.765625, prec 0.036036, recall 0.781655
2017-12-10T11:25:41.953934: step 1103, loss 1.00479, acc 0.8125, prec 0.036149, recall 0.782306
2017-12-10T11:25:42.386120: step 1104, loss 0.847753, acc 0.796875, prec 0.0361717, recall 0.782522
2017-12-10T11:25:42.820269: step 1105, loss 0.72184, acc 0.8125, prec 0.036196, recall 0.782738
2017-12-10T11:25:43.274936: step 1106, loss 0.933024, acc 0.78125, prec 0.0361727, recall 0.782738
2017-12-10T11:25:43.715369: step 1107, loss 0.663789, acc 0.78125, prec 0.0361937, recall 0.782953
2017-12-10T11:25:44.161319: step 1108, loss 0.622117, acc 0.875, prec 0.0362687, recall 0.783383
2017-12-10T11:25:44.604842: step 1109, loss 0.494191, acc 0.890625, prec 0.0363453, recall 0.78381
2017-12-10T11:25:45.049982: step 1110, loss 0.76904, acc 0.78125, prec 0.036322, recall 0.78381
2017-12-10T11:25:45.487560: step 1111, loss 0.525904, acc 0.78125, prec 0.0362988, recall 0.78381
2017-12-10T11:25:45.950506: step 1112, loss 0.721441, acc 0.75, prec 0.0362723, recall 0.78381
2017-12-10T11:25:46.389412: step 1113, loss 0.419069, acc 0.890625, prec 0.0362607, recall 0.78381
2017-12-10T11:25:46.831930: step 1114, loss 0.481358, acc 0.84375, prec 0.0362441, recall 0.78381
2017-12-10T11:25:47.283959: step 1115, loss 0.464974, acc 0.859375, prec 0.0362292, recall 0.78381
2017-12-10T11:25:47.729928: step 1116, loss 0.54334, acc 0.8125, prec 0.0362094, recall 0.78381
2017-12-10T11:25:48.182326: step 1117, loss 1.87821, acc 0.875, prec 0.0361979, recall 0.783037
2017-12-10T11:25:48.642143: step 1118, loss 0.313233, acc 0.921875, prec 0.0361896, recall 0.783037
2017-12-10T11:25:49.081717: step 1119, loss 0.175571, acc 0.953125, prec 0.0361847, recall 0.783037
2017-12-10T11:25:49.527138: step 1120, loss 0.163706, acc 0.953125, prec 0.0361797, recall 0.783037
2017-12-10T11:25:49.964185: step 1121, loss 16.2216, acc 0.9375, prec 0.0362642, recall 0.781925
2017-12-10T11:25:50.410654: step 1122, loss 6.14901, acc 0.921875, prec 0.0362576, recall 0.781158
2017-12-10T11:25:50.862920: step 1123, loss 3.4715, acc 0.875, prec 0.03629, recall 0.780607
2017-12-10T11:25:51.328662: step 1124, loss 1.46939, acc 0.609375, prec 0.0362487, recall 0.780607
2017-12-10T11:25:51.769228: step 1125, loss 0.662988, acc 0.734375, prec 0.0362207, recall 0.780607
2017-12-10T11:25:52.209033: step 1126, loss 3.25714, acc 0.59375, prec 0.0361796, recall 0.779843
2017-12-10T11:25:52.653851: step 1127, loss 1.70467, acc 0.515625, prec 0.0361724, recall 0.780059
2017-12-10T11:25:53.101796: step 1128, loss 1.91327, acc 0.609375, prec 0.0361751, recall 0.780273
2017-12-10T11:25:53.550573: step 1129, loss 2.73418, acc 0.484375, prec 0.0362083, recall 0.780702
2017-12-10T11:25:53.999423: step 1130, loss 2.5711, acc 0.46875, prec 0.0362832, recall 0.781341
2017-12-10T11:25:54.437525: step 1131, loss 2.27524, acc 0.5625, prec 0.0362809, recall 0.781553
2017-12-10T11:25:54.871371: step 1132, loss 2.89853, acc 0.421875, prec 0.0362205, recall 0.781553
2017-12-10T11:25:55.326540: step 1133, loss 2.71063, acc 0.40625, prec 0.0362885, recall 0.782188
2017-12-10T11:25:55.770251: step 1134, loss 2.39402, acc 0.5625, prec 0.0362429, recall 0.782188
2017-12-10T11:25:56.206257: step 1135, loss 2.58735, acc 0.5, prec 0.036191, recall 0.782188
2017-12-10T11:25:56.653811: step 1136, loss 1.75126, acc 0.578125, prec 0.0361473, recall 0.782188
2017-12-10T11:25:57.086611: step 1137, loss 1.20705, acc 0.703125, prec 0.0362027, recall 0.782609
2017-12-10T11:25:57.536300: step 1138, loss 1.59618, acc 0.640625, prec 0.0362086, recall 0.782819
2017-12-10T11:25:57.973639: step 1139, loss 1.48781, acc 0.625, prec 0.0361698, recall 0.782819
2017-12-10T11:25:58.410292: step 1140, loss 1.19216, acc 0.75, prec 0.036187, recall 0.783028
2017-12-10T11:25:58.846003: step 1141, loss 1.07652, acc 0.6875, prec 0.0361548, recall 0.783028
2017-12-10T11:25:59.297545: step 1142, loss 0.865906, acc 0.75, prec 0.0362148, recall 0.783446
2017-12-10T11:25:59.745306: step 1143, loss 1.0688, acc 0.75, prec 0.0362747, recall 0.783862
2017-12-10T11:26:00.185040: step 1144, loss 0.625401, acc 0.84375, prec 0.0362586, recall 0.783862
2017-12-10T11:26:00.638999: step 1145, loss 0.990213, acc 0.796875, prec 0.0363233, recall 0.784276
2017-12-10T11:26:01.091520: step 1146, loss 0.620842, acc 0.796875, prec 0.0363023, recall 0.784276
2017-12-10T11:26:01.528464: step 1147, loss 0.729441, acc 0.890625, prec 0.0363338, recall 0.784483
2017-12-10T11:26:01.982198: step 1148, loss 0.347807, acc 0.859375, prec 0.0364475, recall 0.7851
2017-12-10T11:26:02.446775: step 1149, loss 0.0648254, acc 0.96875, prec 0.0364442, recall 0.7851
2017-12-10T11:26:02.901200: step 1150, loss 0.161462, acc 0.953125, prec 0.0364394, recall 0.7851
2017-12-10T11:26:03.349294: step 1151, loss 4.11241, acc 0.9375, prec 0.03652, recall 0.784762
2017-12-10T11:26:03.807689: step 1152, loss 4.35127, acc 0.90625, prec 0.0365119, recall 0.784015
2017-12-10T11:26:04.263977: step 1153, loss 0.0692424, acc 0.984375, prec 0.0365103, recall 0.784015
2017-12-10T11:26:04.720245: step 1154, loss 2.86619, acc 0.90625, prec 0.0365449, recall 0.783476
2017-12-10T11:26:05.170401: step 1155, loss 0.374478, acc 0.875, prec 0.0365319, recall 0.783476
2017-12-10T11:26:05.605626: step 1156, loss 0.655439, acc 0.828125, prec 0.036642, recall 0.784091
2017-12-10T11:26:06.052040: step 1157, loss 0.599585, acc 0.84375, prec 0.0366684, recall 0.784295
2017-12-10T11:26:06.494233: step 1158, loss 0.773156, acc 0.828125, prec 0.0366506, recall 0.784295
2017-12-10T11:26:06.924771: step 1159, loss 0.729629, acc 0.78125, prec 0.0366279, recall 0.784295
2017-12-10T11:26:07.368547: step 1160, loss 0.495617, acc 0.859375, prec 0.0366134, recall 0.784295
2017-12-10T11:26:07.821004: step 1161, loss 0.925079, acc 0.75, prec 0.03663, recall 0.784499
2017-12-10T11:26:08.252369: step 1162, loss 0.773892, acc 0.75, prec 0.0366042, recall 0.784499
2017-12-10T11:26:08.689818: step 1163, loss 1.35601, acc 0.6875, prec 0.0366568, recall 0.784906
2017-12-10T11:26:09.133155: step 1164, loss 0.643096, acc 0.78125, prec 0.0367615, recall 0.785513
2017-12-10T11:26:09.577274: step 1165, loss 0.827053, acc 0.734375, prec 0.036734, recall 0.785513
2017-12-10T11:26:10.006510: step 1166, loss 0.757161, acc 0.78125, prec 0.0367961, recall 0.785915
2017-12-10T11:26:10.466386: step 1167, loss 0.595556, acc 0.796875, prec 0.036775, recall 0.785915
2017-12-10T11:26:10.910509: step 1168, loss 0.862279, acc 0.8125, prec 0.036798, recall 0.786116
2017-12-10T11:26:11.344765: step 1169, loss 0.600464, acc 0.828125, prec 0.0368225, recall 0.786317
2017-12-10T11:26:11.785932: step 1170, loss 1.05417, acc 0.78125, prec 0.0368421, recall 0.786517
2017-12-10T11:26:12.224835: step 1171, loss 2.73474, acc 0.859375, prec 0.0368292, recall 0.785781
2017-12-10T11:26:12.671455: step 1172, loss 0.496012, acc 0.828125, prec 0.0368958, recall 0.786181
2017-12-10T11:26:13.128488: step 1173, loss 0.691402, acc 0.765625, prec 0.0369138, recall 0.786381
2017-12-10T11:26:13.568612: step 1174, loss 0.318897, acc 0.890625, prec 0.0369025, recall 0.786381
2017-12-10T11:26:14.009158: step 1175, loss 0.898291, acc 0.859375, prec 0.0369722, recall 0.786778
2017-12-10T11:26:14.450999: step 1176, loss 0.599259, acc 0.890625, prec 0.0369609, recall 0.786778
2017-12-10T11:26:14.889596: step 1177, loss 0.688063, acc 0.890625, prec 0.0369496, recall 0.786778
2017-12-10T11:26:15.329673: step 1178, loss 0.809975, acc 0.84375, prec 0.0369755, recall 0.786977
2017-12-10T11:26:15.786590: step 1179, loss 0.527195, acc 0.859375, prec 0.0370031, recall 0.787175
2017-12-10T11:26:16.218115: step 1180, loss 5.15525, acc 0.953125, prec 0.0370419, recall 0.786642
2017-12-10T11:26:16.659340: step 1181, loss 0.457989, acc 0.828125, prec 0.0370661, recall 0.78684
2017-12-10T11:26:17.087298: step 1182, loss 3.13623, acc 0.859375, prec 0.0370532, recall 0.786111
2017-12-10T11:26:17.527126: step 1183, loss 0.495539, acc 0.8125, prec 0.0370338, recall 0.786111
2017-12-10T11:26:17.970477: step 1184, loss 0.683317, acc 0.84375, prec 0.0371016, recall 0.786506
2017-12-10T11:26:18.425569: step 1185, loss 1.18092, acc 0.671875, prec 0.0370677, recall 0.786506
2017-12-10T11:26:18.869237: step 1186, loss 0.860793, acc 0.71875, prec 0.0370806, recall 0.786704
2017-12-10T11:26:19.315430: step 1187, loss 0.801785, acc 0.8125, prec 0.037145, recall 0.787097
2017-12-10T11:26:19.763391: step 1188, loss 0.4472, acc 0.84375, prec 0.0371288, recall 0.787097
2017-12-10T11:26:20.204896: step 1189, loss 0.637064, acc 0.8125, prec 0.0371095, recall 0.787097
2017-12-10T11:26:20.649313: step 1190, loss 0.681231, acc 0.859375, prec 0.0371786, recall 0.787489
2017-12-10T11:26:21.106562: step 1191, loss 0.565846, acc 0.8125, prec 0.0371592, recall 0.787489
2017-12-10T11:26:21.544534: step 1192, loss 0.389068, acc 0.828125, prec 0.037225, recall 0.787879
2017-12-10T11:26:21.997872: step 1193, loss 0.732748, acc 0.859375, prec 0.0372523, recall 0.788073
2017-12-10T11:26:22.435392: step 1194, loss 0.587391, acc 0.8125, prec 0.0373581, recall 0.788655
2017-12-10T11:26:22.883846: step 1195, loss 0.433609, acc 0.859375, prec 0.0373852, recall 0.788848
2017-12-10T11:26:23.316635: step 1196, loss 0.189999, acc 0.921875, prec 0.0374188, recall 0.789041
2017-12-10T11:26:23.774084: step 1197, loss 1.3595, acc 0.796875, prec 0.0375227, recall 0.789617
2017-12-10T11:26:24.211440: step 1198, loss 0.539509, acc 0.859375, prec 0.0375081, recall 0.789617
2017-12-10T11:26:24.668277: step 1199, loss 0.658573, acc 0.890625, prec 0.03758, recall 0.79
2017-12-10T11:26:25.115499: step 1200, loss 0.433251, acc 0.875, prec 0.037567, recall 0.79
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1200

2017-12-10T11:26:27.138405: step 1201, loss 0.646115, acc 0.84375, prec 0.0375924, recall 0.790191
2017-12-10T11:26:27.585444: step 1202, loss 0.232621, acc 0.953125, prec 0.0375875, recall 0.790191
2017-12-10T11:26:28.012326: step 1203, loss 0.621525, acc 0.875, prec 0.0376992, recall 0.790761
2017-12-10T11:26:28.453772: step 1204, loss 0.372113, acc 0.90625, prec 0.037731, recall 0.79095
2017-12-10T11:26:28.907042: step 1205, loss 0.234298, acc 0.890625, prec 0.0377196, recall 0.79095
2017-12-10T11:26:29.335948: step 1206, loss 0.225863, acc 0.921875, prec 0.0377114, recall 0.79095
2017-12-10T11:26:29.779043: step 1207, loss 0.638498, acc 0.84375, prec 0.0377367, recall 0.791139
2017-12-10T11:26:30.214591: step 1208, loss 0.546008, acc 0.921875, prec 0.0378115, recall 0.791516
2017-12-10T11:26:30.662399: step 1209, loss 0.668793, acc 0.890625, prec 0.0378001, recall 0.791516
2017-12-10T11:26:31.093554: step 1210, loss 0.431947, acc 0.875, prec 0.0378285, recall 0.791704
2017-12-10T11:26:31.554498: step 1211, loss 0.114992, acc 0.953125, prec 0.0378236, recall 0.791704
2017-12-10T11:26:31.987204: step 1212, loss 1.65443, acc 0.90625, prec 0.0378155, recall 0.790991
2017-12-10T11:26:32.427811: step 1213, loss 0.459578, acc 0.890625, prec 0.0378455, recall 0.791179
2017-12-10T11:26:32.862941: step 1214, loss 0.319215, acc 0.90625, prec 0.0378357, recall 0.791179
2017-12-10T11:26:33.313167: step 1215, loss 2.86313, acc 0.9375, prec 0.0378723, recall 0.790656
2017-12-10T11:26:33.760166: step 1216, loss 0.470248, acc 0.859375, prec 0.0378576, recall 0.790656
2017-12-10T11:26:34.211784: step 1217, loss 0.328727, acc 0.90625, prec 0.0379306, recall 0.791031
2017-12-10T11:26:34.646945: step 1218, loss 0.349686, acc 0.875, prec 0.0379175, recall 0.791031
2017-12-10T11:26:35.094985: step 1219, loss 0.454021, acc 0.859375, prec 0.0379029, recall 0.791031
2017-12-10T11:26:35.546697: step 1220, loss 0.328467, acc 0.90625, prec 0.0378931, recall 0.791031
2017-12-10T11:26:35.985865: step 1221, loss 0.312687, acc 0.890625, prec 0.037923, recall 0.791219
2017-12-10T11:26:36.411001: step 1222, loss 1.22661, acc 0.90625, prec 0.0379959, recall 0.791592
2017-12-10T11:26:36.875725: step 1223, loss 4.54209, acc 0.84375, prec 0.0380638, recall 0.791258
2017-12-10T11:26:37.318743: step 1224, loss 0.549145, acc 0.828125, prec 0.0380871, recall 0.791444
2017-12-10T11:26:37.756923: step 1225, loss 0.604466, acc 0.84375, prec 0.038112, recall 0.79163
2017-12-10T11:26:38.197903: step 1226, loss 1.03698, acc 0.75, prec 0.0380859, recall 0.79163
2017-12-10T11:26:38.636202: step 1227, loss 1.30972, acc 0.75, prec 0.0381421, recall 0.792
2017-12-10T11:26:39.071126: step 1228, loss 0.920975, acc 0.765625, prec 0.0381176, recall 0.792
2017-12-10T11:26:39.529704: step 1229, loss 0.564641, acc 0.8125, prec 0.0381392, recall 0.792185
2017-12-10T11:26:39.955701: step 1230, loss 1.24798, acc 0.6875, prec 0.0381066, recall 0.792185
2017-12-10T11:26:40.404998: step 1231, loss 1.34386, acc 0.75, prec 0.0381627, recall 0.792553
2017-12-10T11:26:40.838684: step 1232, loss 0.721655, acc 0.78125, prec 0.0381399, recall 0.792553
2017-12-10T11:26:41.284989: step 1233, loss 0.574268, acc 0.84375, prec 0.0381647, recall 0.792737
2017-12-10T11:26:41.724033: step 1234, loss 1.08549, acc 0.71875, prec 0.0382993, recall 0.793469
2017-12-10T11:26:42.186092: step 1235, loss 0.623074, acc 0.828125, prec 0.0382814, recall 0.793469
2017-12-10T11:26:42.614905: step 1236, loss 1.48764, acc 0.75, prec 0.0382553, recall 0.793469
2017-12-10T11:26:43.047614: step 1237, loss 0.620508, acc 0.78125, prec 0.0382734, recall 0.793651
2017-12-10T11:26:43.488981: step 1238, loss 0.725285, acc 0.78125, prec 0.0382915, recall 0.793833
2017-12-10T11:26:43.932771: step 1239, loss 0.873071, acc 0.8125, prec 0.038272, recall 0.793833
2017-12-10T11:26:44.378556: step 1240, loss 0.551408, acc 0.859375, prec 0.0382982, recall 0.794014
2017-12-10T11:26:44.838561: step 1241, loss 0.2372, acc 0.9375, prec 0.0383326, recall 0.794195
2017-12-10T11:26:45.278764: step 1242, loss 0.736984, acc 0.859375, prec 0.0383995, recall 0.794557
2017-12-10T11:26:45.730730: step 1243, loss 5.64683, acc 0.875, prec 0.0384289, recall 0.79404
2017-12-10T11:26:46.178365: step 1244, loss 0.606264, acc 0.859375, prec 0.0384142, recall 0.79404
2017-12-10T11:26:46.644723: step 1245, loss 0.507077, acc 0.875, prec 0.038442, recall 0.794221
2017-12-10T11:26:47.083366: step 1246, loss 0.284427, acc 0.890625, prec 0.0384713, recall 0.794401
2017-12-10T11:26:47.531646: step 1247, loss 2.62001, acc 0.90625, prec 0.0385446, recall 0.794066
2017-12-10T11:26:47.972393: step 1248, loss 0.5756, acc 0.796875, prec 0.0385234, recall 0.794066
2017-12-10T11:26:48.429413: step 1249, loss 0.533702, acc 0.84375, prec 0.0386292, recall 0.794604
2017-12-10T11:26:48.859490: step 1250, loss 0.487146, acc 0.828125, prec 0.0386112, recall 0.794604
2017-12-10T11:26:49.303013: step 1251, loss 0.78966, acc 0.765625, prec 0.0386273, recall 0.794783
2017-12-10T11:26:49.750921: step 1252, loss 0.99636, acc 0.671875, prec 0.0385931, recall 0.794783
2017-12-10T11:26:50.210788: step 1253, loss 0.492774, acc 0.84375, prec 0.0386579, recall 0.795139
2017-12-10T11:26:50.651408: step 1254, loss 0.768348, acc 0.78125, prec 0.0386757, recall 0.795317
2017-12-10T11:26:51.093666: step 1255, loss 0.474822, acc 0.875, prec 0.0387032, recall 0.795494
2017-12-10T11:26:51.536093: step 1256, loss 0.421323, acc 0.875, prec 0.0386901, recall 0.795494
2017-12-10T11:26:51.964216: step 1257, loss 10.4732, acc 0.84375, prec 0.0387159, recall 0.794983
2017-12-10T11:26:52.401439: step 1258, loss 0.509081, acc 0.796875, prec 0.0387352, recall 0.79516
2017-12-10T11:26:52.869157: step 1259, loss 0.708396, acc 0.8125, prec 0.0387561, recall 0.795337
2017-12-10T11:26:53.312772: step 1260, loss 0.694131, acc 0.75, prec 0.0387704, recall 0.795513
2017-12-10T11:26:53.762439: step 1261, loss 0.555984, acc 0.8125, prec 0.0387913, recall 0.79569
2017-12-10T11:26:54.205185: step 1262, loss 0.286812, acc 0.890625, prec 0.038901, recall 0.796217
2017-12-10T11:26:54.641881: step 1263, loss 0.684223, acc 0.859375, prec 0.0388863, recall 0.796217
2017-12-10T11:26:55.086252: step 1264, loss 1.16794, acc 0.78125, prec 0.0389038, recall 0.796392
2017-12-10T11:26:55.542970: step 1265, loss 0.503679, acc 0.859375, prec 0.0389698, recall 0.796741
2017-12-10T11:26:55.982753: step 1266, loss 0.708234, acc 0.796875, prec 0.0389485, recall 0.796741
2017-12-10T11:26:56.431317: step 1267, loss 0.582353, acc 0.84375, prec 0.0389322, recall 0.796741
2017-12-10T11:26:56.875202: step 1268, loss 0.308921, acc 0.890625, prec 0.0390818, recall 0.797436
2017-12-10T11:26:57.333059: step 1269, loss 0.264372, acc 0.921875, prec 0.0390736, recall 0.797436
2017-12-10T11:26:57.772230: step 1270, loss 0.500214, acc 0.84375, prec 0.0390975, recall 0.797609
2017-12-10T11:26:58.221420: step 1271, loss 0.428931, acc 0.84375, prec 0.0390811, recall 0.797609
2017-12-10T11:26:58.609958: step 1272, loss 0.269688, acc 0.9375, prec 0.0390746, recall 0.797609
2017-12-10T11:26:59.001227: step 1273, loss 3.62882, acc 0.890625, prec 0.039105, recall 0.797101
2017-12-10T11:26:59.362226: step 1274, loss 0.547997, acc 0.875, prec 0.0390919, recall 0.797101
2017-12-10T11:26:59.755409: step 1275, loss 4.98838, acc 0.859375, prec 0.0390788, recall 0.796422
2017-12-10T11:27:00.139254: step 1276, loss 0.396229, acc 0.890625, prec 0.0391075, recall 0.796596
2017-12-10T11:27:00.530028: step 1277, loss 0.92403, acc 0.8125, prec 0.0390879, recall 0.796596
2017-12-10T11:27:00.984694: step 1278, loss 2.21831, acc 0.828125, prec 0.0391117, recall 0.796092
2017-12-10T11:27:01.431414: step 1279, loss 0.499877, acc 0.859375, prec 0.0391772, recall 0.796438
2017-12-10T11:27:01.873896: step 1280, loss 0.402828, acc 0.859375, prec 0.0392026, recall 0.79661
2017-12-10T11:27:02.320756: step 1281, loss 0.843852, acc 0.796875, prec 0.0392214, recall 0.796782
2017-12-10T11:27:02.759755: step 1282, loss 1.19604, acc 0.75, prec 0.0391953, recall 0.796782
2017-12-10T11:27:03.211456: step 1283, loss 0.844907, acc 0.8125, prec 0.0392157, recall 0.796954
2017-12-10T11:27:03.655046: step 1284, loss 1.09433, acc 0.6875, prec 0.039223, recall 0.797126
2017-12-10T11:27:04.104346: step 1285, loss 0.896395, acc 0.75, prec 0.0392768, recall 0.797468
2017-12-10T11:27:04.547451: step 1286, loss 0.927552, acc 0.75, prec 0.0392507, recall 0.797468
2017-12-10T11:27:04.977788: step 1287, loss 0.902311, acc 0.734375, prec 0.0393028, recall 0.79781
2017-12-10T11:27:05.425171: step 1288, loss 1.04465, acc 0.796875, prec 0.0392816, recall 0.79781
2017-12-10T11:27:05.866427: step 1289, loss 1.14513, acc 0.8125, prec 0.039262, recall 0.79781
2017-12-10T11:27:06.325581: step 1290, loss 1.30504, acc 0.8125, prec 0.0394017, recall 0.798489
2017-12-10T11:27:06.762753: step 1291, loss 0.919896, acc 0.8125, prec 0.0394219, recall 0.798658
2017-12-10T11:27:07.211818: step 1292, loss 0.420872, acc 0.890625, prec 0.0394503, recall 0.798827
2017-12-10T11:27:07.651469: step 1293, loss 0.893946, acc 0.78125, prec 0.0394274, recall 0.798827
2017-12-10T11:27:08.082898: step 1294, loss 0.444121, acc 0.796875, prec 0.0394459, recall 0.798995
2017-12-10T11:27:08.539069: step 1295, loss 0.640353, acc 0.84375, prec 0.0394296, recall 0.798995
2017-12-10T11:27:08.995812: step 1296, loss 1.19895, acc 0.890625, prec 0.0394976, recall 0.799331
2017-12-10T11:27:09.434473: step 1297, loss 0.477062, acc 0.8125, prec 0.039478, recall 0.799331
2017-12-10T11:27:09.881561: step 1298, loss 0.272199, acc 0.90625, prec 0.0395079, recall 0.799499
2017-12-10T11:27:10.324108: step 1299, loss 0.221623, acc 0.921875, prec 0.0394998, recall 0.799499
2017-12-10T11:27:10.766434: step 1300, loss 0.248376, acc 0.90625, prec 0.03949, recall 0.799499
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1300

2017-12-10T11:27:12.707251: step 1301, loss 0.494666, acc 0.890625, prec 0.0394786, recall 0.799499
2017-12-10T11:27:13.156806: step 1302, loss 0.495901, acc 0.875, prec 0.0394655, recall 0.799499
2017-12-10T11:27:13.599693: step 1303, loss 0.519687, acc 0.875, prec 0.0394525, recall 0.799499
2017-12-10T11:27:14.058790: step 1304, loss 11.8771, acc 0.875, prec 0.0394411, recall 0.798831
2017-12-10T11:27:14.494969: step 1305, loss 0.785403, acc 0.828125, prec 0.0394233, recall 0.798831
2017-12-10T11:27:14.927524: step 1306, loss 0.180647, acc 0.9375, prec 0.0394563, recall 0.798999
2017-12-10T11:27:15.374861: step 1307, loss 0.592645, acc 0.796875, prec 0.0394352, recall 0.798999
2017-12-10T11:27:15.811049: step 1308, loss 0.709738, acc 0.796875, prec 0.0394141, recall 0.798999
2017-12-10T11:27:16.245685: step 1309, loss 0.571982, acc 0.8125, prec 0.0394737, recall 0.799334
2017-12-10T11:27:16.707044: step 1310, loss 1.21865, acc 0.84375, prec 0.0395759, recall 0.799834
2017-12-10T11:27:17.138861: step 1311, loss 0.344696, acc 0.90625, prec 0.0396056, recall 0.8
2017-12-10T11:27:17.585178: step 1312, loss 0.299759, acc 0.921875, prec 0.0397158, recall 0.800497
2017-12-10T11:27:18.011269: step 1313, loss 0.529229, acc 0.859375, prec 0.0397011, recall 0.800497
2017-12-10T11:27:18.464805: step 1314, loss 0.631429, acc 0.875, prec 0.0397275, recall 0.800662
2017-12-10T11:27:18.917202: step 1315, loss 0.33716, acc 0.890625, prec 0.0397161, recall 0.800662
2017-12-10T11:27:19.383241: step 1316, loss 2.80275, acc 0.890625, prec 0.0397063, recall 0.8
2017-12-10T11:27:19.818924: step 1317, loss 0.187806, acc 0.9375, prec 0.0397392, recall 0.800165
2017-12-10T11:27:20.252718: step 1318, loss 10.8586, acc 0.828125, prec 0.0397639, recall 0.799012
2017-12-10T11:27:20.686479: step 1319, loss 0.684385, acc 0.8125, prec 0.0397443, recall 0.799012
2017-12-10T11:27:21.127087: step 1320, loss 1.12556, acc 0.796875, prec 0.0398018, recall 0.799342
2017-12-10T11:27:21.567442: step 1321, loss 0.788128, acc 0.78125, prec 0.0398183, recall 0.799507
2017-12-10T11:27:22.029176: step 1322, loss 0.401812, acc 0.890625, prec 0.0398855, recall 0.799836
2017-12-10T11:27:22.460808: step 1323, loss 0.777005, acc 0.8125, prec 0.0399051, recall 0.8
2017-12-10T11:27:22.893054: step 1324, loss 0.985137, acc 0.703125, prec 0.0398742, recall 0.8
2017-12-10T11:27:23.334655: step 1325, loss 1.24721, acc 0.734375, prec 0.0400033, recall 0.800654
2017-12-10T11:27:23.787248: step 1326, loss 0.936309, acc 0.734375, prec 0.0400147, recall 0.800816
2017-12-10T11:27:24.238833: step 1327, loss 0.790458, acc 0.8125, prec 0.0399951, recall 0.800816
2017-12-10T11:27:24.702293: step 1328, loss 0.86661, acc 0.75, prec 0.0400082, recall 0.800979
2017-12-10T11:27:25.129582: step 1329, loss 2.43278, acc 0.703125, prec 0.040057, recall 0.800651
2017-12-10T11:27:25.576241: step 1330, loss 1.18673, acc 0.6875, prec 0.0401025, recall 0.800975
2017-12-10T11:27:26.014507: step 1331, loss 0.986998, acc 0.765625, prec 0.0401171, recall 0.801136
2017-12-10T11:27:26.462442: step 1332, loss 0.918204, acc 0.734375, prec 0.0401283, recall 0.801298
2017-12-10T11:27:26.907178: step 1333, loss 1.225, acc 0.75, prec 0.0401023, recall 0.801298
2017-12-10T11:27:27.359553: step 1334, loss 1.17192, acc 0.71875, prec 0.040073, recall 0.801298
2017-12-10T11:27:27.797460: step 1335, loss 1.10158, acc 0.75, prec 0.0401248, recall 0.801619
2017-12-10T11:27:28.248008: step 1336, loss 0.820598, acc 0.78125, prec 0.040141, recall 0.80178
2017-12-10T11:27:28.683685: step 1337, loss 1.01269, acc 0.703125, prec 0.0401101, recall 0.80178
2017-12-10T11:27:29.134182: step 1338, loss 0.824487, acc 0.8125, prec 0.0401294, recall 0.80194
2017-12-10T11:27:29.568749: step 1339, loss 0.437354, acc 0.875, prec 0.0401165, recall 0.80194
2017-12-10T11:27:30.010092: step 1340, loss 0.21284, acc 0.9375, prec 0.04011, recall 0.80194
2017-12-10T11:27:30.432244: step 1341, loss 0.473951, acc 0.875, prec 0.040097, recall 0.80194
2017-12-10T11:27:30.878898: step 1342, loss 0.546373, acc 0.859375, prec 0.0400824, recall 0.80194
2017-12-10T11:27:31.305730: step 1343, loss 0.683168, acc 0.828125, prec 0.0400646, recall 0.80194
2017-12-10T11:27:31.749064: step 1344, loss 0.288824, acc 0.890625, prec 0.0400533, recall 0.80194
2017-12-10T11:27:32.194037: step 1345, loss 0.195092, acc 0.921875, prec 0.0400452, recall 0.80194
2017-12-10T11:27:32.652370: step 1346, loss 0.240067, acc 0.9375, prec 0.0400387, recall 0.80194
2017-12-10T11:27:33.096783: step 1347, loss 16.3414, acc 0.921875, prec 0.0400323, recall 0.801292
2017-12-10T11:27:33.543878: step 1348, loss 0.293261, acc 0.90625, prec 0.0400226, recall 0.801292
2017-12-10T11:27:33.969352: step 1349, loss 0.408418, acc 0.953125, prec 0.0400952, recall 0.801613
2017-12-10T11:27:34.421908: step 1350, loss 0.176005, acc 0.9375, prec 0.0400887, recall 0.801613
2017-12-10T11:27:34.867965: step 1351, loss 0.264483, acc 0.953125, prec 0.0400839, recall 0.801613
2017-12-10T11:27:35.329407: step 1352, loss 0.350291, acc 0.9375, prec 0.0401161, recall 0.801773
2017-12-10T11:27:35.762855: step 1353, loss 5.96317, acc 0.921875, prec 0.0401483, recall 0.801287
2017-12-10T11:27:36.212768: step 1354, loss 0.343803, acc 0.90625, prec 0.0401386, recall 0.801287
2017-12-10T11:27:36.650846: step 1355, loss 0.393023, acc 0.90625, prec 0.0401676, recall 0.801447
2017-12-10T11:27:37.082587: step 1356, loss 0.541774, acc 0.921875, prec 0.0402368, recall 0.801766
2017-12-10T11:27:37.521047: step 1357, loss 0.681852, acc 0.875, prec 0.0402625, recall 0.801925
2017-12-10T11:27:37.982740: step 1358, loss 0.880823, acc 0.8125, prec 0.0402817, recall 0.802083
2017-12-10T11:27:38.418147: step 1359, loss 1.27226, acc 0.828125, prec 0.0403025, recall 0.802242
2017-12-10T11:27:38.863955: step 1360, loss 0.402733, acc 0.828125, prec 0.0403232, recall 0.8024
2017-12-10T11:27:39.303286: step 1361, loss 1.02301, acc 0.796875, prec 0.0403793, recall 0.802716
2017-12-10T11:27:39.745978: step 1362, loss 1.26124, acc 0.78125, prec 0.0404336, recall 0.80303
2017-12-10T11:27:40.172121: step 1363, loss 0.961218, acc 0.828125, prec 0.0404158, recall 0.80303
2017-12-10T11:27:40.628860: step 1364, loss 0.726345, acc 0.78125, prec 0.0403931, recall 0.80303
2017-12-10T11:27:41.055814: step 1365, loss 0.956546, acc 0.828125, prec 0.0403753, recall 0.80303
2017-12-10T11:27:41.504771: step 1366, loss 1.09454, acc 0.734375, prec 0.0405016, recall 0.803657
2017-12-10T11:27:41.939435: step 1367, loss 1.00456, acc 0.78125, prec 0.0404789, recall 0.803657
2017-12-10T11:27:42.381196: step 1368, loss 0.45347, acc 0.875, prec 0.0405043, recall 0.803813
2017-12-10T11:27:42.816067: step 1369, loss 0.833409, acc 0.8125, prec 0.0405232, recall 0.803968
2017-12-10T11:27:43.271199: step 1370, loss 0.157355, acc 0.90625, prec 0.0405135, recall 0.803968
2017-12-10T11:27:43.702917: step 1371, loss 0.766978, acc 0.8125, prec 0.0405324, recall 0.804124
2017-12-10T11:27:44.140137: step 1372, loss 0.378479, acc 0.921875, prec 0.0405243, recall 0.804124
2017-12-10T11:27:44.575707: step 1373, loss 0.884997, acc 0.90625, prec 0.0405913, recall 0.804434
2017-12-10T11:27:45.015900: step 1374, loss 0.334241, acc 0.890625, prec 0.0405799, recall 0.804434
2017-12-10T11:27:45.461295: step 1375, loss 0.311147, acc 0.9375, prec 0.0406118, recall 0.804589
2017-12-10T11:27:45.929609: step 1376, loss 1.2227, acc 0.859375, prec 0.0406355, recall 0.804743
2017-12-10T11:27:46.372917: step 1377, loss 0.208819, acc 0.953125, prec 0.0406306, recall 0.804743
2017-12-10T11:27:46.815126: step 1378, loss 0.309276, acc 0.90625, prec 0.0406592, recall 0.804897
2017-12-10T11:27:47.256845: step 1379, loss 0.516686, acc 0.84375, prec 0.0406812, recall 0.805051
2017-12-10T11:27:47.691668: step 1380, loss 1.14446, acc 0.953125, prec 0.0407146, recall 0.805205
2017-12-10T11:27:48.131118: step 1381, loss 0.459563, acc 0.84375, prec 0.0406984, recall 0.805205
2017-12-10T11:27:48.585677: step 1382, loss 0.261941, acc 0.890625, prec 0.0407252, recall 0.805359
2017-12-10T11:27:49.022363: step 1383, loss 0.326139, acc 0.921875, prec 0.0407554, recall 0.805512
2017-12-10T11:27:49.456423: step 1384, loss 0.535377, acc 0.90625, prec 0.0407838, recall 0.805665
2017-12-10T11:27:49.894039: step 1385, loss 0.528133, acc 0.8125, prec 0.0407643, recall 0.805665
2017-12-10T11:27:50.338809: step 1386, loss 1.01271, acc 0.828125, prec 0.040861, recall 0.806122
2017-12-10T11:27:50.793689: step 1387, loss 3.35461, acc 0.90625, prec 0.040891, recall 0.805643
2017-12-10T11:27:51.244657: step 1388, loss 0.361218, acc 0.890625, prec 0.0408796, recall 0.805643
2017-12-10T11:27:51.698759: step 1389, loss 0.253465, acc 0.90625, prec 0.0408699, recall 0.805643
2017-12-10T11:27:52.138067: step 1390, loss 0.586738, acc 0.796875, prec 0.0408488, recall 0.805643
2017-12-10T11:27:52.585745: step 1391, loss 0.403459, acc 0.828125, prec 0.040869, recall 0.805795
2017-12-10T11:27:53.040849: step 1392, loss 0.5315, acc 0.84375, prec 0.0408909, recall 0.805947
2017-12-10T11:27:53.485307: step 1393, loss 0.447442, acc 0.859375, prec 0.0409143, recall 0.806099
2017-12-10T11:27:53.947317: step 1394, loss 0.512368, acc 0.859375, prec 0.0408997, recall 0.806099
2017-12-10T11:27:54.396790: step 1395, loss 0.176132, acc 0.953125, prec 0.0408948, recall 0.806099
2017-12-10T11:27:54.849870: step 1396, loss 0.282173, acc 0.890625, prec 0.0409215, recall 0.80625
2017-12-10T11:27:55.291929: step 1397, loss 2.11441, acc 0.90625, prec 0.0409134, recall 0.805621
2017-12-10T11:27:55.735745: step 1398, loss 4.70581, acc 0.875, prec 0.0409021, recall 0.804992
2017-12-10T11:27:56.185400: step 1399, loss 0.334633, acc 0.859375, prec 0.0408875, recall 0.804992
2017-12-10T11:27:56.634390: step 1400, loss 0.531632, acc 0.875, prec 0.0408745, recall 0.804992
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1400

2017-12-10T11:27:58.619380: step 1401, loss 0.446272, acc 0.90625, prec 0.0408648, recall 0.804992
2017-12-10T11:27:59.060781: step 1402, loss 1.24736, acc 0.84375, prec 0.0408866, recall 0.805144
2017-12-10T11:27:59.525038: step 1403, loss 0.424079, acc 0.84375, prec 0.0409463, recall 0.805447
2017-12-10T11:27:59.962063: step 1404, loss 0.575647, acc 0.890625, prec 0.0409729, recall 0.805599
2017-12-10T11:28:00.403335: step 1405, loss 0.411747, acc 0.84375, prec 0.0409567, recall 0.805599
2017-12-10T11:28:00.847843: step 1406, loss 0.654585, acc 0.796875, prec 0.0409736, recall 0.80575
2017-12-10T11:28:01.291066: step 1407, loss 0.201628, acc 0.921875, prec 0.0409655, recall 0.80575
2017-12-10T11:28:01.737102: step 1408, loss 0.439876, acc 0.84375, prec 0.0409872, recall 0.805901
2017-12-10T11:28:02.191033: step 1409, loss 0.519962, acc 0.859375, prec 0.0409726, recall 0.805901
2017-12-10T11:28:02.626675: step 1410, loss 0.811892, acc 0.828125, prec 0.0410305, recall 0.806202
2017-12-10T11:28:03.072924: step 1411, loss 0.550223, acc 0.796875, prec 0.0410095, recall 0.806202
2017-12-10T11:28:03.520803: step 1412, loss 0.587615, acc 0.84375, prec 0.0410311, recall 0.806352
2017-12-10T11:28:03.963830: step 1413, loss 0.721173, acc 0.921875, prec 0.0410608, recall 0.806502
2017-12-10T11:28:04.400037: step 1414, loss 0.593685, acc 0.78125, prec 0.0410759, recall 0.806651
2017-12-10T11:28:04.853353: step 1415, loss 0.345582, acc 0.859375, prec 0.0410614, recall 0.806651
2017-12-10T11:28:05.296173: step 1416, loss 0.501681, acc 0.9375, prec 0.0410549, recall 0.806651
2017-12-10T11:28:05.730339: step 1417, loss 0.452676, acc 0.875, prec 0.0410797, recall 0.806801
2017-12-10T11:28:06.163239: step 1418, loss 0.438693, acc 0.875, prec 0.0411045, recall 0.80695
2017-12-10T11:28:06.598300: step 1419, loss 0.467134, acc 0.921875, prec 0.0411718, recall 0.807248
2017-12-10T11:28:07.050541: step 1420, loss 0.210981, acc 0.921875, prec 0.0412014, recall 0.807396
2017-12-10T11:28:07.490165: step 1421, loss 0.324841, acc 0.890625, prec 0.0412655, recall 0.807692
2017-12-10T11:28:07.934638: step 1422, loss 0.283327, acc 0.921875, prec 0.0412574, recall 0.807692
2017-12-10T11:28:08.376376: step 1423, loss 0.370382, acc 0.90625, prec 0.0412476, recall 0.807692
2017-12-10T11:28:08.813084: step 1424, loss 1.64895, acc 0.90625, prec 0.0412395, recall 0.807072
2017-12-10T11:28:09.250866: step 1425, loss 0.64616, acc 0.875, prec 0.0412266, recall 0.807072
2017-12-10T11:28:09.683600: step 1426, loss 0.425626, acc 0.875, prec 0.0412513, recall 0.80722
2017-12-10T11:28:10.118966: step 1427, loss 0.406709, acc 0.890625, prec 0.0413152, recall 0.807515
2017-12-10T11:28:10.554779: step 1428, loss 0.122096, acc 0.921875, prec 0.0413071, recall 0.807515
2017-12-10T11:28:10.994928: step 1429, loss 0.560512, acc 0.9375, prec 0.0414134, recall 0.807957
2017-12-10T11:28:11.431477: step 1430, loss 2.98823, acc 0.890625, prec 0.0414412, recall 0.807487
2017-12-10T11:28:11.872546: step 1431, loss 0.327139, acc 0.875, prec 0.0414658, recall 0.807634
2017-12-10T11:28:12.300116: step 1432, loss 1.20333, acc 0.921875, prec 0.0414952, recall 0.80778
2017-12-10T11:28:12.744717: step 1433, loss 0.526374, acc 0.859375, prec 0.0415182, recall 0.807927
2017-12-10T11:28:13.177662: step 1434, loss 0.785797, acc 0.90625, prec 0.0415835, recall 0.808219
2017-12-10T11:28:13.613279: step 1435, loss 0.227172, acc 0.921875, prec 0.0416504, recall 0.808511
2017-12-10T11:28:14.060692: step 1436, loss 0.912221, acc 0.78125, prec 0.041665, recall 0.808656
2017-12-10T11:28:14.511420: step 1437, loss 0.843179, acc 0.71875, prec 0.0416732, recall 0.808801
2017-12-10T11:28:14.974798: step 1438, loss 0.849307, acc 0.828125, prec 0.0416927, recall 0.808946
2017-12-10T11:28:15.425375: step 1439, loss 0.615354, acc 0.828125, prec 0.0417871, recall 0.80938
2017-12-10T11:28:15.859304: step 1440, loss 0.487187, acc 0.859375, prec 0.0417724, recall 0.80938
2017-12-10T11:28:16.289375: step 1441, loss 0.739443, acc 0.8125, prec 0.0418276, recall 0.809668
2017-12-10T11:28:16.732436: step 1442, loss 0.578156, acc 0.8125, prec 0.0418454, recall 0.809811
2017-12-10T11:28:17.181234: step 1443, loss 0.482973, acc 0.84375, prec 0.0418291, recall 0.809811
2017-12-10T11:28:17.618505: step 1444, loss 0.826032, acc 0.734375, prec 0.0418387, recall 0.809955
2017-12-10T11:28:18.061486: step 1445, loss 0.671901, acc 0.75, prec 0.04185, recall 0.810098
2017-12-10T11:28:18.503297: step 1446, loss 0.456066, acc 0.828125, prec 0.0418693, recall 0.810241
2017-12-10T11:28:18.939081: step 1447, loss 0.283836, acc 0.921875, prec 0.0418612, recall 0.810241
2017-12-10T11:28:19.376690: step 1448, loss 0.790932, acc 0.828125, prec 0.0418433, recall 0.810241
2017-12-10T11:28:19.817407: step 1449, loss 0.322892, acc 0.875, prec 0.0418303, recall 0.810241
2017-12-10T11:28:20.251935: step 1450, loss 0.370163, acc 0.84375, prec 0.041814, recall 0.810241
2017-12-10T11:28:20.687572: step 1451, loss 3.44732, acc 0.890625, prec 0.0418415, recall 0.809774
2017-12-10T11:28:21.137688: step 1452, loss 0.527532, acc 0.828125, prec 0.0418608, recall 0.809917
2017-12-10T11:28:21.573872: step 1453, loss 0.745389, acc 0.8125, prec 0.0418413, recall 0.809917
2017-12-10T11:28:22.013694: step 1454, loss 3.22725, acc 0.8125, prec 0.041935, recall 0.809738
2017-12-10T11:28:22.447666: step 1455, loss 0.744269, acc 0.78125, prec 0.0419494, recall 0.80988
2017-12-10T11:28:22.884171: step 1456, loss 0.831873, acc 0.796875, prec 0.0419282, recall 0.80988
2017-12-10T11:28:23.328804: step 1457, loss 1.01313, acc 0.71875, prec 0.041899, recall 0.80988
2017-12-10T11:28:23.778487: step 1458, loss 0.679131, acc 0.859375, prec 0.0418844, recall 0.80988
2017-12-10T11:28:24.220860: step 1459, loss 0.633422, acc 0.78125, prec 0.0418617, recall 0.80988
2017-12-10T11:28:24.665231: step 1460, loss 0.706899, acc 0.8125, prec 0.0418794, recall 0.810022
2017-12-10T11:28:25.086938: step 1461, loss 1.20135, acc 0.890625, prec 0.0419421, recall 0.810306
2017-12-10T11:28:25.522756: step 1462, loss 0.514772, acc 0.796875, prec 0.0419951, recall 0.810589
2017-12-10T11:28:25.955217: step 1463, loss 0.598844, acc 0.8125, prec 0.0420126, recall 0.81073
2017-12-10T11:28:26.386986: step 1464, loss 0.563238, acc 0.828125, prec 0.0420317, recall 0.810871
2017-12-10T11:28:26.833108: step 1465, loss 0.535414, acc 0.828125, prec 0.0420139, recall 0.810871
2017-12-10T11:28:27.283718: step 1466, loss 0.436474, acc 0.859375, prec 0.0419993, recall 0.810871
2017-12-10T11:28:27.731088: step 1467, loss 0.45994, acc 0.859375, prec 0.0419847, recall 0.810871
2017-12-10T11:28:28.173467: step 1468, loss 0.765826, acc 0.828125, prec 0.0420039, recall 0.811012
2017-12-10T11:28:28.626145: step 1469, loss 3.12374, acc 0.859375, prec 0.0419909, recall 0.810409
2017-12-10T11:28:29.055990: step 1470, loss 3.05207, acc 0.84375, prec 0.0420501, recall 0.810089
2017-12-10T11:28:29.504382: step 1471, loss 0.477619, acc 0.84375, prec 0.042034, recall 0.810089
2017-12-10T11:28:29.944844: step 1472, loss 1.00178, acc 0.8125, prec 0.0420514, recall 0.81023
2017-12-10T11:28:30.393673: step 1473, loss 0.609482, acc 0.796875, prec 0.0420672, recall 0.81037
2017-12-10T11:28:30.844696: step 1474, loss 0.676247, acc 0.78125, prec 0.0420814, recall 0.810511
2017-12-10T11:28:31.284082: step 1475, loss 0.838081, acc 0.78125, prec 0.0420588, recall 0.810511
2017-12-10T11:28:31.719563: step 1476, loss 0.633751, acc 0.828125, prec 0.042041, recall 0.810511
2017-12-10T11:28:32.164892: step 1477, loss 0.489489, acc 0.859375, prec 0.0420265, recall 0.810511
2017-12-10T11:28:32.615410: step 1478, loss 0.946715, acc 0.8125, prec 0.0420806, recall 0.810791
2017-12-10T11:28:33.054119: step 1479, loss 0.332389, acc 0.890625, prec 0.0420693, recall 0.810791
2017-12-10T11:28:33.495044: step 1480, loss 0.52436, acc 0.859375, prec 0.0420548, recall 0.810791
2017-12-10T11:28:33.941754: step 1481, loss 0.349814, acc 0.921875, prec 0.0420835, recall 0.810931
2017-12-10T11:28:34.374811: step 1482, loss 0.397023, acc 0.859375, prec 0.0421057, recall 0.81107
2017-12-10T11:28:34.811417: step 1483, loss 3.76392, acc 0.890625, prec 0.0420976, recall 0.809875
2017-12-10T11:28:35.251360: step 1484, loss 0.75968, acc 0.828125, prec 0.0420799, recall 0.809875
2017-12-10T11:28:35.691315: step 1485, loss 0.633295, acc 0.859375, prec 0.0420654, recall 0.809875
2017-12-10T11:28:36.115693: step 1486, loss 0.493392, acc 0.84375, prec 0.0420859, recall 0.810015
2017-12-10T11:28:36.551591: step 1487, loss 0.342087, acc 0.859375, prec 0.0421081, recall 0.810154
2017-12-10T11:28:36.987870: step 1488, loss 0.705479, acc 0.796875, prec 0.0421238, recall 0.810294
2017-12-10T11:28:37.427376: step 1489, loss 0.542329, acc 0.84375, prec 0.0421077, recall 0.810294
2017-12-10T11:28:37.862611: step 1490, loss 0.320629, acc 0.890625, prec 0.042133, recall 0.810434
2017-12-10T11:28:38.255152: step 1491, loss 0.501025, acc 0.862745, prec 0.0421583, recall 0.810573
2017-12-10T11:28:38.714754: step 1492, loss 0.951394, acc 0.78125, prec 0.0421358, recall 0.810573
2017-12-10T11:28:39.153085: step 1493, loss 0.892051, acc 0.796875, prec 0.0421149, recall 0.810573
2017-12-10T11:28:39.587733: step 1494, loss 0.306708, acc 0.875, prec 0.0421021, recall 0.810573
2017-12-10T11:28:40.025437: step 1495, loss 0.475334, acc 0.84375, prec 0.042159, recall 0.81085
2017-12-10T11:28:40.489956: step 1496, loss 0.225724, acc 0.921875, prec 0.0421875, recall 0.810989
2017-12-10T11:28:40.891503: step 1497, loss 2.30409, acc 0.859375, prec 0.0422476, recall 0.810673
2017-12-10T11:28:41.342337: step 1498, loss 0.692702, acc 0.875, prec 0.0422712, recall 0.810811
2017-12-10T11:28:41.770719: step 1499, loss 0.308589, acc 0.921875, prec 0.0422632, recall 0.810811
2017-12-10T11:28:42.217549: step 1500, loss 0.651811, acc 0.84375, prec 0.0422471, recall 0.810811
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1500

2017-12-10T11:28:44.110604: step 1501, loss 0.550862, acc 0.859375, prec 0.0422326, recall 0.810811
2017-12-10T11:28:44.554581: step 1502, loss 0.375806, acc 0.921875, prec 0.042261, recall 0.810949
2017-12-10T11:28:44.999952: step 1503, loss 0.647841, acc 0.828125, prec 0.0422433, recall 0.810949
2017-12-10T11:28:45.440039: step 1504, loss 0.266899, acc 0.9375, prec 0.0422733, recall 0.811087
2017-12-10T11:28:45.873973: step 1505, loss 1.20432, acc 0.84375, prec 0.0423301, recall 0.811362
2017-12-10T11:28:46.314344: step 1506, loss 0.335324, acc 0.859375, prec 0.042352, recall 0.811499
2017-12-10T11:28:46.753551: step 1507, loss 0.392143, acc 0.921875, prec 0.0423439, recall 0.811499
2017-12-10T11:28:47.198400: step 1508, loss 0.440502, acc 0.90625, prec 0.0424433, recall 0.81191
2017-12-10T11:28:47.640750: step 1509, loss 0.296979, acc 0.890625, prec 0.0425047, recall 0.812183
2017-12-10T11:28:48.083335: step 1510, loss 0.395127, acc 0.875, prec 0.0425282, recall 0.812319
2017-12-10T11:28:48.531799: step 1511, loss 0.356742, acc 0.90625, prec 0.0425911, recall 0.81259
2017-12-10T11:28:48.967599: step 1512, loss 0.260463, acc 0.921875, prec 0.0426193, recall 0.812726
2017-12-10T11:28:49.401406: step 1513, loss 0.258683, acc 0.9375, prec 0.0426129, recall 0.812726
2017-12-10T11:28:49.834198: step 1514, loss 0.129591, acc 0.9375, prec 0.0426427, recall 0.812861
2017-12-10T11:28:50.284070: step 1515, loss 0.397267, acc 0.921875, prec 0.0426709, recall 0.812996
2017-12-10T11:28:50.724970: step 1516, loss 0.303895, acc 0.921875, prec 0.0426628, recall 0.812996
2017-12-10T11:28:51.170838: step 1517, loss 0.238643, acc 0.921875, prec 0.042691, recall 0.813131
2017-12-10T11:28:51.614826: step 1518, loss 4.52783, acc 0.90625, prec 0.0427554, recall 0.812815
2017-12-10T11:28:52.055995: step 1519, loss 0.240796, acc 0.9375, prec 0.042749, recall 0.812815
2017-12-10T11:28:52.508976: step 1520, loss 0.224464, acc 0.9375, prec 0.042815, recall 0.813084
2017-12-10T11:28:52.938103: step 1521, loss 0.0503626, acc 0.984375, prec 0.0428133, recall 0.813084
2017-12-10T11:28:53.379081: step 1522, loss 0.399297, acc 0.9375, prec 0.0428431, recall 0.813218
2017-12-10T11:28:53.818843: step 1523, loss 0.562492, acc 0.84375, prec 0.0429355, recall 0.81362
2017-12-10T11:28:54.250038: step 1524, loss 0.387669, acc 0.96875, prec 0.0430047, recall 0.813887
2017-12-10T11:28:54.676907: step 1525, loss 0.237069, acc 0.9375, prec 0.0430343, recall 0.81402
2017-12-10T11:28:55.120230: step 1526, loss 0.377775, acc 0.875, prec 0.0430937, recall 0.814286
2017-12-10T11:28:55.565557: step 1527, loss 0.360846, acc 0.859375, prec 0.043079, recall 0.814286
2017-12-10T11:28:56.007439: step 1528, loss 5.10361, acc 0.890625, prec 0.0430693, recall 0.813704
2017-12-10T11:28:56.448907: step 1529, loss 0.341445, acc 0.875, prec 0.0430562, recall 0.813704
2017-12-10T11:28:56.876434: step 1530, loss 0.178434, acc 0.890625, prec 0.043081, recall 0.813837
2017-12-10T11:28:57.312325: step 1531, loss 0.562132, acc 0.875, prec 0.0431402, recall 0.814103
2017-12-10T11:28:57.752305: step 1532, loss 0.66756, acc 0.84375, prec 0.0431239, recall 0.814103
2017-12-10T11:28:58.196746: step 1533, loss 0.874739, acc 0.828125, prec 0.0432143, recall 0.814499
2017-12-10T11:28:58.638074: step 1534, loss 0.657872, acc 0.8125, prec 0.0432308, recall 0.814631
2017-12-10T11:28:59.071214: step 1535, loss 0.631722, acc 0.8125, prec 0.0432473, recall 0.814762
2017-12-10T11:28:59.507283: step 1536, loss 0.73911, acc 0.75, prec 0.0432573, recall 0.814894
2017-12-10T11:28:59.940497: step 1537, loss 1.06688, acc 0.71875, prec 0.043228, recall 0.814894
2017-12-10T11:29:00.375639: step 1538, loss 0.733335, acc 0.84375, prec 0.0432837, recall 0.815156
2017-12-10T11:29:00.805217: step 1539, loss 0.515631, acc 0.875, prec 0.0432707, recall 0.815156
2017-12-10T11:29:01.243551: step 1540, loss 0.882355, acc 0.765625, prec 0.0432822, recall 0.815287
2017-12-10T11:29:01.679520: step 1541, loss 0.407072, acc 0.890625, prec 0.0433068, recall 0.815417
2017-12-10T11:29:02.120298: step 1542, loss 0.143521, acc 0.9375, prec 0.0433362, recall 0.815548
2017-12-10T11:29:02.555987: step 1543, loss 0.627201, acc 0.828125, prec 0.043426, recall 0.815938
2017-12-10T11:29:03.002444: step 1544, loss 0.492194, acc 0.84375, prec 0.0434097, recall 0.815938
2017-12-10T11:29:03.446270: step 1545, loss 0.534888, acc 0.8125, prec 0.0434261, recall 0.816068
2017-12-10T11:29:03.894678: step 1546, loss 0.391747, acc 0.890625, prec 0.0435581, recall 0.816585
2017-12-10T11:29:04.336193: step 1547, loss 0.936586, acc 0.90625, prec 0.0436558, recall 0.816971
2017-12-10T11:29:04.770061: step 1548, loss 0.228485, acc 0.921875, prec 0.0437193, recall 0.817227
2017-12-10T11:29:05.223097: step 1549, loss 0.519193, acc 0.890625, prec 0.0437437, recall 0.817355
2017-12-10T11:29:05.663735: step 1550, loss 2.13412, acc 0.859375, prec 0.0438363, recall 0.817737
2017-12-10T11:29:06.095659: step 1551, loss 0.461959, acc 0.828125, prec 0.0438541, recall 0.817865
2017-12-10T11:29:06.525065: step 1552, loss 0.26864, acc 0.921875, prec 0.0438459, recall 0.817865
2017-12-10T11:29:06.962133: step 1553, loss 0.351662, acc 0.875, prec 0.0438327, recall 0.817865
2017-12-10T11:29:07.399920: step 1554, loss 0.619067, acc 0.828125, prec 0.0438505, recall 0.817992
2017-12-10T11:29:07.842286: step 1555, loss 0.303043, acc 0.921875, prec 0.0439137, recall 0.818245
2017-12-10T11:29:08.285087: step 1556, loss 0.201141, acc 0.953125, prec 0.0439445, recall 0.818372
2017-12-10T11:29:08.710282: step 1557, loss 0.473529, acc 0.875, prec 0.0439314, recall 0.818372
2017-12-10T11:29:09.141724: step 1558, loss 0.353161, acc 0.921875, prec 0.0439589, recall 0.818498
2017-12-10T11:29:09.580241: step 1559, loss 0.232142, acc 0.921875, prec 0.0439864, recall 0.818624
2017-12-10T11:29:10.011520: step 1560, loss 1.63879, acc 0.953125, prec 0.0439831, recall 0.818056
2017-12-10T11:29:10.448996: step 1561, loss 0.569594, acc 0.859375, prec 0.044004, recall 0.818182
2017-12-10T11:29:10.889250: step 1562, loss 0.476694, acc 0.890625, prec 0.0440282, recall 0.818308
2017-12-10T11:29:11.330163: step 1563, loss 3.97582, acc 0.890625, prec 0.0440184, recall 0.817741
2017-12-10T11:29:11.771436: step 1564, loss 0.437032, acc 0.859375, prec 0.0440749, recall 0.817993
2017-12-10T11:29:12.219927: step 1565, loss 0.244988, acc 0.890625, prec 0.044099, recall 0.818119
2017-12-10T11:29:12.657546: step 1566, loss 0.377491, acc 0.875, prec 0.0440859, recall 0.818119
2017-12-10T11:29:13.099620: step 1567, loss 0.549813, acc 0.8125, prec 0.0440662, recall 0.818119
2017-12-10T11:29:13.534992: step 1568, loss 1.07457, acc 0.78125, prec 0.0440432, recall 0.818119
2017-12-10T11:29:13.978642: step 1569, loss 0.552007, acc 0.796875, prec 0.0440575, recall 0.818245
2017-12-10T11:29:14.419587: step 1570, loss 0.806939, acc 0.875, prec 0.0440799, recall 0.81837
2017-12-10T11:29:14.872998: step 1571, loss 0.363377, acc 0.90625, prec 0.0441056, recall 0.818496
2017-12-10T11:29:15.313210: step 1572, loss 0.664248, acc 0.875, prec 0.0440925, recall 0.818496
2017-12-10T11:29:15.745856: step 1573, loss 0.573075, acc 0.875, prec 0.0441149, recall 0.818621
2017-12-10T11:29:16.182721: step 1574, loss 0.974227, acc 0.875, prec 0.0441728, recall 0.818871
2017-12-10T11:29:16.622093: step 1575, loss 0.705987, acc 0.78125, prec 0.0441499, recall 0.818871
2017-12-10T11:29:17.059737: step 1576, loss 0.455953, acc 0.921875, prec 0.0442126, recall 0.81912
2017-12-10T11:29:17.488621: step 1577, loss 0.569197, acc 0.875, prec 0.0442704, recall 0.819368
2017-12-10T11:29:17.922111: step 1578, loss 0.816167, acc 0.8125, prec 0.0442507, recall 0.819368
2017-12-10T11:29:18.363129: step 1579, loss 0.569128, acc 0.796875, prec 0.0442648, recall 0.819492
2017-12-10T11:29:18.805306: step 1580, loss 0.3556, acc 0.921875, prec 0.0442566, recall 0.819492
2017-12-10T11:29:19.248577: step 1581, loss 0.287387, acc 0.921875, prec 0.0442839, recall 0.819616
2017-12-10T11:29:19.684768: step 1582, loss 0.456669, acc 0.9375, prec 0.0442773, recall 0.819616
2017-12-10T11:29:20.122569: step 1583, loss 0.118082, acc 0.953125, prec 0.0443078, recall 0.81974
2017-12-10T11:29:20.560718: step 1584, loss 0.362338, acc 0.859375, prec 0.044293, recall 0.81974
2017-12-10T11:29:21.001646: step 1585, loss 0.519414, acc 0.875, prec 0.0442799, recall 0.81974
2017-12-10T11:29:21.428593: step 1586, loss 0.815203, acc 0.875, prec 0.0443729, recall 0.820109
2017-12-10T11:29:21.868389: step 1587, loss 0.257072, acc 0.921875, prec 0.0444708, recall 0.820478
2017-12-10T11:29:22.307040: step 1588, loss 0.589086, acc 0.96875, prec 0.0445382, recall 0.820723
2017-12-10T11:29:22.750763: step 1589, loss 0.213627, acc 0.953125, prec 0.0445686, recall 0.820845
2017-12-10T11:29:23.188755: step 1590, loss 0.195554, acc 0.9375, prec 0.044562, recall 0.820845
2017-12-10T11:29:23.632640: step 1591, loss 0.188303, acc 0.9375, prec 0.044626, recall 0.821088
2017-12-10T11:29:24.068906: step 1592, loss 0.252959, acc 0.953125, prec 0.0446211, recall 0.821088
2017-12-10T11:29:24.498068: step 1593, loss 1.42908, acc 0.921875, prec 0.0446145, recall 0.82053
2017-12-10T11:29:24.944141: step 1594, loss 0.0501596, acc 0.984375, prec 0.0446128, recall 0.82053
2017-12-10T11:29:25.381229: step 1595, loss 0.159719, acc 0.953125, prec 0.0446079, recall 0.82053
2017-12-10T11:29:25.839620: step 1596, loss 0.130426, acc 0.921875, prec 0.0445996, recall 0.82053
2017-12-10T11:29:26.285167: step 1597, loss 7.88776, acc 0.90625, prec 0.0446636, recall 0.819661
2017-12-10T11:29:26.729683: step 1598, loss 0.222343, acc 0.921875, prec 0.0446907, recall 0.819783
2017-12-10T11:29:27.165524: step 1599, loss 0.465065, acc 0.828125, prec 0.0446725, recall 0.819783
2017-12-10T11:29:27.609351: step 1600, loss 0.459565, acc 0.859375, prec 0.0446577, recall 0.819783
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1600

2017-12-10T11:29:29.608939: step 1601, loss 0.341152, acc 0.875, prec 0.0446798, recall 0.819905
2017-12-10T11:29:30.044296: step 1602, loss 0.850423, acc 0.90625, prec 0.0447403, recall 0.820149
2017-12-10T11:29:30.480286: step 1603, loss 0.545103, acc 0.8125, prec 0.044791, recall 0.820392
2017-12-10T11:29:30.920854: step 1604, loss 0.253322, acc 0.90625, prec 0.0447811, recall 0.820392
2017-12-10T11:29:31.367604: step 1605, loss 0.552963, acc 0.859375, prec 0.0448014, recall 0.820513
2017-12-10T11:29:31.812129: step 1606, loss 0.777943, acc 0.84375, prec 0.0448904, recall 0.820875
2017-12-10T11:29:32.252836: step 1607, loss 1.15437, acc 0.75, prec 0.044864, recall 0.820875
2017-12-10T11:29:32.675811: step 1608, loss 0.736762, acc 0.703125, prec 0.0449029, recall 0.821116
2017-12-10T11:29:33.115053: step 1609, loss 0.726024, acc 0.765625, prec 0.0448782, recall 0.821116
2017-12-10T11:29:33.553834: step 1610, loss 0.557645, acc 0.84375, prec 0.0448968, recall 0.821237
2017-12-10T11:29:33.998812: step 1611, loss 1.0858, acc 0.84375, prec 0.0449855, recall 0.821596
2017-12-10T11:29:34.444568: step 1612, loss 0.830617, acc 0.75, prec 0.0449941, recall 0.821716
2017-12-10T11:29:34.905757: step 1613, loss 0.607638, acc 0.8125, prec 0.0450444, recall 0.821954
2017-12-10T11:29:35.335187: step 1614, loss 0.760767, acc 0.8125, prec 0.0450246, recall 0.821954
2017-12-10T11:29:35.780219: step 1615, loss 0.656105, acc 0.828125, prec 0.0450414, recall 0.822074
2017-12-10T11:29:36.224205: step 1616, loss 0.682607, acc 0.765625, prec 0.0450516, recall 0.822192
2017-12-10T11:29:36.667500: step 1617, loss 0.642723, acc 0.8125, prec 0.0450319, recall 0.822192
2017-12-10T11:29:37.120556: step 1618, loss 0.48865, acc 0.84375, prec 0.0450154, recall 0.822192
2017-12-10T11:29:37.567927: step 1619, loss 0.67055, acc 0.875, prec 0.0450022, recall 0.822192
2017-12-10T11:29:38.012874: step 1620, loss 1.45856, acc 0.84375, prec 0.0449874, recall 0.821643
2017-12-10T11:29:38.450439: step 1621, loss 0.342227, acc 0.859375, prec 0.0449726, recall 0.821643
2017-12-10T11:29:38.873881: step 1622, loss 0.550549, acc 0.875, prec 0.0450292, recall 0.821881
2017-12-10T11:29:39.307368: step 1623, loss 0.608371, acc 0.84375, prec 0.0450128, recall 0.821881
2017-12-10T11:29:39.748297: step 1624, loss 1.22349, acc 0.8125, prec 0.0450279, recall 0.822
2017-12-10T11:29:40.189814: step 1625, loss 0.486681, acc 0.84375, prec 0.0450115, recall 0.822
2017-12-10T11:29:40.639484: step 1626, loss 0.425433, acc 0.828125, prec 0.0450283, recall 0.822119
2017-12-10T11:29:41.084037: step 1627, loss 0.499224, acc 0.84375, prec 0.0450119, recall 0.822119
2017-12-10T11:29:41.527530: step 1628, loss 0.376461, acc 0.890625, prec 0.0450352, recall 0.822237
2017-12-10T11:29:41.962768: step 1629, loss 0.486037, acc 0.84375, prec 0.0450884, recall 0.822473
2017-12-10T11:29:42.391491: step 1630, loss 0.188008, acc 0.890625, prec 0.0451117, recall 0.822591
2017-12-10T11:29:42.830477: step 1631, loss 0.440347, acc 0.859375, prec 0.0451665, recall 0.822827
2017-12-10T11:29:43.267883: step 1632, loss 0.237608, acc 0.921875, prec 0.045193, recall 0.822944
2017-12-10T11:29:43.708711: step 1633, loss 0.312213, acc 0.921875, prec 0.0452195, recall 0.823062
2017-12-10T11:29:44.163808: step 1634, loss 0.222087, acc 0.9375, prec 0.045213, recall 0.823062
2017-12-10T11:29:44.624485: step 1635, loss 0.0795898, acc 0.953125, prec 0.0452428, recall 0.823179
2017-12-10T11:29:45.061306: step 1636, loss 0.238519, acc 0.890625, prec 0.0452312, recall 0.823179
2017-12-10T11:29:45.504938: step 1637, loss 2.23764, acc 0.90625, prec 0.0452578, recall 0.822751
2017-12-10T11:29:45.946263: step 1638, loss 2.08011, acc 0.90625, prec 0.0452495, recall 0.822208
2017-12-10T11:29:46.385548: step 1639, loss 0.21726, acc 0.953125, prec 0.0452793, recall 0.822325
2017-12-10T11:29:46.824648: step 1640, loss 0.0877675, acc 0.953125, prec 0.0453091, recall 0.822442
2017-12-10T11:29:47.265392: step 1641, loss 0.45783, acc 0.9375, prec 0.0453372, recall 0.822559
2017-12-10T11:29:47.716339: step 1642, loss 0.113405, acc 0.9375, prec 0.0453306, recall 0.822559
2017-12-10T11:29:48.159183: step 1643, loss 0.275404, acc 0.953125, prec 0.0453951, recall 0.822793
2017-12-10T11:29:48.598786: step 1644, loss 0.475182, acc 0.859375, prec 0.0453802, recall 0.822793
2017-12-10T11:29:49.028520: step 1645, loss 2.70017, acc 0.90625, prec 0.045476, recall 0.822602
2017-12-10T11:29:49.459262: step 1646, loss 0.196403, acc 0.90625, prec 0.0455354, recall 0.822835
2017-12-10T11:29:49.892848: step 1647, loss 0.284207, acc 0.921875, prec 0.0455965, recall 0.823067
2017-12-10T11:29:50.332920: step 1648, loss 0.408508, acc 0.84375, prec 0.0456145, recall 0.823183
2017-12-10T11:29:50.781450: step 1649, loss 0.641666, acc 0.78125, prec 0.0455914, recall 0.823183
2017-12-10T11:29:51.218055: step 1650, loss 0.516763, acc 0.8125, prec 0.0456061, recall 0.823298
2017-12-10T11:29:51.652846: step 1651, loss 0.831736, acc 0.828125, prec 0.0456917, recall 0.823645
2017-12-10T11:29:52.084364: step 1652, loss 1.03228, acc 0.78125, prec 0.0456685, recall 0.823645
2017-12-10T11:29:52.515449: step 1653, loss 1.03721, acc 0.703125, prec 0.0456717, recall 0.82376
2017-12-10T11:29:52.969967: step 1654, loss 0.931882, acc 0.703125, prec 0.0456748, recall 0.823875
2017-12-10T11:29:53.408863: step 1655, loss 0.554793, acc 0.8125, prec 0.045724, recall 0.824104
2017-12-10T11:29:53.861926: step 1656, loss 0.994412, acc 0.796875, prec 0.0457715, recall 0.824333
2017-12-10T11:29:54.298025: step 1657, loss 0.543718, acc 0.859375, prec 0.0457911, recall 0.824447
2017-12-10T11:29:54.753440: step 1658, loss 0.299987, acc 0.875, prec 0.0457778, recall 0.824447
2017-12-10T11:29:55.187534: step 1659, loss 0.620753, acc 0.90625, prec 0.0458368, recall 0.824675
2017-12-10T11:29:55.621869: step 1660, loss 0.41365, acc 0.84375, prec 0.0458203, recall 0.824675
2017-12-10T11:29:56.057812: step 1661, loss 0.359314, acc 0.890625, prec 0.0458087, recall 0.824675
2017-12-10T11:29:56.494429: step 1662, loss 0.564235, acc 0.84375, prec 0.0457922, recall 0.824675
2017-12-10T11:29:56.935276: step 1663, loss 0.338505, acc 0.8125, prec 0.0457724, recall 0.824675
2017-12-10T11:29:57.379836: step 1664, loss 0.857181, acc 0.921875, prec 0.0458329, recall 0.824903
2017-12-10T11:29:57.824657: step 1665, loss 0.305282, acc 0.9375, prec 0.0458263, recall 0.824903
2017-12-10T11:29:58.256378: step 1666, loss 0.331506, acc 0.890625, prec 0.0458147, recall 0.824903
2017-12-10T11:29:58.700391: step 1667, loss 0.386955, acc 0.9375, prec 0.0458425, recall 0.825016
2017-12-10T11:29:59.164554: step 1668, loss 0.241179, acc 0.921875, prec 0.0458342, recall 0.825016
2017-12-10T11:29:59.605079: step 1669, loss 0.322146, acc 0.90625, prec 0.0458243, recall 0.825016
2017-12-10T11:30:00.052982: step 1670, loss 0.235452, acc 0.90625, prec 0.0458488, recall 0.82513
2017-12-10T11:30:00.500028: step 1671, loss 0.231593, acc 0.921875, prec 0.0458405, recall 0.82513
2017-12-10T11:30:00.940718: step 1672, loss 1.25566, acc 0.875, prec 0.045896, recall 0.825356
2017-12-10T11:30:01.367985: step 1673, loss 0.173149, acc 0.9375, prec 0.0459237, recall 0.825469
2017-12-10T11:30:01.802304: step 1674, loss 0.300105, acc 0.921875, prec 0.0459154, recall 0.825469
2017-12-10T11:30:02.249510: step 1675, loss 0.18219, acc 0.9375, prec 0.0459431, recall 0.825581
2017-12-10T11:30:02.691906: step 1676, loss 1.59136, acc 0.953125, prec 0.0460753, recall 0.826031
2017-12-10T11:30:03.136566: step 1677, loss 0.221779, acc 0.96875, prec 0.0461063, recall 0.826143
2017-12-10T11:30:03.584572: step 1678, loss 0.399714, acc 0.921875, prec 0.046098, recall 0.826143
2017-12-10T11:30:04.026922: step 1679, loss 0.463392, acc 0.875, prec 0.0461533, recall 0.826367
2017-12-10T11:30:04.478612: step 1680, loss 0.33853, acc 0.84375, prec 0.0461367, recall 0.826367
2017-12-10T11:30:04.918297: step 1681, loss 0.183756, acc 0.921875, prec 0.0461284, recall 0.826367
2017-12-10T11:30:05.358630: step 1682, loss 0.145435, acc 0.953125, prec 0.0461235, recall 0.826367
2017-12-10T11:30:05.799853: step 1683, loss 0.242727, acc 0.9375, prec 0.0461511, recall 0.826478
2017-12-10T11:30:06.256245: step 1684, loss 0.33414, acc 0.890625, prec 0.0461395, recall 0.826478
2017-12-10T11:30:06.727892: step 1685, loss 0.379457, acc 0.890625, prec 0.0461279, recall 0.826478
2017-12-10T11:30:07.182994: step 1686, loss 0.334599, acc 0.90625, prec 0.046118, recall 0.826478
2017-12-10T11:30:07.636993: step 1687, loss 0.480968, acc 0.890625, prec 0.0461748, recall 0.826701
2017-12-10T11:30:08.088125: step 1688, loss 0.106707, acc 0.96875, prec 0.0461715, recall 0.826701
2017-12-10T11:30:08.541445: step 1689, loss 0.50511, acc 0.921875, prec 0.0461632, recall 0.826701
2017-12-10T11:30:08.976729: step 1690, loss 0.0932044, acc 0.953125, prec 0.0461583, recall 0.826701
2017-12-10T11:30:09.410027: step 1691, loss 0.275228, acc 0.953125, prec 0.0461875, recall 0.826812
2017-12-10T11:30:09.851319: step 1692, loss 0.955427, acc 0.9375, prec 0.046215, recall 0.826923
2017-12-10T11:30:10.290587: step 1693, loss 0.174376, acc 0.953125, prec 0.0462101, recall 0.826923
2017-12-10T11:30:10.725279: step 1694, loss 0.556001, acc 0.890625, prec 0.0461985, recall 0.826923
2017-12-10T11:30:11.168268: step 1695, loss 0.123941, acc 0.9375, prec 0.046226, recall 0.827034
2017-12-10T11:30:11.600128: step 1696, loss 0.237284, acc 0.890625, prec 0.0462486, recall 0.827145
2017-12-10T11:30:12.038622: step 1697, loss 0.373485, acc 0.890625, prec 0.0463052, recall 0.827366
2017-12-10T11:30:12.484319: step 1698, loss 0.248595, acc 0.921875, prec 0.046297, recall 0.827366
2017-12-10T11:30:12.923957: step 1699, loss 2.93591, acc 0.9375, prec 0.0463261, recall 0.826948
2017-12-10T11:30:13.375255: step 1700, loss 0.327796, acc 0.890625, prec 0.0463145, recall 0.826948
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1700

2017-12-10T11:30:15.260917: step 1701, loss 0.478211, acc 0.859375, prec 0.0462996, recall 0.826948
2017-12-10T11:30:15.689554: step 1702, loss 0.397361, acc 0.890625, prec 0.046288, recall 0.826948
2017-12-10T11:30:16.133160: step 1703, loss 0.318825, acc 0.890625, prec 0.0463446, recall 0.827168
2017-12-10T11:30:16.571415: step 1704, loss 0.395306, acc 0.921875, prec 0.0463704, recall 0.827278
2017-12-10T11:30:17.021779: step 1705, loss 0.142838, acc 0.9375, prec 0.0464659, recall 0.827608
2017-12-10T11:30:17.462493: step 1706, loss 1.70591, acc 0.90625, prec 0.0465257, recall 0.827302
2017-12-10T11:30:17.897665: step 1707, loss 0.233128, acc 0.921875, prec 0.0465174, recall 0.827302
2017-12-10T11:30:18.338178: step 1708, loss 0.371467, acc 0.921875, prec 0.0465772, recall 0.827521
2017-12-10T11:30:18.787664: step 1709, loss 0.239386, acc 0.921875, prec 0.0465689, recall 0.827521
2017-12-10T11:30:19.232577: step 1710, loss 0.520024, acc 0.875, prec 0.0466236, recall 0.827739
2017-12-10T11:30:19.678564: step 1711, loss 0.658527, acc 0.828125, prec 0.0466393, recall 0.827848
2017-12-10T11:30:20.131561: step 1712, loss 0.423612, acc 0.859375, prec 0.0466244, recall 0.827848
2017-12-10T11:30:20.579988: step 1713, loss 0.47183, acc 0.84375, prec 0.0466757, recall 0.828066
2017-12-10T11:30:21.011766: step 1714, loss 0.526602, acc 0.875, prec 0.0466624, recall 0.828066
2017-12-10T11:30:21.450212: step 1715, loss 0.697319, acc 0.78125, prec 0.046707, recall 0.828283
2017-12-10T11:30:21.891179: step 1716, loss 0.544726, acc 0.765625, prec 0.0466821, recall 0.828283
2017-12-10T11:30:22.318772: step 1717, loss 0.258406, acc 0.921875, prec 0.0466738, recall 0.828283
2017-12-10T11:30:22.766433: step 1718, loss 0.45199, acc 0.921875, prec 0.0467333, recall 0.828499
2017-12-10T11:30:23.206090: step 1719, loss 0.44944, acc 0.890625, prec 0.0467217, recall 0.828499
2017-12-10T11:30:23.640737: step 1720, loss 0.586177, acc 0.875, prec 0.0467423, recall 0.828607
2017-12-10T11:30:24.079945: step 1721, loss 0.182237, acc 0.96875, prec 0.0467728, recall 0.828715
2017-12-10T11:30:24.529131: step 1722, loss 0.602564, acc 0.75, prec 0.0467801, recall 0.828823
2017-12-10T11:30:24.979260: step 1723, loss 0.119223, acc 0.984375, prec 0.0467784, recall 0.828823
2017-12-10T11:30:25.423012: step 1724, loss 0.297858, acc 0.90625, prec 0.0467685, recall 0.828823
2017-12-10T11:30:25.855302: step 1725, loss 0.516179, acc 0.90625, prec 0.0468262, recall 0.829038
2017-12-10T11:30:26.295091: step 1726, loss 0.311595, acc 0.90625, prec 0.0469177, recall 0.82936
2017-12-10T11:30:26.743284: step 1727, loss 0.312304, acc 0.90625, prec 0.0469415, recall 0.829467
2017-12-10T11:30:27.199384: step 1728, loss 8.7399, acc 0.921875, prec 0.0469349, recall 0.828947
2017-12-10T11:30:27.646315: step 1729, loss 1.07543, acc 0.90625, prec 0.0469587, recall 0.829054
2017-12-10T11:30:28.095868: step 1730, loss 0.647992, acc 0.828125, prec 0.0469404, recall 0.829054
2017-12-10T11:30:28.545187: step 1731, loss 2.46938, acc 0.859375, prec 0.0469271, recall 0.828536
2017-12-10T11:30:28.992789: step 1732, loss 0.834691, acc 0.75, prec 0.0469342, recall 0.828643
2017-12-10T11:30:29.444619: step 1733, loss 0.618974, acc 0.765625, prec 0.046943, recall 0.82875
2017-12-10T11:30:29.882101: step 1734, loss 0.632516, acc 0.84375, prec 0.0469264, recall 0.82875
2017-12-10T11:30:30.309611: step 1735, loss 1.01389, acc 0.765625, prec 0.0469015, recall 0.82875
2017-12-10T11:30:30.745096: step 1736, loss 0.902705, acc 0.8125, prec 0.0469153, recall 0.828857
2017-12-10T11:30:31.196619: step 1737, loss 0.645638, acc 0.75, prec 0.0469225, recall 0.828964
2017-12-10T11:30:31.635376: step 1738, loss 0.676464, acc 0.859375, prec 0.0469412, recall 0.829071
2017-12-10T11:30:32.096224: step 1739, loss 0.672775, acc 0.796875, prec 0.0469533, recall 0.829177
2017-12-10T11:30:32.531685: step 1740, loss 0.543907, acc 0.828125, prec 0.0469351, recall 0.829177
2017-12-10T11:30:32.970107: step 1741, loss 0.620574, acc 0.796875, prec 0.0469808, recall 0.82939
2017-12-10T11:30:33.413428: step 1742, loss 0.865345, acc 0.765625, prec 0.046956, recall 0.82939
2017-12-10T11:30:33.867959: step 1743, loss 0.704695, acc 0.796875, prec 0.0470352, recall 0.829708
2017-12-10T11:30:34.318197: step 1744, loss 0.611792, acc 0.78125, prec 0.047012, recall 0.829708
2017-12-10T11:30:34.747435: step 1745, loss 0.475963, acc 0.859375, prec 0.0470642, recall 0.829919
2017-12-10T11:30:35.190992: step 1746, loss 0.755863, acc 0.796875, prec 0.0470762, recall 0.830025
2017-12-10T11:30:35.635700: step 1747, loss 0.707095, acc 0.8125, prec 0.0470899, recall 0.83013
2017-12-10T11:30:36.084893: step 1748, loss 0.368523, acc 0.921875, prec 0.0470816, recall 0.83013
2017-12-10T11:30:36.520594: step 1749, loss 0.641714, acc 0.859375, prec 0.0470667, recall 0.83013
2017-12-10T11:30:36.965564: step 1750, loss 0.224107, acc 0.90625, prec 0.0470902, recall 0.830235
2017-12-10T11:30:37.398823: step 1751, loss 0.422539, acc 0.84375, prec 0.0471741, recall 0.83055
2017-12-10T11:30:37.837866: step 1752, loss 0.456975, acc 0.90625, prec 0.0471642, recall 0.83055
2017-12-10T11:30:38.286381: step 1753, loss 1.42984, acc 0.953125, prec 0.047293, recall 0.830969
2017-12-10T11:30:38.733870: step 1754, loss 0.257842, acc 0.90625, prec 0.0472831, recall 0.830969
2017-12-10T11:30:39.172209: step 1755, loss 0.342147, acc 0.9375, prec 0.0473099, recall 0.831073
2017-12-10T11:30:39.608286: step 1756, loss 0.0749772, acc 0.96875, prec 0.0473065, recall 0.831073
2017-12-10T11:30:40.048605: step 1757, loss 0.174716, acc 0.953125, prec 0.0473016, recall 0.831073
2017-12-10T11:30:40.488873: step 1758, loss 0.719636, acc 0.890625, prec 0.0473568, recall 0.831281
2017-12-10T11:30:40.937691: step 1759, loss 0.309165, acc 0.9375, prec 0.047417, recall 0.831488
2017-12-10T11:30:41.378329: step 1760, loss 0.142836, acc 0.953125, prec 0.0474788, recall 0.831695
2017-12-10T11:30:41.816669: step 1761, loss 0.293751, acc 0.859375, prec 0.0474972, recall 0.831799
2017-12-10T11:30:42.248295: step 1762, loss 0.216326, acc 0.9375, prec 0.0474905, recall 0.831799
2017-12-10T11:30:42.688544: step 1763, loss 0.321594, acc 0.875, prec 0.0474772, recall 0.831799
2017-12-10T11:30:43.136770: step 1764, loss 0.310098, acc 0.953125, prec 0.047539, recall 0.832005
2017-12-10T11:30:43.577803: step 1765, loss 0.338407, acc 0.9375, prec 0.0475657, recall 0.832108
2017-12-10T11:30:44.023006: step 1766, loss 0.642036, acc 1, prec 0.0476324, recall 0.832313
2017-12-10T11:30:44.464976: step 1767, loss 0.557111, acc 0.921875, prec 0.0476574, recall 0.832416
2017-12-10T11:30:44.906483: step 1768, loss 0.178926, acc 0.953125, prec 0.0476524, recall 0.832416
2017-12-10T11:30:45.346700: step 1769, loss 1.07927, acc 0.9375, prec 0.0476791, recall 0.832518
2017-12-10T11:30:45.801153: step 1770, loss 0.14712, acc 0.953125, prec 0.0476741, recall 0.832518
2017-12-10T11:30:46.244572: step 1771, loss 1.35924, acc 0.875, prec 0.0476624, recall 0.83201
2017-12-10T11:30:46.692709: step 1772, loss 0.126966, acc 0.9375, prec 0.047689, recall 0.832112
2017-12-10T11:30:47.141762: step 1773, loss 0.532714, acc 0.875, prec 0.0477423, recall 0.832317
2017-12-10T11:30:47.572512: step 1774, loss 0.603763, acc 0.890625, prec 0.0477639, recall 0.832419
2017-12-10T11:30:48.027200: step 1775, loss 0.345711, acc 0.890625, prec 0.0477522, recall 0.832419
2017-12-10T11:30:48.473787: step 1776, loss 0.174568, acc 0.9375, prec 0.0478121, recall 0.832623
2017-12-10T11:30:48.912938: step 1777, loss 0.257621, acc 0.921875, prec 0.0478038, recall 0.832623
2017-12-10T11:30:49.357523: step 1778, loss 0.488977, acc 0.890625, prec 0.0478253, recall 0.832725
2017-12-10T11:30:49.808689: step 1779, loss 0.568537, acc 0.859375, prec 0.0478103, recall 0.832725
2017-12-10T11:30:50.254785: step 1780, loss 0.207261, acc 0.9375, prec 0.0478701, recall 0.832928
2017-12-10T11:30:50.692865: step 1781, loss 0.476115, acc 0.890625, prec 0.0479249, recall 0.833131
2017-12-10T11:30:51.129584: step 1782, loss 0.341256, acc 0.859375, prec 0.0479098, recall 0.833131
2017-12-10T11:30:51.562932: step 1783, loss 0.26149, acc 0.90625, prec 0.0478998, recall 0.833131
2017-12-10T11:30:51.994880: step 1784, loss 0.424605, acc 0.859375, prec 0.047918, recall 0.833232
2017-12-10T11:30:52.437551: step 1785, loss 0.302242, acc 0.921875, prec 0.0479096, recall 0.833232
2017-12-10T11:30:52.888919: step 1786, loss 0.411756, acc 0.859375, prec 0.0479278, recall 0.833333
2017-12-10T11:30:53.330037: step 1787, loss 0.24993, acc 0.90625, prec 0.0479178, recall 0.833333
2017-12-10T11:30:53.762476: step 1788, loss 0.319957, acc 0.90625, prec 0.0480072, recall 0.833636
2017-12-10T11:30:54.207010: step 1789, loss 0.162979, acc 0.953125, prec 0.0480022, recall 0.833636
2017-12-10T11:30:54.649327: step 1790, loss 0.201275, acc 0.90625, prec 0.0479922, recall 0.833636
2017-12-10T11:30:55.095742: step 1791, loss 6.6404, acc 0.890625, prec 0.0479822, recall 0.833132
2017-12-10T11:30:55.540768: step 1792, loss 0.889587, acc 0.90625, prec 0.0480053, recall 0.833233
2017-12-10T11:30:55.978583: step 1793, loss 0.128045, acc 0.953125, prec 0.0480003, recall 0.833233
2017-12-10T11:30:56.423676: step 1794, loss 0.260789, acc 0.90625, prec 0.0479903, recall 0.833233
2017-12-10T11:30:56.851977: step 1795, loss 0.0887358, acc 0.953125, prec 0.0480184, recall 0.833333
2017-12-10T11:30:57.289140: step 1796, loss 0.368214, acc 0.9375, prec 0.0480779, recall 0.833534
2017-12-10T11:30:57.739711: step 1797, loss 0.392229, acc 0.828125, prec 0.0480926, recall 0.833635
2017-12-10T11:30:58.177863: step 1798, loss 0.296686, acc 0.921875, prec 0.0481174, recall 0.833735
2017-12-10T11:30:58.606598: step 1799, loss 0.429531, acc 0.890625, prec 0.0481718, recall 0.833935
2017-12-10T11:30:59.044922: step 1800, loss 0.330594, acc 0.890625, prec 0.0482263, recall 0.834135
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1800

2017-12-10T11:31:00.926547: step 1801, loss 1.12323, acc 0.90625, prec 0.0482493, recall 0.834234
2017-12-10T11:31:01.363690: step 1802, loss 0.396396, acc 0.84375, prec 0.0482325, recall 0.834234
2017-12-10T11:31:01.797461: step 1803, loss 0.36326, acc 0.875, prec 0.0482191, recall 0.834234
2017-12-10T11:31:02.243285: step 1804, loss 0.335584, acc 0.921875, prec 0.0482438, recall 0.834334
2017-12-10T11:31:02.679946: step 1805, loss 0.159844, acc 0.9375, prec 0.0482371, recall 0.834334
2017-12-10T11:31:03.116863: step 1806, loss 0.332882, acc 0.9375, prec 0.0482634, recall 0.834433
2017-12-10T11:31:03.561647: step 1807, loss 0.403571, acc 0.890625, prec 0.0482517, recall 0.834433
2017-12-10T11:31:04.004456: step 1808, loss 0.102442, acc 0.984375, prec 0.048283, recall 0.834532
2017-12-10T11:31:04.452182: step 1809, loss 0.172765, acc 0.9375, prec 0.0482763, recall 0.834532
2017-12-10T11:31:04.880877: step 1810, loss 0.43306, acc 0.90625, prec 0.0482663, recall 0.834532
2017-12-10T11:31:05.326263: step 1811, loss 0.147599, acc 0.9375, prec 0.0482926, recall 0.834632
2017-12-10T11:31:05.760133: step 1812, loss 0.135726, acc 0.9375, prec 0.0482859, recall 0.834632
2017-12-10T11:31:06.202552: step 1813, loss 0.41942, acc 0.84375, prec 0.0482692, recall 0.834632
2017-12-10T11:31:06.649614: step 1814, loss 2.15483, acc 0.875, prec 0.0482904, recall 0.834231
2017-12-10T11:31:07.094282: step 1815, loss 0.455703, acc 0.921875, prec 0.048315, recall 0.83433
2017-12-10T11:31:07.532332: step 1816, loss 0.212079, acc 0.90625, prec 0.048338, recall 0.834429
2017-12-10T11:31:07.960368: step 1817, loss 2.63207, acc 0.890625, prec 0.0483609, recall 0.83403
2017-12-10T11:31:08.404116: step 1818, loss 0.150243, acc 0.9375, prec 0.0483542, recall 0.83403
2017-12-10T11:31:08.841584: step 1819, loss 1.77951, acc 0.859375, prec 0.048405, recall 0.834228
2017-12-10T11:31:09.282761: step 1820, loss 0.412618, acc 0.859375, prec 0.0484557, recall 0.834425
2017-12-10T11:31:09.723772: step 1821, loss 0.575808, acc 0.875, prec 0.0485081, recall 0.834622
2017-12-10T11:31:10.168005: step 1822, loss 0.697971, acc 0.84375, prec 0.0485571, recall 0.834819
2017-12-10T11:31:10.609511: step 1823, loss 0.683884, acc 0.84375, prec 0.0485403, recall 0.834819
2017-12-10T11:31:11.039823: step 1824, loss 0.590084, acc 0.796875, prec 0.0485514, recall 0.834917
2017-12-10T11:31:11.484467: step 1825, loss 0.318284, acc 0.828125, prec 0.0485658, recall 0.835015
2017-12-10T11:31:11.944974: step 1826, loss 1.36814, acc 0.671875, prec 0.0485963, recall 0.83521
2017-12-10T11:31:12.387545: step 1827, loss 0.461035, acc 0.84375, prec 0.0485795, recall 0.83521
2017-12-10T11:31:12.822601: step 1828, loss 0.278224, acc 0.890625, prec 0.0486006, recall 0.835308
2017-12-10T11:31:13.249383: step 1829, loss 0.67413, acc 0.8125, prec 0.048646, recall 0.835503
2017-12-10T11:31:13.697253: step 1830, loss 0.735314, acc 0.71875, prec 0.0486159, recall 0.835503
2017-12-10T11:31:14.125129: step 1831, loss 0.553591, acc 0.78125, prec 0.0485925, recall 0.835503
2017-12-10T11:31:14.569862: step 1832, loss 0.541311, acc 0.84375, prec 0.0485758, recall 0.835503
2017-12-10T11:31:15.008565: step 1833, loss 0.457124, acc 0.859375, prec 0.0485607, recall 0.835503
2017-12-10T11:31:15.446084: step 1834, loss 0.795291, acc 0.75, prec 0.0485667, recall 0.8356
2017-12-10T11:31:15.887187: step 1835, loss 0.17649, acc 0.9375, prec 0.04856, recall 0.8356
2017-12-10T11:31:16.331162: step 1836, loss 0.26966, acc 0.921875, prec 0.0485517, recall 0.8356
2017-12-10T11:31:16.761112: step 1837, loss 0.427126, acc 0.890625, prec 0.0486054, recall 0.835794
2017-12-10T11:31:17.194814: step 1838, loss 0.462697, acc 0.859375, prec 0.0486884, recall 0.836085
2017-12-10T11:31:17.637373: step 1839, loss 0.287982, acc 0.90625, prec 0.048711, recall 0.836182
2017-12-10T11:31:18.077467: step 1840, loss 0.350231, acc 0.90625, prec 0.0487336, recall 0.836278
2017-12-10T11:31:18.517909: step 1841, loss 0.17218, acc 0.9375, prec 0.0487269, recall 0.836278
2017-12-10T11:31:18.955514: step 1842, loss 0.0619585, acc 0.984375, prec 0.0487253, recall 0.836278
2017-12-10T11:31:19.397629: step 1843, loss 0.268792, acc 0.921875, prec 0.0487169, recall 0.836278
2017-12-10T11:31:19.849962: step 1844, loss 2.43611, acc 0.9375, prec 0.0487119, recall 0.835786
2017-12-10T11:31:20.292115: step 1845, loss 0.165741, acc 0.921875, prec 0.0487035, recall 0.835786
2017-12-10T11:31:20.736991: step 1846, loss 0.182948, acc 0.9375, prec 0.0486968, recall 0.835786
2017-12-10T11:31:21.181962: step 1847, loss 0.223253, acc 0.9375, prec 0.0487228, recall 0.835882
2017-12-10T11:31:21.620171: step 1848, loss 0.159059, acc 0.90625, prec 0.0487128, recall 0.835882
2017-12-10T11:31:22.063778: step 1849, loss 0.19071, acc 0.96875, prec 0.0487746, recall 0.836075
2017-12-10T11:31:22.490899: step 1850, loss 0.17479, acc 0.96875, prec 0.0488039, recall 0.836171
2017-12-10T11:31:22.937745: step 1851, loss 0.255548, acc 0.921875, prec 0.0488281, recall 0.836268
2017-12-10T11:31:23.371266: step 1852, loss 0.211781, acc 0.9375, prec 0.0488214, recall 0.836268
2017-12-10T11:31:23.815485: step 1853, loss 1.23998, acc 0.953125, prec 0.0488816, recall 0.83646
2017-12-10T11:31:24.269636: step 1854, loss 0.254023, acc 0.875, prec 0.0488682, recall 0.83646
2017-12-10T11:31:24.718935: step 1855, loss 0.244228, acc 0.9375, prec 0.0488941, recall 0.836555
2017-12-10T11:31:25.150004: step 1856, loss 0.0882576, acc 0.953125, prec 0.048889, recall 0.836555
2017-12-10T11:31:25.587820: step 1857, loss 1.37832, acc 0.875, prec 0.0488773, recall 0.836066
2017-12-10T11:31:26.022456: step 1858, loss 0.102358, acc 0.96875, prec 0.048874, recall 0.836066
2017-12-10T11:31:26.469122: step 1859, loss 1.62929, acc 0.84375, prec 0.0488589, recall 0.835576
2017-12-10T11:31:26.910992: step 1860, loss 0.706394, acc 0.921875, prec 0.0489156, recall 0.835769
2017-12-10T11:31:27.362979: step 1861, loss 0.201461, acc 0.9375, prec 0.048974, recall 0.83596
2017-12-10T11:31:27.803036: step 1862, loss 0.610394, acc 0.859375, prec 0.0489915, recall 0.836056
2017-12-10T11:31:28.245997: step 1863, loss 0.532986, acc 0.828125, prec 0.0490055, recall 0.836152
2017-12-10T11:31:28.684880: step 1864, loss 0.428889, acc 0.84375, prec 0.0490213, recall 0.836247
2017-12-10T11:31:29.125825: step 1865, loss 0.194397, acc 0.9375, prec 0.0490146, recall 0.836247
2017-12-10T11:31:29.566348: step 1866, loss 0.232355, acc 0.9375, prec 0.0490079, recall 0.836247
2017-12-10T11:31:30.009352: step 1867, loss 0.330929, acc 0.84375, prec 0.0489912, recall 0.836247
2017-12-10T11:31:30.458258: step 1868, loss 0.751521, acc 0.78125, prec 0.0490002, recall 0.836342
2017-12-10T11:31:30.919631: step 1869, loss 0.637617, acc 0.828125, prec 0.0490143, recall 0.836438
2017-12-10T11:31:31.366079: step 1870, loss 0.447663, acc 0.859375, prec 0.0489992, recall 0.836438
2017-12-10T11:31:31.805617: step 1871, loss 0.227431, acc 0.921875, prec 0.0490233, recall 0.836533
2017-12-10T11:31:32.243614: step 1872, loss 0.600706, acc 0.828125, prec 0.0490373, recall 0.836628
2017-12-10T11:31:32.690002: step 1873, loss 0.354306, acc 0.84375, prec 0.0490206, recall 0.836628
2017-12-10T11:31:33.125015: step 1874, loss 0.356264, acc 0.90625, prec 0.0490754, recall 0.836818
2017-12-10T11:31:33.579472: step 1875, loss 0.41763, acc 0.890625, prec 0.0491284, recall 0.837007
2017-12-10T11:31:34.023766: step 1876, loss 3.06465, acc 0.859375, prec 0.0491474, recall 0.836616
2017-12-10T11:31:34.453974: step 1877, loss 0.121897, acc 0.953125, prec 0.0492395, recall 0.8369
2017-12-10T11:31:34.907696: step 1878, loss 0.29693, acc 0.890625, prec 0.0492277, recall 0.8369
2017-12-10T11:31:35.347069: step 1879, loss 0.194804, acc 0.953125, prec 0.0492227, recall 0.8369
2017-12-10T11:31:35.789355: step 1880, loss 0.373893, acc 0.90625, prec 0.0493097, recall 0.837182
2017-12-10T11:31:36.248796: step 1881, loss 0.11253, acc 0.9375, prec 0.0493353, recall 0.837276
2017-12-10T11:31:36.678392: step 1882, loss 0.210029, acc 0.921875, prec 0.0493592, recall 0.83737
2017-12-10T11:31:37.118460: step 1883, loss 0.262065, acc 0.921875, prec 0.0493508, recall 0.83737
2017-12-10T11:31:37.565865: step 1884, loss 0.552802, acc 0.890625, prec 0.0493391, recall 0.83737
2017-12-10T11:31:38.009622: step 1885, loss 0.428142, acc 0.890625, prec 0.0494242, recall 0.837651
2017-12-10T11:31:38.449266: step 1886, loss 0.218199, acc 0.90625, prec 0.0494142, recall 0.837651
2017-12-10T11:31:38.884845: step 1887, loss 0.559635, acc 0.875, prec 0.049433, recall 0.837745
2017-12-10T11:31:39.318395: step 1888, loss 0.238809, acc 0.9375, prec 0.0494586, recall 0.837838
2017-12-10T11:31:39.753973: step 1889, loss 0.327498, acc 0.9375, prec 0.0494841, recall 0.837931
2017-12-10T11:31:40.196162: step 1890, loss 0.130685, acc 0.953125, prec 0.0494791, recall 0.837931
2017-12-10T11:31:40.641261: step 1891, loss 0.182429, acc 0.90625, prec 0.0495013, recall 0.838024
2017-12-10T11:31:41.085307: step 1892, loss 0.136087, acc 0.9375, prec 0.0495268, recall 0.838117
2017-12-10T11:31:41.523842: step 1893, loss 0.236531, acc 0.96875, prec 0.0495557, recall 0.83821
2017-12-10T11:31:41.969667: step 1894, loss 0.373197, acc 0.875, prec 0.0495744, recall 0.838303
2017-12-10T11:31:42.406623: step 1895, loss 0.423753, acc 0.921875, prec 0.0496627, recall 0.83858
2017-12-10T11:31:42.841143: step 1896, loss 0.130524, acc 0.96875, prec 0.0497238, recall 0.838765
2017-12-10T11:31:43.276351: step 1897, loss 0.245562, acc 0.90625, prec 0.0497136, recall 0.838765
2017-12-10T11:31:43.744904: step 1898, loss 1.45033, acc 0.921875, prec 0.0497391, recall 0.838378
2017-12-10T11:31:44.187956: step 1899, loss 0.177698, acc 0.9375, prec 0.0497324, recall 0.838378
2017-12-10T11:31:44.623914: step 1900, loss 0.206375, acc 0.921875, prec 0.0497239, recall 0.838378
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-1900

2017-12-10T11:31:46.471912: step 1901, loss 2.85386, acc 0.875, prec 0.0497443, recall 0.837992
2017-12-10T11:31:46.920115: step 1902, loss 0.128708, acc 0.9375, prec 0.0497376, recall 0.837992
2017-12-10T11:31:47.349924: step 1903, loss 0.151184, acc 0.96875, prec 0.0497342, recall 0.837992
2017-12-10T11:31:47.785861: step 1904, loss 0.152874, acc 0.984375, prec 0.0497325, recall 0.837992
2017-12-10T11:31:48.221968: step 1905, loss 1.16856, acc 0.90625, prec 0.0497868, recall 0.838177
2017-12-10T11:31:48.668440: step 1906, loss 0.338963, acc 0.875, prec 0.0497733, recall 0.838177
2017-12-10T11:31:49.106075: step 1907, loss 0.767045, acc 0.859375, prec 0.0498224, recall 0.838361
2017-12-10T11:31:49.542862: step 1908, loss 0.548193, acc 0.828125, prec 0.0498039, recall 0.838361
2017-12-10T11:31:49.985649: step 1909, loss 0.370301, acc 0.90625, prec 0.0498259, recall 0.838453
2017-12-10T11:31:50.410822: step 1910, loss 0.277569, acc 0.890625, prec 0.0498462, recall 0.838545
2017-12-10T11:31:50.856814: step 1911, loss 0.788306, acc 0.84375, prec 0.0498615, recall 0.838636
2017-12-10T11:31:51.299828: step 1912, loss 0.482509, acc 0.859375, prec 0.0499105, recall 0.83882
2017-12-10T11:31:51.748480: step 1913, loss 0.560216, acc 0.828125, prec 0.049892, recall 0.83882
2017-12-10T11:31:52.209645: step 1914, loss 0.493348, acc 0.84375, prec 0.0498751, recall 0.83882
2017-12-10T11:31:52.642597: step 1915, loss 0.754175, acc 0.78125, prec 0.0499157, recall 0.839002
2017-12-10T11:31:53.068746: step 1916, loss 0.330979, acc 0.875, prec 0.0499022, recall 0.839002
2017-12-10T11:31:53.502620: step 1917, loss 0.424298, acc 0.921875, prec 0.0499258, recall 0.839094
2017-12-10T11:31:53.948477: step 1918, loss 0.581898, acc 0.8125, prec 0.0499377, recall 0.839185
2017-12-10T11:31:54.391342: step 1919, loss 0.462897, acc 0.875, prec 0.0499882, recall 0.839366
2017-12-10T11:31:54.826761: step 1920, loss 0.310834, acc 0.890625, prec 0.0499764, recall 0.839366
2017-12-10T11:31:55.265044: step 1921, loss 0.236764, acc 0.9375, prec 0.0499697, recall 0.839366
2017-12-10T11:31:55.702144: step 1922, loss 0.228067, acc 0.875, prec 0.0499882, recall 0.839457
2017-12-10T11:31:56.158563: step 1923, loss 0.293864, acc 0.921875, prec 0.0500118, recall 0.839548
2017-12-10T11:31:56.601263: step 1924, loss 0.0870415, acc 0.96875, prec 0.0500084, recall 0.839548
2017-12-10T11:31:57.037718: step 1925, loss 0.312142, acc 0.9375, prec 0.0500656, recall 0.839729
2017-12-10T11:31:57.469085: step 1926, loss 0.230746, acc 0.921875, prec 0.0500891, recall 0.839819
2017-12-10T11:31:57.897106: step 1927, loss 1.57649, acc 0.953125, prec 0.0501177, recall 0.839437
2017-12-10T11:31:58.334951: step 1928, loss 0.285804, acc 0.90625, prec 0.0501396, recall 0.839527
2017-12-10T11:31:58.768454: step 1929, loss 0.290745, acc 0.90625, prec 0.0501614, recall 0.839617
2017-12-10T11:31:59.218702: step 1930, loss 1.56688, acc 0.953125, prec 0.050158, recall 0.839145
2017-12-10T11:31:59.667094: step 1931, loss 0.124589, acc 0.953125, prec 0.0501849, recall 0.839236
2017-12-10T11:32:00.112917: step 1932, loss 0.106723, acc 0.953125, prec 0.0501798, recall 0.839236
2017-12-10T11:32:00.559424: step 1933, loss 0.347226, acc 0.921875, prec 0.0501714, recall 0.839236
2017-12-10T11:32:00.997011: step 1934, loss 0.474083, acc 0.890625, prec 0.0501596, recall 0.839236
2017-12-10T11:32:01.443074: step 1935, loss 0.396999, acc 0.875, prec 0.0502099, recall 0.839416
2017-12-10T11:32:01.887571: step 1936, loss 0.262297, acc 0.9375, prec 0.0502669, recall 0.839596
2017-12-10T11:32:02.338170: step 1937, loss 0.209352, acc 0.921875, prec 0.0502585, recall 0.839596
2017-12-10T11:32:02.791839: step 1938, loss 0.457386, acc 0.921875, prec 0.050282, recall 0.839686
2017-12-10T11:32:03.219229: step 1939, loss 0.13439, acc 0.953125, prec 0.0502769, recall 0.839686
2017-12-10T11:32:03.648618: step 1940, loss 0.181151, acc 0.921875, prec 0.0502685, recall 0.839686
2017-12-10T11:32:04.088753: step 1941, loss 1.62609, acc 0.84375, prec 0.0503153, recall 0.839866
2017-12-10T11:32:04.538459: step 1942, loss 0.315299, acc 0.921875, prec 0.0503069, recall 0.839866
2017-12-10T11:32:04.975998: step 1943, loss 0.254019, acc 0.921875, prec 0.0503303, recall 0.839955
2017-12-10T11:32:05.427841: step 1944, loss 0.18375, acc 0.90625, prec 0.0503202, recall 0.839955
2017-12-10T11:32:05.864907: step 1945, loss 0.455328, acc 0.875, prec 0.0503703, recall 0.840134
2017-12-10T11:32:06.315889: step 1946, loss 0.328729, acc 0.859375, prec 0.0503551, recall 0.840134
2017-12-10T11:32:06.749217: step 1947, loss 1.40919, acc 0.875, prec 0.0504053, recall 0.840313
2017-12-10T11:32:07.186449: step 1948, loss 0.136593, acc 0.9375, prec 0.0503985, recall 0.840313
2017-12-10T11:32:07.637157: step 1949, loss 0.492194, acc 0.890625, prec 0.0504185, recall 0.840402
2017-12-10T11:32:08.083237: step 1950, loss 0.351767, acc 0.859375, prec 0.0504033, recall 0.840402
2017-12-10T11:32:08.517661: step 1951, loss 0.515872, acc 0.84375, prec 0.0503864, recall 0.840402
2017-12-10T11:32:08.953758: step 1952, loss 0.308394, acc 0.875, prec 0.0503729, recall 0.840402
2017-12-10T11:32:09.381286: step 1953, loss 0.385872, acc 0.84375, prec 0.0503561, recall 0.840402
2017-12-10T11:32:09.815646: step 1954, loss 0.248935, acc 0.9375, prec 0.0504129, recall 0.84058
2017-12-10T11:32:10.256477: step 1955, loss 0.565371, acc 0.84375, prec 0.050396, recall 0.84058
2017-12-10T11:32:10.700550: step 1956, loss 0.366034, acc 0.875, prec 0.0503825, recall 0.84058
2017-12-10T11:32:11.135048: step 1957, loss 0.251881, acc 0.90625, prec 0.0503724, recall 0.84058
2017-12-10T11:32:11.584047: step 1958, loss 0.350171, acc 0.9375, prec 0.0504292, recall 0.840757
2017-12-10T11:32:12.028424: step 1959, loss 0.0928042, acc 0.96875, prec 0.0504258, recall 0.840757
2017-12-10T11:32:12.471369: step 1960, loss 0.175442, acc 0.96875, prec 0.0504541, recall 0.840846
2017-12-10T11:32:12.919132: step 1961, loss 0.66316, acc 0.84375, prec 0.050469, recall 0.840934
2017-12-10T11:32:13.369051: step 1962, loss 0.327696, acc 0.890625, prec 0.0504889, recall 0.841023
2017-12-10T11:32:13.804639: step 1963, loss 0.064442, acc 0.984375, prec 0.0504872, recall 0.841023
2017-12-10T11:32:14.253828: step 1964, loss 0.278526, acc 0.921875, prec 0.0505104, recall 0.841111
2017-12-10T11:32:14.694490: step 1965, loss 0.277653, acc 0.90625, prec 0.050532, recall 0.841199
2017-12-10T11:32:15.138686: step 1966, loss 0.526289, acc 0.90625, prec 0.0505852, recall 0.841375
2017-12-10T11:32:15.579274: step 1967, loss 0.0997059, acc 0.953125, prec 0.0505802, recall 0.841375
2017-12-10T11:32:16.016183: step 1968, loss 0.231849, acc 0.90625, prec 0.05057, recall 0.841375
2017-12-10T11:32:16.451086: step 1969, loss 3.90237, acc 0.921875, prec 0.0505949, recall 0.840997
2017-12-10T11:32:16.898370: step 1970, loss 0.050458, acc 0.953125, prec 0.0505899, recall 0.840997
2017-12-10T11:32:17.340494: step 1971, loss 0.111173, acc 0.96875, prec 0.0505865, recall 0.840997
2017-12-10T11:32:17.783025: step 1972, loss 1.68144, acc 0.921875, prec 0.0506114, recall 0.84062
2017-12-10T11:32:18.234780: step 1973, loss 0.219447, acc 0.90625, prec 0.0506013, recall 0.84062
2017-12-10T11:32:18.672979: step 1974, loss 0.244671, acc 0.890625, prec 0.0505895, recall 0.84062
2017-12-10T11:32:19.119088: step 1975, loss 0.202483, acc 0.953125, prec 0.0505844, recall 0.84062
2017-12-10T11:32:19.556177: step 1976, loss 0.168576, acc 0.96875, prec 0.0506127, recall 0.840708
2017-12-10T11:32:20.001664: step 1977, loss 0.174803, acc 0.9375, prec 0.0506375, recall 0.840796
2017-12-10T11:32:20.454214: step 1978, loss 0.782972, acc 0.875, prec 0.0506873, recall 0.840972
2017-12-10T11:32:20.903605: step 1979, loss 0.335818, acc 0.875, prec 0.0507054, recall 0.84106
2017-12-10T11:32:21.336298: step 1980, loss 0.283225, acc 0.859375, prec 0.0506902, recall 0.84106
2017-12-10T11:32:21.793330: step 1981, loss 0.460239, acc 0.765625, prec 0.0506649, recall 0.84106
2017-12-10T11:32:22.224004: step 1982, loss 0.63474, acc 0.859375, prec 0.0506497, recall 0.84106
2017-12-10T11:32:22.665390: step 1983, loss 0.491885, acc 0.859375, prec 0.0506977, recall 0.841235
2017-12-10T11:32:23.111868: step 1984, loss 0.357736, acc 0.890625, prec 0.0506859, recall 0.841235
2017-12-10T11:32:23.558576: step 1985, loss 0.481095, acc 0.84375, prec 0.0507006, recall 0.841322
2017-12-10T11:32:24.026949: step 1986, loss 0.385459, acc 0.890625, prec 0.0506888, recall 0.841322
2017-12-10T11:32:24.473347: step 1987, loss 0.268316, acc 0.921875, prec 0.0506804, recall 0.841322
2017-12-10T11:32:24.868334: step 1988, loss 0.273232, acc 0.882353, prec 0.0507018, recall 0.84141
2017-12-10T11:32:25.325242: step 1989, loss 0.292552, acc 0.875, prec 0.0506883, recall 0.84141
2017-12-10T11:32:25.768496: step 1990, loss 0.632425, acc 0.875, prec 0.0507693, recall 0.841671
2017-12-10T11:32:26.211741: step 1991, loss 0.230567, acc 0.921875, prec 0.0507924, recall 0.841758
2017-12-10T11:32:26.652755: step 1992, loss 0.382108, acc 0.953125, prec 0.0508503, recall 0.841932
2017-12-10T11:32:27.108647: step 1993, loss 0.173257, acc 0.921875, prec 0.0508418, recall 0.841932
2017-12-10T11:32:27.556778: step 1994, loss 0.273764, acc 0.859375, prec 0.0508267, recall 0.841932
2017-12-10T11:32:28.001851: step 1995, loss 0.496568, acc 0.90625, prec 0.050848, recall 0.842019
2017-12-10T11:32:28.438221: step 1996, loss 0.409521, acc 0.9375, prec 0.0509042, recall 0.842192
2017-12-10T11:32:28.881685: step 1997, loss 0.379649, acc 0.921875, prec 0.0508957, recall 0.842192
2017-12-10T11:32:29.312424: step 1998, loss 0.215889, acc 0.953125, prec 0.0509221, recall 0.842278
2017-12-10T11:32:29.746586: step 1999, loss 0.200358, acc 0.90625, prec 0.0509434, recall 0.842365
2017-12-10T11:32:30.183653: step 2000, loss 0.0606365, acc 0.96875, prec 0.05094, recall 0.842365
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2000

2017-12-10T11:32:32.182734: step 2001, loss 0.366616, acc 0.890625, prec 0.050991, recall 0.842537
2017-12-10T11:32:32.634819: step 2002, loss 0.125706, acc 0.984375, prec 0.0509893, recall 0.842537
2017-12-10T11:32:33.075066: step 2003, loss 0.0894965, acc 0.984375, prec 0.0510505, recall 0.842709
2017-12-10T11:32:33.538365: step 2004, loss 0.286032, acc 0.9375, prec 0.0510437, recall 0.842709
2017-12-10T11:32:33.983330: step 2005, loss 0.0680181, acc 0.953125, prec 0.0510386, recall 0.842709
2017-12-10T11:32:34.429530: step 2006, loss 0.520493, acc 0.96875, prec 0.0511294, recall 0.842966
2017-12-10T11:32:34.871532: step 2007, loss 0.168828, acc 0.9375, prec 0.0511226, recall 0.842966
2017-12-10T11:32:35.308459: step 2008, loss 0.181615, acc 0.96875, prec 0.051182, recall 0.843137
2017-12-10T11:32:35.753468: step 2009, loss 0.375251, acc 0.90625, prec 0.0512032, recall 0.843223
2017-12-10T11:32:36.189567: step 2010, loss 0.120667, acc 0.96875, prec 0.0512312, recall 0.843308
2017-12-10T11:32:36.628194: step 2011, loss 0.0521225, acc 0.984375, prec 0.0512295, recall 0.843308
2017-12-10T11:32:37.068030: step 2012, loss 0.688296, acc 0.9375, prec 0.0512541, recall 0.843393
2017-12-10T11:32:37.511254: step 2013, loss 0.265649, acc 0.953125, prec 0.051249, recall 0.843393
2017-12-10T11:32:37.949754: step 2014, loss 0.0270844, acc 0.984375, prec 0.0512787, recall 0.843478
2017-12-10T11:32:38.399262: step 2015, loss 0.0680038, acc 0.953125, prec 0.0512736, recall 0.843478
2017-12-10T11:32:38.850433: step 2016, loss 0.201481, acc 0.96875, prec 0.0513015, recall 0.843563
2017-12-10T11:32:39.282549: step 2017, loss 0.105977, acc 0.953125, prec 0.0512965, recall 0.843563
2017-12-10T11:32:39.720891: step 2018, loss 0.120971, acc 0.953125, prec 0.0512914, recall 0.843563
2017-12-10T11:32:40.150188: step 2019, loss 0.101235, acc 0.984375, prec 0.051321, recall 0.843648
2017-12-10T11:32:40.598200: step 2020, loss 0.255475, acc 0.96875, prec 0.0513176, recall 0.843648
2017-12-10T11:32:41.041545: step 2021, loss 0.263218, acc 0.921875, prec 0.0513091, recall 0.843648
2017-12-10T11:32:41.491171: step 2022, loss 0.151123, acc 0.9375, prec 0.0513024, recall 0.843648
2017-12-10T11:32:41.940964: step 2023, loss 0.142198, acc 0.984375, prec 0.051332, recall 0.843733
2017-12-10T11:32:42.385900: step 2024, loss 0.0874745, acc 0.953125, prec 0.0513269, recall 0.843733
2017-12-10T11:32:42.847991: step 2025, loss 0.0549317, acc 0.96875, prec 0.0513548, recall 0.843818
2017-12-10T11:32:43.292624: step 2026, loss 0.171333, acc 0.9375, prec 0.0513794, recall 0.843902
2017-12-10T11:32:43.736756: step 2027, loss 1.039, acc 0.96875, prec 0.0515012, recall 0.84424
2017-12-10T11:32:44.203435: step 2028, loss 0.0720399, acc 0.984375, prec 0.0514995, recall 0.84424
2017-12-10T11:32:44.633796: step 2029, loss 0.126411, acc 0.953125, prec 0.0514944, recall 0.84424
2017-12-10T11:32:45.063291: step 2030, loss 0.174759, acc 0.9375, prec 0.0514876, recall 0.84424
2017-12-10T11:32:45.502614: step 2031, loss 0.0805154, acc 0.96875, prec 0.0515155, recall 0.844324
2017-12-10T11:32:45.945329: step 2032, loss 0.154261, acc 0.9375, prec 0.0515087, recall 0.844324
2017-12-10T11:32:46.392823: step 2033, loss 0.253635, acc 0.96875, prec 0.0515365, recall 0.844408
2017-12-10T11:32:46.847533: step 2034, loss 8.52128, acc 0.953125, prec 0.0515644, recall 0.844037
2017-12-10T11:32:47.285822: step 2035, loss 0.144001, acc 0.96875, prec 0.0515923, recall 0.844121
2017-12-10T11:32:47.728151: step 2036, loss 0.104256, acc 0.953125, prec 0.0516497, recall 0.844289
2017-12-10T11:32:48.182308: step 2037, loss 0.096222, acc 0.96875, prec 0.0516463, recall 0.844289
2017-12-10T11:32:48.619378: step 2038, loss 0.18424, acc 0.9375, prec 0.0516395, recall 0.844289
2017-12-10T11:32:49.057483: step 2039, loss 0.539013, acc 0.859375, prec 0.0516242, recall 0.844289
2017-12-10T11:32:49.494810: step 2040, loss 0.375503, acc 0.9375, prec 0.0516174, recall 0.844289
2017-12-10T11:32:49.930070: step 2041, loss 0.277886, acc 0.859375, prec 0.0516021, recall 0.844289
2017-12-10T11:32:50.370096: step 2042, loss 0.194159, acc 0.953125, prec 0.051597, recall 0.844289
2017-12-10T11:32:50.806056: step 2043, loss 0.301105, acc 0.90625, prec 0.051618, recall 0.844373
2017-12-10T11:32:51.250634: step 2044, loss 0.439426, acc 0.890625, prec 0.0516061, recall 0.844373
2017-12-10T11:32:51.694203: step 2045, loss 0.47834, acc 0.890625, prec 0.0515942, recall 0.844373
2017-12-10T11:32:52.141253: step 2046, loss 0.295412, acc 0.875, prec 0.0515806, recall 0.844373
2017-12-10T11:32:52.587235: step 2047, loss 0.274975, acc 0.9375, prec 0.0515739, recall 0.844373
2017-12-10T11:32:53.027901: step 2048, loss 0.31875, acc 0.921875, prec 0.0516278, recall 0.84454
2017-12-10T11:32:53.462860: step 2049, loss 0.279281, acc 0.921875, prec 0.0516816, recall 0.844707
2017-12-10T11:32:53.900394: step 2050, loss 0.284472, acc 0.90625, prec 0.0516714, recall 0.844707
2017-12-10T11:32:54.333777: step 2051, loss 0.516161, acc 0.875, prec 0.051689, recall 0.844791
2017-12-10T11:32:54.768357: step 2052, loss 0.476431, acc 0.84375, prec 0.0517032, recall 0.844874
2017-12-10T11:32:55.199647: step 2053, loss 0.321671, acc 0.9375, prec 0.0516964, recall 0.844874
2017-12-10T11:32:55.629870: step 2054, loss 0.431072, acc 0.90625, prec 0.0517173, recall 0.844957
2017-12-10T11:32:56.068536: step 2055, loss 0.386794, acc 0.90625, prec 0.0517383, recall 0.84504
2017-12-10T11:32:56.508477: step 2056, loss 0.272846, acc 0.90625, prec 0.0517592, recall 0.845123
2017-12-10T11:32:56.937704: step 2057, loss 0.473992, acc 0.84375, prec 0.0517422, recall 0.845123
2017-12-10T11:32:57.377603: step 2058, loss 0.144844, acc 0.953125, prec 0.0517371, recall 0.845123
2017-12-10T11:32:57.807253: step 2059, loss 0.24357, acc 0.953125, prec 0.0517321, recall 0.845123
2017-12-10T11:32:58.260855: step 2060, loss 1.34025, acc 0.953125, prec 0.0517287, recall 0.844671
2017-12-10T11:32:58.699636: step 2061, loss 0.213764, acc 0.921875, prec 0.0517202, recall 0.844671
2017-12-10T11:32:59.136988: step 2062, loss 0.0744623, acc 0.96875, prec 0.0517168, recall 0.844671
2017-12-10T11:32:59.571282: step 2063, loss 0.287916, acc 0.9375, prec 0.05171, recall 0.844671
2017-12-10T11:33:00.004842: step 2064, loss 0.49566, acc 0.9375, prec 0.0517343, recall 0.844754
2017-12-10T11:33:00.447079: step 2065, loss 0.162281, acc 0.921875, prec 0.051788, recall 0.84492
2017-12-10T11:33:00.908995: step 2066, loss 0.11991, acc 0.984375, prec 0.0518485, recall 0.845085
2017-12-10T11:33:01.350872: step 2067, loss 0.0721005, acc 0.984375, prec 0.0518468, recall 0.845085
2017-12-10T11:33:01.803359: step 2068, loss 0.780586, acc 0.9375, prec 0.051871, recall 0.845168
2017-12-10T11:33:02.246707: step 2069, loss 0.436895, acc 0.90625, prec 0.0518919, recall 0.845251
2017-12-10T11:33:02.674229: step 2070, loss 1.85508, acc 0.9375, prec 0.0519489, recall 0.844965
2017-12-10T11:33:03.119049: step 2071, loss 0.217823, acc 0.953125, prec 0.0519438, recall 0.844965
2017-12-10T11:33:03.574990: step 2072, loss 0.0904064, acc 0.953125, prec 0.0519697, recall 0.845048
2017-12-10T11:33:04.021556: step 2073, loss 0.0780118, acc 0.96875, prec 0.0519663, recall 0.845048
2017-12-10T11:33:04.459717: step 2074, loss 0.304104, acc 0.9375, prec 0.0520216, recall 0.845213
2017-12-10T11:33:04.883570: step 2075, loss 0.0561234, acc 0.96875, prec 0.0520182, recall 0.845213
2017-12-10T11:33:05.331263: step 2076, loss 0.522234, acc 0.96875, prec 0.0521079, recall 0.845459
2017-12-10T11:33:05.772456: step 2077, loss 0.216262, acc 0.953125, prec 0.0521648, recall 0.845623
2017-12-10T11:33:06.205652: step 2078, loss 0.0814659, acc 0.953125, prec 0.0521907, recall 0.845705
2017-12-10T11:33:06.643885: step 2079, loss 0.284445, acc 0.859375, prec 0.0521753, recall 0.845705
2017-12-10T11:33:07.092261: step 2080, loss 0.159603, acc 0.9375, prec 0.0521995, recall 0.845787
2017-12-10T11:33:07.541859: step 2081, loss 2.41227, acc 0.875, prec 0.0522495, recall 0.845503
2017-12-10T11:33:07.985807: step 2082, loss 3.31185, acc 0.921875, prec 0.0522737, recall 0.845137
2017-12-10T11:33:08.425373: step 2083, loss 0.345259, acc 0.890625, prec 0.0522617, recall 0.845137
2017-12-10T11:33:08.872611: step 2084, loss 0.4188, acc 0.890625, prec 0.0522498, recall 0.845137
2017-12-10T11:33:09.325737: step 2085, loss 0.636108, acc 0.796875, prec 0.0522585, recall 0.845219
2017-12-10T11:33:09.758814: step 2086, loss 0.500943, acc 0.859375, prec 0.0523051, recall 0.845383
2017-12-10T11:33:10.193415: step 2087, loss 0.380769, acc 0.875, prec 0.0522914, recall 0.845383
2017-12-10T11:33:10.633128: step 2088, loss 0.567313, acc 0.8125, prec 0.0523637, recall 0.845627
2017-12-10T11:33:11.083235: step 2089, loss 0.607392, acc 0.796875, prec 0.0523415, recall 0.845627
2017-12-10T11:33:11.517349: step 2090, loss 0.807706, acc 0.75, prec 0.0523142, recall 0.845627
2017-12-10T11:33:11.952407: step 2091, loss 0.808897, acc 0.78125, prec 0.0523212, recall 0.845708
2017-12-10T11:33:12.385692: step 2092, loss 0.596819, acc 0.71875, prec 0.0522906, recall 0.845708
2017-12-10T11:33:12.823066: step 2093, loss 0.681384, acc 0.78125, prec 0.0522667, recall 0.845708
2017-12-10T11:33:13.257188: step 2094, loss 0.586365, acc 0.859375, prec 0.0522823, recall 0.845789
2017-12-10T11:33:13.701964: step 2095, loss 0.784492, acc 0.734375, prec 0.052315, recall 0.845952
2017-12-10T11:33:14.159014: step 2096, loss 0.800881, acc 0.71875, prec 0.052346, recall 0.846113
2017-12-10T11:33:14.587946: step 2097, loss 0.491496, acc 0.875, prec 0.0523939, recall 0.846275
2017-12-10T11:33:15.026219: step 2098, loss 0.412117, acc 0.875, prec 0.0523803, recall 0.846275
2017-12-10T11:33:15.471915: step 2099, loss 0.28171, acc 0.90625, prec 0.0524317, recall 0.846436
2017-12-10T11:33:15.917940: step 2100, loss 0.61579, acc 0.84375, prec 0.0524454, recall 0.846516
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2100

2017-12-10T11:33:17.861116: step 2101, loss 0.590344, acc 0.875, prec 0.0524933, recall 0.846677
2017-12-10T11:33:18.317286: step 2102, loss 0.401783, acc 0.859375, prec 0.0524779, recall 0.846677
2017-12-10T11:33:18.756918: step 2103, loss 0.219953, acc 0.890625, prec 0.052466, recall 0.846677
2017-12-10T11:33:19.195213: step 2104, loss 0.409368, acc 0.875, prec 0.0525446, recall 0.846917
2017-12-10T11:33:19.642748: step 2105, loss 0.335631, acc 0.890625, prec 0.0525327, recall 0.846917
2017-12-10T11:33:20.073121: step 2106, loss 0.446918, acc 0.9375, prec 0.0525565, recall 0.846997
2017-12-10T11:33:20.519571: step 2107, loss 0.093062, acc 0.96875, prec 0.0525531, recall 0.846997
2017-12-10T11:33:20.948252: step 2108, loss 0.31236, acc 0.90625, prec 0.0525736, recall 0.847077
2017-12-10T11:33:21.384235: step 2109, loss 0.407919, acc 0.953125, prec 0.0526299, recall 0.847237
2017-12-10T11:33:21.824358: step 2110, loss 0.157465, acc 0.953125, prec 0.0527168, recall 0.847475
2017-12-10T11:33:22.253363: step 2111, loss 0.0329959, acc 0.984375, prec 0.0527458, recall 0.847555
2017-12-10T11:33:22.691754: step 2112, loss 0.231459, acc 0.9375, prec 0.0527696, recall 0.847634
2017-12-10T11:33:23.125359: step 2113, loss 0.104494, acc 0.953125, prec 0.0527645, recall 0.847634
2017-12-10T11:33:23.555404: step 2114, loss 0.94255, acc 0.96875, prec 0.0527917, recall 0.847713
2017-12-10T11:33:23.990452: step 2115, loss 0.0573952, acc 0.984375, prec 0.05279, recall 0.847713
2017-12-10T11:33:24.430656: step 2116, loss 0.117677, acc 0.9375, prec 0.0528138, recall 0.847792
2017-12-10T11:33:24.873703: step 2117, loss 0.286257, acc 0.9375, prec 0.0528376, recall 0.847871
2017-12-10T11:33:25.305738: step 2118, loss 0.173424, acc 0.953125, prec 0.0528325, recall 0.847871
2017-12-10T11:33:25.741480: step 2119, loss 0.144516, acc 0.953125, prec 0.052858, recall 0.84795
2017-12-10T11:33:26.184785: step 2120, loss 0.0796828, acc 0.984375, prec 0.0528563, recall 0.84795
2017-12-10T11:33:26.649245: step 2121, loss 1.58438, acc 0.9375, prec 0.0528512, recall 0.84751
2017-12-10T11:33:27.104067: step 2122, loss 0.182078, acc 0.96875, prec 0.052909, recall 0.847668
2017-12-10T11:33:27.549892: step 2123, loss 0.29907, acc 0.96875, prec 0.0529362, recall 0.847747
2017-12-10T11:33:27.992391: step 2124, loss 0.269917, acc 0.984375, prec 0.0529651, recall 0.847826
2017-12-10T11:33:28.442572: step 2125, loss 0.128228, acc 0.96875, prec 0.0529617, recall 0.847826
2017-12-10T11:33:28.888337: step 2126, loss 0.0245792, acc 1, prec 0.0529617, recall 0.847826
2017-12-10T11:33:29.330405: step 2127, loss 0.200496, acc 1, prec 0.0529923, recall 0.847905
2017-12-10T11:33:29.775446: step 2128, loss 3.69581, acc 0.90625, prec 0.053045, recall 0.847624
2017-12-10T11:33:30.208885: step 2129, loss 0.264881, acc 0.953125, prec 0.0530705, recall 0.847703
2017-12-10T11:33:30.644513: step 2130, loss 0.51999, acc 0.796875, prec 0.0530482, recall 0.847703
2017-12-10T11:33:31.090302: step 2131, loss 0.222441, acc 0.890625, prec 0.0530362, recall 0.847703
2017-12-10T11:33:31.527279: step 2132, loss 0.301862, acc 0.890625, prec 0.0530853, recall 0.84786
2017-12-10T11:33:31.967053: step 2133, loss 0.190628, acc 0.9375, prec 0.0531091, recall 0.847938
2017-12-10T11:33:32.396964: step 2134, loss 0.518187, acc 0.8125, prec 0.0530885, recall 0.847938
2017-12-10T11:33:32.822740: step 2135, loss 0.481837, acc 0.8125, prec 0.0530679, recall 0.847938
2017-12-10T11:33:33.271277: step 2136, loss 0.417461, acc 0.875, prec 0.0530848, recall 0.848017
2017-12-10T11:33:33.706194: step 2137, loss 0.541738, acc 0.8125, prec 0.0530948, recall 0.848095
2017-12-10T11:33:34.145046: step 2138, loss 0.315117, acc 0.890625, prec 0.0531133, recall 0.848173
2017-12-10T11:33:34.582862: step 2139, loss 0.290449, acc 0.90625, prec 0.0531336, recall 0.848251
2017-12-10T11:33:35.028825: step 2140, loss 0.536942, acc 0.78125, prec 0.0531706, recall 0.848407
2017-12-10T11:33:35.469747: step 2141, loss 0.304284, acc 0.90625, prec 0.0531908, recall 0.848485
2017-12-10T11:33:35.905604: step 2142, loss 0.433811, acc 0.90625, prec 0.0532415, recall 0.84864
2017-12-10T11:33:36.356624: step 2143, loss 0.315532, acc 0.90625, prec 0.0532921, recall 0.848795
2017-12-10T11:33:36.793421: step 2144, loss 0.22004, acc 0.921875, prec 0.0532836, recall 0.848795
2017-12-10T11:33:37.230791: step 2145, loss 0.316897, acc 0.90625, prec 0.0532733, recall 0.848795
2017-12-10T11:33:37.664624: step 2146, loss 0.181307, acc 0.9375, prec 0.0532664, recall 0.848795
2017-12-10T11:33:38.108238: step 2147, loss 0.250606, acc 0.90625, prec 0.0534084, recall 0.849182
2017-12-10T11:33:38.548599: step 2148, loss 0.46997, acc 0.84375, prec 0.0533912, recall 0.849182
2017-12-10T11:33:38.988097: step 2149, loss 0.202361, acc 0.9375, prec 0.0533843, recall 0.849182
2017-12-10T11:33:39.441221: step 2150, loss 0.0858428, acc 0.96875, prec 0.0533809, recall 0.849182
2017-12-10T11:33:39.888879: step 2151, loss 0.104196, acc 0.953125, prec 0.0534366, recall 0.849336
2017-12-10T11:33:40.326285: step 2152, loss 0.210784, acc 0.96875, prec 0.0534636, recall 0.849413
2017-12-10T11:33:40.760439: step 2153, loss 0.712906, acc 0.90625, prec 0.0534837, recall 0.84949
2017-12-10T11:33:41.196069: step 2154, loss 0.142264, acc 0.9375, prec 0.0534768, recall 0.84949
2017-12-10T11:33:41.632259: step 2155, loss 0.344463, acc 0.96875, prec 0.0535645, recall 0.84972
2017-12-10T11:33:42.073516: step 2156, loss 0.390142, acc 0.90625, prec 0.0535846, recall 0.849796
2017-12-10T11:33:42.532723: step 2157, loss 0.191627, acc 0.953125, prec 0.0535795, recall 0.849796
2017-12-10T11:33:42.975127: step 2158, loss 0.189245, acc 0.921875, prec 0.0535709, recall 0.849796
2017-12-10T11:33:43.408001: step 2159, loss 0.174659, acc 0.921875, prec 0.0535623, recall 0.849796
2017-12-10T11:33:43.840254: step 2160, loss 0.791564, acc 0.875, prec 0.0535789, recall 0.849873
2017-12-10T11:33:44.286714: step 2161, loss 0.0645487, acc 0.96875, prec 0.0536362, recall 0.850025
2017-12-10T11:33:44.731569: step 2162, loss 0.259444, acc 0.9375, prec 0.0536596, recall 0.850102
2017-12-10T11:33:45.192950: step 2163, loss 0.168908, acc 0.921875, prec 0.053651, recall 0.850102
2017-12-10T11:33:45.648376: step 2164, loss 0.101399, acc 0.953125, prec 0.0536459, recall 0.850102
2017-12-10T11:33:46.096435: step 2165, loss 0.112305, acc 0.9375, prec 0.053639, recall 0.850102
2017-12-10T11:33:46.552298: step 2166, loss 0.231082, acc 0.953125, prec 0.0536945, recall 0.850254
2017-12-10T11:33:47.003349: step 2167, loss 0.215187, acc 0.890625, prec 0.0536825, recall 0.850254
2017-12-10T11:33:47.445266: step 2168, loss 0.1655, acc 0.953125, prec 0.0536773, recall 0.850254
2017-12-10T11:33:47.885494: step 2169, loss 0.12061, acc 0.96875, prec 0.0537345, recall 0.850406
2017-12-10T11:33:48.329668: step 2170, loss 0.165033, acc 0.9375, prec 0.0537882, recall 0.850557
2017-12-10T11:33:48.769461: step 2171, loss 0.20269, acc 0.9375, prec 0.0538117, recall 0.850633
2017-12-10T11:33:49.209117: step 2172, loss 0.0887443, acc 0.984375, prec 0.0539008, recall 0.850859
2017-12-10T11:33:49.647192: step 2173, loss 0.0170546, acc 1, prec 0.0539008, recall 0.850859
2017-12-10T11:33:50.086786: step 2174, loss 0.15358, acc 0.96875, prec 0.0539277, recall 0.850935
2017-12-10T11:33:50.522691: step 2175, loss 0.296508, acc 0.9375, prec 0.0539511, recall 0.85101
2017-12-10T11:33:50.957679: step 2176, loss 0.0461958, acc 0.984375, prec 0.0539796, recall 0.851085
2017-12-10T11:33:51.402216: step 2177, loss 7.39048, acc 0.9375, prec 0.0539745, recall 0.850656
2017-12-10T11:33:51.857467: step 2178, loss 0.193446, acc 0.953125, prec 0.0539693, recall 0.850656
2017-12-10T11:33:52.302999: step 2179, loss 0.375794, acc 0.921875, prec 0.0539909, recall 0.850731
2017-12-10T11:33:52.746183: step 2180, loss 0.18456, acc 0.9375, prec 0.0540143, recall 0.850806
2017-12-10T11:33:53.172538: step 2181, loss 0.0922377, acc 0.984375, prec 0.0540428, recall 0.850882
2017-12-10T11:33:53.611371: step 2182, loss 0.0249226, acc 1, prec 0.0540428, recall 0.850882
2017-12-10T11:33:54.040789: step 2183, loss 0.114696, acc 0.953125, prec 0.0540679, recall 0.850957
2017-12-10T11:33:54.469978: step 2184, loss 0.218769, acc 0.921875, prec 0.0540592, recall 0.850957
2017-12-10T11:33:54.910005: step 2185, loss 0.346433, acc 0.84375, prec 0.054042, recall 0.850957
2017-12-10T11:33:55.343001: step 2186, loss 0.367535, acc 0.890625, prec 0.0540299, recall 0.850957
2017-12-10T11:33:55.780613: step 2187, loss 0.132449, acc 0.953125, prec 0.0540549, recall 0.851032
2017-12-10T11:33:56.224495: step 2188, loss 0.131842, acc 0.9375, prec 0.0540782, recall 0.851107
2017-12-10T11:33:56.662864: step 2189, loss 0.190631, acc 0.953125, prec 0.0541335, recall 0.851256
2017-12-10T11:33:57.107353: step 2190, loss 0.136444, acc 0.96875, prec 0.0541603, recall 0.851331
2017-12-10T11:33:57.557068: step 2191, loss 0.294487, acc 0.921875, prec 0.0541516, recall 0.851331
2017-12-10T11:33:57.999467: step 2192, loss 0.273611, acc 0.921875, prec 0.0542034, recall 0.85148
2017-12-10T11:33:58.447060: step 2193, loss 0.311696, acc 0.9375, prec 0.0542569, recall 0.851629
2017-12-10T11:33:58.892493: step 2194, loss 1.42728, acc 0.90625, prec 0.0542784, recall 0.851277
2017-12-10T11:33:59.339704: step 2195, loss 1.21754, acc 0.921875, prec 0.0543301, recall 0.851426
2017-12-10T11:33:59.786566: step 2196, loss 0.382193, acc 0.84375, prec 0.0543128, recall 0.851426
2017-12-10T11:34:00.246174: step 2197, loss 0.267725, acc 0.921875, prec 0.0543343, recall 0.8515
2017-12-10T11:34:00.703807: step 2198, loss 0.189163, acc 0.953125, prec 0.0543291, recall 0.8515
2017-12-10T11:34:01.140910: step 2199, loss 0.400302, acc 0.890625, prec 0.054317, recall 0.8515
2017-12-10T11:34:01.587243: step 2200, loss 0.952748, acc 0.796875, prec 0.0543548, recall 0.851648
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2200

2017-12-10T11:34:03.461224: step 2201, loss 0.690157, acc 0.859375, prec 0.0543994, recall 0.851796
2017-12-10T11:34:03.906664: step 2202, loss 0.270796, acc 0.921875, prec 0.0543908, recall 0.851796
2017-12-10T11:34:04.338268: step 2203, loss 1.12697, acc 0.859375, prec 0.0544655, recall 0.852018
2017-12-10T11:34:04.771294: step 2204, loss 0.615282, acc 0.84375, prec 0.0544482, recall 0.852018
2017-12-10T11:34:05.216142: step 2205, loss 0.567484, acc 0.875, prec 0.0544343, recall 0.852018
2017-12-10T11:34:05.655870: step 2206, loss 0.567941, acc 0.890625, prec 0.0544222, recall 0.852018
2017-12-10T11:34:06.088906: step 2207, loss 0.895759, acc 0.8125, prec 0.0544014, recall 0.852018
2017-12-10T11:34:06.532152: step 2208, loss 0.436002, acc 0.90625, prec 0.0544512, recall 0.852165
2017-12-10T11:34:06.970109: step 2209, loss 0.321464, acc 0.890625, prec 0.0544391, recall 0.852165
2017-12-10T11:34:07.402430: step 2210, loss 0.857985, acc 0.84375, prec 0.0544518, recall 0.852239
2017-12-10T11:34:07.860222: step 2211, loss 0.642884, acc 0.921875, prec 0.0545333, recall 0.852459
2017-12-10T11:34:08.299816: step 2212, loss 0.275491, acc 0.921875, prec 0.0545247, recall 0.852459
2017-12-10T11:34:08.734951: step 2213, loss 0.267767, acc 0.859375, prec 0.0545691, recall 0.852605
2017-12-10T11:34:09.165411: step 2214, loss 0.431128, acc 0.875, prec 0.0545553, recall 0.852605
2017-12-10T11:34:09.609603: step 2215, loss 0.353544, acc 0.890625, prec 0.0545431, recall 0.852605
2017-12-10T11:34:10.049858: step 2216, loss 0.447495, acc 0.875, prec 0.0545293, recall 0.852605
2017-12-10T11:34:10.479226: step 2217, loss 0.289986, acc 0.890625, prec 0.0545172, recall 0.852605
2017-12-10T11:34:10.914547: step 2218, loss 0.170261, acc 0.9375, prec 0.0545703, recall 0.852752
2017-12-10T11:34:11.363913: step 2219, loss 0.162124, acc 0.953125, prec 0.0545951, recall 0.852825
2017-12-10T11:34:11.803979: step 2220, loss 0.123289, acc 0.953125, prec 0.0546198, recall 0.852897
2017-12-10T11:34:12.252342: step 2221, loss 0.134559, acc 0.96875, prec 0.0546764, recall 0.853043
2017-12-10T11:34:12.697792: step 2222, loss 0.0294424, acc 1, prec 0.0546764, recall 0.853043
2017-12-10T11:34:13.143515: step 2223, loss 0.215458, acc 0.953125, prec 0.0546711, recall 0.853043
2017-12-10T11:34:13.593244: step 2224, loss 0.184061, acc 0.90625, prec 0.0546607, recall 0.853043
2017-12-10T11:34:14.034639: step 2225, loss 0.174307, acc 0.953125, prec 0.0546855, recall 0.853116
2017-12-10T11:34:14.465539: step 2226, loss 0.25652, acc 0.9375, prec 0.0547085, recall 0.853188
2017-12-10T11:34:14.904564: step 2227, loss 0.853323, acc 0.921875, prec 0.0547298, recall 0.853261
2017-12-10T11:34:15.346124: step 2228, loss 0.2764, acc 0.96875, prec 0.0547563, recall 0.853333
2017-12-10T11:34:15.779800: step 2229, loss 1.05975, acc 0.96875, prec 0.0548427, recall 0.85355
2017-12-10T11:34:16.224983: step 2230, loss 0.215531, acc 0.96875, prec 0.054929, recall 0.853767
2017-12-10T11:34:16.676548: step 2231, loss 0.185217, acc 0.953125, prec 0.0549538, recall 0.853839
2017-12-10T11:34:17.111569: step 2232, loss 0.193297, acc 0.9375, prec 0.0550066, recall 0.853982
2017-12-10T11:34:17.562287: step 2233, loss 0.133461, acc 0.921875, prec 0.0550279, recall 0.854054
2017-12-10T11:34:17.996958: step 2234, loss 0.244268, acc 0.9375, prec 0.0550209, recall 0.854054
2017-12-10T11:34:18.445205: step 2235, loss 0.0519399, acc 0.984375, prec 0.0550192, recall 0.854054
2017-12-10T11:34:18.886672: step 2236, loss 0.309509, acc 0.921875, prec 0.0550404, recall 0.854126
2017-12-10T11:34:19.328157: step 2237, loss 0.263728, acc 0.9375, prec 0.0550932, recall 0.854269
2017-12-10T11:34:19.769991: step 2238, loss 0.0761067, acc 0.96875, prec 0.0551196, recall 0.85434
2017-12-10T11:34:20.208067: step 2239, loss 0.261533, acc 0.9375, prec 0.0551126, recall 0.85434
2017-12-10T11:34:20.644659: step 2240, loss 0.155586, acc 0.9375, prec 0.0551654, recall 0.854483
2017-12-10T11:34:21.087969: step 2241, loss 0.220738, acc 0.9375, prec 0.0551883, recall 0.854554
2017-12-10T11:34:21.527866: step 2242, loss 0.173816, acc 0.9375, prec 0.0551814, recall 0.854554
2017-12-10T11:34:21.970423: step 2243, loss 0.196423, acc 0.984375, prec 0.0552394, recall 0.854697
2017-12-10T11:34:22.417795: step 2244, loss 0.206849, acc 0.953125, prec 0.0552939, recall 0.854839
2017-12-10T11:34:22.862233: step 2245, loss 1.68071, acc 0.953125, prec 0.0553202, recall 0.854492
2017-12-10T11:34:23.294810: step 2246, loss 0.246447, acc 0.9375, prec 0.0553431, recall 0.854563
2017-12-10T11:34:23.739580: step 2247, loss 0.103578, acc 0.953125, prec 0.0553677, recall 0.854634
2017-12-10T11:34:24.174215: step 2248, loss 0.191258, acc 0.96875, prec 0.055394, recall 0.854705
2017-12-10T11:34:24.608890: step 2249, loss 0.552845, acc 0.953125, prec 0.0554783, recall 0.854917
2017-12-10T11:34:25.045785: step 2250, loss 0.128898, acc 0.953125, prec 0.0554731, recall 0.854917
2017-12-10T11:34:25.488390: step 2251, loss 0.171418, acc 0.9375, prec 0.0554661, recall 0.854917
2017-12-10T11:34:25.931123: step 2252, loss 2.39379, acc 0.921875, prec 0.0554889, recall 0.854572
2017-12-10T11:34:26.381908: step 2253, loss 0.268915, acc 0.9375, prec 0.0555117, recall 0.854643
2017-12-10T11:34:26.818153: step 2254, loss 0.566552, acc 0.8125, prec 0.0554907, recall 0.854643
2017-12-10T11:34:27.253708: step 2255, loss 2.90245, acc 0.78125, prec 0.0554679, recall 0.854227
2017-12-10T11:34:27.694755: step 2256, loss 0.817677, acc 0.828125, prec 0.0554487, recall 0.854227
2017-12-10T11:34:28.131004: step 2257, loss 0.510227, acc 0.796875, prec 0.0554259, recall 0.854227
2017-12-10T11:34:28.563387: step 2258, loss 0.752413, acc 0.734375, prec 0.0553963, recall 0.854227
2017-12-10T11:34:29.002333: step 2259, loss 0.852853, acc 0.734375, prec 0.0553666, recall 0.854227
2017-12-10T11:34:29.430645: step 2260, loss 1.15115, acc 0.71875, prec 0.0553352, recall 0.854227
2017-12-10T11:34:29.858998: step 2261, loss 1.18744, acc 0.640625, prec 0.0553546, recall 0.854369
2017-12-10T11:34:30.314782: step 2262, loss 0.966214, acc 0.734375, prec 0.0554438, recall 0.854651
2017-12-10T11:34:30.760046: step 2263, loss 0.910837, acc 0.75, prec 0.0554159, recall 0.854651
2017-12-10T11:34:31.199963: step 2264, loss 0.921551, acc 0.78125, prec 0.0554509, recall 0.854792
2017-12-10T11:34:31.637591: step 2265, loss 1.15119, acc 0.71875, prec 0.0554492, recall 0.854862
2017-12-10T11:34:32.082348: step 2266, loss 1.52965, acc 0.734375, prec 0.0554789, recall 0.855002
2017-12-10T11:34:32.522174: step 2267, loss 1.15805, acc 0.71875, prec 0.0554772, recall 0.855072
2017-12-10T11:34:32.959872: step 2268, loss 0.747297, acc 0.8125, prec 0.0555155, recall 0.855212
2017-12-10T11:34:33.395929: step 2269, loss 0.704156, acc 0.796875, prec 0.0555817, recall 0.855422
2017-12-10T11:34:33.833093: step 2270, loss 0.536589, acc 0.828125, prec 0.0555921, recall 0.855491
2017-12-10T11:34:34.275390: step 2271, loss 0.568302, acc 0.84375, prec 0.0555747, recall 0.855491
2017-12-10T11:34:34.724689: step 2272, loss 1.36588, acc 0.859375, prec 0.0555886, recall 0.855561
2017-12-10T11:34:35.161172: step 2273, loss 0.520535, acc 0.890625, prec 0.0555764, recall 0.855561
2017-12-10T11:34:35.604088: step 2274, loss 0.159889, acc 0.90625, prec 0.055566, recall 0.855561
2017-12-10T11:34:36.047451: step 2275, loss 0.470047, acc 0.90625, prec 0.0555851, recall 0.85563
2017-12-10T11:34:36.483747: step 2276, loss 0.339437, acc 0.90625, prec 0.0555747, recall 0.85563
2017-12-10T11:34:36.921130: step 2277, loss 0.406093, acc 0.875, prec 0.0555608, recall 0.85563
2017-12-10T11:34:37.361656: step 2278, loss 0.300329, acc 0.90625, prec 0.0555503, recall 0.85563
2017-12-10T11:34:37.800065: step 2279, loss 0.347621, acc 0.9375, prec 0.0555729, recall 0.8557
2017-12-10T11:34:38.243266: step 2280, loss 0.660658, acc 0.953125, prec 0.0555972, recall 0.855769
2017-12-10T11:34:38.679560: step 2281, loss 0.394101, acc 0.890625, prec 0.0556145, recall 0.855839
2017-12-10T11:34:39.121947: step 2282, loss 0.0462813, acc 0.984375, prec 0.0556128, recall 0.855839
2017-12-10T11:34:39.556409: step 2283, loss 0.745156, acc 0.9375, prec 0.0556648, recall 0.855977
2017-12-10T11:34:40.005010: step 2284, loss 0.071877, acc 0.96875, prec 0.0556614, recall 0.855977
2017-12-10T11:34:40.448752: step 2285, loss 0.135845, acc 0.9375, prec 0.0556544, recall 0.855977
2017-12-10T11:34:40.882917: step 2286, loss 0.123783, acc 0.96875, prec 0.0556509, recall 0.855977
2017-12-10T11:34:41.314094: step 2287, loss 0.114042, acc 0.921875, prec 0.0556422, recall 0.855977
2017-12-10T11:34:41.753917: step 2288, loss 0.120847, acc 0.953125, prec 0.0556665, recall 0.856046
2017-12-10T11:34:42.188133: step 2289, loss 0.0299098, acc 0.984375, prec 0.0556648, recall 0.856046
2017-12-10T11:34:42.626703: step 2290, loss 1.32868, acc 0.84375, prec 0.0556769, recall 0.856115
2017-12-10T11:34:43.063779: step 2291, loss 0.164573, acc 0.9375, prec 0.0556994, recall 0.856184
2017-12-10T11:34:43.518774: step 2292, loss 0.250373, acc 0.953125, prec 0.055753, recall 0.856322
2017-12-10T11:34:43.957208: step 2293, loss 0.129124, acc 0.953125, prec 0.0557773, recall 0.856391
2017-12-10T11:34:44.402270: step 2294, loss 1.52566, acc 0.96875, prec 0.055805, recall 0.85605
2017-12-10T11:34:44.846984: step 2295, loss 2.67001, acc 0.9375, prec 0.0557997, recall 0.855641
2017-12-10T11:34:45.298615: step 2296, loss 0.359737, acc 0.90625, prec 0.0557893, recall 0.855641
2017-12-10T11:34:45.733713: step 2297, loss 0.288334, acc 0.9375, prec 0.0558412, recall 0.855778
2017-12-10T11:34:46.174369: step 2298, loss 0.246171, acc 0.953125, prec 0.055836, recall 0.855778
2017-12-10T11:34:46.614960: step 2299, loss 0.116985, acc 0.953125, prec 0.0558308, recall 0.855778
2017-12-10T11:34:47.054930: step 2300, loss 0.267915, acc 0.921875, prec 0.0558221, recall 0.855778
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2300

2017-12-10T11:34:49.031111: step 2301, loss 0.170084, acc 0.953125, prec 0.0558169, recall 0.855778
2017-12-10T11:34:49.475711: step 2302, loss 0.372106, acc 0.921875, prec 0.0558082, recall 0.855778
2017-12-10T11:34:49.915448: step 2303, loss 0.613695, acc 0.875, prec 0.0557943, recall 0.855778
2017-12-10T11:34:50.342786: step 2304, loss 0.427158, acc 0.859375, prec 0.0558374, recall 0.855916
2017-12-10T11:34:50.779299: step 2305, loss 0.325037, acc 0.90625, prec 0.0558564, recall 0.855985
2017-12-10T11:34:51.212161: step 2306, loss 0.408231, acc 0.890625, prec 0.0559029, recall 0.856122
2017-12-10T11:34:51.651928: step 2307, loss 0.296407, acc 0.921875, prec 0.0560117, recall 0.856396
2017-12-10T11:34:52.085953: step 2308, loss 0.490886, acc 0.921875, prec 0.0560323, recall 0.856464
2017-12-10T11:34:52.530095: step 2309, loss 0.361336, acc 0.90625, prec 0.0560219, recall 0.856464
2017-12-10T11:34:52.980712: step 2310, loss 0.321062, acc 0.90625, prec 0.0560114, recall 0.856464
2017-12-10T11:34:53.410239: step 2311, loss 0.419755, acc 0.953125, prec 0.0560356, recall 0.856532
2017-12-10T11:34:53.842782: step 2312, loss 0.767006, acc 0.859375, prec 0.0561079, recall 0.856736
2017-12-10T11:34:54.277561: step 2313, loss 0.435843, acc 0.90625, prec 0.0560974, recall 0.856736
2017-12-10T11:34:54.709075: step 2314, loss 0.347326, acc 0.890625, prec 0.0560852, recall 0.856736
2017-12-10T11:34:55.146179: step 2315, loss 0.210595, acc 0.921875, prec 0.0561644, recall 0.85694
2017-12-10T11:34:55.574995: step 2316, loss 0.155415, acc 0.953125, prec 0.0562178, recall 0.857075
2017-12-10T11:34:56.011984: step 2317, loss 0.235139, acc 0.921875, prec 0.0562677, recall 0.85721
2017-12-10T11:34:56.454583: step 2318, loss 0.202432, acc 0.90625, prec 0.0562865, recall 0.857278
2017-12-10T11:34:56.891184: step 2319, loss 0.106289, acc 0.921875, prec 0.0562777, recall 0.857278
2017-12-10T11:34:57.330419: step 2320, loss 0.346098, acc 0.953125, prec 0.0563603, recall 0.85748
2017-12-10T11:34:57.775459: step 2321, loss 0.334334, acc 0.921875, prec 0.0564101, recall 0.857614
2017-12-10T11:34:58.204827: step 2322, loss 0.46189, acc 0.90625, prec 0.0564874, recall 0.857815
2017-12-10T11:34:58.645311: step 2323, loss 0.14777, acc 0.90625, prec 0.0565061, recall 0.857882
2017-12-10T11:34:59.076521: step 2324, loss 0.0321961, acc 0.984375, prec 0.0565044, recall 0.857882
2017-12-10T11:34:59.514921: step 2325, loss 1.57471, acc 0.984375, prec 0.0565921, recall 0.85768
2017-12-10T11:34:59.950490: step 2326, loss 0.0198783, acc 1, prec 0.0565921, recall 0.85768
2017-12-10T11:35:00.378759: step 2327, loss 0.359594, acc 0.9375, prec 0.0566143, recall 0.857746
2017-12-10T11:35:00.817109: step 2328, loss 0.247448, acc 0.96875, prec 0.0566693, recall 0.85788
2017-12-10T11:35:01.247513: step 2329, loss 0.309239, acc 0.921875, prec 0.0567481, recall 0.85808
2017-12-10T11:35:01.682154: step 2330, loss 0.0549888, acc 0.984375, prec 0.0567464, recall 0.85808
2017-12-10T11:35:02.116440: step 2331, loss 0.207475, acc 0.90625, prec 0.0567358, recall 0.85808
2017-12-10T11:35:02.555771: step 2332, loss 0.281338, acc 0.90625, prec 0.0567545, recall 0.858146
2017-12-10T11:35:02.984509: step 2333, loss 0.165543, acc 0.90625, prec 0.056744, recall 0.858146
2017-12-10T11:35:03.409769: step 2334, loss 0.308954, acc 0.9375, prec 0.0567661, recall 0.858212
2017-12-10T11:35:03.846140: step 2335, loss 0.302082, acc 0.90625, prec 0.0567556, recall 0.858212
2017-12-10T11:35:04.292566: step 2336, loss 0.204968, acc 0.921875, prec 0.0567468, recall 0.858212
2017-12-10T11:35:04.735353: step 2337, loss 0.387602, acc 0.890625, prec 0.0567637, recall 0.858279
2017-12-10T11:35:05.175401: step 2338, loss 0.347765, acc 0.921875, prec 0.0567549, recall 0.858279
2017-12-10T11:35:05.612519: step 2339, loss 0.17918, acc 0.984375, prec 0.0567823, recall 0.858345
2017-12-10T11:35:06.049236: step 2340, loss 0.06135, acc 0.96875, prec 0.0567788, recall 0.858345
2017-12-10T11:35:06.479509: step 2341, loss 0.0855698, acc 0.96875, prec 0.0568045, recall 0.858411
2017-12-10T11:35:06.918109: step 2342, loss 0.252324, acc 1, prec 0.0568336, recall 0.858477
2017-12-10T11:35:07.360150: step 2343, loss 0.491759, acc 0.953125, prec 0.0568575, recall 0.858543
2017-12-10T11:35:07.818512: step 2344, loss 1.35329, acc 0.953125, prec 0.056854, recall 0.858143
2017-12-10T11:35:08.268896: step 2345, loss 0.21676, acc 0.9375, prec 0.0569053, recall 0.858275
2017-12-10T11:35:08.715026: step 2346, loss 0.218689, acc 0.984375, prec 0.0569327, recall 0.858341
2017-12-10T11:35:09.155558: step 2347, loss 5.53243, acc 0.90625, prec 0.0569239, recall 0.857941
2017-12-10T11:35:09.609522: step 2348, loss 0.308681, acc 0.90625, prec 0.0569133, recall 0.857941
2017-12-10T11:35:10.063065: step 2349, loss 0.151857, acc 0.9375, prec 0.0569354, recall 0.858007
2017-12-10T11:35:10.508325: step 2350, loss 0.0750525, acc 0.953125, prec 0.0569302, recall 0.858007
2017-12-10T11:35:10.941424: step 2351, loss 0.257518, acc 0.953125, prec 0.056954, recall 0.858074
2017-12-10T11:35:11.369398: step 2352, loss 0.421113, acc 0.859375, prec 0.0569673, recall 0.85814
2017-12-10T11:35:11.809289: step 2353, loss 0.257701, acc 0.921875, prec 0.0569876, recall 0.858205
2017-12-10T11:35:12.256130: step 2354, loss 0.497698, acc 0.859375, prec 0.0569718, recall 0.858205
2017-12-10T11:35:12.689482: step 2355, loss 0.534562, acc 0.828125, prec 0.0569816, recall 0.858271
2017-12-10T11:35:13.128386: step 2356, loss 0.737592, acc 0.8125, prec 0.0569605, recall 0.858271
2017-12-10T11:35:13.566636: step 2357, loss 0.475834, acc 0.828125, prec 0.0569702, recall 0.858337
2017-12-10T11:35:14.002928: step 2358, loss 0.637329, acc 0.828125, prec 0.05698, recall 0.858403
2017-12-10T11:35:14.449522: step 2359, loss 0.403661, acc 0.875, prec 0.0570531, recall 0.8586
2017-12-10T11:35:14.899055: step 2360, loss 0.295877, acc 0.90625, prec 0.0570716, recall 0.858665
2017-12-10T11:35:15.343105: step 2361, loss 0.632087, acc 0.84375, prec 0.0571121, recall 0.858796
2017-12-10T11:35:15.774770: step 2362, loss 0.722532, acc 0.796875, prec 0.0570892, recall 0.858796
2017-12-10T11:35:16.215086: step 2363, loss 0.273536, acc 0.921875, prec 0.0571094, recall 0.858862
2017-12-10T11:35:16.658052: step 2364, loss 0.476725, acc 0.828125, prec 0.0570901, recall 0.858862
2017-12-10T11:35:17.100118: step 2365, loss 0.508408, acc 0.890625, prec 0.0571648, recall 0.859057
2017-12-10T11:35:17.533110: step 2366, loss 0.14668, acc 0.921875, prec 0.057214, recall 0.859187
2017-12-10T11:35:17.971633: step 2367, loss 0.208721, acc 0.921875, prec 0.0572052, recall 0.859187
2017-12-10T11:35:18.412524: step 2368, loss 0.154853, acc 0.953125, prec 0.0572289, recall 0.859252
2017-12-10T11:35:18.851650: step 2369, loss 0.372218, acc 0.875, prec 0.0572148, recall 0.859252
2017-12-10T11:35:19.285769: step 2370, loss 0.373544, acc 0.90625, prec 0.0572043, recall 0.859252
2017-12-10T11:35:19.719931: step 2371, loss 0.147916, acc 0.9375, prec 0.0572262, recall 0.859317
2017-12-10T11:35:20.154269: step 2372, loss 0.489412, acc 0.984375, prec 0.0572534, recall 0.859382
2017-12-10T11:35:20.581198: step 2373, loss 0.395127, acc 0.875, prec 0.0572683, recall 0.859447
2017-12-10T11:35:21.013494: step 2374, loss 0.318158, acc 0.921875, prec 0.0572595, recall 0.859447
2017-12-10T11:35:21.452896: step 2375, loss 0.160454, acc 0.953125, prec 0.0573121, recall 0.859576
2017-12-10T11:35:21.899065: step 2376, loss 0.429592, acc 0.953125, prec 0.0573647, recall 0.859706
2017-12-10T11:35:22.368777: step 2377, loss 3.67032, acc 0.96875, prec 0.057363, recall 0.85931
2017-12-10T11:35:22.817146: step 2378, loss 0.302836, acc 1, prec 0.0573919, recall 0.859375
2017-12-10T11:35:23.273751: step 2379, loss 0.0718585, acc 0.984375, prec 0.057448, recall 0.859504
2017-12-10T11:35:23.723492: step 2380, loss 0.446543, acc 0.890625, prec 0.0574646, recall 0.859569
2017-12-10T11:35:24.163124: step 2381, loss 0.287006, acc 0.921875, prec 0.0574847, recall 0.859633
2017-12-10T11:35:24.595584: step 2382, loss 0.223417, acc 0.9375, prec 0.0575065, recall 0.859697
2017-12-10T11:35:25.048777: step 2383, loss 0.277013, acc 0.90625, prec 0.0574959, recall 0.859697
2017-12-10T11:35:25.479113: step 2384, loss 0.203013, acc 0.9375, prec 0.0574889, recall 0.859697
2017-12-10T11:35:25.917931: step 2385, loss 0.360061, acc 0.90625, prec 0.0575361, recall 0.859826
2017-12-10T11:35:26.356835: step 2386, loss 0.38604, acc 0.875, prec 0.0575509, recall 0.85989
2017-12-10T11:35:26.795855: step 2387, loss 0.244875, acc 0.890625, prec 0.0575385, recall 0.85989
2017-12-10T11:35:27.243459: step 2388, loss 0.271667, acc 0.890625, prec 0.0575262, recall 0.85989
2017-12-10T11:35:27.675042: step 2389, loss 0.215944, acc 0.921875, prec 0.0575751, recall 0.860018
2017-12-10T11:35:28.104001: step 2390, loss 0.486992, acc 0.875, prec 0.0576187, recall 0.860146
2017-12-10T11:35:28.547194: step 2391, loss 0.397217, acc 0.890625, prec 0.0576064, recall 0.860146
2017-12-10T11:35:28.982007: step 2392, loss 0.402996, acc 0.90625, prec 0.0576246, recall 0.86021
2017-12-10T11:35:29.416890: step 2393, loss 0.216332, acc 0.9375, prec 0.0576176, recall 0.86021
2017-12-10T11:35:29.861616: step 2394, loss 0.472839, acc 0.953125, prec 0.0576699, recall 0.860338
2017-12-10T11:35:30.309897: step 2395, loss 0.414838, acc 0.890625, prec 0.0576576, recall 0.860338
2017-12-10T11:35:30.750201: step 2396, loss 0.372651, acc 0.875, prec 0.0576435, recall 0.860338
2017-12-10T11:35:31.186428: step 2397, loss 0.169488, acc 0.9375, prec 0.0576364, recall 0.860338
2017-12-10T11:35:31.622157: step 2398, loss 0.144477, acc 0.953125, prec 0.0576312, recall 0.860338
2017-12-10T11:35:32.056714: step 2399, loss 0.143851, acc 0.9375, prec 0.0576241, recall 0.860338
2017-12-10T11:35:32.507140: step 2400, loss 0.123135, acc 0.96875, prec 0.0576494, recall 0.860401
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2400

2017-12-10T11:35:34.357325: step 2401, loss 0.0779066, acc 0.984375, prec 0.0576476, recall 0.860401
2017-12-10T11:35:34.797480: step 2402, loss 3.38672, acc 0.953125, prec 0.0577017, recall 0.860137
2017-12-10T11:35:35.239776: step 2403, loss 0.293252, acc 0.9375, prec 0.0576947, recall 0.860137
2017-12-10T11:35:35.681879: step 2404, loss 0.324403, acc 0.859375, prec 0.0576788, recall 0.860137
2017-12-10T11:35:36.123397: step 2405, loss 0.148585, acc 0.9375, prec 0.0576717, recall 0.860137
2017-12-10T11:35:36.568048: step 2406, loss 0.321575, acc 0.90625, prec 0.05769, recall 0.8602
2017-12-10T11:35:37.006321: step 2407, loss 0.266433, acc 0.921875, prec 0.0577675, recall 0.860391
2017-12-10T11:35:37.453510: step 2408, loss 0.396926, acc 0.875, prec 0.0577534, recall 0.860391
2017-12-10T11:35:37.895109: step 2409, loss 0.806088, acc 0.828125, prec 0.0577627, recall 0.860455
2017-12-10T11:35:38.334934: step 2410, loss 0.514958, acc 0.921875, prec 0.0578114, recall 0.860581
2017-12-10T11:35:38.775895: step 2411, loss 0.316536, acc 0.890625, prec 0.0578278, recall 0.860645
2017-12-10T11:35:39.210650: step 2412, loss 0.319897, acc 0.90625, prec 0.0578172, recall 0.860645
2017-12-10T11:35:39.638310: step 2413, loss 0.166479, acc 0.921875, prec 0.0578946, recall 0.860834
2017-12-10T11:35:40.077437: step 2414, loss 0.173529, acc 0.953125, prec 0.0578893, recall 0.860834
2017-12-10T11:35:40.526069: step 2415, loss 0.346933, acc 0.921875, prec 0.0579666, recall 0.861023
2017-12-10T11:35:40.972402: step 2416, loss 0.502863, acc 0.9375, prec 0.0580169, recall 0.861149
2017-12-10T11:35:41.404690: step 2417, loss 0.207536, acc 0.921875, prec 0.0580081, recall 0.861149
2017-12-10T11:35:41.852940: step 2418, loss 0.364458, acc 0.90625, prec 0.0580549, recall 0.861274
2017-12-10T11:35:42.291245: step 2419, loss 0.137279, acc 0.921875, prec 0.058046, recall 0.861274
2017-12-10T11:35:42.721370: step 2420, loss 0.0872309, acc 0.96875, prec 0.0580712, recall 0.861337
2017-12-10T11:35:43.158958: step 2421, loss 0.210893, acc 0.953125, prec 0.0580659, recall 0.861337
2017-12-10T11:35:43.602994: step 2422, loss 0.189942, acc 0.9375, prec 0.0581162, recall 0.861462
2017-12-10T11:35:44.050876: step 2423, loss 0.110561, acc 0.953125, prec 0.0581395, recall 0.861525
2017-12-10T11:35:44.479613: step 2424, loss 0.220609, acc 0.921875, prec 0.0581594, recall 0.861587
2017-12-10T11:35:44.932317: step 2425, loss 0.331089, acc 0.890625, prec 0.0581756, recall 0.861649
2017-12-10T11:35:45.399161: step 2426, loss 0.0854086, acc 0.953125, prec 0.0581703, recall 0.861649
2017-12-10T11:35:45.851465: step 2427, loss 0.141647, acc 0.953125, prec 0.058165, recall 0.861649
2017-12-10T11:35:46.298007: step 2428, loss 0.0988406, acc 0.953125, prec 0.0581597, recall 0.861649
2017-12-10T11:35:46.740128: step 2429, loss 0.157251, acc 0.96875, prec 0.0581562, recall 0.861649
2017-12-10T11:35:47.176920: step 2430, loss 0.130785, acc 0.984375, prec 0.0581544, recall 0.861649
2017-12-10T11:35:47.625216: step 2431, loss 0.0492661, acc 0.984375, prec 0.0581526, recall 0.861649
2017-12-10T11:35:48.065406: step 2432, loss 0.212331, acc 0.953125, prec 0.0582619, recall 0.861898
2017-12-10T11:35:48.511725: step 2433, loss 0.0874978, acc 0.984375, prec 0.0582887, recall 0.86196
2017-12-10T11:35:48.949752: step 2434, loss 0.0505749, acc 0.953125, prec 0.0582834, recall 0.86196
2017-12-10T11:35:49.399058: step 2435, loss 0.203199, acc 0.96875, prec 0.0583085, recall 0.862022
2017-12-10T11:35:49.844642: step 2436, loss 0.0257784, acc 1, prec 0.0583085, recall 0.862022
2017-12-10T11:35:50.288709: step 2437, loss 0.232037, acc 0.9375, prec 0.0583014, recall 0.862022
2017-12-10T11:35:50.723715: step 2438, loss 0.0247946, acc 1, prec 0.0583014, recall 0.862022
2017-12-10T11:35:51.169074: step 2439, loss 0.0698322, acc 0.96875, prec 0.0582979, recall 0.862022
2017-12-10T11:35:51.611743: step 2440, loss 0.0738835, acc 0.96875, prec 0.0583229, recall 0.862084
2017-12-10T11:35:52.067849: step 2441, loss 0.197555, acc 0.9375, prec 0.0583159, recall 0.862084
2017-12-10T11:35:52.544433: step 2442, loss 0.0142864, acc 1, prec 0.0583445, recall 0.862146
2017-12-10T11:35:52.993800: step 2443, loss 0.0238017, acc 0.984375, prec 0.0583427, recall 0.862146
2017-12-10T11:35:53.443606: step 2444, loss 0.222308, acc 0.96875, prec 0.0583678, recall 0.862208
2017-12-10T11:35:53.870601: step 2445, loss 1.29374, acc 0.96875, prec 0.0584214, recall 0.862332
2017-12-10T11:35:54.327390: step 2446, loss 0.281561, acc 1, prec 0.0584786, recall 0.862455
2017-12-10T11:35:54.771032: step 2447, loss 0.729849, acc 0.984375, prec 0.0585341, recall 0.862578
2017-12-10T11:35:55.217125: step 2448, loss 0.181341, acc 0.953125, prec 0.0585287, recall 0.862578
2017-12-10T11:35:55.658668: step 2449, loss 0.216684, acc 0.9375, prec 0.0585216, recall 0.862578
2017-12-10T11:35:56.103469: step 2450, loss 0.116381, acc 0.984375, prec 0.0585484, recall 0.86264
2017-12-10T11:35:56.549550: step 2451, loss 0.228685, acc 0.96875, prec 0.0585735, recall 0.862701
2017-12-10T11:35:56.998597: step 2452, loss 0.346876, acc 0.953125, prec 0.0586539, recall 0.862885
2017-12-10T11:35:57.446786: step 2453, loss 0.156092, acc 0.9375, prec 0.0586468, recall 0.862885
2017-12-10T11:35:57.893657: step 2454, loss 0.365555, acc 0.921875, prec 0.0587236, recall 0.863069
2017-12-10T11:35:58.337732: step 2455, loss 0.0572393, acc 1, prec 0.0587521, recall 0.86313
2017-12-10T11:35:58.783227: step 2456, loss 0.811666, acc 0.90625, prec 0.0587985, recall 0.863252
2017-12-10T11:35:59.243847: step 2457, loss 0.167747, acc 0.953125, prec 0.0587932, recall 0.863252
2017-12-10T11:35:59.685942: step 2458, loss 0.33182, acc 0.90625, prec 0.0588396, recall 0.863373
2017-12-10T11:36:00.131211: step 2459, loss 0.231221, acc 0.90625, prec 0.0588289, recall 0.863373
2017-12-10T11:36:00.565051: step 2460, loss 0.411574, acc 0.84375, prec 0.058811, recall 0.863373
2017-12-10T11:36:01.017082: step 2461, loss 0.406394, acc 0.859375, prec 0.058795, recall 0.863373
2017-12-10T11:36:01.451098: step 2462, loss 0.312433, acc 0.90625, prec 0.0588128, recall 0.863434
2017-12-10T11:36:01.887601: step 2463, loss 0.309349, acc 0.90625, prec 0.0588021, recall 0.863434
2017-12-10T11:36:02.325202: step 2464, loss 0.447567, acc 0.859375, prec 0.0587861, recall 0.863434
2017-12-10T11:36:02.769222: step 2465, loss 0.90617, acc 0.953125, prec 0.0588663, recall 0.863616
2017-12-10T11:36:03.213654: step 2466, loss 4.01797, acc 0.921875, prec 0.0588876, recall 0.863293
2017-12-10T11:36:03.662252: step 2467, loss 0.304869, acc 0.9375, prec 0.0589375, recall 0.863415
2017-12-10T11:36:04.102980: step 2468, loss 0.883176, acc 0.796875, prec 0.0589143, recall 0.863415
2017-12-10T11:36:04.545846: step 2469, loss 0.467945, acc 0.765625, prec 0.058916, recall 0.863475
2017-12-10T11:36:05.004922: step 2470, loss 0.226547, acc 0.90625, prec 0.0589623, recall 0.863596
2017-12-10T11:36:05.454703: step 2471, loss 0.7547, acc 0.828125, prec 0.0589427, recall 0.863596
2017-12-10T11:36:05.899444: step 2472, loss 0.868525, acc 0.765625, prec 0.0589444, recall 0.863656
2017-12-10T11:36:06.343010: step 2473, loss 0.783065, acc 0.84375, prec 0.0589266, recall 0.863656
2017-12-10T11:36:06.765615: step 2474, loss 1.07626, acc 0.796875, prec 0.0589319, recall 0.863717
2017-12-10T11:36:07.209640: step 2475, loss 0.779908, acc 0.78125, prec 0.0589354, recall 0.863777
2017-12-10T11:36:07.650163: step 2476, loss 0.679519, acc 0.875, prec 0.0589495, recall 0.863837
2017-12-10T11:36:08.093730: step 2477, loss 0.651928, acc 0.8125, prec 0.0589282, recall 0.863837
2017-12-10T11:36:08.536764: step 2478, loss 0.525277, acc 0.890625, prec 0.0589441, recall 0.863898
2017-12-10T11:36:08.988186: step 2479, loss 0.42959, acc 0.90625, prec 0.0589902, recall 0.864018
2017-12-10T11:36:09.440246: step 2480, loss 0.479623, acc 0.875, prec 0.0590611, recall 0.864198
2017-12-10T11:36:09.900783: step 2481, loss 0.391241, acc 0.875, prec 0.0590752, recall 0.864257
2017-12-10T11:36:10.355206: step 2482, loss 0.428466, acc 0.890625, prec 0.059091, recall 0.864317
2017-12-10T11:36:10.789497: step 2483, loss 0.337413, acc 0.890625, prec 0.0590786, recall 0.864317
2017-12-10T11:36:11.231431: step 2484, loss 0.17835, acc 0.953125, prec 0.0590733, recall 0.864317
2017-12-10T11:36:11.633106: step 2485, loss 0.434443, acc 0.882353, prec 0.0590626, recall 0.864317
2017-12-10T11:36:12.082747: step 2486, loss 0.203264, acc 0.921875, prec 0.0591103, recall 0.864437
2017-12-10T11:36:12.499934: step 2487, loss 0.116285, acc 0.96875, prec 0.0591351, recall 0.864496
2017-12-10T11:36:12.935728: step 2488, loss 0.329464, acc 0.890625, prec 0.0591226, recall 0.864496
2017-12-10T11:36:13.366169: step 2489, loss 0.154016, acc 0.953125, prec 0.0591173, recall 0.864496
2017-12-10T11:36:13.814750: step 2490, loss 0.153297, acc 0.9375, prec 0.0591102, recall 0.864496
2017-12-10T11:36:14.248307: step 2491, loss 0.240688, acc 0.96875, prec 0.0592198, recall 0.864734
2017-12-10T11:36:14.696800: step 2492, loss 0.125205, acc 0.96875, prec 0.0592163, recall 0.864734
2017-12-10T11:36:15.139629: step 2493, loss 0.291646, acc 0.9375, prec 0.059294, recall 0.864912
2017-12-10T11:36:15.592134: step 2494, loss 0.179608, acc 0.953125, prec 0.0592887, recall 0.864912
2017-12-10T11:36:16.030530: step 2495, loss 0.266479, acc 0.90625, prec 0.059278, recall 0.864912
2017-12-10T11:36:16.464334: step 2496, loss 0.0895544, acc 0.953125, prec 0.0592726, recall 0.864912
2017-12-10T11:36:16.905520: step 2497, loss 0.139311, acc 0.953125, prec 0.0593521, recall 0.86509
2017-12-10T11:36:17.358715: step 2498, loss 1.93264, acc 0.921875, prec 0.0593732, recall 0.86477
2017-12-10T11:36:17.799069: step 2499, loss 0.38168, acc 0.921875, prec 0.0593643, recall 0.86477
2017-12-10T11:36:18.242340: step 2500, loss 0.180273, acc 0.9375, prec 0.0593854, recall 0.864829
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2500

2017-12-10T11:36:20.095844: step 2501, loss 0.182254, acc 0.953125, prec 0.0594083, recall 0.864888
2017-12-10T11:36:20.535967: step 2502, loss 0.239632, acc 0.9375, prec 0.0594577, recall 0.865007
2017-12-10T11:36:20.983754: step 2503, loss 0.0847969, acc 0.96875, prec 0.0594541, recall 0.865007
2017-12-10T11:36:21.422624: step 2504, loss 0.0530052, acc 0.984375, prec 0.0594806, recall 0.865066
2017-12-10T11:36:21.863558: step 2505, loss 0.437208, acc 0.953125, prec 0.0595034, recall 0.865124
2017-12-10T11:36:22.291028: step 2506, loss 0.12014, acc 0.96875, prec 0.0595281, recall 0.865183
2017-12-10T11:36:22.726886: step 2507, loss 0.155784, acc 0.953125, prec 0.0595792, recall 0.865301
2017-12-10T11:36:23.173992: step 2508, loss 0.100958, acc 0.953125, prec 0.0595738, recall 0.865301
2017-12-10T11:36:23.613302: step 2509, loss 0.0273025, acc 0.984375, prec 0.059572, recall 0.865301
2017-12-10T11:36:24.061465: step 2510, loss 0.167644, acc 0.921875, prec 0.0595631, recall 0.865301
2017-12-10T11:36:24.499360: step 2511, loss 0.478028, acc 0.953125, prec 0.0596142, recall 0.865418
2017-12-10T11:36:24.936625: step 2512, loss 0.147814, acc 0.96875, prec 0.0596388, recall 0.865477
2017-12-10T11:36:25.377528: step 2513, loss 0.0502987, acc 0.984375, prec 0.0596652, recall 0.865535
2017-12-10T11:36:25.811630: step 2514, loss 0.195312, acc 0.9375, prec 0.0597145, recall 0.865652
2017-12-10T11:36:26.246646: step 2515, loss 0.0412699, acc 1, prec 0.0597145, recall 0.865652
2017-12-10T11:36:26.688179: step 2516, loss 0.289355, acc 0.9375, prec 0.0597355, recall 0.865711
2017-12-10T11:36:27.127461: step 2517, loss 0.0819138, acc 0.96875, prec 0.0597319, recall 0.865711
2017-12-10T11:36:27.569759: step 2518, loss 0.144086, acc 0.96875, prec 0.0597565, recall 0.865769
2017-12-10T11:36:28.026984: step 2519, loss 0.108682, acc 0.953125, prec 0.0597793, recall 0.865827
2017-12-10T11:36:28.475598: step 2520, loss 0.0981078, acc 0.96875, prec 0.059804, recall 0.865885
2017-12-10T11:36:28.915965: step 2521, loss 0.195942, acc 0.9375, prec 0.0597968, recall 0.865885
2017-12-10T11:36:29.356409: step 2522, loss 0.299965, acc 0.921875, prec 0.0597878, recall 0.865885
2017-12-10T11:36:29.805960: step 2523, loss 0.0180239, acc 0.984375, prec 0.0598142, recall 0.865944
2017-12-10T11:36:30.252589: step 2524, loss 0.120595, acc 0.953125, prec 0.059837, recall 0.866002
2017-12-10T11:36:30.697659: step 2525, loss 0.107323, acc 0.953125, prec 0.0598316, recall 0.866002
2017-12-10T11:36:31.145755: step 2526, loss 0.21154, acc 0.953125, prec 0.0598826, recall 0.866118
2017-12-10T11:36:31.579140: step 2527, loss 0.143148, acc 0.9375, prec 0.0599036, recall 0.866176
2017-12-10T11:36:32.025406: step 2528, loss 0.187325, acc 0.953125, prec 0.0598982, recall 0.866176
2017-12-10T11:36:32.460135: step 2529, loss 0.186901, acc 0.96875, prec 0.0598946, recall 0.866176
2017-12-10T11:36:32.901736: step 2530, loss 0.155984, acc 0.96875, prec 0.0599191, recall 0.866234
2017-12-10T11:36:33.335668: step 2531, loss 0.0818924, acc 0.96875, prec 0.0599719, recall 0.866349
2017-12-10T11:36:33.780404: step 2532, loss 0.100603, acc 0.953125, prec 0.0599665, recall 0.866349
2017-12-10T11:36:34.223782: step 2533, loss 0.111899, acc 0.953125, prec 0.0599892, recall 0.866407
2017-12-10T11:36:34.679550: step 2534, loss 0.111034, acc 0.96875, prec 0.0600138, recall 0.866465
2017-12-10T11:36:35.125579: step 2535, loss 0.0566254, acc 0.953125, prec 0.0600646, recall 0.86658
2017-12-10T11:36:35.572604: step 2536, loss 0.0949161, acc 1, prec 0.0600928, recall 0.866638
2017-12-10T11:36:36.016887: step 2537, loss 0.11937, acc 0.96875, prec 0.0600892, recall 0.866638
2017-12-10T11:36:36.466962: step 2538, loss 0.893871, acc 0.984375, prec 0.0601717, recall 0.86681
2017-12-10T11:36:36.911593: step 2539, loss 3.23026, acc 0.9375, prec 0.0601663, recall 0.866437
2017-12-10T11:36:37.355608: step 2540, loss 0.046513, acc 0.984375, prec 0.0601645, recall 0.866437
2017-12-10T11:36:37.785455: step 2541, loss 0.0431285, acc 0.984375, prec 0.060219, recall 0.866552
2017-12-10T11:36:38.222278: step 2542, loss 0.220411, acc 0.96875, prec 0.0602435, recall 0.866609
2017-12-10T11:36:38.653580: step 2543, loss 1.20873, acc 0.953125, prec 0.0602943, recall 0.866724
2017-12-10T11:36:39.101836: step 2544, loss 0.679105, acc 0.859375, prec 0.0602781, recall 0.866724
2017-12-10T11:36:39.545794: step 2545, loss 0.343393, acc 0.890625, prec 0.0602935, recall 0.866781
2017-12-10T11:36:39.991589: step 2546, loss 0.220412, acc 0.90625, prec 0.0602827, recall 0.866781
2017-12-10T11:36:40.413970: step 2547, loss 0.144183, acc 0.9375, prec 0.0602755, recall 0.866781
2017-12-10T11:36:40.851788: step 2548, loss 0.348176, acc 0.90625, prec 0.0602928, recall 0.866839
2017-12-10T11:36:41.289210: step 2549, loss 0.216289, acc 0.90625, prec 0.0603101, recall 0.866896
2017-12-10T11:36:41.728613: step 2550, loss 0.395344, acc 0.890625, prec 0.0603536, recall 0.86701
2017-12-10T11:36:42.176605: step 2551, loss 0.309486, acc 0.90625, prec 0.0603708, recall 0.867067
2017-12-10T11:36:42.631238: step 2552, loss 0.329438, acc 0.953125, prec 0.0603935, recall 0.867124
2017-12-10T11:36:43.067220: step 2553, loss 0.249446, acc 0.875, prec 0.0604071, recall 0.867181
2017-12-10T11:36:43.502377: step 2554, loss 0.400096, acc 0.828125, prec 0.0603873, recall 0.867181
2017-12-10T11:36:43.951203: step 2555, loss 0.544799, acc 0.859375, prec 0.0604271, recall 0.867295
2017-12-10T11:36:44.389808: step 2556, loss 1.35092, acc 0.890625, prec 0.0605265, recall 0.867521
2017-12-10T11:36:44.825323: step 2557, loss 0.613777, acc 0.828125, prec 0.0605067, recall 0.867521
2017-12-10T11:36:45.268098: step 2558, loss 0.216688, acc 0.90625, prec 0.0605239, recall 0.867578
2017-12-10T11:36:45.711187: step 2559, loss 0.391163, acc 0.8125, prec 0.0605302, recall 0.867634
2017-12-10T11:36:46.151242: step 2560, loss 0.193507, acc 0.921875, prec 0.0605212, recall 0.867634
2017-12-10T11:36:46.591499: step 2561, loss 0.253523, acc 0.90625, prec 0.0605664, recall 0.867747
2017-12-10T11:36:47.032507: step 2562, loss 0.252423, acc 0.921875, prec 0.0605853, recall 0.867804
2017-12-10T11:36:47.464875: step 2563, loss 0.43558, acc 0.84375, prec 0.0605952, recall 0.86786
2017-12-10T11:36:47.895435: step 2564, loss 0.608561, acc 0.828125, prec 0.0606034, recall 0.867916
2017-12-10T11:36:48.342701: step 2565, loss 0.391651, acc 0.875, prec 0.0605889, recall 0.867916
2017-12-10T11:36:48.781450: step 2566, loss 0.126285, acc 0.953125, prec 0.0606115, recall 0.867973
2017-12-10T11:36:49.228983: step 2567, loss 0.216338, acc 0.9375, prec 0.0606043, recall 0.867973
2017-12-10T11:36:49.669003: step 2568, loss 0.284929, acc 0.90625, prec 0.0605934, recall 0.867973
2017-12-10T11:36:50.103031: step 2569, loss 0.44549, acc 0.890625, prec 0.0605808, recall 0.867973
2017-12-10T11:36:50.558835: step 2570, loss 2.22394, acc 0.890625, prec 0.060598, recall 0.86766
2017-12-10T11:36:51.016569: step 2571, loss 0.103208, acc 0.9375, prec 0.0605908, recall 0.86766
2017-12-10T11:36:51.462388: step 2572, loss 0.101988, acc 0.96875, prec 0.0605872, recall 0.86766
2017-12-10T11:36:51.907329: step 2573, loss 0.150556, acc 0.90625, prec 0.0606043, recall 0.867716
2017-12-10T11:36:52.345215: step 2574, loss 0.292854, acc 0.921875, prec 0.0605953, recall 0.867716
2017-12-10T11:36:52.776459: step 2575, loss 0.374865, acc 0.921875, prec 0.0605863, recall 0.867716
2017-12-10T11:36:53.212560: step 2576, loss 0.247116, acc 0.9375, prec 0.060607, recall 0.867772
2017-12-10T11:36:53.651822: step 2577, loss 0.163093, acc 0.921875, prec 0.060598, recall 0.867772
2017-12-10T11:36:54.099233: step 2578, loss 0.377984, acc 0.875, prec 0.0606115, recall 0.867828
2017-12-10T11:36:54.533704: step 2579, loss 0.31666, acc 0.90625, prec 0.0606564, recall 0.867941
2017-12-10T11:36:54.984785: step 2580, loss 0.0974252, acc 0.96875, prec 0.0606528, recall 0.867941
2017-12-10T11:36:55.420749: step 2581, loss 0.131914, acc 0.96875, prec 0.0606771, recall 0.867997
2017-12-10T11:36:55.868533: step 2582, loss 0.0918816, acc 0.9375, prec 0.0607256, recall 0.868109
2017-12-10T11:36:56.308318: step 2583, loss 0.799744, acc 0.9375, prec 0.0607741, recall 0.86822
2017-12-10T11:36:56.756221: step 2584, loss 0.188579, acc 0.9375, prec 0.0607669, recall 0.86822
2017-12-10T11:36:57.195657: step 2585, loss 0.152335, acc 0.984375, prec 0.0608208, recall 0.868332
2017-12-10T11:36:57.632787: step 2586, loss 0.156356, acc 0.9375, prec 0.0608136, recall 0.868332
2017-12-10T11:36:58.078964: step 2587, loss 0.0258836, acc 1, prec 0.0608136, recall 0.868332
2017-12-10T11:36:58.526982: step 2588, loss 0.162093, acc 0.921875, prec 0.0608324, recall 0.868388
2017-12-10T11:36:58.967221: step 2589, loss 0.100921, acc 0.921875, prec 0.0608513, recall 0.868443
2017-12-10T11:36:59.406398: step 2590, loss 0.300187, acc 0.875, prec 0.0608647, recall 0.868499
2017-12-10T11:36:59.852831: step 2591, loss 0.165683, acc 0.9375, prec 0.0608853, recall 0.868555
2017-12-10T11:37:00.292421: step 2592, loss 0.41553, acc 0.921875, prec 0.0609041, recall 0.86861
2017-12-10T11:37:00.735250: step 2593, loss 0.153314, acc 0.921875, prec 0.0608951, recall 0.86861
2017-12-10T11:37:01.178075: step 2594, loss 0.136361, acc 0.953125, prec 0.0609175, recall 0.868666
2017-12-10T11:37:01.618406: step 2595, loss 0.0295781, acc 1, prec 0.0609175, recall 0.868666
2017-12-10T11:37:02.048891: step 2596, loss 0.0660162, acc 0.96875, prec 0.0609139, recall 0.868666
2017-12-10T11:37:02.481589: step 2597, loss 0.172463, acc 1, prec 0.0609417, recall 0.868721
2017-12-10T11:37:02.923114: step 2598, loss 0.470082, acc 0.96875, prec 0.0609937, recall 0.868832
2017-12-10T11:37:03.360152: step 2599, loss 0.171923, acc 0.9375, prec 0.0610142, recall 0.868887
2017-12-10T11:37:03.799492: step 2600, loss 0.109509, acc 0.96875, prec 0.0610662, recall 0.868997
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2600

2017-12-10T11:37:05.831550: step 2601, loss 0.315685, acc 0.90625, prec 0.0610832, recall 0.869053
2017-12-10T11:37:06.267579: step 2602, loss 0.1352, acc 0.9375, prec 0.0611037, recall 0.869108
2017-12-10T11:37:06.709438: step 2603, loss 0.176837, acc 0.96875, prec 0.0611001, recall 0.869108
2017-12-10T11:37:07.152100: step 2604, loss 0.0851773, acc 0.953125, prec 0.0610947, recall 0.869108
2017-12-10T11:37:07.596285: step 2605, loss 0.0783567, acc 0.96875, prec 0.0610911, recall 0.869108
2017-12-10T11:37:08.034393: step 2606, loss 0.670124, acc 0.96875, prec 0.061143, recall 0.869218
2017-12-10T11:37:08.472121: step 2607, loss 0.0315236, acc 0.984375, prec 0.061169, recall 0.869273
2017-12-10T11:37:08.902360: step 2608, loss 1.06933, acc 0.984375, prec 0.0612227, recall 0.869383
2017-12-10T11:37:09.340242: step 2609, loss 1.7368, acc 0.96875, prec 0.0612209, recall 0.869018
2017-12-10T11:37:09.788878: step 2610, loss 0.171177, acc 0.9375, prec 0.0612136, recall 0.869018
2017-12-10T11:37:10.230983: step 2611, loss 1.30329, acc 0.890625, prec 0.0612565, recall 0.869128
2017-12-10T11:37:10.681351: step 2612, loss 0.353471, acc 0.90625, prec 0.0613011, recall 0.869237
2017-12-10T11:37:11.137058: step 2613, loss 0.0957175, acc 0.984375, prec 0.061327, recall 0.869292
2017-12-10T11:37:11.608921: step 2614, loss 0.287312, acc 0.921875, prec 0.0613734, recall 0.869401
2017-12-10T11:37:12.041344: step 2615, loss 0.260084, acc 0.859375, prec 0.0613571, recall 0.869401
2017-12-10T11:37:12.477904: step 2616, loss 0.291894, acc 0.890625, prec 0.0613444, recall 0.869401
2017-12-10T11:37:12.914179: step 2617, loss 0.299399, acc 0.9375, prec 0.0613649, recall 0.869456
2017-12-10T11:37:13.367812: step 2618, loss 0.0941962, acc 0.96875, prec 0.061389, recall 0.869511
2017-12-10T11:37:13.819222: step 2619, loss 0.34239, acc 0.890625, prec 0.0613763, recall 0.869511
2017-12-10T11:37:14.252674: step 2620, loss 0.300018, acc 0.90625, prec 0.0613654, recall 0.869511
2017-12-10T11:37:14.696745: step 2621, loss 0.462958, acc 0.859375, prec 0.0613768, recall 0.869565
2017-12-10T11:37:15.136768: step 2622, loss 0.457897, acc 0.8125, prec 0.0613551, recall 0.869565
2017-12-10T11:37:15.583408: step 2623, loss 0.431639, acc 0.875, prec 0.0613406, recall 0.869565
2017-12-10T11:37:16.034905: step 2624, loss 0.13342, acc 0.9375, prec 0.0613611, recall 0.86962
2017-12-10T11:37:16.471401: step 2625, loss 0.138459, acc 0.9375, prec 0.0613815, recall 0.869674
2017-12-10T11:37:16.914362: step 2626, loss 0.401823, acc 0.890625, prec 0.0613965, recall 0.869729
2017-12-10T11:37:17.347434: step 2627, loss 0.264654, acc 0.90625, prec 0.0614133, recall 0.869783
2017-12-10T11:37:17.780834: step 2628, loss 0.511399, acc 0.890625, prec 0.061456, recall 0.869892
2017-12-10T11:37:18.218466: step 2629, loss 0.0505933, acc 0.984375, prec 0.0614542, recall 0.869892
2017-12-10T11:37:18.665127: step 2630, loss 0.0864027, acc 0.953125, prec 0.061504, recall 0.87
2017-12-10T11:37:19.109581: step 2631, loss 0.446209, acc 0.921875, prec 0.0615226, recall 0.870054
2017-12-10T11:37:19.561966: step 2632, loss 0.31429, acc 0.921875, prec 0.0615688, recall 0.870162
2017-12-10T11:37:20.012442: step 2633, loss 0.158266, acc 0.921875, prec 0.0615597, recall 0.870162
2017-12-10T11:37:20.466932: step 2634, loss 0.338308, acc 0.90625, prec 0.0615765, recall 0.870216
2017-12-10T11:37:20.914090: step 2635, loss 0.176823, acc 0.984375, prec 0.0616575, recall 0.870378
2017-12-10T11:37:21.351988: step 2636, loss 0.115141, acc 0.96875, prec 0.0616539, recall 0.870378
2017-12-10T11:37:21.799872: step 2637, loss 1.04288, acc 0.984375, prec 0.0617073, recall 0.870486
2017-12-10T11:37:22.250468: step 2638, loss 0.0693168, acc 0.953125, prec 0.0617295, recall 0.870539
2017-12-10T11:37:22.692438: step 2639, loss 0.164004, acc 0.953125, prec 0.0617516, recall 0.870593
2017-12-10T11:37:23.135465: step 2640, loss 0.180456, acc 0.953125, prec 0.0617462, recall 0.870593
2017-12-10T11:37:23.574439: step 2641, loss 0.395931, acc 0.953125, prec 0.0617407, recall 0.870593
2017-12-10T11:37:24.015615: step 2642, loss 0.23321, acc 0.921875, prec 0.0617317, recall 0.870593
2017-12-10T11:37:24.460821: step 2643, loss 0.0256742, acc 1, prec 0.0617317, recall 0.870593
2017-12-10T11:37:24.894208: step 2644, loss 0.0964767, acc 0.953125, prec 0.0617262, recall 0.870593
2017-12-10T11:37:25.340761: step 2645, loss 0.0913003, acc 0.921875, prec 0.0617171, recall 0.870593
2017-12-10T11:37:25.788554: step 2646, loss 0.186567, acc 0.96875, prec 0.0617411, recall 0.870647
2017-12-10T11:37:26.230892: step 2647, loss 2.00012, acc 0.96875, prec 0.061822, recall 0.870447
2017-12-10T11:37:26.679661: step 2648, loss 0.262623, acc 0.875, prec 0.0618075, recall 0.870447
2017-12-10T11:37:27.122775: step 2649, loss 0.576149, acc 0.84375, prec 0.061872, recall 0.870608
2017-12-10T11:37:27.556933: step 2650, loss 0.357552, acc 0.890625, prec 0.0618593, recall 0.870608
2017-12-10T11:37:28.000708: step 2651, loss 0.36043, acc 0.90625, prec 0.0618484, recall 0.870608
2017-12-10T11:37:28.444423: step 2652, loss 0.356277, acc 0.890625, prec 0.0618632, recall 0.870661
2017-12-10T11:37:28.879612: step 2653, loss 0.0895458, acc 0.96875, prec 0.0618596, recall 0.870661
2017-12-10T11:37:29.322719: step 2654, loss 0.136382, acc 0.953125, prec 0.0618542, recall 0.870661
2017-12-10T11:37:29.767549: step 2655, loss 0.214858, acc 0.9375, prec 0.061902, recall 0.870768
2017-12-10T11:37:30.199365: step 2656, loss 0.174788, acc 0.921875, prec 0.0618929, recall 0.870768
2017-12-10T11:37:30.635326: step 2657, loss 0.370202, acc 0.890625, prec 0.0618802, recall 0.870768
2017-12-10T11:37:31.080734: step 2658, loss 0.312668, acc 0.890625, prec 0.061895, recall 0.870821
2017-12-10T11:37:31.514711: step 2659, loss 0.422012, acc 0.890625, prec 0.0618823, recall 0.870821
2017-12-10T11:37:31.956205: step 2660, loss 0.266234, acc 0.921875, prec 0.0618732, recall 0.870821
2017-12-10T11:37:32.393666: step 2661, loss 0.338097, acc 0.890625, prec 0.061888, recall 0.870875
2017-12-10T11:37:32.830641: step 2662, loss 0.2349, acc 0.9375, prec 0.0619083, recall 0.870928
2017-12-10T11:37:33.277833: step 2663, loss 0.356738, acc 0.921875, prec 0.0619267, recall 0.870981
2017-12-10T11:37:33.724822: step 2664, loss 0.432711, acc 0.96875, prec 0.0619505, recall 0.871034
2017-12-10T11:37:34.169954: step 2665, loss 0.724432, acc 0.875, prec 0.061991, recall 0.87114
2017-12-10T11:37:34.607848: step 2666, loss 0.20452, acc 0.953125, prec 0.062013, recall 0.871193
2017-12-10T11:37:35.045099: step 2667, loss 0.197441, acc 0.953125, prec 0.0620625, recall 0.871299
2017-12-10T11:37:35.494809: step 2668, loss 0.300972, acc 0.9375, prec 0.0620827, recall 0.871352
2017-12-10T11:37:35.934438: step 2669, loss 0.155745, acc 0.921875, prec 0.0620736, recall 0.871352
2017-12-10T11:37:36.377650: step 2670, loss 0.263848, acc 0.90625, prec 0.0621451, recall 0.871511
2017-12-10T11:37:36.830281: step 2671, loss 0.299383, acc 0.953125, prec 0.0621671, recall 0.871563
2017-12-10T11:37:37.266090: step 2672, loss 0.234698, acc 0.90625, prec 0.0621562, recall 0.871563
2017-12-10T11:37:37.698971: step 2673, loss 0.122441, acc 0.953125, prec 0.0621507, recall 0.871563
2017-12-10T11:37:38.131892: step 2674, loss 0.288115, acc 0.953125, prec 0.0622001, recall 0.871669
2017-12-10T11:37:38.573765: step 2675, loss 0.207243, acc 0.96875, prec 0.0622239, recall 0.871721
2017-12-10T11:37:39.016165: step 2676, loss 0.219571, acc 0.96875, prec 0.0622751, recall 0.871826
2017-12-10T11:37:39.456275: step 2677, loss 0.704691, acc 0.984375, prec 0.0623007, recall 0.871879
2017-12-10T11:37:39.909109: step 2678, loss 0.0499739, acc 0.984375, prec 0.0623263, recall 0.871931
2017-12-10T11:37:40.339925: step 2679, loss 0.240718, acc 0.921875, prec 0.0623172, recall 0.871931
2017-12-10T11:37:40.782952: step 2680, loss 0.0610464, acc 0.984375, prec 0.0623428, recall 0.871984
2017-12-10T11:37:41.224454: step 2681, loss 0.0960611, acc 0.953125, prec 0.0623648, recall 0.872036
2017-12-10T11:37:41.660840: step 2682, loss 0.0794935, acc 0.96875, prec 0.0623885, recall 0.872088
2017-12-10T11:37:42.097083: step 2683, loss 0.146174, acc 0.953125, prec 0.0623831, recall 0.872088
2017-12-10T11:37:42.550871: step 2684, loss 0.0810701, acc 0.984375, prec 0.0624087, recall 0.872141
2017-12-10T11:37:42.997634: step 2685, loss 1.30802, acc 0.953125, prec 0.062405, recall 0.871784
2017-12-10T11:37:43.451402: step 2686, loss 0.145401, acc 0.96875, prec 0.0624288, recall 0.871837
2017-12-10T11:37:43.903461: step 2687, loss 0.0205647, acc 1, prec 0.0624288, recall 0.871837
2017-12-10T11:37:44.344239: step 2688, loss 0.230137, acc 0.9375, prec 0.0624215, recall 0.871837
2017-12-10T11:37:44.794980: step 2689, loss 2.89348, acc 0.921875, prec 0.0624416, recall 0.871533
2017-12-10T11:37:45.243456: step 2690, loss 3.66175, acc 0.9375, prec 0.0624361, recall 0.871178
2017-12-10T11:37:45.693745: step 2691, loss 0.220398, acc 0.921875, prec 0.062427, recall 0.871178
2017-12-10T11:37:46.131108: step 2692, loss 0.242509, acc 0.90625, prec 0.0624434, recall 0.871231
2017-12-10T11:37:46.572836: step 2693, loss 0.641644, acc 0.8125, prec 0.0624489, recall 0.871283
2017-12-10T11:37:47.030405: step 2694, loss 0.990318, acc 0.75, prec 0.0624471, recall 0.871336
2017-12-10T11:37:47.463974: step 2695, loss 0.429145, acc 0.859375, prec 0.0624307, recall 0.871336
2017-12-10T11:37:47.913149: step 2696, loss 0.764109, acc 0.734375, prec 0.0624271, recall 0.871388
2017-12-10T11:37:48.350928: step 2697, loss 0.664462, acc 0.765625, prec 0.0624271, recall 0.87144
2017-12-10T11:37:48.792809: step 2698, loss 0.592732, acc 0.8125, prec 0.0624326, recall 0.871493
2017-12-10T11:37:49.231805: step 2699, loss 1.13623, acc 0.75, prec 0.0624308, recall 0.871545
2017-12-10T11:37:49.669348: step 2700, loss 0.703009, acc 0.75, prec 0.0624291, recall 0.871597
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2700

2017-12-10T11:37:51.561480: step 2701, loss 1.14475, acc 0.6875, prec 0.06242, recall 0.871649
2017-12-10T11:37:52.004113: step 2702, loss 0.70053, acc 0.78125, prec 0.0624219, recall 0.871701
2017-12-10T11:37:52.441636: step 2703, loss 0.662254, acc 0.78125, prec 0.0623965, recall 0.871701
2017-12-10T11:37:52.881245: step 2704, loss 0.750866, acc 0.78125, prec 0.0623711, recall 0.871701
2017-12-10T11:37:53.334283: step 2705, loss 0.508669, acc 0.828125, prec 0.0623512, recall 0.871701
2017-12-10T11:37:53.774645: step 2706, loss 0.520246, acc 0.890625, prec 0.0623385, recall 0.871701
2017-12-10T11:37:54.217580: step 2707, loss 0.514371, acc 0.875, prec 0.0623785, recall 0.871805
2017-12-10T11:37:54.658954: step 2708, loss 0.378287, acc 0.890625, prec 0.0623658, recall 0.871805
2017-12-10T11:37:55.105172: step 2709, loss 0.3587, acc 0.90625, prec 0.0623549, recall 0.871805
2017-12-10T11:37:55.564747: step 2710, loss 0.153793, acc 0.921875, prec 0.0623731, recall 0.871857
2017-12-10T11:37:55.995095: step 2711, loss 0.480309, acc 0.890625, prec 0.0623604, recall 0.871857
2017-12-10T11:37:56.434493: step 2712, loss 0.194177, acc 0.9375, prec 0.0623532, recall 0.871857
2017-12-10T11:37:56.879399: step 2713, loss 0.205479, acc 0.96875, prec 0.0623496, recall 0.871857
2017-12-10T11:37:57.326510: step 2714, loss 0.273404, acc 0.890625, prec 0.0623369, recall 0.871857
2017-12-10T11:37:57.760525: step 2715, loss 0.865184, acc 0.953125, prec 0.0623587, recall 0.871909
2017-12-10T11:37:58.205809: step 2716, loss 0.380149, acc 0.96875, prec 0.0624094, recall 0.872013
2017-12-10T11:37:58.632476: step 2717, loss 0.0851981, acc 0.984375, prec 0.0624348, recall 0.872065
2017-12-10T11:37:59.073919: step 2718, loss 0.253772, acc 0.921875, prec 0.0624529, recall 0.872117
2017-12-10T11:37:59.512196: step 2719, loss 0.0146736, acc 1, prec 0.0624529, recall 0.872117
2017-12-10T11:37:59.955038: step 2720, loss 0.0988933, acc 0.96875, prec 0.0624493, recall 0.872117
2017-12-10T11:38:00.385810: step 2721, loss 0.162673, acc 0.953125, prec 0.0624439, recall 0.872117
2017-12-10T11:38:00.826849: step 2722, loss 1.68074, acc 0.953125, prec 0.0624402, recall 0.871764
2017-12-10T11:38:01.267379: step 2723, loss 0.0472472, acc 0.96875, prec 0.0624366, recall 0.871764
2017-12-10T11:38:01.707447: step 2724, loss 2.2989, acc 0.953125, prec 0.062433, recall 0.871411
2017-12-10T11:38:02.150407: step 2725, loss 0.134644, acc 0.96875, prec 0.0624837, recall 0.871515
2017-12-10T11:38:02.582145: step 2726, loss 0.141064, acc 0.984375, prec 0.0625091, recall 0.871567
2017-12-10T11:38:03.020380: step 2727, loss 0.0491864, acc 0.984375, prec 0.0625072, recall 0.871567
2017-12-10T11:38:03.467037: step 2728, loss 0.414414, acc 0.921875, prec 0.0624982, recall 0.871567
2017-12-10T11:38:03.916608: step 2729, loss 0.43443, acc 0.9375, prec 0.0625724, recall 0.871722
2017-12-10T11:38:04.361687: step 2730, loss 0.232564, acc 0.890625, prec 0.0625868, recall 0.871774
2017-12-10T11:38:04.802289: step 2731, loss 0.0862954, acc 0.96875, prec 0.0626104, recall 0.871826
2017-12-10T11:38:05.235418: step 2732, loss 0.597514, acc 0.890625, prec 0.0626791, recall 0.871981
2017-12-10T11:38:05.675425: step 2733, loss 0.463545, acc 0.90625, prec 0.0626682, recall 0.871981
2017-12-10T11:38:06.119756: step 2734, loss 0.0839649, acc 0.984375, prec 0.0626935, recall 0.872032
2017-12-10T11:38:06.555741: step 2735, loss 0.365833, acc 0.921875, prec 0.0626844, recall 0.872032
2017-12-10T11:38:07.003950: step 2736, loss 0.184451, acc 0.9375, prec 0.0627043, recall 0.872084
2017-12-10T11:38:07.434827: step 2737, loss 0.489989, acc 0.953125, prec 0.0627801, recall 0.872238
2017-12-10T11:38:07.889554: step 2738, loss 0.397253, acc 0.96875, prec 0.0628036, recall 0.872289
2017-12-10T11:38:08.320356: step 2739, loss 0.192332, acc 0.921875, prec 0.0628487, recall 0.872392
2017-12-10T11:38:08.758459: step 2740, loss 0.608412, acc 0.859375, prec 0.0628595, recall 0.872443
2017-12-10T11:38:09.193916: step 2741, loss 0.368046, acc 0.90625, prec 0.0628756, recall 0.872494
2017-12-10T11:38:09.642217: step 2742, loss 0.0999877, acc 0.96875, prec 0.062872, recall 0.872494
2017-12-10T11:38:10.089112: step 2743, loss 0.149981, acc 0.953125, prec 0.0628666, recall 0.872494
2017-12-10T11:38:10.533254: step 2744, loss 0.46346, acc 0.859375, prec 0.0628502, recall 0.872494
2017-12-10T11:38:10.970968: step 2745, loss 0.232202, acc 0.90625, prec 0.0628934, recall 0.872596
2017-12-10T11:38:11.406754: step 2746, loss 0.323365, acc 0.9375, prec 0.0629673, recall 0.872749
2017-12-10T11:38:11.861580: step 2747, loss 0.160975, acc 0.953125, prec 0.063016, recall 0.872851
2017-12-10T11:38:12.305236: step 2748, loss 0.693158, acc 0.90625, prec 0.0630321, recall 0.872902
2017-12-10T11:38:12.749221: step 2749, loss 0.441012, acc 0.9375, prec 0.0630519, recall 0.872952
2017-12-10T11:38:13.197886: step 2750, loss 0.282808, acc 0.953125, prec 0.0630735, recall 0.873003
2017-12-10T11:38:13.630500: step 2751, loss 0.0742785, acc 0.96875, prec 0.0630698, recall 0.873003
2017-12-10T11:38:14.077909: step 2752, loss 0.202129, acc 0.9375, prec 0.0630625, recall 0.873003
2017-12-10T11:38:14.514445: step 2753, loss 0.100498, acc 0.953125, prec 0.0631111, recall 0.873105
2017-12-10T11:38:14.951101: step 2754, loss 0.22259, acc 0.9375, prec 0.0631309, recall 0.873155
2017-12-10T11:38:15.395907: step 2755, loss 0.142779, acc 0.953125, prec 0.0631254, recall 0.873155
2017-12-10T11:38:15.836998: step 2756, loss 0.0507499, acc 0.984375, prec 0.0631776, recall 0.873256
2017-12-10T11:38:16.265762: step 2757, loss 0.263924, acc 0.890625, prec 0.0631649, recall 0.873256
2017-12-10T11:38:16.698732: step 2758, loss 0.0754586, acc 0.96875, prec 0.0631612, recall 0.873256
2017-12-10T11:38:17.142988: step 2759, loss 0.0849426, acc 0.953125, prec 0.0631828, recall 0.873307
2017-12-10T11:38:17.602601: step 2760, loss 0.101158, acc 0.953125, prec 0.0632043, recall 0.873357
2017-12-10T11:38:18.027954: step 2761, loss 0.100715, acc 0.984375, prec 0.0632295, recall 0.873408
2017-12-10T11:38:18.454698: step 2762, loss 0.175004, acc 0.96875, prec 0.0632528, recall 0.873458
2017-12-10T11:38:18.893321: step 2763, loss 0.0338923, acc 0.984375, prec 0.063305, recall 0.873559
2017-12-10T11:38:19.328718: step 2764, loss 0.242016, acc 1, prec 0.063359, recall 0.873659
2017-12-10T11:38:19.774648: step 2765, loss 0.0906557, acc 0.96875, prec 0.0633823, recall 0.873709
2017-12-10T11:38:20.214811: step 2766, loss 0.0374392, acc 0.984375, prec 0.0633805, recall 0.873709
2017-12-10T11:38:20.657476: step 2767, loss 0.155091, acc 0.953125, prec 0.063402, recall 0.873759
2017-12-10T11:38:21.090557: step 2768, loss 0.0302392, acc 1, prec 0.063402, recall 0.873759
2017-12-10T11:38:21.539503: step 2769, loss 1.68819, acc 0.96875, prec 0.0634541, recall 0.873513
2017-12-10T11:38:21.978875: step 2770, loss 0.0895977, acc 0.96875, prec 0.0634505, recall 0.873513
2017-12-10T11:38:22.438491: step 2771, loss 0.0219981, acc 0.984375, prec 0.0634486, recall 0.873513
2017-12-10T11:38:22.873096: step 2772, loss 0.0548287, acc 0.984375, prec 0.0635007, recall 0.873613
2017-12-10T11:38:23.311268: step 2773, loss 0.189751, acc 0.96875, prec 0.0634971, recall 0.873613
2017-12-10T11:38:23.753646: step 2774, loss 0.0358475, acc 0.984375, prec 0.0635492, recall 0.873713
2017-12-10T11:38:24.207746: step 2775, loss 1.04062, acc 0.96875, prec 0.0635725, recall 0.873763
2017-12-10T11:38:24.654276: step 2776, loss 0.0645851, acc 0.96875, prec 0.0635688, recall 0.873763
2017-12-10T11:38:25.113094: step 2777, loss 0.293111, acc 0.953125, prec 0.0635903, recall 0.873813
2017-12-10T11:38:25.556947: step 2778, loss 0.367147, acc 0.921875, prec 0.0635812, recall 0.873813
2017-12-10T11:38:26.001135: step 2779, loss 0.232175, acc 0.921875, prec 0.063599, recall 0.873863
2017-12-10T11:38:26.437991: step 2780, loss 0.179676, acc 0.9375, prec 0.0635916, recall 0.873863
2017-12-10T11:38:26.873201: step 2781, loss 0.781851, acc 0.953125, prec 0.06364, recall 0.873963
2017-12-10T11:38:27.306107: step 2782, loss 0.473681, acc 0.859375, prec 0.0636235, recall 0.873963
2017-12-10T11:38:27.743713: step 2783, loss 0.19498, acc 0.96875, prec 0.0636468, recall 0.874013
2017-12-10T11:38:28.193272: step 2784, loss 0.384357, acc 0.921875, prec 0.0636646, recall 0.874062
2017-12-10T11:38:28.638117: step 2785, loss 0.456644, acc 0.859375, prec 0.0636481, recall 0.874062
2017-12-10T11:38:29.075441: step 2786, loss 0.148815, acc 0.921875, prec 0.0636659, recall 0.874112
2017-12-10T11:38:29.517235: step 2787, loss 0.286784, acc 0.953125, prec 0.0636873, recall 0.874162
2017-12-10T11:38:29.948657: step 2788, loss 0.103715, acc 0.96875, prec 0.0637375, recall 0.874261
2017-12-10T11:38:30.380274: step 2789, loss 0.304616, acc 0.90625, prec 0.0637534, recall 0.87431
2017-12-10T11:38:30.807773: step 2790, loss 0.238345, acc 0.921875, prec 0.0637442, recall 0.87431
2017-12-10T11:38:31.256195: step 2791, loss 0.286554, acc 0.890625, prec 0.0637314, recall 0.87431
2017-12-10T11:38:31.697187: step 2792, loss 0.129397, acc 0.9375, prec 0.0637241, recall 0.87431
2017-12-10T11:38:32.142413: step 2793, loss 0.472024, acc 0.875, prec 0.0637363, recall 0.87436
2017-12-10T11:38:32.586957: step 2794, loss 0.298012, acc 0.890625, prec 0.0637773, recall 0.874459
2017-12-10T11:38:33.043060: step 2795, loss 0.281552, acc 0.921875, prec 0.063795, recall 0.874508
2017-12-10T11:38:33.478059: step 2796, loss 0.175563, acc 0.90625, prec 0.063784, recall 0.874508
2017-12-10T11:38:33.929705: step 2797, loss 0.137965, acc 0.984375, prec 0.063809, recall 0.874558
2017-12-10T11:38:34.381992: step 2798, loss 0.214651, acc 0.9375, prec 0.0638823, recall 0.874705
2017-12-10T11:38:34.819901: step 2799, loss 0.189627, acc 0.953125, prec 0.0639036, recall 0.874755
2017-12-10T11:38:35.275432: step 2800, loss 0.0556004, acc 0.984375, prec 0.0639018, recall 0.874755
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2800

2017-12-10T11:38:37.312597: step 2801, loss 0.0492345, acc 0.984375, prec 0.0639, recall 0.874755
2017-12-10T11:38:37.741753: step 2802, loss 0.112586, acc 0.96875, prec 0.0638963, recall 0.874755
2017-12-10T11:38:38.178430: step 2803, loss 0.0877122, acc 0.953125, prec 0.0638908, recall 0.874755
2017-12-10T11:38:38.622553: step 2804, loss 0.0366593, acc 0.984375, prec 0.063889, recall 0.874755
2017-12-10T11:38:39.057794: step 2805, loss 0.0729788, acc 0.984375, prec 0.063914, recall 0.874804
2017-12-10T11:38:39.492659: step 2806, loss 0.0540938, acc 0.96875, prec 0.0639103, recall 0.874804
2017-12-10T11:38:39.933052: step 2807, loss 0.181858, acc 0.9375, prec 0.0639298, recall 0.874853
2017-12-10T11:38:40.387413: step 2808, loss 0.197641, acc 0.96875, prec 0.063953, recall 0.874902
2017-12-10T11:38:40.825236: step 2809, loss 0.040579, acc 0.984375, prec 0.063978, recall 0.874951
2017-12-10T11:38:41.273216: step 2810, loss 2.01656, acc 0.96875, prec 0.0640835, recall 0.874804
2017-12-10T11:38:41.715321: step 2811, loss 0.052604, acc 1, prec 0.0641371, recall 0.874902
2017-12-10T11:38:42.148693: step 2812, loss 0.197043, acc 0.953125, prec 0.0641584, recall 0.874951
2017-12-10T11:38:42.591996: step 2813, loss 0.310887, acc 0.875, prec 0.0641437, recall 0.874951
2017-12-10T11:38:43.031888: step 2814, loss 0.0713444, acc 0.96875, prec 0.0641936, recall 0.875049
2017-12-10T11:38:43.482548: step 2815, loss 0.0957935, acc 0.96875, prec 0.06419, recall 0.875049
2017-12-10T11:38:43.933345: step 2816, loss 0.193783, acc 0.9375, prec 0.0642094, recall 0.875098
2017-12-10T11:38:44.373036: step 2817, loss 0.0883118, acc 0.9375, prec 0.0642021, recall 0.875098
2017-12-10T11:38:44.815203: step 2818, loss 0.539287, acc 0.9375, prec 0.0642751, recall 0.875244
2017-12-10T11:38:45.252438: step 2819, loss 2.29867, acc 0.96875, prec 0.0642732, recall 0.874903
2017-12-10T11:38:45.700065: step 2820, loss 0.495666, acc 0.890625, prec 0.0642871, recall 0.874951
2017-12-10T11:38:46.148360: step 2821, loss 0.307421, acc 0.875, prec 0.0642724, recall 0.874951
2017-12-10T11:38:46.598699: step 2822, loss 0.0949292, acc 0.984375, prec 0.0643241, recall 0.875049
2017-12-10T11:38:47.035888: step 2823, loss 0.169135, acc 0.9375, prec 0.0643435, recall 0.875097
2017-12-10T11:38:47.472580: step 2824, loss 0.304367, acc 0.90625, prec 0.0643593, recall 0.875146
2017-12-10T11:38:47.899793: step 2825, loss 0.196814, acc 0.890625, prec 0.0643731, recall 0.875194
2017-12-10T11:38:48.334578: step 2826, loss 0.210541, acc 0.953125, prec 0.0643676, recall 0.875194
2017-12-10T11:38:48.781910: step 2827, loss 0.507676, acc 0.890625, prec 0.0643547, recall 0.875194
2017-12-10T11:38:49.229444: step 2828, loss 0.140472, acc 0.953125, prec 0.064376, recall 0.875243
2017-12-10T11:38:49.670789: step 2829, loss 0.388576, acc 0.921875, prec 0.0643935, recall 0.875291
2017-12-10T11:38:50.108142: step 2830, loss 0.738911, acc 0.90625, prec 0.0644359, recall 0.875388
2017-12-10T11:38:50.535436: step 2831, loss 0.449401, acc 0.828125, prec 0.0644157, recall 0.875388
2017-12-10T11:38:50.973212: step 2832, loss 1.32934, acc 0.875, prec 0.0644544, recall 0.875485
2017-12-10T11:38:51.412246: step 2833, loss 0.229922, acc 0.890625, prec 0.0644949, recall 0.875581
2017-12-10T11:38:51.857818: step 2834, loss 0.431061, acc 0.90625, prec 0.0644839, recall 0.875581
2017-12-10T11:38:52.292465: step 2835, loss 0.253935, acc 0.953125, prec 0.0645051, recall 0.87563
2017-12-10T11:38:52.737772: step 2836, loss 0.318657, acc 0.90625, prec 0.064494, recall 0.87563
2017-12-10T11:38:53.189012: step 2837, loss 0.495872, acc 0.875, prec 0.0645327, recall 0.875726
2017-12-10T11:38:53.629364: step 2838, loss 0.65126, acc 0.84375, prec 0.0645143, recall 0.875726
2017-12-10T11:38:54.066485: step 2839, loss 0.378844, acc 0.859375, prec 0.0644977, recall 0.875726
2017-12-10T11:38:54.501596: step 2840, loss 0.265388, acc 0.875, prec 0.0645364, recall 0.875822
2017-12-10T11:38:54.942997: step 2841, loss 0.247672, acc 0.9375, prec 0.0645557, recall 0.87587
2017-12-10T11:38:55.386089: step 2842, loss 0.149585, acc 0.9375, prec 0.0645483, recall 0.87587
2017-12-10T11:38:55.824858: step 2843, loss 0.513691, acc 0.875, prec 0.0645336, recall 0.87587
2017-12-10T11:38:56.278468: step 2844, loss 0.187378, acc 0.953125, prec 0.0645547, recall 0.875918
2017-12-10T11:38:56.736989: step 2845, loss 0.0639737, acc 0.984375, prec 0.0646062, recall 0.876014
2017-12-10T11:38:57.177248: step 2846, loss 0.318643, acc 0.921875, prec 0.064597, recall 0.876014
2017-12-10T11:38:57.620539: step 2847, loss 0.135922, acc 0.96875, prec 0.0645933, recall 0.876014
2017-12-10T11:38:58.066175: step 2848, loss 0.388897, acc 0.953125, prec 0.0646411, recall 0.87611
2017-12-10T11:38:58.512689: step 2849, loss 0.242272, acc 0.90625, prec 0.0646566, recall 0.876157
2017-12-10T11:38:58.950151: step 2850, loss 0.134777, acc 0.953125, prec 0.0647044, recall 0.876253
2017-12-10T11:38:59.398297: step 2851, loss 0.248782, acc 0.921875, prec 0.0647484, recall 0.876348
2017-12-10T11:38:59.840620: step 2852, loss 0.401155, acc 0.90625, prec 0.0647906, recall 0.876443
2017-12-10T11:39:00.287660: step 2853, loss 0.203869, acc 0.96875, prec 0.0648135, recall 0.876491
2017-12-10T11:39:00.731764: step 2854, loss 11.5854, acc 0.9375, prec 0.0648346, recall 0.876201
2017-12-10T11:39:01.171877: step 2855, loss 0.0975364, acc 0.953125, prec 0.0648556, recall 0.876249
2017-12-10T11:39:01.610998: step 2856, loss 0.203989, acc 0.953125, prec 0.0648501, recall 0.876249
2017-12-10T11:39:02.046575: step 2857, loss 0.138562, acc 0.9375, prec 0.0648959, recall 0.876344
2017-12-10T11:39:02.497521: step 2858, loss 0.0654113, acc 0.96875, prec 0.0649188, recall 0.876392
2017-12-10T11:39:02.944979: step 2859, loss 1.37306, acc 0.921875, prec 0.0649893, recall 0.876534
2017-12-10T11:39:03.397059: step 2860, loss 0.193807, acc 0.9375, prec 0.064982, recall 0.876534
2017-12-10T11:39:03.845066: step 2861, loss 0.206649, acc 0.921875, prec 0.0649993, recall 0.876581
2017-12-10T11:39:04.300340: step 2862, loss 0.278186, acc 0.90625, prec 0.0650148, recall 0.876628
2017-12-10T11:39:04.752785: step 2863, loss 0.440632, acc 0.84375, prec 0.0650229, recall 0.876676
2017-12-10T11:39:05.190450: step 2864, loss 2.2332, acc 0.796875, prec 0.0650007, recall 0.87634
2017-12-10T11:39:05.635716: step 2865, loss 0.424858, acc 0.859375, prec 0.0649841, recall 0.87634
2017-12-10T11:39:06.075565: step 2866, loss 0.251347, acc 0.921875, prec 0.0650014, recall 0.876387
2017-12-10T11:39:06.522243: step 2867, loss 0.464491, acc 0.875, prec 0.0650397, recall 0.876482
2017-12-10T11:39:06.963693: step 2868, loss 0.470789, acc 0.8125, prec 0.0650706, recall 0.876576
2017-12-10T11:39:07.402776: step 2869, loss 0.945707, acc 0.765625, prec 0.065043, recall 0.876576
2017-12-10T11:39:07.835767: step 2870, loss 0.540689, acc 0.8125, prec 0.0650738, recall 0.87667
2017-12-10T11:39:08.266788: step 2871, loss 0.530719, acc 0.828125, prec 0.0650536, recall 0.87667
2017-12-10T11:39:08.708310: step 2872, loss 0.410792, acc 0.90625, prec 0.065069, recall 0.876718
2017-12-10T11:39:09.147820: step 2873, loss 0.524875, acc 0.875, prec 0.0650807, recall 0.876765
2017-12-10T11:39:09.590405: step 2874, loss 0.105335, acc 0.96875, prec 0.06513, recall 0.876859
2017-12-10T11:39:10.037356: step 2875, loss 0.508171, acc 0.84375, prec 0.065138, recall 0.876906
2017-12-10T11:39:10.479258: step 2876, loss 0.383981, acc 0.90625, prec 0.0651269, recall 0.876906
2017-12-10T11:39:10.918027: step 2877, loss 0.238853, acc 0.90625, prec 0.0651688, recall 0.876999
2017-12-10T11:39:11.355624: step 2878, loss 0.29034, acc 0.9375, prec 0.0651614, recall 0.876999
2017-12-10T11:39:11.796056: step 2879, loss 0.302826, acc 0.859375, prec 0.0651448, recall 0.876999
2017-12-10T11:39:12.230680: step 2880, loss 0.281265, acc 0.875, prec 0.065183, recall 0.877093
2017-12-10T11:39:12.683203: step 2881, loss 0.507043, acc 0.8125, prec 0.0651609, recall 0.877093
2017-12-10T11:39:13.121842: step 2882, loss 0.29922, acc 0.921875, prec 0.0651781, recall 0.87714
2017-12-10T11:39:13.571198: step 2883, loss 0.318915, acc 0.90625, prec 0.0651934, recall 0.877186
2017-12-10T11:39:14.010701: step 2884, loss 0.469651, acc 0.953125, prec 0.0652143, recall 0.877233
2017-12-10T11:39:14.448696: step 2885, loss 0.199223, acc 0.953125, prec 0.0652088, recall 0.877233
2017-12-10T11:39:14.892522: step 2886, loss 0.166381, acc 0.9375, prec 0.0652278, recall 0.87728
2017-12-10T11:39:15.331588: step 2887, loss 0.318206, acc 0.9375, prec 0.0652469, recall 0.877326
2017-12-10T11:39:15.778404: step 2888, loss 0.678358, acc 0.921875, prec 0.0652904, recall 0.877419
2017-12-10T11:39:16.226945: step 2889, loss 0.0836787, acc 0.96875, prec 0.0653132, recall 0.877466
2017-12-10T11:39:16.682683: step 2890, loss 0.191067, acc 0.953125, prec 0.0653076, recall 0.877466
2017-12-10T11:39:17.127250: step 2891, loss 0.0179726, acc 1, prec 0.065334, recall 0.877512
2017-12-10T11:39:17.561501: step 2892, loss 0.181727, acc 0.9375, prec 0.0653266, recall 0.877512
2017-12-10T11:39:18.006213: step 2893, loss 0.438835, acc 0.96875, prec 0.0653493, recall 0.877559
2017-12-10T11:39:18.448736: step 2894, loss 0.0705214, acc 0.984375, prec 0.0654266, recall 0.877698
2017-12-10T11:39:18.890955: step 2895, loss 4.22413, acc 0.9375, prec 0.0654211, recall 0.877366
2017-12-10T11:39:19.341651: step 2896, loss 0.0802156, acc 0.984375, prec 0.0654192, recall 0.877366
2017-12-10T11:39:19.773423: step 2897, loss 0.112814, acc 0.96875, prec 0.0654419, recall 0.877412
2017-12-10T11:39:20.209835: step 2898, loss 0.180503, acc 0.953125, prec 0.0654628, recall 0.877458
2017-12-10T11:39:20.650985: step 2899, loss 0.924642, acc 0.96875, prec 0.0655382, recall 0.877597
2017-12-10T11:39:21.097702: step 2900, loss 0.368611, acc 0.90625, prec 0.0655271, recall 0.877597
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-2900

2017-12-10T11:39:23.136878: step 2901, loss 0.47907, acc 0.921875, prec 0.0655705, recall 0.87769
2017-12-10T11:39:23.565235: step 2902, loss 4.29161, acc 0.921875, prec 0.0655631, recall 0.877358
2017-12-10T11:39:23.999656: step 2903, loss 0.146617, acc 0.953125, prec 0.0656366, recall 0.877497
2017-12-10T11:39:24.455243: step 2904, loss 0.283111, acc 0.90625, prec 0.0656255, recall 0.877497
2017-12-10T11:39:24.900090: step 2905, loss 0.288861, acc 0.9375, prec 0.0656181, recall 0.877497
2017-12-10T11:39:25.335965: step 2906, loss 0.451408, acc 0.8125, prec 0.0656223, recall 0.877543
2017-12-10T11:39:25.772428: step 2907, loss 0.560994, acc 0.890625, prec 0.0656093, recall 0.877543
2017-12-10T11:39:26.228115: step 2908, loss 0.879566, acc 0.828125, prec 0.065589, recall 0.877543
2017-12-10T11:39:26.657916: step 2909, loss 1.08787, acc 0.78125, prec 0.0656158, recall 0.877636
2017-12-10T11:39:27.106446: step 2910, loss 0.782999, acc 0.796875, prec 0.0656181, recall 0.877682
2017-12-10T11:39:27.531663: step 2911, loss 0.586623, acc 0.828125, prec 0.0655978, recall 0.877682
2017-12-10T11:39:27.977899: step 2912, loss 0.573268, acc 0.84375, prec 0.0656056, recall 0.877728
2017-12-10T11:39:28.411700: step 2913, loss 0.850475, acc 0.734375, prec 0.0656005, recall 0.877774
2017-12-10T11:39:28.848700: step 2914, loss 0.749473, acc 0.8125, prec 0.0656046, recall 0.87782
2017-12-10T11:39:29.282579: step 2915, loss 0.500474, acc 0.78125, prec 0.0655788, recall 0.87782
2017-12-10T11:39:29.720778: step 2916, loss 0.588784, acc 0.75, prec 0.0655494, recall 0.87782
2017-12-10T11:39:30.162875: step 2917, loss 0.459288, acc 0.859375, prec 0.065559, recall 0.877865
2017-12-10T11:39:30.610301: step 2918, loss 0.417485, acc 0.859375, prec 0.0655687, recall 0.877911
2017-12-10T11:39:31.049436: step 2919, loss 0.490078, acc 0.84375, prec 0.0655503, recall 0.877911
2017-12-10T11:39:31.490058: step 2920, loss 1.13997, acc 0.828125, prec 0.0655825, recall 0.878003
2017-12-10T11:39:31.921171: step 2921, loss 0.254665, acc 0.953125, prec 0.065577, recall 0.878003
2017-12-10T11:39:32.366711: step 2922, loss 0.346626, acc 0.859375, prec 0.0656652, recall 0.878186
2017-12-10T11:39:32.810218: step 2923, loss 0.51968, acc 0.890625, prec 0.0657309, recall 0.878323
2017-12-10T11:39:33.263471: step 2924, loss 0.134129, acc 0.9375, prec 0.0657497, recall 0.878368
2017-12-10T11:39:33.716608: step 2925, loss 0.231424, acc 0.921875, prec 0.0657405, recall 0.878368
2017-12-10T11:39:34.184897: step 2926, loss 0.170581, acc 0.953125, prec 0.0657611, recall 0.878414
2017-12-10T11:39:34.628665: step 2927, loss 0.298914, acc 0.90625, prec 0.06575, recall 0.878414
2017-12-10T11:39:35.071657: step 2928, loss 0.19542, acc 0.9375, prec 0.065795, recall 0.878505
2017-12-10T11:39:35.527084: step 2929, loss 0.301296, acc 0.9375, prec 0.0658138, recall 0.87855
2017-12-10T11:39:35.974653: step 2930, loss 0.237423, acc 0.890625, prec 0.065827, recall 0.878595
2017-12-10T11:39:36.419230: step 2931, loss 0.227308, acc 0.921875, prec 0.0658178, recall 0.878595
2017-12-10T11:39:36.867961: step 2932, loss 0.147836, acc 0.96875, prec 0.0658141, recall 0.878595
2017-12-10T11:39:37.321492: step 2933, loss 0.067052, acc 0.96875, prec 0.0658889, recall 0.878731
2017-12-10T11:39:37.758537: step 2934, loss 0.0935199, acc 0.9375, prec 0.0658815, recall 0.878731
2017-12-10T11:39:38.198872: step 2935, loss 0.198918, acc 0.984375, prec 0.0659058, recall 0.878777
2017-12-10T11:39:38.636966: step 2936, loss 0.229496, acc 0.96875, prec 0.0659543, recall 0.878867
2017-12-10T11:39:39.075187: step 2937, loss 0.172784, acc 0.90625, prec 0.0659694, recall 0.878912
2017-12-10T11:39:39.517110: step 2938, loss 0.0298803, acc 0.984375, prec 0.0659937, recall 0.878957
2017-12-10T11:39:39.963205: step 2939, loss 0.0581138, acc 0.984375, prec 0.0659918, recall 0.878957
2017-12-10T11:39:40.417346: step 2940, loss 0.575715, acc 0.953125, prec 0.0660385, recall 0.879047
2017-12-10T11:39:40.858003: step 2941, loss 0.0751383, acc 0.984375, prec 0.0660628, recall 0.879092
2017-12-10T11:39:41.306709: step 2942, loss 3.26381, acc 0.921875, prec 0.0660815, recall 0.87881
2017-12-10T11:39:41.755597: step 2943, loss 0.121073, acc 0.9375, prec 0.0660741, recall 0.87881
2017-12-10T11:39:42.195386: step 2944, loss 0.148368, acc 0.984375, prec 0.0661245, recall 0.8789
2017-12-10T11:39:42.645563: step 2945, loss 0.0720847, acc 0.96875, prec 0.066173, recall 0.87899
2017-12-10T11:39:43.077182: step 2946, loss 0.186602, acc 0.96875, prec 0.0661693, recall 0.87899
2017-12-10T11:39:43.515486: step 2947, loss 0.0307072, acc 0.984375, prec 0.0661674, recall 0.87899
2017-12-10T11:39:43.953049: step 2948, loss 0.19092, acc 0.921875, prec 0.0661582, recall 0.87899
2017-12-10T11:39:44.396617: step 2949, loss 0.171114, acc 0.96875, prec 0.0661545, recall 0.87899
2017-12-10T11:39:44.832068: step 2950, loss 0.212234, acc 0.9375, prec 0.0661471, recall 0.87899
2017-12-10T11:39:45.276305: step 2951, loss 0.0564192, acc 0.984375, prec 0.0661452, recall 0.87899
2017-12-10T11:39:45.715924: step 2952, loss 0.098236, acc 0.984375, prec 0.0661695, recall 0.879035
2017-12-10T11:39:46.150722: step 2953, loss 0.146048, acc 0.96875, prec 0.0661658, recall 0.879035
2017-12-10T11:39:46.587717: step 2954, loss 0.0938481, acc 0.96875, prec 0.0662143, recall 0.879125
2017-12-10T11:39:47.029317: step 2955, loss 1.74822, acc 0.96875, prec 0.0662124, recall 0.878799
2017-12-10T11:39:47.485089: step 2956, loss 0.130344, acc 0.953125, prec 0.0662069, recall 0.878799
2017-12-10T11:39:47.928870: step 2957, loss 0.242558, acc 0.90625, prec 0.0661958, recall 0.878799
2017-12-10T11:39:48.372304: step 2958, loss 0.238278, acc 0.921875, prec 0.0662126, recall 0.878844
2017-12-10T11:39:48.812787: step 2959, loss 0.119344, acc 0.96875, prec 0.066235, recall 0.878889
2017-12-10T11:39:49.240990: step 2960, loss 0.421777, acc 0.890625, prec 0.066222, recall 0.878889
2017-12-10T11:39:49.689809: step 2961, loss 0.310712, acc 0.921875, prec 0.0662649, recall 0.878979
2017-12-10T11:39:50.135892: step 2962, loss 0.271783, acc 0.90625, prec 0.0662538, recall 0.878979
2017-12-10T11:39:50.578996: step 2963, loss 2.7543, acc 0.859375, prec 0.0663171, recall 0.878788
2017-12-10T11:39:51.025357: step 2964, loss 0.194922, acc 0.9375, prec 0.0663097, recall 0.878788
2017-12-10T11:39:51.454541: step 2965, loss 0.326017, acc 0.90625, prec 0.0663247, recall 0.878833
2017-12-10T11:39:51.891337: step 2966, loss 0.380637, acc 0.859375, prec 0.066308, recall 0.878833
2017-12-10T11:39:52.327940: step 2967, loss 0.274691, acc 0.859375, prec 0.0662914, recall 0.878833
2017-12-10T11:39:52.775267: step 2968, loss 0.723568, acc 0.796875, prec 0.0662674, recall 0.878833
2017-12-10T11:39:53.207915: step 2969, loss 0.627432, acc 0.875, prec 0.0663047, recall 0.878922
2017-12-10T11:39:53.655282: step 2970, loss 2.83057, acc 0.890625, prec 0.0663196, recall 0.878643
2017-12-10T11:39:54.124748: step 2971, loss 0.605292, acc 0.859375, prec 0.0663549, recall 0.878732
2017-12-10T11:39:54.579990: step 2972, loss 0.815856, acc 0.78125, prec 0.0663291, recall 0.878732
2017-12-10T11:39:55.030353: step 2973, loss 1.0557, acc 0.734375, prec 0.0663497, recall 0.878821
2017-12-10T11:39:55.470947: step 2974, loss 0.367886, acc 0.84375, prec 0.0663312, recall 0.878821
2017-12-10T11:39:55.913562: step 2975, loss 0.325475, acc 0.90625, prec 0.0663461, recall 0.878866
2017-12-10T11:39:56.365576: step 2976, loss 0.843752, acc 0.75, prec 0.0663166, recall 0.878866
2017-12-10T11:39:56.805963: step 2977, loss 0.442561, acc 0.890625, prec 0.0663297, recall 0.878911
2017-12-10T11:39:57.242207: step 2978, loss 2.47849, acc 0.796875, prec 0.0663075, recall 0.878587
2017-12-10T11:39:57.680989: step 2979, loss 0.49874, acc 0.84375, prec 0.0663669, recall 0.878721
2017-12-10T11:39:58.122431: step 2980, loss 0.553676, acc 0.828125, prec 0.0663725, recall 0.878766
2017-12-10T11:39:58.560338: step 2981, loss 0.631895, acc 0.8125, prec 0.0664022, recall 0.878855
2017-12-10T11:39:58.954803: step 2982, loss 0.390398, acc 0.901961, prec 0.066393, recall 0.878855
2017-12-10T11:39:59.409051: step 2983, loss 0.629228, acc 0.8125, prec 0.0663968, recall 0.878899
2017-12-10T11:39:59.849214: step 2984, loss 0.333614, acc 0.90625, prec 0.0664375, recall 0.878988
2017-12-10T11:40:00.297818: step 2985, loss 0.349859, acc 0.921875, prec 0.0664283, recall 0.878988
2017-12-10T11:40:00.732698: step 2986, loss 0.304329, acc 0.828125, prec 0.0664081, recall 0.878988
2017-12-10T11:40:01.164652: step 2987, loss 0.620579, acc 0.828125, prec 0.0664137, recall 0.879032
2017-12-10T11:40:01.604774: step 2988, loss 0.537133, acc 0.828125, prec 0.0664193, recall 0.879077
2017-12-10T11:40:02.045250: step 2989, loss 0.512008, acc 0.875, prec 0.0664305, recall 0.879121
2017-12-10T11:40:02.489614: step 2990, loss 0.392534, acc 0.859375, prec 0.0664139, recall 0.879121
2017-12-10T11:40:02.935306: step 2991, loss 0.34106, acc 0.90625, prec 0.0664287, recall 0.879165
2017-12-10T11:40:03.371626: step 2992, loss 0.561092, acc 0.9375, prec 0.066473, recall 0.879254
2017-12-10T11:40:03.814466: step 2993, loss 0.486094, acc 0.8125, prec 0.066451, recall 0.879254
2017-12-10T11:40:04.263442: step 2994, loss 0.143895, acc 0.953125, prec 0.0664713, recall 0.879298
2017-12-10T11:40:04.697174: step 2995, loss 0.198648, acc 0.9375, prec 0.0665155, recall 0.879386
2017-12-10T11:40:05.137602: step 2996, loss 0.512465, acc 0.9375, prec 0.0665856, recall 0.879518
2017-12-10T11:40:05.575227: step 2997, loss 0.193991, acc 0.9375, prec 0.0665782, recall 0.879518
2017-12-10T11:40:06.019406: step 2998, loss 0.136651, acc 0.953125, prec 0.0665727, recall 0.879518
2017-12-10T11:40:06.458536: step 2999, loss 0.171775, acc 0.90625, prec 0.0665617, recall 0.879518
2017-12-10T11:40:06.905140: step 3000, loss 0.187325, acc 0.9375, prec 0.0665801, recall 0.879562
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3000

2017-12-10T11:40:08.716435: step 3001, loss 0.0454684, acc 0.984375, prec 0.066604, recall 0.879606
2017-12-10T11:40:09.157603: step 3002, loss 0.109863, acc 0.953125, prec 0.0666243, recall 0.87965
2017-12-10T11:40:09.611352: step 3003, loss 0.0539212, acc 0.984375, prec 0.0666225, recall 0.87965
2017-12-10T11:40:10.062526: step 3004, loss 0.170687, acc 0.953125, prec 0.0666685, recall 0.879738
2017-12-10T11:40:10.503197: step 3005, loss 0.137925, acc 0.953125, prec 0.066663, recall 0.879738
2017-12-10T11:40:10.956385: step 3006, loss 0.0312601, acc 0.984375, prec 0.0666869, recall 0.879781
2017-12-10T11:40:11.397295: step 3007, loss 0.115146, acc 0.9375, prec 0.0667053, recall 0.879825
2017-12-10T11:40:11.836620: step 3008, loss 0.0842624, acc 1, prec 0.0668084, recall 0.88
2017-12-10T11:40:12.281630: step 3009, loss 0.0224967, acc 0.984375, prec 0.0668323, recall 0.880044
2017-12-10T11:40:12.725319: step 3010, loss 0.32973, acc 0.953125, prec 0.0668783, recall 0.880131
2017-12-10T11:40:13.173917: step 3011, loss 0.341656, acc 0.96875, prec 0.0669518, recall 0.880261
2017-12-10T11:40:13.630191: step 3012, loss 0.0728246, acc 0.984375, prec 0.06695, recall 0.880261
2017-12-10T11:40:14.071543: step 3013, loss 0.0379349, acc 1, prec 0.0669757, recall 0.880305
2017-12-10T11:40:14.510148: step 3014, loss 0.0821667, acc 1, prec 0.0670015, recall 0.880348
2017-12-10T11:40:14.957319: step 3015, loss 0.283839, acc 0.984375, prec 0.0670254, recall 0.880391
2017-12-10T11:40:15.406982: step 3016, loss 0.0605386, acc 1, prec 0.0670769, recall 0.880478
2017-12-10T11:40:15.856182: step 3017, loss 0.00997848, acc 1, prec 0.0671026, recall 0.880521
2017-12-10T11:40:16.300894: step 3018, loss 0.700712, acc 0.984375, prec 0.0671265, recall 0.880565
2017-12-10T11:40:16.750288: step 3019, loss 0.00784263, acc 1, prec 0.0671265, recall 0.880565
2017-12-10T11:40:17.197216: step 3020, loss 0.262664, acc 0.96875, prec 0.0671743, recall 0.880651
2017-12-10T11:40:17.632487: step 3021, loss 0.0586275, acc 0.984375, prec 0.0671724, recall 0.880651
2017-12-10T11:40:18.067806: step 3022, loss 0.25185, acc 0.921875, prec 0.0671889, recall 0.880694
2017-12-10T11:40:18.503988: step 3023, loss 0.15042, acc 0.984375, prec 0.067187, recall 0.880694
2017-12-10T11:40:18.941673: step 3024, loss 0.197906, acc 0.96875, prec 0.067209, recall 0.880737
2017-12-10T11:40:19.382384: step 3025, loss 0.0421732, acc 0.984375, prec 0.0672072, recall 0.880737
2017-12-10T11:40:19.814631: step 3026, loss 0.126477, acc 0.984375, prec 0.0672311, recall 0.88078
2017-12-10T11:40:20.242113: step 3027, loss 0.178976, acc 1, prec 0.0672825, recall 0.880866
2017-12-10T11:40:20.675051: step 3028, loss 0.172518, acc 0.96875, prec 0.0673045, recall 0.880909
2017-12-10T11:40:21.135659: step 3029, loss 5.11085, acc 0.953125, prec 0.0673008, recall 0.880592
2017-12-10T11:40:21.583666: step 3030, loss 0.265332, acc 0.921875, prec 0.0672915, recall 0.880592
2017-12-10T11:40:22.027824: step 3031, loss 0.215001, acc 0.953125, prec 0.0673117, recall 0.880635
2017-12-10T11:40:22.478894: step 3032, loss 0.249893, acc 0.90625, prec 0.0673262, recall 0.880678
2017-12-10T11:40:22.921128: step 3033, loss 0.292849, acc 0.96875, prec 0.0673739, recall 0.880764
2017-12-10T11:40:23.364422: step 3034, loss 0.189787, acc 0.90625, prec 0.0673628, recall 0.880764
2017-12-10T11:40:23.808105: step 3035, loss 0.394698, acc 0.921875, prec 0.0674306, recall 0.880892
2017-12-10T11:40:24.251140: step 3036, loss 0.417187, acc 0.875, prec 0.0674414, recall 0.880935
2017-12-10T11:40:24.700400: step 3037, loss 0.31074, acc 0.890625, prec 0.0674541, recall 0.880978
2017-12-10T11:40:25.127195: step 3038, loss 0.316717, acc 0.890625, prec 0.0674924, recall 0.881064
2017-12-10T11:40:25.567803: step 3039, loss 0.168795, acc 0.96875, prec 0.0674887, recall 0.881064
2017-12-10T11:40:26.013229: step 3040, loss 0.984665, acc 0.765625, prec 0.0675122, recall 0.881149
2017-12-10T11:40:26.455485: step 3041, loss 0.202259, acc 0.9375, prec 0.067556, recall 0.881234
2017-12-10T11:40:26.892072: step 3042, loss 0.190801, acc 0.953125, prec 0.0676018, recall 0.881319
2017-12-10T11:40:27.328916: step 3043, loss 0.378239, acc 0.90625, prec 0.0676162, recall 0.881362
2017-12-10T11:40:27.762685: step 3044, loss 0.315087, acc 0.90625, prec 0.0676307, recall 0.881405
2017-12-10T11:40:28.197929: step 3045, loss 0.305714, acc 0.90625, prec 0.0676452, recall 0.881447
2017-12-10T11:40:28.631851: step 3046, loss 0.326512, acc 0.890625, prec 0.0676322, recall 0.881447
2017-12-10T11:40:29.075631: step 3047, loss 0.4824, acc 0.84375, prec 0.0676648, recall 0.881532
2017-12-10T11:40:29.530836: step 3048, loss 0.258433, acc 0.890625, prec 0.0676774, recall 0.881574
2017-12-10T11:40:29.972191: step 3049, loss 0.281618, acc 0.875, prec 0.0676882, recall 0.881617
2017-12-10T11:40:30.405047: step 3050, loss 0.0501211, acc 0.984375, prec 0.0676863, recall 0.881617
2017-12-10T11:40:30.835666: step 3051, loss 0.119199, acc 0.96875, prec 0.0677082, recall 0.881659
2017-12-10T11:40:31.286227: step 3052, loss 0.169182, acc 0.96875, prec 0.0677813, recall 0.881786
2017-12-10T11:40:31.727072: step 3053, loss 0.12856, acc 0.9375, prec 0.0677738, recall 0.881786
2017-12-10T11:40:32.169282: step 3054, loss 0.13908, acc 0.96875, prec 0.0677701, recall 0.881786
2017-12-10T11:40:32.612850: step 3055, loss 0.105886, acc 0.953125, prec 0.0677901, recall 0.881828
2017-12-10T11:40:33.056251: step 3056, loss 0.0324314, acc 1, prec 0.0677901, recall 0.881828
2017-12-10T11:40:33.509882: step 3057, loss 0.0498254, acc 0.96875, prec 0.067812, recall 0.88187
2017-12-10T11:40:33.945879: step 3058, loss 0.182334, acc 0.953125, prec 0.0678064, recall 0.88187
2017-12-10T11:40:34.380609: step 3059, loss 0.173927, acc 0.953125, prec 0.0678008, recall 0.88187
2017-12-10T11:40:34.815133: step 3060, loss 1.14339, acc 0.984375, prec 0.0678245, recall 0.881912
2017-12-10T11:40:35.263708: step 3061, loss 0.0222903, acc 1, prec 0.0678501, recall 0.881954
2017-12-10T11:40:35.704030: step 3062, loss 0.0531702, acc 0.984375, prec 0.0678738, recall 0.881996
2017-12-10T11:40:36.140218: step 3063, loss 0.116672, acc 0.953125, prec 0.0678938, recall 0.882038
2017-12-10T11:40:36.576204: step 3064, loss 0.219813, acc 0.953125, prec 0.0678882, recall 0.882038
2017-12-10T11:40:37.007807: step 3065, loss 0.0605594, acc 0.96875, prec 0.0678845, recall 0.882038
2017-12-10T11:40:37.438286: step 3066, loss 0.118054, acc 0.96875, prec 0.0678807, recall 0.882038
2017-12-10T11:40:37.878003: step 3067, loss 0.172468, acc 0.953125, prec 0.0679007, recall 0.88208
2017-12-10T11:40:38.314749: step 3068, loss 0.0359563, acc 1, prec 0.0679519, recall 0.882164
2017-12-10T11:40:38.748818: step 3069, loss 0.164559, acc 0.953125, prec 0.0679718, recall 0.882206
2017-12-10T11:40:39.181204: step 3070, loss 0.080077, acc 0.96875, prec 0.0679681, recall 0.882206
2017-12-10T11:40:39.621359: step 3071, loss 0.158615, acc 0.9375, prec 0.0679606, recall 0.882206
2017-12-10T11:40:40.067373: step 3072, loss 0.152281, acc 0.96875, prec 0.0679825, recall 0.882248
2017-12-10T11:40:40.517957: step 3073, loss 0.0393971, acc 0.984375, prec 0.0679806, recall 0.882248
2017-12-10T11:40:40.955126: step 3074, loss 0.122818, acc 1, prec 0.0680317, recall 0.882332
2017-12-10T11:40:41.397285: step 3075, loss 0.186843, acc 0.984375, prec 0.0680809, recall 0.882416
2017-12-10T11:40:41.836428: step 3076, loss 0.24562, acc 0.96875, prec 0.0681538, recall 0.882541
2017-12-10T11:40:42.288857: step 3077, loss 0.0635209, acc 0.984375, prec 0.0681775, recall 0.882582
2017-12-10T11:40:42.755941: step 3078, loss 0.173004, acc 0.953125, prec 0.0682484, recall 0.882707
2017-12-10T11:40:43.210538: step 3079, loss 0.255164, acc 0.921875, prec 0.0682646, recall 0.882749
2017-12-10T11:40:43.657888: step 3080, loss 0.447026, acc 0.984375, prec 0.0683648, recall 0.882915
2017-12-10T11:40:44.105744: step 3081, loss 0.0201561, acc 1, prec 0.0683648, recall 0.882915
2017-12-10T11:40:44.549949: step 3082, loss 0.136335, acc 0.9375, prec 0.0684084, recall 0.882998
2017-12-10T11:40:44.978806: step 3083, loss 0.842427, acc 0.96875, prec 0.0684301, recall 0.883039
2017-12-10T11:40:45.415655: step 3084, loss 0.0672431, acc 0.984375, prec 0.0684283, recall 0.883039
2017-12-10T11:40:45.868896: step 3085, loss 0.559182, acc 0.9375, prec 0.0685228, recall 0.883204
2017-12-10T11:40:46.319522: step 3086, loss 0.253311, acc 0.953125, prec 0.0685171, recall 0.883204
2017-12-10T11:40:46.762078: step 3087, loss 0.193647, acc 0.9375, prec 0.0685606, recall 0.883286
2017-12-10T11:40:47.195375: step 3088, loss 0.0761129, acc 0.984375, prec 0.0685843, recall 0.883327
2017-12-10T11:40:47.637363: step 3089, loss 0.740483, acc 0.9375, prec 0.0686277, recall 0.88341
2017-12-10T11:40:48.069081: step 3090, loss 0.0882209, acc 0.953125, prec 0.0686476, recall 0.883451
2017-12-10T11:40:48.498366: step 3091, loss 0.143521, acc 0.96875, prec 0.0686693, recall 0.883492
2017-12-10T11:40:48.944440: step 3092, loss 0.21167, acc 0.96875, prec 0.0686655, recall 0.883492
2017-12-10T11:40:49.396759: step 3093, loss 0.264846, acc 0.9375, prec 0.0686835, recall 0.883533
2017-12-10T11:40:49.843388: step 3094, loss 0.394837, acc 0.90625, prec 0.0687232, recall 0.883615
2017-12-10T11:40:50.308613: step 3095, loss 0.144853, acc 0.953125, prec 0.0687175, recall 0.883615
2017-12-10T11:40:50.739898: step 3096, loss 0.347015, acc 0.90625, prec 0.0687063, recall 0.883615
2017-12-10T11:40:51.189572: step 3097, loss 0.216174, acc 0.921875, prec 0.0686969, recall 0.883615
2017-12-10T11:40:51.628030: step 3098, loss 0.157775, acc 0.96875, prec 0.068744, recall 0.883696
2017-12-10T11:40:52.070202: step 3099, loss 1.38195, acc 0.90625, prec 0.0687855, recall 0.883468
2017-12-10T11:40:52.511770: step 3100, loss 0.108926, acc 0.984375, prec 0.0688091, recall 0.883509
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3100

2017-12-10T11:40:54.521090: step 3101, loss 0.0559033, acc 0.984375, prec 0.0688072, recall 0.883509
2017-12-10T11:40:54.970306: step 3102, loss 0.320612, acc 0.9375, prec 0.0688251, recall 0.88355
2017-12-10T11:40:55.412194: step 3103, loss 0.253539, acc 0.90625, prec 0.0688139, recall 0.88355
2017-12-10T11:40:55.848908: step 3104, loss 0.578124, acc 0.890625, prec 0.0688261, recall 0.88359
2017-12-10T11:40:56.293366: step 3105, loss 0.0308786, acc 1, prec 0.0689024, recall 0.883713
2017-12-10T11:40:56.746146: step 3106, loss 0.336255, acc 0.921875, prec 0.068893, recall 0.883713
2017-12-10T11:40:57.191215: step 3107, loss 0.318327, acc 0.859375, prec 0.0689269, recall 0.883794
2017-12-10T11:40:57.630970: step 3108, loss 0.157922, acc 0.90625, prec 0.068941, recall 0.883835
2017-12-10T11:40:58.083718: step 3109, loss 0.22099, acc 0.984375, prec 0.0689646, recall 0.883875
2017-12-10T11:40:58.553104: step 3110, loss 0.104201, acc 0.96875, prec 0.0689608, recall 0.883875
2017-12-10T11:40:59.004585: step 3111, loss 0.182395, acc 0.9375, prec 0.0689533, recall 0.883875
2017-12-10T11:40:59.440408: step 3112, loss 0.173349, acc 0.96875, prec 0.0689495, recall 0.883875
2017-12-10T11:40:59.890736: step 3113, loss 0.123837, acc 0.9375, prec 0.0689674, recall 0.883916
2017-12-10T11:41:00.324214: step 3114, loss 0.216408, acc 0.953125, prec 0.0689618, recall 0.883916
2017-12-10T11:41:00.772658: step 3115, loss 0.175665, acc 0.96875, prec 0.0690088, recall 0.883997
2017-12-10T11:41:01.215952: step 3116, loss 0.100652, acc 0.9375, prec 0.069052, recall 0.884078
2017-12-10T11:41:01.656041: step 3117, loss 0.139859, acc 0.984375, prec 0.0691263, recall 0.8842
2017-12-10T11:41:02.093236: step 3118, loss 0.0812885, acc 0.96875, prec 0.0691225, recall 0.8842
2017-12-10T11:41:02.532642: step 3119, loss 0.107727, acc 0.96875, prec 0.0691188, recall 0.8842
2017-12-10T11:41:02.963226: step 3120, loss 0.0636942, acc 0.96875, prec 0.069115, recall 0.8842
2017-12-10T11:41:03.399382: step 3121, loss 0.122386, acc 0.921875, prec 0.069131, recall 0.88424
2017-12-10T11:41:03.840181: step 3122, loss 0.20133, acc 0.984375, prec 0.0691798, recall 0.884321
2017-12-10T11:41:04.290073: step 3123, loss 0.25663, acc 0.96875, prec 0.0692014, recall 0.884361
2017-12-10T11:41:04.736939: step 3124, loss 0.272176, acc 0.90625, prec 0.0692155, recall 0.884401
2017-12-10T11:41:05.181431: step 3125, loss 0.0933147, acc 0.96875, prec 0.0692117, recall 0.884401
2017-12-10T11:41:05.630197: step 3126, loss 0.15755, acc 0.9375, prec 0.0692549, recall 0.884482
2017-12-10T11:41:06.073081: step 3127, loss 0.019342, acc 1, prec 0.0692549, recall 0.884482
2017-12-10T11:41:06.525715: step 3128, loss 0.19589, acc 0.9375, prec 0.0692727, recall 0.884522
2017-12-10T11:41:06.969606: step 3129, loss 0.0514837, acc 1, prec 0.069298, recall 0.884562
2017-12-10T11:41:07.408658: step 3130, loss 0.0689311, acc 0.984375, prec 0.0693468, recall 0.884642
2017-12-10T11:41:07.850895: step 3131, loss 0.0775022, acc 0.984375, prec 0.0693703, recall 0.884682
2017-12-10T11:41:08.296203: step 3132, loss 0.096025, acc 0.96875, prec 0.0693665, recall 0.884682
2017-12-10T11:41:08.744373: step 3133, loss 0.126776, acc 0.9375, prec 0.069359, recall 0.884682
2017-12-10T11:41:09.177391: step 3134, loss 5.72406, acc 0.953125, prec 0.0694078, recall 0.884148
2017-12-10T11:41:09.629750: step 3135, loss 0.0594891, acc 0.96875, prec 0.0694293, recall 0.884189
2017-12-10T11:41:10.072794: step 3136, loss 0.113337, acc 0.984375, prec 0.0694528, recall 0.884229
2017-12-10T11:41:10.517706: step 3137, loss 0.22822, acc 0.953125, prec 0.0694724, recall 0.884269
2017-12-10T11:41:10.953627: step 3138, loss 0.30797, acc 0.890625, prec 0.0694592, recall 0.884269
2017-12-10T11:41:11.387129: step 3139, loss 0.499528, acc 0.890625, prec 0.069446, recall 0.884269
2017-12-10T11:41:11.832828: step 3140, loss 0.549873, acc 0.828125, prec 0.0694252, recall 0.884269
2017-12-10T11:41:12.289119: step 3141, loss 0.781853, acc 0.796875, prec 0.0694512, recall 0.884349
2017-12-10T11:41:12.737822: step 3142, loss 0.476194, acc 0.859375, prec 0.0694342, recall 0.884349
2017-12-10T11:41:13.160337: step 3143, loss 0.468356, acc 0.828125, prec 0.0694135, recall 0.884349
2017-12-10T11:41:13.603067: step 3144, loss 0.29833, acc 0.90625, prec 0.0694527, recall 0.884429
2017-12-10T11:41:14.041563: step 3145, loss 0.540527, acc 0.796875, prec 0.0694535, recall 0.884469
2017-12-10T11:41:14.486869: step 3146, loss 1.07412, acc 0.734375, prec 0.0694214, recall 0.884469
2017-12-10T11:41:14.938595: step 3147, loss 0.410758, acc 0.875, prec 0.0694064, recall 0.884469
2017-12-10T11:41:15.384587: step 3148, loss 0.29248, acc 0.859375, prec 0.0694147, recall 0.884509
2017-12-10T11:41:15.820732: step 3149, loss 0.290446, acc 0.921875, prec 0.0694305, recall 0.884549
2017-12-10T11:41:16.262234: step 3150, loss 0.649296, acc 0.859375, prec 0.0694893, recall 0.884669
2017-12-10T11:41:16.702465: step 3151, loss 0.387024, acc 0.875, prec 0.0694994, recall 0.884708
2017-12-10T11:41:17.152376: step 3152, loss 0.329189, acc 0.90625, prec 0.0695134, recall 0.884748
2017-12-10T11:41:17.604060: step 3153, loss 0.50592, acc 0.90625, prec 0.069502, recall 0.884748
2017-12-10T11:41:18.046074: step 3154, loss 0.160953, acc 0.953125, prec 0.0694964, recall 0.884748
2017-12-10T11:41:18.489667: step 3155, loss 0.0916871, acc 0.953125, prec 0.0694907, recall 0.884748
2017-12-10T11:41:18.919425: step 3156, loss 0.403855, acc 0.875, prec 0.0695009, recall 0.884788
2017-12-10T11:41:19.365236: step 3157, loss 0.186595, acc 0.921875, prec 0.0695167, recall 0.884828
2017-12-10T11:41:19.810792: step 3158, loss 0.195009, acc 0.9375, prec 0.0695092, recall 0.884828
2017-12-10T11:41:20.249594: step 3159, loss 0.168468, acc 0.96875, prec 0.0695054, recall 0.884828
2017-12-10T11:41:20.695751: step 3160, loss 0.426115, acc 0.984375, prec 0.0696043, recall 0.884986
2017-12-10T11:41:21.139606: step 3161, loss 0.344344, acc 0.9375, prec 0.0695968, recall 0.884986
2017-12-10T11:41:21.576427: step 3162, loss 0.933703, acc 0.984375, prec 0.0696201, recall 0.885026
2017-12-10T11:41:22.012548: step 3163, loss 0.155573, acc 0.90625, prec 0.0696088, recall 0.885026
2017-12-10T11:41:22.450230: step 3164, loss 0.118138, acc 0.953125, prec 0.0696283, recall 0.885065
2017-12-10T11:41:22.884701: step 3165, loss 0.13283, acc 0.953125, prec 0.0696227, recall 0.885065
2017-12-10T11:41:23.324841: step 3166, loss 0.132999, acc 0.9375, prec 0.0696151, recall 0.885065
2017-12-10T11:41:23.761324: step 3167, loss 0.075938, acc 0.984375, prec 0.0696132, recall 0.885065
2017-12-10T11:41:24.199397: step 3168, loss 0.196606, acc 0.96875, prec 0.0696346, recall 0.885105
2017-12-10T11:41:24.648291: step 3169, loss 0.222015, acc 0.953125, prec 0.0696793, recall 0.885184
2017-12-10T11:41:25.090680: step 3170, loss 0.0470919, acc 0.96875, prec 0.0697007, recall 0.885223
2017-12-10T11:41:25.527890: step 3171, loss 0.047032, acc 1, prec 0.0697259, recall 0.885263
2017-12-10T11:41:25.966367: step 3172, loss 0.421161, acc 0.9375, prec 0.0697435, recall 0.885302
2017-12-10T11:41:26.414372: step 3173, loss 0.410035, acc 0.96875, prec 0.0697649, recall 0.885342
2017-12-10T11:41:26.863843: step 3174, loss 0.203775, acc 0.890625, prec 0.0697517, recall 0.885342
2017-12-10T11:41:27.309371: step 3175, loss 0.0771709, acc 0.96875, prec 0.0697479, recall 0.885342
2017-12-10T11:41:27.759201: step 3176, loss 0.110537, acc 0.96875, prec 0.0697693, recall 0.885381
2017-12-10T11:41:28.199557: step 3177, loss 0.0628324, acc 0.96875, prec 0.0697656, recall 0.885381
2017-12-10T11:41:28.639772: step 3178, loss 0.170778, acc 0.953125, prec 0.0698353, recall 0.885499
2017-12-10T11:41:29.071567: step 3179, loss 0.500737, acc 0.953125, prec 0.0698548, recall 0.885538
2017-12-10T11:41:29.527084: step 3180, loss 0.310096, acc 0.9375, prec 0.0698473, recall 0.885538
2017-12-10T11:41:29.972066: step 3181, loss 0.210998, acc 0.953125, prec 0.0698668, recall 0.885577
2017-12-10T11:41:30.412169: step 3182, loss 0.0410836, acc 0.984375, prec 0.0698649, recall 0.885577
2017-12-10T11:41:30.855887: step 3183, loss 0.244504, acc 0.9375, prec 0.0698573, recall 0.885577
2017-12-10T11:41:31.309244: step 3184, loss 0.0910603, acc 0.96875, prec 0.0698787, recall 0.885616
2017-12-10T11:41:31.757975: step 3185, loss 0.327754, acc 0.921875, prec 0.0698692, recall 0.885616
2017-12-10T11:41:32.199228: step 3186, loss 0.198649, acc 0.9375, prec 0.0698617, recall 0.885616
2017-12-10T11:41:32.637488: step 3187, loss 0.202173, acc 0.9375, prec 0.0698541, recall 0.885616
2017-12-10T11:41:33.081681: step 3188, loss 1.30801, acc 0.96875, prec 0.0698774, recall 0.885352
2017-12-10T11:41:33.538194: step 3189, loss 0.100585, acc 0.96875, prec 0.0699238, recall 0.885431
2017-12-10T11:41:33.981132: step 3190, loss 0.235272, acc 0.890625, prec 0.0699357, recall 0.88547
2017-12-10T11:41:34.428452: step 3191, loss 0.101226, acc 0.953125, prec 0.0699552, recall 0.885509
2017-12-10T11:41:34.872340: step 3192, loss 0.0780803, acc 0.984375, prec 0.0699533, recall 0.885509
2017-12-10T11:41:35.320050: step 3193, loss 0.157674, acc 0.984375, prec 0.0700016, recall 0.885587
2017-12-10T11:41:35.764353: step 3194, loss 0.0559872, acc 0.96875, prec 0.0699978, recall 0.885587
2017-12-10T11:41:36.211540: step 3195, loss 0.105118, acc 0.96875, prec 0.0699941, recall 0.885587
2017-12-10T11:41:36.654241: step 3196, loss 0.18686, acc 0.90625, prec 0.0699827, recall 0.885587
2017-12-10T11:41:37.100261: step 3197, loss 0.232675, acc 0.921875, prec 0.0699733, recall 0.885587
2017-12-10T11:41:37.553253: step 3198, loss 0.208782, acc 0.9375, prec 0.070041, recall 0.885705
2017-12-10T11:41:38.006061: step 3199, loss 0.046811, acc 0.96875, prec 0.0700623, recall 0.885743
2017-12-10T11:41:38.448876: step 3200, loss 0.555415, acc 0.921875, prec 0.070078, recall 0.885782
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3200

2017-12-10T11:41:40.302412: step 3201, loss 0.148887, acc 0.96875, prec 0.0700742, recall 0.885782
2017-12-10T11:41:40.740864: step 3202, loss 0.272124, acc 0.953125, prec 0.0700936, recall 0.885821
2017-12-10T11:41:41.184747: step 3203, loss 0.0989744, acc 0.984375, prec 0.0701418, recall 0.885899
2017-12-10T11:41:41.630258: step 3204, loss 0.200815, acc 0.984375, prec 0.0701901, recall 0.885977
2017-12-10T11:41:42.064445: step 3205, loss 0.202188, acc 0.96875, prec 0.0702114, recall 0.886016
2017-12-10T11:41:42.509745: step 3206, loss 0.044561, acc 0.984375, prec 0.0702346, recall 0.886054
2017-12-10T11:41:42.950696: step 3207, loss 0.0713861, acc 0.96875, prec 0.0702308, recall 0.886054
2017-12-10T11:41:43.389068: step 3208, loss 0.0996603, acc 0.96875, prec 0.070227, recall 0.886054
2017-12-10T11:41:43.825940: step 3209, loss 0.0765664, acc 0.96875, prec 0.0702733, recall 0.886132
2017-12-10T11:41:44.268079: step 3210, loss 0.0653994, acc 0.96875, prec 0.0702695, recall 0.886132
2017-12-10T11:41:44.716036: step 3211, loss 0.124367, acc 0.921875, prec 0.0702601, recall 0.886132
2017-12-10T11:41:45.161595: step 3212, loss 0.131445, acc 0.96875, prec 0.0702563, recall 0.886132
2017-12-10T11:41:45.604547: step 3213, loss 0.149445, acc 0.953125, prec 0.0702506, recall 0.886132
2017-12-10T11:41:46.043039: step 3214, loss 0.211681, acc 0.96875, prec 0.0702468, recall 0.886132
2017-12-10T11:41:46.478831: step 3215, loss 0.653967, acc 0.984375, prec 0.07027, recall 0.886171
2017-12-10T11:41:46.913517: step 3216, loss 0.0703576, acc 0.984375, prec 0.0703182, recall 0.886248
2017-12-10T11:41:47.358638: step 3217, loss 0.21585, acc 0.9375, prec 0.0703106, recall 0.886248
2017-12-10T11:41:47.811451: step 3218, loss 0.109204, acc 0.984375, prec 0.0703588, recall 0.886325
2017-12-10T11:41:48.161338: step 3219, loss 0.0285889, acc 0.984375, prec 0.0703819, recall 0.886364
2017-12-10T11:41:48.572767: step 3220, loss 0.05409, acc 0.96875, prec 0.0703782, recall 0.886364
2017-12-10T11:41:48.967771: step 3221, loss 0.144715, acc 0.953125, prec 0.0703725, recall 0.886364
2017-12-10T11:41:49.352535: step 3222, loss 0.0801581, acc 0.953125, prec 0.0703918, recall 0.886402
2017-12-10T11:41:49.745539: step 3223, loss 0.209765, acc 0.921875, prec 0.0703823, recall 0.886402
2017-12-10T11:41:50.132150: step 3224, loss 0.0587386, acc 0.984375, prec 0.0703804, recall 0.886402
2017-12-10T11:41:50.520820: step 3225, loss 0.0195584, acc 0.984375, prec 0.0704036, recall 0.886441
2017-12-10T11:41:50.962297: step 3226, loss 0.0305031, acc 0.984375, prec 0.0704017, recall 0.886441
2017-12-10T11:41:51.413163: step 3227, loss 0.142066, acc 1, prec 0.0704267, recall 0.886479
2017-12-10T11:41:51.860526: step 3228, loss 0.311529, acc 1, prec 0.0704768, recall 0.886556
2017-12-10T11:41:52.317324: step 3229, loss 0.421961, acc 0.96875, prec 0.070523, recall 0.886633
2017-12-10T11:41:52.760434: step 3230, loss 0.0195056, acc 1, prec 0.070523, recall 0.886633
2017-12-10T11:41:53.198545: step 3231, loss 0.0354504, acc 0.984375, prec 0.0705461, recall 0.886671
2017-12-10T11:41:53.643167: step 3232, loss 0.063177, acc 0.984375, prec 0.0705442, recall 0.886671
2017-12-10T11:41:54.092459: step 3233, loss 0.0591734, acc 0.984375, prec 0.0705673, recall 0.88671
2017-12-10T11:41:54.525309: step 3234, loss 0.38971, acc 0.953125, prec 0.0705867, recall 0.886748
2017-12-10T11:41:54.971452: step 3235, loss 0.0563591, acc 0.953125, prec 0.070581, recall 0.886748
2017-12-10T11:41:55.419297: step 3236, loss 0.10406, acc 0.96875, prec 0.0705772, recall 0.886748
2017-12-10T11:41:55.866175: step 3237, loss 0.0435924, acc 0.984375, prec 0.0706003, recall 0.886786
2017-12-10T11:41:56.304806: step 3238, loss 0.0614207, acc 0.96875, prec 0.0706215, recall 0.886824
2017-12-10T11:41:56.756456: step 3239, loss 0.0775942, acc 0.984375, prec 0.0706196, recall 0.886824
2017-12-10T11:41:57.189965: step 3240, loss 0.0855105, acc 0.96875, prec 0.0706408, recall 0.886863
2017-12-10T11:41:57.621646: step 3241, loss 0.186973, acc 0.953125, prec 0.0706601, recall 0.886901
2017-12-10T11:41:58.058736: step 3242, loss 0.141231, acc 0.96875, prec 0.0706563, recall 0.886901
2017-12-10T11:41:58.495963: step 3243, loss 0.170647, acc 0.96875, prec 0.0706525, recall 0.886901
2017-12-10T11:41:58.942724: step 3244, loss 0.493181, acc 0.90625, prec 0.070691, recall 0.886977
2017-12-10T11:41:59.376269: step 3245, loss 0.229095, acc 0.9375, prec 0.0707084, recall 0.887015
2017-12-10T11:41:59.809606: step 3246, loss 0.136501, acc 0.953125, prec 0.0707027, recall 0.887015
2017-12-10T11:42:00.249054: step 3247, loss 0.0413854, acc 1, prec 0.0707527, recall 0.887091
2017-12-10T11:42:00.684577: step 3248, loss 0.68725, acc 0.9375, prec 0.070795, recall 0.887167
2017-12-10T11:42:01.132528: step 3249, loss 0.113263, acc 0.96875, prec 0.0708162, recall 0.887205
2017-12-10T11:42:01.567523: step 3250, loss 0.0522562, acc 0.984375, prec 0.0708143, recall 0.887205
2017-12-10T11:42:02.011862: step 3251, loss 0.924234, acc 0.953125, prec 0.0708336, recall 0.887243
2017-12-10T11:42:02.455945: step 3252, loss 0.443291, acc 0.921875, prec 0.070824, recall 0.887243
2017-12-10T11:42:02.901263: step 3253, loss 0.0697411, acc 0.96875, prec 0.0708202, recall 0.887243
2017-12-10T11:42:03.346529: step 3254, loss 0.0962551, acc 0.9375, prec 0.0708376, recall 0.887281
2017-12-10T11:42:03.779318: step 3255, loss 0.192393, acc 0.96875, prec 0.0708837, recall 0.887357
2017-12-10T11:42:04.216188: step 3256, loss 0.141434, acc 0.953125, prec 0.070878, recall 0.887357
2017-12-10T11:42:04.648935: step 3257, loss 0.208423, acc 0.96875, prec 0.0708742, recall 0.887357
2017-12-10T11:42:05.087907: step 3258, loss 0.24296, acc 0.890625, prec 0.0709108, recall 0.887433
2017-12-10T11:42:05.526915: step 3259, loss 0.195757, acc 0.953125, prec 0.07093, recall 0.887471
2017-12-10T11:42:05.963814: step 3260, loss 0.197301, acc 0.9375, prec 0.0709972, recall 0.887584
2017-12-10T11:42:06.420934: step 3261, loss 0.182089, acc 0.9375, prec 0.0710394, recall 0.887659
2017-12-10T11:42:06.857109: step 3262, loss 0.0407564, acc 0.984375, prec 0.0710874, recall 0.887735
2017-12-10T11:42:07.300467: step 3263, loss 0.0596141, acc 0.96875, prec 0.0711085, recall 0.887772
2017-12-10T11:42:07.740418: step 3264, loss 0.162407, acc 0.953125, prec 0.0711028, recall 0.887772
2017-12-10T11:42:08.186408: step 3265, loss 0.278636, acc 0.921875, prec 0.0710932, recall 0.887772
2017-12-10T11:42:08.626108: step 3266, loss 0.189126, acc 0.921875, prec 0.0710837, recall 0.887772
2017-12-10T11:42:09.062286: step 3267, loss 0.266743, acc 0.90625, prec 0.0710972, recall 0.88781
2017-12-10T11:42:09.492752: step 3268, loss 0.0660093, acc 0.984375, prec 0.0711202, recall 0.887847
2017-12-10T11:42:09.927045: step 3269, loss 0.440774, acc 0.890625, prec 0.0711317, recall 0.887885
2017-12-10T11:42:10.362119: step 3270, loss 0.254464, acc 0.9375, prec 0.0711739, recall 0.88796
2017-12-10T11:42:10.800546: step 3271, loss 0.0710297, acc 0.984375, prec 0.0712467, recall 0.888072
2017-12-10T11:42:11.239186: step 3272, loss 0.109132, acc 0.953125, prec 0.0712658, recall 0.88811
2017-12-10T11:42:11.696291: step 3273, loss 0.0640204, acc 0.984375, prec 0.0712888, recall 0.888147
2017-12-10T11:42:12.141800: step 3274, loss 0.350857, acc 0.9375, prec 0.0713061, recall 0.888184
2017-12-10T11:42:12.581271: step 3275, loss 0.0164636, acc 1, prec 0.0713061, recall 0.888184
2017-12-10T11:42:13.014604: step 3276, loss 0.0106948, acc 1, prec 0.0713061, recall 0.888184
2017-12-10T11:42:13.456401: step 3277, loss 0.15368, acc 0.984375, prec 0.0713539, recall 0.888259
2017-12-10T11:42:13.893420: step 3278, loss 0.202462, acc 0.984375, prec 0.0713769, recall 0.888296
2017-12-10T11:42:14.327107: step 3279, loss 0.377181, acc 0.953125, prec 0.071396, recall 0.888333
2017-12-10T11:42:14.769331: step 3280, loss 0.142959, acc 0.96875, prec 0.0714171, recall 0.888371
2017-12-10T11:42:15.204137: step 3281, loss 0.110364, acc 0.984375, prec 0.0714649, recall 0.888445
2017-12-10T11:42:15.645466: step 3282, loss 0.219645, acc 0.96875, prec 0.0714611, recall 0.888445
2017-12-10T11:42:16.086975: step 3283, loss 0.0945893, acc 0.953125, prec 0.0714802, recall 0.888482
2017-12-10T11:42:16.530870: step 3284, loss 0.317705, acc 0.953125, prec 0.0715491, recall 0.888593
2017-12-10T11:42:16.976118: step 3285, loss 0.0281995, acc 0.984375, prec 0.0715472, recall 0.888593
2017-12-10T11:42:17.414713: step 3286, loss 0.143287, acc 0.96875, prec 0.0715682, recall 0.88863
2017-12-10T11:42:17.857705: step 3287, loss 0.20935, acc 0.9375, prec 0.0715605, recall 0.88863
2017-12-10T11:42:18.289895: step 3288, loss 0.112499, acc 0.96875, prec 0.0715815, recall 0.888667
2017-12-10T11:42:18.729130: step 3289, loss 0.563684, acc 0.953125, prec 0.0716006, recall 0.888704
2017-12-10T11:42:19.172785: step 3290, loss 0.143793, acc 0.953125, prec 0.0716197, recall 0.888741
2017-12-10T11:42:19.613463: step 3291, loss 0.0468864, acc 0.984375, prec 0.0716427, recall 0.888778
2017-12-10T11:42:20.060031: step 3292, loss 0.025803, acc 1, prec 0.0716427, recall 0.888778
2017-12-10T11:42:20.517820: step 3293, loss 0.09599, acc 0.953125, prec 0.0716369, recall 0.888778
2017-12-10T11:42:20.952337: step 3294, loss 0.0327309, acc 0.984375, prec 0.071635, recall 0.888778
2017-12-10T11:42:21.397869: step 3295, loss 0.203875, acc 0.984375, prec 0.0716828, recall 0.888852
2017-12-10T11:42:21.842907: step 3296, loss 0.0581806, acc 0.96875, prec 0.0716789, recall 0.888852
2017-12-10T11:42:22.295626: step 3297, loss 0.268841, acc 0.953125, prec 0.071698, recall 0.888889
2017-12-10T11:42:22.742990: step 3298, loss 0.322779, acc 0.9375, prec 0.0717152, recall 0.888926
2017-12-10T11:42:23.179182: step 3299, loss 0.0451963, acc 1, prec 0.07174, recall 0.888963
2017-12-10T11:42:23.615839: step 3300, loss 0.145083, acc 0.984375, prec 0.0717629, recall 0.888999
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3300

2017-12-10T11:42:25.701685: step 3301, loss 0.0246014, acc 0.984375, prec 0.0717858, recall 0.889036
2017-12-10T11:42:26.151068: step 3302, loss 0.106268, acc 0.953125, prec 0.0718049, recall 0.889073
2017-12-10T11:42:26.588848: step 3303, loss 0.0268525, acc 1, prec 0.0718297, recall 0.88911
2017-12-10T11:42:27.031176: step 3304, loss 0.100025, acc 0.953125, prec 0.0718488, recall 0.889146
2017-12-10T11:42:27.463485: step 3305, loss 0.0851746, acc 0.96875, prec 0.0718449, recall 0.889146
2017-12-10T11:42:27.903492: step 3306, loss 0.108499, acc 0.953125, prec 0.0718392, recall 0.889146
2017-12-10T11:42:28.343392: step 3307, loss 0.0938776, acc 0.953125, prec 0.0718582, recall 0.889183
2017-12-10T11:42:28.804714: step 3308, loss 0.0989752, acc 0.96875, prec 0.071904, recall 0.889256
2017-12-10T11:42:29.239924: step 3309, loss 0.044735, acc 0.96875, prec 0.0719001, recall 0.889256
2017-12-10T11:42:29.676748: step 3310, loss 1.20404, acc 0.984375, prec 0.0719478, recall 0.889329
2017-12-10T11:42:30.119661: step 3311, loss 0.186257, acc 0.953125, prec 0.0719917, recall 0.889402
2017-12-10T11:42:30.560424: step 3312, loss 0.0198734, acc 1, prec 0.0719917, recall 0.889402
2017-12-10T11:42:31.002099: step 3313, loss 0.136828, acc 0.953125, prec 0.0719859, recall 0.889402
2017-12-10T11:42:31.446255: step 3314, loss 0.0861705, acc 0.953125, prec 0.0719801, recall 0.889402
2017-12-10T11:42:31.886264: step 3315, loss 0.0183407, acc 1, prec 0.0719801, recall 0.889402
2017-12-10T11:42:32.337407: step 3316, loss 0.0744449, acc 0.953125, prec 0.0719744, recall 0.889402
2017-12-10T11:42:32.773650: step 3317, loss 0.0861685, acc 0.96875, prec 0.0719705, recall 0.889402
2017-12-10T11:42:33.206262: step 3318, loss 0.123055, acc 0.953125, prec 0.0720143, recall 0.889475
2017-12-10T11:42:33.644214: step 3319, loss 0.118444, acc 0.984375, prec 0.0720372, recall 0.889512
2017-12-10T11:42:34.073827: step 3320, loss 0.138911, acc 0.984375, prec 0.0720353, recall 0.889512
2017-12-10T11:42:34.515494: step 3321, loss 0.00885325, acc 1, prec 0.07206, recall 0.889548
2017-12-10T11:42:34.959919: step 3322, loss 0.144198, acc 0.96875, prec 0.0721058, recall 0.889621
2017-12-10T11:42:35.410419: step 3323, loss 0.0502346, acc 0.984375, prec 0.0721038, recall 0.889621
2017-12-10T11:42:35.854269: step 3324, loss 0.0589196, acc 0.984375, prec 0.0721267, recall 0.889657
2017-12-10T11:42:36.299814: step 3325, loss 0.0900215, acc 0.9375, prec 0.0721438, recall 0.889694
2017-12-10T11:42:36.741790: step 3326, loss 0.0879047, acc 0.96875, prec 0.0721399, recall 0.889694
2017-12-10T11:42:37.191992: step 3327, loss 0.106684, acc 0.96875, prec 0.0721361, recall 0.889694
2017-12-10T11:42:37.631725: step 3328, loss 0.35385, acc 0.921875, prec 0.0721512, recall 0.88973
2017-12-10T11:42:38.072141: step 3329, loss 0.446815, acc 0.96875, prec 0.0722216, recall 0.889839
2017-12-10T11:42:38.516877: step 3330, loss 0.255594, acc 0.96875, prec 0.0722425, recall 0.889875
2017-12-10T11:42:38.958987: step 3331, loss 0.182596, acc 0.921875, prec 0.0722577, recall 0.889911
2017-12-10T11:42:39.406123: step 3332, loss 0.110461, acc 0.9375, prec 0.0722499, recall 0.889911
2017-12-10T11:42:39.833916: step 3333, loss 0.284894, acc 0.921875, prec 0.0722651, recall 0.889947
2017-12-10T11:42:40.277581: step 3334, loss 0.404576, acc 0.921875, prec 0.0722802, recall 0.889984
2017-12-10T11:42:40.731316: step 3335, loss 5.95114, acc 0.90625, prec 0.0722953, recall 0.889728
2017-12-10T11:42:41.170419: step 3336, loss 0.184268, acc 0.96875, prec 0.0723656, recall 0.889836
2017-12-10T11:42:41.613052: step 3337, loss 0.220417, acc 0.9375, prec 0.0723826, recall 0.889872
2017-12-10T11:42:42.055075: step 3338, loss 0.387143, acc 0.921875, prec 0.0724224, recall 0.889944
2017-12-10T11:42:42.504593: step 3339, loss 0.155219, acc 0.90625, prec 0.0724109, recall 0.889944
2017-12-10T11:42:42.948046: step 3340, loss 0.26753, acc 0.9375, prec 0.0724526, recall 0.890016
2017-12-10T11:42:43.378445: step 3341, loss 0.307864, acc 0.90625, prec 0.072441, recall 0.890016
2017-12-10T11:42:43.827948: step 3342, loss 0.218836, acc 0.9375, prec 0.0725074, recall 0.890124
2017-12-10T11:42:44.273399: step 3343, loss 0.474102, acc 0.875, prec 0.0725413, recall 0.890196
2017-12-10T11:42:44.709838: step 3344, loss 0.446545, acc 0.890625, prec 0.0725278, recall 0.890196
2017-12-10T11:42:45.154432: step 3345, loss 0.174602, acc 0.953125, prec 0.072522, recall 0.890196
2017-12-10T11:42:45.601600: step 3346, loss 0.572604, acc 0.859375, prec 0.0725293, recall 0.890232
2017-12-10T11:42:46.043841: step 3347, loss 0.68674, acc 0.828125, prec 0.0725328, recall 0.890268
2017-12-10T11:42:46.478509: step 3348, loss 0.350047, acc 0.875, prec 0.0725667, recall 0.890339
2017-12-10T11:42:46.915629: step 3349, loss 0.265441, acc 0.890625, prec 0.0726025, recall 0.890411
2017-12-10T11:42:47.365131: step 3350, loss 0.594866, acc 0.875, prec 0.0726117, recall 0.890447
2017-12-10T11:42:47.809176: step 3351, loss 0.320258, acc 0.859375, prec 0.0725944, recall 0.890447
2017-12-10T11:42:48.240862: step 3352, loss 0.393257, acc 0.90625, prec 0.0725828, recall 0.890447
2017-12-10T11:42:48.697155: step 3353, loss 0.138164, acc 0.9375, prec 0.0725751, recall 0.890447
2017-12-10T11:42:49.133372: step 3354, loss 0.183945, acc 0.953125, prec 0.0725939, recall 0.890482
2017-12-10T11:42:49.574816: step 3355, loss 0.246267, acc 0.921875, prec 0.0726336, recall 0.890554
2017-12-10T11:42:50.013606: step 3356, loss 0.228792, acc 0.921875, prec 0.0726732, recall 0.890625
2017-12-10T11:42:50.447424: step 3357, loss 0.199466, acc 0.953125, prec 0.072692, recall 0.890661
2017-12-10T11:42:50.893049: step 3358, loss 0.392578, acc 0.9375, prec 0.0727089, recall 0.890696
2017-12-10T11:42:51.322606: step 3359, loss 0.271251, acc 0.9375, prec 0.0727258, recall 0.890732
2017-12-10T11:42:51.754813: step 3360, loss 0.0687211, acc 0.96875, prec 0.072722, recall 0.890732
2017-12-10T11:42:52.195954: step 3361, loss 0.0698988, acc 0.953125, prec 0.0727408, recall 0.890767
2017-12-10T11:42:52.635913: step 3362, loss 0.187591, acc 0.921875, prec 0.0727557, recall 0.890803
2017-12-10T11:42:53.087647: step 3363, loss 0.159964, acc 0.96875, prec 0.0727519, recall 0.890803
2017-12-10T11:42:53.524882: step 3364, loss 0.118523, acc 0.953125, prec 0.0727707, recall 0.890838
2017-12-10T11:42:53.968934: step 3365, loss 0.0688555, acc 0.984375, prec 0.0727688, recall 0.890838
2017-12-10T11:42:54.410006: step 3366, loss 0.0750229, acc 0.96875, prec 0.0727649, recall 0.890838
2017-12-10T11:42:54.846592: step 3367, loss 0.0920442, acc 0.96875, prec 0.0727857, recall 0.890874
2017-12-10T11:42:55.285927: step 3368, loss 3.09015, acc 0.953125, prec 0.0728064, recall 0.89062
2017-12-10T11:42:55.723150: step 3369, loss 0.0628048, acc 0.96875, prec 0.0728025, recall 0.89062
2017-12-10T11:42:56.163154: step 3370, loss 2.55552, acc 0.953125, prec 0.0727987, recall 0.890331
2017-12-10T11:42:56.600331: step 3371, loss 0.218086, acc 0.9375, prec 0.0728155, recall 0.890367
2017-12-10T11:42:57.030478: step 3372, loss 0.160236, acc 0.921875, prec 0.0728059, recall 0.890367
2017-12-10T11:42:57.467761: step 3373, loss 0.0910227, acc 0.96875, prec 0.0728266, recall 0.890402
2017-12-10T11:42:57.903964: step 3374, loss 0.229103, acc 0.953125, prec 0.0728946, recall 0.890509
2017-12-10T11:42:58.350741: step 3375, loss 0.206393, acc 0.921875, prec 0.0728849, recall 0.890509
2017-12-10T11:42:58.809135: step 3376, loss 0.348643, acc 0.890625, prec 0.072896, recall 0.890544
2017-12-10T11:42:59.254093: step 3377, loss 0.608601, acc 0.921875, prec 0.0729354, recall 0.890615
2017-12-10T11:42:59.695744: step 3378, loss 0.838261, acc 0.84375, prec 0.0729161, recall 0.890615
2017-12-10T11:43:00.145783: step 3379, loss 0.301554, acc 0.890625, prec 0.0729026, recall 0.890615
2017-12-10T11:43:00.593678: step 3380, loss 0.215238, acc 0.9375, prec 0.0728949, recall 0.890615
2017-12-10T11:43:01.031081: step 3381, loss 0.151528, acc 0.953125, prec 0.0729382, recall 0.890686
2017-12-10T11:43:01.477454: step 3382, loss 0.274319, acc 0.90625, prec 0.0729266, recall 0.890686
2017-12-10T11:43:01.918329: step 3383, loss 0.537233, acc 0.84375, prec 0.0729318, recall 0.890721
2017-12-10T11:43:02.359867: step 3384, loss 1.13053, acc 0.90625, prec 0.0729693, recall 0.890792
2017-12-10T11:43:02.810759: step 3385, loss 0.241269, acc 0.90625, prec 0.0729823, recall 0.890827
2017-12-10T11:43:03.253580: step 3386, loss 0.29933, acc 0.890625, prec 0.0729933, recall 0.890862
2017-12-10T11:43:03.701245: step 3387, loss 0.222235, acc 0.9375, prec 0.0730101, recall 0.890897
2017-12-10T11:43:04.140079: step 3388, loss 0.284005, acc 0.890625, prec 0.0729966, recall 0.890897
2017-12-10T11:43:04.585482: step 3389, loss 0.30769, acc 0.859375, prec 0.0730037, recall 0.890933
2017-12-10T11:43:05.023830: step 3390, loss 1.19822, acc 0.9375, prec 0.073045, recall 0.891003
2017-12-10T11:43:05.462889: step 3391, loss 0.291876, acc 0.921875, prec 0.0730353, recall 0.891003
2017-12-10T11:43:05.889323: step 3392, loss 0.617231, acc 0.921875, prec 0.0730502, recall 0.891038
2017-12-10T11:43:06.328831: step 3393, loss 0.149996, acc 0.96875, prec 0.0730463, recall 0.891038
2017-12-10T11:43:06.774713: step 3394, loss 0.834086, acc 0.921875, prec 0.0730612, recall 0.891073
2017-12-10T11:43:07.223016: step 3395, loss 0.172923, acc 0.953125, prec 0.0730554, recall 0.891073
2017-12-10T11:43:07.669060: step 3396, loss 0.684986, acc 0.859375, prec 0.0730625, recall 0.891108
2017-12-10T11:43:08.108127: step 3397, loss 0.137932, acc 0.96875, prec 0.0731566, recall 0.891248
2017-12-10T11:43:08.547820: step 3398, loss 0.120054, acc 0.984375, prec 0.0731546, recall 0.891248
2017-12-10T11:43:08.985718: step 3399, loss 0.417605, acc 0.9375, prec 0.0731959, recall 0.891318
2017-12-10T11:43:09.443874: step 3400, loss 0.300365, acc 0.953125, prec 0.0732145, recall 0.891353
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3400

2017-12-10T11:43:11.568325: step 3401, loss 0.263912, acc 0.9375, prec 0.0732068, recall 0.891353
2017-12-10T11:43:12.018157: step 3402, loss 0.326019, acc 0.921875, prec 0.0731971, recall 0.891353
2017-12-10T11:43:12.455703: step 3403, loss 0.128059, acc 0.96875, prec 0.0731933, recall 0.891353
2017-12-10T11:43:12.896817: step 3404, loss 0.118805, acc 0.96875, prec 0.0731894, recall 0.891353
2017-12-10T11:43:13.332168: step 3405, loss 0.190052, acc 0.96875, prec 0.0731855, recall 0.891353
2017-12-10T11:43:13.777357: step 3406, loss 0.277237, acc 0.9375, prec 0.0731778, recall 0.891353
2017-12-10T11:43:14.217097: step 3407, loss 1.20677, acc 0.96875, prec 0.0731984, recall 0.891388
2017-12-10T11:43:14.658159: step 3408, loss 0.359179, acc 0.921875, prec 0.0731888, recall 0.891388
2017-12-10T11:43:15.091246: step 3409, loss 0.0861419, acc 0.953125, prec 0.0732074, recall 0.891423
2017-12-10T11:43:15.530276: step 3410, loss 2.5724, acc 0.96875, prec 0.0732055, recall 0.891137
2017-12-10T11:43:15.970096: step 3411, loss 0.256392, acc 0.9375, prec 0.0731978, recall 0.891137
2017-12-10T11:43:16.407625: step 3412, loss 0.221424, acc 0.890625, prec 0.0731842, recall 0.891137
2017-12-10T11:43:16.853635: step 3413, loss 0.223869, acc 0.96875, prec 0.0731804, recall 0.891137
2017-12-10T11:43:17.300408: step 3414, loss 0.110776, acc 0.953125, prec 0.0732235, recall 0.891207
2017-12-10T11:43:17.737854: step 3415, loss 0.142461, acc 0.9375, prec 0.0732402, recall 0.891242
2017-12-10T11:43:18.169871: step 3416, loss 0.40367, acc 0.859375, prec 0.0732472, recall 0.891276
2017-12-10T11:43:18.608865: step 3417, loss 0.152896, acc 0.96875, prec 0.0732922, recall 0.891346
2017-12-10T11:43:19.059507: step 3418, loss 0.607633, acc 0.875, prec 0.0732768, recall 0.891346
2017-12-10T11:43:19.499378: step 3419, loss 0.223517, acc 0.953125, prec 0.073271, recall 0.891346
2017-12-10T11:43:19.942423: step 3420, loss 0.346205, acc 0.921875, prec 0.0732613, recall 0.891346
2017-12-10T11:43:20.381987: step 3421, loss 0.131403, acc 0.96875, prec 0.0732819, recall 0.891381
2017-12-10T11:43:20.832185: step 3422, loss 1.18701, acc 0.984375, prec 0.0732819, recall 0.891095
2017-12-10T11:43:21.277094: step 3423, loss 0.528751, acc 0.9375, prec 0.0733474, recall 0.8912
2017-12-10T11:43:21.702419: step 3424, loss 0.394777, acc 0.921875, prec 0.0733621, recall 0.891235
2017-12-10T11:43:22.143504: step 3425, loss 0.199399, acc 0.90625, prec 0.0733505, recall 0.891235
2017-12-10T11:43:22.593736: step 3426, loss 0.241859, acc 0.90625, prec 0.0733389, recall 0.891235
2017-12-10T11:43:23.044435: step 3427, loss 0.384673, acc 0.90625, prec 0.0733761, recall 0.891304
2017-12-10T11:43:23.491569: step 3428, loss 0.295381, acc 0.90625, prec 0.0733646, recall 0.891304
2017-12-10T11:43:23.934588: step 3429, loss 0.226181, acc 0.921875, prec 0.0733549, recall 0.891304
2017-12-10T11:43:24.384595: step 3430, loss 0.166115, acc 0.953125, prec 0.0733979, recall 0.891374
2017-12-10T11:43:24.816550: step 3431, loss 0.16824, acc 0.921875, prec 0.0734126, recall 0.891409
2017-12-10T11:43:25.249073: step 3432, loss 0.259628, acc 0.921875, prec 0.0734029, recall 0.891409
2017-12-10T11:43:25.688759: step 3433, loss 0.37314, acc 0.84375, prec 0.0734324, recall 0.891478
2017-12-10T11:43:26.123468: step 3434, loss 1.9201, acc 0.859375, prec 0.0734169, recall 0.891193
2017-12-10T11:43:26.576556: step 3435, loss 0.303343, acc 0.984375, prec 0.0734637, recall 0.891263
2017-12-10T11:43:27.029126: step 3436, loss 0.155915, acc 0.9375, prec 0.073529, recall 0.891367
2017-12-10T11:43:27.485468: step 3437, loss 0.321993, acc 0.890625, prec 0.0735398, recall 0.891401
2017-12-10T11:43:27.934287: step 3438, loss 0.124666, acc 0.9375, prec 0.0735321, recall 0.891401
2017-12-10T11:43:28.377202: step 3439, loss 0.482558, acc 0.875, prec 0.0735653, recall 0.89147
2017-12-10T11:43:28.832107: step 3440, loss 0.44633, acc 0.9375, prec 0.0735819, recall 0.891505
2017-12-10T11:43:29.280772: step 3441, loss 0.237048, acc 0.90625, prec 0.0735703, recall 0.891505
2017-12-10T11:43:29.728203: step 3442, loss 0.507503, acc 0.9375, prec 0.0735869, recall 0.891539
2017-12-10T11:43:30.178545: step 3443, loss 0.44708, acc 0.953125, prec 0.0735811, recall 0.891539
2017-12-10T11:43:30.615204: step 3444, loss 0.169641, acc 0.953125, prec 0.0735753, recall 0.891539
2017-12-10T11:43:31.055748: step 3445, loss 0.267372, acc 0.921875, prec 0.0735657, recall 0.891539
2017-12-10T11:43:31.502033: step 3446, loss 0.160643, acc 0.953125, prec 0.0736085, recall 0.891608
2017-12-10T11:43:31.930273: step 3447, loss 0.268135, acc 0.890625, prec 0.073595, recall 0.891608
2017-12-10T11:43:32.367119: step 3448, loss 0.279125, acc 0.90625, prec 0.0736563, recall 0.891712
2017-12-10T11:43:32.793652: step 3449, loss 0.105975, acc 0.984375, prec 0.0736544, recall 0.891712
2017-12-10T11:43:33.231349: step 3450, loss 0.184259, acc 0.9375, prec 0.0736467, recall 0.891712
2017-12-10T11:43:33.679160: step 3451, loss 0.279058, acc 0.90625, prec 0.0736351, recall 0.891712
2017-12-10T11:43:34.115979: step 3452, loss 0.255679, acc 0.9375, prec 0.0736274, recall 0.891712
2017-12-10T11:43:34.548575: step 3453, loss 0.0666774, acc 0.96875, prec 0.0736235, recall 0.891712
2017-12-10T11:43:34.986710: step 3454, loss 0.18169, acc 0.953125, prec 0.073642, recall 0.891746
2017-12-10T11:43:35.428446: step 3455, loss 0.14233, acc 0.9375, prec 0.0736828, recall 0.891815
2017-12-10T11:43:35.866884: step 3456, loss 0.175971, acc 0.953125, prec 0.073677, recall 0.891815
2017-12-10T11:43:36.304611: step 3457, loss 0.089651, acc 0.96875, prec 0.0736975, recall 0.891849
2017-12-10T11:43:36.744883: step 3458, loss 0.922362, acc 0.953125, prec 0.0738373, recall 0.892054
2017-12-10T11:43:37.177036: step 3459, loss 0.0787998, acc 0.96875, prec 0.0738334, recall 0.892054
2017-12-10T11:43:37.615094: step 3460, loss 0.0522941, acc 0.96875, prec 0.0738295, recall 0.892054
2017-12-10T11:43:38.055235: step 3461, loss 0.147558, acc 0.96875, prec 0.0738742, recall 0.892123
2017-12-10T11:43:38.496426: step 3462, loss 0.455465, acc 0.984375, prec 0.0739208, recall 0.892191
2017-12-10T11:43:38.929589: step 3463, loss 0.0727107, acc 0.96875, prec 0.0739169, recall 0.892191
2017-12-10T11:43:39.377385: step 3464, loss 0.20069, acc 0.953125, prec 0.0739111, recall 0.892191
2017-12-10T11:43:39.825129: step 3465, loss 0.199902, acc 0.984375, prec 0.0739577, recall 0.892259
2017-12-10T11:43:40.262452: step 3466, loss 0.0150956, acc 1, prec 0.0739577, recall 0.892259
2017-12-10T11:43:40.703101: step 3467, loss 0.197189, acc 0.921875, prec 0.0739722, recall 0.892293
2017-12-10T11:43:41.137098: step 3468, loss 0.0906803, acc 0.953125, prec 0.0739907, recall 0.892327
2017-12-10T11:43:41.568462: step 3469, loss 0.086148, acc 0.96875, prec 0.0740595, recall 0.892429
2017-12-10T11:43:42.001485: step 3470, loss 0.264278, acc 0.96875, prec 0.0740556, recall 0.892429
2017-12-10T11:43:42.446561: step 3471, loss 0.176587, acc 0.96875, prec 0.0740518, recall 0.892429
2017-12-10T11:43:42.886604: step 3472, loss 0.136266, acc 0.921875, prec 0.0740906, recall 0.892497
2017-12-10T11:43:43.334441: step 3473, loss 0.325811, acc 0.953125, prec 0.0741332, recall 0.892565
2017-12-10T11:43:43.769809: step 3474, loss 0.173048, acc 0.953125, prec 0.0741516, recall 0.892598
2017-12-10T11:43:44.217884: step 3475, loss 0.3316, acc 0.921875, prec 0.0741419, recall 0.892598
2017-12-10T11:43:44.659472: step 3476, loss 0.136057, acc 0.953125, prec 0.0741361, recall 0.892598
2017-12-10T11:43:45.098002: step 3477, loss 0.214395, acc 0.953125, prec 0.0741303, recall 0.892598
2017-12-10T11:43:45.543131: step 3478, loss 5.57339, acc 0.953125, prec 0.0741264, recall 0.892317
2017-12-10T11:43:45.944269: step 3479, loss 0.0430523, acc 0.960784, prec 0.0741709, recall 0.892385
2017-12-10T11:43:46.400979: step 3480, loss 0.0338793, acc 1, prec 0.0741709, recall 0.892385
2017-12-10T11:43:46.853050: step 3481, loss 0.0994717, acc 0.96875, prec 0.0741913, recall 0.892419
2017-12-10T11:43:47.299116: step 3482, loss 0.732866, acc 0.9375, prec 0.0742561, recall 0.89252
2017-12-10T11:43:47.756669: step 3483, loss 0.232282, acc 0.90625, prec 0.0742687, recall 0.892554
2017-12-10T11:43:48.207882: step 3484, loss 0.161997, acc 0.9375, prec 0.0742851, recall 0.892588
2017-12-10T11:43:48.647392: step 3485, loss 0.281699, acc 0.890625, prec 0.0742957, recall 0.892622
2017-12-10T11:43:49.085847: step 3486, loss 0.151482, acc 0.921875, prec 0.0743102, recall 0.892655
2017-12-10T11:43:49.533066: step 3487, loss 0.391873, acc 0.890625, prec 0.0743208, recall 0.892689
2017-12-10T11:43:49.963683: step 3488, loss 0.31954, acc 0.890625, prec 0.0743072, recall 0.892689
2017-12-10T11:43:50.399200: step 3489, loss 0.417879, acc 0.859375, prec 0.0743139, recall 0.892723
2017-12-10T11:43:50.827865: step 3490, loss 0.362924, acc 0.875, prec 0.0742984, recall 0.892723
2017-12-10T11:43:51.263089: step 3491, loss 0.387712, acc 0.90625, prec 0.0743109, recall 0.892756
2017-12-10T11:43:51.700323: step 3492, loss 0.150486, acc 0.9375, prec 0.0743273, recall 0.89279
2017-12-10T11:43:52.154527: step 3493, loss 0.638822, acc 0.796875, prec 0.0743263, recall 0.892824
2017-12-10T11:43:52.604218: step 3494, loss 0.351274, acc 0.890625, prec 0.0743127, recall 0.892824
2017-12-10T11:43:53.042875: step 3495, loss 0.35165, acc 0.875, prec 0.0742972, recall 0.892824
2017-12-10T11:43:53.482799: step 3496, loss 0.142983, acc 0.953125, prec 0.0742914, recall 0.892824
2017-12-10T11:43:53.914148: step 3497, loss 0.171359, acc 0.953125, prec 0.0742856, recall 0.892824
2017-12-10T11:43:54.349796: step 3498, loss 0.0893667, acc 0.96875, prec 0.0743058, recall 0.892857
2017-12-10T11:43:54.791347: step 3499, loss 0.264769, acc 0.890625, prec 0.0743647, recall 0.892958
2017-12-10T11:43:55.229390: step 3500, loss 0.0679172, acc 0.984375, prec 0.0743868, recall 0.892991
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3500

2017-12-10T11:43:57.019125: step 3501, loss 0.483017, acc 0.875, prec 0.0743713, recall 0.892991
2017-12-10T11:43:57.462307: step 3502, loss 0.0443318, acc 0.984375, prec 0.0743935, recall 0.893025
2017-12-10T11:43:57.908640: step 3503, loss 0.224718, acc 0.953125, prec 0.0744118, recall 0.893058
2017-12-10T11:43:58.348738: step 3504, loss 0.0486379, acc 0.984375, prec 0.0744581, recall 0.893125
2017-12-10T11:43:58.796523: step 3505, loss 0.193785, acc 0.96875, prec 0.0744542, recall 0.893125
2017-12-10T11:43:59.239132: step 3506, loss 0.509967, acc 0.9375, prec 0.0744947, recall 0.893192
2017-12-10T11:43:59.683463: step 3507, loss 0.11359, acc 0.984375, prec 0.0745169, recall 0.893225
2017-12-10T11:44:00.117896: step 3508, loss 0.0637931, acc 0.96875, prec 0.0745371, recall 0.893258
2017-12-10T11:44:00.543961: step 3509, loss 0.0527937, acc 0.96875, prec 0.0745573, recall 0.893292
2017-12-10T11:44:00.996462: step 3510, loss 0.0111421, acc 1, prec 0.0745573, recall 0.893292
2017-12-10T11:44:01.434524: step 3511, loss 0.123679, acc 0.953125, prec 0.0745997, recall 0.893358
2017-12-10T11:44:01.882824: step 3512, loss 0.0396817, acc 0.96875, prec 0.0745958, recall 0.893358
2017-12-10T11:44:02.342116: step 3513, loss 0.0330975, acc 0.96875, prec 0.074616, recall 0.893392
2017-12-10T11:44:02.788836: step 3514, loss 0.201527, acc 0.96875, prec 0.0746844, recall 0.893491
2017-12-10T11:44:03.234450: step 3515, loss 0.0713776, acc 0.96875, prec 0.0746805, recall 0.893491
2017-12-10T11:44:03.660318: step 3516, loss 0.429533, acc 0.984375, prec 0.0747267, recall 0.893557
2017-12-10T11:44:04.101470: step 3517, loss 0.137373, acc 0.984375, prec 0.0747248, recall 0.893557
2017-12-10T11:44:04.538125: step 3518, loss 0.181687, acc 0.96875, prec 0.0747209, recall 0.893557
2017-12-10T11:44:04.975983: step 3519, loss 0.115312, acc 1, prec 0.0747931, recall 0.893657
2017-12-10T11:44:05.427522: step 3520, loss 0.0779892, acc 0.96875, prec 0.0748374, recall 0.893723
2017-12-10T11:44:05.870944: step 3521, loss 0.221822, acc 0.953125, prec 0.0748315, recall 0.893723
2017-12-10T11:44:06.311078: step 3522, loss 0.265318, acc 0.96875, prec 0.0749239, recall 0.893855
2017-12-10T11:44:06.757218: step 3523, loss 0.325779, acc 0.953125, prec 0.0749421, recall 0.893888
2017-12-10T11:44:07.202916: step 3524, loss 0.0868, acc 0.953125, prec 0.0749363, recall 0.893888
2017-12-10T11:44:07.648861: step 3525, loss 0.0473687, acc 0.96875, prec 0.0749324, recall 0.893888
2017-12-10T11:44:08.088759: step 3526, loss 0.0221487, acc 0.984375, prec 0.0749545, recall 0.893921
2017-12-10T11:44:08.543259: step 3527, loss 0.0269061, acc 1, prec 0.0750026, recall 0.893986
2017-12-10T11:44:08.989325: step 3528, loss 0.0373446, acc 0.984375, prec 0.0750007, recall 0.893986
2017-12-10T11:44:09.443553: step 3529, loss 0.029394, acc 1, prec 0.0750247, recall 0.894019
2017-12-10T11:44:09.891926: step 3530, loss 0.0150795, acc 1, prec 0.0750488, recall 0.894052
2017-12-10T11:44:10.325911: step 3531, loss 0.0293497, acc 0.984375, prec 0.0750949, recall 0.894118
2017-12-10T11:44:10.765702: step 3532, loss 0.0144775, acc 1, prec 0.0750949, recall 0.894118
2017-12-10T11:44:11.226079: step 3533, loss 0.0049424, acc 1, prec 0.075119, recall 0.89415
2017-12-10T11:44:11.675099: step 3534, loss 0.179638, acc 1, prec 0.075143, recall 0.894183
2017-12-10T11:44:12.120408: step 3535, loss 0.00715447, acc 1, prec 0.075143, recall 0.894183
2017-12-10T11:44:12.573234: step 3536, loss 0.111182, acc 0.953125, prec 0.0751612, recall 0.894216
2017-12-10T11:44:13.008660: step 3537, loss 0.00989214, acc 1, prec 0.0751612, recall 0.894216
2017-12-10T11:44:13.450611: step 3538, loss 0.0198494, acc 1, prec 0.0751852, recall 0.894249
2017-12-10T11:44:13.890527: step 3539, loss 2.6017, acc 0.984375, prec 0.0751852, recall 0.893972
2017-12-10T11:44:14.343921: step 3540, loss 0.0787474, acc 1, prec 0.0753054, recall 0.894136
2017-12-10T11:44:14.782545: step 3541, loss 0.0545083, acc 0.984375, prec 0.0753275, recall 0.894168
2017-12-10T11:44:15.220108: step 3542, loss 0.188884, acc 0.921875, prec 0.0753177, recall 0.894168
2017-12-10T11:44:15.660128: step 3543, loss 0.281467, acc 0.953125, prec 0.0753599, recall 0.894234
2017-12-10T11:44:16.099706: step 3544, loss 0.0580141, acc 0.953125, prec 0.075354, recall 0.894234
2017-12-10T11:44:16.553410: step 3545, loss 0.326651, acc 0.96875, prec 0.0753741, recall 0.894266
2017-12-10T11:44:17.007149: step 3546, loss 0.308226, acc 0.90625, prec 0.0754104, recall 0.894331
2017-12-10T11:44:17.439673: step 3547, loss 0.221234, acc 0.953125, prec 0.0754046, recall 0.894331
2017-12-10T11:44:17.878301: step 3548, loss 0.203684, acc 0.9375, prec 0.0753967, recall 0.894331
2017-12-10T11:44:18.312419: step 3549, loss 0.334518, acc 0.90625, prec 0.075409, recall 0.894364
2017-12-10T11:44:18.758976: step 3550, loss 0.78337, acc 0.9375, prec 0.0754492, recall 0.894429
2017-12-10T11:44:19.206592: step 3551, loss 0.18052, acc 0.90625, prec 0.0754374, recall 0.894429
2017-12-10T11:44:19.644401: step 3552, loss 0.403902, acc 0.875, prec 0.0754697, recall 0.894494
2017-12-10T11:44:20.094339: step 3553, loss 0.162731, acc 0.9375, prec 0.0755099, recall 0.894559
2017-12-10T11:44:20.550812: step 3554, loss 0.141182, acc 0.953125, prec 0.075528, recall 0.894591
2017-12-10T11:44:20.990090: step 3555, loss 0.221315, acc 0.875, prec 0.0755363, recall 0.894624
2017-12-10T11:44:21.426396: step 3556, loss 0.229402, acc 0.9375, prec 0.0755285, recall 0.894624
2017-12-10T11:44:21.877439: step 3557, loss 0.317272, acc 0.921875, prec 0.0755426, recall 0.894656
2017-12-10T11:44:22.310661: step 3558, loss 0.184421, acc 0.921875, prec 0.0755568, recall 0.894688
2017-12-10T11:44:22.748067: step 3559, loss 0.150431, acc 0.9375, prec 0.0755969, recall 0.894753
2017-12-10T11:44:23.181112: step 3560, loss 0.155026, acc 0.921875, prec 0.0756111, recall 0.894785
2017-12-10T11:44:23.615974: step 3561, loss 0.262108, acc 0.90625, prec 0.0756233, recall 0.894818
2017-12-10T11:44:24.039525: step 3562, loss 0.416306, acc 0.875, prec 0.0756316, recall 0.89485
2017-12-10T11:44:24.481849: step 3563, loss 0.26477, acc 0.921875, prec 0.0756697, recall 0.894914
2017-12-10T11:44:24.933613: step 3564, loss 0.154677, acc 0.90625, prec 0.0756579, recall 0.894914
2017-12-10T11:44:25.371776: step 3565, loss 0.0711601, acc 0.96875, prec 0.0757019, recall 0.894979
2017-12-10T11:44:25.825496: step 3566, loss 0.430097, acc 0.921875, prec 0.0756921, recall 0.894979
2017-12-10T11:44:26.261924: step 3567, loss 0.242804, acc 0.90625, prec 0.0757282, recall 0.895043
2017-12-10T11:44:26.695257: step 3568, loss 0.173863, acc 0.953125, prec 0.075794, recall 0.895139
2017-12-10T11:44:27.124657: step 3569, loss 0.601521, acc 0.921875, prec 0.0757842, recall 0.895139
2017-12-10T11:44:27.565609: step 3570, loss 0.0630922, acc 0.984375, prec 0.075854, recall 0.895235
2017-12-10T11:44:27.995309: step 3571, loss 0.225005, acc 0.953125, prec 0.0758481, recall 0.895235
2017-12-10T11:44:28.436157: step 3572, loss 0.13235, acc 0.96875, prec 0.075892, recall 0.895299
2017-12-10T11:44:28.875227: step 3573, loss 0.117508, acc 0.9375, prec 0.0759081, recall 0.895331
2017-12-10T11:44:29.318488: step 3574, loss 0.0885024, acc 0.953125, prec 0.0759022, recall 0.895331
2017-12-10T11:44:29.754621: step 3575, loss 0.0342354, acc 0.984375, prec 0.0759002, recall 0.895331
2017-12-10T11:44:30.201905: step 3576, loss 0.0854742, acc 0.96875, prec 0.0759202, recall 0.895363
2017-12-10T11:44:30.646094: step 3577, loss 0.196317, acc 0.921875, prec 0.0759104, recall 0.895363
2017-12-10T11:44:31.084826: step 3578, loss 0.11312, acc 0.96875, prec 0.0759304, recall 0.895395
2017-12-10T11:44:31.532775: step 3579, loss 2.43071, acc 0.96875, prec 0.0759284, recall 0.895122
2017-12-10T11:44:31.988834: step 3580, loss 0.143685, acc 0.984375, prec 0.0759981, recall 0.895218
2017-12-10T11:44:32.434041: step 3581, loss 0.0516422, acc 0.96875, prec 0.0759942, recall 0.895218
2017-12-10T11:44:32.874550: step 3582, loss 0.052883, acc 0.984375, prec 0.07604, recall 0.895282
2017-12-10T11:44:33.324051: step 3583, loss 0.0932465, acc 0.96875, prec 0.0760361, recall 0.895282
2017-12-10T11:44:33.789158: step 3584, loss 0.230604, acc 0.953125, prec 0.0760302, recall 0.895282
2017-12-10T11:44:34.233277: step 3585, loss 0.135292, acc 0.953125, prec 0.0760243, recall 0.895282
2017-12-10T11:44:34.679696: step 3586, loss 0.176088, acc 0.953125, prec 0.07609, recall 0.895377
2017-12-10T11:44:35.119078: step 3587, loss 0.0421156, acc 0.984375, prec 0.076112, recall 0.895409
2017-12-10T11:44:35.565226: step 3588, loss 0.0591257, acc 0.984375, prec 0.07611, recall 0.895409
2017-12-10T11:44:36.017035: step 3589, loss 0.0882055, acc 0.96875, prec 0.0761061, recall 0.895409
2017-12-10T11:44:36.451540: step 3590, loss 0.220843, acc 0.953125, prec 0.076124, recall 0.895441
2017-12-10T11:44:36.891004: step 3591, loss 0.342463, acc 0.921875, prec 0.0761142, recall 0.895441
2017-12-10T11:44:37.325139: step 3592, loss 0.059342, acc 0.96875, prec 0.0761341, recall 0.895473
2017-12-10T11:44:37.769365: step 3593, loss 0.106415, acc 0.96875, prec 0.0761541, recall 0.895504
2017-12-10T11:44:38.211705: step 3594, loss 0.230863, acc 0.921875, prec 0.0761442, recall 0.895504
2017-12-10T11:44:38.664897: step 3595, loss 0.152224, acc 0.921875, prec 0.0761344, recall 0.895504
2017-12-10T11:44:39.102066: step 3596, loss 0.163415, acc 0.953125, prec 0.0761285, recall 0.895504
2017-12-10T11:44:39.539632: step 3597, loss 0.147765, acc 0.96875, prec 0.0761484, recall 0.895536
2017-12-10T11:44:39.979543: step 3598, loss 0.25222, acc 0.921875, prec 0.0761386, recall 0.895536
2017-12-10T11:44:40.417202: step 3599, loss 0.11655, acc 0.96875, prec 0.0761347, recall 0.895536
2017-12-10T11:44:40.858141: step 3600, loss 0.199412, acc 0.953125, prec 0.0761288, recall 0.895536
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3600

2017-12-10T11:44:42.877351: step 3601, loss 0.157722, acc 0.921875, prec 0.0761428, recall 0.895568
2017-12-10T11:44:43.322735: step 3602, loss 0.10769, acc 0.96875, prec 0.0761627, recall 0.895599
2017-12-10T11:44:43.754930: step 3603, loss 0.339877, acc 0.9375, prec 0.0762025, recall 0.895663
2017-12-10T11:44:44.213892: step 3604, loss 0.0599406, acc 0.96875, prec 0.0761986, recall 0.895663
2017-12-10T11:44:44.657182: step 3605, loss 1.96998, acc 0.9375, prec 0.0762165, recall 0.895423
2017-12-10T11:44:45.121997: step 3606, loss 0.13387, acc 0.953125, prec 0.0762106, recall 0.895423
2017-12-10T11:44:45.572998: step 3607, loss 0.0946269, acc 0.953125, prec 0.0762047, recall 0.895423
2017-12-10T11:44:46.011827: step 3608, loss 0.268565, acc 0.953125, prec 0.0762465, recall 0.895486
2017-12-10T11:44:46.457854: step 3609, loss 0.28867, acc 0.921875, prec 0.0762367, recall 0.895486
2017-12-10T11:44:46.903833: step 3610, loss 0.159388, acc 0.96875, prec 0.0762565, recall 0.895518
2017-12-10T11:44:47.338089: step 3611, loss 0.148527, acc 0.921875, prec 0.0762943, recall 0.895581
2017-12-10T11:44:47.787718: step 3612, loss 0.152797, acc 0.953125, prec 0.0762884, recall 0.895581
2017-12-10T11:44:48.234923: step 3613, loss 0.0700548, acc 1, prec 0.0763361, recall 0.895644
2017-12-10T11:44:48.668182: step 3614, loss 0.129022, acc 0.96875, prec 0.0763321, recall 0.895644
2017-12-10T11:44:49.111718: step 3615, loss 0.325449, acc 0.96875, prec 0.076352, recall 0.895676
2017-12-10T11:44:49.562606: step 3616, loss 0.0697814, acc 0.984375, prec 0.0763739, recall 0.895707
2017-12-10T11:44:50.012611: step 3617, loss 0.27625, acc 0.921875, prec 0.0763878, recall 0.895739
2017-12-10T11:44:50.454707: step 3618, loss 0.291701, acc 0.90625, prec 0.0763998, recall 0.89577
2017-12-10T11:44:50.894028: step 3619, loss 0.252172, acc 0.953125, prec 0.0764177, recall 0.895802
2017-12-10T11:44:51.329039: step 3620, loss 0.14361, acc 0.96875, prec 0.0764138, recall 0.895802
2017-12-10T11:44:51.765689: step 3621, loss 0.524962, acc 0.953125, prec 0.0764078, recall 0.895802
2017-12-10T11:44:52.208502: step 3622, loss 0.201745, acc 0.96875, prec 0.0764277, recall 0.895833
2017-12-10T11:44:52.653255: step 3623, loss 0.111479, acc 0.953125, prec 0.0764218, recall 0.895833
2017-12-10T11:44:53.100317: step 3624, loss 0.129276, acc 0.953125, prec 0.0764397, recall 0.895865
2017-12-10T11:44:53.551682: step 3625, loss 0.124223, acc 0.984375, prec 0.0764377, recall 0.895865
2017-12-10T11:44:54.001301: step 3626, loss 0.409558, acc 0.90625, prec 0.0764497, recall 0.895896
2017-12-10T11:44:54.439497: step 3627, loss 0.120447, acc 0.96875, prec 0.0764458, recall 0.895896
2017-12-10T11:44:54.892372: step 3628, loss 0.0479078, acc 0.984375, prec 0.0764438, recall 0.895896
2017-12-10T11:44:55.339664: step 3629, loss 0.0184345, acc 1, prec 0.0764438, recall 0.895896
2017-12-10T11:44:55.783840: step 3630, loss 0.0751432, acc 0.96875, prec 0.0764636, recall 0.895928
2017-12-10T11:44:56.226712: step 3631, loss 0.059681, acc 0.984375, prec 0.0764617, recall 0.895928
2017-12-10T11:44:56.668255: step 3632, loss 1.04787, acc 0.984375, prec 0.076531, recall 0.896022
2017-12-10T11:44:57.116238: step 3633, loss 0.257939, acc 0.9375, prec 0.0765469, recall 0.896053
2017-12-10T11:44:57.565052: step 3634, loss 0.344863, acc 0.90625, prec 0.0765351, recall 0.896053
2017-12-10T11:44:58.010769: step 3635, loss 0.423529, acc 0.953125, prec 0.0765767, recall 0.896116
2017-12-10T11:44:58.452916: step 3636, loss 0.0470583, acc 0.984375, prec 0.0765985, recall 0.896147
2017-12-10T11:44:58.895421: step 3637, loss 0.127484, acc 0.984375, prec 0.0766678, recall 0.896241
2017-12-10T11:44:59.334762: step 3638, loss 0.128848, acc 0.9375, prec 0.0766599, recall 0.896241
2017-12-10T11:44:59.763257: step 3639, loss 0.109984, acc 0.96875, prec 0.0766797, recall 0.896272
2017-12-10T11:45:00.191894: step 3640, loss 0.0915302, acc 0.984375, prec 0.0766777, recall 0.896272
2017-12-10T11:45:00.631103: step 3641, loss 0.0365534, acc 0.984375, prec 0.0766995, recall 0.896303
2017-12-10T11:45:01.064439: step 3642, loss 0.0540529, acc 0.96875, prec 0.0767193, recall 0.896334
2017-12-10T11:45:01.515644: step 3643, loss 0.0896362, acc 0.96875, prec 0.0767628, recall 0.896396
2017-12-10T11:45:01.954139: step 3644, loss 0.111676, acc 0.9375, prec 0.0767787, recall 0.896428
2017-12-10T11:45:02.409185: step 3645, loss 0.147484, acc 0.984375, prec 0.0767767, recall 0.896428
2017-12-10T11:45:02.857717: step 3646, loss 0.0130986, acc 1, prec 0.0768242, recall 0.89649
2017-12-10T11:45:03.313785: step 3647, loss 0.0501324, acc 0.984375, prec 0.076846, recall 0.896521
2017-12-10T11:45:03.756767: step 3648, loss 0.116577, acc 0.96875, prec 0.0768657, recall 0.896552
2017-12-10T11:45:04.204806: step 3649, loss 0.116004, acc 0.96875, prec 0.0768618, recall 0.896552
2017-12-10T11:45:04.647376: step 3650, loss 0.467006, acc 1, prec 0.076933, recall 0.896645
2017-12-10T11:45:05.088096: step 3651, loss 0.256573, acc 0.953125, prec 0.0769508, recall 0.896676
2017-12-10T11:45:05.528747: step 3652, loss 0.161908, acc 0.9375, prec 0.0769666, recall 0.896707
2017-12-10T11:45:05.964435: step 3653, loss 0.240198, acc 0.9375, prec 0.0769824, recall 0.896738
2017-12-10T11:45:06.412573: step 3654, loss 0.290375, acc 0.953125, prec 0.0770002, recall 0.896768
2017-12-10T11:45:06.852572: step 3655, loss 0.146187, acc 0.96875, prec 0.0769962, recall 0.896768
2017-12-10T11:45:07.297354: step 3656, loss 0.0714657, acc 0.96875, prec 0.077016, recall 0.896799
2017-12-10T11:45:07.740092: step 3657, loss 0.223672, acc 0.9375, prec 0.0770555, recall 0.896861
2017-12-10T11:45:08.180649: step 3658, loss 0.0693825, acc 0.984375, prec 0.0771009, recall 0.896923
2017-12-10T11:45:08.635288: step 3659, loss 0.129923, acc 0.9375, prec 0.077093, recall 0.896923
2017-12-10T11:45:09.086389: step 3660, loss 0.0951434, acc 0.96875, prec 0.0771127, recall 0.896953
2017-12-10T11:45:09.530313: step 3661, loss 0.0711063, acc 0.96875, prec 0.0771087, recall 0.896953
2017-12-10T11:45:09.970802: step 3662, loss 0.110437, acc 0.96875, prec 0.0771048, recall 0.896953
2017-12-10T11:45:10.418746: step 3663, loss 0.86522, acc 0.921875, prec 0.0771186, recall 0.896984
2017-12-10T11:45:10.869407: step 3664, loss 0.422156, acc 0.984375, prec 0.077164, recall 0.897046
2017-12-10T11:45:11.303293: step 3665, loss 0.216987, acc 0.96875, prec 0.0771837, recall 0.897076
2017-12-10T11:45:11.743635: step 3666, loss 0.238213, acc 0.96875, prec 0.0772034, recall 0.897107
2017-12-10T11:45:12.192785: step 3667, loss 0.0698805, acc 0.953125, prec 0.0771975, recall 0.897107
2017-12-10T11:45:12.634614: step 3668, loss 3.33479, acc 0.921875, prec 0.0771896, recall 0.89684
2017-12-10T11:45:13.085397: step 3669, loss 0.0551913, acc 0.96875, prec 0.0771856, recall 0.89684
2017-12-10T11:45:13.524757: step 3670, loss 0.19422, acc 0.90625, prec 0.0771974, recall 0.89687
2017-12-10T11:45:13.969223: step 3671, loss 0.427503, acc 0.921875, prec 0.0772348, recall 0.896932
2017-12-10T11:45:14.412647: step 3672, loss 0.528466, acc 0.890625, prec 0.0772446, recall 0.896962
2017-12-10T11:45:14.854345: step 3673, loss 0.376026, acc 0.921875, prec 0.0772584, recall 0.896993
2017-12-10T11:45:15.298097: step 3674, loss 0.54798, acc 0.859375, prec 0.0772406, recall 0.896993
2017-12-10T11:45:15.730305: step 3675, loss 0.71129, acc 0.796875, prec 0.0772385, recall 0.897024
2017-12-10T11:45:16.172987: step 3676, loss 0.233537, acc 0.953125, prec 0.0772325, recall 0.897024
2017-12-10T11:45:16.617776: step 3677, loss 0.235338, acc 0.890625, prec 0.0772423, recall 0.897054
2017-12-10T11:45:17.052607: step 3678, loss 0.584801, acc 0.859375, prec 0.0772482, recall 0.897085
2017-12-10T11:45:17.503908: step 3679, loss 0.31068, acc 0.875, prec 0.0772796, recall 0.897146
2017-12-10T11:45:17.945726: step 3680, loss 0.421277, acc 0.890625, prec 0.0772894, recall 0.897177
2017-12-10T11:45:18.377752: step 3681, loss 0.608797, acc 0.890625, prec 0.0772991, recall 0.897207
2017-12-10T11:45:18.805688: step 3682, loss 0.410636, acc 0.875, prec 0.0773069, recall 0.897238
2017-12-10T11:45:19.249203: step 3683, loss 0.464038, acc 0.875, prec 0.0772911, recall 0.897238
2017-12-10T11:45:19.689177: step 3684, loss 0.230704, acc 0.90625, prec 0.0773264, recall 0.897299
2017-12-10T11:45:20.121674: step 3685, loss 0.202728, acc 0.921875, prec 0.0773402, recall 0.897329
2017-12-10T11:45:20.566441: step 3686, loss 0.162708, acc 0.921875, prec 0.0773303, recall 0.897329
2017-12-10T11:45:21.008383: step 3687, loss 0.107974, acc 0.953125, prec 0.0773243, recall 0.897329
2017-12-10T11:45:21.455305: step 3688, loss 0.313095, acc 0.921875, prec 0.0773145, recall 0.897329
2017-12-10T11:45:21.898530: step 3689, loss 0.427491, acc 0.84375, prec 0.0772947, recall 0.897329
2017-12-10T11:45:22.346139: step 3690, loss 0.492302, acc 0.921875, prec 0.0773084, recall 0.89736
2017-12-10T11:45:22.790925: step 3691, loss 0.339026, acc 0.890625, prec 0.0773181, recall 0.89739
2017-12-10T11:45:23.234714: step 3692, loss 0.293225, acc 0.90625, prec 0.0773534, recall 0.897451
2017-12-10T11:45:23.670997: step 3693, loss 0.183057, acc 0.9375, prec 0.0773691, recall 0.897482
2017-12-10T11:45:24.115865: step 3694, loss 0.204649, acc 0.984375, prec 0.0774142, recall 0.897542
2017-12-10T11:45:24.556510: step 3695, loss 0.130678, acc 0.984375, prec 0.0774358, recall 0.897573
2017-12-10T11:45:25.010527: step 3696, loss 0.108201, acc 0.984375, prec 0.0774574, recall 0.897603
2017-12-10T11:45:25.455273: step 3697, loss 0.0497314, acc 0.96875, prec 0.0774535, recall 0.897603
2017-12-10T11:45:25.921793: step 3698, loss 0.118215, acc 0.96875, prec 0.0774495, recall 0.897603
2017-12-10T11:45:26.354985: step 3699, loss 0.0909714, acc 1, prec 0.0774731, recall 0.897633
2017-12-10T11:45:26.805862: step 3700, loss 0.124735, acc 1, prec 0.0775437, recall 0.897724
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3700

2017-12-10T11:45:28.610610: step 3701, loss 0.326544, acc 0.921875, prec 0.0775574, recall 0.897754
2017-12-10T11:45:29.038854: step 3702, loss 0.18173, acc 0.984375, prec 0.0776025, recall 0.897815
2017-12-10T11:45:29.474634: step 3703, loss 0.0189906, acc 1, prec 0.0776025, recall 0.897815
2017-12-10T11:45:29.925168: step 3704, loss 0.0325419, acc 1, prec 0.077626, recall 0.897845
2017-12-10T11:45:30.351569: step 3705, loss 0.0736628, acc 0.984375, prec 0.0776241, recall 0.897845
2017-12-10T11:45:30.803960: step 3706, loss 0.0740022, acc 0.96875, prec 0.0776436, recall 0.897875
2017-12-10T11:45:31.253487: step 3707, loss 0.0177749, acc 1, prec 0.0776672, recall 0.897905
2017-12-10T11:45:31.707818: step 3708, loss 0.00172446, acc 1, prec 0.0776672, recall 0.897905
2017-12-10T11:45:32.145736: step 3709, loss 0.16813, acc 0.96875, prec 0.0776632, recall 0.897905
2017-12-10T11:45:32.584271: step 3710, loss 0.142262, acc 1, prec 0.0776867, recall 0.897935
2017-12-10T11:45:33.036325: step 3711, loss 0.326982, acc 0.984375, prec 0.0777789, recall 0.898055
2017-12-10T11:45:33.483794: step 3712, loss 0.016411, acc 1, prec 0.0777789, recall 0.898055
2017-12-10T11:45:33.931726: step 3713, loss 0.16428, acc 0.953125, prec 0.0777965, recall 0.898085
2017-12-10T11:45:34.376491: step 3714, loss 0.268807, acc 0.984375, prec 0.077818, recall 0.898115
2017-12-10T11:45:34.814950: step 3715, loss 0.171271, acc 0.96875, prec 0.0778846, recall 0.898205
2017-12-10T11:45:35.252423: step 3716, loss 0.00565123, acc 1, prec 0.0779082, recall 0.898235
2017-12-10T11:45:35.707909: step 3717, loss 0.0763494, acc 0.96875, prec 0.0779042, recall 0.898235
2017-12-10T11:45:36.152173: step 3718, loss 0.0794538, acc 1, prec 0.0779512, recall 0.898295
2017-12-10T11:45:36.600997: step 3719, loss 0.0519029, acc 0.984375, prec 0.0779492, recall 0.898295
2017-12-10T11:45:37.058773: step 3720, loss 0.045093, acc 0.984375, prec 0.0779473, recall 0.898295
2017-12-10T11:45:37.497905: step 3721, loss 0.304716, acc 0.953125, prec 0.0779648, recall 0.898325
2017-12-10T11:45:37.936768: step 3722, loss 0.168946, acc 0.953125, prec 0.0780059, recall 0.898385
2017-12-10T11:45:38.379858: step 3723, loss 0.402633, acc 1, prec 0.0780999, recall 0.898504
2017-12-10T11:45:38.825008: step 3724, loss 0.0480002, acc 0.984375, prec 0.0780979, recall 0.898504
2017-12-10T11:45:39.266159: step 3725, loss 1.09195, acc 0.96875, prec 0.0781409, recall 0.898563
2017-12-10T11:45:39.700477: step 3726, loss 0.048435, acc 0.984375, prec 0.0781624, recall 0.898593
2017-12-10T11:45:40.136169: step 3727, loss 0.0438351, acc 0.984375, prec 0.0781839, recall 0.898623
2017-12-10T11:45:40.580775: step 3728, loss 0.151368, acc 0.953125, prec 0.078225, recall 0.898682
2017-12-10T11:45:41.032623: step 3729, loss 3.4865, acc 0.953125, prec 0.078268, recall 0.898479
2017-12-10T11:45:41.486515: step 3730, loss 0.162566, acc 0.953125, prec 0.0782855, recall 0.898508
2017-12-10T11:45:41.923649: step 3731, loss 0.145597, acc 0.96875, prec 0.0783285, recall 0.898568
2017-12-10T11:45:42.355922: step 3732, loss 0.498254, acc 0.84375, prec 0.0783085, recall 0.898568
2017-12-10T11:45:42.807994: step 3733, loss 0.731339, acc 0.796875, prec 0.078306, recall 0.898597
2017-12-10T11:45:43.257057: step 3734, loss 0.459214, acc 0.859375, prec 0.0782881, recall 0.898597
2017-12-10T11:45:43.698933: step 3735, loss 0.521418, acc 0.875, prec 0.0782956, recall 0.898627
2017-12-10T11:45:44.134692: step 3736, loss 0.600092, acc 0.828125, prec 0.0782737, recall 0.898627
2017-12-10T11:45:44.579187: step 3737, loss 0.367383, acc 0.890625, prec 0.0782598, recall 0.898627
2017-12-10T11:45:45.030413: step 3738, loss 0.714063, acc 0.8125, prec 0.0782359, recall 0.898627
2017-12-10T11:45:45.475944: step 3739, loss 0.342159, acc 0.90625, prec 0.0782239, recall 0.898627
2017-12-10T11:45:45.925532: step 3740, loss 0.807682, acc 0.8125, prec 0.0782001, recall 0.898627
2017-12-10T11:45:46.377399: step 3741, loss 0.361416, acc 0.859375, prec 0.0781822, recall 0.898627
2017-12-10T11:45:46.823611: step 3742, loss 0.137184, acc 0.953125, prec 0.0781997, recall 0.898657
2017-12-10T11:45:47.254767: step 3743, loss 0.572403, acc 0.84375, prec 0.0781798, recall 0.898657
2017-12-10T11:45:47.692623: step 3744, loss 0.364479, acc 0.859375, prec 0.0781853, recall 0.898686
2017-12-10T11:45:48.124027: step 3745, loss 0.417364, acc 0.875, prec 0.0782865, recall 0.898834
2017-12-10T11:45:48.553116: step 3746, loss 0.396726, acc 0.875, prec 0.078294, recall 0.898863
2017-12-10T11:45:49.004787: step 3747, loss 0.252388, acc 0.921875, prec 0.0783074, recall 0.898893
2017-12-10T11:45:49.442573: step 3748, loss 0.524008, acc 0.859375, prec 0.0783363, recall 0.898952
2017-12-10T11:45:49.864262: step 3749, loss 0.493129, acc 0.875, prec 0.0783438, recall 0.898981
2017-12-10T11:45:50.313290: step 3750, loss 0.277078, acc 0.875, prec 0.0783279, recall 0.898981
2017-12-10T11:45:50.753703: step 3751, loss 0.438551, acc 0.859375, prec 0.0783334, recall 0.89901
2017-12-10T11:45:51.193441: step 3752, loss 0.101162, acc 0.953125, prec 0.0783508, recall 0.89904
2017-12-10T11:45:51.629673: step 3753, loss 0.189043, acc 0.96875, prec 0.0783469, recall 0.89904
2017-12-10T11:45:52.069675: step 3754, loss 0.082556, acc 0.984375, prec 0.0783449, recall 0.89904
2017-12-10T11:45:52.503228: step 3755, loss 0.0205467, acc 0.984375, prec 0.078413, recall 0.899128
2017-12-10T11:45:52.934650: step 3756, loss 0.0663632, acc 0.984375, prec 0.078411, recall 0.899128
2017-12-10T11:45:53.373112: step 3757, loss 0.20063, acc 0.96875, prec 0.078407, recall 0.899128
2017-12-10T11:45:53.823323: step 3758, loss 0.0913979, acc 0.96875, prec 0.0784264, recall 0.899157
2017-12-10T11:45:54.265626: step 3759, loss 0.0589909, acc 0.984375, prec 0.0784711, recall 0.899216
2017-12-10T11:45:54.708978: step 3760, loss 0.0819064, acc 0.96875, prec 0.0784672, recall 0.899216
2017-12-10T11:45:55.160770: step 3761, loss 0.33444, acc 0.984375, prec 0.0784652, recall 0.899216
2017-12-10T11:45:55.608327: step 3762, loss 0.0183463, acc 0.984375, prec 0.0785099, recall 0.899274
2017-12-10T11:45:56.049285: step 3763, loss 0.0132587, acc 0.984375, prec 0.0785079, recall 0.899274
2017-12-10T11:45:56.487046: step 3764, loss 0.190354, acc 0.953125, prec 0.0785253, recall 0.899304
2017-12-10T11:45:56.934785: step 3765, loss 0.0816497, acc 0.984375, prec 0.0785233, recall 0.899304
2017-12-10T11:45:57.383713: step 3766, loss 0.0120337, acc 1, prec 0.0785466, recall 0.899333
2017-12-10T11:45:57.820838: step 3767, loss 0.00805483, acc 1, prec 0.0785466, recall 0.899333
2017-12-10T11:45:58.251413: step 3768, loss 0.00474744, acc 1, prec 0.0785466, recall 0.899333
2017-12-10T11:45:58.692508: step 3769, loss 0.0291242, acc 1, prec 0.07857, recall 0.899362
2017-12-10T11:45:59.141489: step 3770, loss 0.0468405, acc 0.96875, prec 0.0785893, recall 0.899391
2017-12-10T11:45:59.581340: step 3771, loss 0.160938, acc 0.953125, prec 0.0785834, recall 0.899391
2017-12-10T11:46:00.014188: step 3772, loss 0.0525064, acc 0.96875, prec 0.0785794, recall 0.899391
2017-12-10T11:46:00.455638: step 3773, loss 0.283405, acc 0.984375, prec 0.0786007, recall 0.89942
2017-12-10T11:46:00.901976: step 3774, loss 0.00208807, acc 1, prec 0.0786007, recall 0.89942
2017-12-10T11:46:01.344039: step 3775, loss 0.00202329, acc 1, prec 0.0786007, recall 0.89942
2017-12-10T11:46:01.785355: step 3776, loss 0.00374423, acc 1, prec 0.0786007, recall 0.89942
2017-12-10T11:46:02.235090: step 3777, loss 0.0178346, acc 0.984375, prec 0.0785987, recall 0.89942
2017-12-10T11:46:02.691145: step 3778, loss 0.790128, acc 0.953125, prec 0.0786161, recall 0.899449
2017-12-10T11:46:03.132495: step 3779, loss 0.00979086, acc 1, prec 0.0786161, recall 0.899449
2017-12-10T11:46:03.573131: step 3780, loss 0.00276919, acc 1, prec 0.0786161, recall 0.899449
2017-12-10T11:46:04.018135: step 3781, loss 0.0334679, acc 0.984375, prec 0.0786141, recall 0.899449
2017-12-10T11:46:04.461827: step 3782, loss 0.0614992, acc 0.984375, prec 0.0786355, recall 0.899479
2017-12-10T11:46:04.910701: step 3783, loss 0.0635611, acc 0.96875, prec 0.0786315, recall 0.899479
2017-12-10T11:46:05.358363: step 3784, loss 5.48917, acc 0.984375, prec 0.0786781, recall 0.899276
2017-12-10T11:46:05.796221: step 3785, loss 0.0326467, acc 0.984375, prec 0.0786995, recall 0.899306
2017-12-10T11:46:06.243422: step 3786, loss 0.0743481, acc 0.96875, prec 0.0787188, recall 0.899335
2017-12-10T11:46:06.686280: step 3787, loss 0.234396, acc 0.953125, prec 0.0787362, recall 0.899364
2017-12-10T11:46:07.133997: step 3788, loss 0.104867, acc 0.984375, prec 0.0787342, recall 0.899364
2017-12-10T11:46:07.591906: step 3789, loss 1.58906, acc 0.953125, prec 0.0787302, recall 0.899104
2017-12-10T11:46:08.013824: step 3790, loss 0.0564356, acc 0.96875, prec 0.0787262, recall 0.899104
2017-12-10T11:46:08.463774: step 3791, loss 0.0873789, acc 0.9375, prec 0.0787182, recall 0.899104
2017-12-10T11:46:08.922441: step 3792, loss 0.0859671, acc 0.96875, prec 0.0787842, recall 0.899191
2017-12-10T11:46:09.369703: step 3793, loss 0.385034, acc 0.90625, prec 0.0787722, recall 0.899191
2017-12-10T11:46:09.816701: step 3794, loss 0.0693419, acc 0.96875, prec 0.0787916, recall 0.89922
2017-12-10T11:46:10.261298: step 3795, loss 0.520074, acc 0.890625, prec 0.0787776, recall 0.89922
2017-12-10T11:46:10.703233: step 3796, loss 0.390914, acc 0.921875, prec 0.0787909, recall 0.899249
2017-12-10T11:46:11.130632: step 3797, loss 0.207191, acc 0.921875, prec 0.078781, recall 0.899249
2017-12-10T11:46:11.563682: step 3798, loss 0.27566, acc 0.875, prec 0.0787883, recall 0.899279
2017-12-10T11:46:12.009697: step 3799, loss 0.166696, acc 0.90625, prec 0.078823, recall 0.899337
2017-12-10T11:46:12.450357: step 3800, loss 0.205147, acc 0.96875, prec 0.078819, recall 0.899337
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3800

2017-12-10T11:46:14.276249: step 3801, loss 0.166169, acc 0.9375, prec 0.0788343, recall 0.899366
2017-12-10T11:46:14.717014: step 3802, loss 0.598911, acc 0.859375, prec 0.0788396, recall 0.899395
2017-12-10T11:46:15.167165: step 3803, loss 0.301835, acc 0.9375, prec 0.0788549, recall 0.899424
2017-12-10T11:46:15.613499: step 3804, loss 0.383554, acc 0.859375, prec 0.0788603, recall 0.899453
2017-12-10T11:46:16.047642: step 3805, loss 0.277263, acc 0.953125, prec 0.0789241, recall 0.899539
2017-12-10T11:46:16.491112: step 3806, loss 0.220437, acc 0.953125, prec 0.0789414, recall 0.899568
2017-12-10T11:46:16.933554: step 3807, loss 0.373337, acc 0.90625, prec 0.0789992, recall 0.899655
2017-12-10T11:46:17.373815: step 3808, loss 0.243027, acc 0.921875, prec 0.0789892, recall 0.899655
2017-12-10T11:46:17.809117: step 3809, loss 0.172179, acc 0.921875, prec 0.0789793, recall 0.899655
2017-12-10T11:46:18.239671: step 3810, loss 0.344445, acc 0.890625, prec 0.0790118, recall 0.899713
2017-12-10T11:46:18.673186: step 3811, loss 0.0979549, acc 0.9375, prec 0.079027, recall 0.899741
2017-12-10T11:46:19.114500: step 3812, loss 0.403917, acc 0.9375, prec 0.0790191, recall 0.899741
2017-12-10T11:46:19.540974: step 3813, loss 0.499446, acc 0.921875, prec 0.0790323, recall 0.89977
2017-12-10T11:46:19.975606: step 3814, loss 0.194756, acc 0.953125, prec 0.0790264, recall 0.89977
2017-12-10T11:46:20.414398: step 3815, loss 0.0668886, acc 0.984375, prec 0.0790244, recall 0.89977
2017-12-10T11:46:20.837134: step 3816, loss 4.70904, acc 0.953125, prec 0.0790204, recall 0.899512
2017-12-10T11:46:21.273801: step 3817, loss 0.0255415, acc 0.984375, prec 0.0790416, recall 0.899541
2017-12-10T11:46:21.705901: step 3818, loss 0.164497, acc 0.96875, prec 0.0791073, recall 0.899627
2017-12-10T11:46:22.148680: step 3819, loss 0.608727, acc 0.890625, prec 0.0790933, recall 0.899627
2017-12-10T11:46:22.584056: step 3820, loss 0.347516, acc 0.921875, prec 0.0790834, recall 0.899627
2017-12-10T11:46:23.019559: step 3821, loss 0.310135, acc 0.921875, prec 0.0791198, recall 0.899685
2017-12-10T11:46:23.465049: step 3822, loss 0.120266, acc 0.96875, prec 0.0791158, recall 0.899685
2017-12-10T11:46:23.908879: step 3823, loss 0.141468, acc 0.921875, prec 0.0791291, recall 0.899713
2017-12-10T11:46:24.334223: step 3824, loss 0.34686, acc 0.90625, prec 0.0791403, recall 0.899742
2017-12-10T11:46:24.786575: step 3825, loss 0.225237, acc 0.9375, prec 0.0791787, recall 0.8998
2017-12-10T11:46:25.230523: step 3826, loss 0.197562, acc 0.953125, prec 0.0791959, recall 0.899828
2017-12-10T11:46:25.668697: step 3827, loss 0.182681, acc 0.96875, prec 0.0792383, recall 0.899886
2017-12-10T11:46:26.110496: step 3828, loss 0.306806, acc 0.90625, prec 0.0792727, recall 0.899943
2017-12-10T11:46:26.565332: step 3829, loss 0.0778107, acc 0.96875, prec 0.0792919, recall 0.899971
2017-12-10T11:46:27.008903: step 3830, loss 0.163707, acc 0.953125, prec 0.0793091, recall 0.9
2017-12-10T11:46:27.453362: step 3831, loss 0.422892, acc 0.859375, prec 0.0793375, recall 0.900057
2017-12-10T11:46:27.905144: step 3832, loss 0.223037, acc 0.921875, prec 0.0793275, recall 0.900057
2017-12-10T11:46:28.356349: step 3833, loss 0.251755, acc 0.9375, prec 0.0793427, recall 0.900086
2017-12-10T11:46:28.795223: step 3834, loss 0.037088, acc 0.984375, prec 0.0793639, recall 0.900114
2017-12-10T11:46:29.239426: step 3835, loss 0.151583, acc 0.96875, prec 0.0794062, recall 0.900171
2017-12-10T11:46:29.689706: step 3836, loss 0.442114, acc 0.90625, prec 0.0793942, recall 0.900171
2017-12-10T11:46:30.122216: step 3837, loss 0.227455, acc 0.96875, prec 0.0793902, recall 0.900171
2017-12-10T11:46:30.555354: step 3838, loss 0.0994036, acc 0.96875, prec 0.0794326, recall 0.900228
2017-12-10T11:46:30.993372: step 3839, loss 0.209436, acc 0.9375, prec 0.0794246, recall 0.900228
2017-12-10T11:46:31.436993: step 3840, loss 0.356832, acc 0.921875, prec 0.0794377, recall 0.900256
2017-12-10T11:46:31.877616: step 3841, loss 0.275401, acc 0.9375, prec 0.0794297, recall 0.900256
2017-12-10T11:46:32.308499: step 3842, loss 0.0828851, acc 0.96875, prec 0.0794257, recall 0.900256
2017-12-10T11:46:32.746608: step 3843, loss 0.354408, acc 0.96875, prec 0.0794449, recall 0.900285
2017-12-10T11:46:33.191518: step 3844, loss 0.0326061, acc 0.984375, prec 0.0794429, recall 0.900285
2017-12-10T11:46:33.632110: step 3845, loss 0.164836, acc 0.90625, prec 0.0794309, recall 0.900285
2017-12-10T11:46:34.065071: step 3846, loss 0.673005, acc 0.984375, prec 0.0794752, recall 0.900342
2017-12-10T11:46:34.509641: step 3847, loss 0.158528, acc 0.96875, prec 0.0795406, recall 0.900427
2017-12-10T11:46:34.946010: step 3848, loss 0.0193379, acc 0.984375, prec 0.0795386, recall 0.900427
2017-12-10T11:46:35.395974: step 3849, loss 0.0656022, acc 0.953125, prec 0.0795326, recall 0.900427
2017-12-10T11:46:35.837562: step 3850, loss 0.00650409, acc 1, prec 0.0795326, recall 0.900427
2017-12-10T11:46:36.274826: step 3851, loss 0.929362, acc 0.96875, prec 0.079598, recall 0.900512
2017-12-10T11:46:36.729717: step 3852, loss 0.0221977, acc 0.984375, prec 0.079596, recall 0.900512
2017-12-10T11:46:37.168871: step 3853, loss 0.060903, acc 1, prec 0.0796191, recall 0.90054
2017-12-10T11:46:37.627361: step 3854, loss 0.105762, acc 0.953125, prec 0.0796362, recall 0.900568
2017-12-10T11:46:38.081239: step 3855, loss 0.120188, acc 0.96875, prec 0.0796785, recall 0.900625
2017-12-10T11:46:38.511041: step 3856, loss 0.253868, acc 0.953125, prec 0.0796956, recall 0.900653
2017-12-10T11:46:38.959418: step 3857, loss 0.172793, acc 0.953125, prec 0.0797127, recall 0.900681
2017-12-10T11:46:39.405295: step 3858, loss 0.0665477, acc 0.984375, prec 0.0797569, recall 0.900737
2017-12-10T11:46:39.849626: step 3859, loss 0.169106, acc 0.96875, prec 0.079776, recall 0.900766
2017-12-10T11:46:40.314974: step 3860, loss 0.206575, acc 0.9375, prec 0.079768, recall 0.900766
2017-12-10T11:46:40.755584: step 3861, loss 0.283659, acc 0.9375, prec 0.07976, recall 0.900766
2017-12-10T11:46:41.204406: step 3862, loss 0.0308191, acc 0.984375, prec 0.079758, recall 0.900766
2017-12-10T11:46:41.645673: step 3863, loss 0.173583, acc 0.921875, prec 0.079748, recall 0.900766
2017-12-10T11:46:42.090430: step 3864, loss 0.0472146, acc 1, prec 0.0797711, recall 0.900794
2017-12-10T11:46:42.539800: step 3865, loss 4.68542, acc 0.90625, prec 0.0798073, recall 0.900595
2017-12-10T11:46:42.985755: step 3866, loss 0.0263636, acc 0.984375, prec 0.0798053, recall 0.900595
2017-12-10T11:46:43.424293: step 3867, loss 0.075543, acc 0.984375, prec 0.0798033, recall 0.900595
2017-12-10T11:46:43.861151: step 3868, loss 0.274143, acc 0.96875, prec 0.0797992, recall 0.900595
2017-12-10T11:46:44.300656: step 3869, loss 0.260557, acc 0.921875, prec 0.0798123, recall 0.900623
2017-12-10T11:46:44.732410: step 3870, loss 0.185042, acc 0.921875, prec 0.0798023, recall 0.900623
2017-12-10T11:46:45.185681: step 3871, loss 0.65637, acc 0.890625, prec 0.0798114, recall 0.900651
2017-12-10T11:46:45.618789: step 3872, loss 0.246878, acc 0.96875, prec 0.0798305, recall 0.900679
2017-12-10T11:46:46.055414: step 3873, loss 0.403236, acc 0.921875, prec 0.0798435, recall 0.900707
2017-12-10T11:46:46.502252: step 3874, loss 0.162962, acc 0.953125, prec 0.0798606, recall 0.900735
2017-12-10T11:46:46.950897: step 3875, loss 0.0737932, acc 0.953125, prec 0.0798777, recall 0.900763
2017-12-10T11:46:47.394362: step 3876, loss 0.161616, acc 0.9375, prec 0.0798927, recall 0.900791
2017-12-10T11:46:47.835548: step 3877, loss 0.463483, acc 0.9375, prec 0.0799078, recall 0.900819
2017-12-10T11:46:48.278093: step 3878, loss 0.580279, acc 0.890625, prec 0.0799168, recall 0.900847
2017-12-10T11:46:48.707541: step 3879, loss 0.0979903, acc 0.984375, prec 0.0799148, recall 0.900847
2017-12-10T11:46:49.140465: step 3880, loss 0.24044, acc 0.9375, prec 0.0799529, recall 0.900903
2017-12-10T11:46:49.579763: step 3881, loss 0.366981, acc 0.90625, prec 0.0799409, recall 0.900903
2017-12-10T11:46:50.020575: step 3882, loss 0.133211, acc 0.953125, prec 0.0799349, recall 0.900903
2017-12-10T11:46:50.459318: step 3883, loss 0.261846, acc 0.953125, prec 0.0799289, recall 0.900903
2017-12-10T11:46:50.900374: step 3884, loss 0.25961, acc 0.953125, prec 0.0799229, recall 0.900903
2017-12-10T11:46:51.345410: step 3885, loss 0.0524, acc 0.984375, prec 0.0799209, recall 0.900903
2017-12-10T11:46:51.792108: step 3886, loss 0.0799738, acc 0.96875, prec 0.0799168, recall 0.900903
2017-12-10T11:46:52.244339: step 3887, loss 0.177347, acc 0.96875, prec 0.0799359, recall 0.900931
2017-12-10T11:46:52.691741: step 3888, loss 0.193773, acc 0.96875, prec 0.0799549, recall 0.900959
2017-12-10T11:46:53.135869: step 3889, loss 0.0993631, acc 0.984375, prec 0.079976, recall 0.900987
2017-12-10T11:46:53.579206: step 3890, loss 0.103655, acc 0.953125, prec 0.07997, recall 0.900987
2017-12-10T11:46:54.024806: step 3891, loss 0.240542, acc 0.953125, prec 0.079987, recall 0.901015
2017-12-10T11:46:54.464463: step 3892, loss 0.194036, acc 0.96875, prec 0.080006, recall 0.901043
2017-12-10T11:46:54.903214: step 3893, loss 0.115281, acc 0.984375, prec 0.080027, recall 0.901071
2017-12-10T11:46:55.355333: step 3894, loss 0.0866911, acc 0.984375, prec 0.0800481, recall 0.901099
2017-12-10T11:46:55.802215: step 3895, loss 0.00797162, acc 1, prec 0.0800481, recall 0.901099
2017-12-10T11:46:56.256080: step 3896, loss 0.280818, acc 0.96875, prec 0.0800671, recall 0.901127
2017-12-10T11:46:56.699506: step 3897, loss 0.0207763, acc 0.984375, prec 0.0800651, recall 0.901127
2017-12-10T11:46:57.137406: step 3898, loss 0.0950931, acc 1, prec 0.0801111, recall 0.901182
2017-12-10T11:46:57.568178: step 3899, loss 0.0445004, acc 0.984375, prec 0.0801321, recall 0.90121
2017-12-10T11:46:58.008363: step 3900, loss 0.0737921, acc 0.984375, prec 0.0801301, recall 0.90121
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-3900

2017-12-10T11:47:00.099661: step 3901, loss 0.168304, acc 0.96875, prec 0.0801952, recall 0.901294
2017-12-10T11:47:00.565045: step 3902, loss 0.0179601, acc 1, prec 0.0802412, recall 0.901349
2017-12-10T11:47:01.019294: step 3903, loss 0.171536, acc 0.953125, prec 0.0802352, recall 0.901349
2017-12-10T11:47:01.463723: step 3904, loss 0.0818209, acc 0.96875, prec 0.0802312, recall 0.901349
2017-12-10T11:47:01.903722: step 3905, loss 0.0401641, acc 0.984375, prec 0.0802522, recall 0.901377
2017-12-10T11:47:02.352706: step 3906, loss 0.0111018, acc 1, prec 0.0802522, recall 0.901377
2017-12-10T11:47:02.787360: step 3907, loss 5.81441, acc 0.96875, prec 0.0802962, recall 0.901179
2017-12-10T11:47:03.234983: step 3908, loss 0.643995, acc 0.9375, prec 0.0803111, recall 0.901207
2017-12-10T11:47:03.664193: step 3909, loss 0.0831545, acc 0.953125, prec 0.0803051, recall 0.901207
2017-12-10T11:47:04.101707: step 3910, loss 0.0985703, acc 0.953125, prec 0.0803221, recall 0.901235
2017-12-10T11:47:04.540317: step 3911, loss 0.0694005, acc 0.96875, prec 0.0803411, recall 0.901262
2017-12-10T11:47:04.983986: step 3912, loss 0.120972, acc 0.921875, prec 0.080354, recall 0.90129
2017-12-10T11:47:05.422684: step 3913, loss 0.1692, acc 0.953125, prec 0.080371, recall 0.901318
2017-12-10T11:47:05.855789: step 3914, loss 0.349089, acc 0.890625, prec 0.0803799, recall 0.901345
2017-12-10T11:47:06.289542: step 3915, loss 0.181439, acc 0.921875, prec 0.0804158, recall 0.901401
2017-12-10T11:47:06.728351: step 3916, loss 0.344019, acc 0.890625, prec 0.0804018, recall 0.901401
2017-12-10T11:47:07.171733: step 3917, loss 0.352598, acc 0.875, prec 0.0804087, recall 0.901428
2017-12-10T11:47:07.610040: step 3918, loss 0.655895, acc 0.875, prec 0.0803926, recall 0.901428
2017-12-10T11:47:08.056661: step 3919, loss 0.121355, acc 0.953125, prec 0.0803866, recall 0.901428
2017-12-10T11:47:08.501392: step 3920, loss 0.379405, acc 0.921875, prec 0.0803995, recall 0.901456
2017-12-10T11:47:08.953661: step 3921, loss 0.375232, acc 0.875, prec 0.0803834, recall 0.901456
2017-12-10T11:47:09.393621: step 3922, loss 0.420071, acc 0.859375, prec 0.0803654, recall 0.901456
2017-12-10T11:47:09.830226: step 3923, loss 0.177238, acc 0.90625, prec 0.0803763, recall 0.901483
2017-12-10T11:47:10.282601: step 3924, loss 0.418294, acc 0.84375, prec 0.0803792, recall 0.901511
2017-12-10T11:47:10.724064: step 3925, loss 0.26714, acc 0.90625, prec 0.0803901, recall 0.901538
2017-12-10T11:47:11.182323: step 3926, loss 0.110497, acc 0.96875, prec 0.0804549, recall 0.901621
2017-12-10T11:47:11.630588: step 3927, loss 0.607755, acc 0.875, prec 0.0804618, recall 0.901649
2017-12-10T11:47:12.065609: step 3928, loss 0.287257, acc 0.9375, prec 0.0805454, recall 0.901758
2017-12-10T11:47:12.497905: step 3929, loss 0.396481, acc 0.953125, prec 0.0805623, recall 0.901786
2017-12-10T11:47:12.929978: step 3930, loss 0.0550995, acc 0.984375, prec 0.0805833, recall 0.901813
2017-12-10T11:47:13.372975: step 3931, loss 0.300221, acc 0.890625, prec 0.0805692, recall 0.901813
2017-12-10T11:47:13.813029: step 3932, loss 0.279078, acc 0.9375, prec 0.0805841, recall 0.901841
2017-12-10T11:47:14.245161: step 3933, loss 0.60187, acc 0.953125, prec 0.080601, recall 0.901868
2017-12-10T11:47:14.693236: step 3934, loss 2.19673, acc 0.9375, prec 0.0805949, recall 0.901617
2017-12-10T11:47:15.131446: step 3935, loss 0.389244, acc 0.90625, prec 0.0805829, recall 0.901617
2017-12-10T11:47:15.568259: step 3936, loss 0.113346, acc 0.96875, prec 0.0806018, recall 0.901644
2017-12-10T11:47:16.017432: step 3937, loss 0.630639, acc 0.921875, prec 0.0805917, recall 0.901644
2017-12-10T11:47:16.466244: step 3938, loss 0.646926, acc 0.9375, prec 0.0806295, recall 0.901699
2017-12-10T11:47:16.910495: step 3939, loss 0.287055, acc 0.9375, prec 0.0806215, recall 0.901699
2017-12-10T11:47:17.346517: step 3940, loss 0.181496, acc 0.921875, prec 0.0806343, recall 0.901726
2017-12-10T11:47:17.789305: step 3941, loss 0.326147, acc 0.953125, prec 0.0806741, recall 0.901781
2017-12-10T11:47:18.217003: step 3942, loss 0.176018, acc 0.953125, prec 0.080668, recall 0.901781
2017-12-10T11:47:18.652334: step 3943, loss 0.455511, acc 0.9375, prec 0.0807058, recall 0.901835
2017-12-10T11:47:19.088336: step 3944, loss 0.306594, acc 0.90625, prec 0.0807166, recall 0.901863
2017-12-10T11:47:19.529867: step 3945, loss 0.326172, acc 0.9375, prec 0.0807314, recall 0.90189
2017-12-10T11:47:19.970494: step 3946, loss 0.410926, acc 0.859375, prec 0.0807591, recall 0.901944
2017-12-10T11:47:20.409772: step 3947, loss 0.210088, acc 0.953125, prec 0.0807759, recall 0.901972
2017-12-10T11:47:20.848948: step 3948, loss 1.269, acc 0.921875, prec 0.0807908, recall 0.901749
2017-12-10T11:47:21.287701: step 3949, loss 0.249893, acc 0.9375, prec 0.0807827, recall 0.901749
2017-12-10T11:47:21.751808: step 3950, loss 0.320349, acc 0.921875, prec 0.0807727, recall 0.901749
2017-12-10T11:47:22.194835: step 3951, loss 0.419766, acc 0.90625, prec 0.0808063, recall 0.901803
2017-12-10T11:47:22.644376: step 3952, loss 0.374614, acc 0.953125, prec 0.0808688, recall 0.901885
2017-12-10T11:47:23.096830: step 3953, loss 0.167874, acc 0.96875, prec 0.0809105, recall 0.901939
2017-12-10T11:47:23.537177: step 3954, loss 0.191593, acc 0.921875, prec 0.0809233, recall 0.901966
2017-12-10T11:47:23.965137: step 3955, loss 0.861749, acc 0.875, prec 0.08093, recall 0.901993
2017-12-10T11:47:24.409470: step 3956, loss 0.234805, acc 0.9375, prec 0.080922, recall 0.901993
2017-12-10T11:47:24.861456: step 3957, loss 0.442955, acc 0.875, prec 0.0809972, recall 0.902102
2017-12-10T11:47:25.307351: step 3958, loss 0.120054, acc 0.96875, prec 0.0809932, recall 0.902102
2017-12-10T11:47:25.757162: step 3959, loss 0.218988, acc 0.921875, prec 0.0810059, recall 0.902129
2017-12-10T11:47:26.206592: step 3960, loss 1.79249, acc 0.921875, prec 0.0810435, recall 0.901934
2017-12-10T11:47:26.652073: step 3961, loss 0.0819742, acc 0.96875, prec 0.0811079, recall 0.902015
2017-12-10T11:47:27.084698: step 3962, loss 0.268336, acc 0.90625, prec 0.0810958, recall 0.902015
2017-12-10T11:47:27.523847: step 3963, loss 0.128886, acc 0.96875, prec 0.0811146, recall 0.902042
2017-12-10T11:47:27.967986: step 3964, loss 0.0798866, acc 0.96875, prec 0.0811334, recall 0.902069
2017-12-10T11:47:28.419029: step 3965, loss 0.515251, acc 0.90625, prec 0.0811213, recall 0.902069
2017-12-10T11:47:28.857905: step 3966, loss 0.307426, acc 0.921875, prec 0.081134, recall 0.902096
2017-12-10T11:47:29.296478: step 3967, loss 0.264421, acc 0.90625, prec 0.0811448, recall 0.902123
2017-12-10T11:47:29.741867: step 3968, loss 0.297982, acc 0.921875, prec 0.0811347, recall 0.902123
2017-12-10T11:47:30.173078: step 3969, loss 0.185536, acc 0.890625, prec 0.0811434, recall 0.90215
2017-12-10T11:47:30.618793: step 3970, loss 0.189133, acc 0.953125, prec 0.0811601, recall 0.902177
2017-12-10T11:47:31.065378: step 3971, loss 0.344934, acc 0.921875, prec 0.0811501, recall 0.902177
2017-12-10T11:47:31.525802: step 3972, loss 0.199673, acc 0.921875, prec 0.08114, recall 0.902177
2017-12-10T11:47:31.974324: step 3973, loss 0.298864, acc 0.890625, prec 0.081126, recall 0.902177
2017-12-10T11:47:32.422918: step 3974, loss 0.239518, acc 0.9375, prec 0.0811407, recall 0.902204
2017-12-10T11:47:32.866993: step 3975, loss 0.386067, acc 0.90625, prec 0.0811286, recall 0.902204
2017-12-10T11:47:33.265075: step 3976, loss 0.424239, acc 0.960784, prec 0.0811701, recall 0.902258
2017-12-10T11:47:33.726354: step 3977, loss 0.206881, acc 0.96875, prec 0.0811661, recall 0.902258
2017-12-10T11:47:34.162310: step 3978, loss 0.189224, acc 0.96875, prec 0.0811621, recall 0.902258
2017-12-10T11:47:34.600814: step 3979, loss 0.273084, acc 0.9375, prec 0.081154, recall 0.902258
2017-12-10T11:47:35.042434: step 3980, loss 0.363133, acc 0.96875, prec 0.08115, recall 0.902258
2017-12-10T11:47:35.485882: step 3981, loss 0.0394323, acc 0.984375, prec 0.0811708, recall 0.902285
2017-12-10T11:47:35.926900: step 3982, loss 0.16616, acc 0.953125, prec 0.0811875, recall 0.902312
2017-12-10T11:47:36.371716: step 3983, loss 0.0941767, acc 0.953125, prec 0.0811815, recall 0.902312
2017-12-10T11:47:36.822069: step 3984, loss 0.113438, acc 0.953125, prec 0.0811982, recall 0.902338
2017-12-10T11:47:37.265348: step 3985, loss 0.0943357, acc 0.96875, prec 0.0812169, recall 0.902365
2017-12-10T11:47:37.715570: step 3986, loss 0.241238, acc 0.9375, prec 0.0812088, recall 0.902365
2017-12-10T11:47:38.155674: step 3987, loss 0.0194372, acc 1, prec 0.0812316, recall 0.902392
2017-12-10T11:47:38.608244: step 3988, loss 0.152499, acc 0.96875, prec 0.0812503, recall 0.902419
2017-12-10T11:47:39.051720: step 3989, loss 0.0609915, acc 0.96875, prec 0.081269, recall 0.902446
2017-12-10T11:47:39.488249: step 3990, loss 2.36902, acc 0.953125, prec 0.081265, recall 0.902198
2017-12-10T11:47:39.922904: step 3991, loss 0.0218584, acc 1, prec 0.081265, recall 0.902198
2017-12-10T11:47:40.355572: step 3992, loss 0.12828, acc 0.96875, prec 0.0812837, recall 0.902225
2017-12-10T11:47:40.799943: step 3993, loss 0.458733, acc 0.96875, prec 0.0813024, recall 0.902251
2017-12-10T11:47:41.254188: step 3994, loss 0.0451196, acc 0.96875, prec 0.0812984, recall 0.902251
2017-12-10T11:47:41.697162: step 3995, loss 0.0550201, acc 0.96875, prec 0.0812944, recall 0.902251
2017-12-10T11:47:42.143952: step 3996, loss 0.0289156, acc 1, prec 0.0812944, recall 0.902251
2017-12-10T11:47:42.578065: step 3997, loss 0.0109663, acc 1, prec 0.0813398, recall 0.902305
2017-12-10T11:47:43.020252: step 3998, loss 0.244415, acc 0.96875, prec 0.0813813, recall 0.902359
2017-12-10T11:47:43.463136: step 3999, loss 0.0653777, acc 0.96875, prec 0.0813772, recall 0.902359
2017-12-10T11:47:43.912491: step 4000, loss 0.113954, acc 0.96875, prec 0.0813959, recall 0.902386

Evaluation:
2017-12-10T11:47:53.405423: step 4000, loss 4.61207, acc 0.953958, prec 0.0818364, recall 0.87746

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4000

2017-12-10T11:47:55.340959: step 4001, loss 0.110813, acc 0.953125, prec 0.0818529, recall 0.877492
2017-12-10T11:47:55.779807: step 4002, loss 0.0480171, acc 0.96875, prec 0.0818489, recall 0.877492
2017-12-10T11:47:56.224857: step 4003, loss 0.225965, acc 0.953125, prec 0.0818653, recall 0.877524
2017-12-10T11:47:56.661908: step 4004, loss 0.616904, acc 0.984375, prec 0.0819083, recall 0.877588
2017-12-10T11:47:57.117790: step 4005, loss 0.207552, acc 0.921875, prec 0.0818982, recall 0.877588
2017-12-10T11:47:57.565238: step 4006, loss 0.0313824, acc 1, prec 0.0819207, recall 0.877621
2017-12-10T11:47:58.006118: step 4007, loss 0.094841, acc 0.96875, prec 0.0819167, recall 0.877621
2017-12-10T11:47:58.444629: step 4008, loss 0.130473, acc 0.984375, prec 0.0819147, recall 0.877621
2017-12-10T11:47:58.878568: step 4009, loss 0.127382, acc 0.953125, prec 0.0819087, recall 0.877621
2017-12-10T11:47:59.316693: step 4010, loss 0.470526, acc 0.921875, prec 0.0819211, recall 0.877653
2017-12-10T11:47:59.759817: step 4011, loss 0.160334, acc 0.96875, prec 0.0819171, recall 0.877653
2017-12-10T11:48:00.215650: step 4012, loss 0.202772, acc 0.953125, prec 0.0819335, recall 0.877685
2017-12-10T11:48:00.663205: step 4013, loss 0.695532, acc 0.96875, prec 0.0819744, recall 0.877749
2017-12-10T11:48:01.104034: step 4014, loss 0.18651, acc 0.96875, prec 0.0819929, recall 0.877781
2017-12-10T11:48:01.554363: step 4015, loss 0.181566, acc 0.9375, prec 0.0819848, recall 0.877781
2017-12-10T11:48:01.995868: step 4016, loss 0.172062, acc 0.953125, prec 0.0819788, recall 0.877781
2017-12-10T11:48:02.452941: step 4017, loss 0.22178, acc 0.96875, prec 0.0820197, recall 0.877845
2017-12-10T11:48:02.883164: step 4018, loss 0.426454, acc 0.984375, prec 0.0820401, recall 0.877877
2017-12-10T11:48:03.332997: step 4019, loss 0.977648, acc 0.953125, prec 0.0820565, recall 0.877908
2017-12-10T11:48:03.777848: step 4020, loss 0.400645, acc 0.890625, prec 0.0820425, recall 0.877908
2017-12-10T11:48:04.211072: step 4021, loss 0.152879, acc 0.9375, prec 0.0820345, recall 0.877908
2017-12-10T11:48:04.651330: step 4022, loss 0.234991, acc 0.921875, prec 0.0820469, recall 0.87794
2017-12-10T11:48:05.091286: step 4023, loss 0.130386, acc 0.984375, prec 0.0820673, recall 0.877972
2017-12-10T11:48:05.540755: step 4024, loss 0.146828, acc 0.953125, prec 0.0821061, recall 0.878036
2017-12-10T11:48:05.987895: step 4025, loss 0.714752, acc 0.875, prec 0.0820901, recall 0.878036
2017-12-10T11:48:06.432123: step 4026, loss 0.158544, acc 0.9375, prec 0.0820821, recall 0.878036
2017-12-10T11:48:06.873425: step 4027, loss 0.207133, acc 0.953125, prec 0.0820761, recall 0.878036
2017-12-10T11:48:07.316392: step 4028, loss 0.179391, acc 0.96875, prec 0.0821169, recall 0.8781
2017-12-10T11:48:07.758015: step 4029, loss 0.131255, acc 0.9375, prec 0.0821089, recall 0.8781
2017-12-10T11:48:08.197097: step 4030, loss 0.270511, acc 0.984375, prec 0.0821069, recall 0.8781
2017-12-10T11:48:08.645426: step 4031, loss 0.0867444, acc 0.96875, prec 0.0821252, recall 0.878132
2017-12-10T11:48:09.090649: step 4032, loss 0.0565498, acc 0.984375, prec 0.0821456, recall 0.878163
2017-12-10T11:48:09.535689: step 4033, loss 0.0350446, acc 1, prec 0.0821904, recall 0.878227
2017-12-10T11:48:09.974941: step 4034, loss 0.190758, acc 0.984375, prec 0.0822332, recall 0.87829
2017-12-10T11:48:10.414521: step 4035, loss 0.139834, acc 0.953125, prec 0.0822496, recall 0.878322
2017-12-10T11:48:10.873545: step 4036, loss 0.259312, acc 0.9375, prec 0.0822416, recall 0.878322
2017-12-10T11:48:11.309377: step 4037, loss 0.0959944, acc 0.96875, prec 0.0822376, recall 0.878322
2017-12-10T11:48:11.753521: step 4038, loss 0.0389912, acc 0.984375, prec 0.0822356, recall 0.878322
2017-12-10T11:48:12.198952: step 4039, loss 0.0197644, acc 0.984375, prec 0.0822336, recall 0.878322
2017-12-10T11:48:12.632857: step 4040, loss 0.0758776, acc 0.984375, prec 0.0822763, recall 0.878385
2017-12-10T11:48:13.079003: step 4041, loss 0.109645, acc 0.953125, prec 0.0822703, recall 0.878385
2017-12-10T11:48:13.522478: step 4042, loss 0.112446, acc 0.96875, prec 0.0822887, recall 0.878417
2017-12-10T11:48:13.966846: step 4043, loss 0.157531, acc 0.96875, prec 0.0823294, recall 0.87848
2017-12-10T11:48:14.413192: step 4044, loss 0.304013, acc 0.953125, prec 0.0823234, recall 0.87848
2017-12-10T11:48:14.863550: step 4045, loss 0.105318, acc 0.9375, prec 0.0823154, recall 0.87848
2017-12-10T11:48:15.305984: step 4046, loss 0.0683912, acc 0.984375, prec 0.0823134, recall 0.87848
2017-12-10T11:48:15.757790: step 4047, loss 0.269163, acc 0.953125, prec 0.0823521, recall 0.878544
2017-12-10T11:48:16.197294: step 4048, loss 0.0392898, acc 0.984375, prec 0.0823501, recall 0.878544
2017-12-10T11:48:16.649999: step 4049, loss 0.181291, acc 0.9375, prec 0.0823644, recall 0.878575
2017-12-10T11:48:17.091847: step 4050, loss 0.0558048, acc 0.96875, prec 0.0823604, recall 0.878575
2017-12-10T11:48:17.543087: step 4051, loss 0.00972081, acc 1, prec 0.0823604, recall 0.878575
2017-12-10T11:48:17.978188: step 4052, loss 0.0606549, acc 0.984375, prec 0.0824255, recall 0.87867
2017-12-10T11:48:18.432235: step 4053, loss 0.354852, acc 0.953125, prec 0.0824418, recall 0.878701
2017-12-10T11:48:18.878382: step 4054, loss 0.0926296, acc 0.96875, prec 0.0824378, recall 0.878701
2017-12-10T11:48:19.326079: step 4055, loss 0.0584656, acc 0.953125, prec 0.0824318, recall 0.878701
2017-12-10T11:48:19.759682: step 4056, loss 0.0495467, acc 0.984375, prec 0.0824298, recall 0.878701
2017-12-10T11:48:20.203379: step 4057, loss 0.00443349, acc 1, prec 0.0824298, recall 0.878701
2017-12-10T11:48:20.652868: step 4058, loss 0.290568, acc 1, prec 0.0824968, recall 0.878796
2017-12-10T11:48:21.108792: step 4059, loss 0.522625, acc 0.984375, prec 0.0825395, recall 0.878859
2017-12-10T11:48:21.555951: step 4060, loss 0.738221, acc 0.984375, prec 0.0825599, recall 0.87889
2017-12-10T11:48:22.006797: step 4061, loss 0.27425, acc 0.96875, prec 0.0826229, recall 0.878984
2017-12-10T11:48:22.443976: step 4062, loss 0.326978, acc 0.90625, prec 0.0826555, recall 0.879047
2017-12-10T11:48:22.879386: step 4063, loss 0.155247, acc 0.953125, prec 0.0826718, recall 0.879078
2017-12-10T11:48:23.332010: step 4064, loss 0.247053, acc 0.9375, prec 0.0827307, recall 0.879172
2017-12-10T11:48:23.781036: step 4065, loss 0.0262598, acc 1, prec 0.0827531, recall 0.879203
2017-12-10T11:48:24.222225: step 4066, loss 0.359605, acc 0.9375, prec 0.082745, recall 0.879203
2017-12-10T11:48:24.661685: step 4067, loss 0.128151, acc 0.96875, prec 0.0827857, recall 0.879266
2017-12-10T11:48:25.106387: step 4068, loss 0.190266, acc 0.921875, prec 0.0827756, recall 0.879266
2017-12-10T11:48:25.542290: step 4069, loss 0.279198, acc 0.9375, prec 0.0827675, recall 0.879266
2017-12-10T11:48:25.987839: step 4070, loss 0.165021, acc 0.90625, prec 0.0828001, recall 0.879328
2017-12-10T11:48:26.432535: step 4071, loss 0.17262, acc 0.96875, prec 0.0828184, recall 0.879359
2017-12-10T11:48:26.887544: step 4072, loss 0.0617619, acc 0.984375, prec 0.0828387, recall 0.87939
2017-12-10T11:48:27.339108: step 4073, loss 0.288716, acc 0.9375, prec 0.0828306, recall 0.87939
2017-12-10T11:48:27.772448: step 4074, loss 0.200357, acc 0.9375, prec 0.0828672, recall 0.879453
2017-12-10T11:48:28.218301: step 4075, loss 0.123929, acc 0.96875, prec 0.0828631, recall 0.879453
2017-12-10T11:48:28.656825: step 4076, loss 0.0406595, acc 0.984375, prec 0.0828834, recall 0.879484
2017-12-10T11:48:29.103601: step 4077, loss 0.0468308, acc 0.984375, prec 0.082926, recall 0.879546
2017-12-10T11:48:29.544548: step 4078, loss 0.267693, acc 0.921875, prec 0.0829605, recall 0.879608
2017-12-10T11:48:29.991453: step 4079, loss 0.206701, acc 0.953125, prec 0.0829545, recall 0.879608
2017-12-10T11:48:30.433734: step 4080, loss 0.287307, acc 0.953125, prec 0.0829707, recall 0.879639
2017-12-10T11:48:30.881869: step 4081, loss 0.0967524, acc 0.984375, prec 0.0829687, recall 0.879639
2017-12-10T11:48:31.329701: step 4082, loss 0.264085, acc 0.9375, prec 0.0829606, recall 0.879639
2017-12-10T11:48:31.773669: step 4083, loss 0.176204, acc 0.953125, prec 0.0829991, recall 0.879701
2017-12-10T11:48:32.228172: step 4084, loss 0.105143, acc 0.953125, prec 0.0829931, recall 0.879701
2017-12-10T11:48:32.662088: step 4085, loss 0.107005, acc 0.984375, prec 0.0830356, recall 0.879763
2017-12-10T11:48:33.094992: step 4086, loss 0.110854, acc 0.96875, prec 0.0830539, recall 0.879794
2017-12-10T11:48:33.540939: step 4087, loss 0.145058, acc 0.953125, prec 0.0830478, recall 0.879794
2017-12-10T11:48:34.004529: step 4088, loss 0.132091, acc 0.96875, prec 0.0830884, recall 0.879856
2017-12-10T11:48:34.443940: step 4089, loss 0.352978, acc 0.953125, prec 0.0831269, recall 0.879918
2017-12-10T11:48:34.890403: step 4090, loss 0.112594, acc 0.953125, prec 0.0831208, recall 0.879918
2017-12-10T11:48:35.334604: step 4091, loss 0.462891, acc 0.953125, prec 0.0831593, recall 0.879979
2017-12-10T11:48:35.777375: step 4092, loss 0.0721628, acc 0.96875, prec 0.0831998, recall 0.880041
2017-12-10T11:48:36.217322: step 4093, loss 0.0102602, acc 1, prec 0.0831998, recall 0.880041
2017-12-10T11:48:36.671770: step 4094, loss 0.0769108, acc 0.953125, prec 0.0831937, recall 0.880041
2017-12-10T11:48:37.111707: step 4095, loss 0.457226, acc 0.984375, prec 0.0832139, recall 0.880072
2017-12-10T11:48:37.552906: step 4096, loss 0.0781373, acc 0.984375, prec 0.0832119, recall 0.880072
2017-12-10T11:48:37.991256: step 4097, loss 0.00383236, acc 1, prec 0.0832119, recall 0.880072
2017-12-10T11:48:38.437442: step 4098, loss 0.140843, acc 0.984375, prec 0.0832322, recall 0.880103
2017-12-10T11:48:38.872412: step 4099, loss 0.0521676, acc 0.96875, prec 0.0832281, recall 0.880103
2017-12-10T11:48:39.317866: step 4100, loss 0.0407009, acc 0.96875, prec 0.0832241, recall 0.880103
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4100

2017-12-10T11:48:41.261739: step 4101, loss 0.0216815, acc 0.984375, prec 0.0832443, recall 0.880133
2017-12-10T11:48:41.706106: step 4102, loss 0.0736055, acc 0.984375, prec 0.0832646, recall 0.880164
2017-12-10T11:48:42.152732: step 4103, loss 0.123145, acc 1, prec 0.0833091, recall 0.880226
2017-12-10T11:48:42.613738: step 4104, loss 0.0351215, acc 0.96875, prec 0.083305, recall 0.880226
2017-12-10T11:48:43.060640: step 4105, loss 0.0177251, acc 1, prec 0.0833273, recall 0.880256
2017-12-10T11:48:43.494696: step 4106, loss 0.11261, acc 0.96875, prec 0.0833677, recall 0.880318
2017-12-10T11:48:43.944832: step 4107, loss 0.0837826, acc 0.96875, prec 0.0833637, recall 0.880318
2017-12-10T11:48:44.389269: step 4108, loss 0.0917044, acc 0.96875, prec 0.0834264, recall 0.88041
2017-12-10T11:48:44.831729: step 4109, loss 0.128498, acc 0.96875, prec 0.0834445, recall 0.88044
2017-12-10T11:48:45.273123: step 4110, loss 0.0617252, acc 0.96875, prec 0.0834627, recall 0.880471
2017-12-10T11:48:45.722434: step 4111, loss 0.0608361, acc 0.96875, prec 0.0834587, recall 0.880471
2017-12-10T11:48:46.174808: step 4112, loss 0.0422348, acc 1, prec 0.0834809, recall 0.880502
2017-12-10T11:48:46.618572: step 4113, loss 0.0142201, acc 1, prec 0.0835032, recall 0.880532
2017-12-10T11:48:47.050492: step 4114, loss 2.90047, acc 0.984375, prec 0.0835699, recall 0.880399
2017-12-10T11:48:47.513917: step 4115, loss 0.0365583, acc 1, prec 0.0836143, recall 0.88046
2017-12-10T11:48:47.955104: step 4116, loss 0.0652026, acc 0.984375, prec 0.0836123, recall 0.88046
2017-12-10T11:48:48.401840: step 4117, loss 0.124648, acc 0.953125, prec 0.0836062, recall 0.88046
2017-12-10T11:48:48.849156: step 4118, loss 0.071391, acc 0.96875, prec 0.0836244, recall 0.88049
2017-12-10T11:48:49.288941: step 4119, loss 0.165428, acc 1, prec 0.0836466, recall 0.880521
2017-12-10T11:48:49.736823: step 4120, loss 0.065114, acc 0.96875, prec 0.083687, recall 0.880582
2017-12-10T11:48:50.167735: step 4121, loss 0.14179, acc 0.9375, prec 0.0836789, recall 0.880582
2017-12-10T11:48:50.616442: step 4122, loss 0.175694, acc 0.96875, prec 0.0837192, recall 0.880643
2017-12-10T11:48:51.056986: step 4123, loss 0.258392, acc 0.90625, prec 0.0837293, recall 0.880673
2017-12-10T11:48:51.499110: step 4124, loss 0.289193, acc 0.890625, prec 0.0837595, recall 0.880734
2017-12-10T11:48:51.945367: step 4125, loss 0.233605, acc 0.921875, prec 0.0837715, recall 0.880764
2017-12-10T11:48:52.386333: step 4126, loss 0.109545, acc 0.96875, prec 0.0838119, recall 0.880825
2017-12-10T11:48:52.822864: step 4127, loss 0.312921, acc 0.890625, prec 0.0837977, recall 0.880825
2017-12-10T11:48:53.273582: step 4128, loss 0.123129, acc 0.953125, prec 0.0838138, recall 0.880855
2017-12-10T11:48:53.714388: step 4129, loss 0.334757, acc 0.953125, prec 0.0838521, recall 0.880916
2017-12-10T11:48:54.160431: step 4130, loss 0.163243, acc 0.953125, prec 0.0838903, recall 0.880977
2017-12-10T11:48:54.610859: step 4131, loss 0.16763, acc 0.9375, prec 0.0838822, recall 0.880977
2017-12-10T11:48:55.059900: step 4132, loss 0.19887, acc 0.9375, prec 0.0838963, recall 0.881007
2017-12-10T11:48:55.496378: step 4133, loss 0.224847, acc 0.953125, prec 0.0839124, recall 0.881037
2017-12-10T11:48:55.930741: step 4134, loss 0.562706, acc 0.84375, prec 0.0839142, recall 0.881067
2017-12-10T11:48:56.372878: step 4135, loss 0.374568, acc 0.921875, prec 0.0839484, recall 0.881128
2017-12-10T11:48:56.823638: step 4136, loss 0.12042, acc 0.9375, prec 0.0839403, recall 0.881128
2017-12-10T11:48:57.270216: step 4137, loss 0.111385, acc 0.953125, prec 0.0839342, recall 0.881128
2017-12-10T11:48:57.706289: step 4138, loss 0.225521, acc 0.953125, prec 0.0839281, recall 0.881128
2017-12-10T11:48:58.141547: step 4139, loss 0.0450086, acc 0.984375, prec 0.0839261, recall 0.881128
2017-12-10T11:48:58.579234: step 4140, loss 0.0591166, acc 0.96875, prec 0.083922, recall 0.881128
2017-12-10T11:48:59.019713: step 4141, loss 0.072665, acc 0.96875, prec 0.0839623, recall 0.881188
2017-12-10T11:48:59.460523: step 4142, loss 0.106033, acc 0.96875, prec 0.0839582, recall 0.881188
2017-12-10T11:48:59.904504: step 4143, loss 0.210183, acc 0.90625, prec 0.083946, recall 0.881188
2017-12-10T11:49:00.336637: step 4144, loss 0.0419144, acc 0.984375, prec 0.0839883, recall 0.881248
2017-12-10T11:49:00.787461: step 4145, loss 0.0728425, acc 0.96875, prec 0.0840285, recall 0.881309
2017-12-10T11:49:01.223879: step 4146, loss 0.445719, acc 0.921875, prec 0.0840184, recall 0.881309
2017-12-10T11:49:01.664421: step 4147, loss 0.0329164, acc 0.984375, prec 0.0840163, recall 0.881309
2017-12-10T11:49:02.114542: step 4148, loss 0.176741, acc 0.96875, prec 0.0840123, recall 0.881309
2017-12-10T11:49:02.555278: step 4149, loss 0.071998, acc 0.96875, prec 0.0840082, recall 0.881309
2017-12-10T11:49:03.020302: step 4150, loss 0.037324, acc 0.984375, prec 0.0840062, recall 0.881309
2017-12-10T11:49:03.463050: step 4151, loss 0.200357, acc 0.96875, prec 0.0840243, recall 0.881339
2017-12-10T11:49:03.911381: step 4152, loss 1.07459, acc 1, prec 0.0840464, recall 0.881369
2017-12-10T11:49:04.360108: step 4153, loss 0.0174854, acc 1, prec 0.0840464, recall 0.881369
2017-12-10T11:49:04.819217: step 4154, loss 0.0541087, acc 0.96875, prec 0.0840423, recall 0.881369
2017-12-10T11:49:05.268053: step 4155, loss 0.166698, acc 0.96875, prec 0.0840604, recall 0.881399
2017-12-10T11:49:05.710383: step 4156, loss 0.0758683, acc 0.96875, prec 0.0840564, recall 0.881399
2017-12-10T11:49:06.151394: step 4157, loss 0.0679178, acc 0.96875, prec 0.0840523, recall 0.881399
2017-12-10T11:49:06.601467: step 4158, loss 0.0606796, acc 0.953125, prec 0.0840462, recall 0.881399
2017-12-10T11:49:07.040393: step 4159, loss 0.387398, acc 0.96875, prec 0.0840421, recall 0.881399
2017-12-10T11:49:07.477055: step 4160, loss 0.0333395, acc 0.984375, prec 0.0840844, recall 0.881459
2017-12-10T11:49:07.918372: step 4161, loss 0.274723, acc 0.890625, prec 0.0840923, recall 0.881489
2017-12-10T11:49:08.357775: step 4162, loss 0.172641, acc 0.96875, prec 0.0841103, recall 0.881519
2017-12-10T11:49:08.798569: step 4163, loss 0.115641, acc 0.953125, prec 0.0841042, recall 0.881519
2017-12-10T11:49:09.247374: step 4164, loss 0.0455414, acc 0.96875, prec 0.0841223, recall 0.881549
2017-12-10T11:49:09.682638: step 4165, loss 0.117443, acc 1, prec 0.0841665, recall 0.881609
2017-12-10T11:49:10.138339: step 4166, loss 0.00720852, acc 1, prec 0.0841665, recall 0.881609
2017-12-10T11:49:10.568494: step 4167, loss 0.180155, acc 0.96875, prec 0.0841625, recall 0.881609
2017-12-10T11:49:11.007546: step 4168, loss 0.0579231, acc 0.984375, prec 0.0841604, recall 0.881609
2017-12-10T11:49:11.461047: step 4169, loss 0.0370356, acc 0.984375, prec 0.0841584, recall 0.881609
2017-12-10T11:49:11.907400: step 4170, loss 0.0560854, acc 0.953125, prec 0.0841523, recall 0.881609
2017-12-10T11:49:12.348727: step 4171, loss 0.120165, acc 0.96875, prec 0.0841704, recall 0.881639
2017-12-10T11:49:12.791238: step 4172, loss 0.139605, acc 0.96875, prec 0.0841884, recall 0.881669
2017-12-10T11:49:13.245013: step 4173, loss 0.147569, acc 0.96875, prec 0.0842507, recall 0.881758
2017-12-10T11:49:13.684854: step 4174, loss 0.16085, acc 0.984375, prec 0.0842486, recall 0.881758
2017-12-10T11:49:14.127874: step 4175, loss 2.28017, acc 0.953125, prec 0.0842667, recall 0.881566
2017-12-10T11:49:14.576175: step 4176, loss 0.0733009, acc 0.984375, prec 0.0842867, recall 0.881596
2017-12-10T11:49:15.029241: step 4177, loss 0.297332, acc 0.953125, prec 0.0842806, recall 0.881596
2017-12-10T11:49:15.467648: step 4178, loss 0.132727, acc 0.984375, prec 0.0842786, recall 0.881596
2017-12-10T11:49:15.906997: step 4179, loss 0.115618, acc 0.953125, prec 0.0842725, recall 0.881596
2017-12-10T11:49:16.356498: step 4180, loss 0.0505338, acc 0.984375, prec 0.0842926, recall 0.881625
2017-12-10T11:49:16.814203: step 4181, loss 0.203368, acc 0.921875, prec 0.0842824, recall 0.881625
2017-12-10T11:49:17.257922: step 4182, loss 0.258836, acc 0.9375, prec 0.0842964, recall 0.881655
2017-12-10T11:49:17.702857: step 4183, loss 0.0891626, acc 0.953125, prec 0.0843124, recall 0.881685
2017-12-10T11:49:18.150720: step 4184, loss 0.385227, acc 0.875, prec 0.0842961, recall 0.881685
2017-12-10T11:49:18.600920: step 4185, loss 0.123572, acc 0.953125, prec 0.0843341, recall 0.881745
2017-12-10T11:49:19.035267: step 4186, loss 0.0178191, acc 1, prec 0.0843783, recall 0.881804
2017-12-10T11:49:19.474554: step 4187, loss 0.212596, acc 0.921875, prec 0.0843902, recall 0.881834
2017-12-10T11:49:19.927132: step 4188, loss 0.097726, acc 0.984375, prec 0.0844103, recall 0.881864
2017-12-10T11:49:20.373609: step 4189, loss 0.295816, acc 0.90625, prec 0.0843981, recall 0.881864
2017-12-10T11:49:20.814081: step 4190, loss 0.254059, acc 0.921875, prec 0.0843879, recall 0.881864
2017-12-10T11:49:21.255215: step 4191, loss 0.36898, acc 0.953125, prec 0.0843818, recall 0.881864
2017-12-10T11:49:21.680514: step 4192, loss 0.41519, acc 0.9375, prec 0.0843957, recall 0.881894
2017-12-10T11:49:22.099025: step 4193, loss 0.510975, acc 0.875, prec 0.0843794, recall 0.881894
2017-12-10T11:49:22.531838: step 4194, loss 0.115666, acc 0.96875, prec 0.0844195, recall 0.881953
2017-12-10T11:49:22.976726: step 4195, loss 0.120896, acc 0.984375, prec 0.0844395, recall 0.881983
2017-12-10T11:49:23.433221: step 4196, loss 0.184463, acc 0.96875, prec 0.0844575, recall 0.882013
2017-12-10T11:49:23.872361: step 4197, loss 0.0618898, acc 0.984375, prec 0.0844996, recall 0.882072
2017-12-10T11:49:24.310695: step 4198, loss 0.0624153, acc 0.96875, prec 0.0845176, recall 0.882102
2017-12-10T11:49:24.753717: step 4199, loss 0.203057, acc 0.984375, prec 0.0845155, recall 0.882102
2017-12-10T11:49:25.199562: step 4200, loss 0.227086, acc 0.96875, prec 0.0845555, recall 0.882161
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4200

2017-12-10T11:49:27.122540: step 4201, loss 0.308458, acc 0.953125, prec 0.0845715, recall 0.88219
2017-12-10T11:49:27.578507: step 4202, loss 0.11618, acc 0.9375, prec 0.0845633, recall 0.88219
2017-12-10T11:49:28.025891: step 4203, loss 0.096843, acc 0.984375, prec 0.0846054, recall 0.88225
2017-12-10T11:49:28.493166: step 4204, loss 0.121692, acc 1, prec 0.0846495, recall 0.882309
2017-12-10T11:49:28.924848: step 4205, loss 0.00596954, acc 1, prec 0.0846715, recall 0.882338
2017-12-10T11:49:29.363134: step 4206, loss 0.027908, acc 0.984375, prec 0.0846695, recall 0.882338
2017-12-10T11:49:29.796214: step 4207, loss 2.42247, acc 0.96875, prec 0.0846674, recall 0.882117
2017-12-10T11:49:30.237129: step 4208, loss 0.459875, acc 0.984375, prec 0.0847095, recall 0.882176
2017-12-10T11:49:30.683096: step 4209, loss 0.271963, acc 0.90625, prec 0.0847413, recall 0.882235
2017-12-10T11:49:31.129025: step 4210, loss 0.0818514, acc 0.96875, prec 0.0847372, recall 0.882235
2017-12-10T11:49:31.579099: step 4211, loss 0.217071, acc 0.890625, prec 0.084789, recall 0.882324
2017-12-10T11:49:32.030068: step 4212, loss 0.25802, acc 0.9375, prec 0.0848029, recall 0.882353
2017-12-10T11:49:32.470672: step 4213, loss 0.321664, acc 0.921875, prec 0.0847927, recall 0.882353
2017-12-10T11:49:32.913522: step 4214, loss 0.741837, acc 0.8125, prec 0.0847902, recall 0.882382
2017-12-10T11:49:33.360364: step 4215, loss 0.382347, acc 0.890625, prec 0.0847759, recall 0.882382
2017-12-10T11:49:33.798946: step 4216, loss 0.0840104, acc 0.96875, prec 0.0847938, recall 0.882412
2017-12-10T11:49:34.234474: step 4217, loss 0.460669, acc 0.828125, prec 0.0847934, recall 0.882441
2017-12-10T11:49:34.678181: step 4218, loss 0.884924, acc 0.859375, prec 0.0847971, recall 0.882471
2017-12-10T11:49:35.111154: step 4219, loss 0.580173, acc 0.875, prec 0.0848028, recall 0.8825
2017-12-10T11:49:35.558817: step 4220, loss 0.361762, acc 0.8125, prec 0.0847783, recall 0.8825
2017-12-10T11:49:36.002541: step 4221, loss 0.577692, acc 0.890625, prec 0.0847641, recall 0.8825
2017-12-10T11:49:36.431134: step 4222, loss 0.108093, acc 0.9375, prec 0.0847559, recall 0.8825
2017-12-10T11:49:36.866557: step 4223, loss 0.25684, acc 0.96875, prec 0.0847738, recall 0.882529
2017-12-10T11:49:37.307152: step 4224, loss 0.160918, acc 0.953125, prec 0.0848336, recall 0.882617
2017-12-10T11:49:37.748699: step 4225, loss 0.228013, acc 0.921875, prec 0.0848454, recall 0.882647
2017-12-10T11:49:38.198145: step 4226, loss 0.242994, acc 0.921875, prec 0.0848792, recall 0.882705
2017-12-10T11:49:38.648374: step 4227, loss 0.555312, acc 0.96875, prec 0.0848971, recall 0.882735
2017-12-10T11:49:39.087633: step 4228, loss 0.143845, acc 0.9375, prec 0.0849109, recall 0.882764
2017-12-10T11:49:39.519921: step 4229, loss 0.231914, acc 0.921875, prec 0.0849446, recall 0.882822
2017-12-10T11:49:39.965039: step 4230, loss 0.362013, acc 0.9375, prec 0.0849584, recall 0.882851
2017-12-10T11:49:40.411769: step 4231, loss 0.0900653, acc 0.96875, prec 0.0849543, recall 0.882851
2017-12-10T11:49:40.853804: step 4232, loss 0.173938, acc 0.984375, prec 0.0850181, recall 0.882939
2017-12-10T11:49:41.301719: step 4233, loss 0.0727721, acc 0.96875, prec 0.085036, recall 0.882968
2017-12-10T11:49:41.750300: step 4234, loss 0.217657, acc 0.9375, prec 0.0850278, recall 0.882968
2017-12-10T11:49:42.202047: step 4235, loss 0.0020999, acc 1, prec 0.0850278, recall 0.882968
2017-12-10T11:49:42.636546: step 4236, loss 0.666096, acc 0.96875, prec 0.0850457, recall 0.882997
2017-12-10T11:49:43.093771: step 4237, loss 0.0584966, acc 0.984375, prec 0.0850436, recall 0.882997
2017-12-10T11:49:43.540369: step 4238, loss 1.68124, acc 0.953125, prec 0.0850615, recall 0.882807
2017-12-10T11:49:43.990730: step 4239, loss 0.0172988, acc 1, prec 0.0850834, recall 0.882836
2017-12-10T11:49:44.431953: step 4240, loss 0.172908, acc 0.984375, prec 0.0851033, recall 0.882865
2017-12-10T11:49:44.879354: step 4241, loss 0.14242, acc 0.96875, prec 0.0851212, recall 0.882894
2017-12-10T11:49:45.329851: step 4242, loss 0.0518944, acc 0.953125, prec 0.0851151, recall 0.882894
2017-12-10T11:49:45.774317: step 4243, loss 0.0975021, acc 0.953125, prec 0.0851309, recall 0.882923
2017-12-10T11:49:46.214918: step 4244, loss 0.144842, acc 0.96875, prec 0.0851706, recall 0.882981
2017-12-10T11:49:46.652713: step 4245, loss 0.162347, acc 0.953125, prec 0.0851645, recall 0.882981
2017-12-10T11:49:47.085492: step 4246, loss 0.168251, acc 0.9375, prec 0.0851783, recall 0.88301
2017-12-10T11:49:47.527016: step 4247, loss 0.316191, acc 0.921875, prec 0.0852119, recall 0.883069
2017-12-10T11:49:47.970096: step 4248, loss 0.189198, acc 0.9375, prec 0.0852037, recall 0.883069
2017-12-10T11:49:48.417205: step 4249, loss 0.671908, acc 0.921875, prec 0.0852154, recall 0.883098
2017-12-10T11:49:48.857125: step 4250, loss 0.324503, acc 0.90625, prec 0.0852032, recall 0.883098
2017-12-10T11:49:49.304470: step 4251, loss 0.0489596, acc 0.984375, prec 0.0852231, recall 0.883127
2017-12-10T11:49:49.737833: step 4252, loss 0.359999, acc 0.921875, prec 0.0852128, recall 0.883127
2017-12-10T11:49:50.194130: step 4253, loss 4.10309, acc 0.921875, prec 0.0852047, recall 0.882907
2017-12-10T11:49:50.675036: step 4254, loss 0.285376, acc 0.921875, prec 0.0851945, recall 0.882907
2017-12-10T11:49:51.113095: step 4255, loss 0.0756271, acc 0.96875, prec 0.0851904, recall 0.882907
2017-12-10T11:49:51.554731: step 4256, loss 0.28943, acc 0.921875, prec 0.0851802, recall 0.882907
2017-12-10T11:49:51.998185: step 4257, loss 0.441736, acc 0.9375, prec 0.0851721, recall 0.882907
2017-12-10T11:49:52.445292: step 4258, loss 0.069801, acc 0.96875, prec 0.085168, recall 0.882907
2017-12-10T11:49:52.887768: step 4259, loss 0.356459, acc 0.890625, prec 0.0851537, recall 0.882907
2017-12-10T11:49:53.327506: step 4260, loss 0.257458, acc 0.921875, prec 0.0851654, recall 0.882937
2017-12-10T11:49:53.769769: step 4261, loss 0.434737, acc 0.84375, prec 0.0851888, recall 0.882995
2017-12-10T11:49:54.202753: step 4262, loss 0.308458, acc 0.921875, prec 0.0851786, recall 0.882995
2017-12-10T11:49:54.647799: step 4263, loss 0.454851, acc 0.890625, prec 0.0852081, recall 0.883053
2017-12-10T11:49:55.091918: step 4264, loss 0.328265, acc 0.921875, prec 0.0852417, recall 0.88311
2017-12-10T11:49:55.534165: step 4265, loss 0.10717, acc 0.96875, prec 0.0852595, recall 0.883139
2017-12-10T11:49:55.984515: step 4266, loss 0.263471, acc 0.953125, prec 0.0853189, recall 0.883226
2017-12-10T11:49:56.422434: step 4267, loss 0.215047, acc 0.953125, prec 0.0854221, recall 0.88337
2017-12-10T11:49:56.878094: step 4268, loss 0.302392, acc 0.96875, prec 0.085418, recall 0.88337
2017-12-10T11:49:57.329624: step 4269, loss 0.351041, acc 0.90625, prec 0.0854058, recall 0.88337
2017-12-10T11:49:57.772839: step 4270, loss 0.178197, acc 0.953125, prec 0.0854433, recall 0.883428
2017-12-10T11:49:58.216156: step 4271, loss 0.213504, acc 0.96875, prec 0.0854611, recall 0.883457
2017-12-10T11:49:58.669891: step 4272, loss 0.140344, acc 0.96875, prec 0.0854789, recall 0.883486
2017-12-10T11:49:59.117827: step 4273, loss 0.0591369, acc 0.96875, prec 0.0854748, recall 0.883486
2017-12-10T11:49:59.552756: step 4274, loss 0.0976813, acc 0.96875, prec 0.0854925, recall 0.883514
2017-12-10T11:49:59.998361: step 4275, loss 0.404352, acc 0.953125, prec 0.0854864, recall 0.883514
2017-12-10T11:50:00.444645: step 4276, loss 0.0123832, acc 1, prec 0.0855082, recall 0.883543
2017-12-10T11:50:00.894913: step 4277, loss 0.30118, acc 0.921875, prec 0.0855417, recall 0.8836
2017-12-10T11:50:01.340469: step 4278, loss 0.13241, acc 0.9375, prec 0.0855554, recall 0.883629
2017-12-10T11:50:01.787576: step 4279, loss 0.147708, acc 0.96875, prec 0.0855731, recall 0.883658
2017-12-10T11:50:02.235530: step 4280, loss 0.00436856, acc 1, prec 0.0855731, recall 0.883658
2017-12-10T11:50:02.678984: step 4281, loss 0.419312, acc 0.953125, prec 0.0856325, recall 0.883744
2017-12-10T11:50:03.116439: step 4282, loss 0.31708, acc 0.984375, prec 0.0856522, recall 0.883772
2017-12-10T11:50:03.562051: step 4283, loss 0.130674, acc 0.953125, prec 0.0856461, recall 0.883772
2017-12-10T11:50:04.010365: step 4284, loss 0.0271391, acc 0.984375, prec 0.0856659, recall 0.883801
2017-12-10T11:50:04.453138: step 4285, loss 0.0271039, acc 1, prec 0.0856659, recall 0.883801
2017-12-10T11:50:04.892648: step 4286, loss 0.104164, acc 0.953125, prec 0.0856597, recall 0.883801
2017-12-10T11:50:05.338848: step 4287, loss 0.00642714, acc 1, prec 0.0857034, recall 0.883858
2017-12-10T11:50:05.777758: step 4288, loss 0.519271, acc 1, prec 0.0857252, recall 0.883887
2017-12-10T11:50:06.215892: step 4289, loss 4.14643, acc 0.96875, prec 0.085747, recall 0.883481
2017-12-10T11:50:06.662826: step 4290, loss 0.107497, acc 0.96875, prec 0.0857429, recall 0.883481
2017-12-10T11:50:07.098556: step 4291, loss 4.69912, acc 0.921875, prec 0.0857347, recall 0.883264
2017-12-10T11:50:07.536825: step 4292, loss 0.365797, acc 0.90625, prec 0.0857661, recall 0.883321
2017-12-10T11:50:07.972814: step 4293, loss 0.319067, acc 0.890625, prec 0.0857518, recall 0.883321
2017-12-10T11:50:08.425444: step 4294, loss 0.114543, acc 0.953125, prec 0.085811, recall 0.883407
2017-12-10T11:50:08.871929: step 4295, loss 0.601362, acc 0.8125, prec 0.0858083, recall 0.883436
2017-12-10T11:50:09.330896: step 4296, loss 0.690062, acc 0.8125, prec 0.0857837, recall 0.883436
2017-12-10T11:50:09.775761: step 4297, loss 0.736909, acc 0.78125, prec 0.0857769, recall 0.883464
2017-12-10T11:50:10.227007: step 4298, loss 0.487777, acc 0.859375, prec 0.0857585, recall 0.883464
2017-12-10T11:50:10.668464: step 4299, loss 0.846426, acc 0.796875, prec 0.085732, recall 0.883464
2017-12-10T11:50:11.124946: step 4300, loss 0.685397, acc 0.8125, prec 0.085751, recall 0.883521
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4300

2017-12-10T11:50:13.075241: step 4301, loss 0.441635, acc 0.828125, prec 0.0857503, recall 0.88355
2017-12-10T11:50:13.508038: step 4302, loss 0.655547, acc 0.84375, prec 0.0857517, recall 0.883578
2017-12-10T11:50:13.944775: step 4303, loss 1.14826, acc 0.765625, prec 0.0857211, recall 0.883578
2017-12-10T11:50:14.397713: step 4304, loss 0.627083, acc 0.8125, prec 0.0857401, recall 0.883635
2017-12-10T11:50:14.834442: step 4305, loss 1.23112, acc 0.75, prec 0.0857075, recall 0.883635
2017-12-10T11:50:15.288603: step 4306, loss 0.48069, acc 0.859375, prec 0.0857109, recall 0.883664
2017-12-10T11:50:15.726805: step 4307, loss 0.730068, acc 0.84375, prec 0.0856905, recall 0.883664
2017-12-10T11:50:16.177805: step 4308, loss 0.57097, acc 0.8125, prec 0.0857095, recall 0.883721
2017-12-10T11:50:16.623368: step 4309, loss 0.388769, acc 0.90625, prec 0.085719, recall 0.883749
2017-12-10T11:50:17.070875: step 4310, loss 0.877582, acc 0.796875, prec 0.0857143, recall 0.883778
2017-12-10T11:50:17.509045: step 4311, loss 0.222852, acc 0.984375, prec 0.085734, recall 0.883806
2017-12-10T11:50:17.956843: step 4312, loss 0.154279, acc 0.984375, prec 0.0857319, recall 0.883806
2017-12-10T11:50:18.394386: step 4313, loss 0.698477, acc 0.953125, prec 0.0857692, recall 0.883863
2017-12-10T11:50:18.841244: step 4314, loss 0.153984, acc 0.953125, prec 0.0857631, recall 0.883863
2017-12-10T11:50:19.278290: step 4315, loss 0.0520788, acc 0.984375, prec 0.0857611, recall 0.883863
2017-12-10T11:50:19.717036: step 4316, loss 0.0966397, acc 0.9375, prec 0.0857746, recall 0.883891
2017-12-10T11:50:20.165886: step 4317, loss 0.246202, acc 0.9375, prec 0.0857882, recall 0.88392
2017-12-10T11:50:20.620437: step 4318, loss 0.358701, acc 0.9375, prec 0.08578, recall 0.88392
2017-12-10T11:50:21.062490: step 4319, loss 0.164452, acc 0.96875, prec 0.085841, recall 0.884005
2017-12-10T11:50:21.511655: step 4320, loss 0.0218991, acc 1, prec 0.085841, recall 0.884005
2017-12-10T11:50:21.945636: step 4321, loss 0.187768, acc 0.953125, prec 0.0858999, recall 0.88409
2017-12-10T11:50:22.390825: step 4322, loss 0.0696676, acc 0.96875, prec 0.0859392, recall 0.884146
2017-12-10T11:50:22.835738: step 4323, loss 0.309047, acc 0.984375, prec 0.0859588, recall 0.884175
2017-12-10T11:50:23.287080: step 4324, loss 0.150161, acc 0.96875, prec 0.0859547, recall 0.884175
2017-12-10T11:50:23.730085: step 4325, loss 0.0108028, acc 1, prec 0.0859547, recall 0.884175
2017-12-10T11:50:24.169047: step 4326, loss 0.362875, acc 1, prec 0.0859764, recall 0.884203
2017-12-10T11:50:24.627239: step 4327, loss 0.0385923, acc 0.984375, prec 0.085996, recall 0.884231
2017-12-10T11:50:25.066909: step 4328, loss 0.065863, acc 0.984375, prec 0.085994, recall 0.884231
2017-12-10T11:50:25.510651: step 4329, loss 0.176692, acc 0.9375, prec 0.0860075, recall 0.884259
2017-12-10T11:50:25.955514: step 4330, loss 0.188614, acc 0.96875, prec 0.0860251, recall 0.884287
2017-12-10T11:50:26.412446: step 4331, loss 0.0238203, acc 1, prec 0.0860684, recall 0.884344
2017-12-10T11:50:26.849590: step 4332, loss 0.0124776, acc 0.984375, prec 0.0860664, recall 0.884344
2017-12-10T11:50:27.292619: step 4333, loss 0.0262038, acc 0.984375, prec 0.0860643, recall 0.884344
2017-12-10T11:50:27.746436: step 4334, loss 0.0688259, acc 0.953125, prec 0.0860582, recall 0.884344
2017-12-10T11:50:28.202199: step 4335, loss 0.16211, acc 0.953125, prec 0.0860521, recall 0.884344
2017-12-10T11:50:28.648167: step 4336, loss 0.146436, acc 0.984375, prec 0.08605, recall 0.884344
2017-12-10T11:50:29.090366: step 4337, loss 0.210529, acc 0.96875, prec 0.086046, recall 0.884344
2017-12-10T11:50:29.548575: step 4338, loss 0.439558, acc 0.96875, prec 0.0860635, recall 0.884372
2017-12-10T11:50:30.000428: step 4339, loss 0.0548045, acc 0.96875, prec 0.0860595, recall 0.884372
2017-12-10T11:50:30.454250: step 4340, loss 0.0205475, acc 0.984375, prec 0.0860791, recall 0.8844
2017-12-10T11:50:30.906475: step 4341, loss 1.9604, acc 0.921875, prec 0.0861358, recall 0.884269
2017-12-10T11:50:31.354065: step 4342, loss 0.113878, acc 1, prec 0.0861791, recall 0.884326
2017-12-10T11:50:31.807698: step 4343, loss 0.0432365, acc 0.984375, prec 0.0861771, recall 0.884326
2017-12-10T11:50:32.253991: step 4344, loss 0.110108, acc 0.984375, prec 0.086175, recall 0.884326
2017-12-10T11:50:32.701509: step 4345, loss 0.0282003, acc 0.984375, prec 0.0861946, recall 0.884354
2017-12-10T11:50:33.140876: step 4346, loss 0.939289, acc 0.953125, prec 0.0862967, recall 0.884494
2017-12-10T11:50:33.585492: step 4347, loss 0.108663, acc 0.984375, prec 0.0863379, recall 0.88455
2017-12-10T11:50:34.045619: step 4348, loss 0.119523, acc 0.984375, prec 0.0863359, recall 0.88455
2017-12-10T11:50:34.492700: step 4349, loss 0.180608, acc 0.96875, prec 0.086375, recall 0.884606
2017-12-10T11:50:34.932550: step 4350, loss 0.297646, acc 0.921875, prec 0.0863864, recall 0.884634
2017-12-10T11:50:35.380266: step 4351, loss 0.225211, acc 0.953125, prec 0.0864884, recall 0.884774
2017-12-10T11:50:35.836596: step 4352, loss 0.260272, acc 0.921875, prec 0.0865214, recall 0.884829
2017-12-10T11:50:36.283552: step 4353, loss 0.358573, acc 0.875, prec 0.086505, recall 0.884829
2017-12-10T11:50:36.724077: step 4354, loss 0.4255, acc 0.9375, prec 0.0864968, recall 0.884829
2017-12-10T11:50:37.168616: step 4355, loss 0.530607, acc 0.890625, prec 0.0865041, recall 0.884857
2017-12-10T11:50:37.617466: step 4356, loss 0.550025, acc 0.875, prec 0.0864878, recall 0.884857
2017-12-10T11:50:38.063971: step 4357, loss 0.332865, acc 0.90625, prec 0.0864971, recall 0.884885
2017-12-10T11:50:38.501336: step 4358, loss 0.618889, acc 0.859375, prec 0.0864787, recall 0.884885
2017-12-10T11:50:38.937900: step 4359, loss 0.290509, acc 0.921875, prec 0.0865332, recall 0.884969
2017-12-10T11:50:39.385892: step 4360, loss 0.353435, acc 0.890625, prec 0.0865621, recall 0.885024
2017-12-10T11:50:39.824482: step 4361, loss 0.194736, acc 0.9375, prec 0.0865539, recall 0.885024
2017-12-10T11:50:40.260560: step 4362, loss 0.183808, acc 0.953125, prec 0.0865478, recall 0.885024
2017-12-10T11:50:40.702145: step 4363, loss 0.339374, acc 0.921875, prec 0.0865376, recall 0.885024
2017-12-10T11:50:41.142048: step 4364, loss 0.0730263, acc 0.96875, prec 0.086555, recall 0.885052
2017-12-10T11:50:41.580340: step 4365, loss 0.138907, acc 0.953125, prec 0.086592, recall 0.885107
2017-12-10T11:50:42.031779: step 4366, loss 0.154151, acc 0.9375, prec 0.0865839, recall 0.885107
2017-12-10T11:50:42.470268: step 4367, loss 0.178847, acc 0.953125, prec 0.0865777, recall 0.885107
2017-12-10T11:50:42.912758: step 4368, loss 0.046269, acc 0.96875, prec 0.0865736, recall 0.885107
2017-12-10T11:50:43.362315: step 4369, loss 0.427302, acc 0.890625, prec 0.086624, recall 0.885191
2017-12-10T11:50:43.804206: step 4370, loss 0.0732778, acc 0.9375, prec 0.0866374, recall 0.885218
2017-12-10T11:50:44.243384: step 4371, loss 0.625407, acc 0.953125, prec 0.0867175, recall 0.885329
2017-12-10T11:50:44.686690: step 4372, loss 0.0620726, acc 1, prec 0.0867606, recall 0.885384
2017-12-10T11:50:45.122110: step 4373, loss 0.0162038, acc 1, prec 0.0867606, recall 0.885384
2017-12-10T11:50:45.559511: step 4374, loss 0.0482789, acc 0.96875, prec 0.086778, recall 0.885412
2017-12-10T11:50:46.006239: step 4375, loss 0.312089, acc 0.921875, prec 0.0867893, recall 0.885439
2017-12-10T11:50:46.448908: step 4376, loss 0.179288, acc 0.953125, prec 0.0868047, recall 0.885467
2017-12-10T11:50:46.881393: step 4377, loss 0.0109518, acc 1, prec 0.0868047, recall 0.885467
2017-12-10T11:50:47.316602: step 4378, loss 0.276074, acc 0.953125, prec 0.0867986, recall 0.885467
2017-12-10T11:50:47.760221: step 4379, loss 0.0138294, acc 1, prec 0.0867986, recall 0.885467
2017-12-10T11:50:48.197970: step 4380, loss 0.0525002, acc 0.96875, prec 0.0867945, recall 0.885467
2017-12-10T11:50:48.644269: step 4381, loss 0.013626, acc 1, prec 0.0867945, recall 0.885467
2017-12-10T11:50:49.090384: step 4382, loss 0.699092, acc 0.984375, prec 0.086814, recall 0.885494
2017-12-10T11:50:49.534296: step 4383, loss 0.215154, acc 0.96875, prec 0.086853, recall 0.885549
2017-12-10T11:50:49.975824: step 4384, loss 0.039899, acc 0.984375, prec 0.0868509, recall 0.885549
2017-12-10T11:50:50.414981: step 4385, loss 0.0605248, acc 0.984375, prec 0.0868704, recall 0.885577
2017-12-10T11:50:50.873042: step 4386, loss 0.0969874, acc 0.96875, prec 0.0868878, recall 0.885604
2017-12-10T11:50:51.319674: step 4387, loss 0.0506588, acc 0.984375, prec 0.0869073, recall 0.885632
2017-12-10T11:50:51.771160: step 4388, loss 0.275182, acc 0.9375, prec 0.0869206, recall 0.885659
2017-12-10T11:50:52.211307: step 4389, loss 0.0758574, acc 0.953125, prec 0.086936, recall 0.885687
2017-12-10T11:50:52.655256: step 4390, loss 0.840076, acc 0.9375, prec 0.0869924, recall 0.885769
2017-12-10T11:50:53.106591: step 4391, loss 0.119778, acc 0.984375, prec 0.0870334, recall 0.885824
2017-12-10T11:50:53.561307: step 4392, loss 0.540141, acc 0.921875, prec 0.0870446, recall 0.885851
2017-12-10T11:50:54.012912: step 4393, loss 0.252949, acc 0.96875, prec 0.087062, recall 0.885879
2017-12-10T11:50:54.455323: step 4394, loss 0.108656, acc 0.953125, prec 0.0870989, recall 0.885933
2017-12-10T11:50:54.912961: step 4395, loss 0.030503, acc 0.984375, prec 0.0871399, recall 0.885988
2017-12-10T11:50:55.355797: step 4396, loss 0.23876, acc 0.96875, prec 0.0871788, recall 0.886043
2017-12-10T11:50:55.797540: step 4397, loss 0.347885, acc 0.921875, prec 0.08719, recall 0.88607
2017-12-10T11:50:56.252938: step 4398, loss 0.219745, acc 0.90625, prec 0.0871992, recall 0.886097
2017-12-10T11:50:56.690303: step 4399, loss 0.121077, acc 0.96875, prec 0.0872166, recall 0.886124
2017-12-10T11:50:57.132581: step 4400, loss 0.534887, acc 0.953125, prec 0.0872319, recall 0.886152
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4400

2017-12-10T11:50:59.017415: step 4401, loss 0.196863, acc 0.921875, prec 0.0872431, recall 0.886179
2017-12-10T11:50:59.457527: step 4402, loss 0.154122, acc 0.984375, prec 0.0872411, recall 0.886179
2017-12-10T11:50:59.910342: step 4403, loss 0.118882, acc 0.921875, prec 0.0872738, recall 0.886233
2017-12-10T11:51:00.364329: step 4404, loss 0.0531054, acc 0.984375, prec 0.0872717, recall 0.886233
2017-12-10T11:51:00.812721: step 4405, loss 0.127083, acc 0.953125, prec 0.087287, recall 0.88626
2017-12-10T11:51:01.265303: step 4406, loss 0.134543, acc 0.96875, prec 0.0873044, recall 0.886288
2017-12-10T11:51:01.713765: step 4407, loss 0.0959176, acc 0.984375, prec 0.0873023, recall 0.886288
2017-12-10T11:51:02.162874: step 4408, loss 0.272506, acc 0.921875, prec 0.0872921, recall 0.886288
2017-12-10T11:51:02.608740: step 4409, loss 0.255093, acc 0.90625, prec 0.0872797, recall 0.886288
2017-12-10T11:51:03.045279: step 4410, loss 0.268325, acc 0.9375, prec 0.0872715, recall 0.886288
2017-12-10T11:51:03.502533: step 4411, loss 0.289039, acc 0.890625, prec 0.0872786, recall 0.886315
2017-12-10T11:51:03.956162: step 4412, loss 0.0972114, acc 0.96875, prec 0.0873175, recall 0.886369
2017-12-10T11:51:04.396215: step 4413, loss 0.308928, acc 0.90625, prec 0.087391, recall 0.886477
2017-12-10T11:51:04.843435: step 4414, loss 0.152516, acc 0.921875, prec 0.0873807, recall 0.886477
2017-12-10T11:51:05.282916: step 4415, loss 0.556368, acc 0.953125, prec 0.0874174, recall 0.886532
2017-12-10T11:51:05.724802: step 4416, loss 0.0462757, acc 0.984375, prec 0.0874368, recall 0.886559
2017-12-10T11:51:06.165214: step 4417, loss 0.110929, acc 0.9375, prec 0.0874286, recall 0.886559
2017-12-10T11:51:06.615666: step 4418, loss 0.19615, acc 0.96875, prec 0.0874888, recall 0.88664
2017-12-10T11:51:07.075477: step 4419, loss 0.119204, acc 0.96875, prec 0.0875062, recall 0.886667
2017-12-10T11:51:07.509303: step 4420, loss 0.09017, acc 0.96875, prec 0.0875449, recall 0.886721
2017-12-10T11:51:07.942192: step 4421, loss 0.0756088, acc 0.96875, prec 0.0875623, recall 0.886748
2017-12-10T11:51:08.383605: step 4422, loss 0.358092, acc 0.890625, prec 0.0876122, recall 0.886828
2017-12-10T11:51:08.824533: step 4423, loss 0.0655213, acc 0.984375, prec 0.0876744, recall 0.886909
2017-12-10T11:51:09.268174: step 4424, loss 0.906597, acc 0.984375, prec 0.0877152, recall 0.886963
2017-12-10T11:51:09.727763: step 4425, loss 0.0846321, acc 0.96875, prec 0.0877111, recall 0.886963
2017-12-10T11:51:10.164970: step 4426, loss 0.109444, acc 0.984375, prec 0.0877518, recall 0.887016
2017-12-10T11:51:10.603081: step 4427, loss 0.0725674, acc 0.984375, prec 0.0877712, recall 0.887043
2017-12-10T11:51:11.048127: step 4428, loss 0.149498, acc 0.96875, prec 0.0877671, recall 0.887043
2017-12-10T11:51:11.488474: step 4429, loss 0.135836, acc 0.9375, prec 0.0877588, recall 0.887043
2017-12-10T11:51:11.928631: step 4430, loss 0.0222774, acc 1, prec 0.0877803, recall 0.88707
2017-12-10T11:51:12.383403: step 4431, loss 0.305704, acc 0.90625, prec 0.0877893, recall 0.887097
2017-12-10T11:51:12.815832: step 4432, loss 0.0713933, acc 0.984375, prec 0.0878087, recall 0.887124
2017-12-10T11:51:13.261123: step 4433, loss 0.11218, acc 0.96875, prec 0.0878045, recall 0.887124
2017-12-10T11:51:13.704504: step 4434, loss 0.0219147, acc 1, prec 0.0878259, recall 0.88715
2017-12-10T11:51:14.138800: step 4435, loss 0.264775, acc 0.96875, prec 0.0878432, recall 0.887177
2017-12-10T11:51:14.581605: step 4436, loss 0.0731162, acc 0.984375, prec 0.0878626, recall 0.887204
2017-12-10T11:51:15.053514: step 4437, loss 0.111571, acc 0.96875, prec 0.0878799, recall 0.887231
2017-12-10T11:51:15.498033: step 4438, loss 0.363538, acc 0.96875, prec 0.0878757, recall 0.887231
2017-12-10T11:51:15.953926: step 4439, loss 0.133843, acc 0.984375, prec 0.0879165, recall 0.887284
2017-12-10T11:51:16.396456: step 4440, loss 0.0658371, acc 0.96875, prec 0.0879123, recall 0.887284
2017-12-10T11:51:16.843008: step 4441, loss 0.144377, acc 0.96875, prec 0.0879082, recall 0.887284
2017-12-10T11:51:17.277861: step 4442, loss 0.336419, acc 0.96875, prec 0.0879255, recall 0.887311
2017-12-10T11:51:17.718498: step 4443, loss 0.0920634, acc 0.953125, prec 0.0879193, recall 0.887311
2017-12-10T11:51:18.169226: step 4444, loss 0.0232813, acc 0.984375, prec 0.0879172, recall 0.887311
2017-12-10T11:51:18.621279: step 4445, loss 0.0675927, acc 0.984375, prec 0.0879794, recall 0.887391
2017-12-10T11:51:19.060722: step 4446, loss 0.107433, acc 0.984375, prec 0.0879773, recall 0.887391
2017-12-10T11:51:19.505912: step 4447, loss 0.0920959, acc 0.984375, prec 0.0879752, recall 0.887391
2017-12-10T11:51:19.946106: step 4448, loss 0.00673928, acc 1, prec 0.0879752, recall 0.887391
2017-12-10T11:51:20.392304: step 4449, loss 0.111222, acc 0.96875, prec 0.0879925, recall 0.887417
2017-12-10T11:51:20.832881: step 4450, loss 0.00226438, acc 1, prec 0.0879925, recall 0.887417
2017-12-10T11:51:21.264812: step 4451, loss 0.845985, acc 0.984375, prec 0.0880332, recall 0.88747
2017-12-10T11:51:21.718278: step 4452, loss 0.315031, acc 0.953125, prec 0.0880484, recall 0.887497
2017-12-10T11:51:22.168048: step 4453, loss 0.155517, acc 0.984375, prec 0.0880891, recall 0.88755
2017-12-10T11:51:22.610506: step 4454, loss 0.0332389, acc 0.984375, prec 0.088087, recall 0.88755
2017-12-10T11:51:23.041859: step 4455, loss 0.0782109, acc 0.96875, prec 0.0880829, recall 0.88755
2017-12-10T11:51:23.485041: step 4456, loss 0.110515, acc 0.96875, prec 0.0880788, recall 0.88755
2017-12-10T11:51:23.925092: step 4457, loss 0.188812, acc 0.96875, prec 0.088096, recall 0.887577
2017-12-10T11:51:24.374734: step 4458, loss 0.52186, acc 0.890625, prec 0.0881243, recall 0.88763
2017-12-10T11:51:24.823032: step 4459, loss 0.175091, acc 1, prec 0.0881884, recall 0.887709
2017-12-10T11:51:25.263217: step 4460, loss 0.568029, acc 0.984375, prec 0.0882077, recall 0.887736
2017-12-10T11:51:25.710414: step 4461, loss 0.276105, acc 0.9375, prec 0.0881995, recall 0.887736
2017-12-10T11:51:26.154491: step 4462, loss 0.249589, acc 0.953125, prec 0.088236, recall 0.887789
2017-12-10T11:51:26.590326: step 4463, loss 0.17809, acc 0.921875, prec 0.0882256, recall 0.887789
2017-12-10T11:51:27.038732: step 4464, loss 0.182982, acc 0.984375, prec 0.0882449, recall 0.887815
2017-12-10T11:51:27.493864: step 4465, loss 0.187253, acc 0.96875, prec 0.0882408, recall 0.887815
2017-12-10T11:51:27.931785: step 4466, loss 0.0574181, acc 0.96875, prec 0.088258, recall 0.887842
2017-12-10T11:51:28.372153: step 4467, loss 0.134945, acc 0.9375, prec 0.0882498, recall 0.887842
2017-12-10T11:51:28.825639: step 4468, loss 0.0762476, acc 0.984375, prec 0.088269, recall 0.887868
2017-12-10T11:51:29.260167: step 4469, loss 2.15803, acc 0.953125, prec 0.0882649, recall 0.887659
2017-12-10T11:51:29.704778: step 4470, loss 0.112787, acc 0.96875, prec 0.0883248, recall 0.887738
2017-12-10T11:51:30.144554: step 4471, loss 0.0271033, acc 0.984375, prec 0.0883441, recall 0.887765
2017-12-10T11:51:30.584444: step 4472, loss 0.0820394, acc 0.984375, prec 0.088342, recall 0.887765
2017-12-10T11:51:30.973669: step 4473, loss 0.11945, acc 0.941176, prec 0.0883358, recall 0.887765
2017-12-10T11:51:31.436591: step 4474, loss 0.336074, acc 0.921875, prec 0.0883255, recall 0.887765
2017-12-10T11:51:31.878413: step 4475, loss 0.0515859, acc 0.984375, prec 0.0883448, recall 0.887791
2017-12-10T11:51:32.317892: step 4476, loss 0.136311, acc 0.9375, prec 0.0883365, recall 0.887791
2017-12-10T11:51:32.766744: step 4477, loss 0.0284419, acc 1, prec 0.0883365, recall 0.887791
2017-12-10T11:51:33.202302: step 4478, loss 0.125565, acc 0.96875, prec 0.0883324, recall 0.887791
2017-12-10T11:51:33.638534: step 4479, loss 0.383254, acc 0.9375, prec 0.0883241, recall 0.887791
2017-12-10T11:51:34.112277: step 4480, loss 0.343796, acc 0.90625, prec 0.088333, recall 0.887818
2017-12-10T11:51:34.542537: step 4481, loss 0.0714704, acc 0.984375, prec 0.0883949, recall 0.887897
2017-12-10T11:51:34.992482: step 4482, loss 0.219471, acc 0.953125, prec 0.0883887, recall 0.887897
2017-12-10T11:51:35.438547: step 4483, loss 0.107233, acc 0.921875, prec 0.0883784, recall 0.887897
2017-12-10T11:51:35.875702: step 4484, loss 0.369431, acc 0.984375, prec 0.0883977, recall 0.887923
2017-12-10T11:51:36.319546: step 4485, loss 0.0128783, acc 1, prec 0.088419, recall 0.887949
2017-12-10T11:51:36.751402: step 4486, loss 0.334684, acc 0.9375, prec 0.0884107, recall 0.887949
2017-12-10T11:51:37.182788: step 4487, loss 0.0531611, acc 0.984375, prec 0.0884513, recall 0.888002
2017-12-10T11:51:37.628179: step 4488, loss 0.10459, acc 0.984375, prec 0.0884705, recall 0.888028
2017-12-10T11:51:38.070157: step 4489, loss 0.0784219, acc 0.953125, prec 0.0884643, recall 0.888028
2017-12-10T11:51:38.516089: step 4490, loss 0.0762477, acc 0.96875, prec 0.0884815, recall 0.888054
2017-12-10T11:51:38.959034: step 4491, loss 0.0483592, acc 0.984375, prec 0.0885007, recall 0.888081
2017-12-10T11:51:39.397221: step 4492, loss 0.210051, acc 0.9375, prec 0.0885138, recall 0.888107
2017-12-10T11:51:39.839281: step 4493, loss 0.0754297, acc 0.953125, prec 0.0885289, recall 0.888133
2017-12-10T11:51:40.295495: step 4494, loss 0.220039, acc 0.953125, prec 0.088544, recall 0.888159
2017-12-10T11:51:40.746511: step 4495, loss 0.00995985, acc 1, prec 0.0885653, recall 0.888186
2017-12-10T11:51:41.189980: step 4496, loss 0.041466, acc 0.984375, prec 0.0886058, recall 0.888238
2017-12-10T11:51:41.629884: step 4497, loss 0.0857529, acc 0.953125, prec 0.0886209, recall 0.888264
2017-12-10T11:51:42.076143: step 4498, loss 0.0718219, acc 0.96875, prec 0.0886594, recall 0.888317
2017-12-10T11:51:42.528101: step 4499, loss 0.0783432, acc 0.984375, prec 0.0886786, recall 0.888343
2017-12-10T11:51:42.975176: step 4500, loss 0.169486, acc 0.984375, prec 0.0886765, recall 0.888343
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4500

2017-12-10T11:51:44.967110: step 4501, loss 0.0942662, acc 0.953125, prec 0.0886703, recall 0.888343
2017-12-10T11:51:45.410945: step 4502, loss 0.00368696, acc 1, prec 0.0886703, recall 0.888343
2017-12-10T11:51:45.843744: step 4503, loss 0.0980543, acc 0.953125, prec 0.0886641, recall 0.888343
2017-12-10T11:51:46.282992: step 4504, loss 0.0634484, acc 0.984375, prec 0.0887046, recall 0.888395
2017-12-10T11:51:46.735954: step 4505, loss 0.00413242, acc 1, prec 0.0887046, recall 0.888395
2017-12-10T11:51:47.176384: step 4506, loss 0.0276752, acc 0.984375, prec 0.0887451, recall 0.888447
2017-12-10T11:51:47.619431: step 4507, loss 0.0377422, acc 0.984375, prec 0.0887643, recall 0.888473
2017-12-10T11:51:48.067287: step 4508, loss 0.0197583, acc 0.984375, prec 0.0887622, recall 0.888473
2017-12-10T11:51:48.509064: step 4509, loss 0.438931, acc 0.953125, prec 0.088756, recall 0.888473
2017-12-10T11:51:48.951512: step 4510, loss 0.00538207, acc 1, prec 0.088756, recall 0.888473
2017-12-10T11:51:49.396074: step 4511, loss 7.73125, acc 0.953125, prec 0.0887944, recall 0.888318
2017-12-10T11:51:49.843619: step 4512, loss 0.472987, acc 0.921875, prec 0.0888053, recall 0.888344
2017-12-10T11:51:50.292820: step 4513, loss 0.0613691, acc 0.984375, prec 0.0888033, recall 0.888344
2017-12-10T11:51:50.766992: step 4514, loss 0.187731, acc 0.984375, prec 0.088865, recall 0.888422
2017-12-10T11:51:51.205464: step 4515, loss 0.0991092, acc 0.96875, prec 0.0888821, recall 0.888448
2017-12-10T11:51:51.643432: step 4516, loss 0.0856548, acc 0.984375, prec 0.0889013, recall 0.888474
2017-12-10T11:51:52.077016: step 4517, loss 0.28823, acc 0.90625, prec 0.0889102, recall 0.8885
2017-12-10T11:51:52.526277: step 4518, loss 0.169507, acc 0.921875, prec 0.0888998, recall 0.8885
2017-12-10T11:51:52.969714: step 4519, loss 0.386693, acc 0.9375, prec 0.0889127, recall 0.888526
2017-12-10T11:51:53.404825: step 4520, loss 0.117454, acc 0.953125, prec 0.0889065, recall 0.888526
2017-12-10T11:51:53.852356: step 4521, loss 0.181439, acc 0.921875, prec 0.0888961, recall 0.888526
2017-12-10T11:51:54.293948: step 4522, loss 0.143067, acc 0.921875, prec 0.0888858, recall 0.888526
2017-12-10T11:51:54.738675: step 4523, loss 0.183553, acc 0.96875, prec 0.0889241, recall 0.888578
2017-12-10T11:51:55.188169: step 4524, loss 0.255345, acc 0.921875, prec 0.088935, recall 0.888604
2017-12-10T11:51:55.635524: step 4525, loss 0.224745, acc 0.9375, prec 0.0889267, recall 0.888604
2017-12-10T11:51:56.084592: step 4526, loss 0.158347, acc 0.921875, prec 0.0889588, recall 0.888656
2017-12-10T11:51:56.531562: step 4527, loss 0.228154, acc 0.90625, prec 0.0889464, recall 0.888656
2017-12-10T11:51:56.977043: step 4528, loss 0.163926, acc 0.9375, prec 0.0889593, recall 0.888682
2017-12-10T11:51:57.418002: step 4529, loss 0.107871, acc 0.9375, prec 0.0890572, recall 0.888811
2017-12-10T11:51:57.869744: step 4530, loss 0.17303, acc 0.96875, prec 0.0890955, recall 0.888863
2017-12-10T11:51:58.313419: step 4531, loss 0.0797616, acc 0.953125, prec 0.0890893, recall 0.888863
2017-12-10T11:51:58.751128: step 4532, loss 0.399192, acc 0.921875, prec 0.0891001, recall 0.888889
2017-12-10T11:51:59.193634: step 4533, loss 0.190583, acc 0.921875, prec 0.0890898, recall 0.888889
2017-12-10T11:51:59.642320: step 4534, loss 0.437089, acc 0.890625, prec 0.0890965, recall 0.888915
2017-12-10T11:52:00.091467: step 4535, loss 0.0902726, acc 0.9375, prec 0.0890882, recall 0.888915
2017-12-10T11:52:00.547538: step 4536, loss 0.228219, acc 0.90625, prec 0.0890757, recall 0.888915
2017-12-10T11:52:00.992671: step 4537, loss 0.312257, acc 0.90625, prec 0.0890633, recall 0.888915
2017-12-10T11:52:01.428921: step 4538, loss 0.433664, acc 0.9375, prec 0.0890762, recall 0.888941
2017-12-10T11:52:01.869712: step 4539, loss 0.133679, acc 0.921875, prec 0.0891082, recall 0.888992
2017-12-10T11:52:02.312797: step 4540, loss 0.0716412, acc 0.96875, prec 0.0891253, recall 0.889018
2017-12-10T11:52:02.762120: step 4541, loss 0.0964115, acc 0.96875, prec 0.0891423, recall 0.889044
2017-12-10T11:52:03.210677: step 4542, loss 0.217775, acc 0.96875, prec 0.0891382, recall 0.889044
2017-12-10T11:52:03.662544: step 4543, loss 0.0432885, acc 0.984375, prec 0.0891361, recall 0.889044
2017-12-10T11:52:04.108511: step 4544, loss 0.0626626, acc 0.984375, prec 0.089134, recall 0.889044
2017-12-10T11:52:04.556124: step 4545, loss 0.0429629, acc 0.984375, prec 0.0891531, recall 0.889069
2017-12-10T11:52:04.999664: step 4546, loss 0.259044, acc 0.953125, prec 0.0891681, recall 0.889095
2017-12-10T11:52:05.444652: step 4547, loss 0.0521543, acc 0.984375, prec 0.089166, recall 0.889095
2017-12-10T11:52:05.882112: step 4548, loss 0.0615469, acc 0.984375, prec 0.0892064, recall 0.889147
2017-12-10T11:52:06.325815: step 4549, loss 0.0723861, acc 0.984375, prec 0.0892043, recall 0.889147
2017-12-10T11:52:06.773829: step 4550, loss 0.0380549, acc 0.984375, prec 0.0892234, recall 0.889172
2017-12-10T11:52:07.233899: step 4551, loss 0.00346352, acc 1, prec 0.0892234, recall 0.889172
2017-12-10T11:52:07.674371: step 4552, loss 0.112798, acc 0.96875, prec 0.0892192, recall 0.889172
2017-12-10T11:52:08.125933: step 4553, loss 0.0655882, acc 0.96875, prec 0.0892787, recall 0.889249
2017-12-10T11:52:08.571678: step 4554, loss 0.64314, acc 1, prec 0.0892998, recall 0.889275
2017-12-10T11:52:09.027396: step 4555, loss 0.0411018, acc 0.984375, prec 0.0893401, recall 0.889326
2017-12-10T11:52:09.467012: step 4556, loss 0.167932, acc 0.96875, prec 0.0893572, recall 0.889352
2017-12-10T11:52:09.914756: step 4557, loss 0.0777622, acc 0.984375, prec 0.0893551, recall 0.889352
2017-12-10T11:52:10.365252: step 4558, loss 0.089222, acc 0.984375, prec 0.089353, recall 0.889352
2017-12-10T11:52:10.803302: step 4559, loss 0.051701, acc 1, prec 0.0894165, recall 0.889429
2017-12-10T11:52:11.247025: step 4560, loss 0.104183, acc 0.984375, prec 0.0894356, recall 0.889454
2017-12-10T11:52:11.702387: step 4561, loss 1.82574, acc 0.96875, prec 0.0894547, recall 0.889274
2017-12-10T11:52:12.144140: step 4562, loss 0.670957, acc 0.953125, prec 0.0894485, recall 0.889274
2017-12-10T11:52:12.616495: step 4563, loss 3.90131, acc 0.9375, prec 0.0894634, recall 0.889094
2017-12-10T11:52:13.073517: step 4564, loss 0.299507, acc 0.921875, prec 0.0894742, recall 0.88912
2017-12-10T11:52:13.512857: step 4565, loss 0.262131, acc 0.953125, prec 0.0895314, recall 0.889197
2017-12-10T11:52:13.964030: step 4566, loss 0.349941, acc 0.890625, prec 0.089538, recall 0.889222
2017-12-10T11:52:14.403943: step 4567, loss 0.519745, acc 0.859375, prec 0.0895193, recall 0.889222
2017-12-10T11:52:14.850156: step 4568, loss 0.463471, acc 0.859375, prec 0.0895006, recall 0.889222
2017-12-10T11:52:15.285043: step 4569, loss 1.31942, acc 0.75, prec 0.0894673, recall 0.889222
2017-12-10T11:52:15.736167: step 4570, loss 0.544744, acc 0.84375, prec 0.0894466, recall 0.889222
2017-12-10T11:52:16.180121: step 4571, loss 0.616203, acc 0.8125, prec 0.0894428, recall 0.889248
2017-12-10T11:52:16.623723: step 4572, loss 1.09829, acc 0.765625, prec 0.0894117, recall 0.889248
2017-12-10T11:52:17.069983: step 4573, loss 0.3626, acc 0.859375, prec 0.089393, recall 0.889248
2017-12-10T11:52:17.518448: step 4574, loss 0.702368, acc 0.875, prec 0.0894186, recall 0.889299
2017-12-10T11:52:17.961012: step 4575, loss 0.697062, acc 0.84375, prec 0.089419, recall 0.889324
2017-12-10T11:52:18.397349: step 4576, loss 0.314316, acc 0.84375, prec 0.0893983, recall 0.889324
2017-12-10T11:52:18.849898: step 4577, loss 0.362475, acc 0.859375, prec 0.0894008, recall 0.88935
2017-12-10T11:52:19.287077: step 4578, loss 0.67873, acc 0.8125, prec 0.0893759, recall 0.88935
2017-12-10T11:52:19.735241: step 4579, loss 0.721212, acc 0.796875, prec 0.089349, recall 0.88935
2017-12-10T11:52:20.182396: step 4580, loss 0.970989, acc 0.8125, prec 0.0893242, recall 0.88935
2017-12-10T11:52:20.632960: step 4581, loss 0.670917, acc 0.890625, prec 0.0893308, recall 0.889375
2017-12-10T11:52:21.083383: step 4582, loss 2.3312, acc 0.84375, prec 0.0893543, recall 0.889222
2017-12-10T11:52:21.532139: step 4583, loss 0.420018, acc 0.84375, prec 0.0893336, recall 0.889222
2017-12-10T11:52:21.981938: step 4584, loss 0.420242, acc 0.90625, prec 0.0893212, recall 0.889222
2017-12-10T11:52:22.421690: step 4585, loss 0.364368, acc 0.90625, prec 0.0893931, recall 0.889324
2017-12-10T11:52:22.868873: step 4586, loss 0.614384, acc 0.859375, prec 0.0893745, recall 0.889324
2017-12-10T11:52:23.318645: step 4587, loss 0.299292, acc 0.90625, prec 0.0894042, recall 0.889374
2017-12-10T11:52:23.765832: step 4588, loss 0.493195, acc 0.921875, prec 0.089436, recall 0.889425
2017-12-10T11:52:24.210353: step 4589, loss 0.211465, acc 0.921875, prec 0.0894256, recall 0.889425
2017-12-10T11:52:24.667150: step 4590, loss 0.230565, acc 0.953125, prec 0.0894194, recall 0.889425
2017-12-10T11:52:25.114565: step 4591, loss 0.25734, acc 0.953125, prec 0.0894553, recall 0.889476
2017-12-10T11:52:25.568188: step 4592, loss 0.0751527, acc 0.96875, prec 0.0894512, recall 0.889476
2017-12-10T11:52:26.020236: step 4593, loss 0.255649, acc 0.90625, prec 0.0894388, recall 0.889476
2017-12-10T11:52:26.468364: step 4594, loss 0.211914, acc 0.96875, prec 0.0894978, recall 0.889552
2017-12-10T11:52:26.908275: step 4595, loss 0.217862, acc 0.921875, prec 0.0895085, recall 0.889578
2017-12-10T11:52:27.343466: step 4596, loss 0.41772, acc 0.984375, prec 0.0895695, recall 0.889654
2017-12-10T11:52:27.786646: step 4597, loss 0.0901918, acc 0.96875, prec 0.0895653, recall 0.889654
2017-12-10T11:52:28.228653: step 4598, loss 1.1682, acc 0.953125, prec 0.0895612, recall 0.88945
2017-12-10T11:52:28.682629: step 4599, loss 0.23394, acc 0.953125, prec 0.089555, recall 0.88945
2017-12-10T11:52:29.124696: step 4600, loss 0.47289, acc 0.953125, prec 0.0895698, recall 0.889475
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4600

2017-12-10T11:52:30.965320: step 4601, loss 0.894815, acc 0.90625, prec 0.0895784, recall 0.8895
2017-12-10T11:52:31.415590: step 4602, loss 0.0477678, acc 0.984375, prec 0.0895974, recall 0.889526
2017-12-10T11:52:31.870342: step 4603, loss 0.1041, acc 0.9375, prec 0.0895891, recall 0.889526
2017-12-10T11:52:32.324131: step 4604, loss 0.0999822, acc 0.953125, prec 0.0896039, recall 0.889551
2017-12-10T11:52:32.771599: step 4605, loss 0.175795, acc 0.96875, prec 0.0895998, recall 0.889551
2017-12-10T11:52:33.223025: step 4606, loss 0.47397, acc 0.921875, prec 0.0896104, recall 0.889576
2017-12-10T11:52:33.668748: step 4607, loss 0.127707, acc 0.96875, prec 0.0896063, recall 0.889576
2017-12-10T11:52:34.118529: step 4608, loss 0.19895, acc 0.921875, prec 0.089617, recall 0.889601
2017-12-10T11:52:34.563454: step 4609, loss 0.265097, acc 0.9375, prec 0.0896297, recall 0.889627
2017-12-10T11:52:35.008310: step 4610, loss 0.114419, acc 0.953125, prec 0.0896655, recall 0.889677
2017-12-10T11:52:35.444834: step 4611, loss 0.150614, acc 0.953125, prec 0.0896803, recall 0.889702
2017-12-10T11:52:35.871254: step 4612, loss 0.0569985, acc 0.96875, prec 0.0896762, recall 0.889702
2017-12-10T11:52:36.321368: step 4613, loss 0.235163, acc 0.953125, prec 0.089712, recall 0.889753
2017-12-10T11:52:36.767391: step 4614, loss 0.0164035, acc 1, prec 0.0897329, recall 0.889778
2017-12-10T11:52:37.212535: step 4615, loss 0.180834, acc 0.984375, prec 0.0897519, recall 0.889803
2017-12-10T11:52:37.662361: step 4616, loss 0.057921, acc 0.96875, prec 0.0897477, recall 0.889803
2017-12-10T11:52:38.104871: step 4617, loss 0.0946752, acc 0.984375, prec 0.0898296, recall 0.889904
2017-12-10T11:52:38.548786: step 4618, loss 0.020289, acc 1, prec 0.0898506, recall 0.889929
2017-12-10T11:52:38.995384: step 4619, loss 0.065198, acc 0.96875, prec 0.0899094, recall 0.890005
2017-12-10T11:52:39.421583: step 4620, loss 0.0584872, acc 0.984375, prec 0.0899493, recall 0.890055
2017-12-10T11:52:39.869504: step 4621, loss 0.228776, acc 0.9375, prec 0.089941, recall 0.890055
2017-12-10T11:52:40.314729: step 4622, loss 0.0512494, acc 0.984375, prec 0.0899389, recall 0.890055
2017-12-10T11:52:40.754175: step 4623, loss 0.0714295, acc 0.984375, prec 0.0899578, recall 0.89008
2017-12-10T11:52:41.200253: step 4624, loss 0.0156012, acc 1, prec 0.0899998, recall 0.89013
2017-12-10T11:52:41.643914: step 4625, loss 0.579015, acc 0.953125, prec 0.0899935, recall 0.89013
2017-12-10T11:52:42.092050: step 4626, loss 0.138737, acc 0.984375, prec 0.0899915, recall 0.89013
2017-12-10T11:52:42.538959: step 4627, loss 0.14522, acc 0.953125, prec 0.0900062, recall 0.890155
2017-12-10T11:52:42.987669: step 4628, loss 4.83037, acc 0.9375, prec 0.09, recall 0.889952
2017-12-10T11:52:43.443667: step 4629, loss 0.110962, acc 0.984375, prec 0.0900189, recall 0.889977
2017-12-10T11:52:43.890136: step 4630, loss 0.00219769, acc 1, prec 0.0900399, recall 0.890002
2017-12-10T11:52:44.339448: step 4631, loss 0.282446, acc 0.9375, prec 0.0900525, recall 0.890027
2017-12-10T11:52:44.785786: step 4632, loss 0.269041, acc 0.953125, prec 0.0901092, recall 0.890102
2017-12-10T11:52:45.229072: step 4633, loss 0.299287, acc 0.90625, prec 0.0901386, recall 0.890152
2017-12-10T11:52:45.683264: step 4634, loss 0.305523, acc 0.953125, prec 0.0901324, recall 0.890152
2017-12-10T11:52:46.140615: step 4635, loss 0.365229, acc 0.90625, prec 0.09012, recall 0.890152
2017-12-10T11:52:46.584223: step 4636, loss 0.057701, acc 0.96875, prec 0.0901368, recall 0.890177
2017-12-10T11:52:47.021052: step 4637, loss 0.544918, acc 0.90625, prec 0.0901662, recall 0.890227
2017-12-10T11:52:47.467577: step 4638, loss 0.398568, acc 0.90625, prec 0.0901537, recall 0.890227
2017-12-10T11:52:47.916210: step 4639, loss 0.191091, acc 0.953125, prec 0.0901894, recall 0.890277
2017-12-10T11:52:48.356814: step 4640, loss 0.0839338, acc 0.96875, prec 0.0902062, recall 0.890302
2017-12-10T11:52:48.813061: step 4641, loss 0.0653783, acc 1, prec 0.0902271, recall 0.890327
2017-12-10T11:52:49.257781: step 4642, loss 0.19969, acc 0.9375, prec 0.0902188, recall 0.890327
2017-12-10T11:52:49.695816: step 4643, loss 0.251024, acc 0.9375, prec 0.0902105, recall 0.890327
2017-12-10T11:52:50.153803: step 4644, loss 0.502454, acc 0.890625, prec 0.090196, recall 0.890327
2017-12-10T11:52:50.606569: step 4645, loss 0.117649, acc 0.953125, prec 0.0902107, recall 0.890352
2017-12-10T11:52:51.054330: step 4646, loss 0.729384, acc 0.921875, prec 0.0902422, recall 0.890402
2017-12-10T11:52:51.492514: step 4647, loss 0.219969, acc 0.9375, prec 0.0902757, recall 0.890451
2017-12-10T11:52:51.922996: step 4648, loss 0.28622, acc 0.90625, prec 0.0902632, recall 0.890451
2017-12-10T11:52:52.365286: step 4649, loss 0.16703, acc 0.9375, prec 0.0902759, recall 0.890476
2017-12-10T11:52:52.828642: step 4650, loss 0.131272, acc 0.9375, prec 0.0903094, recall 0.890526
2017-12-10T11:52:53.272690: step 4651, loss 0.0470381, acc 0.984375, prec 0.0903073, recall 0.890526
2017-12-10T11:52:53.715175: step 4652, loss 0.245222, acc 0.953125, prec 0.0903011, recall 0.890526
2017-12-10T11:52:54.172057: step 4653, loss 0.161244, acc 0.96875, prec 0.0903387, recall 0.890575
2017-12-10T11:52:54.624711: step 4654, loss 0.292629, acc 0.9375, prec 0.0903722, recall 0.890625
2017-12-10T11:52:55.052924: step 4655, loss 0.199212, acc 0.953125, prec 0.0903869, recall 0.89065
2017-12-10T11:52:55.499211: step 4656, loss 0.0794785, acc 0.9375, prec 0.0904204, recall 0.890699
2017-12-10T11:52:55.943677: step 4657, loss 0.0667126, acc 0.96875, prec 0.0904162, recall 0.890699
2017-12-10T11:52:56.384317: step 4658, loss 0.110486, acc 0.984375, prec 0.0904142, recall 0.890699
2017-12-10T11:52:56.830283: step 4659, loss 0.454709, acc 0.96875, prec 0.09041, recall 0.890699
2017-12-10T11:52:57.268854: step 4660, loss 0.205986, acc 0.953125, prec 0.0904247, recall 0.890724
2017-12-10T11:52:57.712602: step 4661, loss 0.0934382, acc 0.96875, prec 0.0904623, recall 0.890773
2017-12-10T11:52:58.163687: step 4662, loss 0.174863, acc 0.96875, prec 0.0904581, recall 0.890773
2017-12-10T11:52:58.611944: step 4663, loss 0.355447, acc 0.90625, prec 0.0904457, recall 0.890773
2017-12-10T11:52:59.055718: step 4664, loss 0.0828473, acc 0.96875, prec 0.0904415, recall 0.890773
2017-12-10T11:52:59.496163: step 4665, loss 0.0389311, acc 0.984375, prec 0.0904395, recall 0.890773
2017-12-10T11:52:59.938246: step 4666, loss 0.457581, acc 0.984375, prec 0.0904583, recall 0.890798
2017-12-10T11:53:00.387828: step 4667, loss 0.276788, acc 0.953125, prec 0.0904938, recall 0.890847
2017-12-10T11:53:00.826628: step 4668, loss 0.211817, acc 0.96875, prec 0.0905314, recall 0.890897
2017-12-10T11:53:01.269029: step 4669, loss 0.0114546, acc 1, prec 0.0905523, recall 0.890921
2017-12-10T11:53:01.724127: step 4670, loss 0.772915, acc 1, prec 0.0905731, recall 0.890946
2017-12-10T11:53:02.164391: step 4671, loss 0.070695, acc 0.96875, prec 0.0905899, recall 0.890971
2017-12-10T11:53:02.604304: step 4672, loss 0.192494, acc 0.984375, prec 0.0906086, recall 0.890995
2017-12-10T11:53:03.040065: step 4673, loss 0.100937, acc 0.984375, prec 0.0906692, recall 0.891069
2017-12-10T11:53:03.506665: step 4674, loss 0.0401142, acc 0.984375, prec 0.0906671, recall 0.891069
2017-12-10T11:53:03.954783: step 4675, loss 0.0704238, acc 0.984375, prec 0.090665, recall 0.891069
2017-12-10T11:53:04.413149: step 4676, loss 0.111387, acc 0.9375, prec 0.0906567, recall 0.891069
2017-12-10T11:53:04.853589: step 4677, loss 0.282099, acc 0.921875, prec 0.0906672, recall 0.891094
2017-12-10T11:53:05.308214: step 4678, loss 0.133745, acc 0.953125, prec 0.0906609, recall 0.891094
2017-12-10T11:53:05.760217: step 4679, loss 0.0513876, acc 0.984375, prec 0.0906797, recall 0.891118
2017-12-10T11:53:06.202548: step 4680, loss 0.417648, acc 0.96875, prec 0.0906964, recall 0.891143
2017-12-10T11:53:06.642155: step 4681, loss 0.160138, acc 0.953125, prec 0.0906902, recall 0.891143
2017-12-10T11:53:07.085192: step 4682, loss 0.106789, acc 0.96875, prec 0.0907277, recall 0.891192
2017-12-10T11:53:07.538771: step 4683, loss 0.189927, acc 0.875, prec 0.0907736, recall 0.891265
2017-12-10T11:53:07.978909: step 4684, loss 0.103074, acc 0.96875, prec 0.0907694, recall 0.891265
2017-12-10T11:53:08.421940: step 4685, loss 0.128064, acc 0.96875, prec 0.0907861, recall 0.89129
2017-12-10T11:53:08.873485: step 4686, loss 0.373427, acc 0.90625, prec 0.0907736, recall 0.89129
2017-12-10T11:53:09.321761: step 4687, loss 0.240498, acc 0.9375, prec 0.0907862, recall 0.891314
2017-12-10T11:53:09.760733: step 4688, loss 0.142583, acc 0.9375, prec 0.0908195, recall 0.891363
2017-12-10T11:53:10.196409: step 4689, loss 0.986398, acc 0.953125, prec 0.0908341, recall 0.891387
2017-12-10T11:53:10.659638: step 4690, loss 0.27252, acc 0.921875, prec 0.0908445, recall 0.891412
2017-12-10T11:53:11.107357: step 4691, loss 0.145915, acc 0.953125, prec 0.0908383, recall 0.891412
2017-12-10T11:53:11.558612: step 4692, loss 0.352536, acc 0.953125, prec 0.0908737, recall 0.891461
2017-12-10T11:53:12.005905: step 4693, loss 0.0930444, acc 0.953125, prec 0.0909091, recall 0.891509
2017-12-10T11:53:12.445168: step 4694, loss 0.105301, acc 0.96875, prec 0.0909049, recall 0.891509
2017-12-10T11:53:12.885186: step 4695, loss 0.225531, acc 0.90625, prec 0.0909341, recall 0.891558
2017-12-10T11:53:13.352075: step 4696, loss 0.43116, acc 0.953125, prec 0.0909278, recall 0.891558
2017-12-10T11:53:13.781710: step 4697, loss 1.49037, acc 0.875, prec 0.0909549, recall 0.891407
2017-12-10T11:53:14.223683: step 4698, loss 0.140236, acc 0.96875, prec 0.0909923, recall 0.891455
2017-12-10T11:53:14.665741: step 4699, loss 0.287127, acc 0.9375, prec 0.0910464, recall 0.891528
2017-12-10T11:53:15.112483: step 4700, loss 0.38225, acc 0.921875, prec 0.0910568, recall 0.891553
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4700

2017-12-10T11:53:17.270408: step 4701, loss 0.371652, acc 0.90625, prec 0.0910443, recall 0.891553
2017-12-10T11:53:17.721340: step 4702, loss 0.436327, acc 0.9375, prec 0.0910776, recall 0.891601
2017-12-10T11:53:18.169347: step 4703, loss 0.562901, acc 0.828125, prec 0.0910754, recall 0.891626
2017-12-10T11:53:18.617178: step 4704, loss 0.427076, acc 0.90625, prec 0.0910837, recall 0.89165
2017-12-10T11:53:19.063847: step 4705, loss 0.289356, acc 0.921875, prec 0.0910733, recall 0.89165
2017-12-10T11:53:19.506770: step 4706, loss 0.514779, acc 0.890625, prec 0.0910587, recall 0.89165
2017-12-10T11:53:19.965882: step 4707, loss 0.182645, acc 0.953125, prec 0.0910525, recall 0.89165
2017-12-10T11:53:20.411390: step 4708, loss 0.303473, acc 0.90625, prec 0.0910816, recall 0.891698
2017-12-10T11:53:20.858853: step 4709, loss 0.806833, acc 0.890625, prec 0.091067, recall 0.891698
2017-12-10T11:53:21.300751: step 4710, loss 0.224093, acc 0.921875, prec 0.0910773, recall 0.891723
2017-12-10T11:53:21.753379: step 4711, loss 0.210779, acc 0.953125, prec 0.0910919, recall 0.891747
2017-12-10T11:53:22.203323: step 4712, loss 0.187235, acc 0.953125, prec 0.0911064, recall 0.891771
2017-12-10T11:53:22.637399: step 4713, loss 0.0408059, acc 0.96875, prec 0.0911022, recall 0.891771
2017-12-10T11:53:23.080701: step 4714, loss 0.621432, acc 0.921875, prec 0.0911126, recall 0.891795
2017-12-10T11:53:23.524694: step 4715, loss 0.242641, acc 0.953125, prec 0.0911063, recall 0.891795
2017-12-10T11:53:23.991145: step 4716, loss 0.177183, acc 0.953125, prec 0.0911001, recall 0.891795
2017-12-10T11:53:24.443703: step 4717, loss 0.0775587, acc 0.984375, prec 0.0911188, recall 0.891819
2017-12-10T11:53:24.904467: step 4718, loss 0.0132512, acc 1, prec 0.0911188, recall 0.891819
2017-12-10T11:53:25.355148: step 4719, loss 0.0790179, acc 0.984375, prec 0.0911582, recall 0.891868
2017-12-10T11:53:25.802672: step 4720, loss 0.0558668, acc 0.953125, prec 0.091152, recall 0.891868
2017-12-10T11:53:26.249887: step 4721, loss 0.579843, acc 0.9375, prec 0.0911644, recall 0.891892
2017-12-10T11:53:26.700222: step 4722, loss 0.0793682, acc 0.984375, prec 0.0911623, recall 0.891892
2017-12-10T11:53:27.148059: step 4723, loss 0.162859, acc 0.953125, prec 0.0911976, recall 0.89194
2017-12-10T11:53:27.591257: step 4724, loss 0.0557373, acc 0.96875, prec 0.0911934, recall 0.89194
2017-12-10T11:53:28.044040: step 4725, loss 0.0472011, acc 0.984375, prec 0.091212, recall 0.891964
2017-12-10T11:53:28.493911: step 4726, loss 0.234323, acc 0.96875, prec 0.0912286, recall 0.891988
2017-12-10T11:53:28.933031: step 4727, loss 0.324237, acc 0.9375, prec 0.0912825, recall 0.892061
2017-12-10T11:53:29.380874: step 4728, loss 0.0172033, acc 0.984375, prec 0.0912804, recall 0.892061
2017-12-10T11:53:29.819095: step 4729, loss 0.0058104, acc 1, prec 0.0912804, recall 0.892061
2017-12-10T11:53:30.257530: step 4730, loss 0.0149718, acc 0.984375, prec 0.0912991, recall 0.892085
2017-12-10T11:53:30.693834: step 4731, loss 0.0549348, acc 0.953125, prec 0.0912928, recall 0.892085
2017-12-10T11:53:31.140919: step 4732, loss 0.0460538, acc 0.96875, prec 0.0913094, recall 0.892109
2017-12-10T11:53:31.590068: step 4733, loss 0.311466, acc 1, prec 0.0913716, recall 0.892181
2017-12-10T11:53:32.030355: step 4734, loss 0.23627, acc 0.984375, prec 0.0913902, recall 0.892205
2017-12-10T11:53:32.483989: step 4735, loss 0.205691, acc 0.96875, prec 0.0914483, recall 0.892277
2017-12-10T11:53:32.948407: step 4736, loss 0.0481729, acc 0.96875, prec 0.0914648, recall 0.892301
2017-12-10T11:53:33.404467: step 4737, loss 0.175117, acc 0.96875, prec 0.0915021, recall 0.892349
2017-12-10T11:53:33.866251: step 4738, loss 0.207412, acc 1, prec 0.0915435, recall 0.892397
2017-12-10T11:53:34.316679: step 4739, loss 0.0455319, acc 0.984375, prec 0.0915414, recall 0.892397
2017-12-10T11:53:34.769646: step 4740, loss 0.101515, acc 0.984375, prec 0.0915601, recall 0.892421
2017-12-10T11:53:35.215591: step 4741, loss 0.0202056, acc 1, prec 0.0915601, recall 0.892421
2017-12-10T11:53:35.658407: step 4742, loss 0.187926, acc 0.953125, prec 0.0915538, recall 0.892421
2017-12-10T11:53:36.110665: step 4743, loss 0.00659108, acc 1, prec 0.0915538, recall 0.892421
2017-12-10T11:53:36.559457: step 4744, loss 0.0594594, acc 0.984375, prec 0.0915517, recall 0.892421
2017-12-10T11:53:37.013690: step 4745, loss 0.154798, acc 0.9375, prec 0.0915641, recall 0.892444
2017-12-10T11:53:37.464743: step 4746, loss 0.0533468, acc 0.984375, prec 0.0915827, recall 0.892468
2017-12-10T11:53:37.912555: step 4747, loss 0.0421663, acc 0.96875, prec 0.0915785, recall 0.892468
2017-12-10T11:53:38.359363: step 4748, loss 0.821236, acc 0.984375, prec 0.0915971, recall 0.892492
2017-12-10T11:53:38.810528: step 4749, loss 0.17797, acc 0.984375, prec 0.0916365, recall 0.89254
2017-12-10T11:53:39.266200: step 4750, loss 0.107804, acc 0.96875, prec 0.0916323, recall 0.89254
2017-12-10T11:53:39.724208: step 4751, loss 0.122611, acc 0.984375, prec 0.0916509, recall 0.892564
2017-12-10T11:53:40.183919: step 4752, loss 0.130405, acc 0.96875, prec 0.0916674, recall 0.892588
2017-12-10T11:53:40.636412: step 4753, loss 0.359838, acc 0.96875, prec 0.0917047, recall 0.892635
2017-12-10T11:53:41.085089: step 4754, loss 0.0218686, acc 1, prec 0.0917047, recall 0.892635
2017-12-10T11:53:41.538875: step 4755, loss 7.59267, acc 0.953125, prec 0.0917419, recall 0.892485
2017-12-10T11:53:41.973769: step 4756, loss 0.0871667, acc 0.96875, prec 0.0917377, recall 0.892485
2017-12-10T11:53:42.429588: step 4757, loss 0.0238951, acc 1, prec 0.0917377, recall 0.892485
2017-12-10T11:53:42.877269: step 4758, loss 0.0757138, acc 0.953125, prec 0.0917314, recall 0.892485
2017-12-10T11:53:43.322047: step 4759, loss 0.30707, acc 0.9375, prec 0.0917231, recall 0.892485
2017-12-10T11:53:43.777379: step 4760, loss 0.325667, acc 0.859375, prec 0.0917456, recall 0.892533
2017-12-10T11:53:44.213414: step 4761, loss 0.446911, acc 0.875, prec 0.0917703, recall 0.89258
2017-12-10T11:53:44.660774: step 4762, loss 0.41569, acc 0.890625, prec 0.0917763, recall 0.892604
2017-12-10T11:53:45.104565: step 4763, loss 0.911059, acc 0.796875, prec 0.0917492, recall 0.892604
2017-12-10T11:53:45.548970: step 4764, loss 0.352429, acc 0.890625, prec 0.0917759, recall 0.892652
2017-12-10T11:53:45.986196: step 4765, loss 0.218375, acc 0.921875, prec 0.0918274, recall 0.892723
2017-12-10T11:53:46.426595: step 4766, loss 0.575385, acc 0.890625, prec 0.0918541, recall 0.89277
2017-12-10T11:53:46.868242: step 4767, loss 0.102592, acc 0.96875, prec 0.0918706, recall 0.892794
2017-12-10T11:53:47.316898: step 4768, loss 0.68249, acc 0.8125, prec 0.0918455, recall 0.892794
2017-12-10T11:53:47.770211: step 4769, loss 0.566381, acc 0.859375, prec 0.0918268, recall 0.892794
2017-12-10T11:53:48.221194: step 4770, loss 0.422073, acc 0.890625, prec 0.0918741, recall 0.892865
2017-12-10T11:53:48.666967: step 4771, loss 0.628749, acc 0.859375, prec 0.0918553, recall 0.892865
2017-12-10T11:53:49.110956: step 4772, loss 0.416749, acc 0.875, prec 0.0918799, recall 0.892912
2017-12-10T11:53:49.561338: step 4773, loss 0.315431, acc 0.890625, prec 0.0918859, recall 0.892936
2017-12-10T11:53:50.001636: step 4774, loss 0.335332, acc 0.921875, prec 0.0918754, recall 0.892936
2017-12-10T11:53:50.456682: step 4775, loss 0.455487, acc 0.875, prec 0.0918794, recall 0.89296
2017-12-10T11:53:50.896980: step 4776, loss 0.286842, acc 0.90625, prec 0.0919081, recall 0.893007
2017-12-10T11:53:51.346089: step 4777, loss 0.126959, acc 0.953125, prec 0.0919018, recall 0.893007
2017-12-10T11:53:51.785515: step 4778, loss 0.209056, acc 0.9375, prec 0.0919141, recall 0.89303
2017-12-10T11:53:52.225151: step 4779, loss 0.381164, acc 0.953125, prec 0.0919285, recall 0.893054
2017-12-10T11:53:52.677673: step 4780, loss 0.346435, acc 0.90625, prec 0.0919159, recall 0.893054
2017-12-10T11:53:53.113818: step 4781, loss 0.276923, acc 0.921875, prec 0.0919261, recall 0.893078
2017-12-10T11:53:53.558420: step 4782, loss 0.106173, acc 0.96875, prec 0.0919425, recall 0.893101
2017-12-10T11:53:53.990699: step 4783, loss 0.0592587, acc 0.96875, prec 0.0919384, recall 0.893101
2017-12-10T11:53:54.449370: step 4784, loss 0.236982, acc 0.953125, prec 0.0919527, recall 0.893125
2017-12-10T11:53:54.891593: step 4785, loss 0.077446, acc 0.984375, prec 0.0919918, recall 0.893172
2017-12-10T11:53:55.342139: step 4786, loss 0.0527221, acc 0.984375, prec 0.0920103, recall 0.893195
2017-12-10T11:53:55.787113: step 4787, loss 0.0501336, acc 0.96875, prec 0.0920268, recall 0.893219
2017-12-10T11:53:56.239914: step 4788, loss 0.142445, acc 0.984375, prec 0.0920247, recall 0.893219
2017-12-10T11:53:56.698694: step 4789, loss 0.0245499, acc 1, prec 0.0920659, recall 0.893266
2017-12-10T11:53:57.141604: step 4790, loss 0.0286327, acc 0.984375, prec 0.0920638, recall 0.893266
2017-12-10T11:53:57.588304: step 4791, loss 0.0256307, acc 0.984375, prec 0.0920617, recall 0.893266
2017-12-10T11:53:58.035998: step 4792, loss 0.139133, acc 0.96875, prec 0.0920575, recall 0.893266
2017-12-10T11:53:58.482954: step 4793, loss 0.120191, acc 0.96875, prec 0.0920739, recall 0.893289
2017-12-10T11:53:58.936736: step 4794, loss 0.00450886, acc 1, prec 0.0920739, recall 0.893289
2017-12-10T11:53:59.386755: step 4795, loss 0.00587304, acc 1, prec 0.0920945, recall 0.893313
2017-12-10T11:53:59.835143: step 4796, loss 0.172595, acc 0.953125, prec 0.0920883, recall 0.893313
2017-12-10T11:54:00.274722: step 4797, loss 0.000867054, acc 1, prec 0.0921088, recall 0.893336
2017-12-10T11:54:00.713806: step 4798, loss 0.00284241, acc 1, prec 0.0921088, recall 0.893336
2017-12-10T11:54:01.158968: step 4799, loss 0.192318, acc 0.953125, prec 0.0921026, recall 0.893336
2017-12-10T11:54:01.606762: step 4800, loss 0.124141, acc 0.984375, prec 0.0921211, recall 0.89336
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4800

2017-12-10T11:54:03.426344: step 4801, loss 0.000731597, acc 1, prec 0.0921211, recall 0.89336
2017-12-10T11:54:03.877535: step 4802, loss 0.0627933, acc 0.96875, prec 0.0921786, recall 0.89343
2017-12-10T11:54:04.325720: step 4803, loss 9.51033, acc 0.96875, prec 0.0921766, recall 0.893234
2017-12-10T11:54:04.786184: step 4804, loss 0.0120971, acc 1, prec 0.0921766, recall 0.893234
2017-12-10T11:54:05.262147: step 4805, loss 0.0155916, acc 0.984375, prec 0.0921745, recall 0.893234
2017-12-10T11:54:05.720530: step 4806, loss 0.135549, acc 0.984375, prec 0.0922135, recall 0.893281
2017-12-10T11:54:06.169915: step 4807, loss 0.00587479, acc 1, prec 0.0922341, recall 0.893304
2017-12-10T11:54:06.617489: step 4808, loss 0.107841, acc 0.96875, prec 0.0922505, recall 0.893327
2017-12-10T11:54:07.062091: step 4809, loss 0.166682, acc 0.96875, prec 0.0922875, recall 0.893374
2017-12-10T11:54:07.504544: step 4810, loss 0.0963212, acc 0.953125, prec 0.0923018, recall 0.893398
2017-12-10T11:54:07.943495: step 4811, loss 0.749468, acc 0.9375, prec 0.092314, recall 0.893421
2017-12-10T11:54:08.395459: step 4812, loss 0.0156888, acc 1, prec 0.0923345, recall 0.893444
2017-12-10T11:54:08.849121: step 4813, loss 0.0541877, acc 0.953125, prec 0.0923694, recall 0.893491
2017-12-10T11:54:09.290074: step 4814, loss 0.33894, acc 0.890625, prec 0.0923547, recall 0.893491
2017-12-10T11:54:09.736131: step 4815, loss 0.31143, acc 0.921875, prec 0.0923648, recall 0.893514
2017-12-10T11:54:10.183303: step 4816, loss 0.100926, acc 0.96875, prec 0.0924018, recall 0.893561
2017-12-10T11:54:10.634666: step 4817, loss 0.389662, acc 0.890625, prec 0.0924282, recall 0.893608
2017-12-10T11:54:11.080424: step 4818, loss 0.263601, acc 0.9375, prec 0.0924404, recall 0.893631
2017-12-10T11:54:11.534167: step 4819, loss 0.390486, acc 0.921875, prec 0.092471, recall 0.893678
2017-12-10T11:54:11.973070: step 4820, loss 0.0870641, acc 0.96875, prec 0.0924874, recall 0.893701
2017-12-10T11:54:12.407614: step 4821, loss 0.370083, acc 0.953125, prec 0.0925222, recall 0.893747
2017-12-10T11:54:12.851162: step 4822, loss 0.404148, acc 0.953125, prec 0.0925364, recall 0.893771
2017-12-10T11:54:13.304060: step 4823, loss 0.311445, acc 0.96875, prec 0.0925528, recall 0.893794
2017-12-10T11:54:13.758039: step 4824, loss 0.379552, acc 0.9375, prec 0.0925855, recall 0.89384
2017-12-10T11:54:14.188911: step 4825, loss 0.0151607, acc 1, prec 0.0925855, recall 0.89384
2017-12-10T11:54:14.645000: step 4826, loss 0.342697, acc 0.921875, prec 0.092575, recall 0.89384
2017-12-10T11:54:15.088474: step 4827, loss 0.262001, acc 0.953125, prec 0.0925687, recall 0.89384
2017-12-10T11:54:15.547919: step 4828, loss 0.309345, acc 0.9375, prec 0.0925809, recall 0.893863
2017-12-10T11:54:15.991645: step 4829, loss 0.0377348, acc 0.96875, prec 0.0925767, recall 0.893863
2017-12-10T11:54:16.437435: step 4830, loss 0.258649, acc 0.953125, prec 0.0925909, recall 0.893886
2017-12-10T11:54:16.872101: step 4831, loss 3.07624, acc 0.953125, prec 0.0926073, recall 0.893715
2017-12-10T11:54:17.326675: step 4832, loss 0.243881, acc 0.890625, prec 0.0926131, recall 0.893738
2017-12-10T11:54:17.775721: step 4833, loss 0.265416, acc 0.96875, prec 0.0926089, recall 0.893738
2017-12-10T11:54:18.213041: step 4834, loss 0.0760477, acc 0.953125, prec 0.0926026, recall 0.893738
2017-12-10T11:54:18.658042: step 4835, loss 0.378646, acc 0.890625, prec 0.0926085, recall 0.893761
2017-12-10T11:54:19.115109: step 4836, loss 0.549004, acc 0.875, prec 0.0926328, recall 0.893807
2017-12-10T11:54:19.564745: step 4837, loss 0.425625, acc 0.90625, prec 0.0926612, recall 0.893854
2017-12-10T11:54:20.022103: step 4838, loss 0.110199, acc 0.96875, prec 0.0926775, recall 0.893877
2017-12-10T11:54:20.477604: step 4839, loss 0.328332, acc 0.90625, prec 0.0927265, recall 0.893946
2017-12-10T11:54:20.924390: step 4840, loss 0.447327, acc 0.90625, prec 0.0927549, recall 0.893992
2017-12-10T11:54:21.371139: step 4841, loss 0.775405, acc 0.859375, prec 0.0927565, recall 0.894015
2017-12-10T11:54:21.815727: step 4842, loss 0.310216, acc 0.921875, prec 0.092787, recall 0.894061
2017-12-10T11:54:22.255882: step 4843, loss 0.343384, acc 0.9375, prec 0.0928401, recall 0.89413
2017-12-10T11:54:22.706425: step 4844, loss 0.259972, acc 0.9375, prec 0.0928317, recall 0.89413
2017-12-10T11:54:23.149502: step 4845, loss 0.0765932, acc 0.96875, prec 0.0928684, recall 0.894176
2017-12-10T11:54:23.586075: step 4846, loss 0.095063, acc 0.96875, prec 0.0928847, recall 0.894199
2017-12-10T11:54:24.032643: step 4847, loss 0.108062, acc 0.96875, prec 0.0929215, recall 0.894245
2017-12-10T11:54:24.465880: step 4848, loss 0.195946, acc 0.96875, prec 0.0929173, recall 0.894245
2017-12-10T11:54:24.936003: step 4849, loss 0.16733, acc 0.953125, prec 0.0929519, recall 0.894291
2017-12-10T11:54:25.374650: step 4850, loss 0.199992, acc 1, prec 0.0929928, recall 0.894337
2017-12-10T11:54:25.817567: step 4851, loss 0.108468, acc 0.953125, prec 0.093007, recall 0.89436
2017-12-10T11:54:26.266053: step 4852, loss 0.173381, acc 0.96875, prec 0.0930437, recall 0.894406
2017-12-10T11:54:26.719987: step 4853, loss 0.0647466, acc 0.96875, prec 0.0930395, recall 0.894406
2017-12-10T11:54:27.167747: step 4854, loss 0.317665, acc 0.953125, prec 0.0930741, recall 0.894452
2017-12-10T11:54:27.620761: step 4855, loss 0.191306, acc 0.984375, prec 0.093072, recall 0.894452
2017-12-10T11:54:28.056755: step 4856, loss 0.291329, acc 0.921875, prec 0.093082, recall 0.894475
2017-12-10T11:54:28.511556: step 4857, loss 0.0598476, acc 0.984375, prec 0.0930799, recall 0.894475
2017-12-10T11:54:28.944960: step 4858, loss 0.0394517, acc 0.984375, prec 0.0930778, recall 0.894475
2017-12-10T11:54:29.391898: step 4859, loss 0.0256942, acc 0.984375, prec 0.0930757, recall 0.894475
2017-12-10T11:54:29.843882: step 4860, loss 0.445984, acc 0.984375, prec 0.093094, recall 0.894497
2017-12-10T11:54:30.295721: step 4861, loss 0.0496277, acc 0.984375, prec 0.0931124, recall 0.89452
2017-12-10T11:54:30.723907: step 4862, loss 0.0415747, acc 0.984375, prec 0.0931103, recall 0.89452
2017-12-10T11:54:31.165159: step 4863, loss 0.0116013, acc 1, prec 0.0931103, recall 0.89452
2017-12-10T11:54:31.617622: step 4864, loss 0.294408, acc 0.96875, prec 0.0931265, recall 0.894543
2017-12-10T11:54:32.060513: step 4865, loss 0.216847, acc 0.96875, prec 0.0931223, recall 0.894543
2017-12-10T11:54:32.511554: step 4866, loss 0.0285519, acc 0.984375, prec 0.0931202, recall 0.894543
2017-12-10T11:54:32.967843: step 4867, loss 0.0323165, acc 0.984375, prec 0.0931181, recall 0.894543
2017-12-10T11:54:33.410233: step 4868, loss 0.319181, acc 0.953125, prec 0.0931118, recall 0.894543
2017-12-10T11:54:33.850421: step 4869, loss 0.243694, acc 0.984375, prec 0.0931302, recall 0.894566
2017-12-10T11:54:34.301685: step 4870, loss 0.0302014, acc 1, prec 0.0931506, recall 0.894589
2017-12-10T11:54:34.736353: step 4871, loss 0.100682, acc 1, prec 0.0931711, recall 0.894612
2017-12-10T11:54:35.176321: step 4872, loss 0.0218958, acc 0.984375, prec 0.0931894, recall 0.894634
2017-12-10T11:54:35.620961: step 4873, loss 0.0725893, acc 1, prec 0.0932098, recall 0.894657
2017-12-10T11:54:36.069113: step 4874, loss 0.0089468, acc 1, prec 0.0932098, recall 0.894657
2017-12-10T11:54:36.517096: step 4875, loss 0.0233486, acc 0.984375, prec 0.0932282, recall 0.89468
2017-12-10T11:54:36.961339: step 4876, loss 0.347732, acc 0.9375, prec 0.0932402, recall 0.894703
2017-12-10T11:54:37.419104: step 4877, loss 0.0256896, acc 0.984375, prec 0.0932585, recall 0.894725
2017-12-10T11:54:37.873485: step 4878, loss 0.0629618, acc 0.984375, prec 0.0932564, recall 0.894725
2017-12-10T11:54:38.314362: step 4879, loss 0.0876789, acc 0.953125, prec 0.0932501, recall 0.894725
2017-12-10T11:54:38.769402: step 4880, loss 0.0438575, acc 0.984375, prec 0.0932889, recall 0.894771
2017-12-10T11:54:39.218408: step 4881, loss 0.0966221, acc 0.96875, prec 0.0932847, recall 0.894771
2017-12-10T11:54:39.674061: step 4882, loss 0.159154, acc 0.96875, prec 0.0932805, recall 0.894771
2017-12-10T11:54:40.118881: step 4883, loss 0.0451212, acc 0.984375, prec 0.0933192, recall 0.894816
2017-12-10T11:54:40.566280: step 4884, loss 0.0483151, acc 0.984375, prec 0.0933171, recall 0.894816
2017-12-10T11:54:41.001305: step 4885, loss 0.0558744, acc 0.984375, prec 0.093315, recall 0.894816
2017-12-10T11:54:41.448228: step 4886, loss 0.0466743, acc 0.96875, prec 0.0933312, recall 0.894839
2017-12-10T11:54:41.901130: step 4887, loss 0.0677914, acc 1, prec 0.0933721, recall 0.894885
2017-12-10T11:54:42.345760: step 4888, loss 0.121964, acc 0.953125, prec 0.0934066, recall 0.89493
2017-12-10T11:54:42.787204: step 4889, loss 0.00573839, acc 1, prec 0.0934066, recall 0.89493
2017-12-10T11:54:43.230817: step 4890, loss 0.046035, acc 0.96875, prec 0.0934024, recall 0.89493
2017-12-10T11:54:43.694252: step 4891, loss 0.01042, acc 1, prec 0.0934024, recall 0.89493
2017-12-10T11:54:44.159907: step 4892, loss 0.00685491, acc 1, prec 0.0934024, recall 0.89493
2017-12-10T11:54:44.607698: step 4893, loss 0.168537, acc 0.984375, prec 0.0934411, recall 0.894975
2017-12-10T11:54:45.070796: step 4894, loss 0.930971, acc 1, prec 0.0934615, recall 0.894998
2017-12-10T11:54:45.529130: step 4895, loss 0.0379353, acc 0.984375, prec 0.0934594, recall 0.894998
2017-12-10T11:54:45.983137: step 4896, loss 0.436286, acc 0.953125, prec 0.0934735, recall 0.89502
2017-12-10T11:54:46.441409: step 4897, loss 0.0346536, acc 0.984375, prec 0.0934714, recall 0.89502
2017-12-10T11:54:46.880694: step 4898, loss 0.243248, acc 0.96875, prec 0.0934672, recall 0.89502
2017-12-10T11:54:47.324140: step 4899, loss 0.405604, acc 0.921875, prec 0.0934771, recall 0.895043
2017-12-10T11:54:47.770291: step 4900, loss 0.128026, acc 0.953125, prec 0.0934708, recall 0.895043
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_0/1512922633/checkpoints/model-4900

2017-12-10T11:54:49.635839: step 4901, loss 0.119578, acc 0.953125, prec 0.0934645, recall 0.895043
2017-12-10T11:54:50.093087: step 4902, loss 0.21912, acc 0.90625, prec 0.0934722, recall 0.895066
2017-12-10T11:54:50.534212: step 4903, loss 0.161865, acc 0.953125, prec 0.0934863, recall 0.895088
2017-12-10T11:54:50.983869: step 4904, loss 0.438839, acc 0.9375, prec 0.0934983, recall 0.895111
2017-12-10T11:54:51.423307: step 4905, loss 0.290157, acc 0.921875, prec 0.0935082, recall 0.895133
2017-12-10T11:54:51.868451: step 4906, loss 0.458183, acc 0.90625, prec 0.0934956, recall 0.895133
2017-12-10T11:54:52.314588: step 4907, loss 0.199524, acc 0.921875, prec 0.0934851, recall 0.895133
2017-12-10T11:54:52.751592: step 4908, loss 0.612413, acc 0.953125, prec 0.0935195, recall 0.895179
2017-12-10T11:54:53.201394: step 4909, loss 0.47339, acc 0.921875, prec 0.0935498, recall 0.895224
2017-12-10T11:54:53.654309: step 4910, loss 0.192291, acc 0.953125, prec 0.0935435, recall 0.895224
2017-12-10T11:54:54.099828: step 4911, loss 0.0635146, acc 0.96875, prec 0.0935393, recall 0.895224
2017-12-10T11:54:54.544407: step 4912, loss 0.0391843, acc 0.984375, prec 0.0935371, recall 0.895224
2017-12-10T11:54:54.990119: step 4913, loss 0.140977, acc 0.9375, prec 0.0935287, recall 0.895224
2017-12-10T11:54:55.410909: step 4914, loss 0.118612, acc 0.953125, prec 0.0935632, recall 0.895269
2017-12-10T11:54:55.865715: step 4915, loss 0.126113, acc 0.96875, prec 0.0935793, recall 0.895291
2017-12-10T11:54:56.311026: step 4916, loss 0.167676, acc 0.921875, prec 0.0935688, recall 0.895291
2017-12-10T11:54:56.769318: step 4917, loss 0.283583, acc 0.9375, prec 0.0936215, recall 0.895359
2017-12-10T11:54:57.228157: step 4918, loss 0.137295, acc 0.9375, prec 0.0936335, recall 0.895381
2017-12-10T11:54:57.672765: step 4919, loss 0.198343, acc 0.921875, prec 0.0936433, recall 0.895404
2017-12-10T11:54:58.131198: step 4920, loss 0.104721, acc 0.96875, prec 0.0936595, recall 0.895426
2017-12-10T11:54:58.575592: step 4921, loss 0.15924, acc 0.9375, prec 0.0936714, recall 0.895449
2017-12-10T11:54:59.019962: step 4922, loss 0.296583, acc 0.953125, prec 0.0937058, recall 0.895494
2017-12-10T11:54:59.473656: step 4923, loss 0.097254, acc 1, prec 0.0937261, recall 0.895516
2017-12-10T11:54:59.919076: step 4924, loss 0.0625377, acc 0.984375, prec 0.0937444, recall 0.895538
2017-12-10T11:55:00.362934: step 4925, loss 0.168347, acc 0.953125, prec 0.0937381, recall 0.895538
2017-12-10T11:55:00.810125: step 4926, loss 0.148678, acc 0.953125, prec 0.0937521, recall 0.895561
2017-12-10T11:55:01.258006: step 4927, loss 0.305457, acc 0.9375, prec 0.093764, recall 0.895583
2017-12-10T11:55:01.705440: step 4928, loss 3.08282, acc 0.984375, prec 0.0937844, recall 0.895414
2017-12-10T11:55:02.146482: step 4929, loss 0.0361342, acc 0.984375, prec 0.0938026, recall 0.895436
2017-12-10T11:55:02.584877: step 4930, loss 0.107473, acc 0.953125, prec 0.0937963, recall 0.895436
2017-12-10T11:55:03.029631: step 4931, loss 0.23041, acc 1, prec 0.093837, recall 0.895481
2017-12-10T11:55:03.475336: step 4932, loss 0.263669, acc 0.921875, prec 0.0938468, recall 0.895503
2017-12-10T11:55:03.922453: step 4933, loss 0.183584, acc 0.921875, prec 0.0938566, recall 0.895526
2017-12-10T11:55:04.377070: step 4934, loss 0.204956, acc 0.9375, prec 0.0938482, recall 0.895526
2017-12-10T11:55:04.815952: step 4935, loss 1.30404, acc 0.9375, prec 0.0938804, recall 0.89557
2017-12-10T11:55:05.259013: step 4936, loss 0.152651, acc 0.953125, prec 0.0938741, recall 0.89557
2017-12-10T11:55:05.705200: step 4937, loss 0.448356, acc 0.890625, prec 0.0938797, recall 0.895593
2017-12-10T11:55:06.149012: step 4938, loss 0.457186, acc 0.90625, prec 0.093867, recall 0.895593
2017-12-10T11:55:06.589065: step 4939, loss 0.411422, acc 0.859375, prec 0.0938684, recall 0.895615
2017-12-10T11:55:07.022104: step 4940, loss 0.643716, acc 0.8125, prec 0.0938635, recall 0.895637
2017-12-10T11:55:07.467845: step 4941, loss 0.506202, acc 0.84375, prec 0.0938627, recall 0.89566
2017-12-10T11:55:07.914392: step 4942, loss 0.415182, acc 0.875, prec 0.0938662, recall 0.895682
2017-12-10T11:55:08.355122: step 4943, loss 0.188643, acc 0.9375, prec 0.0938578, recall 0.895682
2017-12-10T11:55:08.793993: step 4944, loss 0.279192, acc 0.90625, prec 0.0938655, recall 0.895704
2017-12-10T11:55:09.235678: step 4945, loss 0.539318, acc 0.90625, prec 0.0938529, recall 0.895704
2017-12-10T11:55:09.686219: step 4946, loss 0.310247, acc 0.859375, prec 0.093834, recall 0.895704
2017-12-10T11:55:10.123374: step 4947, loss 0.398984, acc 0.90625, prec 0.0938214, recall 0.895704
2017-12-10T11:55:10.571322: step 4948, loss 0.153549, acc 0.984375, prec 0.0938598, recall 0.895749
2017-12-10T11:55:11.033825: step 4949, loss 0.358129, acc 0.90625, prec 0.0939081, recall 0.895816
2017-12-10T11:55:11.471099: step 4950, loss 0.22298, acc 0.953125, prec 0.093922, recall 0.895838
2017-12-10T11:55:11.912762: step 4951, loss 0.275341, acc 0.9375, prec 0.0939542, recall 0.895882
2017-12-10T11:55:12.348626: step 4952, loss 0.69508, acc 0.890625, prec 0.0939597, recall 0.895904
2017-12-10T11:55:12.798585: step 4953, loss 0.284574, acc 0.921875, prec 0.0939898, recall 0.895949
2017-12-10T11:55:13.236071: step 4954, loss 0.410809, acc 0.921875, prec 0.0939995, recall 0.895971
2017-12-10T11:55:13.695229: step 4955, loss 0.224317, acc 0.9375, prec 0.0940114, recall 0.895993
2017-12-10T11:55:14.136182: step 4956, loss 0.709407, acc 0.90625, prec 0.0940595, recall 0.89606
2017-12-10T11:55:14.583706: step 4957, loss 0.0395929, acc 1, prec 0.0941, recall 0.896104
2017-12-10T11:55:15.035349: step 4958, loss 0.0610201, acc 0.984375, prec 0.0941182, recall 0.896126
2017-12-10T11:55:15.482429: step 4959, loss 0.194215, acc 0.953125, prec 0.0941321, recall 0.896148
2017-12-10T11:55:15.929078: step 4960, loss 0.0394525, acc 0.984375, prec 0.09413, recall 0.896148
2017-12-10T11:55:16.385281: step 4961, loss 0.060387, acc 0.984375, prec 0.0941684, recall 0.896192
2017-12-10T11:55:16.831936: step 4962, loss 0.120005, acc 0.953125, prec 0.0941823, recall 0.896214
2017-12-10T11:55:17.275885: step 4963, loss 0.0281869, acc 0.96875, prec 0.0941781, recall 0.896214
2017-12-10T11:55:17.731553: step 4964, loss 0.0851831, acc 0.984375, prec 0.0941963, recall 0.896236
2017-12-10T11:55:18.179927: step 4965, loss 0.203417, acc 0.96875, prec 0.0941921, recall 0.896236
2017-12-10T11:55:18.633820: step 4966, loss 0.0507277, acc 0.96875, prec 0.0942081, recall 0.896259
2017-12-10T11:55:19.083160: step 4967, loss 0.0978349, acc 0.96875, prec 0.0942039, recall 0.896259
2017-12-10T11:55:19.522667: step 4968, loss 0.175269, acc 0.96875, prec 0.0941997, recall 0.896259
2017-12-10T11:55:19.960734: step 4969, loss 0.0270043, acc 0.984375, prec 0.0941976, recall 0.896259
2017-12-10T11:55:20.362265: step 4970, loss 0.115231, acc 0.980392, prec 0.0941954, recall 0.896259
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 256
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR embedding_size_256_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921

Start training
2017-12-10T11:55:25.587321: step 1, loss 3.87777, acc 0.53125, prec 0, recall 0
2017-12-10T11:55:26.029530: step 2, loss 16.8167, acc 0.796875, prec 0, recall 0
2017-12-10T11:55:26.477714: step 3, loss 18.7605, acc 0.5625, prec 0, recall 0
2017-12-10T11:55:26.917787: step 4, loss 8.38335, acc 0.59375, prec 0, recall 0
2017-12-10T11:55:27.357449: step 5, loss 21.3756, acc 0.46875, prec 0.00793651, recall 0.142857
2017-12-10T11:55:27.792990: step 6, loss 5.77393, acc 0.296875, prec 0.00584795, recall 0.142857
2017-12-10T11:55:28.231100: step 7, loss 6.40518, acc 0.234375, prec 0.00454545, recall 0.142857
2017-12-10T11:55:28.670911: step 8, loss 8.75764, acc 0.078125, prec 0.00358423, recall 0.142857
2017-12-10T11:55:29.108119: step 9, loss 8.28263, acc 0.125, prec 0.00595238, recall 0.25
2017-12-10T11:55:29.549756: step 10, loss 8.37408, acc 0.15625, prec 0.00512821, recall 0.25
2017-12-10T11:55:29.996926: step 11, loss 7.98788, acc 0.1875, prec 0.00452489, recall 0.25
2017-12-10T11:55:30.437055: step 12, loss 12.7655, acc 0.3125, prec 0.00821355, recall 0.363636
2017-12-10T11:55:30.877254: step 13, loss 6.72597, acc 0.15625, prec 0.00922509, recall 0.416667
2017-12-10T11:55:31.318184: step 14, loss 5.74644, acc 0.234375, prec 0.0101351, recall 0.461538
2017-12-10T11:55:31.759904: step 15, loss 7.58775, acc 0.34375, prec 0.011041, recall 0.466667
2017-12-10T11:55:32.186198: step 16, loss 3.95189, acc 0.40625, prec 0.0118871, recall 0.5
2017-12-10T11:55:32.630529: step 17, loss 3.94931, acc 0.40625, prec 0.0112518, recall 0.5
2017-12-10T11:55:33.081632: step 18, loss 22.9203, acc 0.46875, prec 0.0120968, recall 0.473684
2017-12-10T11:55:33.521836: step 19, loss 2.87596, acc 0.484375, prec 0.0128535, recall 0.5
2017-12-10T11:55:33.955760: step 20, loss 2.49888, acc 0.65625, prec 0.0125, recall 0.5
2017-12-10T11:55:34.387748: step 21, loss 3.39788, acc 0.625, prec 0.0133333, recall 0.52381
2017-12-10T11:55:34.819365: step 22, loss 2.48429, acc 0.578125, prec 0.0129108, recall 0.52381
2017-12-10T11:55:35.249477: step 23, loss 1.99556, acc 0.625, prec 0.0125571, recall 0.52381
2017-12-10T11:55:35.686217: step 24, loss 9.39997, acc 0.640625, prec 0.0122494, recall 0.5
2017-12-10T11:55:36.131669: step 25, loss 18.2184, acc 0.609375, prec 0.0130011, recall 0.5
2017-12-10T11:55:36.581631: step 26, loss 8.15802, acc 0.59375, prec 0.0126582, recall 0.48
2017-12-10T11:55:37.028790: step 27, loss 2.23319, acc 0.671875, prec 0.0134021, recall 0.5
2017-12-10T11:55:37.476950: step 28, loss 2.86409, acc 0.53125, prec 0.013, recall 0.5
2017-12-10T11:55:37.926722: step 29, loss 26.325, acc 0.5, prec 0.0126091, recall 0.481481
2017-12-10T11:55:38.371668: step 30, loss 3.09554, acc 0.546875, prec 0.0122642, recall 0.481481
2017-12-10T11:55:38.812034: step 31, loss 3.06732, acc 0.5, prec 0.0128088, recall 0.5
2017-12-10T11:55:39.254799: step 32, loss 2.80321, acc 0.5625, prec 0.0142476, recall 0.533333
2017-12-10T11:55:39.692713: step 33, loss 3.8772, acc 0.40625, prec 0.0146299, recall 0.548387
2017-12-10T11:55:40.130046: step 34, loss 4.20535, acc 0.4375, prec 0.0141903, recall 0.548387
2017-12-10T11:55:40.560784: step 35, loss 7.12155, acc 0.5, prec 0.0138324, recall 0.53125
2017-12-10T11:55:40.995964: step 36, loss 3.83381, acc 0.53125, prec 0.0142971, recall 0.529412
2017-12-10T11:55:41.434911: step 37, loss 3.75812, acc 0.375, prec 0.0138568, recall 0.529412
2017-12-10T11:55:41.875831: step 38, loss 4.056, acc 0.46875, prec 0.0142429, recall 0.542857
2017-12-10T11:55:42.322794: step 39, loss 20.6897, acc 0.453125, prec 0.0146092, recall 0.540541
2017-12-10T11:55:42.764478: step 40, loss 3.90653, acc 0.4375, prec 0.014936, recall 0.552632
2017-12-10T11:55:43.194160: step 41, loss 2.81262, acc 0.59375, prec 0.0160391, recall 0.575
2017-12-10T11:55:43.608654: step 42, loss 2.49837, acc 0.515625, prec 0.0163711, recall 0.585366
2017-12-10T11:55:44.049977: step 43, loss 1.86195, acc 0.640625, prec 0.0161182, recall 0.585366
2017-12-10T11:55:44.493030: step 44, loss 3.58315, acc 0.546875, prec 0.0158103, recall 0.585366
2017-12-10T11:55:44.945003: step 45, loss 9.71952, acc 0.6875, prec 0.0156148, recall 0.571429
2017-12-10T11:55:45.395528: step 46, loss 17.7333, acc 0.6875, prec 0.0166881, recall 0.577778
2017-12-10T11:55:45.850162: step 47, loss 7.96981, acc 0.640625, prec 0.0170778, recall 0.574468
2017-12-10T11:55:46.299169: step 48, loss 3.42526, acc 0.4375, prec 0.0166976, recall 0.574468
2017-12-10T11:55:46.733204: step 49, loss 2.65139, acc 0.5, prec 0.0163736, recall 0.574468
2017-12-10T11:55:47.165245: step 50, loss 3.65124, acc 0.5625, prec 0.0166865, recall 0.583333
2017-12-10T11:55:47.610979: step 51, loss 2.87286, acc 0.5, prec 0.0163743, recall 0.583333
2017-12-10T11:55:48.053993: step 52, loss 5.38369, acc 0.421875, prec 0.0165999, recall 0.58
2017-12-10T11:55:48.507612: step 53, loss 4.05134, acc 0.53125, prec 0.0174255, recall 0.596154
2017-12-10T11:55:48.972762: step 54, loss 4.85012, acc 0.46875, prec 0.0176406, recall 0.603774
2017-12-10T11:55:49.423371: step 55, loss 3.19306, acc 0.53125, prec 0.0184182, recall 0.618182
2017-12-10T11:55:49.879260: step 56, loss 3.2245, acc 0.5625, prec 0.0186667, recall 0.625
2017-12-10T11:55:50.315556: step 57, loss 2.418, acc 0.578125, prec 0.0189175, recall 0.631579
2017-12-10T11:55:50.763610: step 58, loss 2.63674, acc 0.65625, prec 0.0187013, recall 0.631579
2017-12-10T11:55:51.200334: step 59, loss 8.01916, acc 0.5625, prec 0.0184521, recall 0.610169
2017-12-10T11:55:51.639755: step 60, loss 3.56469, acc 0.5625, prec 0.0186869, recall 0.616667
2017-12-10T11:55:52.081982: step 61, loss 3.10527, acc 0.53125, prec 0.0193837, recall 0.629032
2017-12-10T11:55:52.530828: step 62, loss 15.9109, acc 0.421875, prec 0.0195217, recall 0.625
2017-12-10T11:55:52.987946: step 63, loss 3.53219, acc 0.46875, prec 0.0201439, recall 0.636364
2017-12-10T11:55:53.433620: step 64, loss 3.80606, acc 0.4375, prec 0.0202639, recall 0.641791
2017-12-10T11:55:53.883990: step 65, loss 3.60278, acc 0.53125, prec 0.0213457, recall 0.657143
2017-12-10T11:55:54.318757: step 66, loss 3.20068, acc 0.515625, prec 0.0228311, recall 0.675676
2017-12-10T11:55:54.763810: step 67, loss 4.00241, acc 0.46875, prec 0.0229213, recall 0.68
2017-12-10T11:55:55.201729: step 68, loss 2.64459, acc 0.59375, prec 0.0226566, recall 0.68
2017-12-10T11:55:55.643901: step 69, loss 3.51899, acc 0.578125, prec 0.0236738, recall 0.692308
2017-12-10T11:55:56.076041: step 70, loss 3.21079, acc 0.59375, prec 0.023407, recall 0.692308
2017-12-10T11:55:56.520980: step 71, loss 2.23577, acc 0.625, prec 0.0235849, recall 0.696203
2017-12-10T11:55:56.973614: step 72, loss 1.93611, acc 0.625, prec 0.0233447, recall 0.696203
2017-12-10T11:55:57.442568: step 73, loss 2.5747, acc 0.578125, prec 0.0230802, recall 0.696203
2017-12-10T11:55:57.886164: step 74, loss 2.25418, acc 0.703125, prec 0.0237105, recall 0.703704
2017-12-10T11:55:58.328243: step 75, loss 1.47724, acc 0.6875, prec 0.0235149, recall 0.703704
2017-12-10T11:55:58.769474: step 76, loss 1.06317, acc 0.859375, prec 0.0238291, recall 0.707317
2017-12-10T11:55:59.218799: step 77, loss 7.18822, acc 0.796875, prec 0.0237122, recall 0.698795
2017-12-10T11:55:59.659718: step 78, loss 1.07329, acc 0.84375, prec 0.0236156, recall 0.698795
2017-12-10T11:56:00.103640: step 79, loss 4.87174, acc 0.703125, prec 0.0234438, recall 0.690476
2017-12-10T11:56:00.544040: step 80, loss 11.0948, acc 0.796875, prec 0.0233307, recall 0.682353
2017-12-10T11:56:00.985763: step 81, loss 1.09775, acc 0.765625, prec 0.0231907, recall 0.682353
2017-12-10T11:56:01.427336: step 82, loss 16.7839, acc 0.734375, prec 0.0230525, recall 0.666667
2017-12-10T11:56:01.886565: step 83, loss 2.9023, acc 0.734375, prec 0.0244383, recall 0.681319
2017-12-10T11:56:02.333404: step 84, loss 2.37286, acc 0.6875, prec 0.0250098, recall 0.688172
2017-12-10T11:56:02.766045: step 85, loss 4.38217, acc 0.640625, prec 0.0247966, recall 0.680851
2017-12-10T11:56:03.215273: step 86, loss 3.73817, acc 0.484375, prec 0.0244836, recall 0.680851
2017-12-10T11:56:03.656171: step 87, loss 5.09334, acc 0.390625, prec 0.0244913, recall 0.684211
2017-12-10T11:56:04.105287: step 88, loss 5.43079, acc 0.40625, prec 0.024508, recall 0.6875
2017-12-10T11:56:04.542984: step 89, loss 4.58636, acc 0.40625, prec 0.024167, recall 0.6875
2017-12-10T11:56:04.985128: step 90, loss 4.43974, acc 0.40625, prec 0.0238353, recall 0.6875
2017-12-10T11:56:05.426652: step 91, loss 10.6138, acc 0.3125, prec 0.0234708, recall 0.680412
2017-12-10T11:56:05.884288: step 92, loss 10.6572, acc 0.296875, prec 0.0231173, recall 0.666667
2017-12-10T11:56:06.326033: step 93, loss 4.97246, acc 0.421875, prec 0.0234969, recall 0.673267
2017-12-10T11:56:06.773362: step 94, loss 7.59468, acc 0.21875, prec 0.0230978, recall 0.673267
2017-12-10T11:56:07.209057: step 95, loss 7.73557, acc 0.203125, prec 0.0227045, recall 0.673267
2017-12-10T11:56:07.636359: step 96, loss 5.19763, acc 0.40625, prec 0.0227423, recall 0.676471
2017-12-10T11:56:08.071186: step 97, loss 6.89325, acc 0.203125, prec 0.0226831, recall 0.679612
2017-12-10T11:56:08.509567: step 98, loss 7.00328, acc 0.28125, prec 0.0223499, recall 0.679612
2017-12-10T11:56:08.959863: step 99, loss 6.15748, acc 0.3125, prec 0.0220403, recall 0.679612
2017-12-10T11:56:09.397719: step 100, loss 5.26397, acc 0.375, prec 0.0223741, recall 0.685714
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-100

2017-12-10T11:56:11.555900: step 101, loss 5.1674, acc 0.4375, prec 0.0221266, recall 0.685714
2017-12-10T11:56:12.008704: step 102, loss 3.12986, acc 0.53125, prec 0.0219245, recall 0.685714
2017-12-10T11:56:12.459703: step 103, loss 3.24463, acc 0.609375, prec 0.0220544, recall 0.688679
2017-12-10T11:56:12.907171: step 104, loss 1.82073, acc 0.6875, prec 0.0222156, recall 0.691589
2017-12-10T11:56:13.346510: step 105, loss 2.07031, acc 0.625, prec 0.0220566, recall 0.691589
2017-12-10T11:56:13.784857: step 106, loss 1.4815, acc 0.75, prec 0.0219519, recall 0.691589
2017-12-10T11:56:14.224204: step 107, loss 11.4647, acc 0.828125, prec 0.021887, recall 0.685185
2017-12-10T11:56:14.669450: step 108, loss 1.88113, acc 0.890625, prec 0.0227072, recall 0.693694
2017-12-10T11:56:15.115911: step 109, loss 14.6583, acc 0.78125, prec 0.0226204, recall 0.6875
2017-12-10T11:56:15.557941: step 110, loss 21.4332, acc 0.796875, prec 0.022827, recall 0.684211
2017-12-10T11:56:16.012595: step 111, loss 19.3554, acc 0.78125, prec 0.0230321, recall 0.675214
2017-12-10T11:56:16.454694: step 112, loss 1.48671, acc 0.75, prec 0.0229251, recall 0.675214
2017-12-10T11:56:16.891632: step 113, loss 1.76061, acc 0.765625, prec 0.0233901, recall 0.680672
2017-12-10T11:56:17.337283: step 114, loss 2.61675, acc 0.640625, prec 0.0235159, recall 0.683333
2017-12-10T11:56:17.787446: step 115, loss 6.78716, acc 0.671875, prec 0.0236602, recall 0.680328
2017-12-10T11:56:18.236374: step 116, loss 2.67343, acc 0.625, prec 0.0234994, recall 0.680328
2017-12-10T11:56:18.670404: step 117, loss 6.10651, acc 0.375, prec 0.0232363, recall 0.680328
2017-12-10T11:56:19.109502: step 118, loss 9.75043, acc 0.46875, prec 0.0235653, recall 0.68
2017-12-10T11:56:19.553161: step 119, loss 4.28741, acc 0.46875, prec 0.0236134, recall 0.68254
2017-12-10T11:56:19.976538: step 120, loss 5.2359, acc 0.390625, prec 0.0233632, recall 0.68254
2017-12-10T11:56:20.424581: step 121, loss 23.049, acc 0.421875, prec 0.0231369, recall 0.677165
2017-12-10T11:56:20.865262: step 122, loss 6.52468, acc 0.328125, prec 0.0233918, recall 0.682171
2017-12-10T11:56:21.317004: step 123, loss 6.24622, acc 0.375, prec 0.0234026, recall 0.684615
2017-12-10T11:56:21.779354: step 124, loss 5.61398, acc 0.359375, prec 0.023153, recall 0.684615
2017-12-10T11:56:22.211293: step 125, loss 5.39526, acc 0.421875, prec 0.0229322, recall 0.684615
2017-12-10T11:56:22.639561: step 126, loss 5.79266, acc 0.40625, prec 0.0227099, recall 0.684615
2017-12-10T11:56:23.082513: step 127, loss 4.55951, acc 0.515625, prec 0.022779, recall 0.687023
2017-12-10T11:56:23.510962: step 128, loss 3.56305, acc 0.53125, prec 0.0226074, recall 0.687023
2017-12-10T11:56:23.959696: step 129, loss 4.67056, acc 0.390625, prec 0.0223881, recall 0.687023
2017-12-10T11:56:24.392179: step 130, loss 3.17966, acc 0.578125, prec 0.0224802, recall 0.689394
2017-12-10T11:56:24.825807: step 131, loss 2.80862, acc 0.609375, prec 0.0225822, recall 0.691729
2017-12-10T11:56:25.254896: step 132, loss 1.97973, acc 0.671875, prec 0.0224664, recall 0.691729
2017-12-10T11:56:25.689790: step 133, loss 3.08842, acc 0.53125, prec 0.0227768, recall 0.696296
2017-12-10T11:56:26.122246: step 134, loss 23.4123, acc 0.71875, prec 0.0226889, recall 0.686131
2017-12-10T11:56:26.566553: step 135, loss 1.52319, acc 0.765625, prec 0.0230769, recall 0.690647
2017-12-10T11:56:27.002780: step 136, loss 1.58245, acc 0.6875, prec 0.0232002, recall 0.692857
2017-12-10T11:56:27.436612: step 137, loss 1.26437, acc 0.828125, prec 0.0231393, recall 0.692857
2017-12-10T11:56:27.870412: step 138, loss 2.5822, acc 0.65625, prec 0.023482, recall 0.697183
2017-12-10T11:56:28.304384: step 139, loss 10.7514, acc 0.65625, prec 0.0233656, recall 0.692308
2017-12-10T11:56:28.747443: step 140, loss 1.35802, acc 0.71875, prec 0.0232667, recall 0.692308
2017-12-10T11:56:29.184952: step 141, loss 2.54817, acc 0.671875, prec 0.0233809, recall 0.694444
2017-12-10T11:56:29.617396: step 142, loss 1.97528, acc 0.65625, prec 0.0237154, recall 0.69863
2017-12-10T11:56:30.063481: step 143, loss 1.79641, acc 0.71875, prec 0.0236166, recall 0.69863
2017-12-10T11:56:30.512950: step 144, loss 1.97137, acc 0.703125, prec 0.0235131, recall 0.69863
2017-12-10T11:56:30.952870: step 145, loss 2.23144, acc 0.671875, prec 0.0233999, recall 0.69863
2017-12-10T11:56:31.397959: step 146, loss 2.16622, acc 0.734375, prec 0.0237551, recall 0.702703
2017-12-10T11:56:31.830261: step 147, loss 3.1259, acc 0.546875, prec 0.0235988, recall 0.702703
2017-12-10T11:56:32.257924: step 148, loss 2.1353, acc 0.625, prec 0.0236913, recall 0.704698
2017-12-10T11:56:32.699323: step 149, loss 3.69046, acc 0.578125, prec 0.0235479, recall 0.704698
2017-12-10T11:56:33.143470: step 150, loss 16.4815, acc 0.65625, prec 0.0234427, recall 0.695364
2017-12-10T11:56:33.606422: step 151, loss 2.18326, acc 0.671875, prec 0.0233333, recall 0.695364
2017-12-10T11:56:34.045725: step 152, loss 1.18904, acc 0.78125, prec 0.023261, recall 0.695364
2017-12-10T11:56:34.499230: step 153, loss 4.0288, acc 0.71875, prec 0.0231737, recall 0.690789
2017-12-10T11:56:34.947329: step 154, loss 1.71298, acc 0.703125, prec 0.0230769, recall 0.690789
2017-12-10T11:56:35.390746: step 155, loss 1.83394, acc 0.71875, prec 0.0231998, recall 0.69281
2017-12-10T11:56:35.838272: step 156, loss 16.8653, acc 0.6875, prec 0.0233166, recall 0.690323
2017-12-10T11:56:36.286916: step 157, loss 2.65313, acc 0.640625, prec 0.0232003, recall 0.690323
2017-12-10T11:56:36.737218: step 158, loss 2.52108, acc 0.609375, prec 0.0230753, recall 0.690323
2017-12-10T11:56:37.185314: step 159, loss 6.77896, acc 0.46875, prec 0.0231214, recall 0.687898
2017-12-10T11:56:37.637670: step 160, loss 4.07656, acc 0.578125, prec 0.0234043, recall 0.691824
2017-12-10T11:56:38.084943: step 161, loss 4.60147, acc 0.46875, prec 0.0234424, recall 0.69375
2017-12-10T11:56:38.527675: step 162, loss 18.7826, acc 0.453125, prec 0.0234801, recall 0.691358
2017-12-10T11:56:38.972403: step 163, loss 7.03202, acc 0.515625, prec 0.0233333, recall 0.687117
2017-12-10T11:56:39.412271: step 164, loss 4.94107, acc 0.375, prec 0.0231405, recall 0.687117
2017-12-10T11:56:39.847073: step 165, loss 5.89535, acc 0.40625, prec 0.0231605, recall 0.689024
2017-12-10T11:56:40.284644: step 166, loss 7.12073, acc 0.28125, prec 0.0229442, recall 0.689024
2017-12-10T11:56:40.726472: step 167, loss 22.4982, acc 0.359375, prec 0.0231528, recall 0.688623
2017-12-10T11:56:41.167315: step 168, loss 5.11067, acc 0.46875, prec 0.0229954, recall 0.688623
2017-12-10T11:56:41.614188: step 169, loss 6.03875, acc 0.3125, prec 0.0231821, recall 0.692308
2017-12-10T11:56:42.052632: step 170, loss 5.03144, acc 0.40625, prec 0.0232009, recall 0.694118
2017-12-10T11:56:42.488225: step 171, loss 5.42377, acc 0.40625, prec 0.0230289, recall 0.694118
2017-12-10T11:56:42.942828: step 172, loss 5.08705, acc 0.34375, prec 0.0232198, recall 0.697674
2017-12-10T11:56:43.387265: step 173, loss 4.40745, acc 0.46875, prec 0.0232558, recall 0.699422
2017-12-10T11:56:43.818070: step 174, loss 5.26769, acc 0.375, prec 0.0230784, recall 0.699422
2017-12-10T11:56:44.274835: step 175, loss 3.47143, acc 0.59375, prec 0.0231499, recall 0.701149
2017-12-10T11:56:44.709969: step 176, loss 4.04732, acc 0.484375, prec 0.02319, recall 0.702857
2017-12-10T11:56:45.145638: step 177, loss 4.83194, acc 0.546875, prec 0.0234346, recall 0.702247
2017-12-10T11:56:45.585908: step 178, loss 1.52343, acc 0.703125, prec 0.0233514, recall 0.702247
2017-12-10T11:56:46.022030: step 179, loss 1.58824, acc 0.765625, prec 0.0232861, recall 0.702247
2017-12-10T11:56:46.454753: step 180, loss 3.31617, acc 0.703125, prec 0.0232083, recall 0.698324
2017-12-10T11:56:46.892694: step 181, loss 12.3025, acc 0.765625, prec 0.0231481, recall 0.694444
2017-12-10T11:56:47.349933: step 182, loss 1.68628, acc 0.765625, prec 0.0232644, recall 0.696133
2017-12-10T11:56:47.798766: step 183, loss 1.7408, acc 0.703125, prec 0.0231831, recall 0.696133
2017-12-10T11:56:48.239963: step 184, loss 1.20112, acc 0.8125, prec 0.023132, recall 0.696133
2017-12-10T11:56:48.677621: step 185, loss 1.34872, acc 0.78125, prec 0.0232516, recall 0.697802
2017-12-10T11:56:49.127459: step 186, loss 18.6599, acc 0.765625, prec 0.0231963, recall 0.690217
2017-12-10T11:56:49.567924: step 187, loss 0.921129, acc 0.765625, prec 0.023133, recall 0.690217
2017-12-10T11:56:50.007492: step 188, loss 1.85005, acc 0.65625, prec 0.0232178, recall 0.691892
2017-12-10T11:56:50.441826: step 189, loss 6.0383, acc 0.546875, prec 0.0232768, recall 0.68984
2017-12-10T11:56:50.895030: step 190, loss 1.49488, acc 0.78125, prec 0.0232181, recall 0.68984
2017-12-10T11:56:51.340831: step 191, loss 2.70934, acc 0.65625, prec 0.0233017, recall 0.691489
2017-12-10T11:56:51.776840: step 192, loss 1.39571, acc 0.765625, prec 0.0232392, recall 0.691489
2017-12-10T11:56:52.214999: step 193, loss 28.234, acc 0.65625, prec 0.0238519, recall 0.690722
2017-12-10T11:56:52.651680: step 194, loss 2.5569, acc 0.578125, prec 0.0237378, recall 0.690722
2017-12-10T11:56:53.089498: step 195, loss 3.27814, acc 0.453125, prec 0.0237634, recall 0.692308
2017-12-10T11:56:53.522523: step 196, loss 16.6207, acc 0.484375, prec 0.0236303, recall 0.688776
2017-12-10T11:56:53.956992: step 197, loss 5.63576, acc 0.359375, prec 0.0234619, recall 0.688776
2017-12-10T11:56:54.396089: step 198, loss 5.98139, acc 0.3125, prec 0.0232839, recall 0.688776
2017-12-10T11:56:54.852710: step 199, loss 16.3112, acc 0.296875, prec 0.0234428, recall 0.688442
2017-12-10T11:56:55.320776: step 200, loss 6.1747, acc 0.25, prec 0.0234176, recall 0.69
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-200

2017-12-10T11:56:58.950495: step 201, loss 5.89297, acc 0.265625, prec 0.0235611, recall 0.693069
2017-12-10T11:56:59.400484: step 202, loss 5.62933, acc 0.34375, prec 0.0233957, recall 0.693069
2017-12-10T11:56:59.833452: step 203, loss 5.08006, acc 0.40625, prec 0.0232481, recall 0.693069
2017-12-10T11:57:00.265931: step 204, loss 5.51225, acc 0.390625, prec 0.0232597, recall 0.694581
2017-12-10T11:57:00.697015: step 205, loss 5.82956, acc 0.328125, prec 0.0232558, recall 0.696078
2017-12-10T11:57:01.134895: step 206, loss 4.1932, acc 0.4375, prec 0.0231195, recall 0.696078
2017-12-10T11:57:01.576798: step 207, loss 4.39069, acc 0.4375, prec 0.0229848, recall 0.696078
2017-12-10T11:57:02.022948: step 208, loss 2.37109, acc 0.5625, prec 0.0230385, recall 0.697561
2017-12-10T11:57:02.460287: step 209, loss 2.02902, acc 0.578125, prec 0.0230954, recall 0.699029
2017-12-10T11:57:02.907471: step 210, loss 1.47445, acc 0.6875, prec 0.0231777, recall 0.700483
2017-12-10T11:57:03.348337: step 211, loss 1.65007, acc 0.6875, prec 0.0232595, recall 0.701923
2017-12-10T11:57:03.809817: step 212, loss 11.5939, acc 0.65625, prec 0.0231856, recall 0.695238
2017-12-10T11:57:04.251057: step 213, loss 1.43681, acc 0.671875, prec 0.0232632, recall 0.696682
2017-12-10T11:57:04.687178: step 214, loss 4.66122, acc 0.71875, prec 0.0232008, recall 0.693396
2017-12-10T11:57:05.139947: step 215, loss 1.27947, acc 0.75, prec 0.0231423, recall 0.693396
2017-12-10T11:57:05.581613: step 216, loss 1.46048, acc 0.71875, prec 0.0232303, recall 0.694836
2017-12-10T11:57:06.026068: step 217, loss 7.8059, acc 0.671875, prec 0.0234632, recall 0.694444
2017-12-10T11:57:06.454356: step 218, loss 8.07863, acc 0.6875, prec 0.0233973, recall 0.688073
2017-12-10T11:57:06.904242: step 219, loss 2.00715, acc 0.734375, prec 0.0234873, recall 0.689498
2017-12-10T11:57:07.354160: step 220, loss 9.06842, acc 0.640625, prec 0.0235586, recall 0.687783
2017-12-10T11:57:07.793712: step 221, loss 8.62557, acc 0.53125, prec 0.0234532, recall 0.684685
2017-12-10T11:57:08.237987: step 222, loss 4.61138, acc 0.46875, prec 0.0233344, recall 0.681614
2017-12-10T11:57:08.679339: step 223, loss 4.85184, acc 0.375, prec 0.0231919, recall 0.681614
2017-12-10T11:57:09.114485: step 224, loss 5.42405, acc 0.40625, prec 0.0230583, recall 0.681614
2017-12-10T11:57:09.550923: step 225, loss 6.36797, acc 0.34375, prec 0.0229123, recall 0.681614
2017-12-10T11:57:09.982896: step 226, loss 6.60735, acc 0.3125, prec 0.0230539, recall 0.684444
2017-12-10T11:57:10.416288: step 227, loss 6.5409, acc 0.328125, prec 0.0230518, recall 0.685841
2017-12-10T11:57:10.849243: step 228, loss 5.69983, acc 0.3125, prec 0.0230462, recall 0.687225
2017-12-10T11:57:11.281389: step 229, loss 5.58097, acc 0.28125, prec 0.023034, recall 0.688596
2017-12-10T11:57:11.725652: step 230, loss 4.32572, acc 0.546875, prec 0.0230792, recall 0.689956
2017-12-10T11:57:12.168061: step 231, loss 3.396, acc 0.515625, prec 0.0229751, recall 0.689956
2017-12-10T11:57:12.607979: step 232, loss 4.64818, acc 0.46875, prec 0.0228621, recall 0.689956
2017-12-10T11:57:13.067379: step 233, loss 5.31408, acc 0.53125, prec 0.0230481, recall 0.689655
2017-12-10T11:57:13.512427: step 234, loss 3.88977, acc 0.515625, prec 0.0229456, recall 0.689655
2017-12-10T11:57:13.973623: step 235, loss 4.5585, acc 0.515625, prec 0.0228441, recall 0.689655
2017-12-10T11:57:14.420548: step 236, loss 2.37546, acc 0.609375, prec 0.0227628, recall 0.689655
2017-12-10T11:57:14.861234: step 237, loss 2.86203, acc 0.546875, prec 0.0230845, recall 0.693617
2017-12-10T11:57:15.304094: step 238, loss 2.12033, acc 0.65625, prec 0.0234265, recall 0.697479
2017-12-10T11:57:15.745527: step 239, loss 9.72382, acc 0.8125, prec 0.0235278, recall 0.695833
2017-12-10T11:57:16.189556: step 240, loss 2.53713, acc 0.6875, prec 0.0234616, recall 0.695833
2017-12-10T11:57:16.641580: step 241, loss 1.34877, acc 0.703125, prec 0.0233992, recall 0.695833
2017-12-10T11:57:17.096969: step 242, loss 15.5622, acc 0.671875, prec 0.0233371, recall 0.690083
2017-12-10T11:57:17.531542: step 243, loss 2.21877, acc 0.625, prec 0.0233951, recall 0.691358
2017-12-10T11:57:17.982922: step 244, loss 2.32491, acc 0.640625, prec 0.0233204, recall 0.691358
2017-12-10T11:57:18.423411: step 245, loss 2.5177, acc 0.640625, prec 0.0233813, recall 0.692623
2017-12-10T11:57:18.868798: step 246, loss 5.63112, acc 0.65625, prec 0.023583, recall 0.692308
2017-12-10T11:57:19.306164: step 247, loss 2.0622, acc 0.578125, prec 0.0236296, recall 0.693548
2017-12-10T11:57:19.744446: step 248, loss 2.54184, acc 0.734375, prec 0.0237084, recall 0.694779
2017-12-10T11:57:20.199805: step 249, loss 4.38883, acc 0.546875, prec 0.023881, recall 0.697211
2017-12-10T11:57:20.646871: step 250, loss 3.82713, acc 0.578125, prec 0.023926, recall 0.698413
2017-12-10T11:57:21.091845: step 251, loss 3.50021, acc 0.46875, prec 0.023948, recall 0.699605
2017-12-10T11:57:21.535923: step 252, loss 2.69825, acc 0.59375, prec 0.0238641, recall 0.699605
2017-12-10T11:57:21.978215: step 253, loss 2.67321, acc 0.578125, prec 0.0237775, recall 0.699605
2017-12-10T11:57:22.424642: step 254, loss 2.71419, acc 0.625, prec 0.0239625, recall 0.701961
2017-12-10T11:57:22.868185: step 255, loss 2.77175, acc 0.53125, prec 0.0238667, recall 0.701961
2017-12-10T11:57:23.301737: step 256, loss 3.12755, acc 0.484375, prec 0.0237621, recall 0.701961
2017-12-10T11:57:23.737884: step 257, loss 4.04517, acc 0.515625, prec 0.0237938, recall 0.703125
2017-12-10T11:57:24.177358: step 258, loss 1.87213, acc 0.703125, prec 0.0237342, recall 0.703125
2017-12-10T11:57:24.611632: step 259, loss 11.0172, acc 0.71875, prec 0.0236811, recall 0.700389
2017-12-10T11:57:25.049548: step 260, loss 1.34726, acc 0.75, prec 0.0236314, recall 0.700389
2017-12-10T11:57:25.487760: step 261, loss 2.14805, acc 0.671875, prec 0.0236942, recall 0.70155
2017-12-10T11:57:25.927812: step 262, loss 2.44643, acc 0.65625, prec 0.0236262, recall 0.70155
2017-12-10T11:57:26.380274: step 263, loss 1.12794, acc 0.796875, prec 0.0235861, recall 0.70155
2017-12-10T11:57:26.819824: step 264, loss 1.27751, acc 0.6875, prec 0.0236517, recall 0.702703
2017-12-10T11:57:27.271202: step 265, loss 1.23226, acc 0.765625, prec 0.0237323, recall 0.703846
2017-12-10T11:57:27.726694: step 266, loss 1.21065, acc 0.765625, prec 0.0236863, recall 0.703846
2017-12-10T11:57:28.170376: step 267, loss 16.9712, acc 0.71875, prec 0.0237603, recall 0.70229
2017-12-10T11:57:28.630341: step 268, loss 18.4423, acc 0.78125, prec 0.0237205, recall 0.69962
2017-12-10T11:57:29.091442: step 269, loss 7.80996, acc 0.71875, prec 0.0237942, recall 0.698113
2017-12-10T11:57:29.525714: step 270, loss 1.51136, acc 0.71875, prec 0.0238645, recall 0.699248
2017-12-10T11:57:29.968158: step 271, loss 2.46849, acc 0.59375, prec 0.0237852, recall 0.699248
2017-12-10T11:57:30.423002: step 272, loss 1.88391, acc 0.625, prec 0.0238368, recall 0.700375
2017-12-10T11:57:30.869984: step 273, loss 2.34655, acc 0.625, prec 0.0238882, recall 0.701493
2017-12-10T11:57:31.309341: step 274, loss 2.89845, acc 0.609375, prec 0.0238125, recall 0.701493
2017-12-10T11:57:31.765921: step 275, loss 3.25347, acc 0.484375, prec 0.0239596, recall 0.703704
2017-12-10T11:57:32.214451: step 276, loss 2.22555, acc 0.609375, prec 0.024007, recall 0.704797
2017-12-10T11:57:32.657818: step 277, loss 2.35207, acc 0.609375, prec 0.0240541, recall 0.705882
2017-12-10T11:57:33.110378: step 278, loss 1.86424, acc 0.734375, prec 0.024247, recall 0.708029
2017-12-10T11:57:33.571878: step 279, loss 1.90337, acc 0.671875, prec 0.0243051, recall 0.709091
2017-12-10T11:57:34.019386: step 280, loss 1.93732, acc 0.71875, prec 0.0244933, recall 0.711191
2017-12-10T11:57:34.456907: step 281, loss 1.23754, acc 0.78125, prec 0.0245719, recall 0.71223
2017-12-10T11:57:34.911044: step 282, loss 3.23215, acc 0.71875, prec 0.0245201, recall 0.709677
2017-12-10T11:57:35.348606: step 283, loss 1.36805, acc 0.671875, prec 0.024577, recall 0.710714
2017-12-10T11:57:35.808057: step 284, loss 6.01518, acc 0.8125, prec 0.0245437, recall 0.708185
2017-12-10T11:57:36.261533: step 285, loss 1.10732, acc 0.75, prec 0.0246154, recall 0.70922
2017-12-10T11:57:36.713614: step 286, loss 1.26192, acc 0.78125, prec 0.024573, recall 0.70922
2017-12-10T11:57:37.168223: step 287, loss 14.8627, acc 0.703125, prec 0.0245218, recall 0.704225
2017-12-10T11:57:37.611804: step 288, loss 1.92286, acc 0.703125, prec 0.0245841, recall 0.705263
2017-12-10T11:57:38.052783: step 289, loss 1.75722, acc 0.765625, prec 0.0247772, recall 0.707317
2017-12-10T11:57:38.493601: step 290, loss 2.40943, acc 0.609375, prec 0.0247019, recall 0.707317
2017-12-10T11:57:38.937689: step 291, loss 13.74, acc 0.59375, prec 0.0247453, recall 0.705882
2017-12-10T11:57:39.372193: step 292, loss 2.50702, acc 0.59375, prec 0.0247854, recall 0.706897
2017-12-10T11:57:39.828312: step 293, loss 3.23353, acc 0.484375, prec 0.0246869, recall 0.706897
2017-12-10T11:57:40.265887: step 294, loss 15.7934, acc 0.40625, prec 0.0245774, recall 0.704467
2017-12-10T11:57:40.706189: step 295, loss 2.87527, acc 0.484375, prec 0.0244805, recall 0.704467
2017-12-10T11:57:41.166907: step 296, loss 3.60829, acc 0.46875, prec 0.0244976, recall 0.705479
2017-12-10T11:57:41.609965: step 297, loss 5.3178, acc 0.34375, prec 0.0243758, recall 0.705479
2017-12-10T11:57:42.045564: step 298, loss 4.17628, acc 0.375, prec 0.0243759, recall 0.706485
2017-12-10T11:57:42.500362: step 299, loss 3.44628, acc 0.40625, prec 0.0243817, recall 0.707483
2017-12-10T11:57:42.944500: step 300, loss 3.33181, acc 0.53125, prec 0.0242962, recall 0.707483
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-300

2017-12-10T11:57:44.859028: step 301, loss 2.71532, acc 0.53125, prec 0.024325, recall 0.708475
2017-12-10T11:57:45.288572: step 302, loss 3.02506, acc 0.484375, prec 0.024345, recall 0.709459
2017-12-10T11:57:45.735205: step 303, loss 2.40329, acc 0.640625, prec 0.0242803, recall 0.709459
2017-12-10T11:57:46.177059: step 304, loss 1.92147, acc 0.609375, prec 0.0244352, recall 0.711409
2017-12-10T11:57:46.620788: step 305, loss 3.21764, acc 0.53125, prec 0.024351, recall 0.711409
2017-12-10T11:57:47.066953: step 306, loss 1.14289, acc 0.71875, prec 0.0243008, recall 0.711409
2017-12-10T11:57:47.497574: step 307, loss 6.96209, acc 0.765625, prec 0.0242618, recall 0.70903
2017-12-10T11:57:47.930560: step 308, loss 5.62851, acc 0.8125, prec 0.0243429, recall 0.707641
2017-12-10T11:57:48.366350: step 309, loss 4.83995, acc 0.71875, prec 0.0242957, recall 0.705298
2017-12-10T11:57:48.807177: step 310, loss 1.91922, acc 0.625, prec 0.0242293, recall 0.705298
2017-12-10T11:57:49.244877: step 311, loss 1.11801, acc 0.78125, prec 0.0241908, recall 0.705298
2017-12-10T11:57:49.702937: step 312, loss 2.82149, acc 0.84375, prec 0.0241661, recall 0.70297
2017-12-10T11:57:50.153393: step 313, loss 2.09948, acc 0.59375, prec 0.024095, recall 0.70297
2017-12-10T11:57:50.596031: step 314, loss 2.00446, acc 0.703125, prec 0.0241535, recall 0.703947
2017-12-10T11:57:51.040473: step 315, loss 8.83582, acc 0.6875, prec 0.0242117, recall 0.702614
2017-12-10T11:57:51.488669: step 316, loss 2.63779, acc 0.578125, prec 0.0241383, recall 0.702614
2017-12-10T11:57:51.922829: step 317, loss 1.69382, acc 0.6875, prec 0.0240842, recall 0.702614
2017-12-10T11:57:52.363373: step 318, loss 3.95889, acc 0.625, prec 0.0243494, recall 0.703226
2017-12-10T11:57:52.798291: step 319, loss 3.67541, acc 0.546875, prec 0.0243794, recall 0.70418
2017-12-10T11:57:53.252875: step 320, loss 3.03819, acc 0.5625, prec 0.0244119, recall 0.705128
2017-12-10T11:57:53.695665: step 321, loss 11.3145, acc 0.421875, prec 0.0243148, recall 0.702875
2017-12-10T11:57:54.131895: step 322, loss 4.93831, acc 0.46875, prec 0.0242264, recall 0.700637
2017-12-10T11:57:54.579868: step 323, loss 4.0912, acc 0.453125, prec 0.0242404, recall 0.701587
2017-12-10T11:57:55.020222: step 324, loss 3.18184, acc 0.578125, prec 0.0241689, recall 0.701587
2017-12-10T11:57:55.460442: step 325, loss 4.48674, acc 0.375, prec 0.0240636, recall 0.701587
2017-12-10T11:57:55.901285: step 326, loss 3.82397, acc 0.4375, prec 0.0241813, recall 0.70347
2017-12-10T11:57:56.348704: step 327, loss 3.1765, acc 0.53125, prec 0.0241029, recall 0.70347
2017-12-10T11:57:56.795039: step 328, loss 4.02482, acc 0.40625, prec 0.0240043, recall 0.70347
2017-12-10T11:57:57.235947: step 329, loss 3.64472, acc 0.46875, prec 0.0239168, recall 0.70347
2017-12-10T11:57:57.667315: step 330, loss 2.83865, acc 0.5, prec 0.0239393, recall 0.704403
2017-12-10T11:57:58.094883: step 331, loss 2.98891, acc 0.5, prec 0.0240656, recall 0.70625
2017-12-10T11:57:58.534242: step 332, loss 2.89942, acc 0.5625, prec 0.0240977, recall 0.707165
2017-12-10T11:57:58.982516: step 333, loss 1.70132, acc 0.671875, prec 0.0240441, recall 0.707165
2017-12-10T11:57:59.429745: step 334, loss 5.9503, acc 0.734375, prec 0.0240034, recall 0.704969
2017-12-10T11:57:59.882253: step 335, loss 1.54113, acc 0.6875, prec 0.0239527, recall 0.704969
2017-12-10T11:58:00.331091: step 336, loss 1.13263, acc 0.75, prec 0.0239124, recall 0.704969
2017-12-10T11:58:00.773196: step 337, loss 1.12825, acc 0.8125, prec 0.0239849, recall 0.705882
2017-12-10T11:58:01.202210: step 338, loss 1.48129, acc 0.703125, prec 0.0241419, recall 0.707692
2017-12-10T11:58:01.651663: step 339, loss 9.96746, acc 0.71875, prec 0.0240989, recall 0.705521
2017-12-10T11:58:02.088497: step 340, loss 0.900775, acc 0.8125, prec 0.0240686, recall 0.705521
2017-12-10T11:58:02.550083: step 341, loss 11.1663, acc 0.734375, prec 0.0240284, recall 0.703364
2017-12-10T11:58:02.996061: step 342, loss 0.751383, acc 0.84375, prec 0.0240033, recall 0.703364
2017-12-10T11:58:03.453510: step 343, loss 0.743578, acc 0.828125, prec 0.0239758, recall 0.703364
2017-12-10T11:58:03.898932: step 344, loss 1.01532, acc 0.8125, prec 0.0239459, recall 0.703364
2017-12-10T11:58:04.328728: step 345, loss 1.5179, acc 0.71875, prec 0.0241039, recall 0.705167
2017-12-10T11:58:04.781551: step 346, loss 0.875203, acc 0.75, prec 0.0240639, recall 0.705167
2017-12-10T11:58:05.218866: step 347, loss 34.1186, acc 0.796875, prec 0.0241351, recall 0.703928
2017-12-10T11:58:05.669269: step 348, loss 2.98249, acc 0.65625, prec 0.0241835, recall 0.702703
2017-12-10T11:58:06.112805: step 349, loss 3.22788, acc 0.734375, prec 0.0241436, recall 0.700599
2017-12-10T11:58:06.555319: step 350, loss 1.41376, acc 0.703125, prec 0.0240964, recall 0.700599
2017-12-10T11:58:06.999699: step 351, loss 1.79591, acc 0.65625, prec 0.0240419, recall 0.700599
2017-12-10T11:58:07.448752: step 352, loss 2.12769, acc 0.640625, prec 0.0240853, recall 0.701493
2017-12-10T11:58:07.894981: step 353, loss 2.40103, acc 0.625, prec 0.0240262, recall 0.701493
2017-12-10T11:58:08.347302: step 354, loss 2.65375, acc 0.546875, prec 0.0239551, recall 0.701493
2017-12-10T11:58:08.796955: step 355, loss 12.4592, acc 0.40625, prec 0.0238651, recall 0.699405
2017-12-10T11:58:09.232417: step 356, loss 2.54433, acc 0.578125, prec 0.0237999, recall 0.699405
2017-12-10T11:58:09.667710: step 357, loss 2.12838, acc 0.578125, prec 0.023735, recall 0.699405
2017-12-10T11:58:10.110293: step 358, loss 3.59692, acc 0.453125, prec 0.0236514, recall 0.699405
2017-12-10T11:58:10.556015: step 359, loss 2.37515, acc 0.578125, prec 0.0237832, recall 0.701183
2017-12-10T11:58:10.999843: step 360, loss 2.21639, acc 0.578125, prec 0.0238167, recall 0.702065
2017-12-10T11:58:11.450355: step 361, loss 18.7524, acc 0.53125, prec 0.0237501, recall 0.697947
2017-12-10T11:58:11.890366: step 362, loss 3.72623, acc 0.53125, prec 0.0237764, recall 0.69883
2017-12-10T11:58:12.331639: step 363, loss 1.40004, acc 0.625, prec 0.0237197, recall 0.69883
2017-12-10T11:58:12.770968: step 364, loss 7.56504, acc 0.609375, prec 0.0236634, recall 0.696793
2017-12-10T11:58:13.224748: step 365, loss 3.27638, acc 0.5, prec 0.023685, recall 0.697674
2017-12-10T11:58:13.671945: step 366, loss 2.43916, acc 0.546875, prec 0.0237135, recall 0.698551
2017-12-10T11:58:14.108299: step 367, loss 2.90755, acc 0.53125, prec 0.0238352, recall 0.700288
2017-12-10T11:58:14.555497: step 368, loss 4.50163, acc 0.390625, prec 0.0237444, recall 0.700288
2017-12-10T11:58:15.014051: step 369, loss 3.28931, acc 0.5, prec 0.0237655, recall 0.701149
2017-12-10T11:58:15.456325: step 370, loss 6.12095, acc 0.515625, prec 0.0236962, recall 0.69914
2017-12-10T11:58:15.895315: step 371, loss 3.48385, acc 0.375, prec 0.0236045, recall 0.69914
2017-12-10T11:58:16.339141: step 372, loss 2.86116, acc 0.5625, prec 0.023635, recall 0.7
2017-12-10T11:58:16.788486: step 373, loss 3.29459, acc 0.5, prec 0.0235622, recall 0.7
2017-12-10T11:58:17.231394: step 374, loss 3.54637, acc 0.546875, prec 0.0235903, recall 0.700855
2017-12-10T11:58:17.674382: step 375, loss 3.03563, acc 0.609375, prec 0.0236273, recall 0.701705
2017-12-10T11:58:18.114089: step 376, loss 2.04887, acc 0.609375, prec 0.0236641, recall 0.70255
2017-12-10T11:58:18.551948: step 377, loss 2.33994, acc 0.5625, prec 0.0236011, recall 0.70255
2017-12-10T11:58:18.997039: step 378, loss 3.21138, acc 0.515625, prec 0.0235316, recall 0.70255
2017-12-10T11:58:19.442260: step 379, loss 2.18469, acc 0.640625, prec 0.0234804, recall 0.70255
2017-12-10T11:58:19.890938: step 380, loss 1.9396, acc 0.640625, prec 0.0234294, recall 0.70255
2017-12-10T11:58:20.324894: step 381, loss 1.40165, acc 0.71875, prec 0.0234817, recall 0.70339
2017-12-10T11:58:20.759336: step 382, loss 1.79842, acc 0.671875, prec 0.023711, recall 0.705882
2017-12-10T11:58:21.215612: step 383, loss 20.1621, acc 0.765625, prec 0.0236798, recall 0.703911
2017-12-10T11:58:21.658793: step 384, loss 5.99257, acc 0.8125, prec 0.0236575, recall 0.7
2017-12-10T11:58:22.101043: step 385, loss 1.15116, acc 0.71875, prec 0.0236176, recall 0.7
2017-12-10T11:58:22.539572: step 386, loss 10.4265, acc 0.640625, prec 0.023569, recall 0.698061
2017-12-10T11:58:22.996493: step 387, loss 2.51118, acc 0.5625, prec 0.0235075, recall 0.698061
2017-12-10T11:58:23.448283: step 388, loss 1.22194, acc 0.734375, prec 0.0235612, recall 0.698895
2017-12-10T11:58:23.889995: step 389, loss 2.2429, acc 0.640625, prec 0.0235108, recall 0.698895
2017-12-10T11:58:24.328837: step 390, loss 1.61511, acc 0.703125, prec 0.0234694, recall 0.698895
2017-12-10T11:58:24.763659: step 391, loss 1.56841, acc 0.71875, prec 0.0234303, recall 0.698895
2017-12-10T11:58:25.207433: step 392, loss 2.04138, acc 0.640625, prec 0.0235609, recall 0.700549
2017-12-10T11:58:25.654110: step 393, loss 9.57439, acc 0.640625, prec 0.0235131, recall 0.69863
2017-12-10T11:58:26.102506: step 394, loss 2.16426, acc 0.640625, prec 0.0235532, recall 0.699454
2017-12-10T11:58:26.538923: step 395, loss 2.68588, acc 0.703125, prec 0.023604, recall 0.69837
2017-12-10T11:58:26.978259: step 396, loss 2.27543, acc 0.640625, prec 0.0235542, recall 0.69837
2017-12-10T11:58:27.417118: step 397, loss 2.46785, acc 0.53125, prec 0.0235789, recall 0.699187
2017-12-10T11:58:27.849610: step 398, loss 3.17779, acc 0.671875, prec 0.0235359, recall 0.697297
2017-12-10T11:58:28.291957: step 399, loss 1.12893, acc 0.734375, prec 0.0236773, recall 0.698925
2017-12-10T11:58:28.737894: step 400, loss 2.41666, acc 0.5625, prec 0.023617, recall 0.698925
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-400

2017-12-10T11:58:30.835952: step 401, loss 1.69323, acc 0.6875, prec 0.0235742, recall 0.698925
2017-12-10T11:58:31.291179: step 402, loss 1.9837, acc 0.671875, prec 0.0235294, recall 0.698925
2017-12-10T11:58:31.733200: step 403, loss 1.77918, acc 0.640625, prec 0.0235687, recall 0.699732
2017-12-10T11:58:32.183894: step 404, loss 15.9197, acc 0.625, prec 0.02361, recall 0.696809
2017-12-10T11:58:32.625282: step 405, loss 1.97567, acc 0.6875, prec 0.0237431, recall 0.698413
2017-12-10T11:58:33.072493: step 406, loss 14.174, acc 0.640625, prec 0.0237839, recall 0.697368
2017-12-10T11:58:33.513229: step 407, loss 2.08668, acc 0.671875, prec 0.0237391, recall 0.697368
2017-12-10T11:58:33.974398: step 408, loss 3.04926, acc 0.578125, prec 0.0237691, recall 0.698163
2017-12-10T11:58:34.421724: step 409, loss 3.50863, acc 0.546875, prec 0.0238817, recall 0.699739
2017-12-10T11:58:34.867771: step 410, loss 5.59125, acc 0.53125, prec 0.0239936, recall 0.699482
2017-12-10T11:58:35.318061: step 411, loss 3.39816, acc 0.625, prec 0.0240291, recall 0.700258
2017-12-10T11:58:35.762565: step 412, loss 3.92478, acc 0.46875, prec 0.0239569, recall 0.700258
2017-12-10T11:58:36.201133: step 413, loss 3.52557, acc 0.53125, prec 0.0239795, recall 0.701031
2017-12-10T11:58:36.642985: step 414, loss 3.66955, acc 0.5, prec 0.0239979, recall 0.7018
2017-12-10T11:58:37.091850: step 415, loss 3.1732, acc 0.53125, prec 0.0240203, recall 0.702564
2017-12-10T11:58:37.540249: step 416, loss 3.63503, acc 0.578125, prec 0.024049, recall 0.703325
2017-12-10T11:58:37.982813: step 417, loss 4.22993, acc 0.453125, prec 0.0240607, recall 0.704082
2017-12-10T11:58:38.441401: step 418, loss 2.80478, acc 0.65625, prec 0.0243542, recall 0.707071
2017-12-10T11:58:38.878846: step 419, loss 3.26456, acc 0.46875, prec 0.0242824, recall 0.707071
2017-12-10T11:58:39.324873: step 420, loss 8.33419, acc 0.421875, prec 0.0242911, recall 0.70603
2017-12-10T11:58:39.767553: step 421, loss 5.31352, acc 0.546875, prec 0.0242325, recall 0.704261
2017-12-10T11:58:40.202743: step 422, loss 2.3736, acc 0.546875, prec 0.024256, recall 0.705
2017-12-10T11:58:40.654750: step 423, loss 3.82973, acc 0.578125, prec 0.0241998, recall 0.705
2017-12-10T11:58:41.095574: step 424, loss 4.71744, acc 0.453125, prec 0.0241294, recall 0.703242
2017-12-10T11:58:41.546360: step 425, loss 2.56321, acc 0.53125, prec 0.0240676, recall 0.703242
2017-12-10T11:58:41.999494: step 426, loss 2.21933, acc 0.59375, prec 0.0240143, recall 0.703242
2017-12-10T11:58:42.439868: step 427, loss 2.70675, acc 0.578125, prec 0.0240421, recall 0.70398
2017-12-10T11:58:42.890103: step 428, loss 2.20429, acc 0.625, prec 0.0239932, recall 0.70398
2017-12-10T11:58:43.339416: step 429, loss 3.28221, acc 0.4375, prec 0.0239202, recall 0.70398
2017-12-10T11:58:43.771654: step 430, loss 9.70336, acc 0.703125, prec 0.0240486, recall 0.703704
2017-12-10T11:58:44.217454: step 431, loss 2.70058, acc 0.625, prec 0.024, recall 0.703704
2017-12-10T11:58:44.662558: step 432, loss 2.30091, acc 0.65625, prec 0.0239556, recall 0.703704
2017-12-10T11:58:45.113832: step 433, loss 1.58526, acc 0.578125, prec 0.0239832, recall 0.704434
2017-12-10T11:58:45.566644: step 434, loss 0.90522, acc 0.828125, prec 0.0239611, recall 0.704434
2017-12-10T11:58:46.008173: step 435, loss 1.70734, acc 0.71875, prec 0.0240067, recall 0.70516
2017-12-10T11:58:46.458832: step 436, loss 5.78561, acc 0.5625, prec 0.0239526, recall 0.703431
2017-12-10T11:58:46.903726: step 437, loss 1.23316, acc 0.75, prec 0.024002, recall 0.704156
2017-12-10T11:58:47.344368: step 438, loss 1.70355, acc 0.6875, prec 0.0239621, recall 0.704156
2017-12-10T11:58:47.784059: step 439, loss 10.7178, acc 0.796875, prec 0.0240193, recall 0.703163
2017-12-10T11:58:48.249107: step 440, loss 0.95716, acc 0.765625, prec 0.0240704, recall 0.703883
2017-12-10T11:58:48.691416: step 441, loss 1.80208, acc 0.671875, prec 0.0241094, recall 0.704601
2017-12-10T11:58:49.131465: step 442, loss 1.26152, acc 0.6875, prec 0.0240695, recall 0.704601
2017-12-10T11:58:49.578548: step 443, loss 1.12157, acc 0.828125, prec 0.0241282, recall 0.705314
2017-12-10T11:58:50.022743: step 444, loss 2.20662, acc 0.734375, prec 0.0242554, recall 0.706731
2017-12-10T11:58:50.462093: step 445, loss 1.32278, acc 0.71875, prec 0.0242195, recall 0.706731
2017-12-10T11:58:50.905530: step 446, loss 1.70273, acc 0.75, prec 0.0243481, recall 0.708134
2017-12-10T11:58:51.356834: step 447, loss 1.41506, acc 0.71875, prec 0.0243922, recall 0.708831
2017-12-10T11:58:51.803725: step 448, loss 1.37902, acc 0.75, prec 0.0245203, recall 0.710214
2017-12-10T11:58:52.247188: step 449, loss 1.1302, acc 0.78125, prec 0.0244921, recall 0.710214
2017-12-10T11:58:52.698029: step 450, loss 1.80405, acc 0.734375, prec 0.0246176, recall 0.711584
2017-12-10T11:58:53.148510: step 451, loss 0.778812, acc 0.796875, prec 0.0245915, recall 0.711584
2017-12-10T11:58:53.592794: step 452, loss 2.34577, acc 0.75, prec 0.0245614, recall 0.709906
2017-12-10T11:58:54.032287: step 453, loss 1.27918, acc 0.765625, prec 0.0246109, recall 0.710588
2017-12-10T11:58:54.486865: step 454, loss 1.29108, acc 0.8125, prec 0.0245868, recall 0.710588
2017-12-10T11:58:54.932271: step 455, loss 5.00026, acc 0.78125, prec 0.0247195, recall 0.71028
2017-12-10T11:58:55.388248: step 456, loss 1.75247, acc 0.6875, prec 0.0246793, recall 0.71028
2017-12-10T11:58:55.836265: step 457, loss 2.90204, acc 0.6875, prec 0.0247204, recall 0.709302
2017-12-10T11:58:56.272396: step 458, loss 1.98245, acc 0.65625, prec 0.0247553, recall 0.709977
2017-12-10T11:58:56.704947: step 459, loss 2.43725, acc 0.71875, prec 0.0247981, recall 0.710648
2017-12-10T11:58:57.147140: step 460, loss 1.88704, acc 0.671875, prec 0.0247561, recall 0.710648
2017-12-10T11:58:57.593738: step 461, loss 1.40523, acc 0.75, prec 0.0247242, recall 0.710648
2017-12-10T11:58:58.041124: step 462, loss 2.17709, acc 0.640625, prec 0.0247569, recall 0.711316
2017-12-10T11:58:58.480508: step 463, loss 1.55741, acc 0.703125, prec 0.0247974, recall 0.711982
2017-12-10T11:58:58.939050: step 464, loss 3.42109, acc 0.53125, prec 0.0248939, recall 0.713303
2017-12-10T11:58:59.382162: step 465, loss 2.18518, acc 0.671875, prec 0.0248522, recall 0.713303
2017-12-10T11:58:59.815899: step 466, loss 2.51279, acc 0.71875, prec 0.0249721, recall 0.714612
2017-12-10T11:59:00.257690: step 467, loss 7.6848, acc 0.578125, prec 0.025, recall 0.712018
2017-12-10T11:59:00.696577: step 468, loss 2.67128, acc 0.578125, prec 0.0249464, recall 0.712018
2017-12-10T11:59:01.150498: step 469, loss 2.35441, acc 0.625, prec 0.0249762, recall 0.71267
2017-12-10T11:59:01.582532: step 470, loss 2.44526, acc 0.625, prec 0.0249288, recall 0.71267
2017-12-10T11:59:02.014942: step 471, loss 2.69666, acc 0.5625, prec 0.0249507, recall 0.713318
2017-12-10T11:59:02.462176: step 472, loss 2.30826, acc 0.5625, prec 0.0249724, recall 0.713964
2017-12-10T11:59:02.906067: step 473, loss 20.9036, acc 0.671875, prec 0.0250118, recall 0.711409
2017-12-10T11:59:03.344453: step 474, loss 1.72256, acc 0.671875, prec 0.0250471, recall 0.712054
2017-12-10T11:59:03.794523: step 475, loss 2.18693, acc 0.5625, prec 0.0250685, recall 0.712695
2017-12-10T11:59:04.228604: step 476, loss 4.97746, acc 0.71875, prec 0.0251115, recall 0.711752
2017-12-10T11:59:04.676515: step 477, loss 3.64589, acc 0.484375, prec 0.0250468, recall 0.711752
2017-12-10T11:59:05.114782: step 478, loss 1.87096, acc 0.578125, prec 0.0250701, recall 0.712389
2017-12-10T11:59:05.546793: step 479, loss 2.95934, acc 0.515625, prec 0.0252368, recall 0.714286
2017-12-10T11:59:05.974874: step 480, loss 2.9495, acc 0.59375, prec 0.0252615, recall 0.714912
2017-12-10T11:59:06.398309: step 481, loss 3.93445, acc 0.453125, prec 0.0252685, recall 0.715536
2017-12-10T11:59:06.846041: step 482, loss 7.61214, acc 0.453125, prec 0.0252023, recall 0.713974
2017-12-10T11:59:07.299590: step 483, loss 3.15919, acc 0.578125, prec 0.02515, recall 0.713974
2017-12-10T11:59:07.763327: step 484, loss 1.87141, acc 0.609375, prec 0.0251765, recall 0.714597
2017-12-10T11:59:08.204998: step 485, loss 3.34562, acc 0.5, prec 0.0251149, recall 0.714597
2017-12-10T11:59:08.646949: step 486, loss 1.31849, acc 0.71875, prec 0.0250803, recall 0.714597
2017-12-10T11:59:09.082524: step 487, loss 1.88041, acc 0.640625, prec 0.0250363, recall 0.714597
2017-12-10T11:59:09.529175: step 488, loss 1.65428, acc 0.671875, prec 0.0249962, recall 0.714597
2017-12-10T11:59:09.977859: step 489, loss 1.62183, acc 0.75, prec 0.02504, recall 0.715217
2017-12-10T11:59:10.422593: step 490, loss 1.25305, acc 0.71875, prec 0.0250057, recall 0.715217
2017-12-10T11:59:10.856697: step 491, loss 6.19157, acc 0.71875, prec 0.0249734, recall 0.713666
2017-12-10T11:59:11.298230: step 492, loss 1.65218, acc 0.703125, prec 0.0250114, recall 0.714286
2017-12-10T11:59:11.742411: step 493, loss 1.00203, acc 0.796875, prec 0.0250606, recall 0.714903
2017-12-10T11:59:12.192993: step 494, loss 1.43697, acc 0.71875, prec 0.0250265, recall 0.714903
2017-12-10T11:59:12.637509: step 495, loss 7.81898, acc 0.703125, prec 0.0250661, recall 0.713978
2017-12-10T11:59:13.077579: step 496, loss 1.72599, acc 0.65625, prec 0.0250245, recall 0.713978
2017-12-10T11:59:13.470124: step 497, loss 6.6582, acc 0.666667, prec 0.0250678, recall 0.713062
2017-12-10T11:59:13.927737: step 498, loss 1.15248, acc 0.78125, prec 0.0251147, recall 0.713675
2017-12-10T11:59:14.364910: step 499, loss 3.42927, acc 0.75, prec 0.0251596, recall 0.712766
2017-12-10T11:59:14.800072: step 500, loss 1.29642, acc 0.671875, prec 0.02512, recall 0.712766
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-500

2017-12-10T11:59:16.613972: step 501, loss 1.75336, acc 0.671875, prec 0.0251535, recall 0.713376
2017-12-10T11:59:17.056465: step 502, loss 2.62671, acc 0.609375, prec 0.0252521, recall 0.714588
2017-12-10T11:59:17.503386: step 503, loss 2.04288, acc 0.640625, prec 0.0254269, recall 0.716387
2017-12-10T11:59:17.942364: step 504, loss 36.5448, acc 0.625, prec 0.0254597, recall 0.7125
2017-12-10T11:59:18.390836: step 505, loss 2.51067, acc 0.578125, prec 0.025481, recall 0.713098
2017-12-10T11:59:18.843129: step 506, loss 3.44405, acc 0.46875, prec 0.025489, recall 0.713693
2017-12-10T11:59:19.290767: step 507, loss 3.1147, acc 0.46875, prec 0.025425, recall 0.713693
2017-12-10T11:59:19.730315: step 508, loss 4.13071, acc 0.4375, prec 0.0253575, recall 0.713693
2017-12-10T11:59:20.171360: step 509, loss 6.87759, acc 0.359375, prec 0.0253546, recall 0.71281
2017-12-10T11:59:20.628448: step 510, loss 3.40263, acc 0.40625, prec 0.0254268, recall 0.713992
2017-12-10T11:59:21.086699: step 511, loss 4.25885, acc 0.421875, prec 0.0254293, recall 0.714579
2017-12-10T11:59:21.555103: step 512, loss 4.63373, acc 0.390625, prec 0.0254281, recall 0.715164
2017-12-10T11:59:22.000395: step 513, loss 3.77097, acc 0.484375, prec 0.0253671, recall 0.715164
2017-12-10T11:59:22.435134: step 514, loss 4.73043, acc 0.359375, prec 0.0253623, recall 0.715746
2017-12-10T11:59:22.893332: step 515, loss 3.61178, acc 0.421875, prec 0.0253649, recall 0.716327
2017-12-10T11:59:23.334832: step 516, loss 4.4369, acc 0.390625, prec 0.0252937, recall 0.716327
2017-12-10T11:59:23.783178: step 517, loss 4.17406, acc 0.421875, prec 0.0252264, recall 0.716327
2017-12-10T11:59:24.242910: step 518, loss 2.29756, acc 0.515625, prec 0.0251703, recall 0.716327
2017-12-10T11:59:24.689004: step 519, loss 2.30359, acc 0.59375, prec 0.0251235, recall 0.716327
2017-12-10T11:59:25.137125: step 520, loss 1.86226, acc 0.625, prec 0.0250804, recall 0.716327
2017-12-10T11:59:25.583630: step 521, loss 1.2848, acc 0.796875, prec 0.0251963, recall 0.71748
2017-12-10T11:59:26.030713: step 522, loss 1.58464, acc 0.703125, prec 0.0253011, recall 0.718623
2017-12-10T11:59:26.466884: step 523, loss 2.62565, acc 0.640625, prec 0.0253984, recall 0.719758
2017-12-10T11:59:26.903468: step 524, loss 1.63892, acc 0.6875, prec 0.0254316, recall 0.720322
2017-12-10T11:59:27.343736: step 525, loss 1.97706, acc 0.765625, prec 0.0255428, recall 0.721443
2017-12-10T11:59:27.800051: step 526, loss 3.86005, acc 0.734375, prec 0.0255138, recall 0.72
2017-12-10T11:59:28.250065: step 527, loss 1.35568, acc 0.703125, prec 0.0255485, recall 0.720559
2017-12-10T11:59:28.704143: step 528, loss 3.82121, acc 0.6875, prec 0.025583, recall 0.719682
2017-12-10T11:59:29.155794: step 529, loss 1.11853, acc 0.828125, prec 0.0255632, recall 0.719682
2017-12-10T11:59:29.597966: step 530, loss 15.988, acc 0.765625, prec 0.0255379, recall 0.718254
2017-12-10T11:59:30.038395: step 531, loss 1.0114, acc 0.8125, prec 0.0255163, recall 0.718254
2017-12-10T11:59:30.474990: step 532, loss 0.747983, acc 0.8125, prec 0.025632, recall 0.719368
2017-12-10T11:59:30.909976: step 533, loss 0.931041, acc 0.828125, prec 0.0256122, recall 0.719368
2017-12-10T11:59:31.357090: step 534, loss 1.44366, acc 0.765625, prec 0.0255852, recall 0.719368
2017-12-10T11:59:31.793505: step 535, loss 1.35134, acc 0.71875, prec 0.0256212, recall 0.719921
2017-12-10T11:59:32.252382: step 536, loss 7.33724, acc 0.765625, prec 0.0256644, recall 0.719057
2017-12-10T11:59:32.703090: step 537, loss 2.65222, acc 0.609375, prec 0.0256877, recall 0.719608
2017-12-10T11:59:33.142581: step 538, loss 1.23227, acc 0.671875, prec 0.02565, recall 0.719608
2017-12-10T11:59:33.582980: step 539, loss 1.33587, acc 0.703125, prec 0.025684, recall 0.720157
2017-12-10T11:59:34.029870: step 540, loss 1.86955, acc 0.6875, prec 0.0257161, recall 0.720703
2017-12-10T11:59:34.480688: step 541, loss 1.62666, acc 0.6875, prec 0.0258837, recall 0.72233
2017-12-10T11:59:34.933217: step 542, loss 1.42168, acc 0.75, prec 0.0258549, recall 0.72233
2017-12-10T11:59:35.374541: step 543, loss 1.03988, acc 0.71875, prec 0.0258226, recall 0.72233
2017-12-10T11:59:35.811675: step 544, loss 1.03701, acc 0.75, prec 0.0258615, recall 0.722868
2017-12-10T11:59:36.249289: step 545, loss 10.306, acc 0.8125, prec 0.0259092, recall 0.722008
2017-12-10T11:59:36.696070: step 546, loss 1.74978, acc 0.703125, prec 0.02601, recall 0.723077
2017-12-10T11:59:37.142572: step 547, loss 0.872275, acc 0.796875, prec 0.0261212, recall 0.724138
2017-12-10T11:59:37.590140: step 548, loss 1.28364, acc 0.71875, prec 0.0260888, recall 0.724138
2017-12-10T11:59:38.038425: step 549, loss 0.75095, acc 0.84375, prec 0.0260708, recall 0.724138
2017-12-10T11:59:38.484936: step 550, loss 1.57889, acc 0.625, prec 0.0260277, recall 0.724138
2017-12-10T11:59:38.916332: step 551, loss 1.04624, acc 0.765625, prec 0.0260008, recall 0.724138
2017-12-10T11:59:39.365240: step 552, loss 0.709833, acc 0.84375, prec 0.025983, recall 0.724138
2017-12-10T11:59:39.801267: step 553, loss 1.01984, acc 0.78125, prec 0.0260249, recall 0.724665
2017-12-10T11:59:40.240830: step 554, loss 0.769886, acc 0.890625, prec 0.0260124, recall 0.724665
2017-12-10T11:59:40.687849: step 555, loss 0.223125, acc 0.9375, prec 0.0260052, recall 0.724665
2017-12-10T11:59:41.140271: step 556, loss 0.380737, acc 0.875, prec 0.0259909, recall 0.724665
2017-12-10T11:59:41.591919: step 557, loss 0.514627, acc 0.8125, prec 0.0259696, recall 0.724665
2017-12-10T11:59:42.032098: step 558, loss 11.9518, acc 0.84375, prec 0.0260203, recall 0.72381
2017-12-10T11:59:42.469925: step 559, loss 8.25857, acc 0.734375, prec 0.026125, recall 0.723485
2017-12-10T11:59:42.908015: step 560, loss 0.918017, acc 0.796875, prec 0.0261684, recall 0.724008
2017-12-10T11:59:43.341689: step 561, loss 1.15299, acc 0.75, prec 0.0261398, recall 0.724008
2017-12-10T11:59:43.790692: step 562, loss 0.875652, acc 0.796875, prec 0.0261166, recall 0.724008
2017-12-10T11:59:44.232840: step 563, loss 2.32768, acc 0.84375, prec 0.026167, recall 0.723164
2017-12-10T11:59:44.681401: step 564, loss 1.17714, acc 0.6875, prec 0.0261313, recall 0.723164
2017-12-10T11:59:45.130317: step 565, loss 1.10066, acc 0.734375, prec 0.0261673, recall 0.723684
2017-12-10T11:59:45.583495: step 566, loss 1.19337, acc 0.71875, prec 0.0262676, recall 0.724719
2017-12-10T11:59:46.018980: step 567, loss 12.6082, acc 0.765625, prec 0.0262426, recall 0.723364
2017-12-10T11:59:46.456885: step 568, loss 12.2246, acc 0.671875, prec 0.026339, recall 0.723048
2017-12-10T11:59:46.905040: step 569, loss 1.56839, acc 0.671875, prec 0.0263016, recall 0.723048
2017-12-10T11:59:47.351515: step 570, loss 1.99098, acc 0.65625, prec 0.026394, recall 0.724074
2017-12-10T11:59:47.798930: step 571, loss 3.16905, acc 0.5, prec 0.0263371, recall 0.724074
2017-12-10T11:59:48.247716: step 572, loss 2.71698, acc 0.5, prec 0.0262804, recall 0.724074
2017-12-10T11:59:48.693816: step 573, loss 2.8442, acc 0.5, prec 0.026224, recall 0.724074
2017-12-10T11:59:49.159839: step 574, loss 14.4029, acc 0.640625, prec 0.0262506, recall 0.723247
2017-12-10T11:59:49.598609: step 575, loss 2.98507, acc 0.484375, prec 0.0262578, recall 0.723757
2017-12-10T11:59:50.033009: step 576, loss 2.28464, acc 0.515625, prec 0.0263333, recall 0.724771
2017-12-10T11:59:50.466063: step 577, loss 2.54336, acc 0.5, prec 0.026342, recall 0.725275
2017-12-10T11:59:50.903902: step 578, loss 2.46263, acc 0.5, prec 0.0262861, recall 0.725275
2017-12-10T11:59:51.348817: step 579, loss 2.42136, acc 0.640625, prec 0.0263106, recall 0.725777
2017-12-10T11:59:51.785092: step 580, loss 2.63913, acc 0.5625, prec 0.0263262, recall 0.726277
2017-12-10T11:59:52.230831: step 581, loss 2.34408, acc 0.5625, prec 0.0263418, recall 0.726776
2017-12-10T11:59:52.673483: step 582, loss 2.33041, acc 0.59375, prec 0.0263609, recall 0.727273
2017-12-10T11:59:53.115517: step 583, loss 12.1217, acc 0.515625, prec 0.0263729, recall 0.726449
2017-12-10T11:59:53.567842: step 584, loss 2.16724, acc 0.5625, prec 0.0263244, recall 0.726449
2017-12-10T11:59:54.017851: step 585, loss 2.14309, acc 0.625, prec 0.026283, recall 0.726449
2017-12-10T11:59:54.461088: step 586, loss 1.9996, acc 0.59375, prec 0.0262383, recall 0.726449
2017-12-10T11:59:54.897149: step 587, loss 1.66383, acc 0.703125, prec 0.0262057, recall 0.726449
2017-12-10T11:59:55.337105: step 588, loss 1.2588, acc 0.640625, prec 0.0262935, recall 0.727437
2017-12-10T11:59:55.770151: step 589, loss 1.67268, acc 0.609375, prec 0.0264409, recall 0.728905
2017-12-10T11:59:56.207220: step 590, loss 7.5671, acc 0.703125, prec 0.0264733, recall 0.728086
2017-12-10T11:59:56.650326: step 591, loss 1.8771, acc 0.703125, prec 0.0266303, recall 0.729537
2017-12-10T11:59:57.103436: step 592, loss 1.43973, acc 0.734375, prec 0.0266009, recall 0.729537
2017-12-10T11:59:57.544212: step 593, loss 9.96479, acc 0.640625, prec 0.0266261, recall 0.728723
2017-12-10T11:59:57.981824: step 594, loss 1.43907, acc 0.6875, prec 0.0265916, recall 0.728723
2017-12-10T11:59:58.431066: step 595, loss 6.36096, acc 0.765625, prec 0.0266305, recall 0.727915
2017-12-10T11:59:58.866791: step 596, loss 2.32461, acc 0.625, prec 0.0265892, recall 0.727915
2017-12-10T11:59:59.296153: step 597, loss 2.09632, acc 0.640625, prec 0.0266125, recall 0.728395
2017-12-10T11:59:59.726306: step 598, loss 1.45261, acc 0.6875, prec 0.0265783, recall 0.728395
2017-12-10T12:00:00.170464: step 599, loss 2.14229, acc 0.5625, prec 0.0265305, recall 0.728395
2017-12-10T12:00:00.626896: step 600, loss 1.60999, acc 0.6875, prec 0.0264964, recall 0.728395
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-600

2017-12-10T12:00:02.759421: step 601, loss 1.80585, acc 0.609375, prec 0.026454, recall 0.728395
2017-12-10T12:00:03.188307: step 602, loss 1.73282, acc 0.578125, prec 0.0265328, recall 0.72935
2017-12-10T12:00:03.626051: step 603, loss 1.66572, acc 0.640625, prec 0.0264939, recall 0.72935
2017-12-10T12:00:04.063127: step 604, loss 1.96238, acc 0.671875, prec 0.0264584, recall 0.72935
2017-12-10T12:00:04.505140: step 605, loss 1.43773, acc 0.65625, prec 0.0264833, recall 0.729825
2017-12-10T12:00:04.947605: step 606, loss 1.89956, acc 0.640625, prec 0.0264446, recall 0.729825
2017-12-10T12:00:05.407883: step 607, loss 0.57236, acc 0.8125, prec 0.0265481, recall 0.730769
2017-12-10T12:00:05.862572: step 608, loss 0.358589, acc 0.875, prec 0.0265346, recall 0.730769
2017-12-10T12:00:06.312702: step 609, loss 0.659791, acc 0.78125, prec 0.0265728, recall 0.731239
2017-12-10T12:00:06.753242: step 610, loss 0.833815, acc 0.796875, prec 0.0265509, recall 0.731239
2017-12-10T12:00:07.210862: step 611, loss 2.40799, acc 0.8125, prec 0.0266557, recall 0.730903
2017-12-10T12:00:07.646768: step 612, loss 0.72224, acc 0.765625, prec 0.0266304, recall 0.730903
2017-12-10T12:00:08.089625: step 613, loss 0.427552, acc 0.90625, prec 0.0266203, recall 0.730903
2017-12-10T12:00:08.542644: step 614, loss 6.6727, acc 0.859375, prec 0.0266684, recall 0.730104
2017-12-10T12:00:08.999152: step 615, loss 0.594161, acc 0.84375, prec 0.026713, recall 0.73057
2017-12-10T12:00:09.449568: step 616, loss 0.825615, acc 0.953125, prec 0.0269537, recall 0.732419
2017-12-10T12:00:09.895200: step 617, loss 0.503003, acc 0.890625, prec 0.0270032, recall 0.732877
2017-12-10T12:00:10.341848: step 618, loss 0.920196, acc 0.890625, prec 0.0270526, recall 0.733333
2017-12-10T12:00:10.776855: step 619, loss 0.918467, acc 0.78125, prec 0.0272126, recall 0.734694
2017-12-10T12:00:11.219890: step 620, loss 0.711743, acc 0.90625, prec 0.0272636, recall 0.735144
2017-12-10T12:00:11.658555: step 621, loss 0.710632, acc 0.765625, prec 0.0272378, recall 0.735144
2017-12-10T12:00:12.105590: step 622, loss 1.48881, acc 0.71875, prec 0.027207, recall 0.735144
2017-12-10T12:00:12.535062: step 623, loss 0.856436, acc 0.8125, prec 0.0272476, recall 0.735593
2017-12-10T12:00:12.984795: step 624, loss 0.498007, acc 0.84375, prec 0.0272305, recall 0.735593
2017-12-10T12:00:13.432888: step 625, loss 6.81709, acc 0.71875, prec 0.0273234, recall 0.735245
2017-12-10T12:00:13.878191: step 626, loss 0.741283, acc 0.796875, prec 0.0273012, recall 0.735245
2017-12-10T12:00:14.315736: step 627, loss 0.613741, acc 0.859375, prec 0.0273467, recall 0.73569
2017-12-10T12:00:14.751107: step 628, loss 0.9965, acc 0.796875, prec 0.0273245, recall 0.73569
2017-12-10T12:00:15.198806: step 629, loss 8.59119, acc 0.78125, prec 0.027304, recall 0.733221
2017-12-10T12:00:15.642686: step 630, loss 1.48969, acc 0.71875, prec 0.027334, recall 0.733668
2017-12-10T12:00:16.085460: step 631, loss 1.58981, acc 0.796875, prec 0.0273725, recall 0.734114
2017-12-10T12:00:16.533767: step 632, loss 1.92033, acc 0.609375, prec 0.0273299, recall 0.734114
2017-12-10T12:00:16.977502: step 633, loss 1.62867, acc 0.65625, prec 0.0274134, recall 0.735
2017-12-10T12:00:17.409118: step 634, loss 1.97326, acc 0.5625, prec 0.0273658, recall 0.735
2017-12-10T12:00:17.847340: step 635, loss 1.72366, acc 0.59375, prec 0.0273217, recall 0.735
2017-12-10T12:00:18.291828: step 636, loss 1.72658, acc 0.59375, prec 0.0272778, recall 0.735
2017-12-10T12:00:18.732514: step 637, loss 1.91148, acc 0.609375, prec 0.0272357, recall 0.735
2017-12-10T12:00:19.178814: step 638, loss 1.83336, acc 0.53125, prec 0.0272453, recall 0.735441
2017-12-10T12:00:19.630243: step 639, loss 2.26129, acc 0.75, prec 0.02728, recall 0.73466
2017-12-10T12:00:20.069506: step 640, loss 1.23019, acc 0.703125, prec 0.0272481, recall 0.73466
2017-12-10T12:00:20.517269: step 641, loss 1.05644, acc 0.75, prec 0.0273409, recall 0.735537
2017-12-10T12:00:20.949660: step 642, loss 4.75919, acc 0.65625, prec 0.0274847, recall 0.735632
2017-12-10T12:00:21.388811: step 643, loss 1.75983, acc 0.578125, prec 0.0274392, recall 0.735632
2017-12-10T12:00:21.821260: step 644, loss 0.982272, acc 0.703125, prec 0.0274073, recall 0.735632
2017-12-10T12:00:22.264150: step 645, loss 1.36784, acc 0.65625, prec 0.0274299, recall 0.736066
2017-12-10T12:00:22.719272: step 646, loss 1.17125, acc 0.78125, prec 0.0275252, recall 0.736928
2017-12-10T12:00:23.168227: step 647, loss 1.7359, acc 0.703125, prec 0.0277304, recall 0.738636
2017-12-10T12:00:23.632572: step 648, loss 0.940816, acc 0.796875, prec 0.0277084, recall 0.738636
2017-12-10T12:00:24.066180: step 649, loss 1.04726, acc 0.828125, prec 0.0276899, recall 0.738636
2017-12-10T12:00:24.513704: step 650, loss 0.855134, acc 0.8125, prec 0.0277288, recall 0.73906
2017-12-10T12:00:24.947608: step 651, loss 0.663027, acc 0.84375, prec 0.0278301, recall 0.739903
2017-12-10T12:00:25.379491: step 652, loss 1.31973, acc 0.65625, prec 0.0277929, recall 0.739903
2017-12-10T12:00:25.819409: step 653, loss 0.682948, acc 0.796875, prec 0.027771, recall 0.739903
2017-12-10T12:00:26.265414: step 654, loss 1.46433, acc 0.859375, prec 0.0278165, recall 0.73913
2017-12-10T12:00:26.713337: step 655, loss 8.14601, acc 0.8125, prec 0.0279157, recall 0.738782
2017-12-10T12:00:27.155331: step 656, loss 0.440921, acc 0.84375, prec 0.0278988, recall 0.738782
2017-12-10T12:00:27.585356: step 657, loss 0.843443, acc 0.75, prec 0.0278718, recall 0.738782
2017-12-10T12:00:28.035303: step 658, loss 7.5023, acc 0.734375, prec 0.0279036, recall 0.738019
2017-12-10T12:00:28.480605: step 659, loss 0.782492, acc 0.828125, prec 0.0280024, recall 0.738854
2017-12-10T12:00:28.926205: step 660, loss 0.906815, acc 0.78125, prec 0.0279788, recall 0.738854
2017-12-10T12:00:29.367855: step 661, loss 1.04612, acc 0.75, prec 0.0279518, recall 0.738854
2017-12-10T12:00:29.805473: step 662, loss 8.14433, acc 0.71875, prec 0.0279232, recall 0.737679
2017-12-10T12:00:30.253886: step 663, loss 1.91028, acc 0.640625, prec 0.0278846, recall 0.737679
2017-12-10T12:00:30.707583: step 664, loss 1.08522, acc 0.671875, prec 0.0279078, recall 0.738095
2017-12-10T12:00:31.156364: step 665, loss 1.54317, acc 0.640625, prec 0.0279276, recall 0.73851
2017-12-10T12:00:31.600905: step 666, loss 2.55655, acc 0.609375, prec 0.0280022, recall 0.739336
2017-12-10T12:00:32.028898: step 667, loss 1.46909, acc 0.671875, prec 0.027967, recall 0.739336
2017-12-10T12:00:32.467933: step 668, loss 1.27185, acc 0.703125, prec 0.0281093, recall 0.740566
2017-12-10T12:00:32.914565: step 669, loss 1.48691, acc 0.703125, prec 0.0281354, recall 0.740973
2017-12-10T12:00:33.351561: step 670, loss 1.24992, acc 0.734375, prec 0.028107, recall 0.740973
2017-12-10T12:00:33.797767: step 671, loss 2.38666, acc 0.609375, prec 0.0280652, recall 0.740973
2017-12-10T12:00:34.236931: step 672, loss 1.91222, acc 0.671875, prec 0.0280879, recall 0.741379
2017-12-10T12:00:34.675082: step 673, loss 1.31981, acc 0.65625, prec 0.0281089, recall 0.741784
2017-12-10T12:00:35.117286: step 674, loss 1.12139, acc 0.734375, prec 0.0280806, recall 0.741784
2017-12-10T12:00:35.569132: step 675, loss 6.33247, acc 0.75, prec 0.0281132, recall 0.74103
2017-12-10T12:00:36.017331: step 676, loss 1.52807, acc 0.75, prec 0.0280866, recall 0.74103
2017-12-10T12:00:36.472754: step 677, loss 1.22439, acc 0.71875, prec 0.0280567, recall 0.74103
2017-12-10T12:00:36.918698: step 678, loss 1.09532, acc 0.71875, prec 0.0281989, recall 0.742236
2017-12-10T12:00:37.367901: step 679, loss 2.36516, acc 0.78125, prec 0.0281773, recall 0.741085
2017-12-10T12:00:37.825819: step 680, loss 0.675812, acc 0.828125, prec 0.0281591, recall 0.741085
2017-12-10T12:00:38.268695: step 681, loss 1.14093, acc 0.75, prec 0.0281325, recall 0.741085
2017-12-10T12:00:38.708339: step 682, loss 1.11927, acc 0.734375, prec 0.0281616, recall 0.741486
2017-12-10T12:00:39.162722: step 683, loss 0.696467, acc 0.765625, prec 0.0282509, recall 0.742284
2017-12-10T12:00:39.603289: step 684, loss 0.979616, acc 0.734375, prec 0.0283368, recall 0.743077
2017-12-10T12:00:40.040955: step 685, loss 8.94308, acc 0.828125, prec 0.0283201, recall 0.741935
2017-12-10T12:00:40.485807: step 686, loss 1.38015, acc 0.6875, prec 0.028287, recall 0.741935
2017-12-10T12:00:40.936824: step 687, loss 2.47692, acc 0.703125, prec 0.028314, recall 0.741194
2017-12-10T12:00:41.379341: step 688, loss 1.03268, acc 0.78125, prec 0.0282909, recall 0.741194
2017-12-10T12:00:41.820338: step 689, loss 1.19862, acc 0.78125, prec 0.0283812, recall 0.741985
2017-12-10T12:00:42.257302: step 690, loss 1.42235, acc 0.75, prec 0.0284681, recall 0.74277
2017-12-10T12:00:42.697650: step 691, loss 11.8672, acc 0.703125, prec 0.0284382, recall 0.741641
2017-12-10T12:00:43.144488: step 692, loss 0.85138, acc 0.796875, prec 0.0285298, recall 0.742424
2017-12-10T12:00:43.597043: step 693, loss 1.19941, acc 0.734375, prec 0.0285016, recall 0.742424
2017-12-10T12:00:44.048930: step 694, loss 1.40583, acc 0.6875, prec 0.0284685, recall 0.742424
2017-12-10T12:00:44.505915: step 695, loss 1.65261, acc 0.671875, prec 0.0284338, recall 0.742424
2017-12-10T12:00:44.933982: step 696, loss 1.76383, acc 0.703125, prec 0.0284025, recall 0.742424
2017-12-10T12:00:45.406042: step 697, loss 3.63239, acc 0.671875, prec 0.0284259, recall 0.741692
2017-12-10T12:00:45.853987: step 698, loss 1.84227, acc 0.671875, prec 0.0285599, recall 0.742857
2017-12-10T12:00:46.298679: step 699, loss 1.75354, acc 0.734375, prec 0.0285318, recall 0.742857
2017-12-10T12:00:46.738557: step 700, loss 1.66288, acc 0.578125, prec 0.0284874, recall 0.742857
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-700

2017-12-10T12:00:48.641595: step 701, loss 1.64238, acc 0.625, prec 0.0285599, recall 0.743628
2017-12-10T12:00:49.077883: step 702, loss 1.33432, acc 0.671875, prec 0.028693, recall 0.744776
2017-12-10T12:00:49.515790: step 703, loss 2.02991, acc 0.71875, prec 0.0286633, recall 0.744776
2017-12-10T12:00:49.957085: step 704, loss 1.36349, acc 0.5625, prec 0.028673, recall 0.745156
2017-12-10T12:00:50.398217: step 705, loss 1.75215, acc 0.71875, prec 0.0286991, recall 0.745536
2017-12-10T12:00:50.846539: step 706, loss 1.39984, acc 0.71875, prec 0.0287251, recall 0.745914
2017-12-10T12:00:51.278371: step 707, loss 0.718056, acc 0.78125, prec 0.0287576, recall 0.746291
2017-12-10T12:00:51.713212: step 708, loss 0.468278, acc 0.84375, prec 0.0287412, recall 0.746291
2017-12-10T12:00:52.154390: step 709, loss 0.899865, acc 0.8125, prec 0.0287215, recall 0.746291
2017-12-10T12:00:52.606234: step 710, loss 0.969254, acc 0.75, prec 0.0286953, recall 0.746291
2017-12-10T12:00:53.059445: step 711, loss 0.786495, acc 0.828125, prec 0.0287327, recall 0.746667
2017-12-10T12:00:53.532748: step 712, loss 0.386508, acc 0.875, prec 0.0287749, recall 0.747041
2017-12-10T12:00:53.984067: step 713, loss 0.529518, acc 0.890625, prec 0.0287635, recall 0.747041
2017-12-10T12:00:54.423973: step 714, loss 0.521242, acc 0.875, prec 0.0287504, recall 0.747041
2017-12-10T12:00:54.870240: step 715, loss 0.773516, acc 0.859375, prec 0.0287356, recall 0.747041
2017-12-10T12:00:55.309344: step 716, loss 8.96698, acc 0.921875, prec 0.0288396, recall 0.746686
2017-12-10T12:00:55.747839: step 717, loss 0.609577, acc 0.890625, prec 0.0288281, recall 0.746686
2017-12-10T12:00:56.190483: step 718, loss 0.605206, acc 0.8125, prec 0.0288085, recall 0.746686
2017-12-10T12:00:56.636251: step 719, loss 0.679125, acc 0.828125, prec 0.0287905, recall 0.746686
2017-12-10T12:00:57.064004: step 720, loss 0.45434, acc 0.890625, prec 0.0288341, recall 0.747059
2017-12-10T12:00:57.505232: step 721, loss 0.261418, acc 0.890625, prec 0.0288778, recall 0.74743
2017-12-10T12:00:57.948387: step 722, loss 0.257222, acc 0.890625, prec 0.0288663, recall 0.74743
2017-12-10T12:00:58.384971: step 723, loss 0.525524, acc 0.875, prec 0.0288532, recall 0.74743
2017-12-10T12:00:58.831450: step 724, loss 0.398317, acc 0.890625, prec 0.0288418, recall 0.74743
2017-12-10T12:00:59.273958: step 725, loss 7.99583, acc 0.921875, prec 0.0288353, recall 0.746334
2017-12-10T12:00:59.718103: step 726, loss 0.138086, acc 0.9375, prec 0.0288837, recall 0.746706
2017-12-10T12:01:00.153510: step 727, loss 0.251356, acc 0.890625, prec 0.0288723, recall 0.746706
2017-12-10T12:01:00.585962: step 728, loss 0.637528, acc 0.8125, prec 0.0288527, recall 0.746706
2017-12-10T12:01:01.027959: step 729, loss 2.25969, acc 0.859375, prec 0.0288396, recall 0.745614
2017-12-10T12:01:01.474249: step 730, loss 0.561606, acc 0.84375, prec 0.028988, recall 0.746725
2017-12-10T12:01:01.929123: step 731, loss 10.0457, acc 0.84375, prec 0.0289749, recall 0.744557
2017-12-10T12:01:02.373694: step 732, loss 0.638373, acc 0.796875, prec 0.0290084, recall 0.744928
2017-12-10T12:01:02.810599: step 733, loss 0.579746, acc 0.859375, prec 0.0290485, recall 0.745297
2017-12-10T12:01:03.259656: step 734, loss 0.62303, acc 0.78125, prec 0.0290803, recall 0.745665
2017-12-10T12:01:03.710436: step 735, loss 1.18038, acc 0.71875, prec 0.0291054, recall 0.746032
2017-12-10T12:01:04.159748: step 736, loss 1.19571, acc 0.78125, prec 0.0290825, recall 0.746032
2017-12-10T12:01:04.601994: step 737, loss 0.966752, acc 0.71875, prec 0.0291622, recall 0.746763
2017-12-10T12:01:05.040377: step 738, loss 1.07294, acc 0.703125, prec 0.0291311, recall 0.746763
2017-12-10T12:01:05.483564: step 739, loss 1.78869, acc 0.75, prec 0.0293227, recall 0.748212
2017-12-10T12:01:05.919453: step 740, loss 1.99268, acc 0.625, prec 0.0293377, recall 0.748571
2017-12-10T12:01:06.356343: step 741, loss 1.3201, acc 0.671875, prec 0.0293575, recall 0.74893
2017-12-10T12:01:06.800234: step 742, loss 4.84081, acc 0.703125, prec 0.029328, recall 0.747863
2017-12-10T12:01:07.242640: step 743, loss 2.39817, acc 0.65625, prec 0.0294003, recall 0.74858
2017-12-10T12:01:07.685986: step 744, loss 1.44205, acc 0.703125, prec 0.0293691, recall 0.74858
2017-12-10T12:01:08.130195: step 745, loss 2.68184, acc 0.546875, prec 0.0294298, recall 0.749292
2017-12-10T12:01:08.578403: step 746, loss 2.15821, acc 0.734375, prec 0.0295098, recall 0.75
2017-12-10T12:01:09.033518: step 747, loss 1.42072, acc 0.625, prec 0.0294705, recall 0.75
2017-12-10T12:01:09.477983: step 748, loss 1.87323, acc 0.640625, prec 0.0294868, recall 0.750353
2017-12-10T12:01:09.927091: step 749, loss 2.3373, acc 0.5625, prec 0.0294411, recall 0.750353
2017-12-10T12:01:10.354318: step 750, loss 1.80073, acc 0.578125, prec 0.0294508, recall 0.750704
2017-12-10T12:01:10.791792: step 751, loss 0.861725, acc 0.796875, prec 0.0294296, recall 0.750704
2017-12-10T12:01:11.224717: step 752, loss 0.944144, acc 0.75, prec 0.0294037, recall 0.750704
2017-12-10T12:01:11.659782: step 753, loss 1.23253, acc 0.671875, prec 0.0293696, recall 0.750704
2017-12-10T12:01:12.093204: step 754, loss 1.08887, acc 0.8125, prec 0.0294571, recall 0.751405
2017-12-10T12:01:12.532431: step 755, loss 1.83533, acc 0.78125, prec 0.0295412, recall 0.752101
2017-12-10T12:01:12.976142: step 756, loss 1.07666, acc 0.8125, prec 0.0296817, recall 0.753138
2017-12-10T12:01:13.422836: step 757, loss 0.931102, acc 0.8125, prec 0.0296622, recall 0.753138
2017-12-10T12:01:13.860001: step 758, loss 3.31347, acc 0.796875, prec 0.0298557, recall 0.753463
2017-12-10T12:01:14.292923: step 759, loss 3.25204, acc 0.8125, prec 0.0298909, recall 0.752762
2017-12-10T12:01:14.739424: step 760, loss 0.749572, acc 0.8125, prec 0.0299775, recall 0.753444
2017-12-10T12:01:15.197357: step 761, loss 1.01715, acc 0.78125, prec 0.0300077, recall 0.753783
2017-12-10T12:01:15.636160: step 762, loss 1.88446, acc 0.625, prec 0.0299683, recall 0.753783
2017-12-10T12:01:16.079450: step 763, loss 0.825605, acc 0.78125, prec 0.0299454, recall 0.753783
2017-12-10T12:01:16.517582: step 764, loss 1.07222, acc 0.734375, prec 0.0299705, recall 0.754121
2017-12-10T12:01:16.956016: step 765, loss 2.37683, acc 0.765625, prec 0.0299476, recall 0.753086
2017-12-10T12:01:17.393100: step 766, loss 1.63473, acc 0.8125, prec 0.029928, recall 0.753086
2017-12-10T12:01:17.835730: step 767, loss 1.29909, acc 0.671875, prec 0.0298938, recall 0.753086
2017-12-10T12:01:18.284654: step 768, loss 0.826665, acc 0.71875, prec 0.0298645, recall 0.753086
2017-12-10T12:01:18.718366: step 769, loss 0.81698, acc 0.828125, prec 0.0298467, recall 0.753086
2017-12-10T12:01:19.158710: step 770, loss 1.13752, acc 0.8125, prec 0.0298272, recall 0.753086
2017-12-10T12:01:19.591806: step 771, loss 0.932332, acc 0.703125, prec 0.0298491, recall 0.753425
2017-12-10T12:01:20.022680: step 772, loss 0.809007, acc 0.78125, prec 0.0298791, recall 0.753762
2017-12-10T12:01:20.463170: step 773, loss 1.19725, acc 0.6875, prec 0.0298467, recall 0.753762
2017-12-10T12:01:20.911819: step 774, loss 6.54225, acc 0.84375, prec 0.0298322, recall 0.752732
2017-12-10T12:01:21.365232: step 775, loss 4.72995, acc 0.8125, prec 0.029816, recall 0.750681
2017-12-10T12:01:21.805668: step 776, loss 1.38964, acc 0.71875, prec 0.0298395, recall 0.75102
2017-12-10T12:01:22.261092: step 777, loss 1.26769, acc 0.765625, prec 0.0298153, recall 0.75102
2017-12-10T12:01:22.700613: step 778, loss 1.4235, acc 0.703125, prec 0.0297847, recall 0.75102
2017-12-10T12:01:23.148796: step 779, loss 1.27517, acc 0.71875, prec 0.0298081, recall 0.751359
2017-12-10T12:01:23.595668: step 780, loss 1.58065, acc 0.671875, prec 0.0298266, recall 0.751696
2017-12-10T12:01:24.036847: step 781, loss 0.88247, acc 0.828125, prec 0.0299134, recall 0.752368
2017-12-10T12:01:24.488931: step 782, loss 2.33384, acc 0.625, prec 0.0298748, recall 0.752368
2017-12-10T12:01:24.937587: step 783, loss 1.32246, acc 0.734375, prec 0.0298475, recall 0.752368
2017-12-10T12:01:25.385873: step 784, loss 1.63911, acc 0.671875, prec 0.0298139, recall 0.752368
2017-12-10T12:01:25.832425: step 785, loss 1.67828, acc 0.625, prec 0.0297756, recall 0.752368
2017-12-10T12:01:26.275146: step 786, loss 1.36691, acc 0.75, prec 0.029802, recall 0.752703
2017-12-10T12:01:26.714111: step 787, loss 1.80198, acc 0.75, prec 0.0297765, recall 0.752703
2017-12-10T12:01:27.150409: step 788, loss 1.0863, acc 0.78125, prec 0.0298579, recall 0.753369
2017-12-10T12:01:27.620433: step 789, loss 1.29542, acc 0.71875, prec 0.029881, recall 0.753701
2017-12-10T12:01:28.071782: step 790, loss 0.925874, acc 0.78125, prec 0.0299104, recall 0.754032
2017-12-10T12:01:28.509661: step 791, loss 0.917465, acc 0.8125, prec 0.029943, recall 0.754362
2017-12-10T12:01:28.944594: step 792, loss 0.87503, acc 0.859375, prec 0.0299286, recall 0.754362
2017-12-10T12:01:29.390420: step 793, loss 0.794255, acc 0.859375, prec 0.0299143, recall 0.754362
2017-12-10T12:01:29.839561: step 794, loss 0.221193, acc 0.890625, prec 0.0299548, recall 0.754692
2017-12-10T12:01:30.285813: step 795, loss 3.96947, acc 0.90625, prec 0.0299468, recall 0.753681
2017-12-10T12:01:30.726434: step 796, loss 0.522409, acc 0.859375, prec 0.0299325, recall 0.753681
2017-12-10T12:01:31.169184: step 797, loss 3.45103, acc 0.890625, prec 0.0299229, recall 0.752674
2017-12-10T12:01:31.608494: step 798, loss 0.381367, acc 0.90625, prec 0.0300165, recall 0.753333
2017-12-10T12:01:32.058475: step 799, loss 0.526256, acc 0.859375, prec 0.0301051, recall 0.753989
2017-12-10T12:01:32.494991: step 800, loss 0.299958, acc 0.890625, prec 0.0300939, recall 0.753989
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-800

2017-12-10T12:01:34.527926: step 801, loss 0.530407, acc 0.859375, prec 0.030131, recall 0.754316
2017-12-10T12:01:34.975614: step 802, loss 0.45585, acc 0.90625, prec 0.0302243, recall 0.754967
2017-12-10T12:01:35.411282: step 803, loss 10.4535, acc 0.75, prec 0.0302517, recall 0.754293
2017-12-10T12:01:35.864324: step 804, loss 0.826663, acc 0.90625, prec 0.0302934, recall 0.754617
2017-12-10T12:01:36.308903: step 805, loss 0.566937, acc 0.875, prec 0.0302806, recall 0.754617
2017-12-10T12:01:36.743082: step 806, loss 5.47752, acc 0.734375, prec 0.0303062, recall 0.753947
2017-12-10T12:01:37.198945: step 807, loss 0.679875, acc 0.84375, prec 0.0302902, recall 0.753947
2017-12-10T12:01:37.637751: step 808, loss 1.21427, acc 0.734375, prec 0.0303142, recall 0.754271
2017-12-10T12:01:38.083601: step 809, loss 1.5161, acc 0.734375, prec 0.030287, recall 0.754271
2017-12-10T12:01:38.531693: step 810, loss 1.05775, acc 0.734375, prec 0.0302599, recall 0.754271
2017-12-10T12:01:38.966458: step 811, loss 1.45048, acc 0.671875, prec 0.0302264, recall 0.754271
2017-12-10T12:01:39.416323: step 812, loss 2.04097, acc 0.578125, prec 0.0301835, recall 0.754271
2017-12-10T12:01:39.881491: step 813, loss 1.25492, acc 0.78125, prec 0.0301613, recall 0.754271
2017-12-10T12:01:40.329897: step 814, loss 1.55417, acc 0.6875, prec 0.0301297, recall 0.754271
2017-12-10T12:01:40.771767: step 815, loss 0.668426, acc 0.8125, prec 0.0301107, recall 0.754271
2017-12-10T12:01:41.210444: step 816, loss 1.48775, acc 0.734375, prec 0.0301347, recall 0.754593
2017-12-10T12:01:41.662317: step 817, loss 1.0104, acc 0.765625, prec 0.030111, recall 0.754593
2017-12-10T12:01:42.109718: step 818, loss 1.80458, acc 0.796875, prec 0.0300921, recall 0.753604
2017-12-10T12:01:42.558783: step 819, loss 0.803068, acc 0.75, prec 0.0301176, recall 0.753927
2017-12-10T12:01:42.992859: step 820, loss 7.96241, acc 0.78125, prec 0.0300988, recall 0.751958
2017-12-10T12:01:43.441177: step 821, loss 1.21579, acc 0.765625, prec 0.0300752, recall 0.751958
2017-12-10T12:01:43.888008: step 822, loss 0.806579, acc 0.765625, prec 0.0300517, recall 0.751958
2017-12-10T12:01:44.331250: step 823, loss 0.928575, acc 0.828125, prec 0.0300344, recall 0.751958
2017-12-10T12:01:44.773994: step 824, loss 1.07138, acc 0.734375, prec 0.0300078, recall 0.751958
2017-12-10T12:01:45.206514: step 825, loss 1.07905, acc 0.765625, prec 0.0299844, recall 0.751958
2017-12-10T12:01:45.628171: step 826, loss 0.685399, acc 0.8125, prec 0.0299657, recall 0.751958
2017-12-10T12:01:46.069709: step 827, loss 1.45256, acc 0.703125, prec 0.0300873, recall 0.752926
2017-12-10T12:01:46.530624: step 828, loss 0.685625, acc 0.828125, prec 0.0300701, recall 0.752926
2017-12-10T12:01:46.972617: step 829, loss 0.558481, acc 0.875, prec 0.0300576, recall 0.752926
2017-12-10T12:01:47.439556: step 830, loss 0.79589, acc 0.8125, prec 0.0300389, recall 0.752926
2017-12-10T12:01:47.896247: step 831, loss 3.39868, acc 0.875, prec 0.0300783, recall 0.75227
2017-12-10T12:01:48.351661: step 832, loss 1.13342, acc 0.75, prec 0.0300534, recall 0.75227
2017-12-10T12:01:48.793087: step 833, loss 1.10492, acc 0.734375, prec 0.0300771, recall 0.752591
2017-12-10T12:01:49.245759: step 834, loss 1.32588, acc 0.734375, prec 0.0301009, recall 0.752911
2017-12-10T12:01:49.686043: step 835, loss 8.10521, acc 0.8125, prec 0.0300837, recall 0.751938
2017-12-10T12:01:50.126838: step 836, loss 0.723129, acc 0.796875, prec 0.0301136, recall 0.752258
2017-12-10T12:01:50.575405: step 837, loss 0.504402, acc 0.890625, prec 0.0301028, recall 0.752258
2017-12-10T12:01:51.027710: step 838, loss 0.945639, acc 0.78125, prec 0.030131, recall 0.752577
2017-12-10T12:01:51.468757: step 839, loss 1.03864, acc 0.75, prec 0.0301062, recall 0.752577
2017-12-10T12:01:51.907051: step 840, loss 1.37508, acc 0.734375, prec 0.0301298, recall 0.752896
2017-12-10T12:01:52.334089: step 841, loss 1.03767, acc 0.78125, prec 0.0302079, recall 0.75353
2017-12-10T12:01:52.773725: step 842, loss 0.521253, acc 0.84375, prec 0.0301924, recall 0.75353
2017-12-10T12:01:53.228592: step 843, loss 0.94055, acc 0.796875, prec 0.0302719, recall 0.754161
2017-12-10T12:01:53.681358: step 844, loss 10.2261, acc 0.84375, prec 0.0303575, recall 0.753827
2017-12-10T12:01:54.136800: step 845, loss 12.4978, acc 0.8125, prec 0.0303901, recall 0.753181
2017-12-10T12:01:54.583398: step 846, loss 0.499815, acc 0.828125, prec 0.030373, recall 0.753181
2017-12-10T12:01:55.028184: step 847, loss 1.53634, acc 0.71875, prec 0.0303947, recall 0.753494
2017-12-10T12:01:55.470389: step 848, loss 1.33894, acc 0.625, prec 0.0303573, recall 0.753494
2017-12-10T12:01:55.903706: step 849, loss 1.17087, acc 0.765625, prec 0.030334, recall 0.753494
2017-12-10T12:01:56.343970: step 850, loss 1.25587, acc 0.71875, prec 0.0303061, recall 0.753494
2017-12-10T12:01:56.778808: step 851, loss 1.32574, acc 0.578125, prec 0.0302644, recall 0.753494
2017-12-10T12:01:57.225295: step 852, loss 1.22312, acc 0.6875, prec 0.0302335, recall 0.753494
2017-12-10T12:01:57.670930: step 853, loss 1.41036, acc 0.59375, prec 0.0301935, recall 0.753494
2017-12-10T12:01:58.108006: step 854, loss 2.11597, acc 0.578125, prec 0.030152, recall 0.753494
2017-12-10T12:01:58.554429: step 855, loss 0.833779, acc 0.75, prec 0.0301768, recall 0.753807
2017-12-10T12:01:58.986485: step 856, loss 10.9901, acc 0.859375, prec 0.030263, recall 0.753477
2017-12-10T12:01:59.434984: step 857, loss 1.48155, acc 0.65625, prec 0.0302293, recall 0.753477
2017-12-10T12:01:59.872471: step 858, loss 0.991256, acc 0.765625, prec 0.0302063, recall 0.753477
2017-12-10T12:02:00.316746: step 859, loss 1.37465, acc 0.65625, prec 0.0301726, recall 0.753477
2017-12-10T12:02:00.776471: step 860, loss 0.987111, acc 0.765625, prec 0.0303459, recall 0.754717
2017-12-10T12:02:01.208309: step 861, loss 1.11523, acc 0.734375, prec 0.0303199, recall 0.754717
2017-12-10T12:02:01.672584: step 862, loss 0.755567, acc 0.828125, prec 0.030401, recall 0.755332
2017-12-10T12:02:02.124809: step 863, loss 1.80982, acc 0.828125, prec 0.0303856, recall 0.754386
2017-12-10T12:02:02.563239: step 864, loss 1.11271, acc 0.75, prec 0.0304589, recall 0.755
2017-12-10T12:02:03.012675: step 865, loss 7.50808, acc 0.8125, prec 0.0304909, recall 0.754364
2017-12-10T12:02:03.458372: step 866, loss 2.26953, acc 0.75, prec 0.0304678, recall 0.753425
2017-12-10T12:02:03.898239: step 867, loss 0.929675, acc 0.671875, prec 0.0304357, recall 0.753425
2017-12-10T12:02:04.342834: step 868, loss 5.57307, acc 0.71875, prec 0.0304097, recall 0.752488
2017-12-10T12:02:04.787950: step 869, loss 4.94872, acc 0.578125, prec 0.0304186, recall 0.751861
2017-12-10T12:02:05.232173: step 870, loss 6.90674, acc 0.578125, prec 0.0304276, recall 0.751238
2017-12-10T12:02:05.681072: step 871, loss 2.48526, acc 0.515625, prec 0.0304289, recall 0.751545
2017-12-10T12:02:06.131231: step 872, loss 2.88122, acc 0.5, prec 0.0304771, recall 0.752158
2017-12-10T12:02:06.571063: step 873, loss 2.723, acc 0.515625, prec 0.0304784, recall 0.752463
2017-12-10T12:02:07.021857: step 874, loss 2.94928, acc 0.421875, prec 0.0304705, recall 0.752768
2017-12-10T12:02:07.472223: step 875, loss 4.39595, acc 0.3125, prec 0.0304039, recall 0.752768
2017-12-10T12:02:07.912284: step 876, loss 4.11272, acc 0.296875, prec 0.0303361, recall 0.752768
2017-12-10T12:02:08.347305: step 877, loss 2.91324, acc 0.46875, prec 0.030381, recall 0.753374
2017-12-10T12:02:08.786834: step 878, loss 2.97099, acc 0.59375, prec 0.030342, recall 0.753374
2017-12-10T12:02:09.230413: step 879, loss 4.33635, acc 0.390625, prec 0.0302836, recall 0.753374
2017-12-10T12:02:09.675297: step 880, loss 3.25892, acc 0.546875, prec 0.0302881, recall 0.753676
2017-12-10T12:02:10.118959: step 881, loss 1.58906, acc 0.703125, prec 0.0303552, recall 0.754279
2017-12-10T12:02:10.552537: step 882, loss 3.04748, acc 0.5, prec 0.0303075, recall 0.754279
2017-12-10T12:02:10.986945: step 883, loss 1.41183, acc 0.65625, prec 0.0303699, recall 0.754878
2017-12-10T12:02:11.423921: step 884, loss 1.1289, acc 0.734375, prec 0.0304397, recall 0.755474
2017-12-10T12:02:11.869888: step 885, loss 2.09103, acc 0.578125, prec 0.0303995, recall 0.755474
2017-12-10T12:02:12.317589: step 886, loss 3.48291, acc 0.78125, prec 0.0303801, recall 0.754556
2017-12-10T12:02:12.761894: step 887, loss 1.28524, acc 0.796875, prec 0.0303608, recall 0.754556
2017-12-10T12:02:13.204015: step 888, loss 0.646008, acc 0.84375, prec 0.0303934, recall 0.754854
2017-12-10T12:02:13.637593: step 889, loss 0.79887, acc 0.8125, prec 0.0304229, recall 0.755152
2017-12-10T12:02:14.082286: step 890, loss 0.85912, acc 0.859375, prec 0.0304095, recall 0.755152
2017-12-10T12:02:14.532722: step 891, loss 0.514798, acc 0.859375, prec 0.0303962, recall 0.755152
2017-12-10T12:02:14.975044: step 892, loss 5.24196, acc 0.875, prec 0.0303858, recall 0.754237
2017-12-10T12:02:15.413347: step 893, loss 0.546717, acc 0.84375, prec 0.0304655, recall 0.754831
2017-12-10T12:02:15.852567: step 894, loss 3.79156, acc 0.765625, prec 0.030492, recall 0.754217
2017-12-10T12:02:16.307485: step 895, loss 0.335219, acc 0.859375, prec 0.0305258, recall 0.754513
2017-12-10T12:02:16.751306: step 896, loss 0.959765, acc 0.75, prec 0.0305492, recall 0.754808
2017-12-10T12:02:17.190314: step 897, loss 1.15149, acc 0.765625, prec 0.0305269, recall 0.754808
2017-12-10T12:02:17.619627: step 898, loss 0.32289, acc 0.890625, prec 0.0305165, recall 0.754808
2017-12-10T12:02:18.068412: step 899, loss 0.343836, acc 0.859375, prec 0.0305032, recall 0.754808
2017-12-10T12:02:18.507458: step 900, loss 0.494859, acc 0.828125, prec 0.0304869, recall 0.754808
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-900

2017-12-10T12:02:20.412736: step 901, loss 0.862197, acc 0.796875, prec 0.0305147, recall 0.755102
2017-12-10T12:02:20.858651: step 902, loss 1.14335, acc 0.78125, prec 0.030541, recall 0.755396
2017-12-10T12:02:21.311828: step 903, loss 0.664017, acc 0.84375, prec 0.0305732, recall 0.755689
2017-12-10T12:02:21.768947: step 904, loss 3.30587, acc 0.796875, prec 0.0306493, recall 0.75537
2017-12-10T12:02:22.221702: step 905, loss 0.676646, acc 0.859375, prec 0.0306829, recall 0.755661
2017-12-10T12:02:22.666959: step 906, loss 3.95659, acc 0.8125, prec 0.0307134, recall 0.755054
2017-12-10T12:02:23.113512: step 907, loss 0.895006, acc 0.8125, prec 0.0306956, recall 0.755054
2017-12-10T12:02:23.561827: step 908, loss 0.905203, acc 0.765625, prec 0.0307202, recall 0.755344
2017-12-10T12:02:23.998447: step 909, loss 1.28542, acc 0.71875, prec 0.0306935, recall 0.755344
2017-12-10T12:02:24.448324: step 910, loss 0.896604, acc 0.734375, prec 0.0306683, recall 0.755344
2017-12-10T12:02:24.895790: step 911, loss 0.919412, acc 0.75, prec 0.0306914, recall 0.755635
2017-12-10T12:02:25.329798: step 912, loss 1.22939, acc 0.765625, prec 0.0306692, recall 0.755635
2017-12-10T12:02:25.775107: step 913, loss 0.642637, acc 0.859375, prec 0.0307492, recall 0.756213
2017-12-10T12:02:26.212798: step 914, loss 2.42592, acc 0.734375, prec 0.0307722, recall 0.755608
2017-12-10T12:02:26.661898: step 915, loss 1.96551, acc 0.65625, prec 0.0308328, recall 0.756184
2017-12-10T12:02:27.112802: step 916, loss 1.69427, acc 0.65625, prec 0.0308467, recall 0.756471
2017-12-10T12:02:27.554866: step 917, loss 0.564055, acc 0.8125, prec 0.0308754, recall 0.756757
2017-12-10T12:02:27.996654: step 918, loss 1.35454, acc 0.796875, prec 0.0309955, recall 0.757611
2017-12-10T12:02:28.445398: step 919, loss 0.840543, acc 0.796875, prec 0.031069, recall 0.758178
2017-12-10T12:02:28.885623: step 920, loss 1.21054, acc 0.796875, prec 0.031096, recall 0.75846
2017-12-10T12:02:29.336550: step 921, loss 7.87497, acc 0.765625, prec 0.0311678, recall 0.75814
2017-12-10T12:02:29.780131: step 922, loss 1.11697, acc 0.734375, prec 0.0311888, recall 0.75842
2017-12-10T12:02:30.234985: step 923, loss 1.20485, acc 0.796875, prec 0.0313082, recall 0.759259
2017-12-10T12:02:30.700164: step 924, loss 1.41562, acc 0.71875, prec 0.0312813, recall 0.759259
2017-12-10T12:02:31.144417: step 925, loss 0.596584, acc 0.90625, prec 0.0314109, recall 0.760092
2017-12-10T12:02:31.586399: step 926, loss 0.682569, acc 0.796875, prec 0.0313914, recall 0.760092
2017-12-10T12:02:32.041898: step 927, loss 1.15648, acc 0.78125, prec 0.0313705, recall 0.760092
2017-12-10T12:02:32.479934: step 928, loss 0.916554, acc 0.765625, prec 0.0314863, recall 0.76092
2017-12-10T12:02:32.917795: step 929, loss 0.789265, acc 0.78125, prec 0.0314654, recall 0.76092
2017-12-10T12:02:33.361985: step 930, loss 0.882689, acc 0.8125, prec 0.0314934, recall 0.761194
2017-12-10T12:02:33.810815: step 931, loss 5.16544, acc 0.84375, prec 0.03148, recall 0.760321
2017-12-10T12:02:34.253648: step 932, loss 0.922015, acc 0.765625, prec 0.0314576, recall 0.760321
2017-12-10T12:02:34.688137: step 933, loss 0.77995, acc 0.828125, prec 0.0314412, recall 0.760321
2017-12-10T12:02:35.134096: step 934, loss 0.735602, acc 0.84375, prec 0.0315181, recall 0.76087
2017-12-10T12:02:35.582775: step 935, loss 0.576945, acc 0.828125, prec 0.0315475, recall 0.761143
2017-12-10T12:02:36.024380: step 936, loss 0.48561, acc 0.859375, prec 0.0315799, recall 0.761416
2017-12-10T12:02:36.457411: step 937, loss 1.21927, acc 0.71875, prec 0.0315531, recall 0.761416
2017-12-10T12:02:36.903247: step 938, loss 0.230174, acc 0.921875, prec 0.0315456, recall 0.761416
2017-12-10T12:02:37.353009: step 939, loss 0.38025, acc 0.890625, prec 0.0316267, recall 0.761959
2017-12-10T12:02:37.802181: step 940, loss 3.62452, acc 0.8125, prec 0.031656, recall 0.761364
2017-12-10T12:02:38.247541: step 941, loss 15.692, acc 0.84375, prec 0.0316426, recall 0.760499
2017-12-10T12:02:38.701525: step 942, loss 0.995351, acc 0.75, prec 0.0316187, recall 0.760499
2017-12-10T12:02:39.145660: step 943, loss 1.0582, acc 0.75, prec 0.0316405, recall 0.760771
2017-12-10T12:02:39.575535: step 944, loss 1.66975, acc 0.75, prec 0.0316623, recall 0.761042
2017-12-10T12:02:40.011636: step 945, loss 0.904964, acc 0.78125, prec 0.0317326, recall 0.761582
2017-12-10T12:02:40.460340: step 946, loss 3.53816, acc 0.703125, prec 0.0318423, recall 0.76153
2017-12-10T12:02:40.907716: step 947, loss 2.38781, acc 0.640625, prec 0.0318534, recall 0.761798
2017-12-10T12:02:41.349100: step 948, loss 4.03553, acc 0.734375, prec 0.0318295, recall 0.760943
2017-12-10T12:02:41.794900: step 949, loss 1.78479, acc 0.6875, prec 0.0319358, recall 0.761745
2017-12-10T12:02:42.243575: step 950, loss 1.45424, acc 0.671875, prec 0.0319498, recall 0.762011
2017-12-10T12:02:42.677769: step 951, loss 1.74568, acc 0.59375, prec 0.0319109, recall 0.762011
2017-12-10T12:02:43.112349: step 952, loss 2.18313, acc 0.546875, prec 0.0319581, recall 0.762542
2017-12-10T12:02:43.551976: step 953, loss 6.76472, acc 0.53125, prec 0.0319601, recall 0.761958
2017-12-10T12:02:43.999060: step 954, loss 3.18151, acc 0.34375, prec 0.0318976, recall 0.761958
2017-12-10T12:02:44.433429: step 955, loss 3.4482, acc 0.375, prec 0.0318383, recall 0.761958
2017-12-10T12:02:44.880016: step 956, loss 2.71708, acc 0.515625, prec 0.0317924, recall 0.761958
2017-12-10T12:02:45.321041: step 957, loss 3.06027, acc 0.5, prec 0.0317453, recall 0.761958
2017-12-10T12:02:45.756377: step 958, loss 3.15759, acc 0.484375, prec 0.0316968, recall 0.761958
2017-12-10T12:02:46.188104: step 959, loss 2.29648, acc 0.5625, prec 0.0317006, recall 0.762222
2017-12-10T12:02:46.633162: step 960, loss 8.76241, acc 0.5625, prec 0.0317057, recall 0.761641
2017-12-10T12:02:47.082233: step 961, loss 2.73015, acc 0.515625, prec 0.0316604, recall 0.761641
2017-12-10T12:02:47.526157: step 962, loss 2.38764, acc 0.5625, prec 0.0316196, recall 0.761641
2017-12-10T12:02:47.975944: step 963, loss 1.78904, acc 0.578125, prec 0.0316249, recall 0.761905
2017-12-10T12:02:48.414585: step 964, loss 2.42135, acc 0.640625, prec 0.0315915, recall 0.761905
2017-12-10T12:02:48.845289: step 965, loss 2.09748, acc 0.5625, prec 0.0315509, recall 0.761905
2017-12-10T12:02:49.292114: step 966, loss 4.80703, acc 0.546875, prec 0.0315548, recall 0.761326
2017-12-10T12:02:49.729862: step 967, loss 1.75163, acc 0.609375, prec 0.0315631, recall 0.761589
2017-12-10T12:02:50.175989: step 968, loss 1.37707, acc 0.65625, prec 0.0316641, recall 0.762376
2017-12-10T12:02:50.625338: step 969, loss 1.40235, acc 0.734375, prec 0.0316395, recall 0.762376
2017-12-10T12:02:51.090885: step 970, loss 1.48018, acc 0.703125, prec 0.0316121, recall 0.762376
2017-12-10T12:02:51.537151: step 971, loss 1.15255, acc 0.78125, prec 0.031636, recall 0.762637
2017-12-10T12:02:51.995666: step 972, loss 0.987723, acc 0.75, prec 0.031613, recall 0.762637
2017-12-10T12:02:52.445819: step 973, loss 1.09402, acc 0.734375, prec 0.0316767, recall 0.763158
2017-12-10T12:02:52.888531: step 974, loss 1.38237, acc 0.734375, prec 0.0316522, recall 0.763158
2017-12-10T12:02:53.326509: step 975, loss 0.479151, acc 0.859375, prec 0.0316392, recall 0.763158
2017-12-10T12:02:53.773884: step 976, loss 0.828177, acc 0.8125, prec 0.031622, recall 0.763158
2017-12-10T12:02:54.218365: step 977, loss 13.3209, acc 0.890625, prec 0.0316148, recall 0.761488
2017-12-10T12:02:54.680006: step 978, loss 0.659551, acc 0.8125, prec 0.0316415, recall 0.761749
2017-12-10T12:02:55.124471: step 979, loss 0.822737, acc 0.859375, prec 0.0316286, recall 0.761749
2017-12-10T12:02:55.570244: step 980, loss 10.4912, acc 0.84375, prec 0.0316596, recall 0.761178
2017-12-10T12:02:56.014104: step 981, loss 0.419438, acc 0.796875, prec 0.0316849, recall 0.761438
2017-12-10T12:02:56.463289: step 982, loss 1.30194, acc 0.796875, prec 0.0317101, recall 0.761697
2017-12-10T12:02:56.909517: step 983, loss 0.841129, acc 0.78125, prec 0.0317338, recall 0.761957
2017-12-10T12:02:57.353582: step 984, loss 1.26724, acc 0.765625, prec 0.0317561, recall 0.762215
2017-12-10T12:02:57.796307: step 985, loss 3.29256, acc 0.75, prec 0.0317346, recall 0.761388
2017-12-10T12:02:58.244457: step 986, loss 0.860714, acc 0.75, prec 0.0317554, recall 0.761647
2017-12-10T12:02:58.679679: step 987, loss 1.59962, acc 0.671875, prec 0.0317253, recall 0.761647
2017-12-10T12:02:59.115345: step 988, loss 0.62539, acc 0.828125, prec 0.0317969, recall 0.762162
2017-12-10T12:02:59.546364: step 989, loss 0.725888, acc 0.796875, prec 0.0319091, recall 0.762931
2017-12-10T12:02:59.994000: step 990, loss 1.2371, acc 0.78125, prec 0.0319326, recall 0.763186
2017-12-10T12:03:00.422720: step 991, loss 1.40707, acc 0.703125, prec 0.0319053, recall 0.763186
2017-12-10T12:03:00.871352: step 992, loss 1.35241, acc 0.625, prec 0.0319579, recall 0.763695
2017-12-10T12:03:01.313626: step 993, loss 1.1252, acc 0.671875, prec 0.0320582, recall 0.764454
2017-12-10T12:03:01.715602: step 994, loss 1.03329, acc 0.745098, prec 0.0320395, recall 0.764454
2017-12-10T12:03:02.173914: step 995, loss 0.991395, acc 0.796875, prec 0.0320208, recall 0.764454
2017-12-10T12:03:02.610856: step 996, loss 1.16918, acc 0.859375, prec 0.0321381, recall 0.765208
2017-12-10T12:03:03.046844: step 997, loss 0.591408, acc 0.78125, prec 0.0321613, recall 0.765458
2017-12-10T12:03:03.490616: step 998, loss 0.593121, acc 0.734375, prec 0.0321368, recall 0.765458
2017-12-10T12:03:03.941956: step 999, loss 12.5552, acc 0.8125, prec 0.032121, recall 0.764643
2017-12-10T12:03:04.385271: step 1000, loss 1.12952, acc 0.734375, prec 0.0321398, recall 0.764894
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1000

2017-12-10T12:03:06.416305: step 1001, loss 1.32847, acc 0.765625, prec 0.0321183, recall 0.764894
2017-12-10T12:03:06.809861: step 1002, loss 1.07419, acc 0.78125, prec 0.0320982, recall 0.764894
2017-12-10T12:03:07.265135: step 1003, loss 1.21192, acc 0.71875, prec 0.0321156, recall 0.765143
2017-12-10T12:03:07.710590: step 1004, loss 0.441188, acc 0.859375, prec 0.0321027, recall 0.765143
2017-12-10T12:03:08.152547: step 1005, loss 0.441435, acc 0.890625, prec 0.032179, recall 0.765642
2017-12-10T12:03:08.598076: step 1006, loss 0.710083, acc 0.859375, prec 0.0321661, recall 0.765642
2017-12-10T12:03:09.036600: step 1007, loss 0.630265, acc 0.84375, prec 0.0321518, recall 0.765642
2017-12-10T12:03:09.468718: step 1008, loss 0.864011, acc 0.8125, prec 0.0321346, recall 0.765642
2017-12-10T12:03:09.901475: step 1009, loss 0.693417, acc 0.859375, prec 0.0321217, recall 0.765642
2017-12-10T12:03:10.332253: step 1010, loss 3.50584, acc 0.8125, prec 0.032106, recall 0.764831
2017-12-10T12:03:10.787847: step 1011, loss 0.86574, acc 0.828125, prec 0.0320903, recall 0.764831
2017-12-10T12:03:11.220149: step 1012, loss 0.693651, acc 0.828125, prec 0.0320746, recall 0.764831
2017-12-10T12:03:11.655816: step 1013, loss 0.541971, acc 0.859375, prec 0.0321048, recall 0.765079
2017-12-10T12:03:12.097536: step 1014, loss 0.73824, acc 0.8125, prec 0.0320877, recall 0.765079
2017-12-10T12:03:12.536783: step 1015, loss 0.932288, acc 0.796875, prec 0.0320692, recall 0.765079
2017-12-10T12:03:12.980464: step 1016, loss 0.581476, acc 0.84375, prec 0.032055, recall 0.765079
2017-12-10T12:03:13.430635: step 1017, loss 0.890979, acc 0.890625, prec 0.0321308, recall 0.765576
2017-12-10T12:03:13.881430: step 1018, loss 0.301149, acc 0.921875, prec 0.0321666, recall 0.765823
2017-12-10T12:03:14.324433: step 1019, loss 0.729981, acc 0.890625, prec 0.0321566, recall 0.765823
2017-12-10T12:03:14.761565: step 1020, loss 0.121877, acc 0.953125, prec 0.0321523, recall 0.765823
2017-12-10T12:03:15.197344: step 1021, loss 0.363442, acc 0.859375, prec 0.0321824, recall 0.76607
2017-12-10T12:03:15.637947: step 1022, loss 0.679096, acc 0.890625, prec 0.0322152, recall 0.766316
2017-12-10T12:03:16.078694: step 1023, loss 0.385996, acc 0.890625, prec 0.0322053, recall 0.766316
2017-12-10T12:03:16.522532: step 1024, loss 0.485969, acc 0.890625, prec 0.0322381, recall 0.766562
2017-12-10T12:03:16.961151: step 1025, loss 0.323172, acc 0.90625, prec 0.0323151, recall 0.767051
2017-12-10T12:03:17.407596: step 1026, loss 0.452585, acc 0.90625, prec 0.0323493, recall 0.767296
2017-12-10T12:03:17.852035: step 1027, loss 0.101306, acc 0.953125, prec 0.032345, recall 0.767296
2017-12-10T12:03:18.299371: step 1028, loss 0.610251, acc 0.875, prec 0.0323336, recall 0.767296
2017-12-10T12:03:18.748264: step 1029, loss 7.39879, acc 0.890625, prec 0.0323677, recall 0.766736
2017-12-10T12:03:19.186494: step 1030, loss 0.197246, acc 0.9375, prec 0.032362, recall 0.766736
2017-12-10T12:03:19.634070: step 1031, loss 0.194607, acc 0.9375, prec 0.0323563, recall 0.766736
2017-12-10T12:03:20.084064: step 1032, loss 1.1859, acc 0.8125, prec 0.0324246, recall 0.767223
2017-12-10T12:03:20.527362: step 1033, loss 0.700975, acc 0.859375, prec 0.0324117, recall 0.767223
2017-12-10T12:03:20.964935: step 1034, loss 1.48233, acc 0.90625, prec 0.0324045, recall 0.766423
2017-12-10T12:03:21.422404: step 1035, loss 0.237757, acc 0.890625, prec 0.0323946, recall 0.766423
2017-12-10T12:03:21.866709: step 1036, loss 0.255683, acc 0.921875, prec 0.0323874, recall 0.766423
2017-12-10T12:03:22.306568: step 1037, loss 0.242614, acc 0.9375, prec 0.0323817, recall 0.766423
2017-12-10T12:03:22.752933: step 1038, loss 0.570885, acc 0.90625, prec 0.0323731, recall 0.766423
2017-12-10T12:03:23.187176: step 1039, loss 0.804039, acc 0.828125, prec 0.0324001, recall 0.766667
2017-12-10T12:03:23.622518: step 1040, loss 0.64134, acc 0.78125, prec 0.0324227, recall 0.766909
2017-12-10T12:03:24.067735: step 1041, loss 0.408495, acc 0.875, prec 0.0324113, recall 0.766909
2017-12-10T12:03:24.490158: step 1042, loss 0.645247, acc 0.84375, prec 0.0324396, recall 0.767152
2017-12-10T12:03:24.940908: step 1043, loss 0.707884, acc 0.828125, prec 0.0325089, recall 0.767635
2017-12-10T12:03:25.385830: step 1044, loss 0.818619, acc 0.828125, prec 0.0325782, recall 0.768116
2017-12-10T12:03:25.823840: step 1045, loss 0.326777, acc 0.859375, prec 0.0326077, recall 0.768356
2017-12-10T12:03:26.260145: step 1046, loss 0.702695, acc 0.828125, prec 0.032592, recall 0.768356
2017-12-10T12:03:26.710229: step 1047, loss 1.39078, acc 0.859375, prec 0.0326216, recall 0.768595
2017-12-10T12:03:27.159173: step 1048, loss 8.52597, acc 0.828125, prec 0.0326073, recall 0.767802
2017-12-10T12:03:27.613949: step 1049, loss 0.771459, acc 0.796875, prec 0.0325887, recall 0.767802
2017-12-10T12:03:28.059358: step 1050, loss 0.521385, acc 0.890625, prec 0.0325787, recall 0.767802
2017-12-10T12:03:28.504958: step 1051, loss 0.564555, acc 0.796875, prec 0.0327295, recall 0.768756
2017-12-10T12:03:28.945936: step 1052, loss 0.622357, acc 0.90625, prec 0.0327632, recall 0.768994
2017-12-10T12:03:29.381561: step 1053, loss 0.744452, acc 0.734375, prec 0.0327389, recall 0.768994
2017-12-10T12:03:29.827355: step 1054, loss 1.09405, acc 0.71875, prec 0.0327976, recall 0.769467
2017-12-10T12:03:30.269344: step 1055, loss 0.767979, acc 0.828125, prec 0.0328241, recall 0.769703
2017-12-10T12:03:30.705819: step 1056, loss 0.519167, acc 0.875, prec 0.0328126, recall 0.769703
2017-12-10T12:03:31.140326: step 1057, loss 0.611435, acc 0.765625, prec 0.0328333, recall 0.769939
2017-12-10T12:03:31.577588: step 1058, loss 0.678003, acc 0.828125, prec 0.0328598, recall 0.770174
2017-12-10T12:03:32.015527: step 1059, loss 0.71468, acc 0.8125, prec 0.0330532, recall 0.771341
2017-12-10T12:03:32.451395: step 1060, loss 0.220751, acc 0.953125, prec 0.033091, recall 0.771574
2017-12-10T12:03:32.890543: step 1061, loss 0.651017, acc 0.84375, prec 0.0331186, recall 0.771805
2017-12-10T12:03:33.328322: step 1062, loss 0.866477, acc 0.84375, prec 0.0331463, recall 0.772036
2017-12-10T12:03:33.778309: step 1063, loss 1.94163, acc 0.78125, prec 0.0331276, recall 0.771255
2017-12-10T12:03:34.223585: step 1064, loss 0.576128, acc 0.828125, prec 0.0331117, recall 0.771255
2017-12-10T12:03:34.662498: step 1065, loss 0.403018, acc 0.859375, prec 0.0330988, recall 0.771255
2017-12-10T12:03:35.099865: step 1066, loss 0.513797, acc 0.859375, prec 0.0330858, recall 0.771255
2017-12-10T12:03:35.539812: step 1067, loss 0.62555, acc 0.859375, prec 0.0331568, recall 0.771717
2017-12-10T12:03:35.992549: step 1068, loss 0.400783, acc 0.859375, prec 0.0331439, recall 0.771717
2017-12-10T12:03:36.427291: step 1069, loss 0.180102, acc 0.953125, prec 0.0332235, recall 0.772177
2017-12-10T12:03:36.894629: step 1070, loss 0.590985, acc 0.875, prec 0.0332119, recall 0.772177
2017-12-10T12:03:37.334830: step 1071, loss 1.6998, acc 0.90625, prec 0.0332871, recall 0.772636
2017-12-10T12:03:37.769613: step 1072, loss 0.276492, acc 0.890625, prec 0.033277, recall 0.772636
2017-12-10T12:03:38.211199: step 1073, loss 0.392449, acc 0.90625, prec 0.0333521, recall 0.773092
2017-12-10T12:03:38.663513: step 1074, loss 0.305009, acc 0.9375, prec 0.03343, recall 0.773547
2017-12-10T12:03:39.099308: step 1075, loss 0.736225, acc 0.953125, prec 0.0335512, recall 0.774226
2017-12-10T12:03:39.554041: step 1076, loss 0.377746, acc 0.890625, prec 0.0336247, recall 0.774676
2017-12-10T12:03:39.999835: step 1077, loss 0.770286, acc 0.90625, prec 0.0336578, recall 0.7749
2017-12-10T12:03:40.447325: step 1078, loss 1.55077, acc 0.90625, prec 0.0336923, recall 0.774354
2017-12-10T12:03:40.895129: step 1079, loss 0.487321, acc 0.828125, prec 0.0336763, recall 0.774354
2017-12-10T12:03:41.337763: step 1080, loss 0.422678, acc 0.9375, prec 0.033754, recall 0.774802
2017-12-10T12:03:41.777647: step 1081, loss 0.459183, acc 0.921875, prec 0.0338719, recall 0.77547
2017-12-10T12:03:42.226298: step 1082, loss 0.880529, acc 0.84375, prec 0.033899, recall 0.775692
2017-12-10T12:03:42.658843: step 1083, loss 4.32644, acc 0.859375, prec 0.0339707, recall 0.775369
2017-12-10T12:03:43.096785: step 1084, loss 1.01583, acc 0.90625, prec 0.0340453, recall 0.775811
2017-12-10T12:03:43.538429: step 1085, loss 0.499227, acc 0.78125, prec 0.0341914, recall 0.77669
2017-12-10T12:03:43.987653: step 1086, loss 0.616233, acc 0.859375, prec 0.0342613, recall 0.777126
2017-12-10T12:03:44.428514: step 1087, loss 1.17249, acc 0.75, prec 0.0342377, recall 0.777126
2017-12-10T12:03:44.865439: step 1088, loss 0.868964, acc 0.765625, prec 0.0342572, recall 0.777344
2017-12-10T12:03:45.296290: step 1089, loss 0.784758, acc 0.78125, prec 0.0342781, recall 0.777561
2017-12-10T12:03:45.735381: step 1090, loss 1.34687, acc 0.640625, prec 0.0343687, recall 0.77821
2017-12-10T12:03:46.173128: step 1091, loss 0.493438, acc 0.78125, prec 0.034348, recall 0.77821
2017-12-10T12:03:46.621443: step 1092, loss 1.2503, acc 0.734375, prec 0.034323, recall 0.77821
2017-12-10T12:03:47.064847: step 1093, loss 0.792121, acc 0.765625, prec 0.0343009, recall 0.77821
2017-12-10T12:03:47.500954: step 1094, loss 1.3825, acc 0.65625, prec 0.0342686, recall 0.77821
2017-12-10T12:03:47.944958: step 1095, loss 0.952533, acc 0.734375, prec 0.034285, recall 0.778426
2017-12-10T12:03:48.375305: step 1096, loss 1.10041, acc 0.765625, prec 0.0343456, recall 0.778856
2017-12-10T12:03:48.827457: step 1097, loss 0.577688, acc 0.84375, prec 0.0343722, recall 0.77907
2017-12-10T12:03:49.270724: step 1098, loss 0.363326, acc 0.875, prec 0.0344017, recall 0.779284
2017-12-10T12:03:49.706794: step 1099, loss 0.791396, acc 0.828125, prec 0.0344268, recall 0.779497
2017-12-10T12:03:50.155191: step 1100, loss 0.766314, acc 0.859375, prec 0.0344548, recall 0.77971
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1100

2017-12-10T12:03:52.096213: step 1101, loss 0.396634, acc 0.828125, prec 0.0344386, recall 0.77971
2017-12-10T12:03:52.542810: step 1102, loss 0.264247, acc 0.96875, prec 0.0344769, recall 0.779923
2017-12-10T12:03:52.988707: step 1103, loss 0.117272, acc 0.96875, prec 0.0345151, recall 0.780135
2017-12-10T12:03:53.438686: step 1104, loss 0.242852, acc 0.9375, prec 0.0345504, recall 0.780347
2017-12-10T12:03:53.883797: step 1105, loss 0.469052, acc 0.90625, prec 0.0345827, recall 0.780558
2017-12-10T12:03:54.334853: step 1106, loss 12.0736, acc 0.890625, prec 0.0346151, recall 0.780019
2017-12-10T12:03:54.787618: step 1107, loss 2.20008, acc 0.8125, prec 0.0345988, recall 0.779271
2017-12-10T12:03:55.234090: step 1108, loss 0.665503, acc 0.890625, prec 0.0346296, recall 0.779482
2017-12-10T12:03:55.680509: step 1109, loss 0.31933, acc 0.890625, prec 0.0346193, recall 0.779482
2017-12-10T12:03:56.117816: step 1110, loss 0.430835, acc 0.875, prec 0.0346075, recall 0.779482
2017-12-10T12:03:56.565372: step 1111, loss 1.33386, acc 0.90625, prec 0.0346809, recall 0.779904
2017-12-10T12:03:57.026610: step 1112, loss 0.564573, acc 0.8125, prec 0.0347042, recall 0.780115
2017-12-10T12:03:57.456419: step 1113, loss 0.490645, acc 0.875, prec 0.0347334, recall 0.780325
2017-12-10T12:03:57.898330: step 1114, loss 0.51475, acc 0.859375, prec 0.0347612, recall 0.780534
2017-12-10T12:03:58.328036: step 1115, loss 0.528356, acc 0.828125, prec 0.0349089, recall 0.781369
2017-12-10T12:03:58.767732: step 1116, loss 0.285184, acc 0.890625, prec 0.0348985, recall 0.781369
2017-12-10T12:03:59.218287: step 1117, loss 0.729416, acc 0.8125, prec 0.0348808, recall 0.781369
2017-12-10T12:03:59.650168: step 1118, loss 0.578662, acc 0.84375, prec 0.0349888, recall 0.781991
2017-12-10T12:04:00.103474: step 1119, loss 0.337872, acc 0.875, prec 0.0349769, recall 0.781991
2017-12-10T12:04:00.575663: step 1120, loss 0.928231, acc 0.828125, prec 0.0349606, recall 0.781991
2017-12-10T12:04:01.005272: step 1121, loss 0.462753, acc 0.828125, prec 0.0349443, recall 0.781991
2017-12-10T12:04:01.443463: step 1122, loss 0.579782, acc 0.8125, prec 0.0349265, recall 0.781991
2017-12-10T12:04:01.889944: step 1123, loss 0.673595, acc 0.84375, prec 0.0349934, recall 0.782403
2017-12-10T12:04:02.329602: step 1124, loss 1.01922, acc 0.765625, prec 0.0349712, recall 0.782403
2017-12-10T12:04:02.774786: step 1125, loss 3.93041, acc 0.796875, prec 0.0349535, recall 0.781664
2017-12-10T12:04:03.231232: step 1126, loss 0.760054, acc 0.859375, prec 0.0350218, recall 0.782075
2017-12-10T12:04:03.685728: step 1127, loss 0.752134, acc 0.796875, prec 0.0350025, recall 0.782075
2017-12-10T12:04:04.130144: step 1128, loss 0.639778, acc 0.796875, prec 0.0350241, recall 0.782281
2017-12-10T12:04:04.573170: step 1129, loss 0.439729, acc 0.859375, prec 0.0350108, recall 0.782281
2017-12-10T12:04:05.009521: step 1130, loss 0.572819, acc 0.875, prec 0.0350396, recall 0.782486
2017-12-10T12:04:05.447827: step 1131, loss 0.636454, acc 0.796875, prec 0.0350611, recall 0.782691
2017-12-10T12:04:05.893443: step 1132, loss 0.777146, acc 0.765625, prec 0.035039, recall 0.782691
2017-12-10T12:04:06.338701: step 1133, loss 0.298105, acc 0.90625, prec 0.0350301, recall 0.782691
2017-12-10T12:04:06.782299: step 1134, loss 0.584795, acc 0.796875, prec 0.0350921, recall 0.783099
2017-12-10T12:04:07.229609: step 1135, loss 0.565701, acc 0.859375, prec 0.0351194, recall 0.783302
2017-12-10T12:04:07.673557: step 1136, loss 4.87009, acc 0.9375, prec 0.0351556, recall 0.782772
2017-12-10T12:04:08.121354: step 1137, loss 0.444052, acc 0.859375, prec 0.0351423, recall 0.782772
2017-12-10T12:04:08.565460: step 1138, loss 0.401017, acc 0.859375, prec 0.035129, recall 0.782772
2017-12-10T12:04:09.012268: step 1139, loss 0.463292, acc 0.875, prec 0.0351983, recall 0.783178
2017-12-10T12:04:09.468117: step 1140, loss 0.70119, acc 0.921875, prec 0.0352314, recall 0.78338
2017-12-10T12:04:09.930377: step 1141, loss 0.354175, acc 0.921875, prec 0.035305, recall 0.783784
2017-12-10T12:04:10.384489: step 1142, loss 0.813134, acc 0.75, prec 0.0353218, recall 0.783985
2017-12-10T12:04:10.831443: step 1143, loss 0.440863, acc 0.84375, prec 0.0353069, recall 0.783985
2017-12-10T12:04:11.271235: step 1144, loss 0.688034, acc 0.84375, prec 0.0353326, recall 0.784186
2017-12-10T12:04:11.700288: step 1145, loss 0.774594, acc 0.8125, prec 0.0353956, recall 0.784587
2017-12-10T12:04:12.140808: step 1146, loss 0.464096, acc 0.859375, prec 0.0354227, recall 0.784787
2017-12-10T12:04:12.591370: step 1147, loss 0.213733, acc 0.90625, prec 0.0354138, recall 0.784787
2017-12-10T12:04:13.037429: step 1148, loss 0.435603, acc 0.84375, prec 0.0354797, recall 0.785185
2017-12-10T12:04:13.472233: step 1149, loss 0.726436, acc 0.875, prec 0.0355888, recall 0.78578
2017-12-10T12:04:13.898628: step 1150, loss 0.591026, acc 0.8125, prec 0.035571, recall 0.78578
2017-12-10T12:04:14.338512: step 1151, loss 0.478852, acc 0.859375, prec 0.0355576, recall 0.78578
2017-12-10T12:04:14.767035: step 1152, loss 0.439722, acc 0.890625, prec 0.0355875, recall 0.785978
2017-12-10T12:04:15.212240: step 1153, loss 0.377665, acc 0.921875, prec 0.0356606, recall 0.786372
2017-12-10T12:04:15.669811: step 1154, loss 0.31673, acc 0.921875, prec 0.0356532, recall 0.786372
2017-12-10T12:04:16.123476: step 1155, loss 0.36988, acc 0.875, prec 0.0356412, recall 0.786372
2017-12-10T12:04:16.567900: step 1156, loss 0.727264, acc 0.921875, prec 0.0356741, recall 0.786569
2017-12-10T12:04:17.017968: step 1157, loss 0.752719, acc 0.90625, prec 0.0356651, recall 0.786569
2017-12-10T12:04:17.470399: step 1158, loss 6.83262, acc 0.875, prec 0.0356562, recall 0.785124
2017-12-10T12:04:17.910427: step 1159, loss 0.251583, acc 0.90625, prec 0.0357277, recall 0.785518
2017-12-10T12:04:18.346713: step 1160, loss 0.507841, acc 0.890625, prec 0.0357574, recall 0.785714
2017-12-10T12:04:18.799931: step 1161, loss 0.438997, acc 0.890625, prec 0.035747, recall 0.785714
2017-12-10T12:04:19.235614: step 1162, loss 7.00491, acc 0.875, prec 0.0357366, recall 0.784995
2017-12-10T12:04:19.683050: step 1163, loss 2.44197, acc 0.828125, prec 0.0357217, recall 0.784278
2017-12-10T12:04:20.132434: step 1164, loss 0.488092, acc 0.90625, prec 0.0357529, recall 0.784475
2017-12-10T12:04:20.588771: step 1165, loss 1.00798, acc 0.734375, prec 0.0357277, recall 0.784475
2017-12-10T12:04:21.049651: step 1166, loss 1.38981, acc 0.6875, prec 0.035698, recall 0.784475
2017-12-10T12:04:21.506257: step 1167, loss 0.965373, acc 0.703125, prec 0.0357499, recall 0.784868
2017-12-10T12:04:21.954364: step 1168, loss 2.0206, acc 0.59375, prec 0.0357513, recall 0.785064
2017-12-10T12:04:22.410812: step 1169, loss 1.78608, acc 0.5625, prec 0.0357498, recall 0.785259
2017-12-10T12:04:22.855274: step 1170, loss 2.36087, acc 0.578125, prec 0.0357498, recall 0.785455
2017-12-10T12:04:23.307609: step 1171, loss 1.50636, acc 0.65625, prec 0.0357172, recall 0.785455
2017-12-10T12:04:23.744076: step 1172, loss 1.71385, acc 0.515625, prec 0.0356715, recall 0.785455
2017-12-10T12:04:24.200944: step 1173, loss 1.48851, acc 0.640625, prec 0.0356377, recall 0.785455
2017-12-10T12:04:24.643721: step 1174, loss 1.43788, acc 0.65625, prec 0.0356451, recall 0.785649
2017-12-10T12:04:25.077811: step 1175, loss 1.85205, acc 0.703125, prec 0.0356966, recall 0.786038
2017-12-10T12:04:25.516419: step 1176, loss 2.58824, acc 0.65625, prec 0.0357055, recall 0.78552
2017-12-10T12:04:25.958247: step 1177, loss 1.17674, acc 0.734375, prec 0.0356805, recall 0.78552
2017-12-10T12:04:26.409710: step 1178, loss 1.13285, acc 0.78125, prec 0.0356996, recall 0.785714
2017-12-10T12:04:26.849896: step 1179, loss 3.07236, acc 0.8125, prec 0.0356835, recall 0.785004
2017-12-10T12:04:27.287100: step 1180, loss 1.05206, acc 0.734375, prec 0.0356586, recall 0.785004
2017-12-10T12:04:27.730323: step 1181, loss 0.555785, acc 0.78125, prec 0.0356381, recall 0.785004
2017-12-10T12:04:28.173319: step 1182, loss 0.612967, acc 0.859375, prec 0.035625, recall 0.785004
2017-12-10T12:04:28.615372: step 1183, loss 0.494653, acc 0.84375, prec 0.0356104, recall 0.785004
2017-12-10T12:04:29.070916: step 1184, loss 0.713974, acc 0.84375, prec 0.0355958, recall 0.785004
2017-12-10T12:04:29.520720: step 1185, loss 2.04388, acc 0.890625, prec 0.0356265, recall 0.784491
2017-12-10T12:04:29.967489: step 1186, loss 0.213295, acc 0.96875, prec 0.0356236, recall 0.784491
2017-12-10T12:04:30.406436: step 1187, loss 0.421017, acc 0.84375, prec 0.0356485, recall 0.784685
2017-12-10T12:04:30.846742: step 1188, loss 0.452236, acc 0.828125, prec 0.0356719, recall 0.784878
2017-12-10T12:04:31.289249: step 1189, loss 0.488435, acc 0.796875, prec 0.035653, recall 0.784878
2017-12-10T12:04:31.730886: step 1190, loss 0.359566, acc 0.859375, prec 0.0356793, recall 0.785072
2017-12-10T12:04:32.177866: step 1191, loss 0.488354, acc 0.875, prec 0.035707, recall 0.785265
2017-12-10T12:04:32.621415: step 1192, loss 0.352566, acc 0.890625, prec 0.0357362, recall 0.785458
2017-12-10T12:04:33.055251: step 1193, loss 0.229413, acc 0.953125, prec 0.0357712, recall 0.78565
2017-12-10T12:04:33.505687: step 1194, loss 0.301906, acc 0.921875, prec 0.0357639, recall 0.78565
2017-12-10T12:04:33.947351: step 1195, loss 0.890041, acc 0.875, prec 0.0358702, recall 0.786225
2017-12-10T12:04:34.387295: step 1196, loss 0.286511, acc 0.953125, prec 0.0358658, recall 0.786225
2017-12-10T12:04:34.840196: step 1197, loss 19.021, acc 0.90625, prec 0.0358993, recall 0.785013
2017-12-10T12:04:35.281397: step 1198, loss 0.619013, acc 0.921875, prec 0.0359706, recall 0.785396
2017-12-10T12:04:35.724302: step 1199, loss 0.363815, acc 0.875, prec 0.0359589, recall 0.785396
2017-12-10T12:04:36.167800: step 1200, loss 0.616407, acc 0.828125, prec 0.0360214, recall 0.785778
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1200

2017-12-10T12:04:38.075468: step 1201, loss 0.283158, acc 0.890625, prec 0.0360111, recall 0.785778
2017-12-10T12:04:38.521596: step 1202, loss 0.61272, acc 0.890625, prec 0.0360008, recall 0.785778
2017-12-10T12:04:38.969527: step 1203, loss 0.747139, acc 0.8125, prec 0.0359832, recall 0.785778
2017-12-10T12:04:39.422203: step 1204, loss 0.651376, acc 0.796875, prec 0.0360426, recall 0.786158
2017-12-10T12:04:39.864707: step 1205, loss 0.478538, acc 0.875, prec 0.0360309, recall 0.786158
2017-12-10T12:04:40.295980: step 1206, loss 0.741149, acc 0.765625, prec 0.0360089, recall 0.786158
2017-12-10T12:04:40.731728: step 1207, loss 0.848415, acc 0.8125, prec 0.0360305, recall 0.786348
2017-12-10T12:04:41.159934: step 1208, loss 3.84423, acc 0.796875, prec 0.0360521, recall 0.785841
2017-12-10T12:04:41.602645: step 1209, loss 0.385919, acc 0.84375, prec 0.0360766, recall 0.78603
2017-12-10T12:04:42.041931: step 1210, loss 1.20299, acc 0.734375, prec 0.0360908, recall 0.786219
2017-12-10T12:04:42.495174: step 1211, loss 4.36379, acc 0.625, prec 0.0360963, recall 0.785714
2017-12-10T12:04:42.939191: step 1212, loss 1.27279, acc 0.6875, prec 0.036067, recall 0.785714
2017-12-10T12:04:43.383845: step 1213, loss 1.04563, acc 0.6875, prec 0.0360768, recall 0.785903
2017-12-10T12:04:43.833531: step 1214, loss 1.11554, acc 0.65625, prec 0.0360837, recall 0.786092
2017-12-10T12:04:44.266791: step 1215, loss 0.998228, acc 0.78125, prec 0.0361023, recall 0.78628
2017-12-10T12:04:44.710721: step 1216, loss 0.851601, acc 0.75, prec 0.0360789, recall 0.78628
2017-12-10T12:04:45.161161: step 1217, loss 1.6174, acc 0.640625, prec 0.0360843, recall 0.786467
2017-12-10T12:04:45.620835: step 1218, loss 0.726613, acc 0.765625, prec 0.0361014, recall 0.786655
2017-12-10T12:04:46.067596: step 1219, loss 1.29361, acc 0.640625, prec 0.0361455, recall 0.787029
2017-12-10T12:04:46.516215: step 1220, loss 0.884966, acc 0.765625, prec 0.0361625, recall 0.787215
2017-12-10T12:04:46.954297: step 1221, loss 1.12941, acc 0.734375, prec 0.036254, recall 0.787773
2017-12-10T12:04:47.393831: step 1222, loss 0.641756, acc 0.8125, prec 0.0362753, recall 0.787958
2017-12-10T12:04:47.834769: step 1223, loss 0.519329, acc 0.8125, prec 0.0362578, recall 0.787958
2017-12-10T12:04:48.271152: step 1224, loss 0.650129, acc 0.84375, prec 0.0362819, recall 0.788143
2017-12-10T12:04:48.719130: step 1225, loss 0.944103, acc 0.75, prec 0.0362586, recall 0.788143
2017-12-10T12:04:49.159664: step 1226, loss 0.60328, acc 0.78125, prec 0.0362769, recall 0.788328
2017-12-10T12:04:49.595096: step 1227, loss 0.662012, acc 0.796875, prec 0.0362966, recall 0.788512
2017-12-10T12:04:50.027423: step 1228, loss 0.248097, acc 0.90625, prec 0.0363265, recall 0.788696
2017-12-10T12:04:50.472789: step 1229, loss 0.406296, acc 0.875, prec 0.0363149, recall 0.788696
2017-12-10T12:04:50.913584: step 1230, loss 0.720279, acc 0.875, prec 0.0363804, recall 0.789062
2017-12-10T12:04:51.362909: step 1231, loss 0.458023, acc 0.828125, prec 0.0363644, recall 0.789062
2017-12-10T12:04:51.796838: step 1232, loss 0.26577, acc 0.921875, prec 0.0363571, recall 0.789062
2017-12-10T12:04:52.243053: step 1233, loss 2.08068, acc 0.90625, prec 0.0363884, recall 0.788562
2017-12-10T12:04:52.697302: step 1234, loss 0.350563, acc 0.84375, prec 0.0363738, recall 0.788562
2017-12-10T12:04:53.142886: step 1235, loss 4.35538, acc 0.9375, prec 0.0363695, recall 0.787879
2017-12-10T12:04:53.593889: step 1236, loss 5.97988, acc 0.921875, prec 0.0363651, recall 0.786517
2017-12-10T12:04:54.044301: step 1237, loss 0.275865, acc 0.953125, prec 0.0363607, recall 0.786517
2017-12-10T12:04:54.492365: step 1238, loss 3.67192, acc 0.875, prec 0.0363506, recall 0.785838
2017-12-10T12:04:54.938812: step 1239, loss 0.867921, acc 0.71875, prec 0.0363244, recall 0.785838
2017-12-10T12:04:55.376577: step 1240, loss 1.32339, acc 0.71875, prec 0.0363368, recall 0.786022
2017-12-10T12:04:55.809148: step 1241, loss 0.867627, acc 0.78125, prec 0.0363549, recall 0.786207
2017-12-10T12:04:56.242588: step 1242, loss 1.89679, acc 0.5625, prec 0.0363911, recall 0.786575
2017-12-10T12:04:56.682295: step 1243, loss 1.07922, acc 0.6875, prec 0.0363622, recall 0.786575
2017-12-10T12:04:57.126608: step 1244, loss 1.53871, acc 0.59375, prec 0.0364012, recall 0.786942
2017-12-10T12:04:57.565083: step 1245, loss 1.44514, acc 0.71875, prec 0.0364517, recall 0.787307
2017-12-10T12:04:57.990895: step 1246, loss 1.72156, acc 0.65625, prec 0.0364963, recall 0.787671
2017-12-10T12:04:58.427094: step 1247, loss 1.98757, acc 0.515625, prec 0.0364897, recall 0.787853
2017-12-10T12:04:58.875399: step 1248, loss 2.75723, acc 0.46875, prec 0.0364787, recall 0.788034
2017-12-10T12:04:59.307923: step 1249, loss 1.23942, acc 0.6875, prec 0.036488, recall 0.788215
2017-12-10T12:04:59.755791: step 1250, loss 1.70996, acc 0.65625, prec 0.0364943, recall 0.788396
2017-12-10T12:05:00.190250: step 1251, loss 0.932718, acc 0.78125, prec 0.0364742, recall 0.788396
2017-12-10T12:05:00.637254: step 1252, loss 1.41547, acc 0.609375, prec 0.0364762, recall 0.788576
2017-12-10T12:05:01.073645: step 1253, loss 0.80112, acc 0.78125, prec 0.0364941, recall 0.788756
2017-12-10T12:05:01.511914: step 1254, loss 1.13366, acc 0.765625, prec 0.0365104, recall 0.788936
2017-12-10T12:05:01.954379: step 1255, loss 1.76182, acc 0.75, prec 0.0365633, recall 0.789295
2017-12-10T12:05:02.396513: step 1256, loss 0.517409, acc 0.8125, prec 0.0366218, recall 0.789652
2017-12-10T12:05:02.828990: step 1257, loss 0.845179, acc 0.765625, prec 0.0366381, recall 0.789831
2017-12-10T12:05:03.275362: step 1258, loss 0.331906, acc 0.859375, prec 0.0366251, recall 0.789831
2017-12-10T12:05:03.722822: step 1259, loss 0.563597, acc 0.828125, prec 0.0366093, recall 0.789831
2017-12-10T12:05:04.161874: step 1260, loss 0.601451, acc 0.828125, prec 0.036707, recall 0.790363
2017-12-10T12:05:04.600226: step 1261, loss 0.821644, acc 0.84375, prec 0.0366926, recall 0.790363
2017-12-10T12:05:05.030900: step 1262, loss 0.652832, acc 0.8125, prec 0.0366753, recall 0.790363
2017-12-10T12:05:05.464967: step 1263, loss 0.326925, acc 0.890625, prec 0.036703, recall 0.790541
2017-12-10T12:05:05.903000: step 1264, loss 0.103781, acc 0.953125, prec 0.0366987, recall 0.790541
2017-12-10T12:05:06.345866: step 1265, loss 0.576783, acc 0.9375, prec 0.0367685, recall 0.790894
2017-12-10T12:05:06.799485: step 1266, loss 0.487291, acc 0.890625, prec 0.0367584, recall 0.790894
2017-12-10T12:05:07.239585: step 1267, loss 0.415281, acc 0.890625, prec 0.036786, recall 0.79107
2017-12-10T12:05:07.703150: step 1268, loss 2.84378, acc 0.921875, prec 0.0367803, recall 0.790404
2017-12-10T12:05:08.148232: step 1269, loss 0.504695, acc 0.921875, prec 0.0368108, recall 0.79058
2017-12-10T12:05:08.587565: step 1270, loss 1.53391, acc 0.9375, prec 0.0368819, recall 0.790268
2017-12-10T12:05:09.037655: step 1271, loss 0.28399, acc 0.921875, prec 0.0368747, recall 0.790268
2017-12-10T12:05:09.492930: step 1272, loss 0.497333, acc 0.859375, prec 0.0368617, recall 0.790268
2017-12-10T12:05:09.938324: step 1273, loss 12.1476, acc 0.921875, prec 0.0368559, recall 0.789606
2017-12-10T12:05:10.390058: step 1274, loss 0.336562, acc 0.921875, prec 0.0368864, recall 0.789782
2017-12-10T12:05:10.841746: step 1275, loss 0.357191, acc 0.890625, prec 0.0369139, recall 0.789958
2017-12-10T12:05:11.293980: step 1276, loss 0.406612, acc 0.890625, prec 0.0369415, recall 0.790134
2017-12-10T12:05:11.728028: step 1277, loss 0.581323, acc 0.8125, prec 0.0369242, recall 0.790134
2017-12-10T12:05:12.167702: step 1278, loss 1.47643, acc 0.84375, prec 0.036985, recall 0.790484
2017-12-10T12:05:12.618048: step 1279, loss 0.83518, acc 0.765625, prec 0.0369633, recall 0.790484
2017-12-10T12:05:13.062163: step 1280, loss 0.364031, acc 0.859375, prec 0.0370255, recall 0.790833
2017-12-10T12:05:13.507706: step 1281, loss 5.98368, acc 0.796875, prec 0.0370457, recall 0.790349
2017-12-10T12:05:13.958899: step 1282, loss 0.973145, acc 0.703125, prec 0.0370558, recall 0.790524
2017-12-10T12:05:14.393357: step 1283, loss 0.656304, acc 0.828125, prec 0.0370399, recall 0.790524
2017-12-10T12:05:14.838251: step 1284, loss 1.90123, acc 0.78125, prec 0.0370947, recall 0.790871
2017-12-10T12:05:15.285384: step 1285, loss 1.04403, acc 0.765625, prec 0.037148, recall 0.791218
2017-12-10T12:05:15.728475: step 1286, loss 3.43226, acc 0.59375, prec 0.0371119, recall 0.790563
2017-12-10T12:05:16.164661: step 1287, loss 0.95184, acc 0.640625, prec 0.0371161, recall 0.790736
2017-12-10T12:05:16.600737: step 1288, loss 1.89498, acc 0.578125, prec 0.0371519, recall 0.791082
2017-12-10T12:05:17.051704: step 1289, loss 1.11253, acc 0.765625, prec 0.0371303, recall 0.791082
2017-12-10T12:05:17.489899: step 1290, loss 1.86471, acc 0.59375, prec 0.0372048, recall 0.791598
2017-12-10T12:05:17.929813: step 1291, loss 1.12862, acc 0.6875, prec 0.0372133, recall 0.79177
2017-12-10T12:05:18.372539: step 1292, loss 1.62302, acc 0.578125, prec 0.0372116, recall 0.791941
2017-12-10T12:05:18.811335: step 1293, loss 2.12181, acc 0.53125, prec 0.0371686, recall 0.791941
2017-12-10T12:05:19.243663: step 1294, loss 1.26694, acc 0.765625, prec 0.037147, recall 0.791941
2017-12-10T12:05:19.688338: step 1295, loss 1.34075, acc 0.6875, prec 0.0371555, recall 0.792112
2017-12-10T12:05:20.127038: step 1296, loss 1.01032, acc 0.765625, prec 0.0371711, recall 0.792282
2017-12-10T12:05:20.563460: step 1297, loss 1.17135, acc 0.6875, prec 0.0371425, recall 0.792282
2017-12-10T12:05:21.002070: step 1298, loss 0.829519, acc 0.796875, prec 0.037124, recall 0.792282
2017-12-10T12:05:21.450695: step 1299, loss 0.829169, acc 0.796875, prec 0.0371054, recall 0.792282
2017-12-10T12:05:21.900997: step 1300, loss 0.805453, acc 0.734375, prec 0.0370812, recall 0.792282
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1300

2017-12-10T12:05:23.778503: step 1301, loss 0.899118, acc 0.828125, prec 0.0371764, recall 0.792793
2017-12-10T12:05:24.217914: step 1302, loss 0.645341, acc 0.84375, prec 0.0371991, recall 0.792962
2017-12-10T12:05:24.656252: step 1303, loss 0.735805, acc 0.890625, prec 0.0373369, recall 0.793638
2017-12-10T12:05:25.101007: step 1304, loss 0.501021, acc 0.84375, prec 0.0373964, recall 0.793974
2017-12-10T12:05:25.556819: step 1305, loss 0.523548, acc 0.828125, prec 0.0374176, recall 0.794142
2017-12-10T12:05:26.004532: step 1306, loss 0.25945, acc 0.9375, prec 0.0374118, recall 0.794142
2017-12-10T12:05:26.443478: step 1307, loss 0.475497, acc 0.859375, prec 0.0373989, recall 0.794142
2017-12-10T12:05:26.897042: step 1308, loss 2.20466, acc 0.9375, prec 0.0373946, recall 0.793496
2017-12-10T12:05:27.352224: step 1309, loss 5.19631, acc 0.90625, prec 0.0373875, recall 0.792851
2017-12-10T12:05:27.822218: step 1310, loss 0.331697, acc 0.953125, prec 0.03742, recall 0.793019
2017-12-10T12:05:28.287314: step 1311, loss 8.29509, acc 0.890625, prec 0.0374114, recall 0.792376
2017-12-10T12:05:28.738145: step 1312, loss 2.0134, acc 0.90625, prec 0.0374411, recall 0.791903
2017-12-10T12:05:29.177802: step 1313, loss 1.0433, acc 0.84375, prec 0.0375005, recall 0.792239
2017-12-10T12:05:29.629380: step 1314, loss 0.612914, acc 0.78125, prec 0.0375172, recall 0.792407
2017-12-10T12:05:30.074742: step 1315, loss 0.762877, acc 0.75, prec 0.0374943, recall 0.792407
2017-12-10T12:05:30.519959: step 1316, loss 0.877209, acc 0.765625, prec 0.0374728, recall 0.792407
2017-12-10T12:05:30.965310: step 1317, loss 1.50589, acc 0.703125, prec 0.0375558, recall 0.792909
2017-12-10T12:05:31.406419: step 1318, loss 0.969805, acc 0.765625, prec 0.0376812, recall 0.793574
2017-12-10T12:05:31.855416: step 1319, loss 1.45658, acc 0.671875, prec 0.037651, recall 0.793574
2017-12-10T12:05:32.297069: step 1320, loss 1.21849, acc 0.703125, prec 0.0376604, recall 0.79374
2017-12-10T12:05:32.734295: step 1321, loss 0.951429, acc 0.71875, prec 0.0376346, recall 0.79374
2017-12-10T12:05:33.173108: step 1322, loss 0.963404, acc 0.84375, prec 0.0376935, recall 0.794071
2017-12-10T12:05:33.606226: step 1323, loss 1.26459, acc 0.734375, prec 0.0376691, recall 0.794071
2017-12-10T12:05:34.062031: step 1324, loss 0.938782, acc 0.765625, prec 0.0376477, recall 0.794071
2017-12-10T12:05:34.509287: step 1325, loss 0.948657, acc 0.6875, prec 0.0376191, recall 0.794071
2017-12-10T12:05:34.948977: step 1326, loss 0.830392, acc 0.796875, prec 0.0376005, recall 0.794071
2017-12-10T12:05:35.387625: step 1327, loss 0.576572, acc 0.8125, prec 0.0375834, recall 0.794071
2017-12-10T12:05:35.832395: step 1328, loss 0.826229, acc 0.8125, prec 0.0376028, recall 0.794235
2017-12-10T12:05:36.275073: step 1329, loss 0.831912, acc 0.703125, prec 0.0375758, recall 0.794235
2017-12-10T12:05:36.726103: step 1330, loss 1.71616, acc 0.71875, prec 0.037623, recall 0.794564
2017-12-10T12:05:37.169762: step 1331, loss 1.09264, acc 0.71875, prec 0.0376338, recall 0.794728
2017-12-10T12:05:37.612054: step 1332, loss 0.578591, acc 0.828125, prec 0.0376545, recall 0.794892
2017-12-10T12:05:38.053357: step 1333, loss 0.461384, acc 0.828125, prec 0.0376389, recall 0.794892
2017-12-10T12:05:38.493263: step 1334, loss 0.634347, acc 0.796875, prec 0.0376931, recall 0.795219
2017-12-10T12:05:38.937387: step 1335, loss 0.450457, acc 0.828125, prec 0.0377138, recall 0.795382
2017-12-10T12:05:39.383887: step 1336, loss 0.253463, acc 0.875, prec 0.0377024, recall 0.795382
2017-12-10T12:05:39.830516: step 1337, loss 2.25037, acc 0.828125, prec 0.0378696, recall 0.795563
2017-12-10T12:05:40.273735: step 1338, loss 0.538178, acc 0.8125, prec 0.0378888, recall 0.795724
2017-12-10T12:05:40.734833: step 1339, loss 0.425581, acc 0.8125, prec 0.0378717, recall 0.795724
2017-12-10T12:05:41.180382: step 1340, loss 0.756927, acc 0.875, prec 0.0378965, recall 0.795886
2017-12-10T12:05:41.623314: step 1341, loss 1.93214, acc 0.953125, prec 0.0379661, recall 0.79558
2017-12-10T12:05:42.082001: step 1342, loss 0.160624, acc 0.953125, prec 0.037998, recall 0.795741
2017-12-10T12:05:42.528596: step 1343, loss 0.471166, acc 0.953125, prec 0.03803, recall 0.795902
2017-12-10T12:05:42.977316: step 1344, loss 0.393388, acc 0.921875, prec 0.0380228, recall 0.795902
2017-12-10T12:05:43.423400: step 1345, loss 0.491075, acc 0.84375, prec 0.0380085, recall 0.795902
2017-12-10T12:05:43.874446: step 1346, loss 0.690233, acc 0.859375, prec 0.0380318, recall 0.796063
2017-12-10T12:05:44.326369: step 1347, loss 0.540184, acc 0.890625, prec 0.038058, recall 0.796223
2017-12-10T12:05:44.764117: step 1348, loss 0.758357, acc 0.796875, prec 0.0380756, recall 0.796384
2017-12-10T12:05:45.202599: step 1349, loss 0.515282, acc 0.875, prec 0.0381002, recall 0.796544
2017-12-10T12:05:45.653717: step 1350, loss 0.417012, acc 0.890625, prec 0.0380902, recall 0.796544
2017-12-10T12:05:46.103548: step 1351, loss 0.408333, acc 0.890625, prec 0.0381525, recall 0.796863
2017-12-10T12:05:46.547309: step 1352, loss 0.438725, acc 0.890625, prec 0.0381424, recall 0.796863
2017-12-10T12:05:46.992945: step 1353, loss 0.547638, acc 0.875, prec 0.038131, recall 0.796863
2017-12-10T12:05:47.420926: step 1354, loss 0.425279, acc 0.84375, prec 0.0382249, recall 0.79734
2017-12-10T12:05:47.853199: step 1355, loss 0.37285, acc 0.90625, prec 0.0382163, recall 0.79734
2017-12-10T12:05:48.293373: step 1356, loss 0.547497, acc 0.875, prec 0.0382049, recall 0.79734
2017-12-10T12:05:48.737046: step 1357, loss 0.16941, acc 0.921875, prec 0.0382337, recall 0.797498
2017-12-10T12:05:49.182258: step 1358, loss 0.434055, acc 0.828125, prec 0.0382901, recall 0.797814
2017-12-10T12:05:49.633723: step 1359, loss 0.218467, acc 0.875, prec 0.0382786, recall 0.797814
2017-12-10T12:05:50.071650: step 1360, loss 0.620407, acc 0.828125, prec 0.0382988, recall 0.797972
2017-12-10T12:05:50.518623: step 1361, loss 0.117474, acc 0.9375, prec 0.0382931, recall 0.797972
2017-12-10T12:05:50.965925: step 1362, loss 0.485054, acc 0.796875, prec 0.0383104, recall 0.798129
2017-12-10T12:05:51.401944: step 1363, loss 0.446932, acc 0.875, prec 0.038299, recall 0.798129
2017-12-10T12:05:51.847716: step 1364, loss 0.447212, acc 0.8125, prec 0.0382818, recall 0.798129
2017-12-10T12:05:52.292639: step 1365, loss 0.0938651, acc 0.953125, prec 0.0382775, recall 0.798129
2017-12-10T12:05:52.743372: step 1366, loss 0.111481, acc 0.9375, prec 0.0382718, recall 0.798129
2017-12-10T12:05:53.169549: step 1367, loss 0.142519, acc 0.953125, prec 0.0383034, recall 0.798287
2017-12-10T12:05:53.624659: step 1368, loss 1.09008, acc 0.9375, prec 0.0384414, recall 0.798913
2017-12-10T12:05:54.073294: step 1369, loss 0.415521, acc 0.921875, prec 0.0385061, recall 0.799225
2017-12-10T12:05:54.513364: step 1370, loss 0.24477, acc 0.921875, prec 0.0385348, recall 0.79938
2017-12-10T12:05:54.959721: step 1371, loss 0.14265, acc 0.96875, prec 0.0385678, recall 0.799536
2017-12-10T12:05:55.405988: step 1372, loss 2.32254, acc 0.96875, prec 0.0386023, recall 0.799073
2017-12-10T12:05:55.852757: step 1373, loss 0.287812, acc 0.96875, prec 0.0386353, recall 0.799228
2017-12-10T12:05:56.301839: step 1374, loss 0.192357, acc 0.9375, prec 0.0386295, recall 0.799228
2017-12-10T12:05:56.748762: step 1375, loss 15.8028, acc 0.90625, prec 0.0386237, recall 0.797995
2017-12-10T12:05:57.185351: step 1376, loss 0.294342, acc 0.890625, prec 0.0386495, recall 0.798151
2017-12-10T12:05:57.630522: step 1377, loss 0.675849, acc 0.859375, prec 0.0386365, recall 0.798151
2017-12-10T12:05:58.089931: step 1378, loss 0.258657, acc 0.890625, prec 0.0386623, recall 0.798306
2017-12-10T12:05:58.538806: step 1379, loss 0.26328, acc 0.921875, prec 0.0386909, recall 0.798462
2017-12-10T12:05:58.986428: step 1380, loss 0.409186, acc 0.890625, prec 0.0387883, recall 0.798926
2017-12-10T12:05:59.435885: step 1381, loss 0.83647, acc 0.78125, prec 0.0387681, recall 0.798926
2017-12-10T12:05:59.865511: step 1382, loss 0.545575, acc 0.828125, prec 0.038788, recall 0.79908
2017-12-10T12:06:00.312819: step 1383, loss 0.657931, acc 0.8125, prec 0.0388064, recall 0.799234
2017-12-10T12:06:00.757958: step 1384, loss 0.93346, acc 0.8125, prec 0.0388606, recall 0.799541
2017-12-10T12:06:01.203744: step 1385, loss 0.905753, acc 0.828125, prec 0.0388804, recall 0.799694
2017-12-10T12:06:01.649223: step 1386, loss 0.786025, acc 0.75, prec 0.038893, recall 0.799847
2017-12-10T12:06:02.079722: step 1387, loss 0.567846, acc 0.796875, prec 0.0389099, recall 0.8
2017-12-10T12:06:02.513066: step 1388, loss 1.23162, acc 0.75, prec 0.0389581, recall 0.800305
2017-12-10T12:06:02.951136: step 1389, loss 1.47164, acc 0.6875, prec 0.0389649, recall 0.800457
2017-12-10T12:06:03.387859: step 1390, loss 1.14511, acc 0.703125, prec 0.0389375, recall 0.800457
2017-12-10T12:06:03.834437: step 1391, loss 0.865292, acc 0.796875, prec 0.0389543, recall 0.800609
2017-12-10T12:06:04.287534: step 1392, loss 0.625918, acc 0.859375, prec 0.0389769, recall 0.80076
2017-12-10T12:06:04.726327: step 1393, loss 0.69518, acc 0.828125, prec 0.0389966, recall 0.800912
2017-12-10T12:06:05.163724: step 1394, loss 0.438924, acc 0.8125, prec 0.0389793, recall 0.800912
2017-12-10T12:06:05.592244: step 1395, loss 0.415514, acc 0.859375, prec 0.0390018, recall 0.801063
2017-12-10T12:06:06.036512: step 1396, loss 0.557349, acc 0.8125, prec 0.0389846, recall 0.801063
2017-12-10T12:06:06.468442: step 1397, loss 0.47584, acc 0.84375, prec 0.0389702, recall 0.801063
2017-12-10T12:06:06.908570: step 1398, loss 1.93524, acc 0.828125, prec 0.0389558, recall 0.800455
2017-12-10T12:06:07.358257: step 1399, loss 0.671592, acc 0.828125, prec 0.0389399, recall 0.800455
2017-12-10T12:06:07.806889: step 1400, loss 0.643693, acc 0.859375, prec 0.0389625, recall 0.800607
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1400

2017-12-10T12:06:09.640371: step 1401, loss 0.786276, acc 0.796875, prec 0.0389792, recall 0.800758
2017-12-10T12:06:10.079950: step 1402, loss 0.317435, acc 0.890625, prec 0.0389692, recall 0.800758
2017-12-10T12:06:10.522669: step 1403, loss 0.167962, acc 0.953125, prec 0.0389649, recall 0.800758
2017-12-10T12:06:10.966697: step 1404, loss 0.385136, acc 0.8125, prec 0.0389476, recall 0.800758
2017-12-10T12:06:11.416560: step 1405, loss 0.452396, acc 0.921875, prec 0.0390113, recall 0.801059
2017-12-10T12:06:11.859344: step 1406, loss 0.396917, acc 0.90625, prec 0.039038, recall 0.801209
2017-12-10T12:06:12.308157: step 1407, loss 0.266456, acc 0.90625, prec 0.0390294, recall 0.801209
2017-12-10T12:06:12.741073: step 1408, loss 1.07106, acc 0.9375, prec 0.0391652, recall 0.801809
2017-12-10T12:06:13.189466: step 1409, loss 0.457321, acc 0.921875, prec 0.0391933, recall 0.801958
2017-12-10T12:06:13.637902: step 1410, loss 0.0668518, acc 0.984375, prec 0.0392272, recall 0.802107
2017-12-10T12:06:14.089672: step 1411, loss 0.174749, acc 0.921875, prec 0.03922, recall 0.802107
2017-12-10T12:06:14.535708: step 1412, loss 0.419605, acc 0.859375, prec 0.0392424, recall 0.802256
2017-12-10T12:06:14.969354: step 1413, loss 0.212061, acc 0.890625, prec 0.0392676, recall 0.802404
2017-12-10T12:06:15.415638: step 1414, loss 13.5505, acc 0.890625, prec 0.0392942, recall 0.801951
2017-12-10T12:06:15.863033: step 1415, loss 0.161433, acc 0.953125, prec 0.0393252, recall 0.802099
2017-12-10T12:06:16.304246: step 1416, loss 0.277454, acc 0.890625, prec 0.0393151, recall 0.802099
2017-12-10T12:06:16.751033: step 1417, loss 1.80423, acc 0.828125, prec 0.039336, recall 0.801647
2017-12-10T12:06:17.190357: step 1418, loss 1.89392, acc 0.859375, prec 0.0393244, recall 0.801047
2017-12-10T12:06:17.626442: step 1419, loss 1.95899, acc 0.859375, prec 0.0393129, recall 0.800448
2017-12-10T12:06:18.069036: step 1420, loss 1.84494, acc 0.90625, prec 0.0393056, recall 0.799851
2017-12-10T12:06:18.514456: step 1421, loss 0.721962, acc 0.8125, prec 0.0392883, recall 0.799851
2017-12-10T12:06:18.953181: step 1422, loss 0.343801, acc 0.890625, prec 0.0393487, recall 0.800149
2017-12-10T12:06:19.395467: step 1423, loss 0.553305, acc 0.859375, prec 0.0394414, recall 0.800595
2017-12-10T12:06:19.846588: step 1424, loss 1.23994, acc 0.703125, prec 0.0394139, recall 0.800595
2017-12-10T12:06:20.289217: step 1425, loss 1.16432, acc 0.75, prec 0.0394963, recall 0.801039
2017-12-10T12:06:20.750625: step 1426, loss 0.712302, acc 0.796875, prec 0.0395127, recall 0.801187
2017-12-10T12:06:21.208214: step 1427, loss 1.25117, acc 0.734375, prec 0.0394881, recall 0.801187
2017-12-10T12:06:21.669500: step 1428, loss 0.925304, acc 0.765625, prec 0.0395016, recall 0.801334
2017-12-10T12:06:22.120956: step 1429, loss 0.593007, acc 0.8125, prec 0.0395544, recall 0.801628
2017-12-10T12:06:22.566172: step 1430, loss 0.748518, acc 0.78125, prec 0.0396043, recall 0.801922
2017-12-10T12:06:23.003193: step 1431, loss 0.733632, acc 0.8125, prec 0.039622, recall 0.802068
2017-12-10T12:06:23.450692: step 1432, loss 0.988662, acc 0.75, prec 0.0395989, recall 0.802068
2017-12-10T12:06:23.885569: step 1433, loss 1.22871, acc 0.6875, prec 0.03964, recall 0.80236
2017-12-10T12:06:24.331535: step 1434, loss 0.584369, acc 0.8125, prec 0.0396227, recall 0.80236
2017-12-10T12:06:24.770442: step 1435, loss 0.354632, acc 0.84375, prec 0.0396083, recall 0.80236
2017-12-10T12:06:25.194503: step 1436, loss 0.428151, acc 0.875, prec 0.0396667, recall 0.802651
2017-12-10T12:06:25.638261: step 1437, loss 1.1106, acc 0.78125, prec 0.0396814, recall 0.802796
2017-12-10T12:06:26.078760: step 1438, loss 1.28508, acc 0.75, prec 0.0396583, recall 0.802796
2017-12-10T12:06:26.525300: step 1439, loss 1.1153, acc 0.859375, prec 0.0396802, recall 0.802941
2017-12-10T12:06:26.965479: step 1440, loss 0.561543, acc 0.78125, prec 0.0396949, recall 0.803086
2017-12-10T12:06:27.410376: step 1441, loss 0.670247, acc 0.84375, prec 0.0397502, recall 0.803375
2017-12-10T12:06:27.846458: step 1442, loss 0.429708, acc 0.921875, prec 0.0397779, recall 0.803519
2017-12-10T12:06:28.284625: step 1443, loss 0.589773, acc 0.859375, prec 0.0398346, recall 0.803807
2017-12-10T12:06:28.723080: step 1444, loss 0.653139, acc 0.828125, prec 0.0398187, recall 0.803807
2017-12-10T12:06:29.154524: step 1445, loss 0.701874, acc 0.8125, prec 0.0398362, recall 0.80395
2017-12-10T12:06:29.592514: step 1446, loss 0.454592, acc 0.875, prec 0.0398246, recall 0.80395
2017-12-10T12:06:30.036204: step 1447, loss 0.284526, acc 0.921875, prec 0.0398174, recall 0.80395
2017-12-10T12:06:30.480200: step 1448, loss 0.131304, acc 0.953125, prec 0.0398131, recall 0.80395
2017-12-10T12:06:30.926198: step 1449, loss 0.390623, acc 0.921875, prec 0.0398059, recall 0.80395
2017-12-10T12:06:31.369152: step 1450, loss 0.279141, acc 0.921875, prec 0.0398334, recall 0.804094
2017-12-10T12:06:31.812868: step 1451, loss 0.279416, acc 0.953125, prec 0.0398291, recall 0.804094
2017-12-10T12:06:32.255420: step 1452, loss 0.662871, acc 0.96875, prec 0.039861, recall 0.804237
2017-12-10T12:06:32.697205: step 1453, loss 0.0681825, acc 0.953125, prec 0.0398566, recall 0.804237
2017-12-10T12:06:33.130490: step 1454, loss 0.12742, acc 0.96875, prec 0.0398885, recall 0.80438
2017-12-10T12:06:33.579683: step 1455, loss 0.229582, acc 0.96875, prec 0.0398856, recall 0.80438
2017-12-10T12:06:34.023178: step 1456, loss 15.8434, acc 0.90625, prec 0.0398799, recall 0.803207
2017-12-10T12:06:34.470217: step 1457, loss 0.472825, acc 0.921875, prec 0.0399074, recall 0.80335
2017-12-10T12:06:34.928107: step 1458, loss 0.172479, acc 0.9375, prec 0.0399016, recall 0.80335
2017-12-10T12:06:35.367657: step 1459, loss 0.310914, acc 0.890625, prec 0.0399262, recall 0.803493
2017-12-10T12:06:35.809504: step 1460, loss 0.468606, acc 0.796875, prec 0.0399075, recall 0.803493
2017-12-10T12:06:36.253876: step 1461, loss 0.6888, acc 0.828125, prec 0.039961, recall 0.803779
2017-12-10T12:06:36.703727: step 1462, loss 0.342015, acc 0.921875, prec 0.0399538, recall 0.803779
2017-12-10T12:06:37.137649: step 1463, loss 0.737321, acc 0.796875, prec 0.0400043, recall 0.804064
2017-12-10T12:06:37.573570: step 1464, loss 0.542951, acc 0.828125, prec 0.0400577, recall 0.804348
2017-12-10T12:06:38.018091: step 1465, loss 0.825979, acc 0.78125, prec 0.0400375, recall 0.804348
2017-12-10T12:06:38.466029: step 1466, loss 0.784584, acc 0.8125, prec 0.0400202, recall 0.804348
2017-12-10T12:06:38.919685: step 1467, loss 0.974758, acc 0.75, prec 0.0400317, recall 0.804489
2017-12-10T12:06:39.355041: step 1468, loss 0.705186, acc 0.796875, prec 0.0400475, recall 0.804631
2017-12-10T12:06:39.797323: step 1469, loss 0.727464, acc 0.8125, prec 0.0400648, recall 0.804772
2017-12-10T12:06:40.241856: step 1470, loss 0.814487, acc 0.8125, prec 0.0400475, recall 0.804772
2017-12-10T12:06:40.678054: step 1471, loss 0.56859, acc 0.875, prec 0.040105, recall 0.805054
2017-12-10T12:06:41.110176: step 1472, loss 1.34613, acc 0.78125, prec 0.0401193, recall 0.805195
2017-12-10T12:06:41.552330: step 1473, loss 0.47159, acc 0.8125, prec 0.0401021, recall 0.805195
2017-12-10T12:06:41.995379: step 1474, loss 0.730065, acc 0.8125, prec 0.0401537, recall 0.805476
2017-12-10T12:06:42.447434: step 1475, loss 0.487453, acc 0.875, prec 0.0401422, recall 0.805476
2017-12-10T12:06:42.888664: step 1476, loss 0.94985, acc 0.75, prec 0.0401536, recall 0.805616
2017-12-10T12:06:43.334523: step 1477, loss 0.501195, acc 0.84375, prec 0.0401392, recall 0.805616
2017-12-10T12:06:43.777086: step 1478, loss 0.095387, acc 0.953125, prec 0.0401693, recall 0.805755
2017-12-10T12:06:44.226445: step 1479, loss 3.0348, acc 0.828125, prec 0.0401893, recall 0.805316
2017-12-10T12:06:44.683902: step 1480, loss 0.157257, acc 0.953125, prec 0.0402194, recall 0.805456
2017-12-10T12:06:45.129903: step 1481, loss 0.706086, acc 0.8125, prec 0.0402365, recall 0.805595
2017-12-10T12:06:45.576205: step 1482, loss 0.223286, acc 0.9375, prec 0.0402307, recall 0.805595
2017-12-10T12:06:46.027661: step 1483, loss 0.633142, acc 0.84375, prec 0.0402163, recall 0.805595
2017-12-10T12:06:46.475655: step 1484, loss 0.530467, acc 0.859375, prec 0.0402033, recall 0.805595
2017-12-10T12:06:46.923959: step 1485, loss 0.73683, acc 0.8125, prec 0.0402204, recall 0.805735
2017-12-10T12:06:47.368058: step 1486, loss 0.293581, acc 0.890625, prec 0.0402104, recall 0.805735
2017-12-10T12:06:47.802733: step 1487, loss 3.12435, acc 0.90625, prec 0.0402375, recall 0.805297
2017-12-10T12:06:48.251642: step 1488, loss 0.576047, acc 0.859375, prec 0.0402589, recall 0.805436
2017-12-10T12:06:48.709277: step 1489, loss 0.656054, acc 0.828125, prec 0.0403116, recall 0.805714
2017-12-10T12:06:49.161586: step 1490, loss 0.86489, acc 0.84375, prec 0.0403315, recall 0.805853
2017-12-10T12:06:49.548728: step 1491, loss 0.63699, acc 0.803922, prec 0.0403171, recall 0.805853
2017-12-10T12:06:50.007898: step 1492, loss 0.593589, acc 0.828125, prec 0.0403698, recall 0.80613
2017-12-10T12:06:50.452064: step 1493, loss 0.463462, acc 0.84375, prec 0.0403896, recall 0.806268
2017-12-10T12:06:50.898968: step 1494, loss 0.375025, acc 0.890625, prec 0.0403795, recall 0.806268
2017-12-10T12:06:51.344138: step 1495, loss 0.647593, acc 0.859375, prec 0.0404008, recall 0.806406
2017-12-10T12:06:51.778593: step 1496, loss 0.35407, acc 0.890625, prec 0.0403907, recall 0.806406
2017-12-10T12:06:52.211605: step 1497, loss 0.402448, acc 0.875, prec 0.0404134, recall 0.806543
2017-12-10T12:06:52.664280: step 1498, loss 0.451907, acc 0.859375, prec 0.0404688, recall 0.806818
2017-12-10T12:06:53.107855: step 1499, loss 0.509973, acc 0.90625, prec 0.0404602, recall 0.806818
2017-12-10T12:06:53.553249: step 1500, loss 0.701452, acc 0.859375, prec 0.0404472, recall 0.806818
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1500

2017-12-10T12:06:55.507251: step 1501, loss 1.04089, acc 0.859375, prec 0.0405025, recall 0.807092
2017-12-10T12:06:55.945421: step 1502, loss 0.623234, acc 0.734375, prec 0.0404781, recall 0.807092
2017-12-10T12:06:56.389282: step 1503, loss 0.493402, acc 0.859375, prec 0.0404992, recall 0.807229
2017-12-10T12:06:56.838192: step 1504, loss 0.159548, acc 0.921875, prec 0.040492, recall 0.807229
2017-12-10T12:06:57.277712: step 1505, loss 0.463647, acc 0.890625, prec 0.040516, recall 0.807365
2017-12-10T12:06:57.721779: step 1506, loss 0.331407, acc 0.90625, prec 0.0405415, recall 0.807502
2017-12-10T12:06:58.160673: step 1507, loss 0.160635, acc 0.9375, prec 0.0405357, recall 0.807502
2017-12-10T12:06:58.605272: step 1508, loss 0.176737, acc 0.90625, prec 0.0405271, recall 0.807502
2017-12-10T12:06:59.056391: step 1509, loss 0.122586, acc 0.953125, prec 0.0405228, recall 0.807502
2017-12-10T12:06:59.515641: step 1510, loss 0.67692, acc 0.890625, prec 0.0405468, recall 0.807638
2017-12-10T12:06:59.944952: step 1511, loss 0.40986, acc 0.890625, prec 0.0405367, recall 0.807638
2017-12-10T12:07:00.397687: step 1512, loss 2.57332, acc 0.953125, prec 0.0405338, recall 0.807067
2017-12-10T12:07:00.844486: step 1513, loss 0.947025, acc 0.953125, prec 0.0405636, recall 0.807203
2017-12-10T12:07:01.292824: step 1514, loss 0.201526, acc 0.953125, prec 0.0405592, recall 0.807203
2017-12-10T12:07:01.738176: step 1515, loss 0.269946, acc 0.890625, prec 0.0405832, recall 0.807339
2017-12-10T12:07:02.174425: step 1516, loss 0.122843, acc 0.96875, prec 0.0405803, recall 0.807339
2017-12-10T12:07:02.602756: step 1517, loss 0.892077, acc 0.9375, prec 0.0406086, recall 0.807475
2017-12-10T12:07:03.040409: step 1518, loss 1.04626, acc 0.921875, prec 0.0406694, recall 0.807746
2017-12-10T12:07:03.491501: step 1519, loss 0.676679, acc 0.890625, prec 0.0407274, recall 0.808017
2017-12-10T12:07:03.938729: step 1520, loss 3.27089, acc 0.859375, prec 0.0407498, recall 0.807584
2017-12-10T12:07:04.386589: step 1521, loss 0.464595, acc 0.875, prec 0.0407722, recall 0.807719
2017-12-10T12:07:04.824402: step 1522, loss 0.821499, acc 0.859375, prec 0.0407592, recall 0.807719
2017-12-10T12:07:05.275181: step 1523, loss 0.7773, acc 0.796875, prec 0.0407744, recall 0.807854
2017-12-10T12:07:05.722749: step 1524, loss 0.720804, acc 0.765625, prec 0.0407528, recall 0.807854
2017-12-10T12:07:06.167526: step 1525, loss 0.923247, acc 0.734375, prec 0.0407283, recall 0.807854
2017-12-10T12:07:06.613397: step 1526, loss 1.00138, acc 0.75, prec 0.0407731, recall 0.808123
2017-12-10T12:07:07.059229: step 1527, loss 1.02547, acc 0.796875, prec 0.040856, recall 0.808526
2017-12-10T12:07:07.500229: step 1528, loss 0.53216, acc 0.828125, prec 0.0409078, recall 0.808793
2017-12-10T12:07:07.947155: step 1529, loss 0.839469, acc 0.75, prec 0.0408847, recall 0.808793
2017-12-10T12:07:08.380820: step 1530, loss 0.649142, acc 0.90625, prec 0.0409099, recall 0.808926
2017-12-10T12:07:08.829762: step 1531, loss 0.841243, acc 0.765625, prec 0.0408883, recall 0.808926
2017-12-10T12:07:09.274255: step 1532, loss 0.72346, acc 0.765625, prec 0.0409342, recall 0.809192
2017-12-10T12:07:09.720418: step 1533, loss 0.71158, acc 0.796875, prec 0.0409493, recall 0.809325
2017-12-10T12:07:10.167596: step 1534, loss 0.93442, acc 0.734375, prec 0.0409248, recall 0.809325
2017-12-10T12:07:10.618317: step 1535, loss 3.63564, acc 0.765625, prec 0.0409046, recall 0.808762
2017-12-10T12:07:11.065787: step 1536, loss 0.675961, acc 0.84375, prec 0.040924, recall 0.808895
2017-12-10T12:07:11.530444: step 1537, loss 0.130271, acc 0.921875, prec 0.0409842, recall 0.80916
2017-12-10T12:07:11.981159: step 1538, loss 0.965205, acc 0.8125, prec 0.0410006, recall 0.809293
2017-12-10T12:07:12.425707: step 1539, loss 0.634795, acc 0.828125, prec 0.0410184, recall 0.809425
2017-12-10T12:07:12.886195: step 1540, loss 1.33868, acc 0.84375, prec 0.0410714, recall 0.809689
2017-12-10T12:07:13.328058: step 1541, loss 0.48843, acc 0.84375, prec 0.041057, recall 0.809689
2017-12-10T12:07:13.770702: step 1542, loss 0.498192, acc 0.875, prec 0.0411463, recall 0.810083
2017-12-10T12:07:14.209870: step 1543, loss 0.40172, acc 0.828125, prec 0.0411641, recall 0.810214
2017-12-10T12:07:14.651720: step 1544, loss 0.41484, acc 0.859375, prec 0.0411847, recall 0.810345
2017-12-10T12:07:15.093122: step 1545, loss 1.04851, acc 0.6875, prec 0.0411559, recall 0.810345
2017-12-10T12:07:15.537395: step 1546, loss 0.739038, acc 0.828125, prec 0.0412072, recall 0.810606
2017-12-10T12:07:15.982453: step 1547, loss 0.376024, acc 0.921875, prec 0.0412335, recall 0.810736
2017-12-10T12:07:16.414834: step 1548, loss 0.66573, acc 0.859375, prec 0.0412541, recall 0.810867
2017-12-10T12:07:16.853779: step 1549, loss 0.224449, acc 0.890625, prec 0.041244, recall 0.810867
2017-12-10T12:07:17.283209: step 1550, loss 0.559012, acc 0.890625, prec 0.0413009, recall 0.811126
2017-12-10T12:07:17.720957: step 1551, loss 0.500363, acc 0.890625, prec 0.0412908, recall 0.811126
2017-12-10T12:07:18.157352: step 1552, loss 0.0880816, acc 0.953125, prec 0.04132, recall 0.811256
2017-12-10T12:07:18.598979: step 1553, loss 0.216864, acc 0.921875, prec 0.0413463, recall 0.811385
2017-12-10T12:07:19.037378: step 1554, loss 0.531012, acc 0.953125, prec 0.0413755, recall 0.811515
2017-12-10T12:07:19.476414: step 1555, loss 0.428291, acc 0.9375, prec 0.0414032, recall 0.811644
2017-12-10T12:07:19.912802: step 1556, loss 0.160191, acc 0.953125, prec 0.0414323, recall 0.811773
2017-12-10T12:07:20.362242: step 1557, loss 0.282777, acc 0.953125, prec 0.0415284, recall 0.812158
2017-12-10T12:07:20.816740: step 1558, loss 0.617395, acc 0.84375, prec 0.0416143, recall 0.812543
2017-12-10T12:07:21.269183: step 1559, loss 0.272934, acc 0.9375, prec 0.0416419, recall 0.81267
2017-12-10T12:07:21.715978: step 1560, loss 0.370638, acc 0.984375, prec 0.0417074, recall 0.812925
2017-12-10T12:07:22.167289: step 1561, loss 1.74773, acc 0.90625, prec 0.0417335, recall 0.8125
2017-12-10T12:07:22.616274: step 1562, loss 0.0890645, acc 0.984375, prec 0.0417321, recall 0.8125
2017-12-10T12:07:23.060046: step 1563, loss 0.224972, acc 0.9375, prec 0.0417263, recall 0.8125
2017-12-10T12:07:23.496413: step 1564, loss 0.198731, acc 0.96875, prec 0.0417234, recall 0.8125
2017-12-10T12:07:23.922953: step 1565, loss 0.322259, acc 0.953125, prec 0.0418193, recall 0.812881
2017-12-10T12:07:24.372108: step 1566, loss 0.359407, acc 0.9375, prec 0.0419137, recall 0.813261
2017-12-10T12:07:24.815868: step 1567, loss 1.20048, acc 0.890625, prec 0.0419702, recall 0.813514
2017-12-10T12:07:25.273138: step 1568, loss 0.252461, acc 0.890625, prec 0.0419934, recall 0.813639
2017-12-10T12:07:25.718888: step 1569, loss 0.503102, acc 0.8125, prec 0.0420426, recall 0.813891
2017-12-10T12:07:26.159695: step 1570, loss 0.294927, acc 0.9375, prec 0.0420367, recall 0.813891
2017-12-10T12:07:26.603458: step 1571, loss 0.601483, acc 0.875, prec 0.0420917, recall 0.814141
2017-12-10T12:07:27.049830: step 1572, loss 1.72734, acc 0.796875, prec 0.0421727, recall 0.814516
2017-12-10T12:07:27.494944: step 1573, loss 0.338944, acc 0.875, prec 0.0422276, recall 0.814765
2017-12-10T12:07:27.946600: step 1574, loss 0.235919, acc 0.953125, prec 0.0422565, recall 0.814889
2017-12-10T12:07:28.390305: step 1575, loss 0.964998, acc 0.921875, prec 0.042349, recall 0.815261
2017-12-10T12:07:28.843691: step 1576, loss 0.221027, acc 0.921875, prec 0.0423749, recall 0.815385
2017-12-10T12:07:29.293699: step 1577, loss 0.597289, acc 0.90625, prec 0.0423661, recall 0.815385
2017-12-10T12:07:29.746096: step 1578, loss 0.284551, acc 0.875, prec 0.0423543, recall 0.815385
2017-12-10T12:07:30.189209: step 1579, loss 0.524988, acc 0.828125, prec 0.0423714, recall 0.815508
2017-12-10T12:07:30.632631: step 1580, loss 0.627805, acc 0.75, prec 0.0423479, recall 0.815508
2017-12-10T12:07:31.075450: step 1581, loss 0.496356, acc 0.828125, prec 0.0423317, recall 0.815508
2017-12-10T12:07:31.513084: step 1582, loss 1.05391, acc 0.8125, prec 0.0423473, recall 0.815631
2017-12-10T12:07:31.961870: step 1583, loss 0.664997, acc 0.84375, prec 0.042399, recall 0.815877
2017-12-10T12:07:32.424942: step 1584, loss 0.598632, acc 0.75, prec 0.0423755, recall 0.815877
2017-12-10T12:07:32.874992: step 1585, loss 0.295167, acc 0.859375, prec 0.0423623, recall 0.815877
2017-12-10T12:07:33.319910: step 1586, loss 0.6466, acc 0.828125, prec 0.0423462, recall 0.815877
2017-12-10T12:07:33.761290: step 1587, loss 0.543312, acc 0.84375, prec 0.0423647, recall 0.816
2017-12-10T12:07:34.191117: step 1588, loss 0.259719, acc 0.890625, prec 0.0423875, recall 0.816123
2017-12-10T12:07:34.640610: step 1589, loss 0.240501, acc 0.953125, prec 0.0423831, recall 0.816123
2017-12-10T12:07:35.073182: step 1590, loss 2.80295, acc 0.84375, prec 0.0424362, recall 0.815824
2017-12-10T12:07:35.528338: step 1591, loss 0.606996, acc 0.90625, prec 0.0424605, recall 0.815947
2017-12-10T12:07:35.972578: step 1592, loss 0.823944, acc 0.828125, prec 0.0425436, recall 0.816313
2017-12-10T12:07:36.414147: step 1593, loss 0.599984, acc 0.828125, prec 0.0425605, recall 0.816435
2017-12-10T12:07:36.846536: step 1594, loss 0.477327, acc 0.859375, prec 0.0426134, recall 0.816678
2017-12-10T12:07:37.285576: step 1595, loss 0.377646, acc 0.859375, prec 0.0426002, recall 0.816678
2017-12-10T12:07:37.712745: step 1596, loss 0.595137, acc 0.875, prec 0.0425884, recall 0.816678
2017-12-10T12:07:38.148931: step 1597, loss 0.486175, acc 0.875, prec 0.0425767, recall 0.816678
2017-12-10T12:07:38.593123: step 1598, loss 0.395959, acc 0.875, prec 0.042631, recall 0.81692
2017-12-10T12:07:39.031531: step 1599, loss 0.435402, acc 0.828125, prec 0.0426808, recall 0.817162
2017-12-10T12:07:39.476980: step 1600, loss 1.192, acc 0.890625, prec 0.0427035, recall 0.817282
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1600

2017-12-10T12:07:41.474408: step 1601, loss 0.679037, acc 0.921875, prec 0.0427951, recall 0.817643
2017-12-10T12:07:41.924346: step 1602, loss 0.503607, acc 0.859375, prec 0.0427819, recall 0.817643
2017-12-10T12:07:42.367870: step 1603, loss 0.257625, acc 0.90625, prec 0.042773, recall 0.817643
2017-12-10T12:07:42.799990: step 1604, loss 0.389664, acc 0.84375, prec 0.0427583, recall 0.817643
2017-12-10T12:07:43.238030: step 1605, loss 0.373995, acc 0.890625, prec 0.0427809, recall 0.817763
2017-12-10T12:07:43.681258: step 1606, loss 2.15338, acc 0.78125, prec 0.0428277, recall 0.817466
2017-12-10T12:07:44.125183: step 1607, loss 0.125901, acc 0.953125, prec 0.0428562, recall 0.817585
2017-12-10T12:07:44.587150: step 1608, loss 0.698392, acc 0.8125, prec 0.0428385, recall 0.817585
2017-12-10T12:07:45.024933: step 1609, loss 0.650122, acc 0.875, prec 0.0428596, recall 0.817705
2017-12-10T12:07:45.472911: step 1610, loss 0.486761, acc 0.84375, prec 0.0428778, recall 0.817824
2017-12-10T12:07:45.917467: step 1611, loss 0.470396, acc 0.859375, prec 0.0429302, recall 0.818063
2017-12-10T12:07:46.363350: step 1612, loss 2.30571, acc 0.90625, prec 0.0429229, recall 0.817528
2017-12-10T12:07:46.810399: step 1613, loss 0.339829, acc 0.890625, prec 0.0429126, recall 0.817528
2017-12-10T12:07:47.257785: step 1614, loss 0.237744, acc 0.90625, prec 0.0429694, recall 0.817766
2017-12-10T12:07:47.699027: step 1615, loss 0.390277, acc 0.859375, prec 0.042989, recall 0.817885
2017-12-10T12:07:48.136485: step 1616, loss 0.504934, acc 0.8125, prec 0.0430041, recall 0.818004
2017-12-10T12:07:48.574632: step 1617, loss 0.293997, acc 0.84375, prec 0.0429894, recall 0.818004
2017-12-10T12:07:49.007481: step 1618, loss 0.479565, acc 0.859375, prec 0.0430089, recall 0.818123
2017-12-10T12:07:49.444633: step 1619, loss 0.732093, acc 0.796875, prec 0.0430553, recall 0.818359
2017-12-10T12:07:49.881447: step 1620, loss 0.43811, acc 0.84375, prec 0.0430733, recall 0.818478
2017-12-10T12:07:50.322331: step 1621, loss 0.41013, acc 0.859375, prec 0.0430928, recall 0.818596
2017-12-10T12:07:50.769904: step 1622, loss 0.816478, acc 0.890625, prec 0.0431807, recall 0.818949
2017-12-10T12:07:51.222749: step 1623, loss 0.329713, acc 0.859375, prec 0.0431674, recall 0.818949
2017-12-10T12:07:51.679209: step 1624, loss 0.563126, acc 0.828125, prec 0.0431512, recall 0.818949
2017-12-10T12:07:52.118631: step 1625, loss 0.46678, acc 0.859375, prec 0.0431706, recall 0.819066
2017-12-10T12:07:52.576902: step 1626, loss 0.315011, acc 0.875, prec 0.0431915, recall 0.819183
2017-12-10T12:07:53.018861: step 1627, loss 0.897575, acc 0.875, prec 0.0432451, recall 0.819417
2017-12-10T12:07:53.473678: step 1628, loss 0.481231, acc 0.859375, prec 0.0432318, recall 0.819417
2017-12-10T12:07:53.925165: step 1629, loss 0.621781, acc 0.875, prec 0.0432527, recall 0.819534
2017-12-10T12:07:54.383877: step 1630, loss 0.316376, acc 0.875, prec 0.0433061, recall 0.819767
2017-12-10T12:07:54.824927: step 1631, loss 2.53631, acc 0.890625, prec 0.0433299, recall 0.819355
2017-12-10T12:07:55.271885: step 1632, loss 0.676497, acc 0.875, prec 0.0433834, recall 0.819588
2017-12-10T12:07:55.723944: step 1633, loss 0.375665, acc 0.890625, prec 0.0434056, recall 0.819704
2017-12-10T12:07:56.171698: step 1634, loss 0.435007, acc 0.890625, prec 0.0434279, recall 0.81982
2017-12-10T12:07:56.606431: step 1635, loss 0.331666, acc 0.90625, prec 0.0434842, recall 0.820051
2017-12-10T12:07:57.046375: step 1636, loss 0.786329, acc 0.84375, prec 0.0435997, recall 0.820513
2017-12-10T12:07:57.486402: step 1637, loss 0.299252, acc 0.953125, prec 0.0435952, recall 0.820513
2017-12-10T12:07:57.938323: step 1638, loss 0.210075, acc 0.9375, prec 0.0436544, recall 0.820743
2017-12-10T12:07:58.392107: step 1639, loss 0.631036, acc 0.8125, prec 0.0436692, recall 0.820857
2017-12-10T12:07:58.839467: step 1640, loss 0.447643, acc 0.828125, prec 0.0436854, recall 0.820972
2017-12-10T12:07:59.283005: step 1641, loss 0.405287, acc 0.890625, prec 0.0437075, recall 0.821086
2017-12-10T12:07:59.731667: step 1642, loss 0.536804, acc 0.84375, prec 0.0437577, recall 0.821315
2017-12-10T12:08:00.173615: step 1643, loss 0.76595, acc 0.8125, prec 0.0437398, recall 0.821315
2017-12-10T12:08:00.602900: step 1644, loss 0.254123, acc 0.875, prec 0.0437279, recall 0.821315
2017-12-10T12:08:01.047931: step 1645, loss 2.12422, acc 0.859375, prec 0.0438769, recall 0.821883
2017-12-10T12:08:01.484884: step 1646, loss 0.285409, acc 0.9375, prec 0.043871, recall 0.821883
2017-12-10T12:08:01.927081: step 1647, loss 0.584674, acc 0.890625, prec 0.043893, recall 0.821996
2017-12-10T12:08:02.373722: step 1648, loss 0.233891, acc 0.859375, prec 0.0439445, recall 0.822222
2017-12-10T12:08:02.810383: step 1649, loss 4.2527, acc 0.8125, prec 0.0439929, recall 0.821926
2017-12-10T12:08:03.251105: step 1650, loss 0.368968, acc 0.875, prec 0.043981, recall 0.821926
2017-12-10T12:08:03.688744: step 1651, loss 5.42573, acc 0.828125, prec 0.0440633, recall 0.821745
2017-12-10T12:08:04.140650: step 1652, loss 0.485355, acc 0.828125, prec 0.0441117, recall 0.82197
2017-12-10T12:08:04.585241: step 1653, loss 0.711389, acc 0.8125, prec 0.0440937, recall 0.82197
2017-12-10T12:08:05.034401: step 1654, loss 0.946365, acc 0.78125, prec 0.0440728, recall 0.82197
2017-12-10T12:08:05.488794: step 1655, loss 0.585116, acc 0.84375, prec 0.0440903, recall 0.822082
2017-12-10T12:08:05.915829: step 1656, loss 0.950504, acc 0.734375, prec 0.0440649, recall 0.822082
2017-12-10T12:08:06.358741: step 1657, loss 0.808138, acc 0.703125, prec 0.0440366, recall 0.822082
2017-12-10T12:08:06.789091: step 1658, loss 0.700175, acc 0.734375, prec 0.0440114, recall 0.822082
2017-12-10T12:08:07.240134: step 1659, loss 0.795389, acc 0.75, prec 0.0440521, recall 0.822306
2017-12-10T12:08:07.679365: step 1660, loss 1.0171, acc 0.703125, prec 0.0440561, recall 0.822418
2017-12-10T12:08:08.125547: step 1661, loss 1.16467, acc 0.671875, prec 0.0440572, recall 0.82253
2017-12-10T12:08:08.569081: step 1662, loss 0.797722, acc 0.765625, prec 0.0440671, recall 0.822641
2017-12-10T12:08:09.021208: step 1663, loss 0.953823, acc 0.78125, prec 0.0440785, recall 0.822753
2017-12-10T12:08:09.456464: step 1664, loss 0.583989, acc 0.78125, prec 0.0441221, recall 0.822976
2017-12-10T12:08:09.904291: step 1665, loss 0.928402, acc 0.765625, prec 0.044132, recall 0.823087
2017-12-10T12:08:10.342961: step 1666, loss 0.73322, acc 0.84375, prec 0.0441493, recall 0.823197
2017-12-10T12:08:10.790027: step 1667, loss 0.956086, acc 0.84375, prec 0.0441666, recall 0.823308
2017-12-10T12:08:11.234039: step 1668, loss 0.665468, acc 0.828125, prec 0.0441503, recall 0.823308
2017-12-10T12:08:11.679693: step 1669, loss 0.701005, acc 0.90625, prec 0.0441735, recall 0.823419
2017-12-10T12:08:12.116433: step 1670, loss 0.74789, acc 0.828125, prec 0.0442213, recall 0.82364
2017-12-10T12:08:12.568612: step 1671, loss 0.590174, acc 0.828125, prec 0.044205, recall 0.82364
2017-12-10T12:08:13.008991: step 1672, loss 0.482878, acc 0.859375, prec 0.0442558, recall 0.82386
2017-12-10T12:08:13.452347: step 1673, loss 0.266182, acc 0.875, prec 0.044276, recall 0.82397
2017-12-10T12:08:13.894654: step 1674, loss 3.24869, acc 0.828125, prec 0.0442611, recall 0.823456
2017-12-10T12:08:14.336930: step 1675, loss 0.629954, acc 0.828125, prec 0.0442448, recall 0.823456
2017-12-10T12:08:14.776752: step 1676, loss 0.294498, acc 0.890625, prec 0.0442344, recall 0.823456
2017-12-10T12:08:15.225050: step 1677, loss 0.327695, acc 0.828125, prec 0.0442181, recall 0.823456
2017-12-10T12:08:15.663649: step 1678, loss 0.443429, acc 0.90625, prec 0.0442733, recall 0.823676
2017-12-10T12:08:16.097713: step 1679, loss 0.283675, acc 0.890625, prec 0.0443909, recall 0.824114
2017-12-10T12:08:16.538292: step 1680, loss 0.747149, acc 0.859375, prec 0.0444734, recall 0.824442
2017-12-10T12:08:16.982654: step 1681, loss 0.699736, acc 0.875, prec 0.0444615, recall 0.824442
2017-12-10T12:08:17.426659: step 1682, loss 0.518622, acc 0.84375, prec 0.0444467, recall 0.824442
2017-12-10T12:08:17.871207: step 1683, loss 0.446517, acc 0.84375, prec 0.0444318, recall 0.824442
2017-12-10T12:08:18.321280: step 1684, loss 0.598501, acc 0.953125, prec 0.0444593, recall 0.824551
2017-12-10T12:08:18.769643: step 1685, loss 0.248838, acc 0.9375, prec 0.0445172, recall 0.824768
2017-12-10T12:08:19.204228: step 1686, loss 0.380145, acc 0.90625, prec 0.0445402, recall 0.824876
2017-12-10T12:08:19.652144: step 1687, loss 0.584361, acc 0.890625, prec 0.0445936, recall 0.825093
2017-12-10T12:08:20.095691: step 1688, loss 0.970935, acc 0.90625, prec 0.0446485, recall 0.825309
2017-12-10T12:08:20.543933: step 1689, loss 0.104758, acc 0.96875, prec 0.0446774, recall 0.825416
2017-12-10T12:08:20.985988: step 1690, loss 0.225229, acc 0.921875, prec 0.04467, recall 0.825416
2017-12-10T12:08:21.435894: step 1691, loss 0.555597, acc 0.859375, prec 0.0446884, recall 0.825524
2017-12-10T12:08:21.902909: step 1692, loss 0.302401, acc 0.9375, prec 0.0446825, recall 0.825524
2017-12-10T12:08:22.350629: step 1693, loss 0.0729434, acc 0.953125, prec 0.0447099, recall 0.825632
2017-12-10T12:08:22.800128: step 1694, loss 0.359523, acc 0.859375, prec 0.0447283, recall 0.825739
2017-12-10T12:08:23.252539: step 1695, loss 0.199719, acc 0.953125, prec 0.0447557, recall 0.825846
2017-12-10T12:08:23.696303: step 1696, loss 0.547098, acc 0.890625, prec 0.0447771, recall 0.825953
2017-12-10T12:08:24.140357: step 1697, loss 0.248056, acc 0.921875, prec 0.0448015, recall 0.82606
2017-12-10T12:08:24.585637: step 1698, loss 0.36739, acc 0.875, prec 0.0448532, recall 0.826274
2017-12-10T12:08:25.019212: step 1699, loss 0.151273, acc 0.921875, prec 0.0448457, recall 0.826274
2017-12-10T12:08:25.469812: step 1700, loss 0.24259, acc 0.9375, prec 0.0448716, recall 0.82638
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1700

2017-12-10T12:08:27.239800: step 1701, loss 0.119422, acc 0.9375, prec 0.0448656, recall 0.82638
2017-12-10T12:08:27.689042: step 1702, loss 0.338691, acc 0.875, prec 0.0448537, recall 0.82638
2017-12-10T12:08:28.130867: step 1703, loss 0.33601, acc 0.953125, prec 0.0449128, recall 0.826593
2017-12-10T12:08:28.581302: step 1704, loss 0.343263, acc 0.875, prec 0.0449008, recall 0.826593
2017-12-10T12:08:29.028142: step 1705, loss 0.162297, acc 0.953125, prec 0.0449281, recall 0.826699
2017-12-10T12:08:29.466064: step 1706, loss 0.43458, acc 0.921875, prec 0.0449206, recall 0.826699
2017-12-10T12:08:29.909367: step 1707, loss 0.623403, acc 0.9375, prec 0.0449464, recall 0.826805
2017-12-10T12:08:30.373348: step 1708, loss 0.153427, acc 0.9375, prec 0.0449405, recall 0.826805
2017-12-10T12:08:30.812440: step 1709, loss 0.0651016, acc 0.96875, prec 0.0449692, recall 0.826911
2017-12-10T12:08:31.259385: step 1710, loss 0.218522, acc 0.9375, prec 0.044995, recall 0.827017
2017-12-10T12:08:31.695816: step 1711, loss 0.0778077, acc 0.9375, prec 0.044989, recall 0.827017
2017-12-10T12:08:32.144190: step 1712, loss 0.289215, acc 0.96875, prec 0.044986, recall 0.827017
2017-12-10T12:08:32.579409: step 1713, loss 0.34653, acc 0.9375, prec 0.0450435, recall 0.827228
2017-12-10T12:08:33.036284: step 1714, loss 0.302671, acc 0.9375, prec 0.045101, recall 0.827439
2017-12-10T12:08:33.476579: step 1715, loss 0.12706, acc 0.953125, prec 0.0450965, recall 0.827439
2017-12-10T12:08:33.916169: step 1716, loss 0.17291, acc 0.9375, prec 0.0450905, recall 0.827439
2017-12-10T12:08:34.356093: step 1717, loss 0.0537108, acc 0.984375, prec 0.045089, recall 0.827439
2017-12-10T12:08:34.798767: step 1718, loss 0.627316, acc 0.9375, prec 0.0451465, recall 0.827649
2017-12-10T12:08:35.250380: step 1719, loss 1.52297, acc 0.984375, prec 0.0451465, recall 0.827145
2017-12-10T12:08:35.695182: step 1720, loss 0.137177, acc 0.96875, prec 0.0451435, recall 0.827145
2017-12-10T12:08:36.153356: step 1721, loss 0.27165, acc 0.96875, prec 0.0451722, recall 0.827251
2017-12-10T12:08:36.602491: step 1722, loss 0.144291, acc 0.9375, prec 0.0451662, recall 0.827251
2017-12-10T12:08:37.040557: step 1723, loss 2.21935, acc 0.9375, prec 0.0451617, recall 0.826748
2017-12-10T12:08:37.474654: step 1724, loss 0.453959, acc 0.953125, prec 0.0452206, recall 0.826958
2017-12-10T12:08:37.914790: step 1725, loss 0.158181, acc 0.96875, prec 0.0452493, recall 0.827063
2017-12-10T12:08:38.368203: step 1726, loss 0.728017, acc 0.890625, prec 0.0453022, recall 0.827273
2017-12-10T12:08:38.808269: step 1727, loss 3.08284, acc 0.890625, prec 0.0453248, recall 0.826877
2017-12-10T12:08:39.256284: step 1728, loss 0.437955, acc 0.859375, prec 0.0453746, recall 0.827086
2017-12-10T12:08:39.705182: step 1729, loss 0.425111, acc 0.90625, prec 0.0453656, recall 0.827086
2017-12-10T12:08:40.148357: step 1730, loss 0.468463, acc 0.859375, prec 0.0453837, recall 0.82719
2017-12-10T12:08:40.591483: step 1731, loss 0.3969, acc 0.859375, prec 0.0453702, recall 0.82719
2017-12-10T12:08:41.045335: step 1732, loss 0.503961, acc 0.828125, prec 0.0453537, recall 0.82719
2017-12-10T12:08:41.489482: step 1733, loss 0.637564, acc 0.78125, prec 0.0453326, recall 0.82719
2017-12-10T12:08:41.930489: step 1734, loss 0.430612, acc 0.875, prec 0.0453522, recall 0.827295
2017-12-10T12:08:42.375863: step 1735, loss 0.410454, acc 0.875, prec 0.0454034, recall 0.827503
2017-12-10T12:08:42.820548: step 1736, loss 0.252143, acc 0.953125, prec 0.0454305, recall 0.827607
2017-12-10T12:08:43.255394: step 1737, loss 1.06877, acc 0.75, prec 0.045438, recall 0.827711
2017-12-10T12:08:43.702593: step 1738, loss 0.441785, acc 0.84375, prec 0.0454545, recall 0.827815
2017-12-10T12:08:44.139587: step 1739, loss 0.43909, acc 0.90625, prec 0.0454455, recall 0.827815
2017-12-10T12:08:44.595779: step 1740, loss 0.506153, acc 0.84375, prec 0.0454305, recall 0.827815
2017-12-10T12:08:45.041945: step 1741, loss 0.632502, acc 0.828125, prec 0.0454455, recall 0.827918
2017-12-10T12:08:45.481560: step 1742, loss 0.388505, acc 0.90625, prec 0.0454996, recall 0.828125
2017-12-10T12:08:45.929789: step 1743, loss 0.671344, acc 0.859375, prec 0.0455806, recall 0.828434
2017-12-10T12:08:46.378462: step 1744, loss 0.680902, acc 0.796875, prec 0.045561, recall 0.828434
2017-12-10T12:08:46.828836: step 1745, loss 0.367401, acc 0.84375, prec 0.0456089, recall 0.82864
2017-12-10T12:08:47.262026: step 1746, loss 0.268298, acc 0.859375, prec 0.0455954, recall 0.82864
2017-12-10T12:08:47.703244: step 1747, loss 0.125644, acc 0.953125, prec 0.0456224, recall 0.828743
2017-12-10T12:08:48.137667: step 1748, loss 0.295131, acc 0.890625, prec 0.0456118, recall 0.828743
2017-12-10T12:08:48.574265: step 1749, loss 0.272285, acc 0.921875, prec 0.0456043, recall 0.828743
2017-12-10T12:08:49.023899: step 1750, loss 0.411662, acc 0.984375, prec 0.0456657, recall 0.828947
2017-12-10T12:08:49.465467: step 1751, loss 0.0583882, acc 0.96875, prec 0.0456627, recall 0.828947
2017-12-10T12:08:49.898272: step 1752, loss 0.251394, acc 0.890625, prec 0.0456836, recall 0.82905
2017-12-10T12:08:50.348679: step 1753, loss 3.64234, acc 0.9375, prec 0.0456791, recall 0.828554
2017-12-10T12:08:50.794668: step 1754, loss 0.213899, acc 0.9375, prec 0.0456731, recall 0.828554
2017-12-10T12:08:51.234464: step 1755, loss 0.0743109, acc 0.96875, prec 0.0457015, recall 0.828657
2017-12-10T12:08:51.688960: step 1756, loss 0.232017, acc 0.921875, prec 0.045694, recall 0.828657
2017-12-10T12:08:52.118691: step 1757, loss 0.396206, acc 0.953125, prec 0.0457209, recall 0.828759
2017-12-10T12:08:52.567054: step 1758, loss 0.131681, acc 0.953125, prec 0.0457164, recall 0.828759
2017-12-10T12:08:53.012682: step 1759, loss 0.27643, acc 0.90625, prec 0.0457073, recall 0.828759
2017-12-10T12:08:53.461675: step 1760, loss 0.163668, acc 0.953125, prec 0.0457342, recall 0.828861
2017-12-10T12:08:53.901377: step 1761, loss 0.153739, acc 0.953125, prec 0.0457925, recall 0.829065
2017-12-10T12:08:54.345282: step 1762, loss 0.127024, acc 0.921875, prec 0.045785, recall 0.829065
2017-12-10T12:08:54.779270: step 1763, loss 0.343787, acc 0.90625, prec 0.0457759, recall 0.829065
2017-12-10T12:08:55.227292: step 1764, loss 0.458326, acc 0.890625, prec 0.0458281, recall 0.829268
2017-12-10T12:08:55.663831: step 1765, loss 0.8092, acc 0.921875, prec 0.045946, recall 0.829674
2017-12-10T12:08:56.109106: step 1766, loss 0.289196, acc 0.9375, prec 0.04594, recall 0.829674
2017-12-10T12:08:56.549867: step 1767, loss 0.178332, acc 0.9375, prec 0.045934, recall 0.829674
2017-12-10T12:08:56.994979: step 1768, loss 0.237001, acc 0.921875, prec 0.0459578, recall 0.829775
2017-12-10T12:08:57.442725: step 1769, loss 0.458866, acc 0.84375, prec 0.0459427, recall 0.829775
2017-12-10T12:08:57.883806: step 1770, loss 0.0850775, acc 0.953125, prec 0.0459381, recall 0.829775
2017-12-10T12:08:58.324969: step 1771, loss 0.323854, acc 0.90625, prec 0.0459291, recall 0.829775
2017-12-10T12:08:58.783791: step 1772, loss 0.327703, acc 0.921875, prec 0.0460155, recall 0.830077
2017-12-10T12:08:59.240968: step 1773, loss 0.120009, acc 0.9375, prec 0.0460408, recall 0.830177
2017-12-10T12:08:59.683198: step 1774, loss 0.115016, acc 0.953125, prec 0.0460362, recall 0.830177
2017-12-10T12:09:00.127761: step 1775, loss 0.831029, acc 0.8125, prec 0.0460807, recall 0.830378
2017-12-10T12:09:00.574495: step 1776, loss 0.269131, acc 0.921875, prec 0.0460731, recall 0.830378
2017-12-10T12:09:01.027182: step 1777, loss 0.269636, acc 0.9375, prec 0.0460984, recall 0.830478
2017-12-10T12:09:01.464581: step 1778, loss 0.294089, acc 0.90625, prec 0.0461831, recall 0.830778
2017-12-10T12:09:01.898170: step 1779, loss 0.264948, acc 0.90625, prec 0.046174, recall 0.830778
2017-12-10T12:09:02.348114: step 1780, loss 0.307882, acc 0.9375, prec 0.0461992, recall 0.830878
2017-12-10T12:09:02.798750: step 1781, loss 0.0792909, acc 0.96875, prec 0.0462274, recall 0.830978
2017-12-10T12:09:03.242300: step 1782, loss 0.106178, acc 0.953125, prec 0.0462229, recall 0.830978
2017-12-10T12:09:03.676693: step 1783, loss 0.199298, acc 0.953125, prec 0.0462184, recall 0.830978
2017-12-10T12:09:04.119708: step 1784, loss 0.767037, acc 0.890625, prec 0.0462702, recall 0.831176
2017-12-10T12:09:04.561313: step 1785, loss 0.0856049, acc 0.984375, prec 0.0462687, recall 0.831176
2017-12-10T12:09:04.992155: step 1786, loss 0.104286, acc 0.96875, prec 0.0462969, recall 0.831276
2017-12-10T12:09:05.440469: step 1787, loss 0.065413, acc 0.984375, prec 0.0463266, recall 0.831375
2017-12-10T12:09:05.893610: step 1788, loss 2.01332, acc 0.84375, prec 0.046313, recall 0.830887
2017-12-10T12:09:06.344462: step 1789, loss 0.0788565, acc 0.984375, prec 0.0463114, recall 0.830887
2017-12-10T12:09:06.789630: step 1790, loss 0.119388, acc 0.9375, prec 0.0463366, recall 0.830986
2017-12-10T12:09:07.237913: step 1791, loss 5.2461, acc 0.90625, prec 0.0463914, recall 0.830697
2017-12-10T12:09:07.694169: step 1792, loss 0.435655, acc 0.84375, prec 0.0463762, recall 0.830697
2017-12-10T12:09:08.136009: step 1793, loss 0.108929, acc 0.953125, prec 0.0463717, recall 0.830697
2017-12-10T12:09:08.581703: step 1794, loss 0.445458, acc 0.84375, prec 0.0463877, recall 0.830796
2017-12-10T12:09:09.010819: step 1795, loss 0.717606, acc 0.890625, prec 0.0464394, recall 0.830994
2017-12-10T12:09:09.446647: step 1796, loss 0.219407, acc 0.9375, prec 0.0464645, recall 0.831093
2017-12-10T12:09:09.884563: step 1797, loss 0.854809, acc 0.890625, prec 0.046485, recall 0.831192
2017-12-10T12:09:10.327891: step 1798, loss 0.662735, acc 0.828125, prec 0.0465929, recall 0.831585
2017-12-10T12:09:10.776386: step 1799, loss 0.74137, acc 0.796875, prec 0.0465731, recall 0.831585
2017-12-10T12:09:11.220985: step 1800, loss 0.514205, acc 0.8125, prec 0.046586, recall 0.831683
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1800

2017-12-10T12:09:13.193456: step 1801, loss 0.892838, acc 0.828125, prec 0.0466004, recall 0.831781
2017-12-10T12:09:13.639506: step 1802, loss 0.600449, acc 0.828125, prec 0.0466147, recall 0.831879
2017-12-10T12:09:14.078030: step 1803, loss 0.974752, acc 0.765625, prec 0.0465919, recall 0.831879
2017-12-10T12:09:14.507437: step 1804, loss 0.807529, acc 0.8125, prec 0.0466048, recall 0.831977
2017-12-10T12:09:14.943663: step 1805, loss 0.533833, acc 0.828125, prec 0.0465881, recall 0.831977
2017-12-10T12:09:15.389141: step 1806, loss 0.54709, acc 0.796875, prec 0.0465994, recall 0.832074
2017-12-10T12:09:15.830253: step 1807, loss 0.457576, acc 0.84375, prec 0.0465843, recall 0.832074
2017-12-10T12:09:16.281201: step 1808, loss 0.343985, acc 0.859375, prec 0.0466016, recall 0.832172
2017-12-10T12:09:16.729091: step 1809, loss 0.45513, acc 0.90625, prec 0.0466235, recall 0.832269
2017-12-10T12:09:17.180344: step 1810, loss 0.398369, acc 0.9375, prec 0.0466175, recall 0.832269
2017-12-10T12:09:17.611218: step 1811, loss 0.569771, acc 0.828125, prec 0.0466008, recall 0.832269
2017-12-10T12:09:18.056388: step 1812, loss 0.302759, acc 0.921875, prec 0.0466242, recall 0.832367
2017-12-10T12:09:18.496494: step 1813, loss 0.465323, acc 0.828125, prec 0.0466076, recall 0.832367
2017-12-10T12:09:18.939008: step 1814, loss 0.177822, acc 0.890625, prec 0.046597, recall 0.832367
2017-12-10T12:09:19.373249: step 1815, loss 0.290454, acc 0.9375, prec 0.0466219, recall 0.832464
2017-12-10T12:09:19.808352: step 1816, loss 0.275283, acc 0.921875, prec 0.0466143, recall 0.832464
2017-12-10T12:09:20.254060: step 1817, loss 0.259575, acc 0.9375, prec 0.0466392, recall 0.832561
2017-12-10T12:09:20.691960: step 1818, loss 0.191418, acc 0.90625, prec 0.0466301, recall 0.832561
2017-12-10T12:09:21.133811: step 1819, loss 0.355205, acc 0.90625, prec 0.046621, recall 0.832561
2017-12-10T12:09:21.598666: step 1820, loss 0.274726, acc 0.90625, prec 0.0466429, recall 0.832658
2017-12-10T12:09:22.051722: step 1821, loss 2.09592, acc 0.9375, prec 0.0467002, recall 0.83237
2017-12-10T12:09:22.507651: step 1822, loss 8.51414, acc 0.96875, prec 0.0466987, recall 0.831889
2017-12-10T12:09:22.958667: step 1823, loss 0.675085, acc 0.953125, prec 0.046725, recall 0.831986
2017-12-10T12:09:23.408977: step 1824, loss 0.224682, acc 0.921875, prec 0.0467793, recall 0.83218
2017-12-10T12:09:23.856599: step 1825, loss 0.338637, acc 0.90625, prec 0.0468011, recall 0.832277
2017-12-10T12:09:24.298821: step 1826, loss 0.431323, acc 0.890625, prec 0.0467904, recall 0.832277
2017-12-10T12:09:24.744137: step 1827, loss 0.708661, acc 0.921875, prec 0.0469064, recall 0.832662
2017-12-10T12:09:25.186430: step 1828, loss 0.671114, acc 0.796875, prec 0.0468866, recall 0.832662
2017-12-10T12:09:25.636320: step 1829, loss 0.455849, acc 0.84375, prec 0.0469023, recall 0.832759
2017-12-10T12:09:26.079355: step 1830, loss 0.287502, acc 0.921875, prec 0.0469564, recall 0.832951
2017-12-10T12:09:26.514752: step 1831, loss 0.392055, acc 0.921875, prec 0.0469488, recall 0.832951
2017-12-10T12:09:26.969303: step 1832, loss 0.456747, acc 0.90625, prec 0.0469705, recall 0.833046
2017-12-10T12:09:27.416255: step 1833, loss 0.684736, acc 0.8125, prec 0.0469523, recall 0.833046
2017-12-10T12:09:27.857250: step 1834, loss 0.46946, acc 0.796875, prec 0.0469326, recall 0.833046
2017-12-10T12:09:28.292255: step 1835, loss 0.708111, acc 0.84375, prec 0.046979, recall 0.833238
2017-12-10T12:09:28.731133: step 1836, loss 0.533308, acc 0.84375, prec 0.0469638, recall 0.833238
2017-12-10T12:09:29.191933: step 1837, loss 0.713591, acc 0.8125, prec 0.0469764, recall 0.833333
2017-12-10T12:09:29.630771: step 1838, loss 0.506726, acc 0.84375, prec 0.046992, recall 0.833429
2017-12-10T12:09:30.074193: step 1839, loss 0.342663, acc 0.921875, prec 0.0470152, recall 0.833524
2017-12-10T12:09:30.524635: step 1840, loss 0.202899, acc 0.90625, prec 0.047129, recall 0.833904
2017-12-10T12:09:30.973647: step 1841, loss 0.277708, acc 0.875, prec 0.0471476, recall 0.833999
2017-12-10T12:09:31.401337: step 1842, loss 0.293435, acc 0.890625, prec 0.047137, recall 0.833999
2017-12-10T12:09:31.851433: step 1843, loss 0.426667, acc 0.90625, prec 0.0471893, recall 0.834188
2017-12-10T12:09:32.297653: step 1844, loss 0.344544, acc 0.875, prec 0.0471771, recall 0.834188
2017-12-10T12:09:32.749332: step 1845, loss 0.250495, acc 0.9375, prec 0.0472017, recall 0.834282
2017-12-10T12:09:33.184477: step 1846, loss 0.304039, acc 0.953125, prec 0.0472279, recall 0.834377
2017-12-10T12:09:33.630624: step 1847, loss 0.123827, acc 0.96875, prec 0.0472248, recall 0.834377
2017-12-10T12:09:34.071341: step 1848, loss 0.467319, acc 0.921875, prec 0.0472479, recall 0.834471
2017-12-10T12:09:34.507634: step 1849, loss 0.2769, acc 0.9375, prec 0.0472418, recall 0.834471
2017-12-10T12:09:34.957524: step 1850, loss 0.357094, acc 0.90625, prec 0.0472327, recall 0.834471
2017-12-10T12:09:35.391042: step 1851, loss 0.123986, acc 0.96875, prec 0.0472296, recall 0.834471
2017-12-10T12:09:35.825648: step 1852, loss 0.148607, acc 0.96875, prec 0.0472879, recall 0.834659
2017-12-10T12:09:36.276874: step 1853, loss 0.30868, acc 0.921875, prec 0.0472803, recall 0.834659
2017-12-10T12:09:36.732077: step 1854, loss 2.17942, acc 0.875, prec 0.0473003, recall 0.834279
2017-12-10T12:09:37.181225: step 1855, loss 0.384245, acc 0.90625, prec 0.0473219, recall 0.834373
2017-12-10T12:09:37.634262: step 1856, loss 1.30751, acc 0.9375, prec 0.0473173, recall 0.8339
2017-12-10T12:09:38.080919: step 1857, loss 0.105008, acc 0.953125, prec 0.0473127, recall 0.8339
2017-12-10T12:09:38.527257: step 1858, loss 0.305506, acc 0.953125, prec 0.0473388, recall 0.833994
2017-12-10T12:09:38.970067: step 1859, loss 0.246502, acc 0.9375, prec 0.0473327, recall 0.833994
2017-12-10T12:09:39.410746: step 1860, loss 0.144384, acc 0.921875, prec 0.0473557, recall 0.834088
2017-12-10T12:09:39.858870: step 1861, loss 0.0821092, acc 0.96875, prec 0.0473527, recall 0.834088
2017-12-10T12:09:40.292138: step 1862, loss 0.12684, acc 0.953125, prec 0.0473481, recall 0.834088
2017-12-10T12:09:40.719632: step 1863, loss 0.327206, acc 0.875, prec 0.0473359, recall 0.834088
2017-12-10T12:09:41.169039: step 1864, loss 0.179833, acc 0.96875, prec 0.0473635, recall 0.834182
2017-12-10T12:09:41.614664: step 1865, loss 1.70804, acc 0.9375, prec 0.047359, recall 0.83371
2017-12-10T12:09:42.062500: step 1866, loss 0.322322, acc 0.875, prec 0.0473774, recall 0.833804
2017-12-10T12:09:42.506872: step 1867, loss 0.232267, acc 0.9375, prec 0.0473713, recall 0.833804
2017-12-10T12:09:42.950587: step 1868, loss 2.94529, acc 0.84375, prec 0.0473576, recall 0.833333
2017-12-10T12:09:43.399716: step 1869, loss 0.614407, acc 0.859375, prec 0.0473439, recall 0.833333
2017-12-10T12:09:43.848326: step 1870, loss 0.539479, acc 0.875, prec 0.0473623, recall 0.833427
2017-12-10T12:09:44.286341: step 1871, loss 0.662241, acc 0.9375, prec 0.0473868, recall 0.833521
2017-12-10T12:09:44.746285: step 1872, loss 0.766473, acc 0.765625, prec 0.0473946, recall 0.833615
2017-12-10T12:09:45.201452: step 1873, loss 0.533835, acc 0.828125, prec 0.0474084, recall 0.833709
2017-12-10T12:09:45.644170: step 1874, loss 0.786318, acc 0.796875, prec 0.0474497, recall 0.833896
2017-12-10T12:09:46.095217: step 1875, loss 0.944774, acc 0.75, prec 0.0474559, recall 0.83399
2017-12-10T12:09:46.535989: step 1876, loss 0.791955, acc 0.8125, prec 0.0474986, recall 0.834176
2017-12-10T12:09:46.983898: step 1877, loss 1.568, acc 0.75, prec 0.0475353, recall 0.834363
2017-12-10T12:09:47.419289: step 1878, loss 0.615458, acc 0.84375, prec 0.0475201, recall 0.834363
2017-12-10T12:09:47.851747: step 1879, loss 0.718288, acc 0.75, prec 0.0474958, recall 0.834363
2017-12-10T12:09:48.284733: step 1880, loss 0.663472, acc 0.859375, prec 0.047543, recall 0.834549
2017-12-10T12:09:48.723637: step 1881, loss 1.2408, acc 0.734375, prec 0.047578, recall 0.834734
2017-12-10T12:09:49.171603: step 1882, loss 0.764361, acc 0.84375, prec 0.0475932, recall 0.834826
2017-12-10T12:09:49.630527: step 1883, loss 0.743558, acc 0.796875, prec 0.0475735, recall 0.834826
2017-12-10T12:09:50.065729: step 1884, loss 0.647252, acc 0.828125, prec 0.0476783, recall 0.835196
2017-12-10T12:09:50.515111: step 1885, loss 0.491518, acc 0.78125, prec 0.0476874, recall 0.835288
2017-12-10T12:09:50.951309: step 1886, loss 0.609097, acc 0.8125, prec 0.0476691, recall 0.835288
2017-12-10T12:09:51.402087: step 1887, loss 0.787846, acc 0.78125, prec 0.0476479, recall 0.835288
2017-12-10T12:09:51.853735: step 1888, loss 0.637571, acc 0.859375, prec 0.0476342, recall 0.835288
2017-12-10T12:09:52.287939: step 1889, loss 0.883212, acc 0.78125, prec 0.0476736, recall 0.835471
2017-12-10T12:09:52.735092: step 1890, loss 0.570228, acc 0.875, prec 0.0477221, recall 0.835655
2017-12-10T12:09:53.177120: step 1891, loss 0.403287, acc 0.890625, prec 0.0477417, recall 0.835746
2017-12-10T12:09:53.612976: step 1892, loss 0.442044, acc 0.828125, prec 0.0477856, recall 0.835929
2017-12-10T12:09:54.041723: step 1893, loss 0.562025, acc 0.84375, prec 0.0477704, recall 0.835929
2017-12-10T12:09:54.467202: step 1894, loss 0.226993, acc 0.921875, prec 0.0477628, recall 0.835929
2017-12-10T12:09:54.920406: step 1895, loss 0.254035, acc 0.890625, prec 0.0477522, recall 0.835929
2017-12-10T12:09:55.360781: step 1896, loss 0.234567, acc 0.921875, prec 0.0477446, recall 0.835929
2017-12-10T12:09:55.817650: step 1897, loss 0.648209, acc 0.96875, prec 0.0477718, recall 0.83602
2017-12-10T12:09:56.266841: step 1898, loss 0.0277037, acc 0.984375, prec 0.0477703, recall 0.83602
2017-12-10T12:09:56.715859: step 1899, loss 0.0503846, acc 0.984375, prec 0.047799, recall 0.836111
2017-12-10T12:09:57.164070: step 1900, loss 0.514262, acc 0.9375, prec 0.0478232, recall 0.836202
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-1900

2017-12-10T12:09:59.029751: step 1901, loss 0.742497, acc 0.984375, prec 0.0478821, recall 0.836384
2017-12-10T12:09:59.481918: step 1902, loss 0.218523, acc 0.921875, prec 0.0479048, recall 0.836474
2017-12-10T12:09:59.920928: step 1903, loss 0.769684, acc 0.9375, prec 0.0479289, recall 0.836565
2017-12-10T12:10:00.388035: step 1904, loss 0.0929765, acc 0.953125, prec 0.0479243, recall 0.836565
2017-12-10T12:10:00.826763: step 1905, loss 0.145804, acc 0.9375, prec 0.0479183, recall 0.836565
2017-12-10T12:10:01.275034: step 1906, loss 0.45135, acc 0.9375, prec 0.0479726, recall 0.836746
2017-12-10T12:10:01.716455: step 1907, loss 0.207846, acc 0.9375, prec 0.0479665, recall 0.836746
2017-12-10T12:10:02.158179: step 1908, loss 4.74581, acc 0.953125, prec 0.0479635, recall 0.836283
2017-12-10T12:10:02.607685: step 1909, loss 0.38186, acc 0.90625, prec 0.0479543, recall 0.836283
2017-12-10T12:10:03.051010: step 1910, loss 0.416539, acc 0.9375, prec 0.0479784, recall 0.836374
2017-12-10T12:10:03.508280: step 1911, loss 0.620369, acc 0.84375, prec 0.0479632, recall 0.836374
2017-12-10T12:10:03.956969: step 1912, loss 0.401739, acc 0.859375, prec 0.0479797, recall 0.836464
2017-12-10T12:10:04.399523: step 1913, loss 0.595248, acc 0.828125, prec 0.0480233, recall 0.836645
2017-12-10T12:10:04.841207: step 1914, loss 0.487205, acc 0.890625, prec 0.0480127, recall 0.836645
2017-12-10T12:10:05.288360: step 1915, loss 0.412591, acc 0.890625, prec 0.048002, recall 0.836645
2017-12-10T12:10:05.738422: step 1916, loss 1.14665, acc 0.890625, prec 0.0480215, recall 0.836735
2017-12-10T12:10:06.184568: step 1917, loss 0.566841, acc 0.84375, prec 0.0480666, recall 0.836915
2017-12-10T12:10:06.616970: step 1918, loss 0.507754, acc 0.84375, prec 0.0480815, recall 0.837004
2017-12-10T12:10:07.048188: step 1919, loss 0.381631, acc 0.875, prec 0.0480693, recall 0.837004
2017-12-10T12:10:07.496620: step 1920, loss 0.351056, acc 0.890625, prec 0.0480888, recall 0.837094
2017-12-10T12:10:07.946853: step 1921, loss 0.417525, acc 0.828125, prec 0.0480721, recall 0.837094
2017-12-10T12:10:08.391656: step 1922, loss 0.326101, acc 0.90625, prec 0.048093, recall 0.837184
2017-12-10T12:10:08.828915: step 1923, loss 0.652718, acc 0.8125, prec 0.0481049, recall 0.837273
2017-12-10T12:10:09.276128: step 1924, loss 1.26639, acc 0.71875, prec 0.0481076, recall 0.837363
2017-12-10T12:10:09.717780: step 1925, loss 0.444173, acc 0.859375, prec 0.048154, recall 0.837541
2017-12-10T12:10:10.157986: step 1926, loss 0.398212, acc 0.859375, prec 0.0481703, recall 0.83763
2017-12-10T12:10:10.600921: step 1927, loss 0.1867, acc 0.9375, prec 0.0481943, recall 0.837719
2017-12-10T12:10:11.045309: step 1928, loss 1.43092, acc 0.921875, prec 0.0482182, recall 0.837349
2017-12-10T12:10:11.497942: step 1929, loss 0.263823, acc 0.921875, prec 0.0482106, recall 0.837349
2017-12-10T12:10:11.947448: step 1930, loss 0.184966, acc 0.921875, prec 0.048203, recall 0.837349
2017-12-10T12:10:12.380236: step 1931, loss 1.44273, acc 0.921875, prec 0.0482254, recall 0.837438
2017-12-10T12:10:12.822194: step 1932, loss 0.613063, acc 0.859375, prec 0.0482417, recall 0.837527
2017-12-10T12:10:13.268843: step 1933, loss 0.327069, acc 0.875, prec 0.0482296, recall 0.837527
2017-12-10T12:10:13.725044: step 1934, loss 0.459804, acc 0.84375, prec 0.0482444, recall 0.837616
2017-12-10T12:10:14.165274: step 1935, loss 0.330375, acc 0.875, prec 0.0482322, recall 0.837616
2017-12-10T12:10:14.605599: step 1936, loss 0.246856, acc 0.90625, prec 0.0482531, recall 0.837705
2017-12-10T12:10:15.068597: step 1937, loss 0.383844, acc 0.84375, prec 0.0482678, recall 0.837794
2017-12-10T12:10:15.506204: step 1938, loss 0.19724, acc 0.890625, prec 0.0482871, recall 0.837882
2017-12-10T12:10:15.944949: step 1939, loss 0.357077, acc 0.921875, prec 0.0482795, recall 0.837882
2017-12-10T12:10:16.389867: step 1940, loss 0.282013, acc 0.90625, prec 0.0483303, recall 0.838059
2017-12-10T12:10:16.835360: step 1941, loss 0.383202, acc 0.859375, prec 0.0483465, recall 0.838147
2017-12-10T12:10:17.273968: step 1942, loss 0.400335, acc 0.890625, prec 0.0483957, recall 0.838323
2017-12-10T12:10:17.719804: step 1943, loss 0.235901, acc 0.890625, prec 0.0483851, recall 0.838323
2017-12-10T12:10:18.164263: step 1944, loss 0.520879, acc 0.953125, prec 0.0484104, recall 0.838411
2017-12-10T12:10:18.603437: step 1945, loss 0.186025, acc 0.953125, prec 0.0484357, recall 0.838499
2017-12-10T12:10:19.046228: step 1946, loss 0.226486, acc 0.890625, prec 0.0484848, recall 0.838675
2017-12-10T12:10:19.481671: step 1947, loss 0.224577, acc 0.890625, prec 0.0485041, recall 0.838762
2017-12-10T12:10:19.912875: step 1948, loss 0.296136, acc 0.953125, prec 0.0484995, recall 0.838762
2017-12-10T12:10:20.349902: step 1949, loss 6.79003, acc 0.890625, prec 0.0485202, recall 0.838395
2017-12-10T12:10:20.792860: step 1950, loss 1.16816, acc 0.875, prec 0.0485379, recall 0.838482
2017-12-10T12:10:21.242934: step 1951, loss 0.334271, acc 0.875, prec 0.0485257, recall 0.838482
2017-12-10T12:10:21.697293: step 1952, loss 0.522238, acc 0.859375, prec 0.048512, recall 0.838482
2017-12-10T12:10:22.131551: step 1953, loss 0.420413, acc 0.859375, prec 0.0484983, recall 0.838482
2017-12-10T12:10:22.591284: step 1954, loss 0.214098, acc 0.921875, prec 0.0484907, recall 0.838482
2017-12-10T12:10:23.033958: step 1955, loss 0.41535, acc 0.84375, prec 0.0485054, recall 0.83857
2017-12-10T12:10:23.485207: step 1956, loss 0.697476, acc 0.859375, prec 0.0485215, recall 0.838657
2017-12-10T12:10:23.932683: step 1957, loss 0.805138, acc 0.8125, prec 0.0485628, recall 0.838832
2017-12-10T12:10:24.378301: step 1958, loss 0.238398, acc 0.890625, prec 0.048582, recall 0.838919
2017-12-10T12:10:24.829045: step 1959, loss 0.376542, acc 0.859375, prec 0.0485981, recall 0.839006
2017-12-10T12:10:25.273918: step 1960, loss 0.249875, acc 0.921875, prec 0.0485905, recall 0.839006
2017-12-10T12:10:25.712351: step 1961, loss 0.228667, acc 0.921875, prec 0.0486424, recall 0.83918
2017-12-10T12:10:26.145948: step 1962, loss 0.438045, acc 0.890625, prec 0.0486317, recall 0.83918
2017-12-10T12:10:26.586060: step 1963, loss 0.438794, acc 0.90625, prec 0.0486226, recall 0.83918
2017-12-10T12:10:27.028837: step 1964, loss 0.426342, acc 0.84375, prec 0.0486372, recall 0.839266
2017-12-10T12:10:27.486478: step 1965, loss 0.454999, acc 0.96875, prec 0.0486639, recall 0.839353
2017-12-10T12:10:27.936981: step 1966, loss 0.387478, acc 0.84375, prec 0.0486784, recall 0.83944
2017-12-10T12:10:28.377976: step 1967, loss 0.0599346, acc 0.984375, prec 0.0486769, recall 0.83944
2017-12-10T12:10:28.809714: step 1968, loss 0.488029, acc 0.90625, prec 0.0486677, recall 0.83944
2017-12-10T12:10:29.258084: step 1969, loss 0.150201, acc 0.96875, prec 0.0486647, recall 0.83944
2017-12-10T12:10:29.708676: step 1970, loss 0.42335, acc 0.921875, prec 0.0486571, recall 0.83944
2017-12-10T12:10:30.147767: step 1971, loss 0.983765, acc 0.875, prec 0.0486746, recall 0.839526
2017-12-10T12:10:30.592442: step 1972, loss 0.233289, acc 0.90625, prec 0.0486655, recall 0.839526
2017-12-10T12:10:31.035855: step 1973, loss 0.256579, acc 0.921875, prec 0.0486876, recall 0.839612
2017-12-10T12:10:31.478126: step 1974, loss 0.32717, acc 0.859375, prec 0.0486739, recall 0.839612
2017-12-10T12:10:31.925163: step 1975, loss 0.23451, acc 0.90625, prec 0.0486648, recall 0.839612
2017-12-10T12:10:32.370923: step 1976, loss 0.0968828, acc 0.96875, prec 0.0487211, recall 0.839785
2017-12-10T12:10:32.821943: step 1977, loss 0.203294, acc 0.9375, prec 0.0487744, recall 0.839957
2017-12-10T12:10:33.285814: step 1978, loss 0.310812, acc 0.9375, prec 0.048798, recall 0.840043
2017-12-10T12:10:33.731303: step 1979, loss 0.163716, acc 0.90625, prec 0.0487889, recall 0.840043
2017-12-10T12:10:34.175179: step 1980, loss 0.076086, acc 0.96875, prec 0.0488155, recall 0.840129
2017-12-10T12:10:34.618659: step 1981, loss 0.163961, acc 0.921875, prec 0.0488079, recall 0.840129
2017-12-10T12:10:35.058136: step 1982, loss 0.186062, acc 0.9375, prec 0.0488314, recall 0.840214
2017-12-10T12:10:35.500786: step 1983, loss 0.551425, acc 0.890625, prec 0.0488208, recall 0.840214
2017-12-10T12:10:35.939231: step 1984, loss 0.192918, acc 0.921875, prec 0.0488132, recall 0.840214
2017-12-10T12:10:36.390084: step 1985, loss 0.094021, acc 0.96875, prec 0.0488101, recall 0.840214
2017-12-10T12:10:36.829757: step 1986, loss 0.132211, acc 0.984375, prec 0.0488382, recall 0.8403
2017-12-10T12:10:37.276604: step 1987, loss 0.0298504, acc 1, prec 0.0488678, recall 0.840386
2017-12-10T12:10:37.678034: step 1988, loss 0.199938, acc 0.960784, prec 0.0488944, recall 0.840471
2017-12-10T12:10:38.152463: step 1989, loss 0.332983, acc 0.9375, prec 0.0488883, recall 0.840471
2017-12-10T12:10:38.595507: step 1990, loss 1.21748, acc 0.96875, prec 0.0488868, recall 0.840021
2017-12-10T12:10:39.047209: step 1991, loss 0.016981, acc 1, prec 0.0488868, recall 0.840021
2017-12-10T12:10:39.485468: step 1992, loss 0.183497, acc 0.984375, prec 0.0489445, recall 0.840192
2017-12-10T12:10:39.929548: step 1993, loss 0.32622, acc 0.9375, prec 0.048968, recall 0.840278
2017-12-10T12:10:40.368774: step 1994, loss 0.0684941, acc 0.96875, prec 0.048965, recall 0.840278
2017-12-10T12:10:40.812137: step 1995, loss 0.15522, acc 0.96875, prec 0.0490211, recall 0.840448
2017-12-10T12:10:41.260414: step 1996, loss 0.0760849, acc 0.96875, prec 0.0490773, recall 0.840618
2017-12-10T12:10:41.697641: step 1997, loss 0.224506, acc 0.90625, prec 0.0490977, recall 0.840703
2017-12-10T12:10:42.139533: step 1998, loss 0.212048, acc 0.859375, prec 0.0491431, recall 0.840873
2017-12-10T12:10:42.586561: step 1999, loss 0.167205, acc 0.9375, prec 0.049137, recall 0.840873
2017-12-10T12:10:43.036330: step 2000, loss 0.205514, acc 0.953125, prec 0.0491324, recall 0.840873
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2000

2017-12-10T12:10:44.853790: step 2001, loss 0.253958, acc 0.921875, prec 0.0491543, recall 0.840957
2017-12-10T12:10:45.291816: step 2002, loss 0.112047, acc 0.96875, prec 0.0491808, recall 0.841042
2017-12-10T12:10:45.733786: step 2003, loss 0.0818655, acc 0.953125, prec 0.0491762, recall 0.841042
2017-12-10T12:10:46.172317: step 2004, loss 0.0591252, acc 0.96875, prec 0.0492027, recall 0.841126
2017-12-10T12:10:46.609790: step 2005, loss 0.205195, acc 0.953125, prec 0.0492277, recall 0.841211
2017-12-10T12:10:47.052034: step 2006, loss 0.512875, acc 0.96875, prec 0.0492542, recall 0.841295
2017-12-10T12:10:47.512641: step 2007, loss 2.58259, acc 0.984375, prec 0.0493133, recall 0.841017
2017-12-10T12:10:47.967312: step 2008, loss 0.114995, acc 0.96875, prec 0.0493102, recall 0.841017
2017-12-10T12:10:48.408997: step 2009, loss 0.258073, acc 0.953125, prec 0.0493056, recall 0.841017
2017-12-10T12:10:48.849262: step 2010, loss 0.129522, acc 0.953125, prec 0.0493306, recall 0.841102
2017-12-10T12:10:49.293403: step 2011, loss 0.323157, acc 0.953125, prec 0.0493555, recall 0.841186
2017-12-10T12:10:49.731271: step 2012, loss 0.293243, acc 0.921875, prec 0.0493773, recall 0.84127
2017-12-10T12:10:50.174517: step 2013, loss 0.176142, acc 0.96875, prec 0.0493743, recall 0.84127
2017-12-10T12:10:50.616608: step 2014, loss 1.15058, acc 0.921875, prec 0.0493961, recall 0.841354
2017-12-10T12:10:51.048624: step 2015, loss 0.221833, acc 0.9375, prec 0.0494195, recall 0.841438
2017-12-10T12:10:51.490294: step 2016, loss 0.180297, acc 0.953125, prec 0.0494149, recall 0.841438
2017-12-10T12:10:51.941663: step 2017, loss 0.338881, acc 0.890625, prec 0.0494337, recall 0.841521
2017-12-10T12:10:52.379877: step 2018, loss 0.0522177, acc 0.96875, prec 0.0494306, recall 0.841521
2017-12-10T12:10:52.822149: step 2019, loss 0.388576, acc 0.921875, prec 0.0494229, recall 0.841521
2017-12-10T12:10:53.265589: step 2020, loss 0.110878, acc 0.953125, prec 0.0494478, recall 0.841605
2017-12-10T12:10:53.714257: step 2021, loss 0.366809, acc 0.90625, prec 0.0495271, recall 0.841856
2017-12-10T12:10:54.155106: step 2022, loss 0.366451, acc 0.921875, prec 0.0495488, recall 0.841939
2017-12-10T12:10:54.589682: step 2023, loss 0.206446, acc 0.9375, prec 0.0495722, recall 0.842022
2017-12-10T12:10:55.019753: step 2024, loss 0.559244, acc 0.859375, prec 0.0496467, recall 0.842271
2017-12-10T12:10:55.468461: step 2025, loss 0.565189, acc 0.84375, prec 0.0496608, recall 0.842354
2017-12-10T12:10:55.927609: step 2026, loss 0.378281, acc 0.90625, prec 0.049681, recall 0.842437
2017-12-10T12:10:56.366310: step 2027, loss 0.293882, acc 0.828125, prec 0.0496935, recall 0.84252
2017-12-10T12:10:56.818307: step 2028, loss 0.19692, acc 0.921875, prec 0.0497152, recall 0.842602
2017-12-10T12:10:57.266732: step 2029, loss 0.0946205, acc 0.953125, prec 0.04974, recall 0.842685
2017-12-10T12:10:57.704090: step 2030, loss 0.490687, acc 0.9375, prec 0.0498221, recall 0.842932
2017-12-10T12:10:58.150072: step 2031, loss 0.279891, acc 0.890625, prec 0.0498113, recall 0.842932
2017-12-10T12:10:58.593057: step 2032, loss 0.319813, acc 0.90625, prec 0.049802, recall 0.842932
2017-12-10T12:10:59.050468: step 2033, loss 0.0820602, acc 0.953125, prec 0.0498856, recall 0.843178
2017-12-10T12:10:59.499284: step 2034, loss 0.415283, acc 0.96875, prec 0.0499119, recall 0.84326
2017-12-10T12:10:59.946119: step 2035, loss 0.106617, acc 0.96875, prec 0.0499088, recall 0.84326
2017-12-10T12:11:00.387009: step 2036, loss 0.252897, acc 0.90625, prec 0.0498995, recall 0.84326
2017-12-10T12:11:00.833394: step 2037, loss 0.328696, acc 0.9375, prec 0.0499227, recall 0.843342
2017-12-10T12:11:01.277185: step 2038, loss 0.280476, acc 0.9375, prec 0.0499459, recall 0.843424
2017-12-10T12:11:01.720788: step 2039, loss 0.478868, acc 0.953125, prec 0.05, recall 0.843587
2017-12-10T12:11:02.168537: step 2040, loss 0.218715, acc 0.921875, prec 0.0499923, recall 0.843587
2017-12-10T12:11:02.615706: step 2041, loss 2.81963, acc 0.953125, prec 0.0500479, recall 0.843311
2017-12-10T12:11:03.054457: step 2042, loss 0.306363, acc 0.90625, prec 0.0500386, recall 0.843311
2017-12-10T12:11:03.497497: step 2043, loss 0.210112, acc 0.921875, prec 0.0500309, recall 0.843311
2017-12-10T12:11:03.949795: step 2044, loss 0.30665, acc 0.90625, prec 0.0500216, recall 0.843311
2017-12-10T12:11:04.409645: step 2045, loss 0.298704, acc 0.875, prec 0.0500972, recall 0.843555
2017-12-10T12:11:04.854837: step 2046, loss 0.589404, acc 0.875, prec 0.0501728, recall 0.843799
2017-12-10T12:11:05.300483: step 2047, loss 0.228466, acc 0.890625, prec 0.050162, recall 0.843799
2017-12-10T12:11:05.738866: step 2048, loss 0.237503, acc 0.921875, prec 0.0501835, recall 0.84388
2017-12-10T12:11:06.189841: step 2049, loss 0.234804, acc 0.890625, prec 0.0501727, recall 0.84388
2017-12-10T12:11:06.627673: step 2050, loss 2.7306, acc 0.9375, prec 0.0501973, recall 0.843523
2017-12-10T12:11:07.075550: step 2051, loss 0.238351, acc 0.90625, prec 0.050188, recall 0.843523
2017-12-10T12:11:07.517167: step 2052, loss 0.397253, acc 0.859375, prec 0.0501741, recall 0.843523
2017-12-10T12:11:07.950347: step 2053, loss 0.42942, acc 0.84375, prec 0.0501879, recall 0.843604
2017-12-10T12:11:08.396633: step 2054, loss 0.459324, acc 0.875, prec 0.0502048, recall 0.843685
2017-12-10T12:11:08.833136: step 2055, loss 0.648784, acc 0.78125, prec 0.0501832, recall 0.843685
2017-12-10T12:11:09.279972: step 2056, loss 0.554965, acc 0.828125, prec 0.0501662, recall 0.843685
2017-12-10T12:11:09.729163: step 2057, loss 1.27616, acc 0.703125, prec 0.0501953, recall 0.843847
2017-12-10T12:11:10.175782: step 2058, loss 0.50727, acc 0.84375, prec 0.0502383, recall 0.844008
2017-12-10T12:11:10.625294: step 2059, loss 0.335215, acc 0.90625, prec 0.050229, recall 0.844008
2017-12-10T12:11:11.060105: step 2060, loss 0.507053, acc 0.828125, prec 0.050212, recall 0.844008
2017-12-10T12:11:11.508129: step 2061, loss 0.577456, acc 0.828125, prec 0.0501951, recall 0.844008
2017-12-10T12:11:11.932797: step 2062, loss 0.745746, acc 0.84375, prec 0.050238, recall 0.844169
2017-12-10T12:11:12.372208: step 2063, loss 0.306937, acc 0.859375, prec 0.0502533, recall 0.84425
2017-12-10T12:11:12.824202: step 2064, loss 0.235764, acc 0.9375, prec 0.0502762, recall 0.84433
2017-12-10T12:11:13.266662: step 2065, loss 0.197565, acc 0.921875, prec 0.0503268, recall 0.84449
2017-12-10T12:11:13.712787: step 2066, loss 0.209742, acc 0.921875, prec 0.0503774, recall 0.84465
2017-12-10T12:11:14.150810: step 2067, loss 0.17584, acc 0.953125, prec 0.050431, recall 0.84481
2017-12-10T12:11:14.594870: step 2068, loss 0.285677, acc 0.9375, prec 0.0504539, recall 0.84489
2017-12-10T12:11:15.027373: step 2069, loss 0.170646, acc 0.953125, prec 0.0504493, recall 0.84489
2017-12-10T12:11:15.486318: step 2070, loss 0.208063, acc 0.9375, prec 0.0504431, recall 0.84489
2017-12-10T12:11:15.926068: step 2071, loss 0.21189, acc 0.9375, prec 0.0504369, recall 0.84489
2017-12-10T12:11:16.373603: step 2072, loss 0.38993, acc 0.921875, prec 0.0504292, recall 0.84489
2017-12-10T12:11:16.817946: step 2073, loss 0.425061, acc 0.9375, prec 0.0504521, recall 0.844969
2017-12-10T12:11:17.271709: step 2074, loss 0.0879839, acc 0.984375, prec 0.0505088, recall 0.845128
2017-12-10T12:11:17.711274: step 2075, loss 0.24267, acc 0.984375, prec 0.0506236, recall 0.845445
2017-12-10T12:11:18.157601: step 2076, loss 0.138516, acc 0.96875, prec 0.0506205, recall 0.845445
2017-12-10T12:11:18.606732: step 2077, loss 0.641236, acc 0.953125, prec 0.0506449, recall 0.845524
2017-12-10T12:11:19.040510: step 2078, loss 0.252289, acc 0.96875, prec 0.0506709, recall 0.845603
2017-12-10T12:11:19.487348: step 2079, loss 0.390941, acc 0.9375, prec 0.0506938, recall 0.845682
2017-12-10T12:11:19.934037: step 2080, loss 0.385211, acc 0.9375, prec 0.0507166, recall 0.845761
2017-12-10T12:11:20.391412: step 2081, loss 0.358696, acc 0.921875, prec 0.0507089, recall 0.845761
2017-12-10T12:11:20.832513: step 2082, loss 0.152501, acc 0.9375, prec 0.0507317, recall 0.84584
2017-12-10T12:11:21.257257: step 2083, loss 0.309361, acc 0.859375, prec 0.0507178, recall 0.84584
2017-12-10T12:11:21.695605: step 2084, loss 0.249947, acc 0.9375, prec 0.0507116, recall 0.84584
2017-12-10T12:11:22.159082: step 2085, loss 0.185388, acc 0.96875, prec 0.0507085, recall 0.84584
2017-12-10T12:11:22.606134: step 2086, loss 0.342025, acc 0.96875, prec 0.0508215, recall 0.846154
2017-12-10T12:11:23.057043: step 2087, loss 0.404566, acc 0.96875, prec 0.0508765, recall 0.84631
2017-12-10T12:11:23.502673: step 2088, loss 0.178946, acc 0.96875, prec 0.0508734, recall 0.84631
2017-12-10T12:11:23.944061: step 2089, loss 0.104446, acc 0.96875, prec 0.0508703, recall 0.84631
2017-12-10T12:11:24.400981: step 2090, loss 0.299823, acc 0.921875, prec 0.0508915, recall 0.846389
2017-12-10T12:11:24.840441: step 2091, loss 0.217561, acc 0.890625, prec 0.0508806, recall 0.846389
2017-12-10T12:11:25.288222: step 2092, loss 0.160777, acc 0.984375, prec 0.0508791, recall 0.846389
2017-12-10T12:11:25.726313: step 2093, loss 0.131271, acc 0.9375, prec 0.0508728, recall 0.846389
2017-12-10T12:11:26.168347: step 2094, loss 0.360597, acc 0.90625, prec 0.0508635, recall 0.846389
2017-12-10T12:11:26.611210: step 2095, loss 1.66935, acc 0.9375, prec 0.0508589, recall 0.845958
2017-12-10T12:11:27.070393: step 2096, loss 0.518578, acc 0.890625, prec 0.050877, recall 0.846037
2017-12-10T12:11:27.502326: step 2097, loss 0.857862, acc 0.921875, prec 0.0508982, recall 0.846115
2017-12-10T12:11:27.933879: step 2098, loss 0.252757, acc 0.90625, prec 0.0508889, recall 0.846115
2017-12-10T12:11:28.373888: step 2099, loss 0.373486, acc 0.90625, prec 0.0509085, recall 0.846193
2017-12-10T12:11:28.808169: step 2100, loss 0.227651, acc 0.96875, prec 0.0509054, recall 0.846193
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2100

2017-12-10T12:11:30.773046: step 2101, loss 0.364659, acc 0.890625, prec 0.0508945, recall 0.846193
2017-12-10T12:11:31.205967: step 2102, loss 0.256252, acc 0.90625, prec 0.0509432, recall 0.846349
2017-12-10T12:11:31.651276: step 2103, loss 0.472873, acc 0.859375, prec 0.0509581, recall 0.846427
2017-12-10T12:11:32.096977: step 2104, loss 0.32686, acc 0.921875, prec 0.0510372, recall 0.84666
2017-12-10T12:11:32.546941: step 2105, loss 0.302321, acc 0.9375, prec 0.0510599, recall 0.846738
2017-12-10T12:11:32.986223: step 2106, loss 0.749413, acc 0.828125, prec 0.0510428, recall 0.846738
2017-12-10T12:11:33.421782: step 2107, loss 0.153934, acc 0.9375, prec 0.0510655, recall 0.846815
2017-12-10T12:11:33.878572: step 2108, loss 0.188334, acc 0.921875, prec 0.0510577, recall 0.846815
2017-12-10T12:11:34.311717: step 2109, loss 0.668853, acc 0.875, prec 0.0510742, recall 0.846892
2017-12-10T12:11:34.758930: step 2110, loss 0.302508, acc 0.890625, prec 0.0510922, recall 0.84697
2017-12-10T12:11:35.213695: step 2111, loss 0.0680317, acc 0.984375, prec 0.0510907, recall 0.84697
2017-12-10T12:11:35.656982: step 2112, loss 0.640574, acc 0.859375, prec 0.0511345, recall 0.847124
2017-12-10T12:11:36.104827: step 2113, loss 0.971239, acc 0.921875, prec 0.0511845, recall 0.847278
2017-12-10T12:11:36.557002: step 2114, loss 0.2011, acc 0.921875, prec 0.0511767, recall 0.847278
2017-12-10T12:11:37.013193: step 2115, loss 0.211388, acc 0.90625, prec 0.0511962, recall 0.847355
2017-12-10T12:11:37.473547: step 2116, loss 0.432064, acc 0.875, prec 0.0511837, recall 0.847355
2017-12-10T12:11:37.915562: step 2117, loss 0.455704, acc 0.875, prec 0.0512001, recall 0.847432
2017-12-10T12:11:38.366150: step 2118, loss 0.386611, acc 0.90625, prec 0.0511908, recall 0.847432
2017-12-10T12:11:38.808686: step 2119, loss 0.438582, acc 0.90625, prec 0.0512392, recall 0.847585
2017-12-10T12:11:39.249779: step 2120, loss 0.171402, acc 0.96875, prec 0.0512361, recall 0.847585
2017-12-10T12:11:39.698871: step 2121, loss 0.40366, acc 0.890625, prec 0.0512251, recall 0.847585
2017-12-10T12:11:40.154209: step 2122, loss 0.248486, acc 0.890625, prec 0.0512142, recall 0.847585
2017-12-10T12:11:40.597392: step 2123, loss 0.357456, acc 0.921875, prec 0.0512641, recall 0.847739
2017-12-10T12:11:41.038081: step 2124, loss 0.223643, acc 0.921875, prec 0.0512563, recall 0.847739
2017-12-10T12:11:41.480558: step 2125, loss 0.281632, acc 0.921875, prec 0.0512486, recall 0.847739
2017-12-10T12:11:41.927790: step 2126, loss 0.182652, acc 0.9375, prec 0.0512423, recall 0.847739
2017-12-10T12:11:42.375431: step 2127, loss 0.0975846, acc 0.96875, prec 0.051268, recall 0.847815
2017-12-10T12:11:42.818432: step 2128, loss 0.245458, acc 0.921875, prec 0.0512602, recall 0.847815
2017-12-10T12:11:43.260198: step 2129, loss 0.097536, acc 0.96875, prec 0.0512571, recall 0.847815
2017-12-10T12:11:43.698769: step 2130, loss 0.101193, acc 0.96875, prec 0.0512828, recall 0.847892
2017-12-10T12:11:44.139887: step 2131, loss 0.212315, acc 0.90625, prec 0.0513023, recall 0.847968
2017-12-10T12:11:44.592146: step 2132, loss 1.33471, acc 0.953125, prec 0.0512992, recall 0.847543
2017-12-10T12:11:45.041460: step 2133, loss 0.169617, acc 0.953125, prec 0.0513521, recall 0.847695
2017-12-10T12:11:45.493466: step 2134, loss 0.237775, acc 0.953125, prec 0.0513762, recall 0.847772
2017-12-10T12:11:45.941254: step 2135, loss 0.0690399, acc 0.96875, prec 0.0513731, recall 0.847772
2017-12-10T12:11:46.381515: step 2136, loss 0.147648, acc 0.96875, prec 0.0513988, recall 0.847848
2017-12-10T12:11:46.827211: step 2137, loss 0.506604, acc 0.953125, prec 0.0514804, recall 0.848076
2017-12-10T12:11:47.278763: step 2138, loss 0.131429, acc 0.96875, prec 0.0514773, recall 0.848076
2017-12-10T12:11:47.722168: step 2139, loss 0.238494, acc 1, prec 0.0515348, recall 0.848228
2017-12-10T12:11:48.169666: step 2140, loss 0.248009, acc 0.90625, prec 0.0515254, recall 0.848228
2017-12-10T12:11:48.605774: step 2141, loss 0.0775097, acc 0.984375, prec 0.0515239, recall 0.848228
2017-12-10T12:11:49.040677: step 2142, loss 2.5325, acc 0.953125, prec 0.0515208, recall 0.847804
2017-12-10T12:11:49.491637: step 2143, loss 0.239059, acc 0.9375, prec 0.0515433, recall 0.84788
2017-12-10T12:11:49.940752: step 2144, loss 0.314388, acc 0.90625, prec 0.0515339, recall 0.84788
2017-12-10T12:11:50.384162: step 2145, loss 0.318121, acc 0.921875, prec 0.0515261, recall 0.84788
2017-12-10T12:11:50.836616: step 2146, loss 1.57601, acc 0.875, prec 0.0515152, recall 0.847458
2017-12-10T12:11:51.276469: step 2147, loss 0.32213, acc 0.875, prec 0.0515027, recall 0.847458
2017-12-10T12:11:51.717486: step 2148, loss 0.250129, acc 0.921875, prec 0.0514949, recall 0.847458
2017-12-10T12:11:52.159300: step 2149, loss 0.560964, acc 0.90625, prec 0.0515142, recall 0.847534
2017-12-10T12:11:52.610724: step 2150, loss 1.04791, acc 0.78125, prec 0.0514924, recall 0.847534
2017-12-10T12:11:53.053109: step 2151, loss 0.639729, acc 0.8125, prec 0.0515024, recall 0.84761
2017-12-10T12:11:53.491580: step 2152, loss 0.646385, acc 0.8125, prec 0.0514837, recall 0.84761
2017-12-10T12:11:53.931318: step 2153, loss 0.722713, acc 0.796875, prec 0.0514922, recall 0.847685
2017-12-10T12:11:54.387590: step 2154, loss 0.487592, acc 0.796875, prec 0.0515006, recall 0.847761
2017-12-10T12:11:54.825952: step 2155, loss 0.578267, acc 0.8125, prec 0.0515679, recall 0.847988
2017-12-10T12:11:55.267167: step 2156, loss 0.485027, acc 0.828125, prec 0.0515507, recall 0.847988
2017-12-10T12:11:55.711966: step 2157, loss 0.504822, acc 0.875, prec 0.0515669, recall 0.848064
2017-12-10T12:11:56.156247: step 2158, loss 0.447737, acc 0.84375, prec 0.05158, recall 0.848139
2017-12-10T12:11:56.598182: step 2159, loss 0.577244, acc 0.90625, prec 0.0516279, recall 0.84829
2017-12-10T12:11:57.056793: step 2160, loss 0.6389, acc 0.828125, prec 0.051668, recall 0.84844
2017-12-10T12:11:57.499621: step 2161, loss 0.442883, acc 0.890625, prec 0.0516571, recall 0.84844
2017-12-10T12:11:57.942970: step 2162, loss 0.352644, acc 0.90625, prec 0.0516477, recall 0.84844
2017-12-10T12:11:58.391409: step 2163, loss 0.961383, acc 0.890625, prec 0.051694, recall 0.84859
2017-12-10T12:11:58.841343: step 2164, loss 0.323507, acc 0.875, prec 0.0516815, recall 0.84859
2017-12-10T12:11:59.288748: step 2165, loss 0.189849, acc 0.953125, prec 0.051734, recall 0.84874
2017-12-10T12:11:59.728953: step 2166, loss 0.389725, acc 0.859375, prec 0.05172, recall 0.84874
2017-12-10T12:12:00.178745: step 2167, loss 0.273629, acc 0.921875, prec 0.0517122, recall 0.84874
2017-12-10T12:12:00.625286: step 2168, loss 0.232199, acc 0.921875, prec 0.051733, recall 0.848814
2017-12-10T12:12:01.074001: step 2169, loss 0.347851, acc 0.9375, prec 0.0517838, recall 0.848963
2017-12-10T12:12:01.515500: step 2170, loss 0.317619, acc 0.890625, prec 0.0518015, recall 0.849038
2017-12-10T12:12:01.956708: step 2171, loss 0.428158, acc 0.875, prec 0.051789, recall 0.849038
2017-12-10T12:12:02.403111: step 2172, loss 0.689986, acc 0.921875, prec 0.0518383, recall 0.849187
2017-12-10T12:12:02.853936: step 2173, loss 0.36303, acc 0.859375, prec 0.0518242, recall 0.849187
2017-12-10T12:12:03.291080: step 2174, loss 0.24741, acc 0.953125, prec 0.0518195, recall 0.849187
2017-12-10T12:12:03.734446: step 2175, loss 0.0604521, acc 0.984375, prec 0.0518465, recall 0.849261
2017-12-10T12:12:04.180126: step 2176, loss 0.167182, acc 0.9375, prec 0.0518403, recall 0.849261
2017-12-10T12:12:04.618634: step 2177, loss 0.141688, acc 0.953125, prec 0.0519211, recall 0.849484
2017-12-10T12:12:05.068581: step 2178, loss 7.13907, acc 0.9375, prec 0.0520019, recall 0.849288
2017-12-10T12:12:05.527966: step 2179, loss 0.408675, acc 0.953125, prec 0.0520827, recall 0.84951
2017-12-10T12:12:05.961079: step 2180, loss 0.202955, acc 0.921875, prec 0.0520749, recall 0.84951
2017-12-10T12:12:06.412990: step 2181, loss 0.134924, acc 0.953125, prec 0.0521271, recall 0.849657
2017-12-10T12:12:06.861213: step 2182, loss 0.402605, acc 0.875, prec 0.0521146, recall 0.849657
2017-12-10T12:12:07.301686: step 2183, loss 0.0728227, acc 0.984375, prec 0.0521415, recall 0.849731
2017-12-10T12:12:07.755859: step 2184, loss 0.244467, acc 0.921875, prec 0.0521337, recall 0.849731
2017-12-10T12:12:08.203267: step 2185, loss 0.39375, acc 0.859375, prec 0.0521765, recall 0.849878
2017-12-10T12:12:08.639072: step 2186, loss 0.213634, acc 0.921875, prec 0.0521971, recall 0.849951
2017-12-10T12:12:09.104946: step 2187, loss 0.468616, acc 0.828125, prec 0.0521799, recall 0.849951
2017-12-10T12:12:09.548477: step 2188, loss 0.584772, acc 0.828125, prec 0.0522196, recall 0.850098
2017-12-10T12:12:09.983102: step 2189, loss 0.345442, acc 0.84375, prec 0.0522039, recall 0.850098
2017-12-10T12:12:10.422029: step 2190, loss 0.39652, acc 0.90625, prec 0.0521945, recall 0.850098
2017-12-10T12:12:10.858306: step 2191, loss 0.124474, acc 0.96875, prec 0.0521914, recall 0.850098
2017-12-10T12:12:11.312741: step 2192, loss 0.494295, acc 0.875, prec 0.0521789, recall 0.850098
2017-12-10T12:12:11.754855: step 2193, loss 0.0821805, acc 0.96875, prec 0.0521757, recall 0.850098
2017-12-10T12:12:12.195138: step 2194, loss 0.589936, acc 0.875, prec 0.05222, recall 0.850244
2017-12-10T12:12:12.643190: step 2195, loss 0.301156, acc 0.921875, prec 0.052269, recall 0.85039
2017-12-10T12:12:13.105647: step 2196, loss 0.289153, acc 0.875, prec 0.0522848, recall 0.850463
2017-12-10T12:12:13.541436: step 2197, loss 0.531211, acc 0.921875, prec 0.0523338, recall 0.850608
2017-12-10T12:12:13.982541: step 2198, loss 0.274563, acc 0.859375, prec 0.0523197, recall 0.850608
2017-12-10T12:12:14.419629: step 2199, loss 0.351496, acc 0.890625, prec 0.0523938, recall 0.850826
2017-12-10T12:12:14.865713: step 2200, loss 0.119161, acc 0.9375, prec 0.0523875, recall 0.850826
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2200

2017-12-10T12:12:16.806984: step 2201, loss 1.07041, acc 0.921875, prec 0.0524647, recall 0.851043
2017-12-10T12:12:17.249485: step 2202, loss 0.268103, acc 0.921875, prec 0.0525135, recall 0.851188
2017-12-10T12:12:17.693294: step 2203, loss 0.199338, acc 0.9375, prec 0.0525073, recall 0.851188
2017-12-10T12:12:18.135711: step 2204, loss 0.211515, acc 0.90625, prec 0.0524978, recall 0.851188
2017-12-10T12:12:18.579795: step 2205, loss 0.463571, acc 0.90625, prec 0.0525734, recall 0.851404
2017-12-10T12:12:19.010415: step 2206, loss 0.413038, acc 0.9375, prec 0.0525671, recall 0.851404
2017-12-10T12:12:19.459767: step 2207, loss 0.138142, acc 0.921875, prec 0.0525592, recall 0.851404
2017-12-10T12:12:19.892107: step 2208, loss 0.320343, acc 0.9375, prec 0.0525813, recall 0.851476
2017-12-10T12:12:20.341128: step 2209, loss 0.202562, acc 0.96875, prec 0.0525781, recall 0.851476
2017-12-10T12:12:20.784454: step 2210, loss 0.122714, acc 0.953125, prec 0.0526017, recall 0.851547
2017-12-10T12:12:21.225437: step 2211, loss 0.723289, acc 0.921875, prec 0.0526787, recall 0.851762
2017-12-10T12:12:21.690656: step 2212, loss 0.214997, acc 0.953125, prec 0.0527023, recall 0.851834
2017-12-10T12:12:22.146250: step 2213, loss 0.260916, acc 0.921875, prec 0.052751, recall 0.851977
2017-12-10T12:12:22.591448: step 2214, loss 0.442635, acc 0.890625, prec 0.05274, recall 0.851977
2017-12-10T12:12:23.034330: step 2215, loss 0.170428, acc 0.90625, prec 0.0527305, recall 0.851977
2017-12-10T12:12:23.481193: step 2216, loss 5.55587, acc 0.90625, prec 0.0527509, recall 0.851638
2017-12-10T12:12:23.925342: step 2217, loss 0.463629, acc 0.90625, prec 0.0527697, recall 0.851709
2017-12-10T12:12:24.367013: step 2218, loss 0.396958, acc 0.890625, prec 0.052787, recall 0.851781
2017-12-10T12:12:24.807661: step 2219, loss 0.801778, acc 0.859375, prec 0.0528293, recall 0.851923
2017-12-10T12:12:25.257916: step 2220, loss 0.492839, acc 0.90625, prec 0.0528481, recall 0.851994
2017-12-10T12:12:25.702097: step 2221, loss 0.308117, acc 0.890625, prec 0.0528653, recall 0.852065
2017-12-10T12:12:26.145374: step 2222, loss 0.259789, acc 0.90625, prec 0.0528558, recall 0.852065
2017-12-10T12:12:26.586284: step 2223, loss 0.354674, acc 0.890625, prec 0.0528448, recall 0.852065
2017-12-10T12:12:27.022064: step 2224, loss 0.512936, acc 0.859375, prec 0.052887, recall 0.852207
2017-12-10T12:12:27.467751: step 2225, loss 0.611228, acc 0.828125, prec 0.0528697, recall 0.852207
2017-12-10T12:12:27.913330: step 2226, loss 0.160346, acc 0.921875, prec 0.0528619, recall 0.852207
2017-12-10T12:12:28.359837: step 2227, loss 6.65788, acc 0.90625, prec 0.0528822, recall 0.85187
2017-12-10T12:12:28.821598: step 2228, loss 0.465293, acc 0.859375, prec 0.052868, recall 0.85187
2017-12-10T12:12:29.264149: step 2229, loss 0.431096, acc 0.890625, prec 0.052857, recall 0.85187
2017-12-10T12:12:29.716132: step 2230, loss 0.556484, acc 0.828125, prec 0.0528397, recall 0.85187
2017-12-10T12:12:30.164473: step 2231, loss 0.694032, acc 0.90625, prec 0.0528585, recall 0.851941
2017-12-10T12:12:30.612623: step 2232, loss 0.656162, acc 0.8125, prec 0.0528678, recall 0.852012
2017-12-10T12:12:31.057064: step 2233, loss 0.558371, acc 0.859375, prec 0.0529099, recall 0.852153
2017-12-10T12:12:31.503921: step 2234, loss 0.510376, acc 0.859375, prec 0.0529801, recall 0.852365
2017-12-10T12:12:31.952045: step 2235, loss 0.533326, acc 0.75, prec 0.0529831, recall 0.852436
2017-12-10T12:12:32.399249: step 2236, loss 0.53564, acc 0.8125, prec 0.0530204, recall 0.852576
2017-12-10T12:12:32.836102: step 2237, loss 0.726268, acc 0.8125, prec 0.0530577, recall 0.852717
2017-12-10T12:12:33.269184: step 2238, loss 0.768449, acc 0.8125, prec 0.0530388, recall 0.852717
2017-12-10T12:12:33.716800: step 2239, loss 0.512603, acc 0.828125, prec 0.0530215, recall 0.852717
2017-12-10T12:12:34.159857: step 2240, loss 0.674384, acc 0.796875, prec 0.0530011, recall 0.852717
2017-12-10T12:12:34.612626: step 2241, loss 0.571695, acc 0.84375, prec 0.0529854, recall 0.852717
2017-12-10T12:12:35.066801: step 2242, loss 0.481539, acc 0.84375, prec 0.0529697, recall 0.852717
2017-12-10T12:12:35.513598: step 2243, loss 0.360771, acc 0.890625, prec 0.0530148, recall 0.852857
2017-12-10T12:12:35.963731: step 2244, loss 2.76799, acc 0.90625, prec 0.053035, recall 0.852521
2017-12-10T12:12:36.414676: step 2245, loss 0.306869, acc 0.890625, prec 0.053024, recall 0.852521
2017-12-10T12:12:36.850064: step 2246, loss 0.379092, acc 0.890625, prec 0.0530411, recall 0.852592
2017-12-10T12:12:37.288940: step 2247, loss 0.649041, acc 0.8125, prec 0.0530222, recall 0.852592
2017-12-10T12:12:37.721563: step 2248, loss 0.187246, acc 0.90625, prec 0.0530128, recall 0.852592
2017-12-10T12:12:38.163571: step 2249, loss 0.236665, acc 0.890625, prec 0.0530299, recall 0.852662
2017-12-10T12:12:38.614447: step 2250, loss 0.496247, acc 0.859375, prec 0.0530157, recall 0.852662
2017-12-10T12:12:39.053266: step 2251, loss 0.309654, acc 0.84375, prec 0.0530001, recall 0.852662
2017-12-10T12:12:39.491359: step 2252, loss 0.280764, acc 0.9375, prec 0.0530777, recall 0.852871
2017-12-10T12:12:39.944294: step 2253, loss 0.260649, acc 0.890625, prec 0.0530947, recall 0.852941
2017-12-10T12:12:40.405948: step 2254, loss 0.157292, acc 0.96875, prec 0.0530916, recall 0.852941
2017-12-10T12:12:40.850172: step 2255, loss 0.194095, acc 0.9375, prec 0.0531133, recall 0.853011
2017-12-10T12:12:41.299056: step 2256, loss 1.33925, acc 0.890625, prec 0.0531303, recall 0.853081
2017-12-10T12:12:41.732856: step 2257, loss 0.233471, acc 0.90625, prec 0.0531488, recall 0.85315
2017-12-10T12:12:42.180673: step 2258, loss 0.341583, acc 0.9375, prec 0.0531705, recall 0.85322
2017-12-10T12:12:42.627473: step 2259, loss 0.232613, acc 0.921875, prec 0.0531905, recall 0.853289
2017-12-10T12:12:43.059696: step 2260, loss 0.391608, acc 0.890625, prec 0.0531796, recall 0.853289
2017-12-10T12:12:43.488617: step 2261, loss 0.237188, acc 0.90625, prec 0.053226, recall 0.853428
2017-12-10T12:12:43.927711: step 2262, loss 0.160817, acc 0.9375, prec 0.0532476, recall 0.853497
2017-12-10T12:12:44.365491: step 2263, loss 0.303237, acc 0.90625, prec 0.0532382, recall 0.853497
2017-12-10T12:12:44.804267: step 2264, loss 1.40663, acc 0.96875, prec 0.0533746, recall 0.853843
2017-12-10T12:12:45.271067: step 2265, loss 0.0677167, acc 0.984375, prec 0.0534009, recall 0.853911
2017-12-10T12:12:45.709189: step 2266, loss 0.170278, acc 0.90625, prec 0.0533915, recall 0.853911
2017-12-10T12:12:46.145212: step 2267, loss 0.305751, acc 0.953125, prec 0.0533868, recall 0.853911
2017-12-10T12:12:46.594493: step 2268, loss 0.256889, acc 0.921875, prec 0.0534347, recall 0.854049
2017-12-10T12:12:47.032162: step 2269, loss 0.17337, acc 0.9375, prec 0.0534284, recall 0.854049
2017-12-10T12:12:47.470514: step 2270, loss 0.311059, acc 0.953125, prec 0.0534794, recall 0.854186
2017-12-10T12:12:47.921077: step 2271, loss 0.153788, acc 0.921875, prec 0.0535551, recall 0.854392
2017-12-10T12:12:48.366551: step 2272, loss 0.153268, acc 0.953125, prec 0.0535504, recall 0.854392
2017-12-10T12:12:48.821648: step 2273, loss 0.145621, acc 0.9375, prec 0.0535441, recall 0.854392
2017-12-10T12:12:49.286564: step 2274, loss 1.51172, acc 0.953125, prec 0.0535688, recall 0.854059
2017-12-10T12:12:49.722732: step 2275, loss 0.290754, acc 0.953125, prec 0.0536198, recall 0.854196
2017-12-10T12:12:50.155695: step 2276, loss 0.0885466, acc 0.96875, prec 0.0536445, recall 0.854264
2017-12-10T12:12:50.599499: step 2277, loss 0.422049, acc 0.953125, prec 0.0536676, recall 0.854333
2017-12-10T12:12:51.047659: step 2278, loss 0.444606, acc 0.875, prec 0.053655, recall 0.854333
2017-12-10T12:12:51.497410: step 2279, loss 0.624814, acc 0.90625, prec 0.0536733, recall 0.854401
2017-12-10T12:12:51.941130: step 2280, loss 0.262131, acc 0.890625, prec 0.0536623, recall 0.854401
2017-12-10T12:12:52.405093: step 2281, loss 0.173943, acc 0.921875, prec 0.0536544, recall 0.854401
2017-12-10T12:12:52.855663: step 2282, loss 0.346432, acc 0.875, prec 0.0536418, recall 0.854401
2017-12-10T12:12:53.294574: step 2283, loss 0.458648, acc 0.875, prec 0.053657, recall 0.854469
2017-12-10T12:12:53.734738: step 2284, loss 0.518036, acc 0.90625, prec 0.0537309, recall 0.854673
2017-12-10T12:12:54.188998: step 2285, loss 0.314934, acc 0.90625, prec 0.0537492, recall 0.854741
2017-12-10T12:12:54.639807: step 2286, loss 0.257317, acc 0.9375, prec 0.0537707, recall 0.854809
2017-12-10T12:12:55.084610: step 2287, loss 0.367196, acc 0.875, prec 0.0537581, recall 0.854809
2017-12-10T12:12:55.535497: step 2288, loss 0.333478, acc 0.90625, prec 0.0537764, recall 0.854876
2017-12-10T12:12:55.997871: step 2289, loss 0.202376, acc 0.9375, prec 0.0538256, recall 0.855012
2017-12-10T12:12:56.433881: step 2290, loss 0.358932, acc 0.921875, prec 0.0538455, recall 0.855079
2017-12-10T12:12:56.869485: step 2291, loss 0.242502, acc 0.9375, prec 0.0538392, recall 0.855079
2017-12-10T12:12:57.315198: step 2292, loss 0.210273, acc 0.9375, prec 0.0538606, recall 0.855147
2017-12-10T12:12:57.754310: step 2293, loss 0.0984061, acc 0.96875, prec 0.0538574, recall 0.855147
2017-12-10T12:12:58.203737: step 2294, loss 0.367673, acc 0.875, prec 0.0539003, recall 0.855282
2017-12-10T12:12:58.642643: step 2295, loss 0.332178, acc 0.875, prec 0.0539431, recall 0.855416
2017-12-10T12:12:59.101829: step 2296, loss 0.227618, acc 0.9375, prec 0.0539368, recall 0.855416
2017-12-10T12:12:59.556669: step 2297, loss 0.42687, acc 0.953125, prec 0.0539875, recall 0.85555
2017-12-10T12:13:00.013979: step 2298, loss 2.24092, acc 0.96875, prec 0.0540137, recall 0.85522
2017-12-10T12:13:00.464172: step 2299, loss 0.5305, acc 0.984375, prec 0.0540952, recall 0.855422
2017-12-10T12:13:00.905085: step 2300, loss 0.701183, acc 0.953125, prec 0.0541736, recall 0.855622
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2300

2017-12-10T12:13:02.784225: step 2301, loss 0.184408, acc 0.953125, prec 0.0541689, recall 0.855622
2017-12-10T12:13:03.220737: step 2302, loss 0.279303, acc 0.953125, prec 0.0542195, recall 0.855756
2017-12-10T12:13:03.671471: step 2303, loss 0.201858, acc 0.953125, prec 0.0542147, recall 0.855756
2017-12-10T12:13:04.120960: step 2304, loss 0.160623, acc 0.9375, prec 0.0542084, recall 0.855756
2017-12-10T12:13:04.558831: step 2305, loss 0.199586, acc 0.921875, prec 0.0542282, recall 0.855823
2017-12-10T12:13:04.993568: step 2306, loss 0.339706, acc 0.921875, prec 0.0542202, recall 0.855823
2017-12-10T12:13:05.429513: step 2307, loss 0.86986, acc 0.796875, prec 0.0541996, recall 0.855823
2017-12-10T12:13:05.871413: step 2308, loss 0.302737, acc 0.90625, prec 0.0542177, recall 0.855889
2017-12-10T12:13:06.314464: step 2309, loss 0.551926, acc 0.8125, prec 0.0542817, recall 0.856089
2017-12-10T12:13:06.760698: step 2310, loss 0.32134, acc 0.890625, prec 0.0542706, recall 0.856089
2017-12-10T12:13:07.216219: step 2311, loss 4.57281, acc 0.8125, prec 0.0543084, recall 0.855827
2017-12-10T12:13:07.647556: step 2312, loss 0.566296, acc 0.828125, prec 0.0543186, recall 0.855893
2017-12-10T12:13:08.096247: step 2313, loss 0.238094, acc 0.90625, prec 0.0543643, recall 0.856026
2017-12-10T12:13:08.529816: step 2314, loss 0.520735, acc 0.828125, prec 0.0543469, recall 0.856026
2017-12-10T12:13:08.966532: step 2315, loss 0.649698, acc 0.75, prec 0.0543215, recall 0.856026
2017-12-10T12:13:09.410807: step 2316, loss 0.847018, acc 0.84375, prec 0.054416, recall 0.85629
2017-12-10T12:13:09.859624: step 2317, loss 1.2151, acc 0.796875, prec 0.0544781, recall 0.856488
2017-12-10T12:13:10.305758: step 2318, loss 0.646625, acc 0.84375, prec 0.0545174, recall 0.856619
2017-12-10T12:13:10.740949: step 2319, loss 0.552508, acc 0.828125, prec 0.0545274, recall 0.856685
2017-12-10T12:13:11.188651: step 2320, loss 0.694981, acc 0.78125, prec 0.0545327, recall 0.856751
2017-12-10T12:13:11.635462: step 2321, loss 0.416699, acc 0.90625, prec 0.0545232, recall 0.856751
2017-12-10T12:13:12.076450: step 2322, loss 0.434442, acc 0.84375, prec 0.0545073, recall 0.856751
2017-12-10T12:13:12.522703: step 2323, loss 0.813172, acc 0.859375, prec 0.0545206, recall 0.856816
2017-12-10T12:13:12.968622: step 2324, loss 0.246259, acc 0.90625, prec 0.0545386, recall 0.856882
2017-12-10T12:13:13.417475: step 2325, loss 0.509276, acc 0.8125, prec 0.054547, recall 0.856947
2017-12-10T12:13:13.855487: step 2326, loss 0.262234, acc 0.890625, prec 0.0545359, recall 0.856947
2017-12-10T12:13:14.306062: step 2327, loss 0.207457, acc 0.90625, prec 0.0545539, recall 0.857012
2017-12-10T12:13:14.750763: step 2328, loss 0.712117, acc 0.90625, prec 0.0546269, recall 0.857208
2017-12-10T12:13:15.186762: step 2329, loss 0.319222, acc 0.875, prec 0.0546416, recall 0.857273
2017-12-10T12:13:15.630765: step 2330, loss 0.695738, acc 0.859375, prec 0.0546823, recall 0.857403
2017-12-10T12:13:16.086262: step 2331, loss 0.0863157, acc 0.96875, prec 0.0546791, recall 0.857403
2017-12-10T12:13:16.546389: step 2332, loss 0.144538, acc 0.953125, prec 0.0547018, recall 0.857468
2017-12-10T12:13:16.996328: step 2333, loss 0.149271, acc 0.9375, prec 0.0546954, recall 0.857468
2017-12-10T12:13:17.439878: step 2334, loss 1.47961, acc 0.953125, prec 0.0547197, recall 0.857143
2017-12-10T12:13:17.874972: step 2335, loss 0.527162, acc 0.890625, prec 0.054736, recall 0.857208
2017-12-10T12:13:18.319510: step 2336, loss 3.30776, acc 0.90625, prec 0.0547555, recall 0.856883
2017-12-10T12:13:18.767007: step 2337, loss 0.436967, acc 0.875, prec 0.0547703, recall 0.856948
2017-12-10T12:13:19.213053: step 2338, loss 0.179096, acc 0.9375, prec 0.0547913, recall 0.857013
2017-12-10T12:13:19.659368: step 2339, loss 0.413989, acc 0.890625, prec 0.0548076, recall 0.857078
2017-12-10T12:13:20.102960: step 2340, loss 0.460623, acc 0.875, prec 0.0548223, recall 0.857143
2017-12-10T12:13:20.546209: step 2341, loss 0.393423, acc 0.890625, prec 0.0548386, recall 0.857208
2017-12-10T12:13:20.985709: step 2342, loss 0.669778, acc 0.8125, prec 0.0548469, recall 0.857272
2017-12-10T12:13:21.417608: step 2343, loss 0.365764, acc 0.859375, prec 0.05486, recall 0.857337
2017-12-10T12:13:21.861761: step 2344, loss 1.43733, acc 0.84375, prec 0.0548715, recall 0.857402
2017-12-10T12:13:22.301680: step 2345, loss 0.370049, acc 0.90625, prec 0.0548894, recall 0.857466
2017-12-10T12:13:22.743743: step 2346, loss 1.93808, acc 0.875, prec 0.054933, recall 0.857207
2017-12-10T12:13:23.186581: step 2347, loss 0.534133, acc 0.859375, prec 0.054946, recall 0.857272
2017-12-10T12:13:23.628747: step 2348, loss 0.550616, acc 0.84375, prec 0.0549575, recall 0.857336
2017-12-10T12:13:24.069701: step 2349, loss 0.792583, acc 0.8125, prec 0.0549384, recall 0.857336
2017-12-10T12:13:24.493986: step 2350, loss 0.691109, acc 0.765625, prec 0.0549692, recall 0.857465
2017-12-10T12:13:24.958780: step 2351, loss 0.601498, acc 0.765625, prec 0.0549454, recall 0.857465
2017-12-10T12:13:25.395409: step 2352, loss 0.643011, acc 0.8125, prec 0.0549809, recall 0.857594
2017-12-10T12:13:25.829365: step 2353, loss 0.596018, acc 0.859375, prec 0.0549666, recall 0.857594
2017-12-10T12:13:26.280140: step 2354, loss 0.549134, acc 0.84375, prec 0.0549508, recall 0.857594
2017-12-10T12:13:26.718657: step 2355, loss 0.54615, acc 0.796875, prec 0.0549847, recall 0.857722
2017-12-10T12:13:27.157257: step 2356, loss 0.673345, acc 0.859375, prec 0.0549704, recall 0.857722
2017-12-10T12:13:27.600796: step 2357, loss 0.75056, acc 0.765625, prec 0.0549739, recall 0.857786
2017-12-10T12:13:28.047910: step 2358, loss 0.39732, acc 0.875, prec 0.0549612, recall 0.857786
2017-12-10T12:13:28.479854: step 2359, loss 0.413218, acc 0.875, prec 0.0549485, recall 0.857786
2017-12-10T12:13:28.919909: step 2360, loss 0.420076, acc 0.90625, prec 0.0549935, recall 0.857914
2017-12-10T12:13:29.363842: step 2361, loss 0.582205, acc 0.84375, prec 0.0550049, recall 0.857978
2017-12-10T12:13:29.804495: step 2362, loss 0.607695, acc 0.921875, prec 0.0550514, recall 0.858105
2017-12-10T12:13:30.248883: step 2363, loss 0.379734, acc 0.890625, prec 0.0550403, recall 0.858105
2017-12-10T12:13:30.691900: step 2364, loss 0.183744, acc 0.890625, prec 0.0550292, recall 0.858105
2017-12-10T12:13:31.140522: step 2365, loss 2.54908, acc 0.890625, prec 0.0551013, recall 0.857911
2017-12-10T12:13:31.579077: step 2366, loss 0.176265, acc 0.953125, prec 0.0550966, recall 0.857911
2017-12-10T12:13:32.020364: step 2367, loss 0.373412, acc 0.90625, prec 0.0551143, recall 0.857975
2017-12-10T12:13:32.473367: step 2368, loss 0.17656, acc 0.9375, prec 0.0551079, recall 0.857975
2017-12-10T12:13:32.908850: step 2369, loss 0.183052, acc 0.953125, prec 0.0551032, recall 0.857975
2017-12-10T12:13:33.345659: step 2370, loss 0.151839, acc 0.953125, prec 0.0551256, recall 0.858038
2017-12-10T12:13:33.786719: step 2371, loss 0.0842981, acc 0.96875, prec 0.0551224, recall 0.858038
2017-12-10T12:13:34.226894: step 2372, loss 0.658123, acc 0.9375, prec 0.055252, recall 0.858356
2017-12-10T12:13:34.667118: step 2373, loss 0.0819952, acc 0.984375, prec 0.0552775, recall 0.858419
2017-12-10T12:13:35.116285: step 2374, loss 0.343391, acc 0.890625, prec 0.0553207, recall 0.858545
2017-12-10T12:13:35.556934: step 2375, loss 0.339935, acc 0.921875, prec 0.0553399, recall 0.858608
2017-12-10T12:13:36.005027: step 2376, loss 0.200934, acc 0.9375, prec 0.0553336, recall 0.858608
2017-12-10T12:13:36.449695: step 2377, loss 0.224153, acc 0.921875, prec 0.0553256, recall 0.858608
2017-12-10T12:13:36.899902: step 2378, loss 0.141473, acc 0.953125, prec 0.0553752, recall 0.858734
2017-12-10T12:13:37.343217: step 2379, loss 0.0291498, acc 1, prec 0.0554023, recall 0.858797
2017-12-10T12:13:37.783952: step 2380, loss 0.0618315, acc 0.984375, prec 0.0554007, recall 0.858797
2017-12-10T12:13:38.225243: step 2381, loss 0.143516, acc 0.9375, prec 0.0554758, recall 0.858986
2017-12-10T12:13:38.669842: step 2382, loss 0.294976, acc 0.953125, prec 0.0555252, recall 0.859111
2017-12-10T12:13:39.123850: step 2383, loss 0.117538, acc 0.953125, prec 0.0555476, recall 0.859174
2017-12-10T12:13:39.558176: step 2384, loss 0.335995, acc 0.96875, prec 0.0555986, recall 0.859299
2017-12-10T12:13:40.015182: step 2385, loss 0.145811, acc 0.953125, prec 0.0555938, recall 0.859299
2017-12-10T12:13:40.469743: step 2386, loss 0.64258, acc 0.890625, prec 0.0556098, recall 0.859361
2017-12-10T12:13:40.918817: step 2387, loss 3.53578, acc 0.96875, prec 0.0556082, recall 0.85898
2017-12-10T12:13:41.365163: step 2388, loss 0.13571, acc 0.953125, prec 0.0556034, recall 0.85898
2017-12-10T12:13:41.800930: step 2389, loss 0.158138, acc 0.96875, prec 0.0556273, recall 0.859043
2017-12-10T12:13:42.238995: step 2390, loss 0.154437, acc 0.953125, prec 0.0556225, recall 0.859043
2017-12-10T12:13:42.686445: step 2391, loss 0.345711, acc 0.875, prec 0.0556369, recall 0.859105
2017-12-10T12:13:43.124945: step 2392, loss 0.27577, acc 0.921875, prec 0.055656, recall 0.859167
2017-12-10T12:13:43.571028: step 2393, loss 0.193282, acc 0.953125, prec 0.0557054, recall 0.859292
2017-12-10T12:13:44.022697: step 2394, loss 0.249978, acc 0.921875, prec 0.0556974, recall 0.859292
2017-12-10T12:13:44.465733: step 2395, loss 0.239058, acc 0.9375, prec 0.055691, recall 0.859292
2017-12-10T12:13:44.906494: step 2396, loss 0.190337, acc 0.953125, prec 0.0556862, recall 0.859292
2017-12-10T12:13:45.359867: step 2397, loss 0.377408, acc 0.890625, prec 0.055675, recall 0.859292
2017-12-10T12:13:45.815668: step 2398, loss 0.306967, acc 0.921875, prec 0.055667, recall 0.859292
2017-12-10T12:13:46.263144: step 2399, loss 3.41403, acc 0.890625, prec 0.0556575, recall 0.858912
2017-12-10T12:13:46.711800: step 2400, loss 0.19441, acc 0.90625, prec 0.0556749, recall 0.858974
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2400

2017-12-10T12:13:48.637243: step 2401, loss 0.406872, acc 0.890625, prec 0.0556908, recall 0.859037
2017-12-10T12:13:49.077777: step 2402, loss 0.179276, acc 0.9375, prec 0.0557385, recall 0.859161
2017-12-10T12:13:49.519560: step 2403, loss 0.28933, acc 0.90625, prec 0.055729, recall 0.859161
2017-12-10T12:13:49.963059: step 2404, loss 0.848661, acc 0.796875, prec 0.0557082, recall 0.859161
2017-12-10T12:13:50.404527: step 2405, loss 0.161653, acc 0.921875, prec 0.0557003, recall 0.859161
2017-12-10T12:13:50.844979: step 2406, loss 0.407206, acc 0.890625, prec 0.0557702, recall 0.859347
2017-12-10T12:13:51.288395: step 2407, loss 0.533769, acc 0.859375, prec 0.0557558, recall 0.859347
2017-12-10T12:13:51.735632: step 2408, loss 0.545713, acc 0.84375, prec 0.0557669, recall 0.859409
2017-12-10T12:13:52.172491: step 2409, loss 0.407319, acc 0.890625, prec 0.0557557, recall 0.859409
2017-12-10T12:13:52.626354: step 2410, loss 0.358461, acc 0.90625, prec 0.0557461, recall 0.859409
2017-12-10T12:13:53.069948: step 2411, loss 0.25501, acc 0.890625, prec 0.055735, recall 0.859409
2017-12-10T12:13:53.519796: step 2412, loss 0.838335, acc 0.8125, prec 0.0557429, recall 0.859471
2017-12-10T12:13:53.963598: step 2413, loss 0.363113, acc 0.890625, prec 0.0557587, recall 0.859533
2017-12-10T12:13:54.400653: step 2414, loss 0.468464, acc 0.859375, prec 0.0557444, recall 0.859533
2017-12-10T12:13:54.839781: step 2415, loss 1.1575, acc 0.921875, prec 0.0557634, recall 0.859595
2017-12-10T12:13:55.289081: step 2416, loss 1.506, acc 0.875, prec 0.0557792, recall 0.859279
2017-12-10T12:13:55.726385: step 2417, loss 0.273644, acc 0.890625, prec 0.055768, recall 0.859279
2017-12-10T12:13:56.168125: step 2418, loss 0.234167, acc 0.984375, prec 0.0558203, recall 0.859402
2017-12-10T12:13:56.614114: step 2419, loss 0.15291, acc 0.9375, prec 0.055814, recall 0.859402
2017-12-10T12:13:57.085815: step 2420, loss 0.365644, acc 0.875, prec 0.0558012, recall 0.859402
2017-12-10T12:13:57.522602: step 2421, loss 0.328543, acc 0.890625, prec 0.055817, recall 0.859464
2017-12-10T12:13:57.975374: step 2422, loss 0.41923, acc 0.84375, prec 0.0558549, recall 0.859588
2017-12-10T12:13:58.413767: step 2423, loss 0.344494, acc 0.890625, prec 0.0558707, recall 0.859649
2017-12-10T12:13:58.859191: step 2424, loss 0.20417, acc 0.953125, prec 0.0558928, recall 0.859711
2017-12-10T12:13:59.305960: step 2425, loss 0.17448, acc 0.921875, prec 0.0558849, recall 0.859711
2017-12-10T12:13:59.741826: step 2426, loss 0.25425, acc 0.921875, prec 0.0559038, recall 0.859772
2017-12-10T12:14:00.180012: step 2427, loss 0.0584516, acc 0.96875, prec 0.0559275, recall 0.859834
2017-12-10T12:14:00.624365: step 2428, loss 0.168139, acc 0.9375, prec 0.055948, recall 0.859895
2017-12-10T12:14:01.062233: step 2429, loss 0.187383, acc 0.890625, prec 0.0559369, recall 0.859895
2017-12-10T12:14:01.501495: step 2430, loss 0.311071, acc 0.90625, prec 0.0559542, recall 0.859956
2017-12-10T12:14:01.943084: step 2431, loss 0.134472, acc 0.96875, prec 0.055951, recall 0.859956
2017-12-10T12:14:02.377963: step 2432, loss 0.386304, acc 0.921875, prec 0.0559431, recall 0.859956
2017-12-10T12:14:02.822486: step 2433, loss 0.23339, acc 0.90625, prec 0.0559335, recall 0.859956
2017-12-10T12:14:03.271392: step 2434, loss 0.0965717, acc 0.953125, prec 0.0559556, recall 0.860017
2017-12-10T12:14:03.718801: step 2435, loss 0.0519461, acc 0.96875, prec 0.0559524, recall 0.860017
2017-12-10T12:14:04.167800: step 2436, loss 0.155645, acc 0.96875, prec 0.0559492, recall 0.860017
2017-12-10T12:14:04.634120: step 2437, loss 0.272555, acc 0.984375, prec 0.0559476, recall 0.860017
2017-12-10T12:14:05.083898: step 2438, loss 0.495254, acc 0.9375, prec 0.055995, recall 0.86014
2017-12-10T12:14:05.530801: step 2439, loss 0.131832, acc 0.953125, prec 0.0559902, recall 0.86014
2017-12-10T12:14:05.966434: step 2440, loss 0.303084, acc 0.953125, prec 0.0560123, recall 0.860201
2017-12-10T12:14:06.404672: step 2441, loss 0.118018, acc 0.953125, prec 0.0560075, recall 0.860201
2017-12-10T12:14:06.840086: step 2442, loss 0.385444, acc 0.953125, prec 0.0560027, recall 0.860201
2017-12-10T12:14:07.286353: step 2443, loss 0.176223, acc 0.984375, prec 0.056028, recall 0.860262
2017-12-10T12:14:07.734177: step 2444, loss 0.205308, acc 0.921875, prec 0.05602, recall 0.860262
2017-12-10T12:14:08.186549: step 2445, loss 0.329829, acc 0.96875, prec 0.0560437, recall 0.860323
2017-12-10T12:14:08.641632: step 2446, loss 0.0749064, acc 0.984375, prec 0.0560421, recall 0.860323
2017-12-10T12:14:09.090706: step 2447, loss 0.185103, acc 1, prec 0.0560958, recall 0.860445
2017-12-10T12:14:09.542810: step 2448, loss 0.600754, acc 0.984375, prec 0.0561478, recall 0.860566
2017-12-10T12:14:09.985444: step 2449, loss 0.245326, acc 0.9375, prec 0.0561414, recall 0.860566
2017-12-10T12:14:10.443573: step 2450, loss 0.0162007, acc 1, prec 0.0561414, recall 0.860566
2017-12-10T12:14:10.880238: step 2451, loss 0.171945, acc 0.96875, prec 0.0562187, recall 0.860748
2017-12-10T12:14:11.324284: step 2452, loss 0.207883, acc 0.984375, prec 0.056244, recall 0.860809
2017-12-10T12:14:11.767573: step 2453, loss 0.100264, acc 0.96875, prec 0.0562676, recall 0.86087
2017-12-10T12:14:12.212831: step 2454, loss 0.0336837, acc 1, prec 0.0562676, recall 0.86087
2017-12-10T12:14:12.651579: step 2455, loss 0.0311858, acc 0.984375, prec 0.0562928, recall 0.86093
2017-12-10T12:14:13.098113: step 2456, loss 0.459023, acc 0.984375, prec 0.0563448, recall 0.861051
2017-12-10T12:14:13.544420: step 2457, loss 0.0843277, acc 0.96875, prec 0.0563416, recall 0.861051
2017-12-10T12:14:14.000617: step 2458, loss 0.166596, acc 0.9375, prec 0.056362, recall 0.861111
2017-12-10T12:14:14.462168: step 2459, loss 0.224347, acc 0.953125, prec 0.0563572, recall 0.861111
2017-12-10T12:14:14.902726: step 2460, loss 6.60813, acc 0.984375, prec 0.0563572, recall 0.860738
2017-12-10T12:14:15.364172: step 2461, loss 0.0714188, acc 0.953125, prec 0.0563524, recall 0.860738
2017-12-10T12:14:15.821262: step 2462, loss 0.202863, acc 0.90625, prec 0.0563428, recall 0.860738
2017-12-10T12:14:16.265058: step 2463, loss 0.178367, acc 0.953125, prec 0.0563648, recall 0.860798
2017-12-10T12:14:16.701881: step 2464, loss 0.165744, acc 0.96875, prec 0.0563884, recall 0.860858
2017-12-10T12:14:17.147676: step 2465, loss 0.361872, acc 0.953125, prec 0.0564372, recall 0.860979
2017-12-10T12:14:17.582344: step 2466, loss 1.24835, acc 0.90625, prec 0.0565079, recall 0.861159
2017-12-10T12:14:18.037362: step 2467, loss 0.426849, acc 0.890625, prec 0.0565235, recall 0.861219
2017-12-10T12:14:18.489449: step 2468, loss 0.135152, acc 0.9375, prec 0.0565974, recall 0.861399
2017-12-10T12:14:18.929501: step 2469, loss 0.652169, acc 0.875, prec 0.0565845, recall 0.861399
2017-12-10T12:14:19.367510: step 2470, loss 1.90996, acc 0.890625, prec 0.0566803, recall 0.861638
2017-12-10T12:14:19.822234: step 2471, loss 0.602649, acc 0.828125, prec 0.0566626, recall 0.861638
2017-12-10T12:14:20.262520: step 2472, loss 0.829268, acc 0.765625, prec 0.0566652, recall 0.861698
2017-12-10T12:14:20.702100: step 2473, loss 0.646794, acc 0.796875, prec 0.0566978, recall 0.861817
2017-12-10T12:14:21.137614: step 2474, loss 0.749195, acc 0.8125, prec 0.0567053, recall 0.861876
2017-12-10T12:14:21.593848: step 2475, loss 0.690768, acc 0.78125, prec 0.0566828, recall 0.861876
2017-12-10T12:14:22.034617: step 2476, loss 0.70878, acc 0.8125, prec 0.0567169, recall 0.861995
2017-12-10T12:14:22.486477: step 2477, loss 0.844748, acc 0.75, prec 0.0567179, recall 0.862054
2017-12-10T12:14:22.920512: step 2478, loss 0.373447, acc 0.859375, prec 0.0567035, recall 0.862054
2017-12-10T12:14:23.357878: step 2479, loss 0.809407, acc 0.734375, prec 0.0567029, recall 0.862113
2017-12-10T12:14:23.791582: step 2480, loss 0.799416, acc 0.796875, prec 0.0567087, recall 0.862173
2017-12-10T12:14:24.235288: step 2481, loss 0.491172, acc 0.875, prec 0.0566959, recall 0.862173
2017-12-10T12:14:24.678785: step 2482, loss 0.430393, acc 0.875, prec 0.0567098, recall 0.862232
2017-12-10T12:14:25.114474: step 2483, loss 0.462308, acc 0.8125, prec 0.0567172, recall 0.862291
2017-12-10T12:14:25.543667: step 2484, loss 0.532709, acc 0.828125, prec 0.056806, recall 0.862527
2017-12-10T12:14:25.942769: step 2485, loss 0.405264, acc 0.862745, prec 0.0567948, recall 0.862527
2017-12-10T12:14:26.402278: step 2486, loss 0.395161, acc 0.859375, prec 0.0568336, recall 0.862644
2017-12-10T12:14:26.842180: step 2487, loss 0.437056, acc 0.859375, prec 0.0568457, recall 0.862703
2017-12-10T12:14:27.291080: step 2488, loss 0.521923, acc 0.921875, prec 0.0568643, recall 0.862762
2017-12-10T12:14:27.735817: step 2489, loss 0.417522, acc 0.859375, prec 0.0568764, recall 0.862821
2017-12-10T12:14:28.167371: step 2490, loss 0.116634, acc 0.953125, prec 0.0568716, recall 0.862821
2017-12-10T12:14:28.617214: step 2491, loss 0.0413968, acc 0.984375, prec 0.0568966, recall 0.862879
2017-12-10T12:14:29.060532: step 2492, loss 0.144274, acc 0.921875, prec 0.0569151, recall 0.862938
2017-12-10T12:14:29.503034: step 2493, loss 0.103567, acc 0.96875, prec 0.0569119, recall 0.862938
2017-12-10T12:14:29.957076: step 2494, loss 0.325032, acc 0.890625, prec 0.0569273, recall 0.862996
2017-12-10T12:14:30.405923: step 2495, loss 0.128893, acc 0.9375, prec 0.0569209, recall 0.862996
2017-12-10T12:14:30.845814: step 2496, loss 0.419665, acc 0.9375, prec 0.0569675, recall 0.863113
2017-12-10T12:14:31.281383: step 2497, loss 0.721721, acc 0.921875, prec 0.0570126, recall 0.86323
2017-12-10T12:14:31.717123: step 2498, loss 0.307472, acc 0.96875, prec 0.0570359, recall 0.863288
2017-12-10T12:14:32.173891: step 2499, loss 0.0619604, acc 0.96875, prec 0.0570327, recall 0.863288
2017-12-10T12:14:32.615746: step 2500, loss 0.164439, acc 0.9375, prec 0.0570528, recall 0.863346
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2500

2017-12-10T12:14:34.517804: step 2501, loss 0.13594, acc 0.9375, prec 0.0570729, recall 0.863404
2017-12-10T12:14:34.969077: step 2502, loss 2.19925, acc 0.9375, prec 0.0570946, recall 0.863095
2017-12-10T12:14:35.413295: step 2503, loss 0.0960071, acc 0.96875, prec 0.0570914, recall 0.863095
2017-12-10T12:14:35.845353: step 2504, loss 0.475339, acc 0.875, prec 0.0571051, recall 0.863153
2017-12-10T12:14:36.276543: step 2505, loss 0.181551, acc 0.921875, prec 0.0570971, recall 0.863153
2017-12-10T12:14:36.719565: step 2506, loss 0.233635, acc 0.90625, prec 0.0571139, recall 0.863212
2017-12-10T12:14:37.154979: step 2507, loss 0.29634, acc 0.90625, prec 0.0571308, recall 0.86327
2017-12-10T12:14:37.605755: step 2508, loss 0.186159, acc 0.9375, prec 0.0571244, recall 0.86327
2017-12-10T12:14:38.042583: step 2509, loss 0.0820925, acc 0.953125, prec 0.0571196, recall 0.86327
2017-12-10T12:14:38.488693: step 2510, loss 0.194738, acc 0.9375, prec 0.0571661, recall 0.863386
2017-12-10T12:14:38.936774: step 2511, loss 0.195633, acc 0.921875, prec 0.0571581, recall 0.863386
2017-12-10T12:14:39.382383: step 2512, loss 0.181428, acc 0.9375, prec 0.0572046, recall 0.863501
2017-12-10T12:14:39.829270: step 2513, loss 0.13353, acc 0.921875, prec 0.0571966, recall 0.863501
2017-12-10T12:14:40.266757: step 2514, loss 0.23299, acc 0.90625, prec 0.057187, recall 0.863501
2017-12-10T12:14:40.713648: step 2515, loss 0.472608, acc 0.9375, prec 0.0572335, recall 0.863617
2017-12-10T12:14:41.160308: step 2516, loss 0.387511, acc 0.953125, prec 0.0572551, recall 0.863675
2017-12-10T12:14:41.594877: step 2517, loss 0.187996, acc 0.953125, prec 0.0572768, recall 0.863733
2017-12-10T12:14:42.070364: step 2518, loss 0.223198, acc 0.921875, prec 0.0573216, recall 0.863848
2017-12-10T12:14:42.528554: step 2519, loss 0.145516, acc 0.96875, prec 0.0573449, recall 0.863905
2017-12-10T12:14:42.978989: step 2520, loss 3.06364, acc 0.921875, prec 0.0573913, recall 0.863656
2017-12-10T12:14:43.434704: step 2521, loss 0.150655, acc 0.953125, prec 0.0573865, recall 0.863656
2017-12-10T12:14:43.876108: step 2522, loss 0.429851, acc 0.953125, prec 0.0574081, recall 0.863713
2017-12-10T12:14:44.321899: step 2523, loss 0.211365, acc 0.921875, prec 0.0574265, recall 0.863771
2017-12-10T12:14:44.761189: step 2524, loss 0.146342, acc 0.953125, prec 0.0574216, recall 0.863771
2017-12-10T12:14:45.214849: step 2525, loss 0.347176, acc 0.921875, prec 0.05744, recall 0.863828
2017-12-10T12:14:45.663507: step 2526, loss 0.0987373, acc 0.96875, prec 0.0574896, recall 0.863943
2017-12-10T12:14:46.116060: step 2527, loss 0.201066, acc 0.9375, prec 0.0575624, recall 0.864114
2017-12-10T12:14:46.564679: step 2528, loss 0.803724, acc 0.96875, prec 0.0575856, recall 0.864172
2017-12-10T12:14:47.003796: step 2529, loss 0.214907, acc 0.9375, prec 0.057632, recall 0.864286
2017-12-10T12:14:47.442840: step 2530, loss 0.311795, acc 0.90625, prec 0.0576223, recall 0.864286
2017-12-10T12:14:47.885767: step 2531, loss 0.179455, acc 0.921875, prec 0.057667, recall 0.8644
2017-12-10T12:14:48.328392: step 2532, loss 0.295262, acc 0.90625, prec 0.0576573, recall 0.8644
2017-12-10T12:14:48.773762: step 2533, loss 0.144019, acc 0.953125, prec 0.0576788, recall 0.864457
2017-12-10T12:14:49.217418: step 2534, loss 0.543071, acc 0.90625, prec 0.0576692, recall 0.864457
2017-12-10T12:14:49.668777: step 2535, loss 0.324035, acc 0.953125, prec 0.0576907, recall 0.864513
2017-12-10T12:14:50.112536: step 2536, loss 0.132894, acc 0.96875, prec 0.0576875, recall 0.864513
2017-12-10T12:14:50.560231: step 2537, loss 0.215254, acc 0.90625, prec 0.0576778, recall 0.864513
2017-12-10T12:14:51.005256: step 2538, loss 1.98425, acc 0.890625, prec 0.0576945, recall 0.864208
2017-12-10T12:14:51.448532: step 2539, loss 0.324601, acc 0.90625, prec 0.0577111, recall 0.864265
2017-12-10T12:14:51.888069: step 2540, loss 0.302796, acc 0.890625, prec 0.0576998, recall 0.864265
2017-12-10T12:14:52.328755: step 2541, loss 0.193183, acc 0.921875, prec 0.0576918, recall 0.864265
2017-12-10T12:14:52.762178: step 2542, loss 0.397849, acc 0.921875, prec 0.0577101, recall 0.864322
2017-12-10T12:14:53.206788: step 2543, loss 0.365947, acc 0.90625, prec 0.0577531, recall 0.864435
2017-12-10T12:14:53.648194: step 2544, loss 0.512781, acc 0.8125, prec 0.0577337, recall 0.864435
2017-12-10T12:14:54.080342: step 2545, loss 0.204615, acc 0.90625, prec 0.057724, recall 0.864435
2017-12-10T12:14:54.527511: step 2546, loss 0.558694, acc 0.875, prec 0.0577637, recall 0.864549
2017-12-10T12:14:54.955906: step 2547, loss 0.285088, acc 0.890625, prec 0.0578314, recall 0.864718
2017-12-10T12:14:55.387776: step 2548, loss 0.490682, acc 0.890625, prec 0.0578464, recall 0.864775
2017-12-10T12:14:55.830358: step 2549, loss 0.341937, acc 0.921875, prec 0.0578909, recall 0.864887
2017-12-10T12:14:56.270980: step 2550, loss 0.381422, acc 0.859375, prec 0.0578764, recall 0.864887
2017-12-10T12:14:56.703781: step 2551, loss 0.191158, acc 0.921875, prec 0.0578946, recall 0.864944
2017-12-10T12:14:57.148592: step 2552, loss 0.254545, acc 0.90625, prec 0.0578849, recall 0.864944
2017-12-10T12:14:57.586128: step 2553, loss 0.636315, acc 0.828125, prec 0.0579197, recall 0.865056
2017-12-10T12:14:58.025389: step 2554, loss 0.306036, acc 0.859375, prec 0.0579052, recall 0.865056
2017-12-10T12:14:58.456607: step 2555, loss 0.214324, acc 0.953125, prec 0.0579528, recall 0.865169
2017-12-10T12:14:58.910353: step 2556, loss 0.219894, acc 0.9375, prec 0.0579726, recall 0.865225
2017-12-10T12:14:59.366825: step 2557, loss 0.40885, acc 0.890625, prec 0.0579613, recall 0.865225
2017-12-10T12:14:59.803455: step 2558, loss 0.177743, acc 0.953125, prec 0.0579565, recall 0.865225
2017-12-10T12:15:00.238211: step 2559, loss 0.0539284, acc 0.984375, prec 0.0580074, recall 0.865337
2017-12-10T12:15:00.672803: step 2560, loss 0.11082, acc 0.96875, prec 0.0580041, recall 0.865337
2017-12-10T12:15:01.112971: step 2561, loss 0.251409, acc 0.9375, prec 0.0579977, recall 0.865337
2017-12-10T12:15:01.555627: step 2562, loss 0.215243, acc 0.90625, prec 0.057988, recall 0.865337
2017-12-10T12:15:01.993653: step 2563, loss 0.113153, acc 0.953125, prec 0.0580356, recall 0.865449
2017-12-10T12:15:02.430964: step 2564, loss 0.0521285, acc 0.984375, prec 0.058034, recall 0.865449
2017-12-10T12:15:02.882391: step 2565, loss 0.190424, acc 0.953125, prec 0.0580291, recall 0.865449
2017-12-10T12:15:03.329749: step 2566, loss 0.121234, acc 0.953125, prec 0.0580243, recall 0.865449
2017-12-10T12:15:03.777631: step 2567, loss 0.0539061, acc 1, prec 0.0580505, recall 0.865504
2017-12-10T12:15:04.226709: step 2568, loss 0.282428, acc 0.984375, prec 0.0581013, recall 0.865616
2017-12-10T12:15:04.674407: step 2569, loss 0.124144, acc 0.96875, prec 0.0581243, recall 0.865672
2017-12-10T12:15:05.116977: step 2570, loss 0.0739707, acc 0.96875, prec 0.0581211, recall 0.865672
2017-12-10T12:15:05.557641: step 2571, loss 0.0376939, acc 0.984375, prec 0.0581195, recall 0.865672
2017-12-10T12:15:06.003704: step 2572, loss 0.16973, acc 0.984375, prec 0.0581441, recall 0.865727
2017-12-10T12:15:06.461168: step 2573, loss 0.808258, acc 0.953125, prec 0.0581654, recall 0.865783
2017-12-10T12:15:06.919527: step 2574, loss 0.512626, acc 0.9375, prec 0.0582114, recall 0.865894
2017-12-10T12:15:07.373822: step 2575, loss 0.0223662, acc 0.984375, prec 0.0582097, recall 0.865894
2017-12-10T12:15:07.826428: step 2576, loss 0.14282, acc 0.953125, prec 0.0582049, recall 0.865894
2017-12-10T12:15:08.267058: step 2577, loss 0.0317122, acc 1, prec 0.0582049, recall 0.865894
2017-12-10T12:15:08.709408: step 2578, loss 1.57373, acc 0.984375, prec 0.0582311, recall 0.865591
2017-12-10T12:15:09.148068: step 2579, loss 0.0499673, acc 0.984375, prec 0.0582295, recall 0.865591
2017-12-10T12:15:09.583888: step 2580, loss 0.220618, acc 0.9375, prec 0.058223, recall 0.865591
2017-12-10T12:15:10.031275: step 2581, loss 0.291749, acc 0.9375, prec 0.0582165, recall 0.865591
2017-12-10T12:15:10.478525: step 2582, loss 0.242973, acc 0.953125, prec 0.0582902, recall 0.865758
2017-12-10T12:15:10.926112: step 2583, loss 0.038026, acc 0.984375, prec 0.0582886, recall 0.865758
2017-12-10T12:15:11.373668: step 2584, loss 0.19562, acc 0.921875, prec 0.0583067, recall 0.865813
2017-12-10T12:15:11.822475: step 2585, loss 0.168621, acc 0.9375, prec 0.0583264, recall 0.865869
2017-12-10T12:15:12.263121: step 2586, loss 0.275462, acc 0.90625, prec 0.0583167, recall 0.865869
2017-12-10T12:15:12.710267: step 2587, loss 0.109123, acc 0.953125, prec 0.0583641, recall 0.865979
2017-12-10T12:15:13.150648: step 2588, loss 0.430131, acc 0.9375, prec 0.05841, recall 0.86609
2017-12-10T12:15:13.590408: step 2589, loss 0.666081, acc 0.953125, prec 0.0584836, recall 0.866255
2017-12-10T12:15:14.035372: step 2590, loss 0.208802, acc 0.953125, prec 0.0585572, recall 0.86642
2017-12-10T12:15:14.479455: step 2591, loss 0.618509, acc 0.96875, prec 0.0585801, recall 0.866475
2017-12-10T12:15:14.937998: step 2592, loss 0.174979, acc 0.921875, prec 0.0585981, recall 0.86653
2017-12-10T12:15:15.394148: step 2593, loss 0.13713, acc 0.96875, prec 0.0585948, recall 0.86653
2017-12-10T12:15:15.841925: step 2594, loss 0.215003, acc 0.9375, prec 0.0585883, recall 0.86653
2017-12-10T12:15:16.288072: step 2595, loss 0.151455, acc 0.953125, prec 0.0585834, recall 0.86653
2017-12-10T12:15:16.732006: step 2596, loss 0.18279, acc 0.953125, prec 0.0585786, recall 0.86653
2017-12-10T12:15:17.167414: step 2597, loss 0.287596, acc 0.921875, prec 0.0585966, recall 0.866585
2017-12-10T12:15:17.607761: step 2598, loss 0.289249, acc 0.890625, prec 0.0586113, recall 0.866639
2017-12-10T12:15:18.049060: step 2599, loss 0.240128, acc 0.890625, prec 0.058626, recall 0.866694
2017-12-10T12:15:18.493354: step 2600, loss 0.535376, acc 0.84375, prec 0.0586359, recall 0.866749
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2600

2017-12-10T12:15:20.368257: step 2601, loss 0.275728, acc 0.90625, prec 0.0586784, recall 0.866858
2017-12-10T12:15:20.806514: step 2602, loss 0.338403, acc 0.828125, prec 0.0586866, recall 0.866912
2017-12-10T12:15:21.242692: step 2603, loss 0.268953, acc 0.90625, prec 0.0586768, recall 0.866912
2017-12-10T12:15:21.691351: step 2604, loss 0.069838, acc 0.96875, prec 0.0586735, recall 0.866912
2017-12-10T12:15:22.131910: step 2605, loss 0.103638, acc 0.96875, prec 0.0586703, recall 0.866912
2017-12-10T12:15:22.571740: step 2606, loss 0.380784, acc 0.90625, prec 0.0587127, recall 0.867021
2017-12-10T12:15:23.020903: step 2607, loss 0.128583, acc 0.921875, prec 0.0587567, recall 0.86713
2017-12-10T12:15:23.469093: step 2608, loss 0.202429, acc 0.90625, prec 0.058747, recall 0.86713
2017-12-10T12:15:23.910205: step 2609, loss 0.172608, acc 0.9375, prec 0.0587926, recall 0.867239
2017-12-10T12:15:24.349873: step 2610, loss 0.42633, acc 0.953125, prec 0.0588138, recall 0.867293
2017-12-10T12:15:24.794379: step 2611, loss 0.22492, acc 0.921875, prec 0.0588317, recall 0.867347
2017-12-10T12:15:25.239702: step 2612, loss 0.168695, acc 0.96875, prec 0.0588805, recall 0.867455
2017-12-10T12:15:25.684767: step 2613, loss 0.21437, acc 0.90625, prec 0.0588707, recall 0.867455
2017-12-10T12:15:26.125392: step 2614, loss 0.201505, acc 0.921875, prec 0.0589147, recall 0.867563
2017-12-10T12:15:26.570985: step 2615, loss 0.140433, acc 0.96875, prec 0.0589114, recall 0.867563
2017-12-10T12:15:27.019535: step 2616, loss 0.140776, acc 0.96875, prec 0.0589342, recall 0.867617
2017-12-10T12:15:27.461993: step 2617, loss 2.20217, acc 0.9375, prec 0.0589814, recall 0.867372
2017-12-10T12:15:27.922252: step 2618, loss 0.221773, acc 0.953125, prec 0.0589765, recall 0.867372
2017-12-10T12:15:28.352550: step 2619, loss 0.0276128, acc 1, prec 0.0589765, recall 0.867372
2017-12-10T12:15:28.799052: step 2620, loss 0.0144012, acc 1, prec 0.0589765, recall 0.867372
2017-12-10T12:15:29.234570: step 2621, loss 0.180024, acc 0.96875, prec 0.0590253, recall 0.86748
2017-12-10T12:15:29.678646: step 2622, loss 0.0736242, acc 0.96875, prec 0.059022, recall 0.86748
2017-12-10T12:15:30.119374: step 2623, loss 0.133156, acc 0.96875, prec 0.0590187, recall 0.86748
2017-12-10T12:15:30.559356: step 2624, loss 0.355252, acc 0.9375, prec 0.0590122, recall 0.86748
2017-12-10T12:15:30.989393: step 2625, loss 0.165361, acc 0.9375, prec 0.0590577, recall 0.867587
2017-12-10T12:15:31.425119: step 2626, loss 0.166662, acc 0.953125, prec 0.0590528, recall 0.867587
2017-12-10T12:15:31.858581: step 2627, loss 0.10493, acc 0.953125, prec 0.0590479, recall 0.867587
2017-12-10T12:15:32.304063: step 2628, loss 0.152367, acc 0.9375, prec 0.0590414, recall 0.867587
2017-12-10T12:15:32.751568: step 2629, loss 0.0593231, acc 0.96875, prec 0.0590641, recall 0.867641
2017-12-10T12:15:33.207606: step 2630, loss 0.0816057, acc 0.96875, prec 0.0590869, recall 0.867695
2017-12-10T12:15:33.668111: step 2631, loss 0.240386, acc 0.953125, prec 0.059082, recall 0.867695
2017-12-10T12:15:34.119336: step 2632, loss 0.201022, acc 0.9375, prec 0.0591015, recall 0.867748
2017-12-10T12:15:34.559925: step 2633, loss 0.228999, acc 0.953125, prec 0.0590966, recall 0.867748
2017-12-10T12:15:34.994266: step 2634, loss 0.862929, acc 0.96875, prec 0.0591193, recall 0.867802
2017-12-10T12:15:35.445399: step 2635, loss 0.226817, acc 0.953125, prec 0.0591664, recall 0.867909
2017-12-10T12:15:35.892389: step 2636, loss 0.115665, acc 0.953125, prec 0.0591615, recall 0.867909
2017-12-10T12:15:36.331723: step 2637, loss 0.0835022, acc 0.96875, prec 0.0591582, recall 0.867909
2017-12-10T12:15:36.773206: step 2638, loss 0.27616, acc 0.953125, prec 0.0591533, recall 0.867909
2017-12-10T12:15:37.214256: step 2639, loss 0.157365, acc 0.921875, prec 0.0591711, recall 0.867963
2017-12-10T12:15:37.655930: step 2640, loss 0.306403, acc 0.921875, prec 0.0591629, recall 0.867963
2017-12-10T12:15:38.106027: step 2641, loss 3.01413, acc 0.96875, prec 0.0591613, recall 0.867611
2017-12-10T12:15:38.549402: step 2642, loss 0.133899, acc 0.96875, prec 0.05921, recall 0.867718
2017-12-10T12:15:39.002815: step 2643, loss 0.216593, acc 0.890625, prec 0.0592245, recall 0.867772
2017-12-10T12:15:39.437747: step 2644, loss 0.282644, acc 0.921875, prec 0.0592163, recall 0.867772
2017-12-10T12:15:39.887789: step 2645, loss 0.334569, acc 0.90625, prec 0.0592065, recall 0.867772
2017-12-10T12:15:40.322332: step 2646, loss 0.171761, acc 0.90625, prec 0.0592227, recall 0.867825
2017-12-10T12:15:40.765229: step 2647, loss 0.360216, acc 0.875, prec 0.0592356, recall 0.867879
2017-12-10T12:15:41.214619: step 2648, loss 0.442079, acc 0.90625, prec 0.0592517, recall 0.867932
2017-12-10T12:15:41.664305: step 2649, loss 0.422985, acc 0.90625, prec 0.0592678, recall 0.867985
2017-12-10T12:15:42.105176: step 2650, loss 0.364382, acc 0.921875, prec 0.0592856, recall 0.868039
2017-12-10T12:15:42.552837: step 2651, loss 0.470947, acc 0.9375, prec 0.0593568, recall 0.868198
2017-12-10T12:15:42.990606: step 2652, loss 0.279241, acc 0.90625, prec 0.059347, recall 0.868198
2017-12-10T12:15:43.427767: step 2653, loss 0.400123, acc 0.90625, prec 0.0593372, recall 0.868198
2017-12-10T12:15:43.866959: step 2654, loss 0.234396, acc 0.875, prec 0.05935, recall 0.868251
2017-12-10T12:15:44.311393: step 2655, loss 0.393164, acc 0.875, prec 0.0593629, recall 0.868304
2017-12-10T12:15:44.755300: step 2656, loss 0.0923039, acc 0.953125, prec 0.0593839, recall 0.868357
2017-12-10T12:15:45.195492: step 2657, loss 0.339686, acc 0.921875, prec 0.0593757, recall 0.868357
2017-12-10T12:15:45.655621: step 2658, loss 0.41868, acc 0.90625, prec 0.0593659, recall 0.868357
2017-12-10T12:15:46.105631: step 2659, loss 0.232525, acc 0.875, prec 0.0594046, recall 0.868463
2017-12-10T12:15:46.560330: step 2660, loss 0.15978, acc 0.9375, prec 0.0594498, recall 0.868569
2017-12-10T12:15:46.995337: step 2661, loss 0.0980897, acc 0.953125, prec 0.0594449, recall 0.868569
2017-12-10T12:15:47.433776: step 2662, loss 0.0598233, acc 0.96875, prec 0.0594416, recall 0.868569
2017-12-10T12:15:47.879826: step 2663, loss 0.059896, acc 0.96875, prec 0.0594383, recall 0.868569
2017-12-10T12:15:48.322548: step 2664, loss 0.0188809, acc 1, prec 0.0594383, recall 0.868569
2017-12-10T12:15:48.755834: step 2665, loss 0.0859553, acc 0.96875, prec 0.0594609, recall 0.868622
2017-12-10T12:15:49.194358: step 2666, loss 0.194697, acc 0.9375, prec 0.0594544, recall 0.868622
2017-12-10T12:15:49.640762: step 2667, loss 0.215208, acc 0.953125, prec 0.0594754, recall 0.868675
2017-12-10T12:15:50.087488: step 2668, loss 1.92382, acc 0.90625, prec 0.0594672, recall 0.868326
2017-12-10T12:15:50.533574: step 2669, loss 0.163145, acc 0.9375, prec 0.0594606, recall 0.868326
2017-12-10T12:15:50.972580: step 2670, loss 0.516225, acc 0.9375, prec 0.0595317, recall 0.868484
2017-12-10T12:15:51.409772: step 2671, loss 0.22164, acc 0.890625, prec 0.0595719, recall 0.86859
2017-12-10T12:15:51.848234: step 2672, loss 0.0891803, acc 0.953125, prec 0.0595928, recall 0.868642
2017-12-10T12:15:52.282403: step 2673, loss 0.187244, acc 0.9375, prec 0.0595863, recall 0.868642
2017-12-10T12:15:52.711555: step 2674, loss 0.1923, acc 0.9375, prec 0.0596056, recall 0.868695
2017-12-10T12:15:53.153970: step 2675, loss 0.188662, acc 0.96875, prec 0.0596281, recall 0.868747
2017-12-10T12:15:53.612928: step 2676, loss 0.196445, acc 0.96875, prec 0.0596507, recall 0.8688
2017-12-10T12:15:54.060814: step 2677, loss 0.137426, acc 0.953125, prec 0.0596457, recall 0.8688
2017-12-10T12:15:54.492625: step 2678, loss 0.0643832, acc 0.984375, prec 0.0596441, recall 0.8688
2017-12-10T12:15:54.922698: step 2679, loss 1.38169, acc 0.90625, prec 0.0596617, recall 0.868505
2017-12-10T12:15:55.369003: step 2680, loss 0.444531, acc 0.9375, prec 0.0597068, recall 0.86861
2017-12-10T12:15:55.819820: step 2681, loss 0.283833, acc 0.953125, prec 0.0597535, recall 0.868715
2017-12-10T12:15:56.275781: step 2682, loss 0.144293, acc 0.9375, prec 0.059747, recall 0.868715
2017-12-10T12:15:56.722938: step 2683, loss 0.200374, acc 0.890625, prec 0.0597871, recall 0.86882
2017-12-10T12:15:57.194029: step 2684, loss 0.427137, acc 0.90625, prec 0.059803, recall 0.868872
2017-12-10T12:15:57.634146: step 2685, loss 0.114154, acc 0.953125, prec 0.0597981, recall 0.868872
2017-12-10T12:15:58.079614: step 2686, loss 0.127711, acc 0.96875, prec 0.0598206, recall 0.868924
2017-12-10T12:15:58.520644: step 2687, loss 0.172539, acc 0.921875, prec 0.0598124, recall 0.868924
2017-12-10T12:15:58.972693: step 2688, loss 3.93248, acc 0.84375, prec 0.0597977, recall 0.868578
2017-12-10T12:15:59.424655: step 2689, loss 0.219575, acc 0.953125, prec 0.0597927, recall 0.868578
2017-12-10T12:15:59.880500: step 2690, loss 0.38841, acc 0.9375, prec 0.059812, recall 0.868631
2017-12-10T12:16:00.319622: step 2691, loss 0.298187, acc 0.890625, prec 0.0598778, recall 0.868787
2017-12-10T12:16:00.765141: step 2692, loss 0.592602, acc 0.859375, prec 0.0599145, recall 0.868892
2017-12-10T12:16:01.202758: step 2693, loss 0.601733, acc 0.84375, prec 0.0598981, recall 0.868892
2017-12-10T12:16:01.636088: step 2694, loss 0.451472, acc 0.890625, prec 0.0599124, recall 0.868944
2017-12-10T12:16:02.069784: step 2695, loss 0.34041, acc 0.859375, prec 0.0599234, recall 0.868996
2017-12-10T12:16:02.498302: step 2696, loss 0.311184, acc 0.859375, prec 0.0599086, recall 0.868996
2017-12-10T12:16:02.938331: step 2697, loss 0.459293, acc 0.875, prec 0.0598955, recall 0.868996
2017-12-10T12:16:03.375137: step 2698, loss 0.312257, acc 0.890625, prec 0.059884, recall 0.868996
2017-12-10T12:16:03.830100: step 2699, loss 0.332604, acc 0.921875, prec 0.0599015, recall 0.869048
2017-12-10T12:16:04.301160: step 2700, loss 0.312651, acc 0.890625, prec 0.0599415, recall 0.869151
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2700

2017-12-10T12:16:06.340478: step 2701, loss 0.367446, acc 0.953125, prec 0.059988, recall 0.869255
2017-12-10T12:16:06.783586: step 2702, loss 0.184817, acc 0.921875, prec 0.0599798, recall 0.869255
2017-12-10T12:16:07.228202: step 2703, loss 0.603302, acc 0.859375, prec 0.0600421, recall 0.86941
2017-12-10T12:16:07.663247: step 2704, loss 0.401011, acc 0.890625, prec 0.0600306, recall 0.86941
2017-12-10T12:16:08.105702: step 2705, loss 0.0955643, acc 0.953125, prec 0.0600257, recall 0.86941
2017-12-10T12:16:08.553010: step 2706, loss 0.845574, acc 0.921875, prec 0.0600688, recall 0.869514
2017-12-10T12:16:09.003313: step 2707, loss 0.111823, acc 0.953125, prec 0.0600896, recall 0.869565
2017-12-10T12:16:09.444285: step 2708, loss 0.081874, acc 0.984375, prec 0.0600879, recall 0.869565
2017-12-10T12:16:09.890491: step 2709, loss 2.54978, acc 0.921875, prec 0.0600814, recall 0.869222
2017-12-10T12:16:10.312613: step 2710, loss 0.229317, acc 0.921875, prec 0.0600988, recall 0.869273
2017-12-10T12:16:10.746889: step 2711, loss 0.182199, acc 0.953125, prec 0.0600939, recall 0.869273
2017-12-10T12:16:11.198155: step 2712, loss 0.514651, acc 0.859375, prec 0.0601048, recall 0.869325
2017-12-10T12:16:11.645291: step 2713, loss 0.328575, acc 0.875, prec 0.0600917, recall 0.869325
2017-12-10T12:16:12.086103: step 2714, loss 0.485705, acc 0.828125, prec 0.0600737, recall 0.869325
2017-12-10T12:16:12.518724: step 2715, loss 0.351332, acc 0.875, prec 0.0600606, recall 0.869325
2017-12-10T12:16:12.965195: step 2716, loss 0.123847, acc 0.984375, prec 0.0601358, recall 0.869479
2017-12-10T12:16:13.407006: step 2717, loss 0.392857, acc 0.890625, prec 0.0601243, recall 0.869479
2017-12-10T12:16:13.847041: step 2718, loss 0.442671, acc 0.90625, prec 0.0601657, recall 0.869582
2017-12-10T12:16:14.317193: step 2719, loss 0.349468, acc 0.875, prec 0.0601526, recall 0.869582
2017-12-10T12:16:14.765620: step 2720, loss 0.583393, acc 0.84375, prec 0.0601618, recall 0.869634
2017-12-10T12:16:15.210322: step 2721, loss 0.279003, acc 0.953125, prec 0.0601825, recall 0.869685
2017-12-10T12:16:15.663276: step 2722, loss 0.464214, acc 0.90625, prec 0.0601983, recall 0.869736
2017-12-10T12:16:16.110418: step 2723, loss 0.320156, acc 0.875, prec 0.0602108, recall 0.869788
2017-12-10T12:16:16.555819: step 2724, loss 0.201289, acc 0.953125, prec 0.0602315, recall 0.869839
2017-12-10T12:16:16.989299: step 2725, loss 0.113164, acc 0.9375, prec 0.0602249, recall 0.869839
2017-12-10T12:16:17.426117: step 2726, loss 0.205604, acc 0.9375, prec 0.0602183, recall 0.869839
2017-12-10T12:16:17.869845: step 2727, loss 0.219778, acc 0.9375, prec 0.0602885, recall 0.869992
2017-12-10T12:16:18.327049: step 2728, loss 0.570397, acc 0.90625, prec 0.0603298, recall 0.870094
2017-12-10T12:16:18.768159: step 2729, loss 0.0944348, acc 0.96875, prec 0.0603265, recall 0.870094
2017-12-10T12:16:19.213465: step 2730, loss 1.40028, acc 0.9375, prec 0.0603216, recall 0.869753
2017-12-10T12:16:19.666032: step 2731, loss 0.0910557, acc 0.96875, prec 0.0603183, recall 0.869753
2017-12-10T12:16:20.126844: step 2732, loss 0.0789233, acc 0.96875, prec 0.060315, recall 0.869753
2017-12-10T12:16:20.576161: step 2733, loss 0.53714, acc 0.96875, prec 0.0603629, recall 0.869855
2017-12-10T12:16:21.020964: step 2734, loss 0.141278, acc 0.9375, prec 0.0603819, recall 0.869906
2017-12-10T12:16:21.474773: step 2735, loss 0.312032, acc 0.890625, prec 0.0604215, recall 0.870008
2017-12-10T12:16:21.915254: step 2736, loss 0.627873, acc 0.890625, prec 0.0604355, recall 0.870059
2017-12-10T12:16:22.363743: step 2737, loss 0.211431, acc 0.9375, prec 0.060429, recall 0.870059
2017-12-10T12:16:22.791137: step 2738, loss 1.6587, acc 0.890625, prec 0.0604446, recall 0.869769
2017-12-10T12:16:23.245448: step 2739, loss 0.181162, acc 0.921875, prec 0.0604364, recall 0.869769
2017-12-10T12:16:23.688288: step 2740, loss 0.430835, acc 0.90625, prec 0.0604266, recall 0.869769
2017-12-10T12:16:24.143364: step 2741, loss 0.121613, acc 0.953125, prec 0.0604216, recall 0.869769
2017-12-10T12:16:24.587310: step 2742, loss 0.564352, acc 0.875, prec 0.060434, recall 0.86982
2017-12-10T12:16:25.040866: step 2743, loss 0.103859, acc 0.96875, prec 0.0604818, recall 0.869922
2017-12-10T12:16:25.489412: step 2744, loss 0.176625, acc 0.90625, prec 0.0604719, recall 0.869922
2017-12-10T12:16:25.917430: step 2745, loss 0.442134, acc 0.875, prec 0.0604588, recall 0.869922
2017-12-10T12:16:26.355531: step 2746, loss 0.864738, acc 0.84375, prec 0.0605444, recall 0.870125
2017-12-10T12:16:26.801741: step 2747, loss 0.196471, acc 0.9375, prec 0.0605378, recall 0.870125
2017-12-10T12:16:27.238454: step 2748, loss 0.113095, acc 0.9375, prec 0.0605312, recall 0.870125
2017-12-10T12:16:27.673109: step 2749, loss 0.26009, acc 0.921875, prec 0.060523, recall 0.870125
2017-12-10T12:16:28.127325: step 2750, loss 0.246786, acc 0.921875, prec 0.0605403, recall 0.870175
2017-12-10T12:16:28.574479: step 2751, loss 0.636737, acc 0.9375, prec 0.0606102, recall 0.870327
2017-12-10T12:16:29.014006: step 2752, loss 0.562347, acc 0.953125, prec 0.0606817, recall 0.870478
2017-12-10T12:16:29.463941: step 2753, loss 0.396742, acc 0.890625, prec 0.0606701, recall 0.870478
2017-12-10T12:16:29.911897: step 2754, loss 0.457276, acc 0.84375, prec 0.0606792, recall 0.870529
2017-12-10T12:16:30.350116: step 2755, loss 0.154097, acc 0.953125, prec 0.0606997, recall 0.870579
2017-12-10T12:16:30.789731: step 2756, loss 0.195773, acc 0.921875, prec 0.0607169, recall 0.870629
2017-12-10T12:16:31.236285: step 2757, loss 0.246277, acc 0.953125, prec 0.0607374, recall 0.87068
2017-12-10T12:16:31.673541: step 2758, loss 0.316062, acc 0.921875, prec 0.0607546, recall 0.87073
2017-12-10T12:16:32.125209: step 2759, loss 0.193028, acc 0.9375, prec 0.0607735, recall 0.87078
2017-12-10T12:16:32.558798: step 2760, loss 0.477768, acc 0.953125, prec 0.060794, recall 0.87083
2017-12-10T12:16:32.999155: step 2761, loss 1.03288, acc 0.96875, prec 0.060867, recall 0.87098
2017-12-10T12:16:33.446512: step 2762, loss 0.356471, acc 0.859375, prec 0.0608776, recall 0.87103
2017-12-10T12:16:33.890135: step 2763, loss 0.160154, acc 0.9375, prec 0.060871, recall 0.87103
2017-12-10T12:16:34.329179: step 2764, loss 2.6059, acc 0.859375, prec 0.0608578, recall 0.870693
2017-12-10T12:16:34.779164: step 2765, loss 0.428961, acc 0.890625, prec 0.0608463, recall 0.870693
2017-12-10T12:16:35.236821: step 2766, loss 0.334529, acc 0.890625, prec 0.0608348, recall 0.870693
2017-12-10T12:16:35.668611: step 2767, loss 0.654048, acc 0.84375, prec 0.0608691, recall 0.870793
2017-12-10T12:16:36.113313: step 2768, loss 0.275238, acc 0.921875, prec 0.0608609, recall 0.870793
2017-12-10T12:16:36.581284: step 2769, loss 0.451945, acc 0.890625, prec 0.0608493, recall 0.870793
2017-12-10T12:16:37.032654: step 2770, loss 0.446952, acc 0.828125, prec 0.0608313, recall 0.870793
2017-12-10T12:16:37.479873: step 2771, loss 0.24348, acc 0.875, prec 0.0608689, recall 0.870893
2017-12-10T12:16:37.923357: step 2772, loss 0.551682, acc 0.859375, prec 0.0608541, recall 0.870893
2017-12-10T12:16:38.374404: step 2773, loss 0.471653, acc 0.890625, prec 0.0608933, recall 0.870993
2017-12-10T12:16:38.813789: step 2774, loss 0.40928, acc 0.8125, prec 0.0608989, recall 0.871042
2017-12-10T12:16:39.249592: step 2775, loss 0.189119, acc 0.90625, prec 0.060889, recall 0.871042
2017-12-10T12:16:39.698860: step 2776, loss 0.344024, acc 0.921875, prec 0.0608808, recall 0.871042
2017-12-10T12:16:40.143928: step 2777, loss 0.403623, acc 0.875, prec 0.0609437, recall 0.871192
2017-12-10T12:16:40.582495: step 2778, loss 0.31562, acc 0.890625, prec 0.0609575, recall 0.871241
2017-12-10T12:16:41.032832: step 2779, loss 0.880728, acc 0.859375, prec 0.061044, recall 0.87144
2017-12-10T12:16:41.467685: step 2780, loss 0.188415, acc 0.9375, prec 0.0610627, recall 0.871489
2017-12-10T12:16:41.906320: step 2781, loss 0.139788, acc 0.953125, prec 0.0610831, recall 0.871538
2017-12-10T12:16:42.335933: step 2782, loss 0.308875, acc 0.90625, prec 0.0610732, recall 0.871538
2017-12-10T12:16:42.774097: step 2783, loss 0.439322, acc 0.890625, prec 0.061087, recall 0.871588
2017-12-10T12:16:43.222773: step 2784, loss 0.267505, acc 0.90625, prec 0.0610771, recall 0.871588
2017-12-10T12:16:43.691499: step 2785, loss 0.262364, acc 0.90625, prec 0.0610673, recall 0.871588
2017-12-10T12:16:44.147325: step 2786, loss 0.188015, acc 0.921875, prec 0.061059, recall 0.871588
2017-12-10T12:16:44.594715: step 2787, loss 0.181505, acc 0.9375, prec 0.0610525, recall 0.871588
2017-12-10T12:16:45.045368: step 2788, loss 0.146901, acc 0.921875, prec 0.0610695, recall 0.871637
2017-12-10T12:16:45.499135: step 2789, loss 2.31443, acc 0.921875, prec 0.0610882, recall 0.871352
2017-12-10T12:16:45.939831: step 2790, loss 0.772949, acc 0.96875, prec 0.0611102, recall 0.871401
2017-12-10T12:16:46.372690: step 2791, loss 0.404597, acc 0.90625, prec 0.0611003, recall 0.871401
2017-12-10T12:16:46.811198: step 2792, loss 0.168576, acc 0.953125, prec 0.0611965, recall 0.871598
2017-12-10T12:16:47.244574: step 2793, loss 0.574828, acc 0.875, prec 0.0611833, recall 0.871598
2017-12-10T12:16:47.683928: step 2794, loss 0.342149, acc 0.921875, prec 0.0612003, recall 0.871648
2017-12-10T12:16:48.121447: step 2795, loss 0.214879, acc 0.9375, prec 0.0611937, recall 0.871648
2017-12-10T12:16:48.560398: step 2796, loss 0.376721, acc 0.921875, prec 0.061236, recall 0.871746
2017-12-10T12:16:49.008069: step 2797, loss 0.309552, acc 0.9375, prec 0.0612799, recall 0.871844
2017-12-10T12:16:49.453788: step 2798, loss 0.239651, acc 0.90625, prec 0.0612953, recall 0.871893
2017-12-10T12:16:49.885833: step 2799, loss 0.500949, acc 0.9375, prec 0.0613644, recall 0.87204
2017-12-10T12:16:50.329666: step 2800, loss 0.21176, acc 0.875, prec 0.0613512, recall 0.87204
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2800

2017-12-10T12:16:52.247399: step 2801, loss 0.210605, acc 0.9375, prec 0.061395, recall 0.872137
2017-12-10T12:16:52.698428: step 2802, loss 0.177749, acc 0.921875, prec 0.061412, recall 0.872186
2017-12-10T12:16:53.137654: step 2803, loss 0.404275, acc 0.90625, prec 0.0614273, recall 0.872235
2017-12-10T12:16:53.589995: step 2804, loss 0.324816, acc 0.921875, prec 0.0614695, recall 0.872332
2017-12-10T12:16:54.042928: step 2805, loss 0.107028, acc 0.9375, prec 0.0614629, recall 0.872332
2017-12-10T12:16:54.484269: step 2806, loss 0.177944, acc 0.953125, prec 0.0614831, recall 0.872381
2017-12-10T12:16:54.919442: step 2807, loss 0.629359, acc 0.890625, prec 0.0614716, recall 0.872381
2017-12-10T12:16:55.361111: step 2808, loss 0.531351, acc 0.859375, prec 0.0615071, recall 0.872478
2017-12-10T12:16:55.797045: step 2809, loss 1.17814, acc 0.890625, prec 0.0615962, recall 0.872672
2017-12-10T12:16:56.239896: step 2810, loss 0.207419, acc 0.90625, prec 0.0615863, recall 0.872672
2017-12-10T12:16:56.675082: step 2811, loss 0.241075, acc 0.90625, prec 0.0616268, recall 0.872769
2017-12-10T12:16:57.120232: step 2812, loss 0.316535, acc 0.953125, prec 0.0616721, recall 0.872865
2017-12-10T12:16:57.569299: step 2813, loss 0.110176, acc 0.96875, prec 0.0617191, recall 0.872962
2017-12-10T12:16:58.015233: step 2814, loss 0.179359, acc 0.921875, prec 0.061736, recall 0.87301
2017-12-10T12:16:58.450579: step 2815, loss 0.132229, acc 0.953125, prec 0.0617813, recall 0.873106
2017-12-10T12:16:58.906450: step 2816, loss 0.0169892, acc 1, prec 0.0617813, recall 0.873106
2017-12-10T12:16:59.350655: step 2817, loss 0.0392669, acc 0.984375, prec 0.06183, recall 0.873202
2017-12-10T12:16:59.800083: step 2818, loss 0.102334, acc 0.96875, prec 0.0618267, recall 0.873202
2017-12-10T12:17:00.250655: step 2819, loss 1.28793, acc 0.953125, prec 0.0618971, recall 0.873346
2017-12-10T12:17:00.702859: step 2820, loss 0.0756512, acc 0.953125, prec 0.0618921, recall 0.873346
2017-12-10T12:17:01.154149: step 2821, loss 0.121489, acc 0.953125, prec 0.0618872, recall 0.873346
2017-12-10T12:17:01.585604: step 2822, loss 0.145087, acc 0.953125, prec 0.0619576, recall 0.873489
2017-12-10T12:17:02.024706: step 2823, loss 0.351093, acc 0.890625, prec 0.061946, recall 0.873489
2017-12-10T12:17:02.478548: step 2824, loss 0.102038, acc 0.96875, prec 0.0619426, recall 0.873489
2017-12-10T12:17:02.921993: step 2825, loss 0.188112, acc 0.953125, prec 0.0619879, recall 0.873585
2017-12-10T12:17:03.369931: step 2826, loss 0.221951, acc 0.9375, prec 0.0620064, recall 0.873633
2017-12-10T12:17:03.805955: step 2827, loss 0.0760285, acc 0.96875, prec 0.0620282, recall 0.87368
2017-12-10T12:17:04.240382: step 2828, loss 0.244491, acc 0.921875, prec 0.0620701, recall 0.873775
2017-12-10T12:17:04.680917: step 2829, loss 0.316578, acc 0.859375, prec 0.0620802, recall 0.873823
2017-12-10T12:17:05.134951: step 2830, loss 0.213149, acc 0.953125, prec 0.0621003, recall 0.87387
2017-12-10T12:17:05.567655: step 2831, loss 0.0817527, acc 0.984375, prec 0.0621238, recall 0.873918
2017-12-10T12:17:06.018129: step 2832, loss 0.272299, acc 0.9375, prec 0.0621422, recall 0.873965
2017-12-10T12:17:06.471928: step 2833, loss 0.168129, acc 0.984375, prec 0.0621405, recall 0.873965
2017-12-10T12:17:06.920640: step 2834, loss 0.329134, acc 0.96875, prec 0.0621623, recall 0.874013
2017-12-10T12:17:07.359588: step 2835, loss 2.14377, acc 0.9375, prec 0.0622075, recall 0.873779
2017-12-10T12:17:07.804362: step 2836, loss 0.0500315, acc 0.984375, prec 0.0622309, recall 0.873827
2017-12-10T12:17:08.261783: step 2837, loss 0.116793, acc 0.96875, prec 0.0622276, recall 0.873827
2017-12-10T12:17:08.705869: step 2838, loss 0.124624, acc 0.96875, prec 0.0622242, recall 0.873827
2017-12-10T12:17:09.152594: step 2839, loss 2.53753, acc 0.921875, prec 0.0622427, recall 0.873546
2017-12-10T12:17:09.607579: step 2840, loss 0.29166, acc 0.921875, prec 0.0622845, recall 0.873641
2017-12-10T12:17:10.049880: step 2841, loss 0.302363, acc 0.921875, prec 0.0623263, recall 0.873735
2017-12-10T12:17:10.487951: step 2842, loss 0.440309, acc 0.859375, prec 0.0623113, recall 0.873735
2017-12-10T12:17:10.915649: step 2843, loss 0.203651, acc 0.9375, prec 0.0623297, recall 0.873783
2017-12-10T12:17:11.357936: step 2844, loss 0.34319, acc 0.875, prec 0.0623414, recall 0.87383
2017-12-10T12:17:11.821633: step 2845, loss 0.482579, acc 0.859375, prec 0.0623264, recall 0.87383
2017-12-10T12:17:12.273356: step 2846, loss 0.296364, acc 0.875, prec 0.0623632, recall 0.873924
2017-12-10T12:17:12.723553: step 2847, loss 0.204688, acc 0.984375, prec 0.0624116, recall 0.874019
2017-12-10T12:17:13.175836: step 2848, loss 0.257983, acc 0.84375, prec 0.0624199, recall 0.874066
2017-12-10T12:17:13.616913: step 2849, loss 0.38557, acc 0.875, prec 0.0624817, recall 0.874207
2017-12-10T12:17:14.055984: step 2850, loss 0.755601, acc 0.828125, prec 0.0624633, recall 0.874207
2017-12-10T12:17:14.499259: step 2851, loss 0.42453, acc 0.875, prec 0.062475, recall 0.874254
2017-12-10T12:17:14.944254: step 2852, loss 0.441789, acc 0.84375, prec 0.0625083, recall 0.874348
2017-12-10T12:17:15.382470: step 2853, loss 0.211412, acc 0.921875, prec 0.0625, recall 0.874348
2017-12-10T12:17:15.823194: step 2854, loss 0.529332, acc 0.90625, prec 0.06249, recall 0.874348
2017-12-10T12:17:16.270211: step 2855, loss 0.211598, acc 0.890625, prec 0.0624784, recall 0.874348
2017-12-10T12:17:16.717836: step 2856, loss 0.430938, acc 0.921875, prec 0.062495, recall 0.874394
2017-12-10T12:17:17.163757: step 2857, loss 0.260585, acc 0.921875, prec 0.0624867, recall 0.874394
2017-12-10T12:17:17.630983: step 2858, loss 0.184908, acc 0.921875, prec 0.0624784, recall 0.874394
2017-12-10T12:17:18.065312: step 2859, loss 0.14962, acc 0.921875, prec 0.06247, recall 0.874394
2017-12-10T12:17:18.515646: step 2860, loss 0.201478, acc 0.984375, prec 0.0625433, recall 0.874535
2017-12-10T12:17:18.958686: step 2861, loss 0.142394, acc 0.96875, prec 0.0625649, recall 0.874581
2017-12-10T12:17:19.419666: step 2862, loss 0.0520328, acc 0.984375, prec 0.0625882, recall 0.874628
2017-12-10T12:17:19.862533: step 2863, loss 0.158741, acc 0.953125, prec 0.0625832, recall 0.874628
2017-12-10T12:17:20.314401: step 2864, loss 0.137971, acc 0.953125, prec 0.0625782, recall 0.874628
2017-12-10T12:17:20.760322: step 2865, loss 0.220567, acc 0.9375, prec 0.0625715, recall 0.874628
2017-12-10T12:17:21.204968: step 2866, loss 1.43206, acc 0.9375, prec 0.0625915, recall 0.874349
2017-12-10T12:17:21.647618: step 2867, loss 0.138424, acc 0.9375, prec 0.0626098, recall 0.874396
2017-12-10T12:17:22.100871: step 2868, loss 4.3687, acc 0.9375, prec 0.0626048, recall 0.874071
2017-12-10T12:17:22.552008: step 2869, loss 0.142071, acc 0.9375, prec 0.0625981, recall 0.874071
2017-12-10T12:17:22.996161: step 2870, loss 0.126943, acc 0.953125, prec 0.0625931, recall 0.874071
2017-12-10T12:17:23.439940: step 2871, loss 0.26516, acc 0.953125, prec 0.0625881, recall 0.874071
2017-12-10T12:17:23.894969: step 2872, loss 0.338809, acc 0.90625, prec 0.062628, recall 0.874165
2017-12-10T12:17:24.335840: step 2873, loss 0.294548, acc 0.9375, prec 0.0626213, recall 0.874165
2017-12-10T12:17:24.778400: step 2874, loss 0.16775, acc 0.9375, prec 0.0626147, recall 0.874165
2017-12-10T12:17:25.221448: step 2875, loss 4.29281, acc 0.859375, prec 0.0626263, recall 0.873887
2017-12-10T12:17:25.673799: step 2876, loss 0.264957, acc 0.890625, prec 0.0626146, recall 0.873887
2017-12-10T12:17:26.119727: step 2877, loss 0.270156, acc 0.921875, prec 0.062681, recall 0.874027
2017-12-10T12:17:26.589394: step 2878, loss 0.454358, acc 0.890625, prec 0.0626943, recall 0.874074
2017-12-10T12:17:27.033352: step 2879, loss 0.367887, acc 0.96875, prec 0.0627158, recall 0.874121
2017-12-10T12:17:27.467801: step 2880, loss 0.288184, acc 0.875, prec 0.0627025, recall 0.874121
2017-12-10T12:17:27.912217: step 2881, loss 0.585197, acc 0.84375, prec 0.0627107, recall 0.874167
2017-12-10T12:17:28.357478: step 2882, loss 0.62581, acc 0.8125, prec 0.0626908, recall 0.874167
2017-12-10T12:17:28.808034: step 2883, loss 0.456068, acc 0.859375, prec 0.0627007, recall 0.874214
2017-12-10T12:17:29.254795: step 2884, loss 0.734541, acc 0.8125, prec 0.0627304, recall 0.874307
2017-12-10T12:17:29.712660: step 2885, loss 0.787174, acc 0.796875, prec 0.0627585, recall 0.8744
2017-12-10T12:17:30.141306: step 2886, loss 0.454483, acc 0.84375, prec 0.0627419, recall 0.8744
2017-12-10T12:17:30.579361: step 2887, loss 0.583744, acc 0.84375, prec 0.0627252, recall 0.8744
2017-12-10T12:17:31.014282: step 2888, loss 0.413546, acc 0.890625, prec 0.0627136, recall 0.8744
2017-12-10T12:17:31.457375: step 2889, loss 0.408818, acc 0.8125, prec 0.0626937, recall 0.8744
2017-12-10T12:17:31.898709: step 2890, loss 0.383024, acc 0.84375, prec 0.0627019, recall 0.874446
2017-12-10T12:17:32.337885: step 2891, loss 0.217238, acc 0.90625, prec 0.0627168, recall 0.874492
2017-12-10T12:17:32.777500: step 2892, loss 0.260443, acc 0.921875, prec 0.0627333, recall 0.874539
2017-12-10T12:17:33.225660: step 2893, loss 0.476359, acc 0.8125, prec 0.0627133, recall 0.874539
2017-12-10T12:17:33.667777: step 2894, loss 0.367316, acc 0.921875, prec 0.062705, recall 0.874539
2017-12-10T12:17:34.122586: step 2895, loss 0.186365, acc 0.921875, prec 0.0627463, recall 0.874631
2017-12-10T12:17:34.565508: step 2896, loss 0.201111, acc 0.921875, prec 0.062738, recall 0.874631
2017-12-10T12:17:35.013493: step 2897, loss 0.177864, acc 0.90625, prec 0.0627281, recall 0.874631
2017-12-10T12:17:35.449926: step 2898, loss 0.110517, acc 0.96875, prec 0.0627496, recall 0.874677
2017-12-10T12:17:35.914831: step 2899, loss 0.0276774, acc 1, prec 0.0627743, recall 0.874724
2017-12-10T12:17:36.353283: step 2900, loss 0.0483331, acc 0.96875, prec 0.062771, recall 0.874724
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-2900

2017-12-10T12:17:38.362601: step 2901, loss 0.157826, acc 0.984375, prec 0.0627941, recall 0.87477
2017-12-10T12:17:38.802875: step 2902, loss 0.165773, acc 0.9375, prec 0.0627875, recall 0.87477
2017-12-10T12:17:39.242337: step 2903, loss 0.214186, acc 0.953125, prec 0.0628073, recall 0.874816
2017-12-10T12:17:39.679789: step 2904, loss 0.00557426, acc 1, prec 0.0628073, recall 0.874816
2017-12-10T12:17:40.130111: step 2905, loss 2.60219, acc 0.96875, prec 0.0628304, recall 0.87454
2017-12-10T12:17:40.587968: step 2906, loss 0.433451, acc 0.9375, prec 0.0628733, recall 0.874632
2017-12-10T12:17:41.046732: step 2907, loss 0.00875918, acc 1, prec 0.0628981, recall 0.874678
2017-12-10T12:17:41.486241: step 2908, loss 0.559839, acc 0.9375, prec 0.0629657, recall 0.874816
2017-12-10T12:17:41.934423: step 2909, loss 0.0255992, acc 1, prec 0.0629905, recall 0.874862
2017-12-10T12:17:42.371228: step 2910, loss 0.379086, acc 0.984375, prec 0.0630383, recall 0.874954
2017-12-10T12:17:42.821530: step 2911, loss 0.643529, acc 1, prec 0.0630631, recall 0.875
2017-12-10T12:17:43.260182: step 2912, loss 0.242226, acc 0.9375, prec 0.0630812, recall 0.875046
2017-12-10T12:17:43.693703: step 2913, loss 0.361818, acc 0.90625, prec 0.0631206, recall 0.875137
2017-12-10T12:17:44.081994: step 2914, loss 0.221157, acc 0.9375, prec 0.0631635, recall 0.875229
2017-12-10T12:17:44.467627: step 2915, loss 0.169507, acc 0.921875, prec 0.0631799, recall 0.875274
2017-12-10T12:17:44.863358: step 2916, loss 0.114811, acc 0.96875, prec 0.063226, recall 0.875365
2017-12-10T12:17:45.254896: step 2917, loss 0.324829, acc 0.875, prec 0.0632126, recall 0.875365
2017-12-10T12:17:45.630469: step 2918, loss 0.470852, acc 0.953125, prec 0.0632571, recall 0.875457
2017-12-10T12:17:46.085394: step 2919, loss 0.447627, acc 0.828125, prec 0.0632387, recall 0.875457
2017-12-10T12:17:46.538087: step 2920, loss 0.653227, acc 0.859375, prec 0.0632731, recall 0.875547
2017-12-10T12:17:46.992393: step 2921, loss 0.308674, acc 0.90625, prec 0.0633372, recall 0.875684
2017-12-10T12:17:47.440809: step 2922, loss 0.397125, acc 0.921875, prec 0.0633289, recall 0.875684
2017-12-10T12:17:47.871155: step 2923, loss 0.160069, acc 0.953125, prec 0.0633238, recall 0.875684
2017-12-10T12:17:48.319168: step 2924, loss 0.379939, acc 0.84375, prec 0.0633318, recall 0.875729
2017-12-10T12:17:48.783729: step 2925, loss 0.354279, acc 0.890625, prec 0.0633202, recall 0.875729
2017-12-10T12:17:49.237339: step 2926, loss 0.295547, acc 0.9375, prec 0.0633135, recall 0.875729
2017-12-10T12:17:49.678758: step 2927, loss 1.31481, acc 0.9375, prec 0.0634055, recall 0.87591
2017-12-10T12:17:50.121284: step 2928, loss 0.187319, acc 0.859375, prec 0.0633905, recall 0.87591
2017-12-10T12:17:50.560564: step 2929, loss 0.22678, acc 0.90625, prec 0.0634051, recall 0.875955
2017-12-10T12:17:51.002088: step 2930, loss 0.226269, acc 0.90625, prec 0.0634198, recall 0.876
2017-12-10T12:17:51.435097: step 2931, loss 0.229806, acc 0.890625, prec 0.0634327, recall 0.876045
2017-12-10T12:17:51.878716: step 2932, loss 0.158639, acc 0.984375, prec 0.063505, recall 0.87618
2017-12-10T12:17:52.328769: step 2933, loss 0.315122, acc 0.921875, prec 0.0635459, recall 0.87627
2017-12-10T12:17:52.782457: step 2934, loss 0.483605, acc 0.90625, prec 0.0635359, recall 0.87627
2017-12-10T12:17:53.222800: step 2935, loss 0.142651, acc 0.953125, prec 0.0635555, recall 0.876315
2017-12-10T12:17:53.661389: step 2936, loss 0.521469, acc 0.875, prec 0.0636407, recall 0.876494
2017-12-10T12:17:54.122480: step 2937, loss 0.348387, acc 0.859375, prec 0.0636256, recall 0.876494
2017-12-10T12:17:54.573181: step 2938, loss 0.182622, acc 0.9375, prec 0.0636189, recall 0.876494
2017-12-10T12:17:55.024885: step 2939, loss 0.253916, acc 0.921875, prec 0.0636352, recall 0.876539
2017-12-10T12:17:55.482403: step 2940, loss 0.0607986, acc 0.984375, prec 0.0636581, recall 0.876583
2017-12-10T12:17:55.912647: step 2941, loss 0.154416, acc 0.9375, prec 0.063676, recall 0.876628
2017-12-10T12:17:56.346185: step 2942, loss 0.172217, acc 0.953125, prec 0.0637202, recall 0.876717
2017-12-10T12:17:56.783830: step 2943, loss 0.131261, acc 0.9375, prec 0.0637381, recall 0.876762
2017-12-10T12:17:57.227744: step 2944, loss 3.72817, acc 0.921875, prec 0.0637314, recall 0.876445
2017-12-10T12:17:57.677258: step 2945, loss 0.0714611, acc 0.984375, prec 0.0637543, recall 0.87649
2017-12-10T12:17:58.122845: step 2946, loss 0.270229, acc 0.90625, prec 0.0637443, recall 0.87649
2017-12-10T12:17:58.563994: step 2947, loss 0.61801, acc 0.859375, prec 0.0637292, recall 0.87649
2017-12-10T12:17:59.010215: step 2948, loss 0.193049, acc 0.9375, prec 0.0637471, recall 0.876534
2017-12-10T12:17:59.455241: step 2949, loss 0.415953, acc 0.875, prec 0.0637583, recall 0.876579
2017-12-10T12:17:59.889937: step 2950, loss 0.671148, acc 0.90625, prec 0.063822, recall 0.876712
2017-12-10T12:18:00.318266: step 2951, loss 0.372751, acc 0.890625, prec 0.0638103, recall 0.876712
2017-12-10T12:18:00.767888: step 2952, loss 0.260418, acc 0.9375, prec 0.0638281, recall 0.876757
2017-12-10T12:18:01.214856: step 2953, loss 0.292117, acc 0.90625, prec 0.0638672, recall 0.876846
2017-12-10T12:18:01.658039: step 2954, loss 0.580871, acc 0.890625, prec 0.0639045, recall 0.876934
2017-12-10T12:18:02.098142: step 2955, loss 0.422599, acc 0.90625, prec 0.063919, recall 0.876978
2017-12-10T12:18:02.551812: step 2956, loss 0.469981, acc 0.875, prec 0.0639056, recall 0.876978
2017-12-10T12:18:02.979478: step 2957, loss 0.0956308, acc 0.984375, prec 0.063953, recall 0.877067
2017-12-10T12:18:03.429154: step 2958, loss 0.171805, acc 0.921875, prec 0.0639447, recall 0.877067
2017-12-10T12:18:03.874950: step 2959, loss 0.339815, acc 0.921875, prec 0.0639608, recall 0.877111
2017-12-10T12:18:04.326564: step 2960, loss 0.484589, acc 0.875, prec 0.0639474, recall 0.877111
2017-12-10T12:18:04.765491: step 2961, loss 0.675612, acc 0.921875, prec 0.0639635, recall 0.877155
2017-12-10T12:18:05.209813: step 2962, loss 0.515636, acc 0.84375, prec 0.0639468, recall 0.877155
2017-12-10T12:18:05.652963: step 2963, loss 0.308447, acc 0.90625, prec 0.0639367, recall 0.877155
2017-12-10T12:18:06.107449: step 2964, loss 0.252673, acc 0.890625, prec 0.063925, recall 0.877155
2017-12-10T12:18:06.549087: step 2965, loss 0.270463, acc 0.9375, prec 0.0639183, recall 0.877155
2017-12-10T12:18:06.985028: step 2966, loss 0.266079, acc 0.921875, prec 0.063959, recall 0.877243
2017-12-10T12:18:07.432664: step 2967, loss 0.265583, acc 0.9375, prec 0.0640013, recall 0.877331
2017-12-10T12:18:07.875163: step 2968, loss 0.234135, acc 0.921875, prec 0.0640174, recall 0.877375
2017-12-10T12:18:08.323799: step 2969, loss 0.346323, acc 0.90625, prec 0.0640073, recall 0.877375
2017-12-10T12:18:08.773223: step 2970, loss 0.131643, acc 0.9375, prec 0.0640251, recall 0.877419
2017-12-10T12:18:09.213199: step 2971, loss 0.207664, acc 0.921875, prec 0.0640167, recall 0.877419
2017-12-10T12:18:09.649503: step 2972, loss 1.49952, acc 0.984375, prec 0.0640167, recall 0.877105
2017-12-10T12:18:10.103276: step 2973, loss 0.107669, acc 0.953125, prec 0.0640117, recall 0.877105
2017-12-10T12:18:10.557933: step 2974, loss 0.966586, acc 0.921875, prec 0.0640768, recall 0.877237
2017-12-10T12:18:10.998508: step 2975, loss 0.345082, acc 0.96875, prec 0.0640979, recall 0.877281
2017-12-10T12:18:11.439061: step 2976, loss 0.0276309, acc 1, prec 0.0641468, recall 0.877369
2017-12-10T12:18:11.872822: step 2977, loss 0.199963, acc 0.953125, prec 0.0641418, recall 0.877369
2017-12-10T12:18:12.320494: step 2978, loss 0.517741, acc 0.828125, prec 0.0641478, recall 0.877412
2017-12-10T12:18:12.755207: step 2979, loss 0.137109, acc 0.9375, prec 0.0641411, recall 0.877412
2017-12-10T12:18:13.206096: step 2980, loss 0.305503, acc 0.921875, prec 0.064206, recall 0.877544
2017-12-10T12:18:13.657922: step 2981, loss 0.447023, acc 0.90625, prec 0.0642204, recall 0.877587
2017-12-10T12:18:14.058909: step 2982, loss 0.35596, acc 0.882353, prec 0.0642104, recall 0.877587
2017-12-10T12:18:14.521585: step 2983, loss 0.133486, acc 0.953125, prec 0.0642542, recall 0.877675
2017-12-10T12:18:14.986960: step 2984, loss 0.273276, acc 0.921875, prec 0.0642458, recall 0.877675
2017-12-10T12:18:15.367109: step 2985, loss 0.194818, acc 0.921875, prec 0.0642374, recall 0.877675
2017-12-10T12:18:15.747443: step 2986, loss 0.229938, acc 0.921875, prec 0.0642779, recall 0.877762
2017-12-10T12:18:16.130581: step 2987, loss 0.191595, acc 0.890625, prec 0.0642661, recall 0.877762
2017-12-10T12:18:16.570581: step 2988, loss 0.343297, acc 0.921875, prec 0.064331, recall 0.877892
2017-12-10T12:18:17.017580: step 2989, loss 0.115923, acc 0.96875, prec 0.0643276, recall 0.877892
2017-12-10T12:18:17.470107: step 2990, loss 0.318029, acc 0.9375, prec 0.0643453, recall 0.877936
2017-12-10T12:18:17.920420: step 2991, loss 0.243914, acc 0.9375, prec 0.0643874, recall 0.878023
2017-12-10T12:18:18.360204: step 2992, loss 0.119594, acc 0.96875, prec 0.0644085, recall 0.878066
2017-12-10T12:18:18.802446: step 2993, loss 0.0728154, acc 0.96875, prec 0.0644051, recall 0.878066
2017-12-10T12:18:19.232997: step 2994, loss 0.290734, acc 0.921875, prec 0.0644211, recall 0.878109
2017-12-10T12:18:19.678844: step 2995, loss 0.153011, acc 0.96875, prec 0.0644177, recall 0.878109
2017-12-10T12:18:20.107996: step 2996, loss 0.0261923, acc 0.984375, prec 0.0644161, recall 0.878109
2017-12-10T12:18:20.544688: step 2997, loss 0.161112, acc 0.921875, prec 0.0644564, recall 0.878196
2017-12-10T12:18:20.994434: step 2998, loss 0.0974884, acc 0.953125, prec 0.0644514, recall 0.878196
2017-12-10T12:18:21.425136: step 2999, loss 1.16231, acc 0.953125, prec 0.0645439, recall 0.878369
2017-12-10T12:18:21.868029: step 3000, loss 0.211065, acc 0.9375, prec 0.0645371, recall 0.878369
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3000

2017-12-10T12:18:23.712449: step 3001, loss 0.0734792, acc 0.953125, prec 0.0645321, recall 0.878369
2017-12-10T12:18:24.158633: step 3002, loss 0.200385, acc 0.9375, prec 0.0645497, recall 0.878412
2017-12-10T12:18:24.590365: step 3003, loss 0.433941, acc 0.9375, prec 0.0646161, recall 0.878541
2017-12-10T12:18:25.031665: step 3004, loss 0.283819, acc 0.9375, prec 0.0646337, recall 0.878584
2017-12-10T12:18:25.474890: step 3005, loss 0.186752, acc 0.953125, prec 0.0646287, recall 0.878584
2017-12-10T12:18:25.923032: step 3006, loss 0.220819, acc 0.984375, prec 0.0646757, recall 0.87867
2017-12-10T12:18:26.363010: step 3007, loss 0.292481, acc 0.921875, prec 0.0646916, recall 0.878713
2017-12-10T12:18:26.811655: step 3008, loss 0.065045, acc 0.984375, prec 0.0647143, recall 0.878756
2017-12-10T12:18:27.255601: step 3009, loss 0.086948, acc 0.96875, prec 0.0647109, recall 0.878756
2017-12-10T12:18:27.708846: step 3010, loss 0.180579, acc 0.921875, prec 0.0647512, recall 0.878841
2017-12-10T12:18:28.155846: step 3011, loss 0.199153, acc 0.953125, prec 0.0647461, recall 0.878841
2017-12-10T12:18:28.595252: step 3012, loss 0.124676, acc 0.953125, prec 0.0647654, recall 0.878884
2017-12-10T12:18:29.050232: step 3013, loss 0.0614568, acc 0.984375, prec 0.0647637, recall 0.878884
2017-12-10T12:18:29.484120: step 3014, loss 0.155338, acc 0.953125, prec 0.0647587, recall 0.878884
2017-12-10T12:18:29.933560: step 3015, loss 0.249193, acc 0.953125, prec 0.064778, recall 0.878927
2017-12-10T12:18:30.398039: step 3016, loss 0.252893, acc 0.921875, prec 0.0647695, recall 0.878927
2017-12-10T12:18:30.860108: step 3017, loss 0.0954321, acc 0.96875, prec 0.0647662, recall 0.878927
2017-12-10T12:18:31.315688: step 3018, loss 0.100365, acc 0.96875, prec 0.0647871, recall 0.87897
2017-12-10T12:18:31.755045: step 3019, loss 0.0720706, acc 0.96875, prec 0.0648324, recall 0.879055
2017-12-10T12:18:32.200354: step 3020, loss 1.10086, acc 0.96875, prec 0.0648777, recall 0.87914
2017-12-10T12:18:32.648112: step 3021, loss 0.13646, acc 0.9375, prec 0.0648709, recall 0.87914
2017-12-10T12:18:33.088495: step 3022, loss 0.127186, acc 0.953125, prec 0.0648658, recall 0.87914
2017-12-10T12:18:33.536314: step 3023, loss 0.177147, acc 0.984375, prec 0.0648642, recall 0.87914
2017-12-10T12:18:33.986361: step 3024, loss 0.047966, acc 0.984375, prec 0.0648625, recall 0.87914
2017-12-10T12:18:34.428769: step 3025, loss 0.114607, acc 0.96875, prec 0.0648591, recall 0.87914
2017-12-10T12:18:34.873150: step 3026, loss 0.183896, acc 0.921875, prec 0.0648507, recall 0.87914
2017-12-10T12:18:35.313207: step 3027, loss 0.174952, acc 0.9375, prec 0.0648682, recall 0.879183
2017-12-10T12:18:35.764148: step 3028, loss 0.0979993, acc 0.953125, prec 0.0648632, recall 0.879183
2017-12-10T12:18:36.212498: step 3029, loss 0.143359, acc 0.984375, prec 0.0648858, recall 0.879225
2017-12-10T12:18:36.661324: step 3030, loss 0.27113, acc 0.9375, prec 0.0648791, recall 0.879225
2017-12-10T12:18:37.108993: step 3031, loss 0.0578113, acc 0.96875, prec 0.0649, recall 0.879268
2017-12-10T12:18:37.543651: step 3032, loss 0.245174, acc 0.921875, prec 0.0648915, recall 0.879268
2017-12-10T12:18:37.990298: step 3033, loss 0.201459, acc 0.96875, prec 0.0649125, recall 0.87931
2017-12-10T12:18:38.433290: step 3034, loss 0.0798621, acc 0.984375, prec 0.0649351, recall 0.879353
2017-12-10T12:18:38.868301: step 3035, loss 0.108507, acc 0.953125, prec 0.0649543, recall 0.879395
2017-12-10T12:18:39.317041: step 3036, loss 0.391944, acc 0.921875, prec 0.0649459, recall 0.879395
2017-12-10T12:18:39.771425: step 3037, loss 0.157888, acc 0.921875, prec 0.0649617, recall 0.879438
2017-12-10T12:18:40.211482: step 3038, loss 0.0335401, acc 1, prec 0.0649617, recall 0.879438
2017-12-10T12:18:40.645787: step 3039, loss 0.102139, acc 0.953125, prec 0.0649566, recall 0.879438
2017-12-10T12:18:41.081089: step 3040, loss 0.226553, acc 0.96875, prec 0.0649775, recall 0.87948
2017-12-10T12:18:41.518551: step 3041, loss 0.111245, acc 0.96875, prec 0.0649742, recall 0.87948
2017-12-10T12:18:41.956670: step 3042, loss 0.0264658, acc 1, prec 0.0649742, recall 0.87948
2017-12-10T12:18:42.409808: step 3043, loss 0.148559, acc 0.96875, prec 0.0650193, recall 0.879565
2017-12-10T12:18:42.855476: step 3044, loss 0.0806921, acc 0.96875, prec 0.065016, recall 0.879565
2017-12-10T12:18:43.308313: step 3045, loss 0.505209, acc 1, prec 0.0650888, recall 0.879691
2017-12-10T12:18:43.747363: step 3046, loss 0.468413, acc 0.96875, prec 0.0651582, recall 0.879818
2017-12-10T12:18:44.194910: step 3047, loss 0.0745913, acc 0.96875, prec 0.065179, recall 0.87986
2017-12-10T12:18:44.634982: step 3048, loss 0.0145897, acc 1, prec 0.065179, recall 0.87986
2017-12-10T12:18:45.078192: step 3049, loss 0.0230295, acc 1, prec 0.065179, recall 0.87986
2017-12-10T12:18:45.518049: step 3050, loss 1.05413, acc 1, prec 0.0652033, recall 0.879902
2017-12-10T12:18:45.962305: step 3051, loss 0.0277032, acc 1, prec 0.0652275, recall 0.879944
2017-12-10T12:18:46.415202: step 3052, loss 0.0562976, acc 0.96875, prec 0.0652484, recall 0.879986
2017-12-10T12:18:46.856675: step 3053, loss 0.230716, acc 0.953125, prec 0.0652433, recall 0.879986
2017-12-10T12:18:47.309102: step 3054, loss 0.132924, acc 0.9375, prec 0.0652366, recall 0.879986
2017-12-10T12:18:47.754107: step 3055, loss 0.266961, acc 0.90625, prec 0.0652991, recall 0.880112
2017-12-10T12:18:48.202487: step 3056, loss 0.18475, acc 0.90625, prec 0.0653132, recall 0.880154
2017-12-10T12:18:48.650411: step 3057, loss 0.0920408, acc 0.953125, prec 0.0653081, recall 0.880154
2017-12-10T12:18:49.099225: step 3058, loss 0.103598, acc 0.96875, prec 0.0653047, recall 0.880154
2017-12-10T12:18:49.553182: step 3059, loss 1.11347, acc 0.953125, prec 0.0653239, recall 0.880196
2017-12-10T12:18:50.006440: step 3060, loss 0.0179966, acc 1, prec 0.0653239, recall 0.880196
2017-12-10T12:18:50.446529: step 3061, loss 0.302486, acc 0.9375, prec 0.0653414, recall 0.880237
2017-12-10T12:18:50.885593: step 3062, loss 0.252189, acc 0.9375, prec 0.0653588, recall 0.880279
2017-12-10T12:18:51.339642: step 3063, loss 0.160969, acc 0.9375, prec 0.0654489, recall 0.880446
2017-12-10T12:18:51.792749: step 3064, loss 0.524545, acc 0.9375, prec 0.0654905, recall 0.880529
2017-12-10T12:18:52.225697: step 3065, loss 0.230301, acc 0.90625, prec 0.0655046, recall 0.880571
2017-12-10T12:18:52.669164: step 3066, loss 0.256415, acc 0.875, prec 0.0655152, recall 0.880613
2017-12-10T12:18:53.105501: step 3067, loss 0.160019, acc 0.921875, prec 0.0655309, recall 0.880654
2017-12-10T12:18:53.546589: step 3068, loss 0.4598, acc 0.921875, prec 0.0655466, recall 0.880696
2017-12-10T12:18:53.975247: step 3069, loss 0.192144, acc 0.921875, prec 0.0656107, recall 0.88082
2017-12-10T12:18:54.431978: step 3070, loss 0.526027, acc 0.890625, prec 0.0655988, recall 0.88082
2017-12-10T12:18:54.881969: step 3071, loss 0.479313, acc 0.84375, prec 0.0655818, recall 0.88082
2017-12-10T12:18:55.330140: step 3072, loss 0.262689, acc 0.90625, prec 0.0655958, recall 0.880861
2017-12-10T12:18:55.771475: step 3073, loss 0.19361, acc 0.90625, prec 0.0655856, recall 0.880861
2017-12-10T12:18:56.224365: step 3074, loss 0.480463, acc 0.90625, prec 0.0655996, recall 0.880903
2017-12-10T12:18:56.657067: step 3075, loss 0.0566441, acc 0.984375, prec 0.0656221, recall 0.880944
2017-12-10T12:18:57.100640: step 3076, loss 0.312651, acc 0.9375, prec 0.0656153, recall 0.880944
2017-12-10T12:18:57.542152: step 3077, loss 0.405779, acc 0.90625, prec 0.0656534, recall 0.881027
2017-12-10T12:18:57.984205: step 3078, loss 0.45471, acc 0.921875, prec 0.0657415, recall 0.881192
2017-12-10T12:18:58.432868: step 3079, loss 0.166409, acc 0.953125, prec 0.0658089, recall 0.881315
2017-12-10T12:18:58.880117: step 3080, loss 0.196746, acc 0.921875, prec 0.0658004, recall 0.881315
2017-12-10T12:18:59.331759: step 3081, loss 0.180035, acc 0.921875, prec 0.065816, recall 0.881356
2017-12-10T12:18:59.772635: step 3082, loss 0.109997, acc 0.9375, prec 0.0658092, recall 0.881356
2017-12-10T12:19:00.218983: step 3083, loss 0.749367, acc 0.953125, prec 0.0658282, recall 0.881397
2017-12-10T12:19:00.660439: step 3084, loss 0.0990822, acc 0.96875, prec 0.0658248, recall 0.881397
2017-12-10T12:19:01.101226: step 3085, loss 0.159638, acc 0.96875, prec 0.0658455, recall 0.881438
2017-12-10T12:19:01.537422: step 3086, loss 0.157024, acc 0.984375, prec 0.0658438, recall 0.881438
2017-12-10T12:19:01.989673: step 3087, loss 0.0590237, acc 0.984375, prec 0.0658421, recall 0.881438
2017-12-10T12:19:02.431499: step 3088, loss 0.109258, acc 1, prec 0.0658904, recall 0.88152
2017-12-10T12:19:02.884644: step 3089, loss 1.11665, acc 0.953125, prec 0.0659335, recall 0.881602
2017-12-10T12:19:03.309502: step 3090, loss 0.284847, acc 0.96875, prec 0.0659783, recall 0.881683
2017-12-10T12:19:03.759970: step 3091, loss 0.109382, acc 0.9375, prec 0.0659956, recall 0.881724
2017-12-10T12:19:04.205574: step 3092, loss 0.141839, acc 0.953125, prec 0.0659905, recall 0.881724
2017-12-10T12:19:04.642357: step 3093, loss 1.18518, acc 0.921875, prec 0.0660543, recall 0.881846
2017-12-10T12:19:05.092795: step 3094, loss 0.366831, acc 0.9375, prec 0.0661198, recall 0.881968
2017-12-10T12:19:05.531355: step 3095, loss 0.0949669, acc 0.96875, prec 0.0661404, recall 0.882009
2017-12-10T12:19:05.962192: step 3096, loss 0.185015, acc 0.953125, prec 0.0661594, recall 0.88205
2017-12-10T12:19:06.425246: step 3097, loss 0.4033, acc 0.890625, prec 0.0662197, recall 0.882171
2017-12-10T12:19:06.874400: step 3098, loss 0.379542, acc 0.921875, prec 0.0662112, recall 0.882171
2017-12-10T12:19:07.318939: step 3099, loss 0.283249, acc 0.90625, prec 0.066225, recall 0.882212
2017-12-10T12:19:07.756564: step 3100, loss 0.174401, acc 0.953125, prec 0.0662199, recall 0.882212
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3100

2017-12-10T12:19:09.712914: step 3101, loss 0.591072, acc 0.890625, prec 0.0662561, recall 0.882292
2017-12-10T12:19:10.159107: step 3102, loss 0.225106, acc 0.921875, prec 0.0662475, recall 0.882292
2017-12-10T12:19:10.600350: step 3103, loss 0.244099, acc 0.875, prec 0.0662579, recall 0.882333
2017-12-10T12:19:11.039173: step 3104, loss 0.144183, acc 0.953125, prec 0.066349, recall 0.882494
2017-12-10T12:19:11.472163: step 3105, loss 0.417765, acc 0.859375, prec 0.0663577, recall 0.882534
2017-12-10T12:19:11.907190: step 3106, loss 0.460492, acc 0.875, prec 0.066368, recall 0.882574
2017-12-10T12:19:12.340322: step 3107, loss 0.532632, acc 0.953125, prec 0.066411, recall 0.882655
2017-12-10T12:19:12.797977: step 3108, loss 0.325679, acc 0.9375, prec 0.0664522, recall 0.882735
2017-12-10T12:19:13.240464: step 3109, loss 4.00859, acc 0.953125, prec 0.0664968, recall 0.882514
2017-12-10T12:19:13.691043: step 3110, loss 0.148318, acc 0.921875, prec 0.0665123, recall 0.882554
2017-12-10T12:19:14.147434: step 3111, loss 0.384318, acc 0.859375, prec 0.0665209, recall 0.882594
2017-12-10T12:19:14.592939: step 3112, loss 0.159743, acc 0.921875, prec 0.0665123, recall 0.882594
2017-12-10T12:19:15.040451: step 3113, loss 0.323163, acc 0.890625, prec 0.0665004, recall 0.882594
2017-12-10T12:19:15.485539: step 3114, loss 0.123728, acc 0.9375, prec 0.0664935, recall 0.882594
2017-12-10T12:19:15.929714: step 3115, loss 0.762622, acc 0.78125, prec 0.0664936, recall 0.882634
2017-12-10T12:19:16.369684: step 3116, loss 0.595229, acc 0.84375, prec 0.0665245, recall 0.882714
2017-12-10T12:19:16.821767: step 3117, loss 0.328543, acc 0.90625, prec 0.0665622, recall 0.882794
2017-12-10T12:19:17.262797: step 3118, loss 0.407392, acc 0.875, prec 0.0665725, recall 0.882834
2017-12-10T12:19:17.699196: step 3119, loss 0.703545, acc 0.8125, prec 0.0665759, recall 0.882874
2017-12-10T12:19:18.138132: step 3120, loss 0.326116, acc 0.84375, prec 0.0665589, recall 0.882874
2017-12-10T12:19:18.591537: step 3121, loss 0.341903, acc 0.875, prec 0.0665452, recall 0.882874
2017-12-10T12:19:19.037804: step 3122, loss 0.601008, acc 0.875, prec 0.0665315, recall 0.882874
2017-12-10T12:19:19.479722: step 3123, loss 0.630258, acc 0.875, prec 0.0665179, recall 0.882874
2017-12-10T12:19:19.922109: step 3124, loss 0.327409, acc 0.921875, prec 0.0665094, recall 0.882874
2017-12-10T12:19:20.360387: step 3125, loss 0.544568, acc 0.9375, prec 0.0665504, recall 0.882953
2017-12-10T12:19:20.806952: step 3126, loss 0.652501, acc 0.859375, prec 0.066559, recall 0.882993
2017-12-10T12:19:21.253621: step 3127, loss 0.0659295, acc 0.96875, prec 0.0665795, recall 0.883033
2017-12-10T12:19:21.701717: step 3128, loss 0.2573, acc 0.921875, prec 0.0666427, recall 0.883152
2017-12-10T12:19:22.148469: step 3129, loss 0.430092, acc 0.953125, prec 0.0667333, recall 0.883311
2017-12-10T12:19:22.593906: step 3130, loss 0.327376, acc 0.921875, prec 0.0667965, recall 0.883429
2017-12-10T12:19:23.040612: step 3131, loss 0.210673, acc 0.953125, prec 0.0667913, recall 0.883429
2017-12-10T12:19:23.492946: step 3132, loss 0.126309, acc 0.984375, prec 0.0668375, recall 0.883508
2017-12-10T12:19:23.933648: step 3133, loss 0.0612274, acc 0.953125, prec 0.0668562, recall 0.883548
2017-12-10T12:19:24.375105: step 3134, loss 1.51366, acc 0.9375, prec 0.0668511, recall 0.883249
2017-12-10T12:19:24.823241: step 3135, loss 0.175658, acc 0.9375, prec 0.0668442, recall 0.883249
2017-12-10T12:19:25.266766: step 3136, loss 0.560183, acc 0.9375, prec 0.0668852, recall 0.883328
2017-12-10T12:19:25.717358: step 3137, loss 0.288487, acc 0.890625, prec 0.0668971, recall 0.883367
2017-12-10T12:19:26.167785: step 3138, loss 0.828228, acc 0.890625, prec 0.066909, recall 0.883407
2017-12-10T12:19:26.603656: step 3139, loss 0.186185, acc 0.921875, prec 0.0669243, recall 0.883446
2017-12-10T12:19:27.041771: step 3140, loss 0.100549, acc 0.953125, prec 0.0669192, recall 0.883446
2017-12-10T12:19:27.480713: step 3141, loss 0.225375, acc 0.921875, prec 0.0669106, recall 0.883446
2017-12-10T12:19:27.926710: step 3142, loss 1.32299, acc 0.9375, prec 0.0669055, recall 0.883148
2017-12-10T12:19:28.370941: step 3143, loss 0.323839, acc 0.921875, prec 0.0669446, recall 0.883226
2017-12-10T12:19:28.822983: step 3144, loss 0.0426062, acc 0.984375, prec 0.0669429, recall 0.883226
2017-12-10T12:19:29.264182: step 3145, loss 0.604334, acc 0.921875, prec 0.0669582, recall 0.883266
2017-12-10T12:19:29.725909: step 3146, loss 0.331746, acc 0.890625, prec 0.0669462, recall 0.883266
2017-12-10T12:19:30.174303: step 3147, loss 0.414888, acc 0.875, prec 0.0669564, recall 0.883305
2017-12-10T12:19:30.626300: step 3148, loss 0.385592, acc 0.9375, prec 0.0669973, recall 0.883384
2017-12-10T12:19:31.061468: step 3149, loss 0.338254, acc 0.890625, prec 0.067033, recall 0.883462
2017-12-10T12:19:31.504353: step 3150, loss 0.418806, acc 0.90625, prec 0.0670465, recall 0.883502
2017-12-10T12:19:31.960645: step 3151, loss 0.27195, acc 0.921875, prec 0.067038, recall 0.883502
2017-12-10T12:19:32.403309: step 3152, loss 0.26403, acc 0.921875, prec 0.0670532, recall 0.883541
2017-12-10T12:19:32.844587: step 3153, loss 0.130466, acc 0.953125, prec 0.0670957, recall 0.883619
2017-12-10T12:19:33.285220: step 3154, loss 0.356097, acc 0.859375, prec 0.0670803, recall 0.883619
2017-12-10T12:19:33.735732: step 3155, loss 0.273457, acc 0.890625, prec 0.0670683, recall 0.883619
2017-12-10T12:19:34.181527: step 3156, loss 0.391891, acc 0.890625, prec 0.067104, recall 0.883697
2017-12-10T12:19:34.627648: step 3157, loss 0.292015, acc 0.921875, prec 0.067143, recall 0.883776
2017-12-10T12:19:35.075883: step 3158, loss 0.138153, acc 0.921875, prec 0.0671345, recall 0.883776
2017-12-10T12:19:35.515913: step 3159, loss 0.128475, acc 0.953125, prec 0.0671293, recall 0.883776
2017-12-10T12:19:35.961611: step 3160, loss 0.185047, acc 0.921875, prec 0.0671208, recall 0.883776
2017-12-10T12:19:36.411697: step 3161, loss 0.184712, acc 0.96875, prec 0.0671649, recall 0.883854
2017-12-10T12:19:36.850221: step 3162, loss 0.0754054, acc 0.96875, prec 0.0671853, recall 0.883893
2017-12-10T12:19:37.288857: step 3163, loss 0.225176, acc 0.96875, prec 0.0672057, recall 0.883932
2017-12-10T12:19:37.728536: step 3164, loss 0.123871, acc 0.953125, prec 0.0672005, recall 0.883932
2017-12-10T12:19:38.169493: step 3165, loss 0.0831325, acc 0.96875, prec 0.0672209, recall 0.88397
2017-12-10T12:19:38.609757: step 3166, loss 0.150518, acc 0.90625, prec 0.0672344, recall 0.884009
2017-12-10T12:19:39.052950: step 3167, loss 0.142534, acc 0.96875, prec 0.067231, recall 0.884009
2017-12-10T12:19:39.503301: step 3168, loss 0.226471, acc 0.9375, prec 0.0672479, recall 0.884048
2017-12-10T12:19:39.950284: step 3169, loss 0.197346, acc 0.921875, prec 0.0672869, recall 0.884126
2017-12-10T12:19:40.391304: step 3170, loss 0.0224773, acc 0.984375, prec 0.0672851, recall 0.884126
2017-12-10T12:19:40.830612: step 3171, loss 0.126182, acc 0.984375, prec 0.0672834, recall 0.884126
2017-12-10T12:19:41.272456: step 3172, loss 0.497594, acc 0.96875, prec 0.0673275, recall 0.884203
2017-12-10T12:19:41.722586: step 3173, loss 0.153874, acc 0.9375, prec 0.0673444, recall 0.884242
2017-12-10T12:19:42.166359: step 3174, loss 0.674134, acc 0.984375, prec 0.0673903, recall 0.88432
2017-12-10T12:19:42.607618: step 3175, loss 0.0456007, acc 0.984375, prec 0.0674123, recall 0.884358
2017-12-10T12:19:43.054713: step 3176, loss 0.0672643, acc 0.96875, prec 0.0674326, recall 0.884397
2017-12-10T12:19:43.509507: step 3177, loss 0.0243283, acc 1, prec 0.0674326, recall 0.884397
2017-12-10T12:19:43.955172: step 3178, loss 4.01711, acc 0.96875, prec 0.0674309, recall 0.884102
2017-12-10T12:19:44.406145: step 3179, loss 0.0581445, acc 0.96875, prec 0.0674512, recall 0.88414
2017-12-10T12:19:44.855168: step 3180, loss 0.0344971, acc 0.984375, prec 0.067497, recall 0.884218
2017-12-10T12:19:45.300370: step 3181, loss 0.243019, acc 0.9375, prec 0.0675614, recall 0.884333
2017-12-10T12:19:45.732182: step 3182, loss 0.209463, acc 0.921875, prec 0.0675528, recall 0.884333
2017-12-10T12:19:46.175730: step 3183, loss 0.375741, acc 0.921875, prec 0.0675679, recall 0.884372
2017-12-10T12:19:46.622452: step 3184, loss 0.0659976, acc 1, prec 0.0676154, recall 0.884449
2017-12-10T12:19:47.060097: step 3185, loss 0.297394, acc 0.890625, prec 0.0676508, recall 0.884526
2017-12-10T12:19:47.509175: step 3186, loss 0.0758082, acc 0.953125, prec 0.0676456, recall 0.884526
2017-12-10T12:19:47.968802: step 3187, loss 0.196935, acc 0.953125, prec 0.0676405, recall 0.884526
2017-12-10T12:19:48.403013: step 3188, loss 0.118591, acc 0.9375, prec 0.067681, recall 0.884603
2017-12-10T12:19:48.843296: step 3189, loss 0.369318, acc 0.90625, prec 0.0676707, recall 0.884603
2017-12-10T12:19:49.278919: step 3190, loss 0.288809, acc 0.90625, prec 0.0676841, recall 0.884641
2017-12-10T12:19:49.728868: step 3191, loss 0.255218, acc 0.90625, prec 0.0676975, recall 0.884679
2017-12-10T12:19:50.167958: step 3192, loss 0.231017, acc 0.890625, prec 0.0676854, recall 0.884679
2017-12-10T12:19:50.604165: step 3193, loss 0.147964, acc 0.953125, prec 0.067704, recall 0.884718
2017-12-10T12:19:51.048217: step 3194, loss 0.921039, acc 0.796875, prec 0.0676816, recall 0.884718
2017-12-10T12:19:51.510424: step 3195, loss 0.376803, acc 0.921875, prec 0.0677204, recall 0.884794
2017-12-10T12:19:51.953488: step 3196, loss 0.170518, acc 0.953125, prec 0.0677152, recall 0.884794
2017-12-10T12:19:52.399632: step 3197, loss 0.0946931, acc 0.96875, prec 0.0677355, recall 0.884832
2017-12-10T12:19:52.851143: step 3198, loss 0.254642, acc 0.9375, prec 0.0677286, recall 0.884832
2017-12-10T12:19:53.294320: step 3199, loss 0.136716, acc 0.953125, prec 0.0677945, recall 0.884947
2017-12-10T12:19:53.723949: step 3200, loss 0.158402, acc 0.953125, prec 0.0677893, recall 0.884947
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3200

2017-12-10T12:19:55.538672: step 3201, loss 0.319611, acc 0.921875, prec 0.067828, recall 0.885023
2017-12-10T12:19:55.972576: step 3202, loss 0.192333, acc 0.953125, prec 0.0678229, recall 0.885023
2017-12-10T12:19:56.406526: step 3203, loss 0.204047, acc 0.9375, prec 0.0678633, recall 0.885099
2017-12-10T12:19:56.852134: step 3204, loss 0.23284, acc 0.953125, prec 0.0678581, recall 0.885099
2017-12-10T12:19:57.299566: step 3205, loss 0.304432, acc 0.984375, prec 0.0678801, recall 0.885137
2017-12-10T12:19:57.743799: step 3206, loss 0.213179, acc 1, prec 0.0679274, recall 0.885213
2017-12-10T12:19:58.189926: step 3207, loss 0.306349, acc 0.953125, prec 0.0679222, recall 0.885213
2017-12-10T12:19:58.637221: step 3208, loss 0.18661, acc 0.921875, prec 0.0679609, recall 0.885289
2017-12-10T12:19:59.089080: step 3209, loss 0.0945739, acc 0.984375, prec 0.0679828, recall 0.885327
2017-12-10T12:19:59.524748: step 3210, loss 0.0644566, acc 0.984375, prec 0.0680521, recall 0.885441
2017-12-10T12:19:59.967409: step 3211, loss 0.27018, acc 1, prec 0.0680757, recall 0.885479
2017-12-10T12:20:00.406875: step 3212, loss 0.369456, acc 0.984375, prec 0.0680976, recall 0.885516
2017-12-10T12:20:00.863731: step 3213, loss 1.40916, acc 0.921875, prec 0.068138, recall 0.8853
2017-12-10T12:20:01.308738: step 3214, loss 0.140313, acc 0.984375, prec 0.0681363, recall 0.8853
2017-12-10T12:20:01.752408: step 3215, loss 0.186081, acc 0.96875, prec 0.0681328, recall 0.8853
2017-12-10T12:20:02.206863: step 3216, loss 0.128311, acc 0.96875, prec 0.068153, recall 0.885338
2017-12-10T12:20:02.648880: step 3217, loss 0.199733, acc 0.90625, prec 0.0681426, recall 0.885338
2017-12-10T12:20:03.088120: step 3218, loss 0.190918, acc 0.953125, prec 0.0681611, recall 0.885375
2017-12-10T12:20:03.539014: step 3219, loss 0.312072, acc 0.9375, prec 0.0681542, recall 0.885375
2017-12-10T12:20:04.002325: step 3220, loss 0.311674, acc 0.90625, prec 0.0681674, recall 0.885413
2017-12-10T12:20:04.442591: step 3221, loss 0.139568, acc 0.9375, prec 0.0682077, recall 0.885489
2017-12-10T12:20:04.879420: step 3222, loss 0.109959, acc 0.953125, prec 0.0682026, recall 0.885489
2017-12-10T12:20:05.315367: step 3223, loss 0.0466928, acc 0.984375, prec 0.0682008, recall 0.885489
2017-12-10T12:20:05.763644: step 3224, loss 0.135307, acc 0.90625, prec 0.0681905, recall 0.885489
2017-12-10T12:20:06.201792: step 3225, loss 0.340872, acc 0.921875, prec 0.0681818, recall 0.885489
2017-12-10T12:20:06.653271: step 3226, loss 0.0435404, acc 1, prec 0.068229, recall 0.885564
2017-12-10T12:20:07.117484: step 3227, loss 0.139434, acc 0.953125, prec 0.0682947, recall 0.885677
2017-12-10T12:20:07.569760: step 3228, loss 0.146436, acc 0.921875, prec 0.0683332, recall 0.885752
2017-12-10T12:20:08.013707: step 3229, loss 0.167051, acc 0.953125, prec 0.0683516, recall 0.885789
2017-12-10T12:20:08.439865: step 3230, loss 0.21381, acc 0.9375, prec 0.0683447, recall 0.885789
2017-12-10T12:20:08.876309: step 3231, loss 0.607525, acc 0.953125, prec 0.0683867, recall 0.885864
2017-12-10T12:20:09.319599: step 3232, loss 0.0691593, acc 0.96875, prec 0.0683832, recall 0.885864
2017-12-10T12:20:09.764113: step 3233, loss 1.6837, acc 0.9375, prec 0.068378, recall 0.885574
2017-12-10T12:20:10.206882: step 3234, loss 0.130467, acc 0.96875, prec 0.0683746, recall 0.885574
2017-12-10T12:20:10.641733: step 3235, loss 0.264199, acc 0.953125, prec 0.0683929, recall 0.885611
2017-12-10T12:20:11.087963: step 3236, loss 0.412919, acc 0.921875, prec 0.068455, recall 0.885724
2017-12-10T12:20:11.541062: step 3237, loss 0.230167, acc 0.921875, prec 0.0684464, recall 0.885724
2017-12-10T12:20:11.984630: step 3238, loss 0.26611, acc 0.921875, prec 0.0684613, recall 0.885761
2017-12-10T12:20:12.412911: step 3239, loss 1.07632, acc 0.859375, prec 0.0684692, recall 0.885798
2017-12-10T12:20:12.861376: step 3240, loss 0.261761, acc 0.9375, prec 0.068533, recall 0.88591
2017-12-10T12:20:13.307517: step 3241, loss 0.532806, acc 0.859375, prec 0.0685409, recall 0.885948
2017-12-10T12:20:13.749275: step 3242, loss 0.465426, acc 0.90625, prec 0.0685541, recall 0.885985
2017-12-10T12:20:14.197485: step 3243, loss 0.11883, acc 0.984375, prec 0.0685524, recall 0.885985
2017-12-10T12:20:14.641592: step 3244, loss 0.216974, acc 0.90625, prec 0.068542, recall 0.885985
2017-12-10T12:20:15.087039: step 3245, loss 0.298016, acc 0.9375, prec 0.0685586, recall 0.886022
2017-12-10T12:20:15.529315: step 3246, loss 0.244788, acc 0.921875, prec 0.0685499, recall 0.886022
2017-12-10T12:20:15.994470: step 3247, loss 0.523403, acc 0.90625, prec 0.0685866, recall 0.886097
2017-12-10T12:20:16.444177: step 3248, loss 0.799205, acc 0.890625, prec 0.068598, recall 0.886134
2017-12-10T12:20:16.892879: step 3249, loss 0.38617, acc 0.890625, prec 0.0686094, recall 0.886171
2017-12-10T12:20:17.350290: step 3250, loss 0.297854, acc 0.921875, prec 0.0686007, recall 0.886171
2017-12-10T12:20:17.790646: step 3251, loss 0.270043, acc 0.875, prec 0.0686104, recall 0.886208
2017-12-10T12:20:18.234110: step 3252, loss 0.515798, acc 0.875, prec 0.0685965, recall 0.886208
2017-12-10T12:20:18.665917: step 3253, loss 0.596634, acc 0.84375, prec 0.0686262, recall 0.886282
2017-12-10T12:20:19.111516: step 3254, loss 1.48522, acc 0.875, prec 0.0686359, recall 0.886319
2017-12-10T12:20:19.562403: step 3255, loss 0.233966, acc 0.90625, prec 0.068649, recall 0.886356
2017-12-10T12:20:20.004175: step 3256, loss 0.442631, acc 0.9375, prec 0.0687125, recall 0.886467
2017-12-10T12:20:20.452312: step 3257, loss 0.492107, acc 0.828125, prec 0.0687404, recall 0.886541
2017-12-10T12:20:20.891308: step 3258, loss 0.395682, acc 0.890625, prec 0.0687517, recall 0.886578
2017-12-10T12:20:21.338287: step 3259, loss 0.19369, acc 0.875, prec 0.0687379, recall 0.886578
2017-12-10T12:20:21.776242: step 3260, loss 0.118707, acc 0.953125, prec 0.0687796, recall 0.886652
2017-12-10T12:20:22.221402: step 3261, loss 0.238479, acc 0.890625, prec 0.0687675, recall 0.886652
2017-12-10T12:20:22.675163: step 3262, loss 0.20428, acc 0.953125, prec 0.0687623, recall 0.886652
2017-12-10T12:20:23.124979: step 3263, loss 0.222826, acc 0.90625, prec 0.0687519, recall 0.886652
2017-12-10T12:20:23.576595: step 3264, loss 0.245314, acc 0.890625, prec 0.0687398, recall 0.886652
2017-12-10T12:20:24.020744: step 3265, loss 0.383791, acc 0.875, prec 0.0687259, recall 0.886652
2017-12-10T12:20:24.479580: step 3266, loss 0.330126, acc 0.921875, prec 0.0687173, recall 0.886652
2017-12-10T12:20:24.941548: step 3267, loss 0.328897, acc 0.90625, prec 0.0687069, recall 0.886652
2017-12-10T12:20:25.389558: step 3268, loss 0.129286, acc 0.96875, prec 0.0687269, recall 0.886688
2017-12-10T12:20:25.838633: step 3269, loss 0.0568799, acc 0.984375, prec 0.0687486, recall 0.886725
2017-12-10T12:20:26.286967: step 3270, loss 0.136843, acc 0.953125, prec 0.0687434, recall 0.886725
2017-12-10T12:20:26.733712: step 3271, loss 0.167241, acc 0.96875, prec 0.0687634, recall 0.886762
2017-12-10T12:20:27.184029: step 3272, loss 0.628487, acc 0.921875, prec 0.068825, recall 0.886872
2017-12-10T12:20:27.629664: step 3273, loss 0.0697618, acc 0.984375, prec 0.0688467, recall 0.886909
2017-12-10T12:20:28.055333: step 3274, loss 0.112849, acc 0.953125, prec 0.0688415, recall 0.886909
2017-12-10T12:20:28.501907: step 3275, loss 0.0536094, acc 0.984375, prec 0.0688398, recall 0.886909
2017-12-10T12:20:28.945396: step 3276, loss 0.0628046, acc 0.984375, prec 0.068838, recall 0.886909
2017-12-10T12:20:29.394382: step 3277, loss 0.187371, acc 0.953125, prec 0.0688328, recall 0.886909
2017-12-10T12:20:29.835536: step 3278, loss 0.031353, acc 0.984375, prec 0.0688311, recall 0.886909
2017-12-10T12:20:30.271752: step 3279, loss 0.0554227, acc 0.984375, prec 0.0688294, recall 0.886909
2017-12-10T12:20:30.713103: step 3280, loss 0.110429, acc 0.96875, prec 0.0688259, recall 0.886909
2017-12-10T12:20:31.163676: step 3281, loss 0.387348, acc 1, prec 0.0688727, recall 0.886982
2017-12-10T12:20:31.605509: step 3282, loss 0.348774, acc 0.96875, prec 0.0688927, recall 0.887018
2017-12-10T12:20:32.046595: step 3283, loss 0.482585, acc 0.984375, prec 0.0689144, recall 0.887055
2017-12-10T12:20:32.493032: step 3284, loss 0.0232197, acc 1, prec 0.0689144, recall 0.887055
2017-12-10T12:20:32.942999: step 3285, loss 0.10364, acc 0.984375, prec 0.0689595, recall 0.887128
2017-12-10T12:20:33.397787: step 3286, loss 1.00018, acc 0.984375, prec 0.0690279, recall 0.887237
2017-12-10T12:20:33.872048: step 3287, loss 0.165788, acc 0.984375, prec 0.0690964, recall 0.887347
2017-12-10T12:20:34.311825: step 3288, loss 0.118521, acc 0.984375, prec 0.0691181, recall 0.887383
2017-12-10T12:20:34.766628: step 3289, loss 0.216596, acc 0.9375, prec 0.0691345, recall 0.887419
2017-12-10T12:20:35.213856: step 3290, loss 0.122466, acc 0.984375, prec 0.0691328, recall 0.887419
2017-12-10T12:20:35.651204: step 3291, loss 0.197392, acc 0.984375, prec 0.0691544, recall 0.887456
2017-12-10T12:20:36.089225: step 3292, loss 0.0957005, acc 0.96875, prec 0.0691743, recall 0.887492
2017-12-10T12:20:36.527074: step 3293, loss 0.0828354, acc 0.953125, prec 0.0691691, recall 0.887492
2017-12-10T12:20:36.958419: step 3294, loss 0.438341, acc 0.96875, prec 0.0692358, recall 0.887601
2017-12-10T12:20:37.402270: step 3295, loss 0.204486, acc 0.921875, prec 0.0692505, recall 0.887637
2017-12-10T12:20:37.846605: step 3296, loss 0.347147, acc 0.890625, prec 0.0692617, recall 0.887673
2017-12-10T12:20:38.292672: step 3297, loss 0.26895, acc 0.96875, prec 0.0692816, recall 0.887709
2017-12-10T12:20:38.736384: step 3298, loss 0.233737, acc 0.90625, prec 0.0693412, recall 0.887817
2017-12-10T12:20:39.179446: step 3299, loss 0.246207, acc 0.9375, prec 0.0693343, recall 0.887817
2017-12-10T12:20:39.625534: step 3300, loss 0.5051, acc 0.859375, prec 0.0693186, recall 0.887817
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3300

2017-12-10T12:20:41.479853: step 3301, loss 0.0480898, acc 0.984375, prec 0.0693169, recall 0.887817
2017-12-10T12:20:41.929594: step 3302, loss 0.0742193, acc 0.984375, prec 0.0693618, recall 0.88789
2017-12-10T12:20:42.378505: step 3303, loss 0.18915, acc 0.953125, prec 0.06938, recall 0.887926
2017-12-10T12:20:42.840397: step 3304, loss 0.0314158, acc 1, prec 0.0694267, recall 0.887997
2017-12-10T12:20:43.288081: step 3305, loss 0.360877, acc 0.890625, prec 0.0694145, recall 0.887997
2017-12-10T12:20:43.728179: step 3306, loss 0.2288, acc 0.90625, prec 0.069404, recall 0.887997
2017-12-10T12:20:44.171765: step 3307, loss 0.240912, acc 0.921875, prec 0.0693953, recall 0.887997
2017-12-10T12:20:44.623998: step 3308, loss 0.155379, acc 0.921875, prec 0.0693866, recall 0.887997
2017-12-10T12:20:45.062433: step 3309, loss 0.162106, acc 0.90625, prec 0.0693762, recall 0.887997
2017-12-10T12:20:45.513396: step 3310, loss 0.153517, acc 0.953125, prec 0.069371, recall 0.887997
2017-12-10T12:20:45.956402: step 3311, loss 0.10066, acc 0.9375, prec 0.069364, recall 0.887997
2017-12-10T12:20:46.407899: step 3312, loss 0.136276, acc 0.953125, prec 0.0693588, recall 0.887997
2017-12-10T12:20:46.847666: step 3313, loss 0.0158508, acc 1, prec 0.0693588, recall 0.887997
2017-12-10T12:20:47.297650: step 3314, loss 0.0293583, acc 0.984375, prec 0.0693571, recall 0.887997
2017-12-10T12:20:47.740226: step 3315, loss 0.142198, acc 0.9375, prec 0.0693501, recall 0.887997
2017-12-10T12:20:48.177623: step 3316, loss 0.250578, acc 0.953125, prec 0.0693682, recall 0.888033
2017-12-10T12:20:48.633215: step 3317, loss 0.331518, acc 0.96875, prec 0.0693881, recall 0.888069
2017-12-10T12:20:49.089729: step 3318, loss 0.123644, acc 0.9375, prec 0.0693811, recall 0.888069
2017-12-10T12:20:49.552689: step 3319, loss 0.0635724, acc 0.984375, prec 0.0694027, recall 0.888105
2017-12-10T12:20:49.988232: step 3320, loss 0.364137, acc 0.953125, prec 0.0694208, recall 0.888141
2017-12-10T12:20:50.438655: step 3321, loss 7.60372, acc 0.96875, prec 0.069419, recall 0.887856
2017-12-10T12:20:50.892181: step 3322, loss 0.0908104, acc 0.96875, prec 0.0694156, recall 0.887856
2017-12-10T12:20:51.325447: step 3323, loss 0.0663288, acc 0.96875, prec 0.0694121, recall 0.887856
2017-12-10T12:20:51.762031: step 3324, loss 0.131026, acc 0.953125, prec 0.0694069, recall 0.887856
2017-12-10T12:20:52.192162: step 3325, loss 0.263004, acc 0.90625, prec 0.0693964, recall 0.887856
2017-12-10T12:20:52.629191: step 3326, loss 0.0982353, acc 0.953125, prec 0.0694145, recall 0.887892
2017-12-10T12:20:53.070937: step 3327, loss 0.335724, acc 0.921875, prec 0.0694291, recall 0.887928
2017-12-10T12:20:53.528967: step 3328, loss 0.236113, acc 0.921875, prec 0.069467, recall 0.888
2017-12-10T12:20:53.967727: step 3329, loss 0.208259, acc 0.9375, prec 0.0694601, recall 0.888
2017-12-10T12:20:54.432703: step 3330, loss 0.165033, acc 0.921875, prec 0.0694747, recall 0.888036
2017-12-10T12:20:54.890382: step 3331, loss 0.487212, acc 0.875, prec 0.0694608, recall 0.888036
2017-12-10T12:20:55.342146: step 3332, loss 0.637044, acc 0.9375, prec 0.0695004, recall 0.888107
2017-12-10T12:20:55.791921: step 3333, loss 0.380168, acc 0.875, prec 0.0695563, recall 0.888215
2017-12-10T12:20:56.236762: step 3334, loss 0.108482, acc 0.9375, prec 0.0695493, recall 0.888215
2017-12-10T12:20:56.673752: step 3335, loss 1.07132, acc 0.90625, prec 0.0696087, recall 0.888322
2017-12-10T12:20:57.113477: step 3336, loss 0.202055, acc 0.90625, prec 0.0695983, recall 0.888322
2017-12-10T12:20:57.555653: step 3337, loss 0.297642, acc 0.890625, prec 0.0695861, recall 0.888322
2017-12-10T12:20:58.003629: step 3338, loss 0.329431, acc 0.9375, prec 0.0696256, recall 0.888393
2017-12-10T12:20:58.438052: step 3339, loss 0.447798, acc 0.875, prec 0.0696117, recall 0.888393
2017-12-10T12:20:58.880483: step 3340, loss 0.349242, acc 0.890625, prec 0.0695995, recall 0.888393
2017-12-10T12:20:59.319092: step 3341, loss 0.268972, acc 0.890625, prec 0.0696106, recall 0.888428
2017-12-10T12:20:59.765385: step 3342, loss 0.261617, acc 0.90625, prec 0.0696002, recall 0.888428
2017-12-10T12:21:00.207216: step 3343, loss 0.196865, acc 0.921875, prec 0.0696147, recall 0.888464
2017-12-10T12:21:00.643812: step 3344, loss 0.92758, acc 0.890625, prec 0.069649, recall 0.888535
2017-12-10T12:21:01.082603: step 3345, loss 0.265065, acc 0.90625, prec 0.069685, recall 0.888606
2017-12-10T12:21:01.521262: step 3346, loss 0.146046, acc 0.953125, prec 0.0697262, recall 0.888677
2017-12-10T12:21:01.992706: step 3347, loss 0.309942, acc 0.828125, prec 0.0697071, recall 0.888677
2017-12-10T12:21:02.438097: step 3348, loss 0.368644, acc 0.875, prec 0.0697164, recall 0.888712
2017-12-10T12:21:02.885959: step 3349, loss 0.110717, acc 0.9375, prec 0.0697558, recall 0.888783
2017-12-10T12:21:03.340130: step 3350, loss 0.0814695, acc 0.96875, prec 0.0697524, recall 0.888783
2017-12-10T12:21:03.782686: step 3351, loss 0.413088, acc 0.953125, prec 0.0698399, recall 0.888924
2017-12-10T12:21:04.240840: step 3352, loss 0.175551, acc 0.9375, prec 0.0698562, recall 0.888959
2017-12-10T12:21:04.688304: step 3353, loss 0.0393902, acc 0.984375, prec 0.0698544, recall 0.888959
2017-12-10T12:21:05.123339: step 3354, loss 0.235424, acc 0.953125, prec 0.0698492, recall 0.888959
2017-12-10T12:21:05.568765: step 3355, loss 1.04248, acc 0.984375, prec 0.0698938, recall 0.88903
2017-12-10T12:21:06.022343: step 3356, loss 0.278332, acc 0.9375, prec 0.06991, recall 0.889065
2017-12-10T12:21:06.464354: step 3357, loss 0.18604, acc 0.9375, prec 0.0699031, recall 0.889065
2017-12-10T12:21:06.914073: step 3358, loss 0.142749, acc 0.953125, prec 0.0698978, recall 0.889065
2017-12-10T12:21:07.371244: step 3359, loss 0.0986554, acc 0.921875, prec 0.0699355, recall 0.889135
2017-12-10T12:21:07.824147: step 3360, loss 0.0503025, acc 0.984375, prec 0.0699337, recall 0.889135
2017-12-10T12:21:08.270195: step 3361, loss 0.216338, acc 0.9375, prec 0.0699499, recall 0.88917
2017-12-10T12:21:08.723178: step 3362, loss 0.125644, acc 0.953125, prec 0.0699447, recall 0.88917
2017-12-10T12:21:09.175090: step 3363, loss 0.0239866, acc 1, prec 0.0699679, recall 0.889205
2017-12-10T12:21:09.625679: step 3364, loss 0.237462, acc 0.953125, prec 0.0699858, recall 0.889241
2017-12-10T12:21:10.074799: step 3365, loss 0.069775, acc 0.984375, prec 0.0699841, recall 0.889241
2017-12-10T12:21:10.517747: step 3366, loss 0.114695, acc 0.9375, prec 0.0700002, recall 0.889276
2017-12-10T12:21:10.944807: step 3367, loss 0.165236, acc 0.984375, prec 0.0700217, recall 0.889311
2017-12-10T12:21:11.399107: step 3368, loss 0.0323093, acc 0.984375, prec 0.0700199, recall 0.889311
2017-12-10T12:21:11.846737: step 3369, loss 0.224317, acc 0.953125, prec 0.070061, recall 0.889381
2017-12-10T12:21:12.300478: step 3370, loss 0.0545422, acc 0.96875, prec 0.0700575, recall 0.889381
2017-12-10T12:21:12.746197: step 3371, loss 1.06334, acc 0.96875, prec 0.0701003, recall 0.88945
2017-12-10T12:21:13.184493: step 3372, loss 0.223615, acc 0.96875, prec 0.0700968, recall 0.88945
2017-12-10T12:21:13.619876: step 3373, loss 0.189833, acc 0.9375, prec 0.0700899, recall 0.88945
2017-12-10T12:21:14.062927: step 3374, loss 0.227456, acc 0.9375, prec 0.0700829, recall 0.88945
2017-12-10T12:21:14.504735: step 3375, loss 0.0454995, acc 0.984375, prec 0.0701274, recall 0.88952
2017-12-10T12:21:14.964167: step 3376, loss 0.233803, acc 0.9375, prec 0.0701204, recall 0.88952
2017-12-10T12:21:15.409645: step 3377, loss 0.178159, acc 0.984375, prec 0.0701418, recall 0.889555
2017-12-10T12:21:15.865804: step 3378, loss 0.133731, acc 0.921875, prec 0.0701331, recall 0.889555
2017-12-10T12:21:16.318598: step 3379, loss 0.282247, acc 0.96875, prec 0.0701527, recall 0.88959
2017-12-10T12:21:16.764007: step 3380, loss 0.0387253, acc 0.984375, prec 0.070151, recall 0.88959
2017-12-10T12:21:17.214862: step 3381, loss 0.111445, acc 0.96875, prec 0.0701706, recall 0.889625
2017-12-10T12:21:17.667496: step 3382, loss 0.0567371, acc 0.984375, prec 0.070192, recall 0.88966
2017-12-10T12:21:18.111504: step 3383, loss 0.0280004, acc 0.984375, prec 0.0701903, recall 0.88966
2017-12-10T12:21:18.558043: step 3384, loss 0.112667, acc 0.96875, prec 0.070233, recall 0.889729
2017-12-10T12:21:18.993682: step 3385, loss 0.0404146, acc 0.984375, prec 0.0702313, recall 0.889729
2017-12-10T12:21:19.448794: step 3386, loss 0.259459, acc 0.984375, prec 0.0702758, recall 0.889798
2017-12-10T12:21:19.895190: step 3387, loss 5.764, acc 0.953125, prec 0.0702972, recall 0.889273
2017-12-10T12:21:20.338132: step 3388, loss 0.0890293, acc 0.96875, prec 0.0703168, recall 0.889308
2017-12-10T12:21:20.789097: step 3389, loss 0.907253, acc 0.953125, prec 0.0703578, recall 0.889378
2017-12-10T12:21:21.238789: step 3390, loss 0.24981, acc 0.921875, prec 0.070349, recall 0.889378
2017-12-10T12:21:21.681891: step 3391, loss 0.0819273, acc 0.953125, prec 0.0703669, recall 0.889413
2017-12-10T12:21:22.128093: step 3392, loss 0.437859, acc 0.890625, prec 0.0704239, recall 0.889517
2017-12-10T12:21:22.583699: step 3393, loss 0.304501, acc 0.921875, prec 0.0704383, recall 0.889551
2017-12-10T12:21:23.025407: step 3394, loss 0.453463, acc 0.84375, prec 0.0704439, recall 0.889586
2017-12-10T12:21:23.482140: step 3395, loss 0.442722, acc 0.890625, prec 0.0705009, recall 0.88969
2017-12-10T12:21:23.923203: step 3396, loss 0.665477, acc 0.8125, prec 0.070503, recall 0.889724
2017-12-10T12:21:24.368037: step 3397, loss 0.505346, acc 0.828125, prec 0.0704837, recall 0.889724
2017-12-10T12:21:24.808585: step 3398, loss 0.678131, acc 0.84375, prec 0.0704662, recall 0.889724
2017-12-10T12:21:25.260794: step 3399, loss 0.354343, acc 0.875, prec 0.0704522, recall 0.889724
2017-12-10T12:21:25.713793: step 3400, loss 0.40668, acc 0.890625, prec 0.07044, recall 0.889724
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3400

2017-12-10T12:21:27.734864: step 3401, loss 0.974693, acc 0.8125, prec 0.0704421, recall 0.889759
2017-12-10T12:21:28.179769: step 3402, loss 0.758649, acc 0.796875, prec 0.0704194, recall 0.889759
2017-12-10T12:21:28.617859: step 3403, loss 0.583344, acc 0.859375, prec 0.0704037, recall 0.889759
2017-12-10T12:21:29.066537: step 3404, loss 0.735144, acc 0.875, prec 0.0703897, recall 0.889759
2017-12-10T12:21:29.504135: step 3405, loss 0.320587, acc 0.90625, prec 0.0703793, recall 0.889759
2017-12-10T12:21:29.944328: step 3406, loss 0.281302, acc 0.875, prec 0.0703653, recall 0.889759
2017-12-10T12:21:30.382843: step 3407, loss 0.402646, acc 0.875, prec 0.0703974, recall 0.889828
2017-12-10T12:21:30.817343: step 3408, loss 0.107981, acc 0.984375, prec 0.0704187, recall 0.889862
2017-12-10T12:21:31.267735: step 3409, loss 0.0676677, acc 0.96875, prec 0.0704152, recall 0.889862
2017-12-10T12:21:31.709488: step 3410, loss 0.234639, acc 0.9375, prec 0.0704313, recall 0.889897
2017-12-10T12:21:32.151194: step 3411, loss 0.264119, acc 0.9375, prec 0.0704243, recall 0.889897
2017-12-10T12:21:32.586636: step 3412, loss 0.193111, acc 0.9375, prec 0.0705093, recall 0.890034
2017-12-10T12:21:33.025526: step 3413, loss 0.613629, acc 0.921875, prec 0.0705006, recall 0.890034
2017-12-10T12:21:33.482833: step 3414, loss 0.267262, acc 0.921875, prec 0.0705149, recall 0.890069
2017-12-10T12:21:33.922351: step 3415, loss 0.154134, acc 0.96875, prec 0.0705114, recall 0.890069
2017-12-10T12:21:34.368202: step 3416, loss 0.159478, acc 0.921875, prec 0.0705027, recall 0.890069
2017-12-10T12:21:34.816256: step 3417, loss 0.293243, acc 0.921875, prec 0.0705399, recall 0.890137
2017-12-10T12:21:35.261829: step 3418, loss 1.2131, acc 0.96875, prec 0.0705824, recall 0.890206
2017-12-10T12:21:35.705208: step 3419, loss 0.193937, acc 0.953125, prec 0.0706002, recall 0.89024
2017-12-10T12:21:36.141648: step 3420, loss 0.40942, acc 0.921875, prec 0.0706144, recall 0.890274
2017-12-10T12:21:36.592036: step 3421, loss 0.188035, acc 0.90625, prec 0.0706269, recall 0.890308
2017-12-10T12:21:37.020543: step 3422, loss 0.155486, acc 0.96875, prec 0.0706694, recall 0.890377
2017-12-10T12:21:37.475190: step 3423, loss 0.186892, acc 0.96875, prec 0.0706888, recall 0.890411
2017-12-10T12:21:37.932668: step 3424, loss 0.181074, acc 0.96875, prec 0.0706853, recall 0.890411
2017-12-10T12:21:38.382710: step 3425, loss 0.0624503, acc 0.96875, prec 0.0706819, recall 0.890411
2017-12-10T12:21:38.814646: step 3426, loss 0.121132, acc 0.984375, prec 0.0707031, recall 0.890445
2017-12-10T12:21:39.257708: step 3427, loss 1.03566, acc 0.96875, prec 0.0707225, recall 0.890479
2017-12-10T12:21:39.712386: step 3428, loss 0.192721, acc 0.953125, prec 0.0707173, recall 0.890479
2017-12-10T12:21:40.142316: step 3429, loss 0.223502, acc 0.921875, prec 0.0707086, recall 0.890479
2017-12-10T12:21:40.577005: step 3430, loss 0.167657, acc 0.921875, prec 0.0707228, recall 0.890513
2017-12-10T12:21:41.020630: step 3431, loss 0.17342, acc 0.984375, prec 0.0707669, recall 0.890581
2017-12-10T12:21:41.466629: step 3432, loss 0.349359, acc 0.921875, prec 0.0707812, recall 0.890615
2017-12-10T12:21:41.915758: step 3433, loss 0.327385, acc 0.921875, prec 0.0707954, recall 0.890649
2017-12-10T12:21:42.351184: step 3434, loss 0.482817, acc 0.859375, prec 0.0707796, recall 0.890649
2017-12-10T12:21:42.781959: step 3435, loss 0.100877, acc 0.953125, prec 0.0708203, recall 0.890717
2017-12-10T12:21:43.221470: step 3436, loss 0.591836, acc 0.9375, prec 0.0708591, recall 0.890785
2017-12-10T12:21:43.657137: step 3437, loss 1.08836, acc 0.84375, prec 0.0709104, recall 0.890887
2017-12-10T12:21:44.106395: step 3438, loss 0.365751, acc 0.921875, prec 0.0709475, recall 0.890954
2017-12-10T12:21:44.557813: step 3439, loss 0.133059, acc 0.9375, prec 0.0709634, recall 0.890988
2017-12-10T12:21:45.001057: step 3440, loss 0.47992, acc 0.875, prec 0.0709494, recall 0.890988
2017-12-10T12:21:45.447698: step 3441, loss 0.2069, acc 0.9375, prec 0.0709654, recall 0.891022
2017-12-10T12:21:45.885190: step 3442, loss 0.413111, acc 0.90625, prec 0.0710236, recall 0.891123
2017-12-10T12:21:46.339349: step 3443, loss 0.389675, acc 0.859375, prec 0.0710307, recall 0.891156
2017-12-10T12:21:46.773354: step 3444, loss 3.05938, acc 0.859375, prec 0.0710625, recall 0.890948
2017-12-10T12:21:47.215195: step 3445, loss 0.2463, acc 0.9375, prec 0.0710784, recall 0.890982
2017-12-10T12:21:47.657797: step 3446, loss 0.273031, acc 0.90625, prec 0.0711136, recall 0.891049
2017-12-10T12:21:48.098265: step 3447, loss 0.310648, acc 0.9375, prec 0.0711524, recall 0.891117
2017-12-10T12:21:48.541923: step 3448, loss 0.500422, acc 0.859375, prec 0.0711366, recall 0.891117
2017-12-10T12:21:48.983920: step 3449, loss 0.496218, acc 0.890625, prec 0.0711244, recall 0.891117
2017-12-10T12:21:49.423973: step 3450, loss 0.188037, acc 0.9375, prec 0.0711631, recall 0.891184
2017-12-10T12:21:49.866577: step 3451, loss 0.637732, acc 0.796875, prec 0.0711403, recall 0.891184
2017-12-10T12:21:50.304429: step 3452, loss 0.492488, acc 0.890625, prec 0.0712195, recall 0.891318
2017-12-10T12:21:50.744599: step 3453, loss 0.572439, acc 0.84375, prec 0.0712248, recall 0.891351
2017-12-10T12:21:51.194682: step 3454, loss 0.573424, acc 0.78125, prec 0.0712459, recall 0.891418
2017-12-10T12:21:51.637414: step 3455, loss 0.567654, acc 0.875, prec 0.0713232, recall 0.891551
2017-12-10T12:21:52.073254: step 3456, loss 0.50896, acc 0.890625, prec 0.071311, recall 0.891551
2017-12-10T12:21:52.513332: step 3457, loss 0.345037, acc 0.90625, prec 0.0713461, recall 0.891618
2017-12-10T12:21:52.962007: step 3458, loss 0.212288, acc 0.9375, prec 0.0713391, recall 0.891618
2017-12-10T12:21:53.395945: step 3459, loss 0.424886, acc 0.953125, prec 0.0713338, recall 0.891618
2017-12-10T12:21:53.844491: step 3460, loss 0.219508, acc 0.921875, prec 0.0713479, recall 0.891651
2017-12-10T12:21:54.298207: step 3461, loss 0.151922, acc 0.953125, prec 0.0713426, recall 0.891651
2017-12-10T12:21:54.744415: step 3462, loss 0.519934, acc 0.890625, prec 0.0713304, recall 0.891651
2017-12-10T12:21:55.189914: step 3463, loss 0.356548, acc 0.9375, prec 0.0713234, recall 0.891651
2017-12-10T12:21:55.621342: step 3464, loss 0.222426, acc 0.921875, prec 0.0713146, recall 0.891651
2017-12-10T12:21:56.055847: step 3465, loss 0.116777, acc 0.953125, prec 0.0713321, recall 0.891685
2017-12-10T12:21:56.495825: step 3466, loss 0.311907, acc 0.9375, prec 0.0713251, recall 0.891685
2017-12-10T12:21:56.936529: step 3467, loss 0.170748, acc 0.984375, prec 0.0713234, recall 0.891685
2017-12-10T12:21:57.385507: step 3468, loss 0.0637021, acc 0.96875, prec 0.0713655, recall 0.891751
2017-12-10T12:21:57.840111: step 3469, loss 0.118442, acc 0.96875, prec 0.0713848, recall 0.891784
2017-12-10T12:21:58.277983: step 3470, loss 0.139742, acc 0.96875, prec 0.071404, recall 0.891817
2017-12-10T12:21:58.714563: step 3471, loss 0.224969, acc 0.9375, prec 0.0714198, recall 0.89185
2017-12-10T12:21:59.164536: step 3472, loss 0.102891, acc 0.96875, prec 0.0714391, recall 0.891884
2017-12-10T12:21:59.594321: step 3473, loss 0.022189, acc 1, prec 0.0714619, recall 0.891917
2017-12-10T12:22:00.025004: step 3474, loss 0.0283681, acc 0.984375, prec 0.0714601, recall 0.891917
2017-12-10T12:22:00.467637: step 3475, loss 0.262343, acc 0.953125, prec 0.0714776, recall 0.89195
2017-12-10T12:22:00.923759: step 3476, loss 0.240959, acc 0.953125, prec 0.0714724, recall 0.89195
2017-12-10T12:22:01.377316: step 3477, loss 0.00815101, acc 1, prec 0.0714951, recall 0.891983
2017-12-10T12:22:01.833300: step 3478, loss 0.665618, acc 0.96875, prec 0.0715144, recall 0.892016
2017-12-10T12:22:02.236997: step 3479, loss 0.0195593, acc 0.980392, prec 0.0715127, recall 0.892016
2017-12-10T12:22:02.694327: step 3480, loss 0.0410227, acc 0.984375, prec 0.0715337, recall 0.892049
2017-12-10T12:22:03.153731: step 3481, loss 1.11092, acc 0.96875, prec 0.0715757, recall 0.892115
2017-12-10T12:22:03.617963: step 3482, loss 0.129852, acc 0.96875, prec 0.0715949, recall 0.892148
2017-12-10T12:22:04.058375: step 3483, loss 0.228314, acc 0.9375, prec 0.0716107, recall 0.892181
2017-12-10T12:22:04.506633: step 3484, loss 0.525491, acc 0.953125, prec 0.0716509, recall 0.892247
2017-12-10T12:22:04.963719: step 3485, loss 0.104455, acc 0.984375, prec 0.0716947, recall 0.892312
2017-12-10T12:22:05.398746: step 3486, loss 0.175386, acc 0.9375, prec 0.0716877, recall 0.892312
2017-12-10T12:22:05.855882: step 3487, loss 0.226565, acc 0.953125, prec 0.0717051, recall 0.892345
2017-12-10T12:22:06.300741: step 3488, loss 0.221836, acc 0.9375, prec 0.0716981, recall 0.892345
2017-12-10T12:22:06.750670: step 3489, loss 0.371267, acc 0.96875, prec 0.0717401, recall 0.892411
2017-12-10T12:22:07.196388: step 3490, loss 0.164062, acc 0.96875, prec 0.0717593, recall 0.892444
2017-12-10T12:22:07.645521: step 3491, loss 0.0651234, acc 0.96875, prec 0.0717785, recall 0.892476
2017-12-10T12:22:08.092043: step 3492, loss 0.0827168, acc 0.96875, prec 0.0717978, recall 0.892509
2017-12-10T12:22:08.540065: step 3493, loss 0.4824, acc 0.90625, prec 0.0718099, recall 0.892542
2017-12-10T12:22:08.991594: step 3494, loss 0.227001, acc 0.953125, prec 0.0718047, recall 0.892542
2017-12-10T12:22:09.430284: step 3495, loss 0.150125, acc 0.96875, prec 0.0718466, recall 0.892607
2017-12-10T12:22:09.876285: step 3496, loss 0.0369694, acc 1, prec 0.0718693, recall 0.89264
2017-12-10T12:22:10.326345: step 3497, loss 0.308979, acc 0.921875, prec 0.0718833, recall 0.892673
2017-12-10T12:22:10.768178: step 3498, loss 0.370942, acc 0.90625, prec 0.0718954, recall 0.892705
2017-12-10T12:22:11.207921: step 3499, loss 0.388296, acc 0.859375, prec 0.0718796, recall 0.892705
2017-12-10T12:22:11.650824: step 3500, loss 0.0875945, acc 0.96875, prec 0.0718988, recall 0.892738
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3500

2017-12-10T12:22:13.807294: step 3501, loss 0.0695371, acc 0.96875, prec 0.071918, recall 0.89277
2017-12-10T12:22:14.255082: step 3502, loss 0.402168, acc 0.921875, prec 0.0719092, recall 0.89277
2017-12-10T12:22:14.694038: step 3503, loss 0.14793, acc 0.96875, prec 0.0719284, recall 0.892803
2017-12-10T12:22:15.127101: step 3504, loss 0.105813, acc 0.953125, prec 0.0719458, recall 0.892835
2017-12-10T12:22:15.579909: step 3505, loss 0.116571, acc 0.9375, prec 0.0719614, recall 0.892868
2017-12-10T12:22:16.019779: step 3506, loss 0.186665, acc 0.9375, prec 0.0719998, recall 0.892933
2017-12-10T12:22:16.454005: step 3507, loss 0.186832, acc 0.9375, prec 0.0719928, recall 0.892933
2017-12-10T12:22:16.890661: step 3508, loss 0.448748, acc 0.921875, prec 0.071984, recall 0.892933
2017-12-10T12:22:17.326361: step 3509, loss 0.164025, acc 0.953125, prec 0.0720014, recall 0.892965
2017-12-10T12:22:17.763956: step 3510, loss 0.268439, acc 0.9375, prec 0.072017, recall 0.892998
2017-12-10T12:22:18.207401: step 3511, loss 0.0180366, acc 0.984375, prec 0.0720153, recall 0.892998
2017-12-10T12:22:18.661914: step 3512, loss 0.0761164, acc 0.984375, prec 0.0720815, recall 0.893095
2017-12-10T12:22:19.110705: step 3513, loss 0.204636, acc 0.9375, prec 0.0720745, recall 0.893095
2017-12-10T12:22:19.565751: step 3514, loss 0.174698, acc 0.96875, prec 0.0720937, recall 0.893127
2017-12-10T12:22:19.998573: step 3515, loss 0.0317041, acc 1, prec 0.0720937, recall 0.893127
2017-12-10T12:22:20.435072: step 3516, loss 0.0404661, acc 0.984375, prec 0.0720919, recall 0.893127
2017-12-10T12:22:20.876852: step 3517, loss 0.234217, acc 0.9375, prec 0.0720848, recall 0.893127
2017-12-10T12:22:21.308277: step 3518, loss 0.0577317, acc 0.984375, prec 0.0720831, recall 0.893127
2017-12-10T12:22:21.748843: step 3519, loss 0.042897, acc 0.984375, prec 0.0720813, recall 0.893127
2017-12-10T12:22:22.188651: step 3520, loss 0.131989, acc 0.96875, prec 0.0721458, recall 0.893224
2017-12-10T12:22:22.630487: step 3521, loss 0.675794, acc 1, prec 0.0722138, recall 0.893321
2017-12-10T12:22:23.083651: step 3522, loss 0.106481, acc 0.96875, prec 0.0722103, recall 0.893321
2017-12-10T12:22:23.525716: step 3523, loss 0.0817107, acc 0.953125, prec 0.072205, recall 0.893321
2017-12-10T12:22:23.971573: step 3524, loss 1.61146, acc 0.953125, prec 0.0722241, recall 0.893084
2017-12-10T12:22:24.420587: step 3525, loss 0.041968, acc 0.96875, prec 0.0722206, recall 0.893084
2017-12-10T12:22:24.859010: step 3526, loss 0.200653, acc 0.984375, prec 0.0722188, recall 0.893084
2017-12-10T12:22:25.304211: step 3527, loss 0.0974275, acc 0.984375, prec 0.0722171, recall 0.893084
2017-12-10T12:22:25.754502: step 3528, loss 0.169477, acc 0.984375, prec 0.0722153, recall 0.893084
2017-12-10T12:22:26.216803: step 3529, loss 0.180907, acc 0.953125, prec 0.0722327, recall 0.893116
2017-12-10T12:22:26.665148: step 3530, loss 0.065275, acc 0.96875, prec 0.0722744, recall 0.89318
2017-12-10T12:22:27.113695: step 3531, loss 0.114583, acc 0.96875, prec 0.0723162, recall 0.893245
2017-12-10T12:22:27.565364: step 3532, loss 0.185013, acc 0.96875, prec 0.0723127, recall 0.893245
2017-12-10T12:22:28.019519: step 3533, loss 0.16888, acc 0.96875, prec 0.0723545, recall 0.893309
2017-12-10T12:22:28.465914: step 3534, loss 0.154779, acc 0.953125, prec 0.0723492, recall 0.893309
2017-12-10T12:22:28.923311: step 3535, loss 0.144558, acc 0.96875, prec 0.0723456, recall 0.893309
2017-12-10T12:22:29.370658: step 3536, loss 0.158034, acc 0.921875, prec 0.0723368, recall 0.893309
2017-12-10T12:22:29.815239: step 3537, loss 0.0802464, acc 0.984375, prec 0.0723577, recall 0.893341
2017-12-10T12:22:30.264595: step 3538, loss 0.0337899, acc 1, prec 0.0723803, recall 0.893373
2017-12-10T12:22:30.711071: step 3539, loss 0.148304, acc 0.921875, prec 0.0723715, recall 0.893373
2017-12-10T12:22:31.160110: step 3540, loss 0.158898, acc 0.96875, prec 0.0724358, recall 0.89347
2017-12-10T12:22:31.606187: step 3541, loss 0.0445004, acc 0.96875, prec 0.0724323, recall 0.89347
2017-12-10T12:22:32.047582: step 3542, loss 0.17604, acc 0.953125, prec 0.072427, recall 0.89347
2017-12-10T12:22:32.476068: step 3543, loss 0.0490081, acc 0.984375, prec 0.0724252, recall 0.89347
2017-12-10T12:22:32.917119: step 3544, loss 0.317968, acc 0.953125, prec 0.0724652, recall 0.893534
2017-12-10T12:22:33.373838: step 3545, loss 0.0561773, acc 0.984375, prec 0.072486, recall 0.893566
2017-12-10T12:22:33.816788: step 3546, loss 0.437807, acc 0.921875, prec 0.0724998, recall 0.893598
2017-12-10T12:22:34.263824: step 3547, loss 0.193883, acc 0.953125, prec 0.0725624, recall 0.893694
2017-12-10T12:22:34.714469: step 3548, loss 0.167622, acc 0.96875, prec 0.0725814, recall 0.893726
2017-12-10T12:22:35.156865: step 3549, loss 0.0391874, acc 0.984375, prec 0.0726023, recall 0.893758
2017-12-10T12:22:35.608253: step 3550, loss 0.0393588, acc 0.984375, prec 0.0726005, recall 0.893758
2017-12-10T12:22:36.054618: step 3551, loss 0.0346855, acc 0.984375, prec 0.072644, recall 0.893821
2017-12-10T12:22:36.510990: step 3552, loss 0.0190241, acc 1, prec 0.0727118, recall 0.893917
2017-12-10T12:22:36.950053: step 3553, loss 0.0375696, acc 0.984375, prec 0.0727326, recall 0.893948
2017-12-10T12:22:37.402412: step 3554, loss 0.136345, acc 0.984375, prec 0.0727534, recall 0.89398
2017-12-10T12:22:37.831633: step 3555, loss 1.20688, acc 0.984375, prec 0.0728194, recall 0.894075
2017-12-10T12:22:38.281888: step 3556, loss 0.111976, acc 0.96875, prec 0.0728837, recall 0.89417
2017-12-10T12:22:38.717006: step 3557, loss 0.168798, acc 0.953125, prec 0.0729235, recall 0.894234
2017-12-10T12:22:39.172670: step 3558, loss 0.570952, acc 0.984375, prec 0.0729669, recall 0.894297
2017-12-10T12:22:39.623775: step 3559, loss 0.148926, acc 0.984375, prec 0.0730103, recall 0.89436
2017-12-10T12:22:40.052211: step 3560, loss 0.061375, acc 0.953125, prec 0.073005, recall 0.89436
2017-12-10T12:22:40.493655: step 3561, loss 0.0777774, acc 0.9375, prec 0.0729979, recall 0.89436
2017-12-10T12:22:40.939574: step 3562, loss 0.403838, acc 0.9375, prec 0.0729907, recall 0.89436
2017-12-10T12:22:41.387747: step 3563, loss 0.0584083, acc 0.984375, prec 0.072989, recall 0.89436
2017-12-10T12:22:41.827531: step 3564, loss 0.912557, acc 0.890625, prec 0.0729991, recall 0.894391
2017-12-10T12:22:42.270335: step 3565, loss 0.234052, acc 0.96875, prec 0.0729955, recall 0.894391
2017-12-10T12:22:42.713905: step 3566, loss 0.471653, acc 0.96875, prec 0.0730371, recall 0.894454
2017-12-10T12:22:43.165193: step 3567, loss 0.0845685, acc 0.96875, prec 0.0730561, recall 0.894486
2017-12-10T12:22:43.605851: step 3568, loss 0.205042, acc 0.90625, prec 0.0730455, recall 0.894486
2017-12-10T12:22:44.060652: step 3569, loss 0.278903, acc 0.90625, prec 0.0730348, recall 0.894486
2017-12-10T12:22:44.503369: step 3570, loss 0.119322, acc 0.953125, prec 0.0730295, recall 0.894486
2017-12-10T12:22:44.941969: step 3571, loss 0.663875, acc 0.9375, prec 0.0730675, recall 0.894549
2017-12-10T12:22:45.389310: step 3572, loss 0.154514, acc 0.96875, prec 0.073109, recall 0.894611
2017-12-10T12:22:45.845516: step 3573, loss 0.665013, acc 0.9375, prec 0.0731245, recall 0.894643
2017-12-10T12:22:46.290824: step 3574, loss 0.156009, acc 0.9375, prec 0.0731399, recall 0.894674
2017-12-10T12:22:46.741455: step 3575, loss 0.234508, acc 0.9375, prec 0.0731328, recall 0.894674
2017-12-10T12:22:47.181683: step 3576, loss 0.0742035, acc 0.96875, prec 0.0731518, recall 0.894706
2017-12-10T12:22:47.646871: step 3577, loss 0.332202, acc 0.921875, prec 0.073233, recall 0.894831
2017-12-10T12:22:48.086307: step 3578, loss 0.234143, acc 0.921875, prec 0.0732692, recall 0.894893
2017-12-10T12:22:48.527618: step 3579, loss 0.257685, acc 0.90625, prec 0.0732585, recall 0.894893
2017-12-10T12:22:48.975300: step 3580, loss 0.156712, acc 0.984375, prec 0.0732567, recall 0.894893
2017-12-10T12:22:49.416818: step 3581, loss 4.54776, acc 0.921875, prec 0.0732496, recall 0.894628
2017-12-10T12:22:49.871544: step 3582, loss 0.138905, acc 0.921875, prec 0.0732407, recall 0.894628
2017-12-10T12:22:50.314412: step 3583, loss 0.17073, acc 0.921875, prec 0.0732318, recall 0.894628
2017-12-10T12:22:50.749097: step 3584, loss 0.466135, acc 0.875, prec 0.0732175, recall 0.894628
2017-12-10T12:22:51.182712: step 3585, loss 0.369287, acc 0.875, prec 0.0732033, recall 0.894628
2017-12-10T12:22:51.619420: step 3586, loss 0.342724, acc 0.9375, prec 0.0732637, recall 0.894721
2017-12-10T12:22:52.064300: step 3587, loss 0.174928, acc 0.953125, prec 0.0732584, recall 0.894721
2017-12-10T12:22:52.502326: step 3588, loss 0.278283, acc 0.90625, prec 0.0732702, recall 0.894752
2017-12-10T12:22:52.949725: step 3589, loss 0.672713, acc 0.890625, prec 0.0732803, recall 0.894784
2017-12-10T12:22:53.389035: step 3590, loss 0.410789, acc 0.890625, prec 0.0732903, recall 0.894815
2017-12-10T12:22:53.825337: step 3591, loss 0.450584, acc 0.84375, prec 0.0732725, recall 0.894815
2017-12-10T12:22:54.283224: step 3592, loss 0.379275, acc 0.90625, prec 0.0733068, recall 0.894877
2017-12-10T12:22:54.724957: step 3593, loss 0.797044, acc 0.78125, prec 0.0732819, recall 0.894877
2017-12-10T12:22:55.157475: step 3594, loss 0.120138, acc 0.9375, prec 0.0732973, recall 0.894908
2017-12-10T12:22:55.593485: step 3595, loss 0.557748, acc 0.921875, prec 0.0733333, recall 0.89497
2017-12-10T12:22:56.041305: step 3596, loss 0.116834, acc 0.953125, prec 0.0733505, recall 0.895001
2017-12-10T12:22:56.481225: step 3597, loss 0.40281, acc 0.890625, prec 0.073338, recall 0.895001
2017-12-10T12:22:56.926601: step 3598, loss 0.151573, acc 0.96875, prec 0.0733569, recall 0.895033
2017-12-10T12:22:57.370768: step 3599, loss 0.251933, acc 0.9375, prec 0.0734172, recall 0.895126
2017-12-10T12:22:57.825200: step 3600, loss 0.0509176, acc 0.96875, prec 0.0734136, recall 0.895126
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3600

2017-12-10T12:22:59.876638: step 3601, loss 0.0580735, acc 0.984375, prec 0.0734118, recall 0.895126
2017-12-10T12:23:00.319401: step 3602, loss 0.0278584, acc 0.984375, prec 0.073455, recall 0.895187
2017-12-10T12:23:00.771277: step 3603, loss 0.148979, acc 0.953125, prec 0.0734496, recall 0.895187
2017-12-10T12:23:01.215514: step 3604, loss 0.142015, acc 0.953125, prec 0.0734443, recall 0.895187
2017-12-10T12:23:01.654869: step 3605, loss 0.0120092, acc 1, prec 0.0734667, recall 0.895218
2017-12-10T12:23:02.099542: step 3606, loss 0.29477, acc 0.96875, prec 0.0734632, recall 0.895218
2017-12-10T12:23:02.539109: step 3607, loss 0.256991, acc 0.9375, prec 0.0734785, recall 0.895249
2017-12-10T12:23:02.988400: step 3608, loss 0.845663, acc 0.96875, prec 0.0734974, recall 0.89528
2017-12-10T12:23:03.435369: step 3609, loss 0.0720076, acc 0.984375, prec 0.0734956, recall 0.89528
2017-12-10T12:23:03.877353: step 3610, loss 0.126904, acc 0.984375, prec 0.0734938, recall 0.89528
2017-12-10T12:23:04.320202: step 3611, loss 0.0525125, acc 0.984375, prec 0.0735145, recall 0.895311
2017-12-10T12:23:04.761472: step 3612, loss 0.0266491, acc 0.984375, prec 0.0735127, recall 0.895311
2017-12-10T12:23:05.207995: step 3613, loss 0.0518169, acc 0.96875, prec 0.0735315, recall 0.895342
2017-12-10T12:23:05.653723: step 3614, loss 0.146821, acc 0.96875, prec 0.0735728, recall 0.895404
2017-12-10T12:23:06.102600: step 3615, loss 1.52554, acc 0.96875, prec 0.0735711, recall 0.89514
2017-12-10T12:23:06.555643: step 3616, loss 0.0340606, acc 0.984375, prec 0.0735693, recall 0.89514
2017-12-10T12:23:06.993396: step 3617, loss 0.160836, acc 0.9375, prec 0.073607, recall 0.895202
2017-12-10T12:23:07.431588: step 3618, loss 0.523853, acc 0.953125, prec 0.0736241, recall 0.895232
2017-12-10T12:23:07.880138: step 3619, loss 0.170485, acc 0.953125, prec 0.0736412, recall 0.895263
2017-12-10T12:23:08.329703: step 3620, loss 0.14525, acc 0.96875, prec 0.07366, recall 0.895294
2017-12-10T12:23:08.771567: step 3621, loss 0.286162, acc 0.921875, prec 0.0736511, recall 0.895294
2017-12-10T12:23:09.217929: step 3622, loss 0.327929, acc 0.921875, prec 0.0736646, recall 0.895325
2017-12-10T12:23:09.661879: step 3623, loss 0.0541731, acc 0.96875, prec 0.0737059, recall 0.895386
2017-12-10T12:23:10.117823: step 3624, loss 0.535849, acc 0.953125, prec 0.0737453, recall 0.895448
2017-12-10T12:23:10.554216: step 3625, loss 0.457467, acc 0.828125, prec 0.0737257, recall 0.895448
2017-12-10T12:23:11.001066: step 3626, loss 0.332882, acc 0.890625, prec 0.0737356, recall 0.895479
2017-12-10T12:23:11.445702: step 3627, loss 0.269492, acc 0.90625, prec 0.0737249, recall 0.895479
2017-12-10T12:23:11.891097: step 3628, loss 0.274586, acc 0.953125, prec 0.073742, recall 0.895509
2017-12-10T12:23:12.341694: step 3629, loss 0.120713, acc 0.953125, prec 0.073759, recall 0.89554
2017-12-10T12:23:12.785550: step 3630, loss 0.227873, acc 0.921875, prec 0.0737949, recall 0.895601
2017-12-10T12:23:13.230583: step 3631, loss 0.363125, acc 0.875, prec 0.0737806, recall 0.895601
2017-12-10T12:23:13.684241: step 3632, loss 0.418872, acc 0.890625, prec 0.0737681, recall 0.895601
2017-12-10T12:23:14.123590: step 3633, loss 0.200074, acc 0.890625, prec 0.073778, recall 0.895632
2017-12-10T12:23:14.567831: step 3634, loss 0.206764, acc 0.953125, prec 0.0737727, recall 0.895632
2017-12-10T12:23:15.007244: step 3635, loss 0.130686, acc 0.9375, prec 0.0737655, recall 0.895632
2017-12-10T12:23:15.451161: step 3636, loss 0.233073, acc 0.96875, prec 0.0737843, recall 0.895662
2017-12-10T12:23:15.890653: step 3637, loss 0.687382, acc 0.96875, prec 0.0738031, recall 0.895693
2017-12-10T12:23:16.346063: step 3638, loss 0.579012, acc 0.9375, prec 0.0739078, recall 0.895846
2017-12-10T12:23:16.804846: step 3639, loss 0.147249, acc 0.96875, prec 0.0739266, recall 0.895876
2017-12-10T12:23:17.257755: step 3640, loss 0.393014, acc 0.90625, prec 0.0739159, recall 0.895876
2017-12-10T12:23:17.703852: step 3641, loss 2.09866, acc 0.953125, prec 0.0739123, recall 0.895614
2017-12-10T12:23:18.162481: step 3642, loss 0.145139, acc 0.921875, prec 0.0739481, recall 0.895675
2017-12-10T12:23:18.604079: step 3643, loss 0.309868, acc 0.9375, prec 0.0739633, recall 0.895706
2017-12-10T12:23:19.055174: step 3644, loss 0.188835, acc 0.953125, prec 0.0739803, recall 0.895736
2017-12-10T12:23:19.498436: step 3645, loss 0.368416, acc 0.890625, prec 0.0739678, recall 0.895736
2017-12-10T12:23:19.954223: step 3646, loss 0.304156, acc 0.875, prec 0.0739758, recall 0.895766
2017-12-10T12:23:20.403429: step 3647, loss 0.428977, acc 0.875, prec 0.0739616, recall 0.895766
2017-12-10T12:23:20.843334: step 3648, loss 0.469199, acc 0.875, prec 0.0739473, recall 0.895766
2017-12-10T12:23:21.294632: step 3649, loss 0.305281, acc 0.921875, prec 0.0739384, recall 0.895766
2017-12-10T12:23:21.735494: step 3650, loss 0.762244, acc 0.84375, prec 0.0739429, recall 0.895797
2017-12-10T12:23:22.170346: step 3651, loss 0.307029, acc 0.859375, prec 0.0739492, recall 0.895827
2017-12-10T12:23:22.611877: step 3652, loss 0.616871, acc 0.875, prec 0.0739349, recall 0.895827
2017-12-10T12:23:23.055585: step 3653, loss 1.09754, acc 0.890625, prec 0.0739448, recall 0.895858
2017-12-10T12:23:23.500474: step 3654, loss 0.277986, acc 0.9375, prec 0.0739599, recall 0.895888
2017-12-10T12:23:23.945652: step 3655, loss 0.558765, acc 0.890625, prec 0.0739921, recall 0.895949
2017-12-10T12:23:24.386667: step 3656, loss 0.378676, acc 0.90625, prec 0.0740259, recall 0.896009
2017-12-10T12:23:24.823264: step 3657, loss 0.434389, acc 0.84375, prec 0.0740527, recall 0.89607
2017-12-10T12:23:25.263678: step 3658, loss 0.443147, acc 0.875, prec 0.0740607, recall 0.8961
2017-12-10T12:23:25.696915: step 3659, loss 1.06055, acc 0.8125, prec 0.0741061, recall 0.896191
2017-12-10T12:23:26.147831: step 3660, loss 0.462694, acc 0.859375, prec 0.0741346, recall 0.896251
2017-12-10T12:23:26.596785: step 3661, loss 0.370411, acc 0.890625, prec 0.0741221, recall 0.896251
2017-12-10T12:23:27.036962: step 3662, loss 0.295418, acc 0.859375, prec 0.0741061, recall 0.896251
2017-12-10T12:23:27.472930: step 3663, loss 0.312163, acc 0.921875, prec 0.0741417, recall 0.896311
2017-12-10T12:23:27.923684: step 3664, loss 0.192526, acc 0.96875, prec 0.0741604, recall 0.896341
2017-12-10T12:23:28.362065: step 3665, loss 0.302779, acc 0.90625, prec 0.0741942, recall 0.896402
2017-12-10T12:23:28.808911: step 3666, loss 0.442982, acc 0.890625, prec 0.0741817, recall 0.896402
2017-12-10T12:23:29.252758: step 3667, loss 3.70568, acc 0.90625, prec 0.0741728, recall 0.896142
2017-12-10T12:23:29.693316: step 3668, loss 0.17882, acc 0.953125, prec 0.0742119, recall 0.896202
2017-12-10T12:23:30.141382: step 3669, loss 0.204764, acc 0.96875, prec 0.0742528, recall 0.896262
2017-12-10T12:23:30.587938: step 3670, loss 0.355358, acc 0.890625, prec 0.0742403, recall 0.896262
2017-12-10T12:23:31.026771: step 3671, loss 0.525477, acc 0.890625, prec 0.0742278, recall 0.896262
2017-12-10T12:23:31.465185: step 3672, loss 0.358278, acc 0.84375, prec 0.07421, recall 0.896262
2017-12-10T12:23:31.916003: step 3673, loss 0.315888, acc 0.90625, prec 0.0741993, recall 0.896262
2017-12-10T12:23:32.360972: step 3674, loss 0.238668, acc 0.890625, prec 0.0741869, recall 0.896262
2017-12-10T12:23:32.804596: step 3675, loss 0.374486, acc 0.90625, prec 0.0741762, recall 0.896262
2017-12-10T12:23:33.245111: step 3676, loss 0.308466, acc 0.890625, prec 0.0741638, recall 0.896262
2017-12-10T12:23:33.689284: step 3677, loss 0.159488, acc 0.9375, prec 0.0741567, recall 0.896262
2017-12-10T12:23:34.125197: step 3678, loss 0.892661, acc 0.921875, prec 0.07417, recall 0.896292
2017-12-10T12:23:34.565273: step 3679, loss 0.209699, acc 0.953125, prec 0.0741868, recall 0.896322
2017-12-10T12:23:35.032186: step 3680, loss 0.340619, acc 0.859375, prec 0.074193, recall 0.896352
2017-12-10T12:23:35.463869: step 3681, loss 0.144442, acc 0.890625, prec 0.0742027, recall 0.896382
2017-12-10T12:23:35.909809: step 3682, loss 0.0239128, acc 1, prec 0.0742027, recall 0.896382
2017-12-10T12:23:36.347809: step 3683, loss 0.152598, acc 0.953125, prec 0.0742196, recall 0.896412
2017-12-10T12:23:36.797574: step 3684, loss 0.189271, acc 0.921875, prec 0.0742329, recall 0.896442
2017-12-10T12:23:37.245327: step 3685, loss 0.181631, acc 0.96875, prec 0.0742515, recall 0.896472
2017-12-10T12:23:37.692267: step 3686, loss 0.298896, acc 0.875, prec 0.0743038, recall 0.896562
2017-12-10T12:23:38.148574: step 3687, loss 0.181123, acc 0.9375, prec 0.0743188, recall 0.896592
2017-12-10T12:23:38.592869: step 3688, loss 0.135618, acc 0.953125, prec 0.0743578, recall 0.896651
2017-12-10T12:23:39.034910: step 3689, loss 0.422206, acc 0.890625, prec 0.0743454, recall 0.896651
2017-12-10T12:23:39.477030: step 3690, loss 0.0608874, acc 0.984375, prec 0.0743436, recall 0.896651
2017-12-10T12:23:39.926788: step 3691, loss 0.184782, acc 0.984375, prec 0.0743861, recall 0.896711
2017-12-10T12:23:40.371872: step 3692, loss 0.127975, acc 0.953125, prec 0.0744029, recall 0.896741
2017-12-10T12:23:40.808213: step 3693, loss 1.3287, acc 0.953125, prec 0.0744215, recall 0.896512
2017-12-10T12:23:41.252432: step 3694, loss 0.152401, acc 1, prec 0.0744436, recall 0.896542
2017-12-10T12:23:41.702916: step 3695, loss 0.178583, acc 0.96875, prec 0.0744622, recall 0.896572
2017-12-10T12:23:42.139319: step 3696, loss 0.213318, acc 0.9375, prec 0.0744772, recall 0.896601
2017-12-10T12:23:42.572486: step 3697, loss 0.490226, acc 0.921875, prec 0.0744905, recall 0.896631
2017-12-10T12:23:43.022459: step 3698, loss 0.561474, acc 0.953125, prec 0.0745294, recall 0.896691
2017-12-10T12:23:43.469956: step 3699, loss 0.0999462, acc 0.9375, prec 0.0745444, recall 0.89672
2017-12-10T12:23:43.916755: step 3700, loss 0.0456201, acc 0.96875, prec 0.0745408, recall 0.89672
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3700

2017-12-10T12:23:45.777819: step 3701, loss 0.254062, acc 0.921875, prec 0.0745541, recall 0.89675
2017-12-10T12:23:46.230854: step 3702, loss 0.0413924, acc 0.984375, prec 0.0745744, recall 0.89678
2017-12-10T12:23:46.685915: step 3703, loss 0.0362314, acc 0.984375, prec 0.0746169, recall 0.896839
2017-12-10T12:23:47.121750: step 3704, loss 0.62341, acc 0.90625, prec 0.0746283, recall 0.896869
2017-12-10T12:23:47.570201: step 3705, loss 0.68465, acc 0.921875, prec 0.0746636, recall 0.896928
2017-12-10T12:23:48.014981: step 3706, loss 0.109703, acc 0.96875, prec 0.0746822, recall 0.896958
2017-12-10T12:23:48.458869: step 3707, loss 0.198255, acc 0.96875, prec 0.0747228, recall 0.897017
2017-12-10T12:23:48.905848: step 3708, loss 0.230644, acc 0.9375, prec 0.0747378, recall 0.897046
2017-12-10T12:23:49.349470: step 3709, loss 0.10313, acc 0.9375, prec 0.0747306, recall 0.897046
2017-12-10T12:23:49.794962: step 3710, loss 0.249808, acc 0.890625, prec 0.0747402, recall 0.897076
2017-12-10T12:23:50.235796: step 3711, loss 0.301584, acc 0.96875, prec 0.0747588, recall 0.897105
2017-12-10T12:23:50.684957: step 3712, loss 0.287049, acc 0.921875, prec 0.0747719, recall 0.897135
2017-12-10T12:23:51.123251: step 3713, loss 0.389101, acc 0.953125, prec 0.0747666, recall 0.897135
2017-12-10T12:23:51.570216: step 3714, loss 0.0736622, acc 0.96875, prec 0.074763, recall 0.897135
2017-12-10T12:23:52.010388: step 3715, loss 0.205411, acc 0.953125, prec 0.0747797, recall 0.897164
2017-12-10T12:23:52.451436: step 3716, loss 0.218213, acc 0.921875, prec 0.0747929, recall 0.897194
2017-12-10T12:23:52.889350: step 3717, loss 0.312219, acc 0.953125, prec 0.0747875, recall 0.897194
2017-12-10T12:23:53.334694: step 3718, loss 0.256662, acc 0.9375, prec 0.0747804, recall 0.897194
2017-12-10T12:23:53.779448: step 3719, loss 0.212724, acc 0.953125, prec 0.0748192, recall 0.897252
2017-12-10T12:23:54.217123: step 3720, loss 0.128324, acc 0.953125, prec 0.0748359, recall 0.897282
2017-12-10T12:23:54.659476: step 3721, loss 0.0967754, acc 0.953125, prec 0.0748306, recall 0.897282
2017-12-10T12:23:55.110343: step 3722, loss 0.200421, acc 0.90625, prec 0.0748419, recall 0.897311
2017-12-10T12:23:55.554223: step 3723, loss 3.15292, acc 0.953125, prec 0.0748384, recall 0.897055
2017-12-10T12:23:56.001251: step 3724, loss 0.156852, acc 0.921875, prec 0.0748294, recall 0.897055
2017-12-10T12:23:56.453538: step 3725, loss 0.253168, acc 0.921875, prec 0.0748426, recall 0.897084
2017-12-10T12:23:56.890807: step 3726, loss 0.162718, acc 0.96875, prec 0.074839, recall 0.897084
2017-12-10T12:23:57.338725: step 3727, loss 0.36672, acc 0.890625, prec 0.0748706, recall 0.897143
2017-12-10T12:23:57.771887: step 3728, loss 0.197167, acc 0.921875, prec 0.0749058, recall 0.897202
2017-12-10T12:23:58.216554: step 3729, loss 0.126843, acc 0.96875, prec 0.0749023, recall 0.897202
2017-12-10T12:23:58.649855: step 3730, loss 0.583464, acc 0.8125, prec 0.0749029, recall 0.897231
2017-12-10T12:23:59.084794: step 3731, loss 0.164117, acc 0.953125, prec 0.0748975, recall 0.897231
2017-12-10T12:23:59.538686: step 3732, loss 0.298936, acc 0.9375, prec 0.0748904, recall 0.897231
2017-12-10T12:23:59.981301: step 3733, loss 0.170992, acc 0.875, prec 0.0748761, recall 0.897231
2017-12-10T12:24:00.435528: step 3734, loss 0.230275, acc 0.90625, prec 0.0748875, recall 0.89726
2017-12-10T12:24:00.874433: step 3735, loss 0.260926, acc 0.9375, prec 0.0748803, recall 0.89726
2017-12-10T12:24:01.321126: step 3736, loss 0.125052, acc 0.96875, prec 0.0749428, recall 0.897348
2017-12-10T12:24:01.771913: step 3737, loss 0.334711, acc 0.984375, prec 0.0749631, recall 0.897377
2017-12-10T12:24:02.242313: step 3738, loss 0.152412, acc 0.9375, prec 0.074978, recall 0.897407
2017-12-10T12:24:02.694240: step 3739, loss 0.175744, acc 0.9375, prec 0.0749929, recall 0.897436
2017-12-10T12:24:03.132358: step 3740, loss 0.301288, acc 0.984375, prec 0.0750571, recall 0.897523
2017-12-10T12:24:03.577089: step 3741, loss 0.326499, acc 0.9375, prec 0.07505, recall 0.897523
2017-12-10T12:24:04.021097: step 3742, loss 0.181, acc 0.9375, prec 0.0750649, recall 0.897553
2017-12-10T12:24:04.466499: step 3743, loss 0.143466, acc 0.921875, prec 0.0750559, recall 0.897553
2017-12-10T12:24:04.909051: step 3744, loss 0.240715, acc 0.953125, prec 0.0750506, recall 0.897553
2017-12-10T12:24:05.345337: step 3745, loss 0.620382, acc 0.953125, prec 0.0750892, recall 0.897611
2017-12-10T12:24:05.790202: step 3746, loss 0.226645, acc 0.921875, prec 0.0750803, recall 0.897611
2017-12-10T12:24:06.244781: step 3747, loss 0.146308, acc 0.953125, prec 0.0751189, recall 0.897669
2017-12-10T12:24:06.689317: step 3748, loss 0.0824597, acc 0.953125, prec 0.0751356, recall 0.897698
2017-12-10T12:24:07.130711: step 3749, loss 0.0330162, acc 0.984375, prec 0.0751338, recall 0.897698
2017-12-10T12:24:07.566543: step 3750, loss 0.139972, acc 0.96875, prec 0.0751522, recall 0.897727
2017-12-10T12:24:08.022077: step 3751, loss 0.10366, acc 0.953125, prec 0.0751688, recall 0.897756
2017-12-10T12:24:08.463822: step 3752, loss 0.18714, acc 0.90625, prec 0.0751801, recall 0.897785
2017-12-10T12:24:08.904720: step 3753, loss 0.0698037, acc 0.953125, prec 0.0751967, recall 0.897814
2017-12-10T12:24:09.352067: step 3754, loss 0.161569, acc 0.953125, prec 0.0752353, recall 0.897872
2017-12-10T12:24:09.804983: step 3755, loss 0.0633456, acc 0.96875, prec 0.0752318, recall 0.897872
2017-12-10T12:24:10.240605: step 3756, loss 0.285203, acc 0.890625, prec 0.0752412, recall 0.897901
2017-12-10T12:24:10.689512: step 3757, loss 0.714725, acc 0.96875, prec 0.0752596, recall 0.89793
2017-12-10T12:24:11.138267: step 3758, loss 0.278176, acc 0.90625, prec 0.0753148, recall 0.898017
2017-12-10T12:24:11.585182: step 3759, loss 0.185664, acc 0.9375, prec 0.0753296, recall 0.898046
2017-12-10T12:24:12.037130: step 3760, loss 0.238515, acc 0.953125, prec 0.0753242, recall 0.898046
2017-12-10T12:24:12.503509: step 3761, loss 0.156141, acc 0.96875, prec 0.0753426, recall 0.898075
2017-12-10T12:24:12.940602: step 3762, loss 0.196789, acc 0.9375, prec 0.0753355, recall 0.898075
2017-12-10T12:24:13.376688: step 3763, loss 0.244575, acc 0.9375, prec 0.0753722, recall 0.898132
2017-12-10T12:24:13.829373: step 3764, loss 0.161042, acc 0.9375, prec 0.0753651, recall 0.898132
2017-12-10T12:24:14.275735: step 3765, loss 0.0796292, acc 0.953125, prec 0.0754036, recall 0.89819
2017-12-10T12:24:14.718527: step 3766, loss 0.174136, acc 0.953125, prec 0.0754202, recall 0.898219
2017-12-10T12:24:15.158559: step 3767, loss 0.10972, acc 0.9375, prec 0.0754789, recall 0.898305
2017-12-10T12:24:15.580774: step 3768, loss 0.0361223, acc 1, prec 0.0755447, recall 0.898391
2017-12-10T12:24:16.016945: step 3769, loss 0.286704, acc 0.953125, prec 0.0755612, recall 0.89842
2017-12-10T12:24:16.458038: step 3770, loss 0.0698771, acc 0.953125, prec 0.0755778, recall 0.898449
2017-12-10T12:24:16.906642: step 3771, loss 0.0727433, acc 0.96875, prec 0.0755742, recall 0.898449
2017-12-10T12:24:17.365125: step 3772, loss 0.170036, acc 0.96875, prec 0.0755706, recall 0.898449
2017-12-10T12:24:17.809946: step 3773, loss 0.288553, acc 0.984375, prec 0.0755908, recall 0.898477
2017-12-10T12:24:18.262236: step 3774, loss 0.176736, acc 0.921875, prec 0.0755818, recall 0.898477
2017-12-10T12:24:18.694476: step 3775, loss 0.561258, acc 0.953125, prec 0.0756641, recall 0.898592
2017-12-10T12:24:19.134928: step 3776, loss 0.0739438, acc 0.96875, prec 0.0756605, recall 0.898592
2017-12-10T12:24:19.572447: step 3777, loss 0.292928, acc 0.96875, prec 0.0756789, recall 0.89862
2017-12-10T12:24:20.014677: step 3778, loss 0.163206, acc 0.984375, prec 0.0757428, recall 0.898706
2017-12-10T12:24:20.460164: step 3779, loss 0.414859, acc 0.953125, prec 0.0757813, recall 0.898763
2017-12-10T12:24:20.903528: step 3780, loss 0.174204, acc 0.953125, prec 0.0757759, recall 0.898763
2017-12-10T12:24:21.348418: step 3781, loss 0.0461022, acc 0.984375, prec 0.0757741, recall 0.898763
2017-12-10T12:24:21.794067: step 3782, loss 0.0202102, acc 1, prec 0.0757741, recall 0.898763
2017-12-10T12:24:22.239161: step 3783, loss 0.0153212, acc 1, prec 0.0757741, recall 0.898763
2017-12-10T12:24:22.674482: step 3784, loss 0.0347164, acc 1, prec 0.0757741, recall 0.898763
2017-12-10T12:24:23.116471: step 3785, loss 0.162449, acc 0.921875, prec 0.075787, recall 0.898791
2017-12-10T12:24:23.561215: step 3786, loss 0.102566, acc 0.96875, prec 0.0758053, recall 0.89882
2017-12-10T12:24:24.010191: step 3787, loss 0.24648, acc 0.90625, prec 0.0757946, recall 0.89882
2017-12-10T12:24:24.450415: step 3788, loss 0.141464, acc 0.96875, prec 0.075791, recall 0.89882
2017-12-10T12:24:24.887635: step 3789, loss 0.292125, acc 0.984375, prec 0.075833, recall 0.898876
2017-12-10T12:24:25.329475: step 3790, loss 0.0405061, acc 0.984375, prec 0.0758312, recall 0.898876
2017-12-10T12:24:25.780320: step 3791, loss 0.0423031, acc 0.984375, prec 0.0758294, recall 0.898876
2017-12-10T12:24:26.227366: step 3792, loss 0.045261, acc 0.984375, prec 0.0758714, recall 0.898933
2017-12-10T12:24:26.660733: step 3793, loss 1.36303, acc 0.96875, prec 0.0758915, recall 0.898709
2017-12-10T12:24:27.099669: step 3794, loss 0.385362, acc 0.953125, prec 0.075908, recall 0.898738
2017-12-10T12:24:27.536505: step 3795, loss 0.0118671, acc 1, prec 0.0759299, recall 0.898766
2017-12-10T12:24:27.981482: step 3796, loss 0.136367, acc 0.9375, prec 0.0759227, recall 0.898766
2017-12-10T12:24:28.429684: step 3797, loss 0.0908926, acc 0.984375, prec 0.0759647, recall 0.898823
2017-12-10T12:24:28.866768: step 3798, loss 0.0900307, acc 1, prec 0.0759865, recall 0.898851
2017-12-10T12:24:29.309196: step 3799, loss 0.121886, acc 0.984375, prec 0.0759847, recall 0.898851
2017-12-10T12:24:29.752682: step 3800, loss 0.268425, acc 0.890625, prec 0.0759721, recall 0.898851
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3800

2017-12-10T12:24:31.678455: step 3801, loss 0.280952, acc 0.921875, prec 0.075985, recall 0.89888
2017-12-10T12:24:32.127499: step 3802, loss 0.422656, acc 0.9375, prec 0.0760216, recall 0.898936
2017-12-10T12:24:32.569188: step 3803, loss 0.114862, acc 0.953125, prec 0.0760162, recall 0.898936
2017-12-10T12:24:33.008857: step 3804, loss 0.191206, acc 0.9375, prec 0.0760309, recall 0.898964
2017-12-10T12:24:33.445705: step 3805, loss 0.229548, acc 0.921875, prec 0.0760219, recall 0.898964
2017-12-10T12:24:33.882415: step 3806, loss 0.563773, acc 0.921875, prec 0.0760566, recall 0.899021
2017-12-10T12:24:34.325986: step 3807, loss 0.125834, acc 0.984375, prec 0.0760548, recall 0.899021
2017-12-10T12:24:34.775696: step 3808, loss 0.171283, acc 0.9375, prec 0.0760695, recall 0.899049
2017-12-10T12:24:35.218844: step 3809, loss 0.225238, acc 0.90625, prec 0.0760587, recall 0.899049
2017-12-10T12:24:35.660713: step 3810, loss 0.277841, acc 0.90625, prec 0.0760479, recall 0.899049
2017-12-10T12:24:36.111542: step 3811, loss 0.195067, acc 0.9375, prec 0.0760625, recall 0.899077
2017-12-10T12:24:36.557027: step 3812, loss 0.20628, acc 0.953125, prec 0.0760571, recall 0.899077
2017-12-10T12:24:37.010445: step 3813, loss 0.0469206, acc 0.984375, prec 0.0760553, recall 0.899077
2017-12-10T12:24:37.456343: step 3814, loss 0.0753686, acc 0.984375, prec 0.0760535, recall 0.899077
2017-12-10T12:24:37.905824: step 3815, loss 0.236143, acc 0.953125, prec 0.07607, recall 0.899106
2017-12-10T12:24:38.362110: step 3816, loss 1.45194, acc 0.953125, prec 0.0761301, recall 0.89919
2017-12-10T12:24:38.809340: step 3817, loss 0.0514479, acc 0.96875, prec 0.0761265, recall 0.89919
2017-12-10T12:24:39.250923: step 3818, loss 0.233449, acc 0.9375, prec 0.0761193, recall 0.89919
2017-12-10T12:24:39.695995: step 3819, loss 0.374308, acc 0.96875, prec 0.0761812, recall 0.899275
2017-12-10T12:24:40.150992: step 3820, loss 0.111911, acc 0.953125, prec 0.0761758, recall 0.899275
2017-12-10T12:24:40.590174: step 3821, loss 0.157381, acc 0.953125, prec 0.0761704, recall 0.899275
2017-12-10T12:24:41.021734: step 3822, loss 0.11642, acc 0.921875, prec 0.0761614, recall 0.899275
2017-12-10T12:24:41.469498: step 3823, loss 0.413946, acc 0.9375, prec 0.0761979, recall 0.899331
2017-12-10T12:24:41.913055: step 3824, loss 0.164814, acc 0.921875, prec 0.0761889, recall 0.899331
2017-12-10T12:24:42.360959: step 3825, loss 0.252929, acc 0.953125, prec 0.076249, recall 0.899415
2017-12-10T12:24:42.821801: step 3826, loss 0.521788, acc 0.890625, prec 0.0762582, recall 0.899443
2017-12-10T12:24:43.282776: step 3827, loss 0.240084, acc 0.921875, prec 0.076271, recall 0.899471
2017-12-10T12:24:43.728778: step 3828, loss 0.0584355, acc 0.984375, prec 0.0762692, recall 0.899471
2017-12-10T12:24:44.173770: step 3829, loss 0.194642, acc 0.890625, prec 0.0762784, recall 0.899499
2017-12-10T12:24:44.633971: step 3830, loss 0.326264, acc 0.953125, prec 0.0762948, recall 0.899527
2017-12-10T12:24:45.082028: step 3831, loss 0.167488, acc 0.9375, prec 0.0763312, recall 0.899583
2017-12-10T12:24:45.534600: step 3832, loss 0.144777, acc 0.984375, prec 0.0763512, recall 0.899611
2017-12-10T12:24:45.981640: step 3833, loss 0.0888752, acc 0.953125, prec 0.0763458, recall 0.899611
2017-12-10T12:24:46.426907: step 3834, loss 0.15719, acc 0.96875, prec 0.0763422, recall 0.899611
2017-12-10T12:24:46.856359: step 3835, loss 0.0971194, acc 0.984375, prec 0.0763622, recall 0.899639
2017-12-10T12:24:47.295930: step 3836, loss 0.0655297, acc 0.96875, prec 0.0763804, recall 0.899666
2017-12-10T12:24:47.735409: step 3837, loss 0.16218, acc 0.953125, prec 0.0764185, recall 0.899722
2017-12-10T12:24:48.179064: step 3838, loss 0.306297, acc 0.96875, prec 0.0764367, recall 0.89975
2017-12-10T12:24:48.633196: step 3839, loss 0.072653, acc 0.96875, prec 0.0764549, recall 0.899778
2017-12-10T12:24:49.077069: step 3840, loss 0.0576207, acc 0.984375, prec 0.0764531, recall 0.899778
2017-12-10T12:24:49.522628: step 3841, loss 0.0480762, acc 0.96875, prec 0.0764495, recall 0.899778
2017-12-10T12:24:49.964951: step 3842, loss 0.199462, acc 0.953125, prec 0.0764441, recall 0.899778
2017-12-10T12:24:50.416109: step 3843, loss 0.104585, acc 0.984375, prec 0.0764641, recall 0.899806
2017-12-10T12:24:50.868199: step 3844, loss 0.259141, acc 0.921875, prec 0.0764768, recall 0.899834
2017-12-10T12:24:51.314486: step 3845, loss 0.13872, acc 0.96875, prec 0.076495, recall 0.899861
2017-12-10T12:24:51.773569: step 3846, loss 5.40182, acc 0.921875, prec 0.0765313, recall 0.899667
2017-12-10T12:24:52.228061: step 3847, loss 0.0715071, acc 0.96875, prec 0.0765495, recall 0.899695
2017-12-10T12:24:52.677669: step 3848, loss 2.34594, acc 0.921875, prec 0.076564, recall 0.899474
2017-12-10T12:24:53.134821: step 3849, loss 0.213252, acc 0.921875, prec 0.0765986, recall 0.899529
2017-12-10T12:24:53.573909: step 3850, loss 0.255856, acc 0.9375, prec 0.0765913, recall 0.899529
2017-12-10T12:24:54.025151: step 3851, loss 0.208989, acc 0.921875, prec 0.0766258, recall 0.899585
2017-12-10T12:24:54.466866: step 3852, loss 0.215941, acc 0.90625, prec 0.0766803, recall 0.899668
2017-12-10T12:24:54.916559: step 3853, loss 0.189355, acc 0.953125, prec 0.0766748, recall 0.899668
2017-12-10T12:24:55.355455: step 3854, loss 0.564632, acc 0.78125, prec 0.0766713, recall 0.899696
2017-12-10T12:24:55.798857: step 3855, loss 1.05049, acc 0.796875, prec 0.0766696, recall 0.899724
2017-12-10T12:24:56.249305: step 3856, loss 0.359189, acc 0.875, prec 0.0766551, recall 0.899724
2017-12-10T12:24:56.685171: step 3857, loss 0.43091, acc 0.859375, prec 0.0766824, recall 0.899779
2017-12-10T12:24:57.125513: step 3858, loss 0.687756, acc 0.796875, prec 0.0767023, recall 0.899834
2017-12-10T12:24:57.565288: step 3859, loss 0.623748, acc 0.859375, prec 0.0766861, recall 0.899834
2017-12-10T12:24:58.009764: step 3860, loss 0.311097, acc 0.890625, prec 0.0766952, recall 0.899862
2017-12-10T12:24:58.451367: step 3861, loss 0.538035, acc 0.84375, prec 0.0766772, recall 0.899862
2017-12-10T12:24:58.922155: step 3862, loss 0.615402, acc 0.859375, prec 0.0767043, recall 0.899917
2017-12-10T12:24:59.377405: step 3863, loss 0.343124, acc 0.90625, prec 0.0766935, recall 0.899917
2017-12-10T12:24:59.822378: step 3864, loss 0.577594, acc 0.8125, prec 0.0766936, recall 0.899945
2017-12-10T12:25:00.250219: step 3865, loss 0.548574, acc 0.84375, prec 0.0766973, recall 0.899972
2017-12-10T12:25:00.697063: step 3866, loss 0.787949, acc 0.84375, prec 0.0766793, recall 0.899972
2017-12-10T12:25:01.140259: step 3867, loss 0.356169, acc 0.921875, prec 0.0766703, recall 0.899972
2017-12-10T12:25:01.588079: step 3868, loss 0.516172, acc 0.90625, prec 0.0767245, recall 0.900055
2017-12-10T12:25:02.032970: step 3869, loss 0.307743, acc 0.90625, prec 0.0767137, recall 0.900055
2017-12-10T12:25:02.487599: step 3870, loss 0.200339, acc 0.921875, prec 0.0767263, recall 0.900083
2017-12-10T12:25:02.938998: step 3871, loss 0.0614467, acc 0.984375, prec 0.0767679, recall 0.900138
2017-12-10T12:25:03.389323: step 3872, loss 0.0671611, acc 0.96875, prec 0.0768076, recall 0.900192
2017-12-10T12:25:03.831038: step 3873, loss 0.437561, acc 0.921875, prec 0.0768419, recall 0.900247
2017-12-10T12:25:04.268871: step 3874, loss 0.644204, acc 0.96875, prec 0.0769249, recall 0.900357
2017-12-10T12:25:04.722054: step 3875, loss 0.477406, acc 0.953125, prec 0.0769411, recall 0.900384
2017-12-10T12:25:05.171374: step 3876, loss 0.1583, acc 0.96875, prec 0.0769592, recall 0.900412
2017-12-10T12:25:05.613612: step 3877, loss 0.121231, acc 0.984375, prec 0.076979, recall 0.900439
2017-12-10T12:25:06.076311: step 3878, loss 0.104407, acc 0.96875, prec 0.076997, recall 0.900466
2017-12-10T12:25:06.516753: step 3879, loss 0.293889, acc 0.90625, prec 0.0770078, recall 0.900493
2017-12-10T12:25:06.963447: step 3880, loss 0.431358, acc 0.953125, prec 0.0770241, recall 0.900521
2017-12-10T12:25:07.395510: step 3881, loss 0.273248, acc 0.96875, prec 0.0770421, recall 0.900548
2017-12-10T12:25:07.831012: step 3882, loss 0.115149, acc 0.984375, prec 0.0770403, recall 0.900548
2017-12-10T12:25:08.268200: step 3883, loss 0.0986836, acc 0.96875, prec 0.0770583, recall 0.900575
2017-12-10T12:25:08.700028: step 3884, loss 0.056009, acc 0.96875, prec 0.0770763, recall 0.900602
2017-12-10T12:25:09.144520: step 3885, loss 0.00952784, acc 1, prec 0.0770979, recall 0.90063
2017-12-10T12:25:09.588246: step 3886, loss 0.158102, acc 0.96875, prec 0.0770943, recall 0.90063
2017-12-10T12:25:10.036957: step 3887, loss 0.276636, acc 0.984375, prec 0.0771358, recall 0.900684
2017-12-10T12:25:10.467369: step 3888, loss 0.187391, acc 0.953125, prec 0.0771303, recall 0.900684
2017-12-10T12:25:10.927962: step 3889, loss 0.439577, acc 0.96875, prec 0.0771483, recall 0.900711
2017-12-10T12:25:11.377048: step 3890, loss 0.0296888, acc 0.984375, prec 0.0771465, recall 0.900711
2017-12-10T12:25:11.830551: step 3891, loss 0.0102711, acc 1, prec 0.0771465, recall 0.900711
2017-12-10T12:25:12.280341: step 3892, loss 0.165617, acc 0.984375, prec 0.0771663, recall 0.900738
2017-12-10T12:25:12.730431: step 3893, loss 0.186882, acc 0.96875, prec 0.077206, recall 0.900793
2017-12-10T12:25:13.190970: step 3894, loss 0.15601, acc 0.984375, prec 0.0772042, recall 0.900793
2017-12-10T12:25:13.638734: step 3895, loss 5.98023, acc 0.953125, prec 0.077287, recall 0.900655
2017-12-10T12:25:14.082009: step 3896, loss 0.102956, acc 0.984375, prec 0.0773284, recall 0.900709
2017-12-10T12:25:14.520904: step 3897, loss 0.186541, acc 0.953125, prec 0.0773446, recall 0.900736
2017-12-10T12:25:14.963032: step 3898, loss 0.321318, acc 0.921875, prec 0.0773571, recall 0.900763
2017-12-10T12:25:15.416793: step 3899, loss 0.181377, acc 0.953125, prec 0.0773517, recall 0.900763
2017-12-10T12:25:15.867427: step 3900, loss 0.421913, acc 0.90625, prec 0.0773408, recall 0.900763
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-3900

2017-12-10T12:25:17.877833: step 3901, loss 0.308093, acc 0.921875, prec 0.0773534, recall 0.90079
2017-12-10T12:25:18.311490: step 3902, loss 0.344081, acc 0.90625, prec 0.0773425, recall 0.90079
2017-12-10T12:25:18.754093: step 3903, loss 0.482924, acc 0.875, prec 0.077328, recall 0.90079
2017-12-10T12:25:19.212093: step 3904, loss 0.628801, acc 0.90625, prec 0.0773172, recall 0.90079
2017-12-10T12:25:19.656097: step 3905, loss 0.540139, acc 0.84375, prec 0.0772991, recall 0.90079
2017-12-10T12:25:20.101394: step 3906, loss 0.326095, acc 0.890625, prec 0.077308, recall 0.900817
2017-12-10T12:25:20.544457: step 3907, loss 0.216977, acc 0.90625, prec 0.0772972, recall 0.900817
2017-12-10T12:25:20.994642: step 3908, loss 0.240675, acc 0.921875, prec 0.0773313, recall 0.900871
2017-12-10T12:25:21.425668: step 3909, loss 0.273543, acc 0.9375, prec 0.0773456, recall 0.900898
2017-12-10T12:25:21.864084: step 3910, loss 0.906226, acc 0.859375, prec 0.0773293, recall 0.900898
2017-12-10T12:25:22.301119: step 3911, loss 0.315295, acc 0.890625, prec 0.0773167, recall 0.900898
2017-12-10T12:25:22.749867: step 3912, loss 0.684047, acc 0.875, prec 0.0773022, recall 0.900898
2017-12-10T12:25:23.200730: step 3913, loss 0.488953, acc 0.890625, prec 0.0773112, recall 0.900925
2017-12-10T12:25:23.641702: step 3914, loss 0.204131, acc 0.953125, prec 0.0773488, recall 0.900979
2017-12-10T12:25:24.088359: step 3915, loss 0.407071, acc 0.90625, prec 0.0773595, recall 0.901006
2017-12-10T12:25:24.527608: step 3916, loss 0.373559, acc 0.90625, prec 0.0773702, recall 0.901033
2017-12-10T12:25:24.969316: step 3917, loss 0.219756, acc 0.921875, prec 0.0773828, recall 0.90106
2017-12-10T12:25:25.401261: step 3918, loss 0.420247, acc 0.90625, prec 0.0773935, recall 0.901087
2017-12-10T12:25:25.849552: step 3919, loss 0.493487, acc 0.921875, prec 0.0774275, recall 0.901141
2017-12-10T12:25:26.284126: step 3920, loss 0.0836863, acc 0.984375, prec 0.0774257, recall 0.901141
2017-12-10T12:25:26.729347: step 3921, loss 0.180865, acc 0.9375, prec 0.0774185, recall 0.901141
2017-12-10T12:25:27.176374: step 3922, loss 0.12135, acc 0.953125, prec 0.0774776, recall 0.901221
2017-12-10T12:25:27.624305: step 3923, loss 0.181104, acc 0.953125, prec 0.0774722, recall 0.901221
2017-12-10T12:25:28.072173: step 3924, loss 0.0219565, acc 0.984375, prec 0.0774704, recall 0.901221
2017-12-10T12:25:28.505088: step 3925, loss 0.084733, acc 0.96875, prec 0.0774883, recall 0.901248
2017-12-10T12:25:28.966077: step 3926, loss 0.165534, acc 0.9375, prec 0.077481, recall 0.901248
2017-12-10T12:25:29.417185: step 3927, loss 0.228005, acc 0.9375, prec 0.0774738, recall 0.901248
2017-12-10T12:25:29.865567: step 3928, loss 0.076826, acc 0.953125, prec 0.0774899, recall 0.901275
2017-12-10T12:25:30.324736: step 3929, loss 0.256126, acc 0.96875, prec 0.0775078, recall 0.901302
2017-12-10T12:25:30.770530: step 3930, loss 0.241836, acc 0.984375, prec 0.0775275, recall 0.901328
2017-12-10T12:25:31.216849: step 3931, loss 2.16757, acc 0.96875, prec 0.0775472, recall 0.901111
2017-12-10T12:25:31.659537: step 3932, loss 0.0892019, acc 0.953125, prec 0.0775848, recall 0.901164
2017-12-10T12:25:32.103060: step 3933, loss 0.176522, acc 0.953125, prec 0.0775794, recall 0.901164
2017-12-10T12:25:32.555753: step 3934, loss 2.40376, acc 0.96875, prec 0.0775991, recall 0.900947
2017-12-10T12:25:33.021065: step 3935, loss 0.0642291, acc 0.984375, prec 0.0776403, recall 0.901001
2017-12-10T12:25:33.470678: step 3936, loss 1.29807, acc 0.953125, prec 0.0777011, recall 0.900838
2017-12-10T12:25:33.912079: step 3937, loss 0.221864, acc 0.9375, prec 0.0776939, recall 0.900838
2017-12-10T12:25:34.364730: step 3938, loss 0.278467, acc 0.9375, prec 0.0777081, recall 0.900864
2017-12-10T12:25:34.806204: step 3939, loss 0.194791, acc 0.921875, prec 0.0777206, recall 0.900891
2017-12-10T12:25:35.247031: step 3940, loss 0.154311, acc 0.953125, prec 0.0777151, recall 0.900891
2017-12-10T12:25:35.684338: step 3941, loss 0.15406, acc 0.921875, prec 0.0777491, recall 0.900945
2017-12-10T12:25:36.120134: step 3942, loss 0.156036, acc 0.921875, prec 0.0777615, recall 0.900971
2017-12-10T12:25:36.552483: step 3943, loss 3.97397, acc 0.890625, prec 0.0777506, recall 0.900728
2017-12-10T12:25:37.003960: step 3944, loss 0.438846, acc 0.859375, prec 0.0777343, recall 0.900728
2017-12-10T12:25:37.450496: step 3945, loss 0.658791, acc 0.8125, prec 0.0777126, recall 0.900728
2017-12-10T12:25:37.886372: step 3946, loss 0.636198, acc 0.828125, prec 0.0776927, recall 0.900728
2017-12-10T12:25:38.324131: step 3947, loss 0.680971, acc 0.8125, prec 0.0776925, recall 0.900755
2017-12-10T12:25:38.763014: step 3948, loss 0.641956, acc 0.828125, prec 0.0776941, recall 0.900782
2017-12-10T12:25:39.213708: step 3949, loss 0.743475, acc 0.859375, prec 0.0776778, recall 0.900782
2017-12-10T12:25:39.666302: step 3950, loss 0.726243, acc 0.796875, prec 0.0776543, recall 0.900782
2017-12-10T12:25:40.107492: step 3951, loss 1.37403, acc 0.703125, prec 0.0776415, recall 0.900809
2017-12-10T12:25:40.559747: step 3952, loss 0.903706, acc 0.8125, prec 0.0776841, recall 0.900889
2017-12-10T12:25:41.003869: step 3953, loss 0.255732, acc 0.9375, prec 0.0777197, recall 0.900942
2017-12-10T12:25:41.454525: step 3954, loss 0.583433, acc 0.8125, prec 0.0777409, recall 0.900995
2017-12-10T12:25:41.906560: step 3955, loss 0.293597, acc 0.859375, prec 0.0777247, recall 0.900995
2017-12-10T12:25:42.352910: step 3956, loss 0.401831, acc 0.84375, prec 0.077728, recall 0.901022
2017-12-10T12:25:42.793651: step 3957, loss 1.08721, acc 0.734375, prec 0.0777402, recall 0.901075
2017-12-10T12:25:43.252458: step 3958, loss 0.169647, acc 0.9375, prec 0.0777757, recall 0.901128
2017-12-10T12:25:43.707123: step 3959, loss 0.383629, acc 0.921875, prec 0.0777881, recall 0.901155
2017-12-10T12:25:44.161905: step 3960, loss 0.170508, acc 0.9375, prec 0.0778022, recall 0.901182
2017-12-10T12:25:44.614945: step 3961, loss 0.455906, acc 0.84375, prec 0.0778056, recall 0.901208
2017-12-10T12:25:45.055749: step 3962, loss 0.134587, acc 0.96875, prec 0.0778234, recall 0.901235
2017-12-10T12:25:45.495728: step 3963, loss 0.102069, acc 0.9375, prec 0.0778375, recall 0.901261
2017-12-10T12:25:45.943848: step 3964, loss 0.0483464, acc 0.984375, prec 0.0778357, recall 0.901261
2017-12-10T12:25:46.389712: step 3965, loss 0.307859, acc 0.9375, prec 0.0778712, recall 0.901314
2017-12-10T12:25:46.854151: step 3966, loss 0.16417, acc 0.953125, prec 0.0779085, recall 0.901367
2017-12-10T12:25:47.292619: step 3967, loss 0.0706786, acc 0.953125, prec 0.0779031, recall 0.901367
2017-12-10T12:25:47.738445: step 3968, loss 0.0247202, acc 0.984375, prec 0.0779013, recall 0.901367
2017-12-10T12:25:48.182807: step 3969, loss 0.394794, acc 0.921875, prec 0.0779137, recall 0.901393
2017-12-10T12:25:48.635537: step 3970, loss 0.68932, acc 1, prec 0.077935, recall 0.90142
2017-12-10T12:25:49.092041: step 3971, loss 0.062439, acc 0.96875, prec 0.0779314, recall 0.90142
2017-12-10T12:25:49.540003: step 3972, loss 0.0626788, acc 0.96875, prec 0.0779278, recall 0.90142
2017-12-10T12:25:49.982632: step 3973, loss 0.0477267, acc 0.984375, prec 0.0779473, recall 0.901446
2017-12-10T12:25:50.420563: step 3974, loss 0.0406102, acc 0.96875, prec 0.0779437, recall 0.901446
2017-12-10T12:25:50.865469: step 3975, loss 0.60233, acc 0.96875, prec 0.0779828, recall 0.901499
2017-12-10T12:25:51.265308: step 3976, loss 0.0181605, acc 1, prec 0.0779828, recall 0.901499
2017-12-10T12:25:51.724090: step 3977, loss 0.0641078, acc 0.96875, prec 0.0780006, recall 0.901525
2017-12-10T12:25:52.169318: step 3978, loss 0.11616, acc 0.96875, prec 0.0779969, recall 0.901525
2017-12-10T12:25:52.610630: step 3979, loss 0.0220678, acc 1, prec 0.0779969, recall 0.901525
2017-12-10T12:25:53.050981: step 3980, loss 0.039554, acc 0.984375, prec 0.0780165, recall 0.901552
2017-12-10T12:25:53.497433: step 3981, loss 1.50301, acc 0.96875, prec 0.0780787, recall 0.90139
2017-12-10T12:25:53.949873: step 3982, loss 0.193738, acc 0.953125, prec 0.0780733, recall 0.90139
2017-12-10T12:25:54.417396: step 3983, loss 0.380969, acc 0.90625, prec 0.0781264, recall 0.901469
2017-12-10T12:25:54.851900: step 3984, loss 0.138638, acc 0.953125, prec 0.0781424, recall 0.901495
2017-12-10T12:25:55.295654: step 3985, loss 0.112152, acc 0.953125, prec 0.0782009, recall 0.901574
2017-12-10T12:25:55.745634: step 3986, loss 0.265064, acc 0.953125, prec 0.0782168, recall 0.9016
2017-12-10T12:25:56.203249: step 3987, loss 0.138733, acc 0.9375, prec 0.0782309, recall 0.901626
2017-12-10T12:25:56.639075: step 3988, loss 0.17841, acc 0.9375, prec 0.0782663, recall 0.901679
2017-12-10T12:25:57.084849: step 3989, loss 0.0313466, acc 1, prec 0.0782663, recall 0.901679
2017-12-10T12:25:57.530689: step 3990, loss 0.200294, acc 0.921875, prec 0.0782572, recall 0.901679
2017-12-10T12:25:57.980967: step 3991, loss 0.035163, acc 1, prec 0.0782572, recall 0.901679
2017-12-10T12:25:58.423637: step 3992, loss 0.135072, acc 0.9375, prec 0.07825, recall 0.901679
2017-12-10T12:25:58.866942: step 3993, loss 0.147089, acc 0.9375, prec 0.0782641, recall 0.901705
2017-12-10T12:25:59.305489: step 3994, loss 0.0848771, acc 0.984375, prec 0.0782623, recall 0.901705
2017-12-10T12:25:59.747653: step 3995, loss 0.466715, acc 0.9375, prec 0.0782764, recall 0.901731
2017-12-10T12:26:00.189455: step 3996, loss 0.15987, acc 0.9375, prec 0.0782691, recall 0.901731
2017-12-10T12:26:00.630783: step 3997, loss 0.321936, acc 0.890625, prec 0.0782778, recall 0.901757
2017-12-10T12:26:01.065744: step 3998, loss 0.197037, acc 0.953125, prec 0.0782936, recall 0.901783
2017-12-10T12:26:01.517608: step 3999, loss 0.538094, acc 0.984375, prec 0.0783557, recall 0.901862
2017-12-10T12:26:01.968009: step 4000, loss 0.137768, acc 0.921875, prec 0.0783467, recall 0.901862

Evaluation:
2017-12-10T12:26:11.483374: step 4000, loss 2.48327, acc 0.944617, prec 0.0791249, recall 0.887752

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4000

2017-12-10T12:26:13.369522: step 4001, loss 0.261594, acc 0.9375, prec 0.0791596, recall 0.88781
2017-12-10T12:26:13.811008: step 4002, loss 0.17374, acc 0.953125, prec 0.0791962, recall 0.887867
2017-12-10T12:26:14.253067: step 4003, loss 0.363558, acc 0.96875, prec 0.0792136, recall 0.887896
2017-12-10T12:26:14.698202: step 4004, loss 0.204765, acc 0.9375, prec 0.0792273, recall 0.887924
2017-12-10T12:26:15.155314: step 4005, loss 0.072519, acc 0.96875, prec 0.0792657, recall 0.887982
2017-12-10T12:26:15.597226: step 4006, loss 0.102629, acc 0.984375, prec 0.0792848, recall 0.88801
2017-12-10T12:26:16.065418: step 4007, loss 0.242134, acc 0.890625, prec 0.0792722, recall 0.88801
2017-12-10T12:26:16.502379: step 4008, loss 0.3551, acc 0.90625, prec 0.0792613, recall 0.88801
2017-12-10T12:26:16.938482: step 4009, loss 0.117714, acc 0.9375, prec 0.0792541, recall 0.88801
2017-12-10T12:26:17.378814: step 4010, loss 0.0589487, acc 0.984375, prec 0.0792733, recall 0.888039
2017-12-10T12:26:17.825449: step 4011, loss 0.065634, acc 0.96875, prec 0.0792697, recall 0.888039
2017-12-10T12:26:18.262071: step 4012, loss 0.200237, acc 0.953125, prec 0.0792643, recall 0.888039
2017-12-10T12:26:18.697862: step 4013, loss 0.285254, acc 0.953125, prec 0.0793008, recall 0.888096
2017-12-10T12:26:19.154388: step 4014, loss 0.103736, acc 0.984375, prec 0.079299, recall 0.888096
2017-12-10T12:26:19.598886: step 4015, loss 0.155669, acc 0.96875, prec 0.0793373, recall 0.888153
2017-12-10T12:26:20.040574: step 4016, loss 0.206097, acc 0.953125, prec 0.0793737, recall 0.88821
2017-12-10T12:26:20.487185: step 4017, loss 0.185889, acc 0.953125, prec 0.0793893, recall 0.888238
2017-12-10T12:26:20.922848: step 4018, loss 0.035225, acc 0.984375, prec 0.0793875, recall 0.888238
2017-12-10T12:26:21.357265: step 4019, loss 0.0353355, acc 0.984375, prec 0.0793857, recall 0.888238
2017-12-10T12:26:21.807341: step 4020, loss 0.506367, acc 0.953125, prec 0.0793802, recall 0.888238
2017-12-10T12:26:22.252295: step 4021, loss 0.535982, acc 0.984375, prec 0.0793994, recall 0.888267
2017-12-10T12:26:22.719229: step 4022, loss 0.014283, acc 1, prec 0.0794203, recall 0.888295
2017-12-10T12:26:23.166462: step 4023, loss 0.256355, acc 0.921875, prec 0.0794113, recall 0.888295
2017-12-10T12:26:23.606671: step 4024, loss 0.0440473, acc 0.984375, prec 0.0794095, recall 0.888295
2017-12-10T12:26:24.047147: step 4025, loss 0.0265856, acc 0.984375, prec 0.0794705, recall 0.88838
2017-12-10T12:26:24.493823: step 4026, loss 0.0659543, acc 0.96875, prec 0.0794878, recall 0.888409
2017-12-10T12:26:24.937836: step 4027, loss 0.249601, acc 0.953125, prec 0.0794824, recall 0.888409
2017-12-10T12:26:25.386791: step 4028, loss 0.0716383, acc 0.984375, prec 0.0795015, recall 0.888437
2017-12-10T12:26:25.823758: step 4029, loss 0.260011, acc 0.953125, prec 0.0794961, recall 0.888437
2017-12-10T12:26:26.265584: step 4030, loss 0.174691, acc 0.953125, prec 0.0794907, recall 0.888437
2017-12-10T12:26:26.710870: step 4031, loss 0.258526, acc 0.890625, prec 0.079478, recall 0.888437
2017-12-10T12:26:27.155151: step 4032, loss 0.627477, acc 0.984375, prec 0.0794971, recall 0.888465
2017-12-10T12:26:27.608292: step 4033, loss 0.158706, acc 0.96875, prec 0.0795145, recall 0.888494
2017-12-10T12:26:28.058500: step 4034, loss 0.155223, acc 0.96875, prec 0.0795318, recall 0.888522
2017-12-10T12:26:28.509217: step 4035, loss 0.0761234, acc 0.96875, prec 0.0795282, recall 0.888522
2017-12-10T12:26:28.954099: step 4036, loss 0.14812, acc 0.921875, prec 0.0795191, recall 0.888522
2017-12-10T12:26:29.402910: step 4037, loss 0.127524, acc 0.9375, prec 0.0795328, recall 0.88855
2017-12-10T12:26:29.844711: step 4038, loss 0.108574, acc 0.953125, prec 0.0795274, recall 0.88855
2017-12-10T12:26:30.290628: step 4039, loss 0.135212, acc 0.953125, prec 0.079522, recall 0.88855
2017-12-10T12:26:30.737899: step 4040, loss 0.0829152, acc 0.984375, prec 0.0795202, recall 0.88855
2017-12-10T12:26:31.189677: step 4041, loss 0.00940572, acc 1, prec 0.0795202, recall 0.88855
2017-12-10T12:26:31.633537: step 4042, loss 0.11531, acc 0.984375, prec 0.0795393, recall 0.888579
2017-12-10T12:26:32.095533: step 4043, loss 0.0559041, acc 0.984375, prec 0.0795375, recall 0.888579
2017-12-10T12:26:32.548950: step 4044, loss 0.0414468, acc 0.984375, prec 0.0795566, recall 0.888607
2017-12-10T12:26:32.999704: step 4045, loss 0.113125, acc 0.96875, prec 0.0795738, recall 0.888635
2017-12-10T12:26:33.451565: step 4046, loss 0.396869, acc 1, prec 0.0796157, recall 0.888692
2017-12-10T12:26:33.900651: step 4047, loss 0.0108844, acc 1, prec 0.0796366, recall 0.88872
2017-12-10T12:26:34.345634: step 4048, loss 3.00277, acc 0.9375, prec 0.079652, recall 0.888523
2017-12-10T12:26:34.790586: step 4049, loss 0.785567, acc 0.984375, prec 0.079692, recall 0.888579
2017-12-10T12:26:35.224332: step 4050, loss 0.055885, acc 0.96875, prec 0.0797302, recall 0.888636
2017-12-10T12:26:35.666797: step 4051, loss 0.312045, acc 0.9375, prec 0.079723, recall 0.888636
2017-12-10T12:26:36.103294: step 4052, loss 0.355147, acc 0.90625, prec 0.0797539, recall 0.888692
2017-12-10T12:26:36.542449: step 4053, loss 0.182772, acc 0.9375, prec 0.0797467, recall 0.888692
2017-12-10T12:26:36.986426: step 4054, loss 0.300019, acc 0.96875, prec 0.079743, recall 0.888692
2017-12-10T12:26:37.430170: step 4055, loss 0.133245, acc 0.9375, prec 0.0797358, recall 0.888692
2017-12-10T12:26:37.866879: step 4056, loss 0.235529, acc 0.953125, prec 0.0797304, recall 0.888692
2017-12-10T12:26:38.315286: step 4057, loss 0.262034, acc 0.90625, prec 0.0797613, recall 0.888748
2017-12-10T12:26:38.760000: step 4058, loss 0.171887, acc 0.921875, prec 0.0797731, recall 0.888777
2017-12-10T12:26:39.215000: step 4059, loss 0.451424, acc 0.890625, prec 0.0797604, recall 0.888777
2017-12-10T12:26:39.666260: step 4060, loss 0.374744, acc 0.875, prec 0.0798086, recall 0.888861
2017-12-10T12:26:40.093941: step 4061, loss 0.608512, acc 0.921875, prec 0.0798413, recall 0.888917
2017-12-10T12:26:40.533870: step 4062, loss 0.357111, acc 0.875, prec 0.0798268, recall 0.888917
2017-12-10T12:26:40.985651: step 4063, loss 0.387584, acc 0.9375, prec 0.0798196, recall 0.888917
2017-12-10T12:26:41.419505: step 4064, loss 0.303465, acc 0.90625, prec 0.0798504, recall 0.888973
2017-12-10T12:26:41.866880: step 4065, loss 0.58207, acc 0.890625, prec 0.0798377, recall 0.888973
2017-12-10T12:26:42.310410: step 4066, loss 0.35166, acc 0.875, prec 0.0798441, recall 0.889001
2017-12-10T12:26:42.754582: step 4067, loss 0.372441, acc 0.921875, prec 0.0798351, recall 0.889001
2017-12-10T12:26:43.200431: step 4068, loss 0.522941, acc 0.875, prec 0.0798414, recall 0.889029
2017-12-10T12:26:43.633978: step 4069, loss 0.14684, acc 0.953125, prec 0.079836, recall 0.889029
2017-12-10T12:26:44.087434: step 4070, loss 0.0630176, acc 0.953125, prec 0.0798514, recall 0.889057
2017-12-10T12:26:44.531204: step 4071, loss 0.402175, acc 0.9375, prec 0.0799067, recall 0.889141
2017-12-10T12:26:44.978353: step 4072, loss 0.219401, acc 0.890625, prec 0.0798941, recall 0.889141
2017-12-10T12:26:45.433472: step 4073, loss 0.0282675, acc 1, prec 0.0799149, recall 0.889169
2017-12-10T12:26:45.881610: step 4074, loss 0.482082, acc 0.9375, prec 0.0799285, recall 0.889197
2017-12-10T12:26:46.328983: step 4075, loss 0.0508361, acc 0.984375, prec 0.0799475, recall 0.889225
2017-12-10T12:26:46.778365: step 4076, loss 0.277422, acc 0.953125, prec 0.0799837, recall 0.88928
2017-12-10T12:26:47.234600: step 4077, loss 0.36325, acc 0.90625, prec 0.0799937, recall 0.889308
2017-12-10T12:26:47.669292: step 4078, loss 0.0941756, acc 0.984375, prec 0.0800127, recall 0.889336
2017-12-10T12:26:48.122029: step 4079, loss 0.320736, acc 0.921875, prec 0.0800244, recall 0.889364
2017-12-10T12:26:48.563089: step 4080, loss 0.259848, acc 0.875, prec 0.0800308, recall 0.889392
2017-12-10T12:26:49.009010: step 4081, loss 0.0152464, acc 1, prec 0.0800308, recall 0.889392
2017-12-10T12:26:49.450930: step 4082, loss 0.148613, acc 0.953125, prec 0.0800253, recall 0.889392
2017-12-10T12:26:49.888254: step 4083, loss 0.19063, acc 0.953125, prec 0.0800615, recall 0.889447
2017-12-10T12:26:50.331307: step 4084, loss 0.128523, acc 0.984375, prec 0.0800597, recall 0.889447
2017-12-10T12:26:50.780181: step 4085, loss 0.138976, acc 0.96875, prec 0.0800769, recall 0.889475
2017-12-10T12:26:51.219441: step 4086, loss 0.0146133, acc 1, prec 0.0800977, recall 0.889503
2017-12-10T12:26:51.658256: step 4087, loss 0.121066, acc 0.96875, prec 0.0801149, recall 0.88953
2017-12-10T12:26:52.110189: step 4088, loss 0.238052, acc 0.984375, prec 0.0801547, recall 0.889586
2017-12-10T12:26:52.555998: step 4089, loss 0.00926487, acc 1, prec 0.0801755, recall 0.889614
2017-12-10T12:26:52.999186: step 4090, loss 0.020287, acc 1, prec 0.0801963, recall 0.889641
2017-12-10T12:26:53.448725: step 4091, loss 0.0379106, acc 0.96875, prec 0.0801926, recall 0.889641
2017-12-10T12:26:53.900186: step 4092, loss 0.0226873, acc 0.984375, prec 0.0801908, recall 0.889641
2017-12-10T12:26:54.341963: step 4093, loss 0.398643, acc 0.96875, prec 0.0802496, recall 0.889724
2017-12-10T12:26:54.784316: step 4094, loss 0.451079, acc 0.953125, prec 0.0802649, recall 0.889752
2017-12-10T12:26:55.226935: step 4095, loss 0.377483, acc 0.984375, prec 0.0802839, recall 0.88978
2017-12-10T12:26:55.673521: step 4096, loss 0.0038616, acc 1, prec 0.0803047, recall 0.889807
2017-12-10T12:26:56.131268: step 4097, loss 0.0685361, acc 0.96875, prec 0.0803218, recall 0.889835
2017-12-10T12:26:56.577985: step 4098, loss 0.00540155, acc 1, prec 0.0803218, recall 0.889835
2017-12-10T12:26:57.025657: step 4099, loss 0.181135, acc 0.96875, prec 0.080339, recall 0.889862
2017-12-10T12:26:57.467439: step 4100, loss 0.169491, acc 0.953125, prec 0.0803751, recall 0.889917
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4100

2017-12-10T12:26:59.311323: step 4101, loss 0.107171, acc 0.953125, prec 0.0804112, recall 0.889973
2017-12-10T12:26:59.751734: step 4102, loss 0.0776758, acc 0.984375, prec 0.0804094, recall 0.889973
2017-12-10T12:27:00.204277: step 4103, loss 0.110747, acc 0.953125, prec 0.0804039, recall 0.889973
2017-12-10T12:27:00.640193: step 4104, loss 0.0888928, acc 0.96875, prec 0.0804003, recall 0.889973
2017-12-10T12:27:01.083785: step 4105, loss 0.118233, acc 0.953125, prec 0.0804156, recall 0.89
2017-12-10T12:27:01.531760: step 4106, loss 0.212027, acc 1, prec 0.0804572, recall 0.890055
2017-12-10T12:27:01.980584: step 4107, loss 0.136205, acc 0.96875, prec 0.0804951, recall 0.89011
2017-12-10T12:27:02.435862: step 4108, loss 0.421042, acc 0.984375, prec 0.0805556, recall 0.890192
2017-12-10T12:27:02.895034: step 4109, loss 0.180861, acc 0.984375, prec 0.0805953, recall 0.890247
2017-12-10T12:27:03.346028: step 4110, loss 0.379566, acc 0.953125, prec 0.0805898, recall 0.890247
2017-12-10T12:27:03.791341: step 4111, loss 0.193954, acc 0.953125, prec 0.0805843, recall 0.890247
2017-12-10T12:27:04.246849: step 4112, loss 0.0478192, acc 0.984375, prec 0.0805825, recall 0.890247
2017-12-10T12:27:04.691476: step 4113, loss 0.659255, acc 0.96875, prec 0.0806204, recall 0.890302
2017-12-10T12:27:05.134299: step 4114, loss 0.209201, acc 1, prec 0.0806412, recall 0.890329
2017-12-10T12:27:05.571394: step 4115, loss 0.111536, acc 0.96875, prec 0.0806583, recall 0.890356
2017-12-10T12:27:06.018883: step 4116, loss 0.414479, acc 0.90625, prec 0.0806473, recall 0.890356
2017-12-10T12:27:06.468039: step 4117, loss 0.113522, acc 0.953125, prec 0.0806834, recall 0.890411
2017-12-10T12:27:06.923870: step 4118, loss 0.153666, acc 0.96875, prec 0.0807212, recall 0.890465
2017-12-10T12:27:07.370771: step 4119, loss 0.167192, acc 0.953125, prec 0.0807158, recall 0.890465
2017-12-10T12:27:07.809597: step 4120, loss 0.663315, acc 0.875, prec 0.0807219, recall 0.890493
2017-12-10T12:27:08.251152: step 4121, loss 0.0775325, acc 0.96875, prec 0.0807598, recall 0.890547
2017-12-10T12:27:08.686432: step 4122, loss 0.215943, acc 0.953125, prec 0.0807543, recall 0.890547
2017-12-10T12:27:09.143418: step 4123, loss 0.092619, acc 0.984375, prec 0.0808147, recall 0.890629
2017-12-10T12:27:09.597379: step 4124, loss 0.119413, acc 0.96875, prec 0.0808318, recall 0.890656
2017-12-10T12:27:10.048685: step 4125, loss 0.319962, acc 0.90625, prec 0.0808623, recall 0.89071
2017-12-10T12:27:10.496938: step 4126, loss 0.132963, acc 0.96875, prec 0.0808586, recall 0.89071
2017-12-10T12:27:10.931690: step 4127, loss 0.448519, acc 0.875, prec 0.0808648, recall 0.890738
2017-12-10T12:27:11.378037: step 4128, loss 0.113181, acc 0.953125, prec 0.0809008, recall 0.890792
2017-12-10T12:27:11.813969: step 4129, loss 0.373507, acc 0.9375, prec 0.0809349, recall 0.890846
2017-12-10T12:27:12.263074: step 4130, loss 0.159882, acc 0.9375, prec 0.0809483, recall 0.890873
2017-12-10T12:27:12.699774: step 4131, loss 0.235677, acc 0.953125, prec 0.0809428, recall 0.890873
2017-12-10T12:27:13.150897: step 4132, loss 0.216597, acc 0.96875, prec 0.0809599, recall 0.8909
2017-12-10T12:27:13.601279: step 4133, loss 0.361805, acc 0.953125, prec 0.0809751, recall 0.890927
2017-12-10T12:27:14.037404: step 4134, loss 0.235754, acc 0.921875, prec 0.080966, recall 0.890927
2017-12-10T12:27:14.477930: step 4135, loss 0.255186, acc 0.9375, prec 0.0809587, recall 0.890927
2017-12-10T12:27:14.920840: step 4136, loss 0.124905, acc 0.953125, prec 0.0809532, recall 0.890927
2017-12-10T12:27:15.368795: step 4137, loss 0.527482, acc 0.953125, prec 0.0810099, recall 0.891008
2017-12-10T12:27:15.812166: step 4138, loss 0.0466074, acc 0.984375, prec 0.081008, recall 0.891008
2017-12-10T12:27:16.258962: step 4139, loss 0.0338897, acc 0.984375, prec 0.0810062, recall 0.891008
2017-12-10T12:27:16.710818: step 4140, loss 0.153203, acc 0.9375, prec 0.0809989, recall 0.891008
2017-12-10T12:27:17.157622: step 4141, loss 0.342853, acc 0.96875, prec 0.0810573, recall 0.891089
2017-12-10T12:27:17.601890: step 4142, loss 0.108412, acc 0.96875, prec 0.0810537, recall 0.891089
2017-12-10T12:27:18.047571: step 4143, loss 0.0394189, acc 1, prec 0.0810744, recall 0.891116
2017-12-10T12:27:18.488075: step 4144, loss 0.145551, acc 0.96875, prec 0.0810707, recall 0.891116
2017-12-10T12:27:18.935538: step 4145, loss 0.0691414, acc 0.984375, prec 0.0810896, recall 0.891143
2017-12-10T12:27:19.381977: step 4146, loss 0.902289, acc 1, prec 0.081131, recall 0.891197
2017-12-10T12:27:19.834460: step 4147, loss 0.121338, acc 0.953125, prec 0.0811462, recall 0.891224
2017-12-10T12:27:20.270433: step 4148, loss 0.303327, acc 0.984375, prec 0.081165, recall 0.891251
2017-12-10T12:27:20.732066: step 4149, loss 0.160742, acc 0.953125, prec 0.0811802, recall 0.891277
2017-12-10T12:27:21.171825: step 4150, loss 0.127925, acc 0.953125, prec 0.0811747, recall 0.891277
2017-12-10T12:27:21.625737: step 4151, loss 0.130466, acc 0.984375, prec 0.0811936, recall 0.891304
2017-12-10T12:27:22.053621: step 4152, loss 0.129912, acc 0.96875, prec 0.0811899, recall 0.891304
2017-12-10T12:27:22.499311: step 4153, loss 0.508664, acc 0.90625, prec 0.0811997, recall 0.891331
2017-12-10T12:27:22.948331: step 4154, loss 0.182115, acc 0.9375, prec 0.0812337, recall 0.891385
2017-12-10T12:27:23.385605: step 4155, loss 0.125105, acc 0.96875, prec 0.0812714, recall 0.891438
2017-12-10T12:27:23.834928: step 4156, loss 0.378101, acc 0.90625, prec 0.0812811, recall 0.891465
2017-12-10T12:27:24.273115: step 4157, loss 0.312088, acc 0.953125, prec 0.0812962, recall 0.891492
2017-12-10T12:27:24.715630: step 4158, loss 0.036367, acc 1, prec 0.0813169, recall 0.891519
2017-12-10T12:27:25.179779: step 4159, loss 0.12009, acc 0.96875, prec 0.0813339, recall 0.891545
2017-12-10T12:27:25.628111: step 4160, loss 0.311719, acc 0.953125, prec 0.0813491, recall 0.891572
2017-12-10T12:27:26.076893: step 4161, loss 0.10038, acc 0.96875, prec 0.0813661, recall 0.891599
2017-12-10T12:27:26.526280: step 4162, loss 0.230572, acc 0.9375, prec 0.0813588, recall 0.891599
2017-12-10T12:27:26.976461: step 4163, loss 0.287199, acc 0.9375, prec 0.0813514, recall 0.891599
2017-12-10T12:27:27.424614: step 4164, loss 0.173769, acc 0.953125, prec 0.0813666, recall 0.891626
2017-12-10T12:27:27.868669: step 4165, loss 0.401081, acc 0.953125, prec 0.0814231, recall 0.891706
2017-12-10T12:27:28.314319: step 4166, loss 0.264102, acc 0.96875, prec 0.0814813, recall 0.891786
2017-12-10T12:27:28.758899: step 4167, loss 0.1426, acc 0.96875, prec 0.0815189, recall 0.891839
2017-12-10T12:27:29.202335: step 4168, loss 0.0664564, acc 1, prec 0.0815602, recall 0.891892
2017-12-10T12:27:29.652503: step 4169, loss 0.123475, acc 0.9375, prec 0.0815529, recall 0.891892
2017-12-10T12:27:30.093947: step 4170, loss 0.182529, acc 0.90625, prec 0.0815625, recall 0.891918
2017-12-10T12:27:30.544102: step 4171, loss 0.0197722, acc 1, prec 0.0815625, recall 0.891918
2017-12-10T12:27:30.972949: step 4172, loss 0.0628099, acc 0.96875, prec 0.0815795, recall 0.891945
2017-12-10T12:27:31.419453: step 4173, loss 0.0812627, acc 0.984375, prec 0.0816189, recall 0.891998
2017-12-10T12:27:31.863151: step 4174, loss 0.360427, acc 0.984375, prec 0.0816583, recall 0.892051
2017-12-10T12:27:32.333346: step 4175, loss 0.138573, acc 0.984375, prec 0.0816565, recall 0.892051
2017-12-10T12:27:32.793433: step 4176, loss 0.067023, acc 0.984375, prec 0.0816753, recall 0.892078
2017-12-10T12:27:33.234676: step 4177, loss 0.236208, acc 0.9375, prec 0.0816886, recall 0.892104
2017-12-10T12:27:33.694079: step 4178, loss 0.0867475, acc 0.984375, prec 0.0816867, recall 0.892104
2017-12-10T12:27:34.147591: step 4179, loss 0.0432319, acc 1, prec 0.0817073, recall 0.89213
2017-12-10T12:27:34.584564: step 4180, loss 0.0718355, acc 0.96875, prec 0.0817037, recall 0.89213
2017-12-10T12:27:35.044490: step 4181, loss 0.325233, acc 0.984375, prec 0.0817431, recall 0.892183
2017-12-10T12:27:35.486356: step 4182, loss 0.00880682, acc 1, prec 0.0817431, recall 0.892183
2017-12-10T12:27:35.927859: step 4183, loss 0.0412336, acc 0.984375, prec 0.0817412, recall 0.892183
2017-12-10T12:27:36.377636: step 4184, loss 0.0699804, acc 0.984375, prec 0.08176, recall 0.89221
2017-12-10T12:27:36.824912: step 4185, loss 0.0910939, acc 0.984375, prec 0.0817994, recall 0.892262
2017-12-10T12:27:37.276313: step 4186, loss 0.0926916, acc 0.96875, prec 0.0817957, recall 0.892262
2017-12-10T12:27:37.726863: step 4187, loss 3.30471, acc 0.921875, prec 0.0817884, recall 0.892044
2017-12-10T12:27:38.164144: step 4188, loss 0.147402, acc 0.984375, prec 0.0818072, recall 0.89207
2017-12-10T12:27:38.627809: step 4189, loss 0.439378, acc 1, prec 0.081869, recall 0.89215
2017-12-10T12:27:39.074029: step 4190, loss 0.146049, acc 0.953125, prec 0.0818635, recall 0.89215
2017-12-10T12:27:39.513542: step 4191, loss 0.149571, acc 0.9375, prec 0.0818561, recall 0.89215
2017-12-10T12:27:39.957977: step 4192, loss 0.148072, acc 0.953125, prec 0.0818918, recall 0.892202
2017-12-10T12:27:40.409071: step 4193, loss 0.307434, acc 0.90625, prec 0.0819014, recall 0.892229
2017-12-10T12:27:40.851670: step 4194, loss 0.287805, acc 0.890625, prec 0.0818885, recall 0.892229
2017-12-10T12:27:41.296506: step 4195, loss 0.235358, acc 0.90625, prec 0.0819393, recall 0.892308
2017-12-10T12:27:41.737863: step 4196, loss 0.121114, acc 0.984375, prec 0.081958, recall 0.892334
2017-12-10T12:27:42.179699: step 4197, loss 0.440163, acc 0.859375, prec 0.0819415, recall 0.892334
2017-12-10T12:27:42.622206: step 4198, loss 0.595196, acc 0.890625, prec 0.0819698, recall 0.892387
2017-12-10T12:27:43.065798: step 4199, loss 0.213608, acc 0.9375, prec 0.0819624, recall 0.892387
2017-12-10T12:27:43.508533: step 4200, loss 0.267124, acc 0.890625, prec 0.0819496, recall 0.892387
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4200

2017-12-10T12:27:45.280981: step 4201, loss 0.387056, acc 0.84375, prec 0.0819312, recall 0.892387
2017-12-10T12:27:45.724657: step 4202, loss 0.642882, acc 0.859375, prec 0.0819353, recall 0.892413
2017-12-10T12:27:46.173623: step 4203, loss 0.852808, acc 0.875, prec 0.0819412, recall 0.892439
2017-12-10T12:27:46.622009: step 4204, loss 0.330356, acc 0.90625, prec 0.0819507, recall 0.892465
2017-12-10T12:27:47.060986: step 4205, loss 0.30133, acc 0.921875, prec 0.0819415, recall 0.892465
2017-12-10T12:27:47.507217: step 4206, loss 0.412474, acc 0.90625, prec 0.0819511, recall 0.892491
2017-12-10T12:27:47.955296: step 4207, loss 0.465714, acc 0.875, prec 0.0819364, recall 0.892491
2017-12-10T12:27:48.389074: step 4208, loss 0.190804, acc 0.921875, prec 0.0819272, recall 0.892491
2017-12-10T12:27:48.828461: step 4209, loss 0.0626212, acc 0.984375, prec 0.0819665, recall 0.892544
2017-12-10T12:27:49.282894: step 4210, loss 0.351605, acc 0.9375, prec 0.0819797, recall 0.89257
2017-12-10T12:27:49.726709: step 4211, loss 0.10202, acc 0.96875, prec 0.081976, recall 0.89257
2017-12-10T12:27:50.164971: step 4212, loss 0.0932901, acc 0.96875, prec 0.0819723, recall 0.89257
2017-12-10T12:27:50.615105: step 4213, loss 0.184491, acc 0.96875, prec 0.0819892, recall 0.892596
2017-12-10T12:27:51.057076: step 4214, loss 0.0998989, acc 0.96875, prec 0.0820061, recall 0.892622
2017-12-10T12:27:51.496516: step 4215, loss 0.0315187, acc 1, prec 0.0820061, recall 0.892622
2017-12-10T12:27:51.943736: step 4216, loss 0.228661, acc 0.96875, prec 0.082023, recall 0.892649
2017-12-10T12:27:52.395239: step 4217, loss 0.0672837, acc 1, prec 0.0820435, recall 0.892675
2017-12-10T12:27:52.846358: step 4218, loss 0.001776, acc 1, prec 0.0820435, recall 0.892675
2017-12-10T12:27:53.290042: step 4219, loss 0.956425, acc 1, prec 0.0821051, recall 0.892753
2017-12-10T12:27:53.743620: step 4220, loss 0.0459308, acc 0.984375, prec 0.0821238, recall 0.892779
2017-12-10T12:27:54.181372: step 4221, loss 0.00859595, acc 1, prec 0.0821238, recall 0.892779
2017-12-10T12:27:54.628670: step 4222, loss 0.172258, acc 0.921875, prec 0.0821146, recall 0.892779
2017-12-10T12:27:55.060457: step 4223, loss 0.277222, acc 0.96875, prec 0.0821109, recall 0.892779
2017-12-10T12:27:55.505013: step 4224, loss 0.0734069, acc 0.984375, prec 0.0821296, recall 0.892805
2017-12-10T12:27:55.944959: step 4225, loss 0.115269, acc 0.953125, prec 0.0821651, recall 0.892857
2017-12-10T12:27:56.394680: step 4226, loss 0.708811, acc 1, prec 0.0822267, recall 0.892935
2017-12-10T12:27:56.844360: step 4227, loss 0.449421, acc 0.953125, prec 0.0822827, recall 0.893013
2017-12-10T12:27:57.289336: step 4228, loss 0.0399585, acc 0.984375, prec 0.0822809, recall 0.893013
2017-12-10T12:27:57.726644: step 4229, loss 0.142228, acc 0.9375, prec 0.082294, recall 0.893039
2017-12-10T12:27:58.168601: step 4230, loss 0.301155, acc 0.9375, prec 0.0823277, recall 0.893091
2017-12-10T12:27:58.620199: step 4231, loss 0.193947, acc 0.9375, prec 0.0823409, recall 0.893117
2017-12-10T12:27:59.064645: step 4232, loss 0.421323, acc 0.953125, prec 0.0823763, recall 0.893169
2017-12-10T12:27:59.515426: step 4233, loss 0.167086, acc 0.953125, prec 0.0823913, recall 0.893194
2017-12-10T12:27:59.957662: step 4234, loss 0.0556381, acc 0.96875, prec 0.0823876, recall 0.893194
2017-12-10T12:28:00.400484: step 4235, loss 0.623478, acc 0.90625, prec 0.0823766, recall 0.893194
2017-12-10T12:28:00.856077: step 4236, loss 0.0908665, acc 0.96875, prec 0.0823934, recall 0.89322
2017-12-10T12:28:01.305500: step 4237, loss 0.515127, acc 0.90625, prec 0.0824029, recall 0.893246
2017-12-10T12:28:01.747091: step 4238, loss 0.124517, acc 0.96875, prec 0.0823992, recall 0.893246
2017-12-10T12:28:02.185827: step 4239, loss 0.197959, acc 0.9375, prec 0.0824328, recall 0.893298
2017-12-10T12:28:02.636727: step 4240, loss 0.0456109, acc 0.984375, prec 0.0824514, recall 0.893324
2017-12-10T12:28:03.081464: step 4241, loss 0.160418, acc 0.90625, prec 0.0824404, recall 0.893324
2017-12-10T12:28:03.521672: step 4242, loss 0.0910448, acc 0.96875, prec 0.0824367, recall 0.893324
2017-12-10T12:28:03.963989: step 4243, loss 0.0410215, acc 0.984375, prec 0.0824349, recall 0.893324
2017-12-10T12:28:04.408475: step 4244, loss 0.310233, acc 0.9375, prec 0.082448, recall 0.893349
2017-12-10T12:28:04.842560: step 4245, loss 0.374283, acc 0.9375, prec 0.0824611, recall 0.893375
2017-12-10T12:28:05.289586: step 4246, loss 0.121984, acc 0.96875, prec 0.0824574, recall 0.893375
2017-12-10T12:28:05.741233: step 4247, loss 0.596605, acc 0.953125, prec 0.0824929, recall 0.893427
2017-12-10T12:28:06.190038: step 4248, loss 0.366145, acc 0.9375, prec 0.082506, recall 0.893453
2017-12-10T12:28:06.627845: step 4249, loss 0.239722, acc 0.953125, prec 0.0825004, recall 0.893453
2017-12-10T12:28:07.071605: step 4250, loss 0.213407, acc 0.953125, prec 0.0825563, recall 0.89353
2017-12-10T12:28:07.519959: step 4251, loss 3.48851, acc 0.96875, prec 0.0825954, recall 0.893366
2017-12-10T12:28:07.971724: step 4252, loss 0.0308842, acc 1, prec 0.0826159, recall 0.893391
2017-12-10T12:28:08.410202: step 4253, loss 0.13317, acc 0.953125, prec 0.0826103, recall 0.893391
2017-12-10T12:28:08.854755: step 4254, loss 0.103601, acc 0.96875, prec 0.0826271, recall 0.893417
2017-12-10T12:28:09.296508: step 4255, loss 0.169484, acc 0.921875, prec 0.0826384, recall 0.893443
2017-12-10T12:28:09.734685: step 4256, loss 0.0699246, acc 1, prec 0.0826793, recall 0.893494
2017-12-10T12:28:10.181142: step 4257, loss 0.240364, acc 0.953125, prec 0.0826737, recall 0.893494
2017-12-10T12:28:10.641737: step 4258, loss 0.162127, acc 0.953125, prec 0.0826682, recall 0.893494
2017-12-10T12:28:11.095824: step 4259, loss 0.174293, acc 0.921875, prec 0.082659, recall 0.893494
2017-12-10T12:28:11.545092: step 4260, loss 0.356889, acc 0.9375, prec 0.0826516, recall 0.893494
2017-12-10T12:28:12.007225: step 4261, loss 0.290618, acc 0.921875, prec 0.0827038, recall 0.893571
2017-12-10T12:28:12.430790: step 4262, loss 0.548006, acc 0.859375, prec 0.0827485, recall 0.893648
2017-12-10T12:28:12.883470: step 4263, loss 0.115487, acc 0.953125, prec 0.0827634, recall 0.893673
2017-12-10T12:28:13.320902: step 4264, loss 0.370931, acc 0.890625, prec 0.0827505, recall 0.893673
2017-12-10T12:28:13.763030: step 4265, loss 0.246909, acc 0.9375, prec 0.0827431, recall 0.893673
2017-12-10T12:28:14.206337: step 4266, loss 0.157556, acc 0.921875, prec 0.0827339, recall 0.893673
2017-12-10T12:28:14.650184: step 4267, loss 0.446565, acc 0.875, prec 0.0827396, recall 0.893699
2017-12-10T12:28:15.099459: step 4268, loss 0.0413288, acc 0.984375, prec 0.0827377, recall 0.893699
2017-12-10T12:28:15.545385: step 4269, loss 0.288689, acc 0.921875, prec 0.0827898, recall 0.893776
2017-12-10T12:28:15.985537: step 4270, loss 0.151241, acc 0.9375, prec 0.0827824, recall 0.893776
2017-12-10T12:28:16.424127: step 4271, loss 0.077658, acc 0.96875, prec 0.0827787, recall 0.893776
2017-12-10T12:28:16.858240: step 4272, loss 0.116015, acc 0.953125, prec 0.0827936, recall 0.893801
2017-12-10T12:28:17.312380: step 4273, loss 0.973776, acc 0.9375, prec 0.0828475, recall 0.893878
2017-12-10T12:28:17.761636: step 4274, loss 0.0927683, acc 0.96875, prec 0.0828438, recall 0.893878
2017-12-10T12:28:18.201953: step 4275, loss 0.030666, acc 0.984375, prec 0.0828419, recall 0.893878
2017-12-10T12:28:18.645893: step 4276, loss 0.115266, acc 0.96875, prec 0.0828791, recall 0.893928
2017-12-10T12:28:19.087280: step 4277, loss 0.0631541, acc 0.96875, prec 0.0828754, recall 0.893928
2017-12-10T12:28:19.535029: step 4278, loss 0.0214607, acc 0.984375, prec 0.0829348, recall 0.894005
2017-12-10T12:28:19.989101: step 4279, loss 0.458366, acc 0.984375, prec 0.0829737, recall 0.894056
2017-12-10T12:28:20.441697: step 4280, loss 0.211506, acc 0.9375, prec 0.0829867, recall 0.894081
2017-12-10T12:28:20.892695: step 4281, loss 0.119663, acc 0.953125, prec 0.083022, recall 0.894132
2017-12-10T12:28:21.360272: step 4282, loss 0.131621, acc 0.96875, prec 0.0830183, recall 0.894132
2017-12-10T12:28:21.806667: step 4283, loss 0.179057, acc 0.96875, prec 0.083035, recall 0.894157
2017-12-10T12:28:22.250904: step 4284, loss 0.0124228, acc 1, prec 0.0830554, recall 0.894182
2017-12-10T12:28:22.696139: step 4285, loss 0.0878479, acc 0.984375, prec 0.0830739, recall 0.894208
2017-12-10T12:28:23.144237: step 4286, loss 0.0467718, acc 0.984375, prec 0.0830925, recall 0.894233
2017-12-10T12:28:23.593941: step 4287, loss 0.019879, acc 1, prec 0.0830925, recall 0.894233
2017-12-10T12:28:24.022200: step 4288, loss 0.165558, acc 0.96875, prec 0.0831295, recall 0.894284
2017-12-10T12:28:24.471453: step 4289, loss 0.211998, acc 0.96875, prec 0.0831462, recall 0.894309
2017-12-10T12:28:24.918564: step 4290, loss 0.101219, acc 0.9375, prec 0.0831388, recall 0.894309
2017-12-10T12:28:25.371079: step 4291, loss 0.105513, acc 0.984375, prec 0.0831574, recall 0.894334
2017-12-10T12:28:25.815550: step 4292, loss 0.133262, acc 0.96875, prec 0.0831537, recall 0.894334
2017-12-10T12:28:26.256044: step 4293, loss 0.888454, acc 0.984375, prec 0.0832129, recall 0.89441
2017-12-10T12:28:26.695253: step 4294, loss 0.0449469, acc 0.96875, prec 0.0832296, recall 0.894435
2017-12-10T12:28:27.140769: step 4295, loss 0.123548, acc 0.953125, prec 0.0832444, recall 0.89446
2017-12-10T12:28:27.589147: step 4296, loss 0.0337841, acc 1, prec 0.0832852, recall 0.894511
2017-12-10T12:28:28.039814: step 4297, loss 0.0101391, acc 1, prec 0.0832852, recall 0.894511
2017-12-10T12:28:28.482526: step 4298, loss 0.140476, acc 0.953125, prec 0.0833, recall 0.894536
2017-12-10T12:28:28.922472: step 4299, loss 0.113309, acc 0.984375, prec 0.0832982, recall 0.894536
2017-12-10T12:28:29.368287: step 4300, loss 0.16166, acc 0.9375, prec 0.0832907, recall 0.894536
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4300

2017-12-10T12:28:31.404601: step 4301, loss 0.0124614, acc 1, prec 0.0832907, recall 0.894536
2017-12-10T12:28:31.845796: step 4302, loss 0.27559, acc 0.921875, prec 0.0832815, recall 0.894536
2017-12-10T12:28:32.295773: step 4303, loss 0.215659, acc 0.953125, prec 0.0833167, recall 0.894586
2017-12-10T12:28:32.736067: step 4304, loss 0.208741, acc 0.890625, prec 0.0833037, recall 0.894586
2017-12-10T12:28:33.180224: step 4305, loss 0.0949072, acc 0.984375, prec 0.0833426, recall 0.894636
2017-12-10T12:28:33.625072: step 4306, loss 0.149092, acc 0.984375, prec 0.0833407, recall 0.894636
2017-12-10T12:28:34.070563: step 4307, loss 0.186681, acc 0.9375, prec 0.0833537, recall 0.894662
2017-12-10T12:28:34.532862: step 4308, loss 0.133435, acc 0.953125, prec 0.0833685, recall 0.894687
2017-12-10T12:28:34.978235: step 4309, loss 0.145916, acc 0.96875, prec 0.0834055, recall 0.894737
2017-12-10T12:28:35.432588: step 4310, loss 0.123158, acc 0.9375, prec 0.0834184, recall 0.894762
2017-12-10T12:28:35.885387: step 4311, loss 1.72527, acc 0.921875, prec 0.0834314, recall 0.894574
2017-12-10T12:28:36.336830: step 4312, loss 0.201164, acc 0.921875, prec 0.0834221, recall 0.894574
2017-12-10T12:28:36.790925: step 4313, loss 0.0783137, acc 0.953125, prec 0.0834369, recall 0.894599
2017-12-10T12:28:37.242075: step 4314, loss 0.0621257, acc 0.953125, prec 0.0834313, recall 0.894599
2017-12-10T12:28:37.686595: step 4315, loss 0.26414, acc 0.90625, prec 0.0834406, recall 0.894624
2017-12-10T12:28:38.128404: step 4316, loss 0.393265, acc 0.90625, prec 0.0834498, recall 0.894649
2017-12-10T12:28:38.569208: step 4317, loss 0.128324, acc 0.96875, prec 0.0834664, recall 0.894674
2017-12-10T12:28:39.017547: step 4318, loss 0.597961, acc 0.9375, prec 0.0834793, recall 0.894699
2017-12-10T12:28:39.465819: step 4319, loss 0.342806, acc 0.90625, prec 0.0834682, recall 0.894699
2017-12-10T12:28:39.915915: step 4320, loss 0.343527, acc 0.890625, prec 0.0834959, recall 0.894749
2017-12-10T12:28:40.370806: step 4321, loss 0.139486, acc 0.953125, prec 0.083531, recall 0.894799
2017-12-10T12:28:40.813830: step 4322, loss 0.14289, acc 0.96875, prec 0.0835476, recall 0.894824
2017-12-10T12:28:41.266438: step 4323, loss 0.354938, acc 0.890625, prec 0.0835347, recall 0.894824
2017-12-10T12:28:41.713626: step 4324, loss 0.342789, acc 0.90625, prec 0.0835439, recall 0.894849
2017-12-10T12:28:42.167491: step 4325, loss 0.201473, acc 0.953125, prec 0.0835383, recall 0.894849
2017-12-10T12:28:42.628294: step 4326, loss 0.209532, acc 0.90625, prec 0.0835272, recall 0.894849
2017-12-10T12:28:43.073651: step 4327, loss 0.279937, acc 0.953125, prec 0.0835419, recall 0.894874
2017-12-10T12:28:43.512804: step 4328, loss 0.171965, acc 0.921875, prec 0.0835327, recall 0.894874
2017-12-10T12:28:43.959964: step 4329, loss 0.380684, acc 0.9375, prec 0.0836065, recall 0.894974
2017-12-10T12:28:44.406034: step 4330, loss 0.174939, acc 0.953125, prec 0.0836212, recall 0.894999
2017-12-10T12:28:44.858730: step 4331, loss 0.23121, acc 0.953125, prec 0.0836765, recall 0.895073
2017-12-10T12:28:45.297339: step 4332, loss 0.161916, acc 0.96875, prec 0.0836931, recall 0.895098
2017-12-10T12:28:45.733996: step 4333, loss 0.328125, acc 0.953125, prec 0.0837079, recall 0.895123
2017-12-10T12:28:46.173063: step 4334, loss 0.0610135, acc 0.96875, prec 0.0837244, recall 0.895148
2017-12-10T12:28:46.625090: step 4335, loss 0.107874, acc 0.96875, prec 0.0837207, recall 0.895148
2017-12-10T12:28:47.067211: step 4336, loss 0.21527, acc 0.953125, prec 0.0837354, recall 0.895173
2017-12-10T12:28:47.521123: step 4337, loss 0.0935428, acc 0.953125, prec 0.0837502, recall 0.895198
2017-12-10T12:28:47.970305: step 4338, loss 0.218353, acc 0.9375, prec 0.0838036, recall 0.895272
2017-12-10T12:28:48.425999: step 4339, loss 0.439989, acc 0.9375, prec 0.0838773, recall 0.895371
2017-12-10T12:28:48.877180: step 4340, loss 0.0498832, acc 1, prec 0.0839178, recall 0.89542
2017-12-10T12:28:49.315798: step 4341, loss 0.143484, acc 1, prec 0.0839381, recall 0.895445
2017-12-10T12:28:49.772089: step 4342, loss 0.162559, acc 0.984375, prec 0.0839362, recall 0.895445
2017-12-10T12:28:50.224758: step 4343, loss 0.0414593, acc 0.984375, prec 0.0839343, recall 0.895445
2017-12-10T12:28:50.675893: step 4344, loss 0.150459, acc 0.96875, prec 0.0839306, recall 0.895445
2017-12-10T12:28:51.126695: step 4345, loss 0.0931085, acc 0.984375, prec 0.0839288, recall 0.895445
2017-12-10T12:28:51.582276: step 4346, loss 0.199487, acc 1, prec 0.0840098, recall 0.895544
2017-12-10T12:28:52.023091: step 4347, loss 0.0184278, acc 0.984375, prec 0.084008, recall 0.895544
2017-12-10T12:28:52.466205: step 4348, loss 0.00504229, acc 1, prec 0.084008, recall 0.895544
2017-12-10T12:28:52.919256: step 4349, loss 0.0865199, acc 0.96875, prec 0.0840245, recall 0.895568
2017-12-10T12:28:53.364738: step 4350, loss 0.405155, acc 0.9375, prec 0.0840373, recall 0.895593
2017-12-10T12:28:53.811588: step 4351, loss 0.135714, acc 0.984375, prec 0.0840355, recall 0.895593
2017-12-10T12:28:54.272091: step 4352, loss 0.0256233, acc 0.984375, prec 0.0840336, recall 0.895593
2017-12-10T12:28:54.724549: step 4353, loss 0.123272, acc 0.984375, prec 0.0840723, recall 0.895642
2017-12-10T12:28:55.174172: step 4354, loss 0.280852, acc 0.96875, prec 0.0840888, recall 0.895666
2017-12-10T12:28:55.629740: step 4355, loss 0.00479695, acc 1, prec 0.0840888, recall 0.895666
2017-12-10T12:28:56.077978: step 4356, loss 0.00304986, acc 1, prec 0.0841091, recall 0.895691
2017-12-10T12:28:56.529702: step 4357, loss 0.00382272, acc 1, prec 0.0841293, recall 0.895716
2017-12-10T12:28:56.973476: step 4358, loss 0.161395, acc 0.953125, prec 0.084144, recall 0.89574
2017-12-10T12:28:57.417641: step 4359, loss 0.294399, acc 0.984375, prec 0.0841826, recall 0.895789
2017-12-10T12:28:57.858872: step 4360, loss 0.0477124, acc 0.984375, prec 0.0841807, recall 0.895789
2017-12-10T12:28:58.306099: step 4361, loss 0.280454, acc 0.953125, prec 0.0841752, recall 0.895789
2017-12-10T12:28:58.754168: step 4362, loss 0.136989, acc 0.96875, prec 0.0841714, recall 0.895789
2017-12-10T12:28:59.203883: step 4363, loss 0.130931, acc 0.953125, prec 0.0841659, recall 0.895789
2017-12-10T12:28:59.642854: step 4364, loss 0.0840045, acc 0.984375, prec 0.0841842, recall 0.895814
2017-12-10T12:29:00.087506: step 4365, loss 0.198178, acc 0.96875, prec 0.0842008, recall 0.895838
2017-12-10T12:29:00.530274: step 4366, loss 0.097724, acc 0.984375, prec 0.0842191, recall 0.895863
2017-12-10T12:29:00.972014: step 4367, loss 0.0136111, acc 1, prec 0.0842191, recall 0.895863
2017-12-10T12:29:01.404770: step 4368, loss 5.5138, acc 0.90625, prec 0.0842301, recall 0.895677
2017-12-10T12:29:01.844550: step 4369, loss 3.44148, acc 0.9375, prec 0.0842447, recall 0.895491
2017-12-10T12:29:02.301145: step 4370, loss 0.112919, acc 0.953125, prec 0.0842594, recall 0.895515
2017-12-10T12:29:02.735283: step 4371, loss 0.139141, acc 0.953125, prec 0.0842538, recall 0.895515
2017-12-10T12:29:03.188455: step 4372, loss 0.455977, acc 0.953125, prec 0.0842684, recall 0.89554
2017-12-10T12:29:03.628605: step 4373, loss 0.136792, acc 0.953125, prec 0.0842628, recall 0.89554
2017-12-10T12:29:04.084268: step 4374, loss 0.324743, acc 0.875, prec 0.0842884, recall 0.895589
2017-12-10T12:29:04.540281: step 4375, loss 0.47346, acc 0.890625, prec 0.0843158, recall 0.895638
2017-12-10T12:29:04.985533: step 4376, loss 0.606336, acc 0.78125, prec 0.08431, recall 0.895662
2017-12-10T12:29:05.432712: step 4377, loss 0.833782, acc 0.875, prec 0.0843153, recall 0.895687
2017-12-10T12:29:05.883480: step 4378, loss 0.57009, acc 0.859375, prec 0.0842985, recall 0.895687
2017-12-10T12:29:06.332699: step 4379, loss 0.679268, acc 0.78125, prec 0.0842927, recall 0.895711
2017-12-10T12:29:06.778229: step 4380, loss 0.612574, acc 0.828125, prec 0.0842723, recall 0.895711
2017-12-10T12:29:07.220655: step 4381, loss 0.654704, acc 0.796875, prec 0.0842481, recall 0.895711
2017-12-10T12:29:07.665808: step 4382, loss 0.7492, acc 0.890625, prec 0.0842755, recall 0.89576
2017-12-10T12:29:08.111310: step 4383, loss 0.591501, acc 0.859375, prec 0.0842588, recall 0.89576
2017-12-10T12:29:08.565078: step 4384, loss 0.550406, acc 0.796875, prec 0.0842548, recall 0.895785
2017-12-10T12:29:09.016817: step 4385, loss 0.296684, acc 0.875, prec 0.08424, recall 0.895785
2017-12-10T12:29:09.474245: step 4386, loss 0.399379, acc 0.9375, prec 0.0842527, recall 0.895809
2017-12-10T12:29:09.919420: step 4387, loss 0.225598, acc 0.9375, prec 0.0842856, recall 0.895858
2017-12-10T12:29:10.362010: step 4388, loss 0.35487, acc 0.890625, prec 0.0842928, recall 0.895882
2017-12-10T12:29:10.811346: step 4389, loss 0.273896, acc 0.90625, prec 0.0842817, recall 0.895882
2017-12-10T12:29:11.254774: step 4390, loss 0.48295, acc 0.90625, prec 0.0843108, recall 0.895931
2017-12-10T12:29:11.696813: step 4391, loss 0.192366, acc 0.90625, prec 0.0842997, recall 0.895931
2017-12-10T12:29:12.141375: step 4392, loss 0.203862, acc 0.953125, prec 0.0842941, recall 0.895931
2017-12-10T12:29:12.584081: step 4393, loss 0.206527, acc 0.9375, prec 0.084327, recall 0.895979
2017-12-10T12:29:13.029191: step 4394, loss 0.198522, acc 0.921875, prec 0.0843177, recall 0.895979
2017-12-10T12:29:13.476887: step 4395, loss 0.716402, acc 0.9375, prec 0.0843506, recall 0.896028
2017-12-10T12:29:13.927996: step 4396, loss 0.117437, acc 0.9375, prec 0.0843633, recall 0.896052
2017-12-10T12:29:14.384343: step 4397, loss 0.334296, acc 0.921875, prec 0.0843943, recall 0.896101
2017-12-10T12:29:14.817534: step 4398, loss 0.0764301, acc 0.953125, prec 0.0844089, recall 0.896125
2017-12-10T12:29:15.266441: step 4399, loss 0.0937039, acc 0.953125, prec 0.0844234, recall 0.896149
2017-12-10T12:29:15.712794: step 4400, loss 0.0952184, acc 0.984375, prec 0.0844216, recall 0.896149
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4400

2017-12-10T12:29:17.724327: step 4401, loss 0.0193871, acc 1, prec 0.0844216, recall 0.896149
2017-12-10T12:29:18.161244: step 4402, loss 0.0287526, acc 0.984375, prec 0.0844197, recall 0.896149
2017-12-10T12:29:18.604952: step 4403, loss 0.150121, acc 0.984375, prec 0.0844179, recall 0.896149
2017-12-10T12:29:19.061055: step 4404, loss 0.12573, acc 0.96875, prec 0.0844343, recall 0.896174
2017-12-10T12:29:19.521195: step 4405, loss 0.0174523, acc 1, prec 0.0844544, recall 0.896198
2017-12-10T12:29:19.958683: step 4406, loss 0.547111, acc 0.96875, prec 0.0844507, recall 0.896198
2017-12-10T12:29:20.391630: step 4407, loss 1.99274, acc 0.984375, prec 0.0844708, recall 0.896013
2017-12-10T12:29:20.840839: step 4408, loss 0.25241, acc 0.9375, prec 0.0844835, recall 0.896037
2017-12-10T12:29:21.288867: step 4409, loss 0.0123765, acc 1, prec 0.0845036, recall 0.896062
2017-12-10T12:29:21.734086: step 4410, loss 0.00825026, acc 1, prec 0.0845036, recall 0.896062
2017-12-10T12:29:22.182230: step 4411, loss 0.851172, acc 1, prec 0.0845439, recall 0.89611
2017-12-10T12:29:22.646848: step 4412, loss 0.266831, acc 0.984375, prec 0.084542, recall 0.89611
2017-12-10T12:29:23.094120: step 4413, loss 0.0661284, acc 0.984375, prec 0.0845402, recall 0.89611
2017-12-10T12:29:23.549460: step 4414, loss 0.200095, acc 0.953125, prec 0.0845748, recall 0.896158
2017-12-10T12:29:23.996455: step 4415, loss 0.0279635, acc 0.984375, prec 0.084573, recall 0.896158
2017-12-10T12:29:24.445929: step 4416, loss 0.152095, acc 0.96875, prec 0.0845692, recall 0.896158
2017-12-10T12:29:24.901672: step 4417, loss 0.0701979, acc 0.96875, prec 0.0845856, recall 0.896182
2017-12-10T12:29:25.337234: step 4418, loss 0.056448, acc 0.953125, prec 0.0846203, recall 0.896231
2017-12-10T12:29:25.778656: step 4419, loss 0.170606, acc 0.9375, prec 0.0846128, recall 0.896231
2017-12-10T12:29:26.220033: step 4420, loss 1.84874, acc 0.9375, prec 0.0846073, recall 0.896022
2017-12-10T12:29:26.663577: step 4421, loss 0.313747, acc 0.9375, prec 0.0845998, recall 0.896022
2017-12-10T12:29:27.117605: step 4422, loss 0.125548, acc 0.953125, prec 0.0846345, recall 0.896071
2017-12-10T12:29:27.560193: step 4423, loss 0.266072, acc 0.9375, prec 0.084627, recall 0.896071
2017-12-10T12:29:28.009342: step 4424, loss 0.205889, acc 0.921875, prec 0.0846378, recall 0.896095
2017-12-10T12:29:28.459888: step 4425, loss 0.260885, acc 0.921875, prec 0.0846487, recall 0.896119
2017-12-10T12:29:28.916110: step 4426, loss 0.589585, acc 0.859375, prec 0.0846319, recall 0.896119
2017-12-10T12:29:29.360285: step 4427, loss 0.279643, acc 0.859375, prec 0.0846152, recall 0.896119
2017-12-10T12:29:29.806611: step 4428, loss 0.228033, acc 0.921875, prec 0.0846059, recall 0.896119
2017-12-10T12:29:30.275552: step 4429, loss 0.209995, acc 0.921875, prec 0.0846368, recall 0.896167
2017-12-10T12:29:30.718916: step 4430, loss 0.68782, acc 0.875, prec 0.0846822, recall 0.89624
2017-12-10T12:29:31.181330: step 4431, loss 0.0374136, acc 0.96875, prec 0.0846986, recall 0.896264
2017-12-10T12:29:31.623348: step 4432, loss 0.623974, acc 0.859375, prec 0.0847019, recall 0.896288
2017-12-10T12:29:32.059384: step 4433, loss 0.484882, acc 0.90625, prec 0.0847108, recall 0.896312
2017-12-10T12:29:32.503981: step 4434, loss 0.407762, acc 0.90625, prec 0.0846997, recall 0.896312
2017-12-10T12:29:32.945148: step 4435, loss 0.240701, acc 0.875, prec 0.0846848, recall 0.896312
2017-12-10T12:29:33.394466: step 4436, loss 0.132687, acc 0.953125, prec 0.0846993, recall 0.896336
2017-12-10T12:29:33.839602: step 4437, loss 0.258694, acc 0.90625, prec 0.0847083, recall 0.89636
2017-12-10T12:29:34.289451: step 4438, loss 0.45246, acc 0.859375, prec 0.0846916, recall 0.89636
2017-12-10T12:29:34.723233: step 4439, loss 0.275887, acc 0.9375, prec 0.0847042, recall 0.896384
2017-12-10T12:29:35.153640: step 4440, loss 0.219308, acc 0.96875, prec 0.0847205, recall 0.896408
2017-12-10T12:29:35.604009: step 4441, loss 0.304764, acc 0.984375, prec 0.0847387, recall 0.896432
2017-12-10T12:29:36.046532: step 4442, loss 0.239825, acc 0.921875, prec 0.0847294, recall 0.896432
2017-12-10T12:29:36.483201: step 4443, loss 0.152721, acc 0.953125, prec 0.084764, recall 0.89648
2017-12-10T12:29:36.927637: step 4444, loss 0.083152, acc 0.96875, prec 0.0847803, recall 0.896504
2017-12-10T12:29:37.367096: step 4445, loss 0.260291, acc 0.921875, prec 0.084791, recall 0.896528
2017-12-10T12:29:37.818472: step 4446, loss 0.124757, acc 0.953125, prec 0.0848255, recall 0.896576
2017-12-10T12:29:38.271462: step 4447, loss 0.0611175, acc 0.984375, prec 0.0848437, recall 0.8966
2017-12-10T12:29:38.716503: step 4448, loss 0.036295, acc 0.984375, prec 0.0848619, recall 0.896623
2017-12-10T12:29:39.161977: step 4449, loss 0.0669464, acc 0.96875, prec 0.0848582, recall 0.896623
2017-12-10T12:29:39.619850: step 4450, loss 0.217373, acc 0.953125, prec 0.0848726, recall 0.896647
2017-12-10T12:29:40.070209: step 4451, loss 0.641121, acc 0.953125, prec 0.0849071, recall 0.896695
2017-12-10T12:29:40.517075: step 4452, loss 0.301156, acc 0.96875, prec 0.0849234, recall 0.896719
2017-12-10T12:29:40.963493: step 4453, loss 0.13687, acc 0.96875, prec 0.0849397, recall 0.896743
2017-12-10T12:29:41.404265: step 4454, loss 0.0561154, acc 0.96875, prec 0.084976, recall 0.896791
2017-12-10T12:29:41.850996: step 4455, loss 0.0679748, acc 0.96875, prec 0.0849723, recall 0.896791
2017-12-10T12:29:42.301891: step 4456, loss 0.363138, acc 0.921875, prec 0.0849831, recall 0.896814
2017-12-10T12:29:42.741522: step 4457, loss 5.4475, acc 0.921875, prec 0.0850357, recall 0.896679
2017-12-10T12:29:43.188961: step 4458, loss 0.0578452, acc 1, prec 0.0850757, recall 0.896727
2017-12-10T12:29:43.627560: step 4459, loss 0.280824, acc 0.921875, prec 0.0850664, recall 0.896727
2017-12-10T12:29:44.083111: step 4460, loss 0.0552668, acc 0.96875, prec 0.0851027, recall 0.896774
2017-12-10T12:29:44.538013: step 4461, loss 0.20703, acc 0.9375, prec 0.0851352, recall 0.896822
2017-12-10T12:29:44.992181: step 4462, loss 0.141208, acc 0.96875, prec 0.0851315, recall 0.896822
2017-12-10T12:29:45.430972: step 4463, loss 0.0961831, acc 0.9375, prec 0.0851241, recall 0.896822
2017-12-10T12:29:45.871577: step 4464, loss 0.342841, acc 0.859375, prec 0.0851273, recall 0.896846
2017-12-10T12:29:46.303418: step 4465, loss 0.218777, acc 0.9375, prec 0.0851798, recall 0.896917
2017-12-10T12:29:46.751404: step 4466, loss 0.294532, acc 0.890625, prec 0.0851868, recall 0.89694
2017-12-10T12:29:47.182718: step 4467, loss 0.27727, acc 0.90625, prec 0.0851956, recall 0.896964
2017-12-10T12:29:47.616812: step 4468, loss 0.379601, acc 0.890625, prec 0.0852226, recall 0.897012
2017-12-10T12:29:48.064565: step 4469, loss 0.637667, acc 0.828125, prec 0.0852221, recall 0.897035
2017-12-10T12:29:48.506470: step 4470, loss 0.454437, acc 0.890625, prec 0.085209, recall 0.897035
2017-12-10T12:29:48.964774: step 4471, loss 0.165988, acc 0.9375, prec 0.0852216, recall 0.897059
2017-12-10T12:29:49.423514: step 4472, loss 0.305018, acc 0.921875, prec 0.0852322, recall 0.897082
2017-12-10T12:29:49.820536: step 4473, loss 0.247938, acc 0.921569, prec 0.0852448, recall 0.897106
2017-12-10T12:29:50.289740: step 4474, loss 0.332666, acc 0.90625, prec 0.0852336, recall 0.897106
2017-12-10T12:29:50.734863: step 4475, loss 0.216968, acc 0.9375, prec 0.085306, recall 0.897201
2017-12-10T12:29:51.191780: step 4476, loss 0.220206, acc 0.9375, prec 0.0853385, recall 0.897248
2017-12-10T12:29:51.638091: step 4477, loss 0.358122, acc 0.9375, prec 0.085351, recall 0.897271
2017-12-10T12:29:52.076569: step 4478, loss 0.237663, acc 0.9375, prec 0.0853635, recall 0.897295
2017-12-10T12:29:52.517583: step 4479, loss 0.0514131, acc 1, prec 0.0854034, recall 0.897342
2017-12-10T12:29:52.968842: step 4480, loss 0.314434, acc 0.9375, prec 0.0853959, recall 0.897342
2017-12-10T12:29:53.412199: step 4481, loss 0.0783588, acc 0.96875, prec 0.0854321, recall 0.897389
2017-12-10T12:29:53.857755: step 4482, loss 0.210278, acc 0.953125, prec 0.0854265, recall 0.897389
2017-12-10T12:29:54.302821: step 4483, loss 0.419224, acc 0.984375, prec 0.0854446, recall 0.897412
2017-12-10T12:29:54.758624: step 4484, loss 0.0826654, acc 0.953125, prec 0.085439, recall 0.897412
2017-12-10T12:29:55.208077: step 4485, loss 0.127522, acc 0.953125, prec 0.0854533, recall 0.897436
2017-12-10T12:29:55.661033: step 4486, loss 0.275096, acc 0.953125, prec 0.0854876, recall 0.897483
2017-12-10T12:29:56.110470: step 4487, loss 0.185464, acc 0.9375, prec 0.0854801, recall 0.897483
2017-12-10T12:29:56.559984: step 4488, loss 0.636617, acc 0.9375, prec 0.0855125, recall 0.89753
2017-12-10T12:29:57.007262: step 4489, loss 0.235139, acc 0.953125, prec 0.0855269, recall 0.897553
2017-12-10T12:29:57.460305: step 4490, loss 0.0341146, acc 0.984375, prec 0.085525, recall 0.897553
2017-12-10T12:29:57.900632: step 4491, loss 0.0271893, acc 1, prec 0.085545, recall 0.897577
2017-12-10T12:29:58.352926: step 4492, loss 0.264062, acc 0.921875, prec 0.0855755, recall 0.897623
2017-12-10T12:29:58.792911: step 4493, loss 0.110462, acc 0.953125, prec 0.0855699, recall 0.897623
2017-12-10T12:29:59.234929: step 4494, loss 0.0659742, acc 0.96875, prec 0.085606, recall 0.89767
2017-12-10T12:29:59.676749: step 4495, loss 0.0284294, acc 0.984375, prec 0.0856041, recall 0.89767
2017-12-10T12:30:00.117764: step 4496, loss 0.0796404, acc 0.984375, prec 0.0856023, recall 0.89767
2017-12-10T12:30:00.551798: step 4497, loss 0.0855818, acc 0.984375, prec 0.0856004, recall 0.89767
2017-12-10T12:30:00.994348: step 4498, loss 0.123281, acc 0.9375, prec 0.0855929, recall 0.89767
2017-12-10T12:30:01.448073: step 4499, loss 0.127895, acc 0.96875, prec 0.085629, recall 0.897717
2017-12-10T12:30:01.889342: step 4500, loss 0.0276495, acc 1, prec 0.085629, recall 0.897717
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4500

2017-12-10T12:30:03.787876: step 4501, loss 0.131437, acc 0.96875, prec 0.085705, recall 0.89781
2017-12-10T12:30:04.241035: step 4502, loss 0.0538764, acc 0.984375, prec 0.0857429, recall 0.897857
2017-12-10T12:30:04.686699: step 4503, loss 0.134181, acc 0.953125, prec 0.0857373, recall 0.897857
2017-12-10T12:30:05.137128: step 4504, loss 0.10525, acc 0.953125, prec 0.0857317, recall 0.897857
2017-12-10T12:30:05.579194: step 4505, loss 0.169138, acc 0.9375, prec 0.085764, recall 0.897903
2017-12-10T12:30:06.026855: step 4506, loss 0.0293709, acc 1, prec 0.0857839, recall 0.897927
2017-12-10T12:30:06.493966: step 4507, loss 0.0864579, acc 0.984375, prec 0.0857821, recall 0.897927
2017-12-10T12:30:06.943765: step 4508, loss 0.0312029, acc 1, prec 0.085802, recall 0.89795
2017-12-10T12:30:07.399069: step 4509, loss 0.0860873, acc 0.984375, prec 0.08582, recall 0.897973
2017-12-10T12:30:07.815373: step 4510, loss 0.0427573, acc 0.984375, prec 0.0858181, recall 0.897973
2017-12-10T12:30:08.269543: step 4511, loss 0.0862693, acc 0.953125, prec 0.0858125, recall 0.897973
2017-12-10T12:30:08.704354: step 4512, loss 0.123856, acc 0.96875, prec 0.0858088, recall 0.897973
2017-12-10T12:30:09.150684: step 4513, loss 2.69727, acc 0.96875, prec 0.0858268, recall 0.897792
2017-12-10T12:30:09.602332: step 4514, loss 0.124648, acc 0.9375, prec 0.0858591, recall 0.897838
2017-12-10T12:30:10.051051: step 4515, loss 0.132928, acc 0.96875, prec 0.0858554, recall 0.897838
2017-12-10T12:30:10.483948: step 4516, loss 0.168223, acc 0.9375, prec 0.0858678, recall 0.897862
2017-12-10T12:30:10.932778: step 4517, loss 0.106552, acc 0.953125, prec 0.085902, recall 0.897908
2017-12-10T12:30:11.377479: step 4518, loss 0.250046, acc 0.953125, prec 0.0859163, recall 0.897931
2017-12-10T12:30:11.826973: step 4519, loss 0.110145, acc 0.921875, prec 0.0859069, recall 0.897931
2017-12-10T12:30:12.275129: step 4520, loss 0.0990104, acc 0.96875, prec 0.0859429, recall 0.897978
2017-12-10T12:30:12.723564: step 4521, loss 0.16622, acc 0.9375, prec 0.0859752, recall 0.898024
2017-12-10T12:30:13.187234: step 4522, loss 0.23348, acc 0.90625, prec 0.0860037, recall 0.89807
2017-12-10T12:30:13.633109: step 4523, loss 0.170233, acc 0.96875, prec 0.0860199, recall 0.898094
2017-12-10T12:30:14.076376: step 4524, loss 0.139291, acc 0.9375, prec 0.0860124, recall 0.898094
2017-12-10T12:30:14.522523: step 4525, loss 0.29788, acc 0.859375, prec 0.0859956, recall 0.898094
2017-12-10T12:30:14.965093: step 4526, loss 0.290775, acc 0.921875, prec 0.0860061, recall 0.898117
2017-12-10T12:30:15.404348: step 4527, loss 0.32733, acc 0.9375, prec 0.0860383, recall 0.898163
2017-12-10T12:30:15.856005: step 4528, loss 0.186921, acc 0.96875, prec 0.0860544, recall 0.898186
2017-12-10T12:30:16.303207: step 4529, loss 0.0699592, acc 0.96875, prec 0.0860507, recall 0.898186
2017-12-10T12:30:16.752283: step 4530, loss 0.157367, acc 0.984375, prec 0.0860687, recall 0.898209
2017-12-10T12:30:17.199455: step 4531, loss 0.253231, acc 0.9375, prec 0.0860811, recall 0.898232
2017-12-10T12:30:17.648271: step 4532, loss 0.087665, acc 0.953125, prec 0.0860953, recall 0.898255
2017-12-10T12:30:18.110422: step 4533, loss 0.390183, acc 0.890625, prec 0.0861219, recall 0.898301
2017-12-10T12:30:18.566373: step 4534, loss 0.0321195, acc 0.984375, prec 0.08612, recall 0.898301
2017-12-10T12:30:18.999160: step 4535, loss 0.110978, acc 0.9375, prec 0.0861126, recall 0.898301
2017-12-10T12:30:19.432435: step 4536, loss 0.0460815, acc 0.984375, prec 0.0861107, recall 0.898301
2017-12-10T12:30:19.878905: step 4537, loss 0.425731, acc 0.984375, prec 0.0861485, recall 0.898347
2017-12-10T12:30:20.323322: step 4538, loss 0.156205, acc 0.96875, prec 0.0861646, recall 0.89837
2017-12-10T12:30:20.767034: step 4539, loss 0.166575, acc 0.9375, prec 0.0862166, recall 0.898439
2017-12-10T12:30:21.217514: step 4540, loss 0.153588, acc 0.953125, prec 0.0862308, recall 0.898462
2017-12-10T12:30:21.663976: step 4541, loss 0.108691, acc 0.984375, prec 0.086229, recall 0.898462
2017-12-10T12:30:22.113215: step 4542, loss 0.0905534, acc 0.984375, prec 0.0862271, recall 0.898462
2017-12-10T12:30:22.556911: step 4543, loss 0.163769, acc 0.9375, prec 0.0862989, recall 0.898554
2017-12-10T12:30:23.010090: step 4544, loss 0.102357, acc 0.953125, prec 0.0862933, recall 0.898554
2017-12-10T12:30:23.449980: step 4545, loss 0.142589, acc 0.96875, prec 0.0863094, recall 0.898577
2017-12-10T12:30:23.897545: step 4546, loss 0.0291747, acc 0.984375, prec 0.0863472, recall 0.898623
2017-12-10T12:30:24.345588: step 4547, loss 0.32431, acc 0.984375, prec 0.0863651, recall 0.898646
2017-12-10T12:30:24.791529: step 4548, loss 0.481192, acc 0.96875, prec 0.086401, recall 0.898691
2017-12-10T12:30:25.241683: step 4549, loss 0.179623, acc 0.96875, prec 0.0864171, recall 0.898714
2017-12-10T12:30:25.694821: step 4550, loss 0.149943, acc 0.984375, prec 0.086435, recall 0.898737
2017-12-10T12:30:26.145261: step 4551, loss 0.465218, acc 0.984375, prec 0.086453, recall 0.89876
2017-12-10T12:30:26.587623: step 4552, loss 0.300045, acc 0.96875, prec 0.0864888, recall 0.898805
2017-12-10T12:30:27.027534: step 4553, loss 5.34703, acc 0.984375, prec 0.0864888, recall 0.898603
2017-12-10T12:30:27.485072: step 4554, loss 0.639475, acc 0.96875, prec 0.0865049, recall 0.898626
2017-12-10T12:30:27.934209: step 4555, loss 0.158016, acc 0.953125, prec 0.0865389, recall 0.898671
2017-12-10T12:30:28.391899: step 4556, loss 0.336985, acc 0.9375, prec 0.086571, recall 0.898717
2017-12-10T12:30:28.836677: step 4557, loss 0.194152, acc 0.953125, prec 0.0865654, recall 0.898717
2017-12-10T12:30:29.284494: step 4558, loss 0.564261, acc 0.953125, prec 0.0865795, recall 0.89874
2017-12-10T12:30:29.719733: step 4559, loss 0.182883, acc 0.96875, prec 0.0865758, recall 0.89874
2017-12-10T12:30:30.154224: step 4560, loss 0.10971, acc 0.9375, prec 0.0865683, recall 0.89874
2017-12-10T12:30:30.593139: step 4561, loss 0.339192, acc 0.890625, prec 0.0865551, recall 0.89874
2017-12-10T12:30:31.042333: step 4562, loss 0.0937861, acc 0.96875, prec 0.0865712, recall 0.898763
2017-12-10T12:30:31.485085: step 4563, loss 0.493836, acc 0.953125, prec 0.0865655, recall 0.898763
2017-12-10T12:30:31.940633: step 4564, loss 0.195854, acc 0.921875, prec 0.086576, recall 0.898785
2017-12-10T12:30:32.380600: step 4565, loss 0.309527, acc 0.890625, prec 0.0865826, recall 0.898808
2017-12-10T12:30:32.834849: step 4566, loss 0.421007, acc 0.828125, prec 0.0866213, recall 0.898876
2017-12-10T12:30:33.285044: step 4567, loss 0.37057, acc 0.828125, prec 0.0866403, recall 0.898922
2017-12-10T12:30:33.737053: step 4568, loss 0.711145, acc 0.890625, prec 0.0866667, recall 0.898967
2017-12-10T12:30:34.181192: step 4569, loss 0.181403, acc 0.9375, prec 0.0866592, recall 0.898967
2017-12-10T12:30:34.625459: step 4570, loss 0.0466521, acc 0.96875, prec 0.0866554, recall 0.898967
2017-12-10T12:30:35.071529: step 4571, loss 0.225742, acc 0.890625, prec 0.0866423, recall 0.898967
2017-12-10T12:30:35.511355: step 4572, loss 0.373025, acc 0.90625, prec 0.0866903, recall 0.899035
2017-12-10T12:30:35.959925: step 4573, loss 0.107512, acc 0.96875, prec 0.0867063, recall 0.899058
2017-12-10T12:30:36.409979: step 4574, loss 0.206724, acc 0.9375, prec 0.0867186, recall 0.899081
2017-12-10T12:30:36.856891: step 4575, loss 0.115588, acc 0.984375, prec 0.0867167, recall 0.899081
2017-12-10T12:30:37.304541: step 4576, loss 0.385152, acc 0.921875, prec 0.0867271, recall 0.899103
2017-12-10T12:30:37.755246: step 4577, loss 0.936511, acc 0.96875, prec 0.0868221, recall 0.899216
2017-12-10T12:30:38.201964: step 4578, loss 0.411737, acc 0.921875, prec 0.0868324, recall 0.899239
2017-12-10T12:30:38.648492: step 4579, loss 0.0429685, acc 0.96875, prec 0.0868287, recall 0.899239
2017-12-10T12:30:39.105789: step 4580, loss 0.439397, acc 0.953125, prec 0.0868428, recall 0.899261
2017-12-10T12:30:39.555550: step 4581, loss 0.196105, acc 0.9375, prec 0.0868748, recall 0.899306
2017-12-10T12:30:40.011684: step 4582, loss 0.138267, acc 0.921875, prec 0.0868851, recall 0.899329
2017-12-10T12:30:40.449441: step 4583, loss 0.144349, acc 0.9375, prec 0.0868776, recall 0.899329
2017-12-10T12:30:40.894927: step 4584, loss 0.185415, acc 0.953125, prec 0.0868917, recall 0.899351
2017-12-10T12:30:41.338414: step 4585, loss 0.063922, acc 0.984375, prec 0.0868898, recall 0.899351
2017-12-10T12:30:41.774726: step 4586, loss 0.277963, acc 0.90625, prec 0.086918, recall 0.899396
2017-12-10T12:30:42.224226: step 4587, loss 0.0721413, acc 0.96875, prec 0.0869142, recall 0.899396
2017-12-10T12:30:42.679350: step 4588, loss 0.0931588, acc 0.96875, prec 0.0869302, recall 0.899419
2017-12-10T12:30:43.134328: step 4589, loss 0.247535, acc 0.96875, prec 0.0869659, recall 0.899464
2017-12-10T12:30:43.574805: step 4590, loss 0.0427986, acc 0.984375, prec 0.086964, recall 0.899464
2017-12-10T12:30:44.007718: step 4591, loss 0.0346527, acc 0.984375, prec 0.0869622, recall 0.899464
2017-12-10T12:30:44.448163: step 4592, loss 0.245681, acc 0.9375, prec 0.0869546, recall 0.899464
2017-12-10T12:30:44.899460: step 4593, loss 0.0991847, acc 0.96875, prec 0.0869509, recall 0.899464
2017-12-10T12:30:45.352770: step 4594, loss 0.0480945, acc 0.984375, prec 0.0870082, recall 0.899531
2017-12-10T12:30:45.805769: step 4595, loss 0.0221829, acc 1, prec 0.0870476, recall 0.899576
2017-12-10T12:30:46.267676: step 4596, loss 0.00597487, acc 1, prec 0.0870476, recall 0.899576
2017-12-10T12:30:46.733578: step 4597, loss 0.0447525, acc 0.96875, prec 0.0870636, recall 0.899598
2017-12-10T12:30:47.171494: step 4598, loss 0.0282164, acc 1, prec 0.0870833, recall 0.899621
2017-12-10T12:30:47.623137: step 4599, loss 0.143456, acc 0.9375, prec 0.0870757, recall 0.899621
2017-12-10T12:30:48.070510: step 4600, loss 0.478712, acc 1, prec 0.0871152, recall 0.899666
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4600

2017-12-10T12:30:49.872847: step 4601, loss 0.143095, acc 0.96875, prec 0.0871311, recall 0.899688
2017-12-10T12:30:50.317001: step 4602, loss 1.01235, acc 0.984375, prec 0.0871883, recall 0.899755
2017-12-10T12:30:50.757405: step 4603, loss 0.127298, acc 0.96875, prec 0.0871846, recall 0.899755
2017-12-10T12:30:51.221189: step 4604, loss 3.39789, acc 0.90625, prec 0.0871752, recall 0.899555
2017-12-10T12:30:51.671838: step 4605, loss 0.21832, acc 0.921875, prec 0.0871658, recall 0.899555
2017-12-10T12:30:52.115462: step 4606, loss 0.017034, acc 1, prec 0.0872052, recall 0.899599
2017-12-10T12:30:52.561888: step 4607, loss 0.0539922, acc 0.984375, prec 0.0872033, recall 0.899599
2017-12-10T12:30:53.009695: step 4608, loss 0.153993, acc 0.953125, prec 0.087237, recall 0.899644
2017-12-10T12:30:53.459354: step 4609, loss 0.316341, acc 0.9375, prec 0.0872295, recall 0.899644
2017-12-10T12:30:53.894115: step 4610, loss 0.291312, acc 0.875, prec 0.0872538, recall 0.899689
2017-12-10T12:30:54.338321: step 4611, loss 0.299125, acc 0.921875, prec 0.0872838, recall 0.899733
2017-12-10T12:30:54.781408: step 4612, loss 0.220952, acc 0.953125, prec 0.0873175, recall 0.899778
2017-12-10T12:30:55.242430: step 4613, loss 0.364589, acc 0.890625, prec 0.087324, recall 0.8998
2017-12-10T12:30:55.696111: step 4614, loss 0.388982, acc 0.890625, prec 0.0873108, recall 0.8998
2017-12-10T12:30:56.151985: step 4615, loss 0.29982, acc 0.921875, prec 0.0873014, recall 0.8998
2017-12-10T12:30:56.596005: step 4616, loss 0.269341, acc 0.859375, prec 0.0872845, recall 0.8998
2017-12-10T12:30:57.053769: step 4617, loss 0.261601, acc 0.890625, prec 0.0872713, recall 0.8998
2017-12-10T12:30:57.497040: step 4618, loss 0.438779, acc 0.9375, prec 0.0872638, recall 0.8998
2017-12-10T12:30:57.938007: step 4619, loss 0.109348, acc 0.953125, prec 0.0872582, recall 0.8998
2017-12-10T12:30:58.379926: step 4620, loss 0.175091, acc 0.953125, prec 0.0872525, recall 0.8998
2017-12-10T12:30:58.828257: step 4621, loss 0.221392, acc 0.921875, prec 0.0872431, recall 0.8998
2017-12-10T12:30:59.273058: step 4622, loss 0.130162, acc 0.9375, prec 0.0872356, recall 0.8998
2017-12-10T12:30:59.711425: step 4623, loss 0.196038, acc 0.9375, prec 0.0872477, recall 0.899822
2017-12-10T12:31:00.150473: step 4624, loss 0.498724, acc 0.921875, prec 0.0872383, recall 0.899822
2017-12-10T12:31:00.605660: step 4625, loss 0.100946, acc 0.9375, prec 0.0872505, recall 0.899845
2017-12-10T12:31:01.053607: step 4626, loss 0.424685, acc 0.9375, prec 0.087243, recall 0.899845
2017-12-10T12:31:01.497847: step 4627, loss 0.0569375, acc 0.984375, prec 0.0872607, recall 0.899867
2017-12-10T12:31:01.942116: step 4628, loss 0.0128473, acc 1, prec 0.0872607, recall 0.899867
2017-12-10T12:31:02.397957: step 4629, loss 0.112791, acc 0.984375, prec 0.0872785, recall 0.899889
2017-12-10T12:31:02.848560: step 4630, loss 1.11253, acc 0.96875, prec 0.0872944, recall 0.899911
2017-12-10T12:31:03.310657: step 4631, loss 0.031112, acc 1, prec 0.0872944, recall 0.899911
2017-12-10T12:31:03.770168: step 4632, loss 0.0400263, acc 0.984375, prec 0.0872925, recall 0.899911
2017-12-10T12:31:04.220702: step 4633, loss 0.0449304, acc 0.984375, prec 0.0872907, recall 0.899911
2017-12-10T12:31:04.664190: step 4634, loss 0.400016, acc 0.90625, prec 0.0872794, recall 0.899911
2017-12-10T12:31:05.109793: step 4635, loss 0.0686434, acc 0.96875, prec 0.0872953, recall 0.899933
2017-12-10T12:31:05.564029: step 4636, loss 0.14697, acc 0.96875, prec 0.0872915, recall 0.899933
2017-12-10T12:31:06.005557: step 4637, loss 0.238252, acc 0.953125, prec 0.0873055, recall 0.899956
2017-12-10T12:31:06.436440: step 4638, loss 0.161674, acc 0.96875, prec 0.0873214, recall 0.899978
2017-12-10T12:31:06.876064: step 4639, loss 0.120528, acc 0.9375, prec 0.0873532, recall 0.900022
2017-12-10T12:31:07.319196: step 4640, loss 0.252024, acc 0.984375, prec 0.0873513, recall 0.900022
2017-12-10T12:31:07.782757: step 4641, loss 0.123713, acc 0.96875, prec 0.0873475, recall 0.900022
2017-12-10T12:31:08.233285: step 4642, loss 0.153409, acc 0.96875, prec 0.0873634, recall 0.900044
2017-12-10T12:31:08.692439: step 4643, loss 0.167609, acc 0.9375, prec 0.0874148, recall 0.900111
2017-12-10T12:31:09.127367: step 4644, loss 0.168069, acc 0.9375, prec 0.0874072, recall 0.900111
2017-12-10T12:31:09.576934: step 4645, loss 0.186062, acc 0.953125, prec 0.0874409, recall 0.900155
2017-12-10T12:31:10.022051: step 4646, loss 0.494752, acc 0.984375, prec 0.0874586, recall 0.900177
2017-12-10T12:31:10.475917: step 4647, loss 0.168286, acc 0.921875, prec 0.0874492, recall 0.900177
2017-12-10T12:31:10.917053: step 4648, loss 0.147052, acc 0.96875, prec 0.0874454, recall 0.900177
2017-12-10T12:31:11.358436: step 4649, loss 0.0945706, acc 0.96875, prec 0.0874417, recall 0.900177
2017-12-10T12:31:11.798952: step 4650, loss 0.333755, acc 0.9375, prec 0.0874342, recall 0.900177
2017-12-10T12:31:12.250323: step 4651, loss 0.200874, acc 0.953125, prec 0.0874481, recall 0.900199
2017-12-10T12:31:12.693015: step 4652, loss 0.0586708, acc 0.984375, prec 0.0874659, recall 0.900221
2017-12-10T12:31:13.134495: step 4653, loss 0.0409869, acc 0.984375, prec 0.087464, recall 0.900221
2017-12-10T12:31:13.572628: step 4654, loss 0.236892, acc 0.9375, prec 0.0874565, recall 0.900221
2017-12-10T12:31:14.013131: step 4655, loss 0.162093, acc 0.9375, prec 0.087449, recall 0.900221
2017-12-10T12:31:14.453187: step 4656, loss 0.0942742, acc 0.96875, prec 0.0874452, recall 0.900221
2017-12-10T12:31:14.903414: step 4657, loss 0.101349, acc 0.96875, prec 0.0874611, recall 0.900243
2017-12-10T12:31:15.350377: step 4658, loss 0.0163363, acc 1, prec 0.0874611, recall 0.900243
2017-12-10T12:31:15.793727: step 4659, loss 0.160976, acc 0.984375, prec 0.0874788, recall 0.900265
2017-12-10T12:31:16.240255: step 4660, loss 0.0956118, acc 0.953125, prec 0.0875124, recall 0.900309
2017-12-10T12:31:16.687274: step 4661, loss 0.046177, acc 0.984375, prec 0.0875105, recall 0.900309
2017-12-10T12:31:17.153267: step 4662, loss 0.102703, acc 0.984375, prec 0.0875282, recall 0.900331
2017-12-10T12:31:17.595852: step 4663, loss 0.069083, acc 0.984375, prec 0.0875459, recall 0.900353
2017-12-10T12:31:18.050785: step 4664, loss 0.0147579, acc 1, prec 0.0875459, recall 0.900353
2017-12-10T12:31:18.503145: step 4665, loss 0.0591791, acc 1, prec 0.0875655, recall 0.900376
2017-12-10T12:31:18.952187: step 4666, loss 0.268817, acc 0.953125, prec 0.0875795, recall 0.900398
2017-12-10T12:31:19.388141: step 4667, loss 0.0365458, acc 0.984375, prec 0.0875776, recall 0.900398
2017-12-10T12:31:19.839879: step 4668, loss 0.0949521, acc 0.984375, prec 0.0875953, recall 0.90042
2017-12-10T12:31:20.274599: step 4669, loss 0.00514477, acc 1, prec 0.0876149, recall 0.900442
2017-12-10T12:31:20.714924: step 4670, loss 0.240566, acc 0.984375, prec 0.087613, recall 0.900442
2017-12-10T12:31:21.158545: step 4671, loss 0.0510738, acc 0.984375, prec 0.0876112, recall 0.900442
2017-12-10T12:31:21.601142: step 4672, loss 0.00650082, acc 1, prec 0.0876112, recall 0.900442
2017-12-10T12:31:22.061744: step 4673, loss 0.0176457, acc 1, prec 0.0876112, recall 0.900442
2017-12-10T12:31:22.511003: step 4674, loss 0.0131097, acc 1, prec 0.0876112, recall 0.900442
2017-12-10T12:31:22.952456: step 4675, loss 0.00510967, acc 1, prec 0.0876112, recall 0.900442
2017-12-10T12:31:23.402752: step 4676, loss 0.342407, acc 0.96875, prec 0.087627, recall 0.900463
2017-12-10T12:31:23.845311: step 4677, loss 0.259638, acc 1, prec 0.0876662, recall 0.900507
2017-12-10T12:31:24.292196: step 4678, loss 0.00277391, acc 1, prec 0.0876858, recall 0.900529
2017-12-10T12:31:24.745107: step 4679, loss 0.0186631, acc 1, prec 0.0877054, recall 0.900551
2017-12-10T12:31:25.202587: step 4680, loss 0.158836, acc 0.984375, prec 0.0877231, recall 0.900573
2017-12-10T12:31:25.674118: step 4681, loss 0.00575388, acc 1, prec 0.0877427, recall 0.900595
2017-12-10T12:31:26.115629: step 4682, loss 0.00463362, acc 1, prec 0.0877427, recall 0.900595
2017-12-10T12:31:26.546052: step 4683, loss 0.00384296, acc 1, prec 0.0877622, recall 0.900617
2017-12-10T12:31:27.001655: step 4684, loss 0.131562, acc 1, prec 0.0878014, recall 0.900661
2017-12-10T12:31:27.456056: step 4685, loss 0.0144251, acc 1, prec 0.087821, recall 0.900683
2017-12-10T12:31:27.905389: step 4686, loss 0.146691, acc 1, prec 0.0878406, recall 0.900705
2017-12-10T12:31:28.348497: step 4687, loss 0.218168, acc 0.984375, prec 0.0878583, recall 0.900726
2017-12-10T12:31:28.786906: step 4688, loss 0.0536988, acc 0.96875, prec 0.0878545, recall 0.900726
2017-12-10T12:31:29.235450: step 4689, loss 7.29072, acc 0.953125, prec 0.0878703, recall 0.90055
2017-12-10T12:31:29.688859: step 4690, loss 0.0592926, acc 0.96875, prec 0.0878666, recall 0.90055
2017-12-10T12:31:30.129203: step 4691, loss 0.0556315, acc 0.984375, prec 0.0879234, recall 0.900616
2017-12-10T12:31:30.582353: step 4692, loss 0.258523, acc 0.953125, prec 0.0879373, recall 0.900638
2017-12-10T12:31:31.023496: step 4693, loss 0.0889312, acc 0.9375, prec 0.0879494, recall 0.900659
2017-12-10T12:31:31.478097: step 4694, loss 0.326766, acc 0.90625, prec 0.087938, recall 0.900659
2017-12-10T12:31:31.918093: step 4695, loss 0.248225, acc 0.90625, prec 0.0879658, recall 0.900703
2017-12-10T12:31:32.362382: step 4696, loss 0.551412, acc 0.921875, prec 0.0880151, recall 0.900768
2017-12-10T12:31:32.805014: step 4697, loss 0.146758, acc 0.9375, prec 0.0880271, recall 0.90079
2017-12-10T12:31:33.243271: step 4698, loss 0.244679, acc 0.90625, prec 0.0880353, recall 0.900812
2017-12-10T12:31:33.681706: step 4699, loss 0.721379, acc 0.859375, prec 0.0880184, recall 0.900812
2017-12-10T12:31:34.128190: step 4700, loss 0.509404, acc 0.859375, prec 0.0880014, recall 0.900812
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4700

2017-12-10T12:31:35.990397: step 4701, loss 0.793676, acc 0.875, prec 0.0880058, recall 0.900834
2017-12-10T12:31:36.426715: step 4702, loss 0.586194, acc 0.859375, prec 0.0880279, recall 0.900877
2017-12-10T12:31:36.875921: step 4703, loss 0.589583, acc 0.828125, prec 0.0880072, recall 0.900877
2017-12-10T12:31:37.347881: step 4704, loss 0.546062, acc 0.890625, prec 0.0880331, recall 0.900921
2017-12-10T12:31:37.786891: step 4705, loss 0.104597, acc 0.953125, prec 0.0880274, recall 0.900921
2017-12-10T12:31:38.238944: step 4706, loss 0.643729, acc 0.921875, prec 0.0880961, recall 0.901007
2017-12-10T12:31:38.685513: step 4707, loss 0.206443, acc 0.90625, prec 0.0880848, recall 0.901007
2017-12-10T12:31:39.146388: step 4708, loss 0.17201, acc 0.96875, prec 0.0881396, recall 0.901072
2017-12-10T12:31:39.582265: step 4709, loss 0.222807, acc 0.9375, prec 0.0881906, recall 0.901137
2017-12-10T12:31:40.031036: step 4710, loss 0.064643, acc 0.96875, prec 0.0881868, recall 0.901137
2017-12-10T12:31:40.475925: step 4711, loss 0.225491, acc 0.953125, prec 0.0882202, recall 0.901181
2017-12-10T12:31:40.932511: step 4712, loss 0.29592, acc 0.90625, prec 0.0882089, recall 0.901181
2017-12-10T12:31:41.375615: step 4713, loss 0.153533, acc 0.890625, prec 0.0881956, recall 0.901181
2017-12-10T12:31:41.814843: step 4714, loss 0.0300784, acc 0.984375, prec 0.0881938, recall 0.901181
2017-12-10T12:31:42.259319: step 4715, loss 0.19007, acc 0.9375, prec 0.0881862, recall 0.901181
2017-12-10T12:31:42.715387: step 4716, loss 0.0690636, acc 0.984375, prec 0.0882038, recall 0.901202
2017-12-10T12:31:43.162244: step 4717, loss 0.0882492, acc 0.953125, prec 0.0882372, recall 0.901245
2017-12-10T12:31:43.596299: step 4718, loss 0.191181, acc 0.96875, prec 0.0882334, recall 0.901245
2017-12-10T12:31:44.033427: step 4719, loss 0.182225, acc 0.96875, prec 0.0882491, recall 0.901267
2017-12-10T12:31:44.481816: step 4720, loss 0.241841, acc 0.96875, prec 0.0882649, recall 0.901289
2017-12-10T12:31:44.940553: step 4721, loss 0.0743371, acc 0.953125, prec 0.0882592, recall 0.901289
2017-12-10T12:31:45.390223: step 4722, loss 0.0687273, acc 0.953125, prec 0.088273, recall 0.90131
2017-12-10T12:31:45.829550: step 4723, loss 1.08849, acc 0.96875, prec 0.0883277, recall 0.901375
2017-12-10T12:31:46.273367: step 4724, loss 0.0275365, acc 0.984375, prec 0.0883259, recall 0.901375
2017-12-10T12:31:46.713471: step 4725, loss 0.156475, acc 0.984375, prec 0.088324, recall 0.901375
2017-12-10T12:31:47.168115: step 4726, loss 0.142439, acc 0.9375, prec 0.0883164, recall 0.901375
2017-12-10T12:31:47.605464: step 4727, loss 0.0770762, acc 0.96875, prec 0.0883126, recall 0.901375
2017-12-10T12:31:48.044179: step 4728, loss 0.0458242, acc 0.984375, prec 0.0883302, recall 0.901396
2017-12-10T12:31:48.484374: step 4729, loss 0.228742, acc 0.953125, prec 0.0883246, recall 0.901396
2017-12-10T12:31:48.931081: step 4730, loss 0.0454666, acc 1, prec 0.0883441, recall 0.901418
2017-12-10T12:31:49.374100: step 4731, loss 0.0100278, acc 1, prec 0.0883635, recall 0.901439
2017-12-10T12:31:49.815679: step 4732, loss 0.226363, acc 0.953125, prec 0.0883579, recall 0.901439
2017-12-10T12:31:50.254699: step 4733, loss 0.275409, acc 0.984375, prec 0.0883755, recall 0.901461
2017-12-10T12:31:50.694119: step 4734, loss 0.0292023, acc 0.984375, prec 0.0884125, recall 0.901504
2017-12-10T12:31:51.149164: step 4735, loss 0.5017, acc 0.96875, prec 0.0884283, recall 0.901525
2017-12-10T12:31:51.610661: step 4736, loss 0.0503094, acc 0.96875, prec 0.0884245, recall 0.901525
2017-12-10T12:31:52.058804: step 4737, loss 0.14271, acc 0.9375, prec 0.0884559, recall 0.901568
2017-12-10T12:31:52.515210: step 4738, loss 0.0642166, acc 0.96875, prec 0.088491, recall 0.901611
2017-12-10T12:31:52.965704: step 4739, loss 0.117384, acc 0.96875, prec 0.0885262, recall 0.901654
2017-12-10T12:31:53.409442: step 4740, loss 0.0847352, acc 0.984375, prec 0.0885243, recall 0.901654
2017-12-10T12:31:53.873891: step 4741, loss 0.0244778, acc 1, prec 0.0885243, recall 0.901654
2017-12-10T12:31:54.327665: step 4742, loss 0.168148, acc 0.953125, prec 0.0885186, recall 0.901654
2017-12-10T12:31:54.778859: step 4743, loss 0.0917176, acc 0.96875, prec 0.0885149, recall 0.901654
2017-12-10T12:31:55.222149: step 4744, loss 0.0723643, acc 1, prec 0.0885538, recall 0.901696
2017-12-10T12:31:55.679815: step 4745, loss 0.434809, acc 0.96875, prec 0.08855, recall 0.901696
2017-12-10T12:31:56.126750: step 4746, loss 0.148693, acc 0.96875, prec 0.0885657, recall 0.901718
2017-12-10T12:31:56.579341: step 4747, loss 0.015433, acc 1, prec 0.0885852, recall 0.901739
2017-12-10T12:31:57.016832: step 4748, loss 0.221644, acc 0.984375, prec 0.0886222, recall 0.901782
2017-12-10T12:31:57.466007: step 4749, loss 0.0174457, acc 1, prec 0.0886222, recall 0.901782
2017-12-10T12:31:57.916788: step 4750, loss 0.149687, acc 0.984375, prec 0.0886592, recall 0.901824
2017-12-10T12:31:58.348370: step 4751, loss 0.138223, acc 0.96875, prec 0.0886554, recall 0.901824
2017-12-10T12:31:58.800624: step 4752, loss 3.14715, acc 0.953125, prec 0.0886516, recall 0.901629
2017-12-10T12:31:59.245391: step 4753, loss 0.0637881, acc 0.96875, prec 0.0886479, recall 0.901629
2017-12-10T12:31:59.694314: step 4754, loss 0.755351, acc 0.921875, prec 0.0886384, recall 0.901629
2017-12-10T12:32:00.155354: step 4755, loss 0.156971, acc 0.96875, prec 0.0886346, recall 0.901629
2017-12-10T12:32:00.620398: step 4756, loss 0.0835881, acc 0.96875, prec 0.0886308, recall 0.901629
2017-12-10T12:32:01.060530: step 4757, loss 0.0939864, acc 0.984375, prec 0.0886678, recall 0.901671
2017-12-10T12:32:01.487945: step 4758, loss 0.282794, acc 0.9375, prec 0.0886603, recall 0.901671
2017-12-10T12:32:01.951445: step 4759, loss 0.539067, acc 0.90625, prec 0.0886489, recall 0.901671
2017-12-10T12:32:02.397465: step 4760, loss 0.208523, acc 0.9375, prec 0.0886414, recall 0.901671
2017-12-10T12:32:02.858176: step 4761, loss 0.410768, acc 0.953125, prec 0.0886551, recall 0.901693
2017-12-10T12:32:03.303395: step 4762, loss 0.329366, acc 0.921875, prec 0.0886457, recall 0.901693
2017-12-10T12:32:03.753082: step 4763, loss 0.161454, acc 0.90625, prec 0.0886538, recall 0.901714
2017-12-10T12:32:04.206956: step 4764, loss 0.201956, acc 0.921875, prec 0.0886637, recall 0.901735
2017-12-10T12:32:04.649784: step 4765, loss 0.154649, acc 0.953125, prec 0.0886969, recall 0.901778
2017-12-10T12:32:05.099923: step 4766, loss 0.255196, acc 0.921875, prec 0.0887069, recall 0.901799
2017-12-10T12:32:05.545640: step 4767, loss 0.722681, acc 0.890625, prec 0.0887325, recall 0.901842
2017-12-10T12:32:05.995160: step 4768, loss 0.303587, acc 0.859375, prec 0.0887155, recall 0.901842
2017-12-10T12:32:06.428646: step 4769, loss 0.209871, acc 0.953125, prec 0.0887293, recall 0.901863
2017-12-10T12:32:06.879003: step 4770, loss 0.249959, acc 0.921875, prec 0.0887392, recall 0.901884
2017-12-10T12:32:07.309275: step 4771, loss 0.185501, acc 0.9375, prec 0.0887899, recall 0.901948
2017-12-10T12:32:07.745830: step 4772, loss 0.225108, acc 0.890625, prec 0.0887767, recall 0.901948
2017-12-10T12:32:08.196512: step 4773, loss 0.532335, acc 0.9375, prec 0.0887885, recall 0.901969
2017-12-10T12:32:08.638170: step 4774, loss 0.246149, acc 0.90625, prec 0.0887772, recall 0.901969
2017-12-10T12:32:09.080287: step 4775, loss 0.137238, acc 0.96875, prec 0.0888316, recall 0.902033
2017-12-10T12:32:09.545015: step 4776, loss 0.326536, acc 0.921875, prec 0.0888416, recall 0.902054
2017-12-10T12:32:09.989888: step 4777, loss 0.22694, acc 0.953125, prec 0.0888747, recall 0.902096
2017-12-10T12:32:10.430997: step 4778, loss 0.173694, acc 0.953125, prec 0.088869, recall 0.902096
2017-12-10T12:32:10.863971: step 4779, loss 0.474746, acc 0.921875, prec 0.088879, recall 0.902118
2017-12-10T12:32:11.312192: step 4780, loss 0.0635052, acc 0.96875, prec 0.0888752, recall 0.902118
2017-12-10T12:32:11.759552: step 4781, loss 0.189702, acc 0.90625, prec 0.0888638, recall 0.902118
2017-12-10T12:32:12.200824: step 4782, loss 0.0739042, acc 0.984375, prec 0.0888619, recall 0.902118
2017-12-10T12:32:12.653624: step 4783, loss 0.0487828, acc 0.984375, prec 0.0888794, recall 0.902139
2017-12-10T12:32:13.098496: step 4784, loss 0.0814475, acc 0.9375, prec 0.0888913, recall 0.90216
2017-12-10T12:32:13.530200: step 4785, loss 0.128787, acc 0.984375, prec 0.0889087, recall 0.902181
2017-12-10T12:32:13.980326: step 4786, loss 3.96264, acc 0.96875, prec 0.0889069, recall 0.901986
2017-12-10T12:32:14.421965: step 4787, loss 0.0221065, acc 1, prec 0.0889069, recall 0.901986
2017-12-10T12:32:14.878963: step 4788, loss 0.0425866, acc 0.984375, prec 0.088905, recall 0.901986
2017-12-10T12:32:15.337107: step 4789, loss 0.138088, acc 1, prec 0.0889437, recall 0.902029
2017-12-10T12:32:15.785441: step 4790, loss 0.11514, acc 0.953125, prec 0.0889381, recall 0.902029
2017-12-10T12:32:16.229008: step 4791, loss 0.0313156, acc 0.984375, prec 0.0889556, recall 0.90205
2017-12-10T12:32:16.682452: step 4792, loss 0.257003, acc 0.921875, prec 0.0889461, recall 0.90205
2017-12-10T12:32:17.122510: step 4793, loss 0.232841, acc 0.953125, prec 0.0889598, recall 0.902071
2017-12-10T12:32:17.553841: step 4794, loss 0.157743, acc 0.90625, prec 0.0889678, recall 0.902092
2017-12-10T12:32:17.994497: step 4795, loss 0.222348, acc 0.90625, prec 0.0889565, recall 0.902092
2017-12-10T12:32:18.432043: step 4796, loss 0.820164, acc 0.859375, prec 0.0889588, recall 0.902113
2017-12-10T12:32:18.872309: step 4797, loss 0.43887, acc 0.890625, prec 0.0889456, recall 0.902113
2017-12-10T12:32:19.335504: step 4798, loss 0.236293, acc 0.953125, prec 0.0889786, recall 0.902155
2017-12-10T12:32:19.763271: step 4799, loss 0.232333, acc 0.9375, prec 0.0889711, recall 0.902155
2017-12-10T12:32:20.212619: step 4800, loss 0.454488, acc 0.890625, prec 0.0889966, recall 0.902197
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4800

2017-12-10T12:32:22.104171: step 4801, loss 0.595533, acc 0.875, prec 0.0890008, recall 0.902218
2017-12-10T12:32:22.550749: step 4802, loss 0.291673, acc 0.90625, prec 0.0890088, recall 0.902239
2017-12-10T12:32:23.006332: step 4803, loss 0.32734, acc 0.90625, prec 0.0890168, recall 0.90226
2017-12-10T12:32:23.456207: step 4804, loss 0.136943, acc 0.96875, prec 0.089013, recall 0.90226
2017-12-10T12:32:23.904238: step 4805, loss 0.297068, acc 0.90625, prec 0.0890404, recall 0.902303
2017-12-10T12:32:24.358251: step 4806, loss 0.319743, acc 0.9375, prec 0.0890521, recall 0.902324
2017-12-10T12:32:24.819718: step 4807, loss 0.0404585, acc 0.984375, prec 0.0890889, recall 0.902366
2017-12-10T12:32:25.267227: step 4808, loss 0.353016, acc 0.921875, prec 0.0890795, recall 0.902366
2017-12-10T12:32:25.711885: step 4809, loss 0.130374, acc 0.984375, prec 0.0890776, recall 0.902366
2017-12-10T12:32:26.173232: step 4810, loss 0.160613, acc 0.953125, prec 0.0890913, recall 0.902387
2017-12-10T12:32:26.623341: step 4811, loss 0.168551, acc 0.96875, prec 0.0890875, recall 0.902387
2017-12-10T12:32:27.070348: step 4812, loss 0.0561831, acc 0.984375, prec 0.0890856, recall 0.902387
2017-12-10T12:32:27.528922: step 4813, loss 0.0367044, acc 0.984375, prec 0.089103, recall 0.902408
2017-12-10T12:32:27.991177: step 4814, loss 0.168764, acc 0.9375, prec 0.0891148, recall 0.902429
2017-12-10T12:32:28.436290: step 4815, loss 0.0791108, acc 0.96875, prec 0.0891497, recall 0.90247
2017-12-10T12:32:28.888257: step 4816, loss 0.58032, acc 0.90625, prec 0.0891383, recall 0.90247
2017-12-10T12:32:29.333770: step 4817, loss 0.00556829, acc 1, prec 0.0891383, recall 0.90247
2017-12-10T12:32:29.799212: step 4818, loss 0.0364065, acc 0.984375, prec 0.0891558, recall 0.902491
2017-12-10T12:32:30.255171: step 4819, loss 0.352668, acc 0.9375, prec 0.0891482, recall 0.902491
2017-12-10T12:32:30.699070: step 4820, loss 0.0814555, acc 0.984375, prec 0.0891463, recall 0.902491
2017-12-10T12:32:31.154357: step 4821, loss 0.0109504, acc 1, prec 0.0891463, recall 0.902491
2017-12-10T12:32:31.612498: step 4822, loss 4.208, acc 0.984375, prec 0.0891656, recall 0.902319
2017-12-10T12:32:32.068928: step 4823, loss 0.0371788, acc 1, prec 0.0891849, recall 0.90234
2017-12-10T12:32:32.504032: step 4824, loss 0.00927531, acc 1, prec 0.0891849, recall 0.90234
2017-12-10T12:32:32.948610: step 4825, loss 0.130068, acc 0.984375, prec 0.089241, recall 0.902402
2017-12-10T12:32:33.399189: step 4826, loss 0.319501, acc 0.9375, prec 0.0892528, recall 0.902423
2017-12-10T12:32:33.846713: step 4827, loss 0.153556, acc 0.953125, prec 0.0892857, recall 0.902465
2017-12-10T12:32:34.288738: step 4828, loss 0.141633, acc 0.984375, prec 0.0893031, recall 0.902486
2017-12-10T12:32:34.737206: step 4829, loss 0.0931682, acc 0.96875, prec 0.0893187, recall 0.902507
2017-12-10T12:32:35.187195: step 4830, loss 0.0576972, acc 0.96875, prec 0.0893342, recall 0.902528
2017-12-10T12:32:35.628094: step 4831, loss 0.115204, acc 0.984375, prec 0.0893709, recall 0.90257
2017-12-10T12:32:36.063738: step 4832, loss 0.181886, acc 0.921875, prec 0.0893614, recall 0.90257
2017-12-10T12:32:36.508794: step 4833, loss 0.167779, acc 0.953125, prec 0.0893944, recall 0.902611
2017-12-10T12:32:36.963556: step 4834, loss 0.287655, acc 0.96875, prec 0.0894099, recall 0.902632
2017-12-10T12:32:37.410047: step 4835, loss 0.141854, acc 0.96875, prec 0.0894447, recall 0.902674
2017-12-10T12:32:37.860098: step 4836, loss 0.386152, acc 0.953125, prec 0.089439, recall 0.902674
2017-12-10T12:32:38.321562: step 4837, loss 0.12874, acc 0.9375, prec 0.0894507, recall 0.902695
2017-12-10T12:32:38.776091: step 4838, loss 0.151693, acc 0.9375, prec 0.0894431, recall 0.902695
2017-12-10T12:32:39.225395: step 4839, loss 0.264995, acc 0.921875, prec 0.0894336, recall 0.902695
2017-12-10T12:32:39.663202: step 4840, loss 0.0339298, acc 0.984375, prec 0.089451, recall 0.902715
2017-12-10T12:32:40.110198: step 4841, loss 0.0570281, acc 0.96875, prec 0.0894473, recall 0.902715
2017-12-10T12:32:40.532108: step 4842, loss 0.0969027, acc 0.96875, prec 0.0894628, recall 0.902736
2017-12-10T12:32:40.978562: step 4843, loss 0.124718, acc 0.9375, prec 0.0894552, recall 0.902736
2017-12-10T12:32:41.418326: step 4844, loss 0.0806545, acc 0.96875, prec 0.0894707, recall 0.902757
2017-12-10T12:32:41.866181: step 4845, loss 0.0756176, acc 0.96875, prec 0.0894669, recall 0.902757
2017-12-10T12:32:42.315245: step 4846, loss 0.253908, acc 0.9375, prec 0.0894786, recall 0.902778
2017-12-10T12:32:42.765847: step 4847, loss 0.18221, acc 0.9375, prec 0.089471, recall 0.902778
2017-12-10T12:32:43.213751: step 4848, loss 0.00630912, acc 1, prec 0.089471, recall 0.902778
2017-12-10T12:32:43.672897: step 4849, loss 0.040369, acc 0.96875, prec 0.0894865, recall 0.902799
2017-12-10T12:32:44.140601: step 4850, loss 0.0571378, acc 0.96875, prec 0.0894827, recall 0.902799
2017-12-10T12:32:44.589883: step 4851, loss 0.63521, acc 0.984375, prec 0.0895001, recall 0.902819
2017-12-10T12:32:45.052651: step 4852, loss 0.337391, acc 0.984375, prec 0.0894982, recall 0.902819
2017-12-10T12:32:45.493721: step 4853, loss 0.0131275, acc 1, prec 0.0895368, recall 0.902861
2017-12-10T12:32:45.939492: step 4854, loss 0.045037, acc 1, prec 0.089556, recall 0.902882
2017-12-10T12:32:46.371791: step 4855, loss 0.0121473, acc 1, prec 0.089556, recall 0.902882
2017-12-10T12:32:46.803378: step 4856, loss 0.0603865, acc 0.984375, prec 0.0895734, recall 0.902902
2017-12-10T12:32:47.241329: step 4857, loss 2.99315, acc 0.96875, prec 0.0895715, recall 0.90271
2017-12-10T12:32:47.695505: step 4858, loss 0.0804613, acc 0.953125, prec 0.0896429, recall 0.902793
2017-12-10T12:32:48.142960: step 4859, loss 0.132213, acc 0.96875, prec 0.0896969, recall 0.902855
2017-12-10T12:32:48.601963: step 4860, loss 0.121539, acc 0.96875, prec 0.0896931, recall 0.902855
2017-12-10T12:32:49.049959: step 4861, loss 0.134804, acc 0.984375, prec 0.0897105, recall 0.902875
2017-12-10T12:32:49.497769: step 4862, loss 0.200915, acc 0.921875, prec 0.0897203, recall 0.902896
2017-12-10T12:32:49.944467: step 4863, loss 0.136554, acc 0.984375, prec 0.0897376, recall 0.902917
2017-12-10T12:32:50.386348: step 4864, loss 0.123479, acc 0.953125, prec 0.0897704, recall 0.902958
2017-12-10T12:32:50.830057: step 4865, loss 0.227961, acc 0.90625, prec 0.0897976, recall 0.902999
2017-12-10T12:32:51.282770: step 4866, loss 0.247484, acc 0.9375, prec 0.0898285, recall 0.903041
2017-12-10T12:32:51.733216: step 4867, loss 0.209433, acc 0.9375, prec 0.0898209, recall 0.903041
2017-12-10T12:32:52.181366: step 4868, loss 0.649753, acc 0.84375, prec 0.0898596, recall 0.903102
2017-12-10T12:32:52.621860: step 4869, loss 0.229445, acc 0.921875, prec 0.0898886, recall 0.903144
2017-12-10T12:32:53.065507: step 4870, loss 0.552084, acc 0.859375, prec 0.08991, recall 0.903185
2017-12-10T12:32:53.502716: step 4871, loss 0.480495, acc 0.859375, prec 0.0898929, recall 0.903185
2017-12-10T12:32:53.949109: step 4872, loss 0.361191, acc 0.90625, prec 0.0899199, recall 0.903226
2017-12-10T12:32:54.394160: step 4873, loss 0.223712, acc 0.984375, prec 0.089918, recall 0.903226
2017-12-10T12:32:54.846064: step 4874, loss 0.193841, acc 0.953125, prec 0.0899316, recall 0.903246
2017-12-10T12:32:55.286755: step 4875, loss 0.306711, acc 0.9375, prec 0.0899432, recall 0.903267
2017-12-10T12:32:55.734228: step 4876, loss 0.0994865, acc 0.953125, prec 0.0899567, recall 0.903287
2017-12-10T12:32:56.176958: step 4877, loss 0.323029, acc 0.9375, prec 0.0899491, recall 0.903287
2017-12-10T12:32:56.613662: step 4878, loss 0.114997, acc 0.953125, prec 0.0900011, recall 0.903349
2017-12-10T12:32:57.050419: step 4879, loss 0.137521, acc 0.96875, prec 0.0899973, recall 0.903349
2017-12-10T12:32:57.506952: step 4880, loss 0.0763973, acc 0.96875, prec 0.0900127, recall 0.903369
2017-12-10T12:32:57.965352: step 4881, loss 0.0417006, acc 0.984375, prec 0.09003, recall 0.90339
2017-12-10T12:32:58.412455: step 4882, loss 0.185754, acc 0.921875, prec 0.0900397, recall 0.90341
2017-12-10T12:32:58.862326: step 4883, loss 0.0241604, acc 1, prec 0.0900397, recall 0.90341
2017-12-10T12:32:59.292954: step 4884, loss 0.328616, acc 0.90625, prec 0.0900283, recall 0.90341
2017-12-10T12:32:59.728713: step 4885, loss 0.94935, acc 0.984375, prec 0.0901224, recall 0.903512
2017-12-10T12:33:00.173411: step 4886, loss 0.031071, acc 1, prec 0.0901608, recall 0.903553
2017-12-10T12:33:00.624659: step 4887, loss 0.264983, acc 0.921875, prec 0.0901705, recall 0.903574
2017-12-10T12:33:01.070880: step 4888, loss 0.123262, acc 0.953125, prec 0.090184, recall 0.903594
2017-12-10T12:33:01.515404: step 4889, loss 0.0749903, acc 0.953125, prec 0.0901975, recall 0.903614
2017-12-10T12:33:01.961569: step 4890, loss 0.0196557, acc 1, prec 0.0901975, recall 0.903614
2017-12-10T12:33:02.411398: step 4891, loss 0.221054, acc 0.921875, prec 0.090188, recall 0.903614
2017-12-10T12:33:02.866043: step 4892, loss 1.8215, acc 0.96875, prec 0.0902053, recall 0.903444
2017-12-10T12:33:03.326902: step 4893, loss 0.441877, acc 0.890625, prec 0.0902111, recall 0.903464
2017-12-10T12:33:03.773972: step 4894, loss 0.266806, acc 0.90625, prec 0.0902189, recall 0.903485
2017-12-10T12:33:04.223342: step 4895, loss 0.328201, acc 0.9375, prec 0.0902305, recall 0.903505
2017-12-10T12:33:04.685633: step 4896, loss 0.118632, acc 0.9375, prec 0.0902421, recall 0.903525
2017-12-10T12:33:05.141316: step 4897, loss 0.508709, acc 0.921875, prec 0.0902517, recall 0.903546
2017-12-10T12:33:05.595889: step 4898, loss 0.427056, acc 0.953125, prec 0.0902844, recall 0.903587
2017-12-10T12:33:06.037104: step 4899, loss 0.315991, acc 0.90625, prec 0.0902921, recall 0.903607
2017-12-10T12:33:06.485738: step 4900, loss 0.268982, acc 0.90625, prec 0.090319, recall 0.903647
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_1/1512924921/checkpoints/model-4900

2017-12-10T12:33:08.611157: step 4901, loss 0.392632, acc 0.890625, prec 0.0903441, recall 0.903688
2017-12-10T12:33:09.067555: step 4902, loss 0.448698, acc 0.921875, prec 0.0903537, recall 0.903708
2017-12-10T12:33:09.535385: step 4903, loss 0.378016, acc 0.890625, prec 0.0903404, recall 0.903708
2017-12-10T12:33:09.991672: step 4904, loss 0.105302, acc 0.96875, prec 0.0903366, recall 0.903708
2017-12-10T12:33:10.433185: step 4905, loss 0.256496, acc 0.921875, prec 0.0903654, recall 0.903749
2017-12-10T12:33:10.887742: step 4906, loss 0.0442498, acc 0.984375, prec 0.0904018, recall 0.903789
2017-12-10T12:33:11.331903: step 4907, loss 0.417545, acc 0.96875, prec 0.0904171, recall 0.90381
2017-12-10T12:33:11.775995: step 4908, loss 0.137719, acc 0.9375, prec 0.0904287, recall 0.90383
2017-12-10T12:33:12.218087: step 4909, loss 0.175657, acc 0.96875, prec 0.090444, recall 0.90385
2017-12-10T12:33:12.679113: step 4910, loss 0.0899094, acc 0.96875, prec 0.0904402, recall 0.90385
2017-12-10T12:33:13.123715: step 4911, loss 0.961659, acc 0.953125, prec 0.0904728, recall 0.903891
2017-12-10T12:33:13.577747: step 4912, loss 0.142757, acc 0.984375, prec 0.09049, recall 0.903911
2017-12-10T12:33:14.019812: step 4913, loss 0.203726, acc 0.96875, prec 0.0905436, recall 0.903971
2017-12-10T12:33:14.464632: step 4914, loss 0.0528584, acc 0.984375, prec 0.0905417, recall 0.903971
2017-12-10T12:33:14.912757: step 4915, loss 0.0873105, acc 0.984375, prec 0.0905398, recall 0.903971
2017-12-10T12:33:15.350798: step 4916, loss 0.250541, acc 0.921875, prec 0.0905303, recall 0.903971
2017-12-10T12:33:15.801483: step 4917, loss 0.0733704, acc 0.96875, prec 0.0905648, recall 0.904012
2017-12-10T12:33:16.237559: step 4918, loss 0.172244, acc 0.984375, prec 0.0906203, recall 0.904072
2017-12-10T12:33:16.679629: step 4919, loss 0.634278, acc 0.96875, prec 0.0906356, recall 0.904092
2017-12-10T12:33:17.136070: step 4920, loss 0.167546, acc 0.9375, prec 0.090628, recall 0.904092
2017-12-10T12:33:17.576784: step 4921, loss 0.2812, acc 0.9375, prec 0.0906203, recall 0.904092
2017-12-10T12:33:18.013762: step 4922, loss 0.0683955, acc 0.96875, prec 0.0906356, recall 0.904112
2017-12-10T12:33:18.454367: step 4923, loss 0.150359, acc 0.984375, prec 0.0906337, recall 0.904112
2017-12-10T12:33:18.899664: step 4924, loss 0.24169, acc 0.96875, prec 0.0906299, recall 0.904112
2017-12-10T12:33:19.344157: step 4925, loss 0.0249172, acc 0.984375, prec 0.090628, recall 0.904112
2017-12-10T12:33:19.791657: step 4926, loss 0.115722, acc 0.984375, prec 0.0906261, recall 0.904112
2017-12-10T12:33:20.231783: step 4927, loss 0.0835416, acc 0.984375, prec 0.0906625, recall 0.904153
2017-12-10T12:33:20.674546: step 4928, loss 0.146684, acc 0.96875, prec 0.0906778, recall 0.904173
2017-12-10T12:33:21.112699: step 4929, loss 0.132703, acc 0.984375, prec 0.090695, recall 0.904193
2017-12-10T12:33:21.558046: step 4930, loss 0.0782751, acc 0.953125, prec 0.0907084, recall 0.904213
2017-12-10T12:33:21.999620: step 4931, loss 0.063667, acc 0.96875, prec 0.0907046, recall 0.904213
2017-12-10T12:33:22.451776: step 4932, loss 0.248186, acc 0.9375, prec 0.0907352, recall 0.904253
2017-12-10T12:33:22.905836: step 4933, loss 0.0129791, acc 1, prec 0.0907352, recall 0.904253
2017-12-10T12:33:23.365705: step 4934, loss 0.327953, acc 0.953125, prec 0.0907486, recall 0.904273
2017-12-10T12:33:23.804379: step 4935, loss 0.225627, acc 0.9375, prec 0.0907792, recall 0.904313
2017-12-10T12:33:24.245251: step 4936, loss 0.0462776, acc 0.96875, prec 0.0907753, recall 0.904313
2017-12-10T12:33:24.690956: step 4937, loss 0.490018, acc 0.953125, prec 0.0908078, recall 0.904353
2017-12-10T12:33:25.145778: step 4938, loss 0.0884852, acc 0.953125, prec 0.0908021, recall 0.904353
2017-12-10T12:33:25.586914: step 4939, loss 0.463886, acc 0.984375, prec 0.0908575, recall 0.904413
2017-12-10T12:33:26.045577: step 4940, loss 0.409797, acc 0.984375, prec 0.0908938, recall 0.904453
2017-12-10T12:33:26.488241: step 4941, loss 0.0430995, acc 0.984375, prec 0.0909301, recall 0.904493
2017-12-10T12:33:26.931597: step 4942, loss 0.0239958, acc 0.984375, prec 0.0909282, recall 0.904493
2017-12-10T12:33:27.373306: step 4943, loss 0.0250882, acc 0.984375, prec 0.0909454, recall 0.904513
2017-12-10T12:33:27.829845: step 4944, loss 0.0765828, acc 0.984375, prec 0.0909817, recall 0.904553
2017-12-10T12:33:28.308775: step 4945, loss 0.552472, acc 0.9375, prec 0.0910122, recall 0.904593
2017-12-10T12:33:28.759309: step 4946, loss 0.668015, acc 0.984375, prec 0.0910485, recall 0.904633
2017-12-10T12:33:29.213638: step 4947, loss 0.544096, acc 0.953125, prec 0.0911, recall 0.904692
2017-12-10T12:33:29.652890: step 4948, loss 0.459571, acc 0.921875, prec 0.0910904, recall 0.904692
2017-12-10T12:33:30.097733: step 4949, loss 0.188875, acc 0.9375, prec 0.0910828, recall 0.904692
2017-12-10T12:33:30.537220: step 4950, loss 0.121941, acc 0.96875, prec 0.091079, recall 0.904692
2017-12-10T12:33:30.978709: step 4951, loss 0.264532, acc 0.921875, prec 0.0910694, recall 0.904692
2017-12-10T12:33:31.428361: step 4952, loss 0.854295, acc 0.9375, prec 0.0910808, recall 0.904712
2017-12-10T12:33:31.868474: step 4953, loss 0.347135, acc 0.890625, prec 0.0911247, recall 0.904772
2017-12-10T12:33:32.309926: step 4954, loss 0.12811, acc 0.96875, prec 0.0911781, recall 0.904831
2017-12-10T12:33:32.754113: step 4955, loss 0.20884, acc 0.9375, prec 0.0911895, recall 0.904851
2017-12-10T12:33:33.200345: step 4956, loss 0.305246, acc 0.953125, prec 0.0912028, recall 0.904871
2017-12-10T12:33:33.647705: step 4957, loss 0.305714, acc 0.9375, prec 0.0912142, recall 0.904891
2017-12-10T12:33:34.095175: step 4958, loss 0.245215, acc 0.9375, prec 0.0912447, recall 0.90493
2017-12-10T12:33:34.547211: step 4959, loss 0.869308, acc 0.90625, prec 0.0912713, recall 0.90497
2017-12-10T12:33:35.000606: step 4960, loss 0.378235, acc 0.875, prec 0.0912751, recall 0.90499
2017-12-10T12:33:35.441326: step 4961, loss 0.358004, acc 0.9375, prec 0.0913055, recall 0.905029
2017-12-10T12:33:35.876544: step 4962, loss 0.209042, acc 0.953125, prec 0.0913188, recall 0.905049
2017-12-10T12:33:36.310549: step 4963, loss 0.755654, acc 0.9375, prec 0.0913493, recall 0.905088
2017-12-10T12:33:36.756621: step 4964, loss 0.142284, acc 0.921875, prec 0.0913588, recall 0.905108
2017-12-10T12:33:37.205717: step 4965, loss 0.126913, acc 0.9375, prec 0.0913892, recall 0.905147
2017-12-10T12:33:37.662108: step 4966, loss 0.257657, acc 0.90625, prec 0.0913777, recall 0.905147
2017-12-10T12:33:38.127826: step 4967, loss 0.736858, acc 0.9375, prec 0.0913891, recall 0.905167
2017-12-10T12:33:38.578545: step 4968, loss 0.37881, acc 0.953125, prec 0.0913833, recall 0.905167
2017-12-10T12:33:39.018437: step 4969, loss 0.0901229, acc 0.96875, prec 0.0914176, recall 0.905206
2017-12-10T12:33:39.414262: step 4970, loss 0.110602, acc 0.941176, prec 0.0914118, recall 0.905206
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 256
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR embedding_size_256_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220

Start training
2017-12-10T12:33:44.484709: step 1, loss 15.7858, acc 0.328125, prec 0.0454545, recall 0.666667
2017-12-10T12:33:44.917931: step 2, loss 6.59175, acc 0.296875, prec 0.0337079, recall 0.6
2017-12-10T12:33:45.352066: step 3, loss 4.46975, acc 0.40625, prec 0.023622, recall 0.6
2017-12-10T12:33:45.799436: step 4, loss 4.3017, acc 0.328125, prec 0.0176471, recall 0.6
2017-12-10T12:33:46.257170: step 5, loss 3.69772, acc 0.59375, prec 0.0204082, recall 0.571429
2017-12-10T12:33:46.698693: step 6, loss 11.8527, acc 0.484375, prec 0.0175439, recall 0.5
2017-12-10T12:33:47.134323: step 7, loss 1.73635, acc 0.625, prec 0.0197628, recall 0.555556
2017-12-10T12:33:47.573007: step 8, loss 2.63974, acc 0.546875, prec 0.0177305, recall 0.555556
2017-12-10T12:33:47.999285: step 9, loss 7.51274, acc 0.5625, prec 0.0161812, recall 0.5
2017-12-10T12:33:48.443909: step 10, loss 31.1919, acc 0.484375, prec 0.0147059, recall 0.416667
2017-12-10T12:33:48.885762: step 11, loss 2.75664, acc 0.5, prec 0.0160858, recall 0.461538
2017-12-10T12:33:49.326243: step 12, loss 5.0367, acc 0.578125, prec 0.0175, recall 0.466667
2017-12-10T12:33:49.761403: step 13, loss 12.5782, acc 0.390625, prec 0.0204545, recall 0.5
2017-12-10T12:33:50.195130: step 14, loss 5.90602, acc 0.234375, prec 0.0184049, recall 0.5
2017-12-10T12:33:50.632948: step 15, loss 5.79161, acc 0.3125, prec 0.0187266, recall 0.526316
2017-12-10T12:33:51.069651: step 16, loss 6.21467, acc 0.203125, prec 0.017094, recall 0.526316
2017-12-10T12:33:51.510167: step 17, loss 5.28194, acc 0.375, prec 0.016, recall 0.526316
2017-12-10T12:33:51.953706: step 18, loss 5.50067, acc 0.34375, prec 0.0149925, recall 0.526316
2017-12-10T12:33:52.390279: step 19, loss 6.08101, acc 0.328125, prec 0.0154712, recall 0.55
2017-12-10T12:33:52.832974: step 20, loss 14.6909, acc 0.28125, prec 0.0145503, recall 0.52381
2017-12-10T12:33:53.267965: step 21, loss 6.02477, acc 0.296875, prec 0.0137328, recall 0.52381
2017-12-10T12:33:53.721425: step 22, loss 3.89543, acc 0.453125, prec 0.0131579, recall 0.52381
2017-12-10T12:33:54.165318: step 23, loss 3.3912, acc 0.515625, prec 0.0126874, recall 0.52381
2017-12-10T12:33:54.604194: step 24, loss 2.38722, acc 0.546875, prec 0.0133779, recall 0.545455
2017-12-10T12:33:55.038551: step 25, loss 1.91925, acc 0.75, prec 0.0131435, recall 0.545455
2017-12-10T12:33:55.474877: step 26, loss 1.78319, acc 0.625, prec 0.0128068, recall 0.545455
2017-12-10T12:33:55.914920: step 27, loss 31.5457, acc 0.65625, prec 0.0135558, recall 0.541667
2017-12-10T12:33:56.351181: step 28, loss 1.63477, acc 0.671875, prec 0.0132653, recall 0.541667
2017-12-10T12:33:56.792973: step 29, loss 5.7949, acc 0.796875, prec 0.0140987, recall 0.538462
2017-12-10T12:33:57.228574: step 30, loss 10.9144, acc 0.703125, prec 0.0138477, recall 0.518519
2017-12-10T12:33:57.682577: step 31, loss 26.4369, acc 0.609375, prec 0.0135397, recall 0.482759
2017-12-10T12:33:58.123505: step 32, loss 5.24101, acc 0.6875, prec 0.0132953, recall 0.466667
2017-12-10T12:33:58.563652: step 33, loss 3.90306, acc 0.625, prec 0.0139276, recall 0.46875
2017-12-10T12:33:59.001035: step 34, loss 2.89529, acc 0.59375, prec 0.0153846, recall 0.5
2017-12-10T12:33:59.441535: step 35, loss 5.95146, acc 0.328125, prec 0.0148084, recall 0.5
2017-12-10T12:33:59.892784: step 36, loss 7.103, acc 0.21875, prec 0.0141903, recall 0.5
2017-12-10T12:34:00.327802: step 37, loss 5.05846, acc 0.3125, prec 0.0136876, recall 0.5
2017-12-10T12:34:00.769708: step 38, loss 8.43347, acc 0.296875, prec 0.0139752, recall 0.514286
2017-12-10T12:34:01.215206: step 39, loss 6.58811, acc 0.25, prec 0.0149477, recall 0.540541
2017-12-10T12:34:01.653814: step 40, loss 15.3668, acc 0.15625, prec 0.0143781, recall 0.526316
2017-12-10T12:34:02.092205: step 41, loss 6.79017, acc 0.234375, prec 0.0152566, recall 0.55
2017-12-10T12:34:02.527397: step 42, loss 6.30276, acc 0.265625, prec 0.0154362, recall 0.560976
2017-12-10T12:34:02.971580: step 43, loss 7.07873, acc 0.21875, prec 0.0162127, recall 0.581395
2017-12-10T12:34:03.411570: step 44, loss 7.98361, acc 0.15625, prec 0.0162805, recall 0.590909
2017-12-10T12:34:03.866389: step 45, loss 6.13835, acc 0.296875, prec 0.0164334, recall 0.6
2017-12-10T12:34:04.314677: step 46, loss 4.39359, acc 0.359375, prec 0.0166172, recall 0.608696
2017-12-10T12:34:04.761338: step 47, loss 5.93824, acc 0.296875, prec 0.016185, recall 0.608696
2017-12-10T12:34:05.197356: step 48, loss 3.25736, acc 0.578125, prec 0.0170551, recall 0.625
2017-12-10T12:34:05.645216: step 49, loss 21.4595, acc 0.5625, prec 0.0167973, recall 0.612245
2017-12-10T12:34:06.091631: step 50, loss 2.26409, acc 0.65625, prec 0.0165929, recall 0.612245
2017-12-10T12:34:06.527266: step 51, loss 1.86908, acc 0.765625, prec 0.0169956, recall 0.62
2017-12-10T12:34:06.969523: step 52, loss 2.40901, acc 0.5625, prec 0.0167387, recall 0.62
2017-12-10T12:34:07.410615: step 53, loss 0.855894, acc 0.734375, prec 0.0165864, recall 0.62
2017-12-10T12:34:07.850916: step 54, loss 1.00995, acc 0.71875, prec 0.0164282, recall 0.62
2017-12-10T12:34:08.294401: step 55, loss 1.02285, acc 0.796875, prec 0.0168332, recall 0.627451
2017-12-10T12:34:08.732199: step 56, loss 0.839261, acc 0.84375, prec 0.0167452, recall 0.627451
2017-12-10T12:34:09.186858: step 57, loss 24.1426, acc 0.71875, prec 0.0165975, recall 0.615385
2017-12-10T12:34:09.635949: step 58, loss 24.0211, acc 0.828125, prec 0.0170191, recall 0.611111
2017-12-10T12:34:10.108142: step 59, loss 0.109487, acc 0.9375, prec 0.016984, recall 0.611111
2017-12-10T12:34:10.559987: step 60, loss 0.815913, acc 0.796875, prec 0.0168712, recall 0.611111
2017-12-10T12:34:11.010832: step 61, loss 11.5323, acc 0.84375, prec 0.0167939, recall 0.6
2017-12-10T12:34:11.463891: step 62, loss 45.5593, acc 0.8125, prec 0.0167089, recall 0.578947
2017-12-10T12:34:11.901111: step 63, loss 1.31874, acc 0.765625, prec 0.0170768, recall 0.586207
2017-12-10T12:34:12.333414: step 64, loss 1.95079, acc 0.671875, prec 0.017387, recall 0.59322
2017-12-10T12:34:12.774332: step 65, loss 2.74304, acc 0.5625, prec 0.018591, recall 0.612903
2017-12-10T12:34:13.227728: step 66, loss 1.9259, acc 0.734375, prec 0.0184377, recall 0.612903
2017-12-10T12:34:13.676933: step 67, loss 2.62729, acc 0.625, prec 0.0186961, recall 0.619048
2017-12-10T12:34:14.117199: step 68, loss 1.92962, acc 0.671875, prec 0.0185097, recall 0.619048
2017-12-10T12:34:14.563802: step 69, loss 4.48284, acc 0.515625, prec 0.0187003, recall 0.625
2017-12-10T12:34:15.014509: step 70, loss 2.60574, acc 0.5625, prec 0.0189114, recall 0.630769
2017-12-10T12:34:15.451764: step 71, loss 3.76393, acc 0.53125, prec 0.0186533, recall 0.630769
2017-12-10T12:34:15.891523: step 72, loss 2.96027, acc 0.484375, prec 0.0188172, recall 0.636364
2017-12-10T12:34:16.328460: step 73, loss 3.90631, acc 0.5, prec 0.0189845, recall 0.641791
2017-12-10T12:34:16.773504: step 74, loss 2.804, acc 0.578125, prec 0.0200436, recall 0.657143
2017-12-10T12:34:17.212174: step 75, loss 7.41264, acc 0.625, prec 0.0211116, recall 0.662162
2017-12-10T12:34:17.655635: step 76, loss 3.59361, acc 0.5625, prec 0.0216929, recall 0.671053
2017-12-10T12:34:18.087730: step 77, loss 2.58352, acc 0.65625, prec 0.021904, recall 0.675325
2017-12-10T12:34:18.536700: step 78, loss 2.58808, acc 0.5625, prec 0.0216486, recall 0.675325
2017-12-10T12:34:18.983744: step 79, loss 1.73215, acc 0.625, prec 0.0218377, recall 0.679487
2017-12-10T12:34:19.446909: step 80, loss 3.03645, acc 0.5625, prec 0.021987, recall 0.683544
2017-12-10T12:34:19.886285: step 81, loss 2.44942, acc 0.578125, prec 0.0217479, recall 0.683544
2017-12-10T12:34:20.317503: step 82, loss 2.54217, acc 0.5625, prec 0.0218949, recall 0.6875
2017-12-10T12:34:20.763308: step 83, loss 5.71049, acc 0.65625, prec 0.0217134, recall 0.679012
2017-12-10T12:34:21.198457: step 84, loss 1.86556, acc 0.671875, prec 0.0215348, recall 0.679012
2017-12-10T12:34:21.636002: step 85, loss 1.80161, acc 0.75, prec 0.0217814, recall 0.682927
2017-12-10T12:34:22.077183: step 86, loss 1.93247, acc 0.671875, prec 0.0216049, recall 0.682927
2017-12-10T12:34:22.518699: step 87, loss 8.45676, acc 0.671875, prec 0.0214395, recall 0.674699
2017-12-10T12:34:22.970828: step 88, loss 0.992641, acc 0.796875, prec 0.021706, recall 0.678571
2017-12-10T12:34:23.408791: step 89, loss 1.52048, acc 0.796875, prec 0.0219697, recall 0.682353
2017-12-10T12:34:23.841867: step 90, loss 46.0588, acc 0.78125, prec 0.0218785, recall 0.659091
2017-12-10T12:34:24.290323: step 91, loss 2.25351, acc 0.609375, prec 0.0216741, recall 0.659091
2017-12-10T12:34:24.734902: step 92, loss 8.50431, acc 0.640625, prec 0.0214974, recall 0.651685
2017-12-10T12:34:25.179457: step 93, loss 3.68094, acc 0.40625, prec 0.0211988, recall 0.651685
2017-12-10T12:34:25.615951: step 94, loss 15.7315, acc 0.484375, prec 0.021315, recall 0.641304
2017-12-10T12:34:26.055294: step 95, loss 3.95568, acc 0.375, prec 0.0213599, recall 0.645161
2017-12-10T12:34:26.498091: step 96, loss 7.04279, acc 0.21875, prec 0.0213287, recall 0.648936
2017-12-10T12:34:26.941951: step 97, loss 6.74999, acc 0.21875, prec 0.0212985, recall 0.652632
2017-12-10T12:34:27.382804: step 98, loss 7.52512, acc 0.265625, prec 0.021952, recall 0.663265
2017-12-10T12:34:27.824276: step 99, loss 7.69618, acc 0.140625, prec 0.0215517, recall 0.663265
2017-12-10T12:34:28.252041: step 100, loss 8.08847, acc 0.171875, prec 0.0214984, recall 0.666667
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-100

2017-12-10T12:34:30.210460: step 101, loss 7.61128, acc 0.203125, prec 0.0211471, recall 0.666667
2017-12-10T12:34:30.650533: step 102, loss 6.58868, acc 0.296875, prec 0.0211557, recall 0.67
2017-12-10T12:34:31.085482: step 103, loss 7.78262, acc 0.234375, prec 0.0211377, recall 0.673267
2017-12-10T12:34:31.512150: step 104, loss 4.57323, acc 0.359375, prec 0.0217725, recall 0.682692
2017-12-10T12:34:31.945199: step 105, loss 4.19102, acc 0.421875, prec 0.0215282, recall 0.682692
2017-12-10T12:34:32.384739: step 106, loss 3.78335, acc 0.453125, prec 0.0213021, recall 0.682692
2017-12-10T12:34:32.820329: step 107, loss 2.91895, acc 0.53125, prec 0.0214031, recall 0.685714
2017-12-10T12:34:33.261834: step 108, loss 15.7415, acc 0.59375, prec 0.0215339, recall 0.682243
2017-12-10T12:34:33.709997: step 109, loss 2.12193, acc 0.703125, prec 0.0214139, recall 0.682243
2017-12-10T12:34:34.155730: step 110, loss 1.80939, acc 0.734375, prec 0.0213076, recall 0.682243
2017-12-10T12:34:34.601614: step 111, loss 2.17966, acc 0.625, prec 0.0211594, recall 0.682243
2017-12-10T12:34:35.035567: step 112, loss 10.8339, acc 0.78125, prec 0.02108, recall 0.675926
2017-12-10T12:34:35.490721: step 113, loss 1.50487, acc 0.71875, prec 0.020971, recall 0.675926
2017-12-10T12:34:35.928767: step 114, loss 35.4897, acc 0.765625, prec 0.020893, recall 0.663636
2017-12-10T12:34:36.374625: step 115, loss 1.07826, acc 0.703125, prec 0.0210586, recall 0.666667
2017-12-10T12:34:36.803429: step 116, loss 2.06724, acc 0.71875, prec 0.0209513, recall 0.666667
2017-12-10T12:34:37.241982: step 117, loss 15.3027, acc 0.640625, prec 0.0213783, recall 0.66087
2017-12-10T12:34:37.690122: step 118, loss 2.03027, acc 0.625, prec 0.021235, recall 0.66087
2017-12-10T12:34:38.143249: step 119, loss 5.69261, acc 0.46875, prec 0.0213119, recall 0.65812
2017-12-10T12:34:38.611577: step 120, loss 3.24367, acc 0.515625, prec 0.0213992, recall 0.661017
2017-12-10T12:34:39.057307: step 121, loss 5.04189, acc 0.34375, prec 0.0214208, recall 0.663866
2017-12-10T12:34:39.499936: step 122, loss 3.82812, acc 0.453125, prec 0.0214823, recall 0.666667
2017-12-10T12:34:39.938941: step 123, loss 4.22498, acc 0.453125, prec 0.0218027, recall 0.672131
2017-12-10T12:34:40.383701: step 124, loss 4.92511, acc 0.359375, prec 0.0215676, recall 0.672131
2017-12-10T12:34:40.826335: step 125, loss 4.40787, acc 0.359375, prec 0.0221009, recall 0.68
2017-12-10T12:34:41.280459: step 126, loss 6.46651, acc 0.296875, prec 0.0220966, recall 0.68254
2017-12-10T12:34:41.721612: step 127, loss 4.14424, acc 0.4375, prec 0.022143, recall 0.685039
2017-12-10T12:34:42.166583: step 128, loss 4.28899, acc 0.4375, prec 0.0224351, recall 0.689922
2017-12-10T12:34:42.613812: step 129, loss 10.6763, acc 0.3125, prec 0.0224383, recall 0.687023
2017-12-10T12:34:43.054162: step 130, loss 3.425, acc 0.484375, prec 0.0224969, recall 0.689394
2017-12-10T12:34:43.508299: step 131, loss 10.1528, acc 0.59375, prec 0.0228389, recall 0.688889
2017-12-10T12:34:43.952399: step 132, loss 2.50902, acc 0.625, prec 0.0229436, recall 0.691176
2017-12-10T12:34:44.424152: step 133, loss 6.59481, acc 0.5625, prec 0.0230303, recall 0.688406
2017-12-10T12:34:44.881145: step 134, loss 2.59583, acc 0.546875, prec 0.0228695, recall 0.688406
2017-12-10T12:34:45.326131: step 135, loss 2.83189, acc 0.625, prec 0.0227382, recall 0.688406
2017-12-10T12:34:45.772679: step 136, loss 3.44794, acc 0.484375, prec 0.0232558, recall 0.695035
2017-12-10T12:34:46.228395: step 137, loss 2.97948, acc 0.515625, prec 0.0233161, recall 0.697183
2017-12-10T12:34:46.660301: step 138, loss 1.27641, acc 0.734375, prec 0.0234522, recall 0.699301
2017-12-10T12:34:47.092476: step 139, loss 1.74794, acc 0.671875, prec 0.0233372, recall 0.699301
2017-12-10T12:34:47.536330: step 140, loss 2.63729, acc 0.59375, prec 0.0231965, recall 0.699301
2017-12-10T12:34:47.973954: step 141, loss 1.75823, acc 0.65625, prec 0.0230787, recall 0.699301
2017-12-10T12:34:48.415925: step 142, loss 1.59051, acc 0.75, prec 0.0229938, recall 0.699301
2017-12-10T12:34:48.866196: step 143, loss 21.1032, acc 0.75, prec 0.0229148, recall 0.694444
2017-12-10T12:34:49.315426: step 144, loss 1.19332, acc 0.78125, prec 0.0228415, recall 0.694444
2017-12-10T12:34:49.763246: step 145, loss 1.40568, acc 0.765625, prec 0.0232082, recall 0.69863
2017-12-10T12:34:50.223240: step 146, loss 1.33095, acc 0.734375, prec 0.0231188, recall 0.69863
2017-12-10T12:34:50.676226: step 147, loss 1.37403, acc 0.75, prec 0.0230352, recall 0.69863
2017-12-10T12:34:51.129124: step 148, loss 2.10814, acc 0.796875, prec 0.022973, recall 0.693878
2017-12-10T12:34:51.580878: step 149, loss 9.23897, acc 0.796875, prec 0.0229111, recall 0.689189
2017-12-10T12:34:52.028481: step 150, loss 13.5638, acc 0.859375, prec 0.023089, recall 0.686667
2017-12-10T12:34:52.470205: step 151, loss 0.879911, acc 0.8125, prec 0.0230271, recall 0.686667
2017-12-10T12:34:52.914215: step 152, loss 0.866561, acc 0.796875, prec 0.0229603, recall 0.686667
2017-12-10T12:34:53.366256: step 153, loss 3.96976, acc 0.78125, prec 0.022894, recall 0.682119
2017-12-10T12:34:53.810118: step 154, loss 1.99521, acc 0.671875, prec 0.0230038, recall 0.684211
2017-12-10T12:34:54.245471: step 155, loss 3.61276, acc 0.625, prec 0.0233121, recall 0.688312
2017-12-10T12:34:54.688945: step 156, loss 2.07273, acc 0.578125, prec 0.0231745, recall 0.688312
2017-12-10T12:34:55.127311: step 157, loss 2.25986, acc 0.59375, prec 0.0230435, recall 0.688312
2017-12-10T12:34:55.573518: step 158, loss 1.90894, acc 0.71875, prec 0.0231652, recall 0.690323
2017-12-10T12:34:56.016405: step 159, loss 2.30446, acc 0.59375, prec 0.0230355, recall 0.690323
2017-12-10T12:34:56.460264: step 160, loss 2.77264, acc 0.5625, prec 0.0233155, recall 0.694268
2017-12-10T12:34:56.889977: step 161, loss 4.53244, acc 0.609375, prec 0.0231964, recall 0.689873
2017-12-10T12:34:57.320842: step 162, loss 2.0482, acc 0.59375, prec 0.0232755, recall 0.691824
2017-12-10T12:34:57.760646: step 163, loss 1.9748, acc 0.578125, prec 0.0233488, recall 0.69375
2017-12-10T12:34:58.210951: step 164, loss 3.13097, acc 0.515625, prec 0.0231975, recall 0.69375
2017-12-10T12:34:58.651123: step 165, loss 1.63601, acc 0.625, prec 0.0232848, recall 0.695652
2017-12-10T12:34:59.091568: step 166, loss 1.95453, acc 0.671875, prec 0.0231836, recall 0.695652
2017-12-10T12:34:59.532329: step 167, loss 3.06882, acc 0.640625, prec 0.023275, recall 0.697531
2017-12-10T12:34:59.962201: step 168, loss 1.93686, acc 0.609375, prec 0.0231557, recall 0.697531
2017-12-10T12:35:00.399048: step 169, loss 1.23011, acc 0.734375, prec 0.0232748, recall 0.699386
2017-12-10T12:35:00.844591: step 170, loss 2.03097, acc 0.59375, prec 0.0233503, recall 0.701219
2017-12-10T12:35:01.300053: step 171, loss 2.91785, acc 0.609375, prec 0.0236268, recall 0.704819
2017-12-10T12:35:01.747977: step 172, loss 1.62743, acc 0.734375, prec 0.023546, recall 0.704819
2017-12-10T12:35:02.186173: step 173, loss 1.48271, acc 0.734375, prec 0.0234657, recall 0.704819
2017-12-10T12:35:02.626002: step 174, loss 12.9004, acc 0.6875, prec 0.0233766, recall 0.700599
2017-12-10T12:35:03.072995: step 175, loss 13.9964, acc 0.71875, prec 0.0232975, recall 0.696429
2017-12-10T12:35:03.508728: step 176, loss 1.30651, acc 0.75, prec 0.0234173, recall 0.698225
2017-12-10T12:35:03.964249: step 177, loss 2.09754, acc 0.78125, prec 0.0235457, recall 0.7
2017-12-10T12:35:04.406725: step 178, loss 13.1424, acc 0.765625, prec 0.0236733, recall 0.697674
2017-12-10T12:35:04.849405: step 179, loss 1.90528, acc 0.703125, prec 0.0237768, recall 0.699422
2017-12-10T12:35:05.293782: step 180, loss 1.82896, acc 0.65625, prec 0.0240563, recall 0.702857
2017-12-10T12:35:05.757877: step 181, loss 0.956905, acc 0.734375, prec 0.0239766, recall 0.702857
2017-12-10T12:35:06.197867: step 182, loss 2.00441, acc 0.6875, prec 0.0238835, recall 0.702857
2017-12-10T12:35:06.649347: step 183, loss 1.22904, acc 0.75, prec 0.0238095, recall 0.702857
2017-12-10T12:35:07.085742: step 184, loss 2.28791, acc 0.65625, prec 0.0238967, recall 0.704545
2017-12-10T12:35:07.523790: step 185, loss 9.65162, acc 0.609375, prec 0.0237867, recall 0.700565
2017-12-10T12:35:07.980711: step 186, loss 2.20819, acc 0.578125, prec 0.0236641, recall 0.700565
2017-12-10T12:35:08.423213: step 187, loss 2.3656, acc 0.671875, prec 0.0239407, recall 0.703911
2017-12-10T12:35:08.873654: step 188, loss 1.61643, acc 0.640625, prec 0.0243902, recall 0.708791
2017-12-10T12:35:09.322272: step 189, loss 1.34487, acc 0.75, prec 0.0246844, recall 0.711957
2017-12-10T12:35:09.767706: step 190, loss 2.23332, acc 0.671875, prec 0.0247701, recall 0.713513
2017-12-10T12:35:10.201368: step 191, loss 1.96612, acc 0.6875, prec 0.0246775, recall 0.713513
2017-12-10T12:35:10.637036: step 192, loss 1.3761, acc 0.734375, prec 0.0249627, recall 0.716578
2017-12-10T12:35:11.079507: step 193, loss 1.91504, acc 0.625, prec 0.0248516, recall 0.716578
2017-12-10T12:35:11.531325: step 194, loss 1.71638, acc 0.6875, prec 0.0247598, recall 0.716578
2017-12-10T12:35:11.972266: step 195, loss 1.49836, acc 0.765625, prec 0.0250507, recall 0.719577
2017-12-10T12:35:12.423087: step 196, loss 8.08056, acc 0.6875, prec 0.0249633, recall 0.715789
2017-12-10T12:35:12.864537: step 197, loss 2.0307, acc 0.640625, prec 0.0248583, recall 0.715789
2017-12-10T12:35:13.304322: step 198, loss 1.04962, acc 0.765625, prec 0.0247904, recall 0.715789
2017-12-10T12:35:13.753027: step 199, loss 1.11226, acc 0.75, prec 0.0247183, recall 0.715789
2017-12-10T12:35:14.203642: step 200, loss 11.2759, acc 0.765625, prec 0.0246555, recall 0.712042
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-200

2017-12-10T12:35:16.118700: step 201, loss 6.17873, acc 0.84375, prec 0.0249683, recall 0.71134
2017-12-10T12:35:16.584817: step 202, loss 5.78178, acc 0.8125, prec 0.0249187, recall 0.707692
2017-12-10T12:35:17.034858: step 203, loss 1.0496, acc 0.703125, prec 0.025009, recall 0.709184
2017-12-10T12:35:17.476619: step 204, loss 2.13776, acc 0.6875, prec 0.0249193, recall 0.709184
2017-12-10T12:35:17.904320: step 205, loss 1.42303, acc 0.71875, prec 0.0250134, recall 0.71066
2017-12-10T12:35:18.336012: step 206, loss 7.06748, acc 0.65625, prec 0.0252669, recall 0.71
2017-12-10T12:35:18.780682: step 207, loss 2.42069, acc 0.546875, prec 0.0254822, recall 0.712871
2017-12-10T12:35:19.229567: step 208, loss 2.2787, acc 0.5625, prec 0.0253566, recall 0.712871
2017-12-10T12:35:19.668449: step 209, loss 2.64656, acc 0.5625, prec 0.0254029, recall 0.714286
2017-12-10T12:35:20.110503: step 210, loss 3.49833, acc 0.5, prec 0.0252613, recall 0.714286
2017-12-10T12:35:20.556635: step 211, loss 2.72358, acc 0.625, prec 0.0253252, recall 0.715686
2017-12-10T12:35:21.010795: step 212, loss 4.13187, acc 0.546875, prec 0.0255393, recall 0.714976
2017-12-10T12:35:21.463176: step 213, loss 2.40073, acc 0.59375, prec 0.0257599, recall 0.717703
2017-12-10T12:35:21.916913: step 214, loss 2.682, acc 0.609375, prec 0.0263158, recall 0.723005
2017-12-10T12:35:22.362759: step 215, loss 9.05874, acc 0.40625, prec 0.0261505, recall 0.719626
2017-12-10T12:35:22.812470: step 216, loss 3.20365, acc 0.53125, prec 0.0260179, recall 0.719626
2017-12-10T12:35:23.256916: step 217, loss 4.50522, acc 0.484375, prec 0.0260417, recall 0.717593
2017-12-10T12:35:23.709517: step 218, loss 4.00047, acc 0.546875, prec 0.0259154, recall 0.717593
2017-12-10T12:35:24.149831: step 219, loss 2.73557, acc 0.609375, prec 0.0258075, recall 0.717593
2017-12-10T12:35:24.582755: step 220, loss 3.87604, acc 0.5, prec 0.0256708, recall 0.717593
2017-12-10T12:35:25.027842: step 221, loss 3.26595, acc 0.5, prec 0.0258564, recall 0.720183
2017-12-10T12:35:25.473916: step 222, loss 2.41063, acc 0.671875, prec 0.0259271, recall 0.721461
2017-12-10T12:35:25.910748: step 223, loss 1.8775, acc 0.703125, prec 0.0258466, recall 0.721461
2017-12-10T12:35:26.351862: step 224, loss 1.56133, acc 0.703125, prec 0.0259253, recall 0.722727
2017-12-10T12:35:26.815481: step 225, loss 5.634, acc 0.703125, prec 0.0258495, recall 0.719457
2017-12-10T12:35:27.261221: step 226, loss 1.56582, acc 0.734375, prec 0.0257782, recall 0.719457
2017-12-10T12:35:27.705760: step 227, loss 2.25798, acc 0.703125, prec 0.0261712, recall 0.723214
2017-12-10T12:35:28.147250: step 228, loss 1.4239, acc 0.75, prec 0.0261038, recall 0.723214
2017-12-10T12:35:28.593545: step 229, loss 1.3271, acc 0.765625, prec 0.0261974, recall 0.724444
2017-12-10T12:35:29.039242: step 230, loss 1.89682, acc 0.671875, prec 0.0262652, recall 0.725664
2017-12-10T12:35:29.476146: step 231, loss 1.00084, acc 0.78125, prec 0.026362, recall 0.726872
2017-12-10T12:35:29.922085: step 232, loss 1.40321, acc 0.78125, prec 0.0264584, recall 0.72807
2017-12-10T12:35:30.376622: step 233, loss 1.48558, acc 0.734375, prec 0.0265416, recall 0.729258
2017-12-10T12:35:30.814179: step 234, loss 0.376899, acc 0.859375, prec 0.0266582, recall 0.730435
2017-12-10T12:35:31.262617: step 235, loss 6.77027, acc 0.8125, prec 0.0266118, recall 0.727273
2017-12-10T12:35:31.707891: step 236, loss 1.46905, acc 0.828125, prec 0.0268732, recall 0.729614
2017-12-10T12:35:32.161358: step 237, loss 1.05687, acc 0.734375, prec 0.0268012, recall 0.729614
2017-12-10T12:35:32.603372: step 238, loss 16.831, acc 0.84375, prec 0.0269164, recall 0.72766
2017-12-10T12:35:33.055058: step 239, loss 0.681309, acc 0.78125, prec 0.0268572, recall 0.72766
2017-12-10T12:35:33.528710: step 240, loss 31.1989, acc 0.78125, prec 0.027116, recall 0.720833
2017-12-10T12:35:33.921114: step 241, loss 2.16174, acc 0.703125, prec 0.0271875, recall 0.721992
2017-12-10T12:35:34.319385: step 242, loss 2.28047, acc 0.578125, prec 0.0272246, recall 0.72314
2017-12-10T12:35:34.707683: step 243, loss 4.21548, acc 0.46875, prec 0.0270814, recall 0.72314
2017-12-10T12:35:35.098048: step 244, loss 4.16335, acc 0.484375, prec 0.027393, recall 0.726531
2017-12-10T12:35:35.493071: step 245, loss 6.0693, acc 0.3125, prec 0.0275061, recall 0.728745
2017-12-10T12:35:35.952394: step 246, loss 12.4254, acc 0.40625, prec 0.0274992, recall 0.726908
2017-12-10T12:35:36.395643: step 247, loss 5.53179, acc 0.3125, prec 0.0273166, recall 0.726908
2017-12-10T12:35:36.830255: step 248, loss 5.69741, acc 0.359375, prec 0.0271486, recall 0.726908
2017-12-10T12:35:37.262863: step 249, loss 5.50795, acc 0.265625, prec 0.0271035, recall 0.728
2017-12-10T12:35:37.706854: step 250, loss 6.59115, acc 0.359375, prec 0.027083, recall 0.729084
2017-12-10T12:35:38.149476: step 251, loss 5.90605, acc 0.3125, prec 0.0270509, recall 0.730159
2017-12-10T12:35:38.579959: step 252, loss 5.89779, acc 0.375, prec 0.0268927, recall 0.730159
2017-12-10T12:35:39.009680: step 253, loss 4.40355, acc 0.453125, prec 0.0268974, recall 0.731225
2017-12-10T12:35:39.452148: step 254, loss 3.96776, acc 0.40625, prec 0.0268903, recall 0.732283
2017-12-10T12:35:39.893220: step 255, loss 4.83602, acc 0.4375, prec 0.026751, recall 0.732283
2017-12-10T12:35:40.344459: step 256, loss 2.79689, acc 0.640625, prec 0.0272206, recall 0.736434
2017-12-10T12:35:40.790436: step 257, loss 2.95293, acc 0.546875, prec 0.027108, recall 0.736434
2017-12-10T12:35:41.226586: step 258, loss 1.82312, acc 0.625, prec 0.0270155, recall 0.736434
2017-12-10T12:35:41.668423: step 259, loss 5.62826, acc 0.640625, prec 0.0269351, recall 0.730769
2017-12-10T12:35:42.118000: step 260, loss 2.04076, acc 0.640625, prec 0.0268475, recall 0.730769
2017-12-10T12:35:42.563609: step 261, loss 1.4703, acc 0.765625, prec 0.0267907, recall 0.730769
2017-12-10T12:35:43.010789: step 262, loss 1.18642, acc 0.75, prec 0.0268674, recall 0.731801
2017-12-10T12:35:43.477919: step 263, loss 0.331205, acc 0.921875, prec 0.0269852, recall 0.732824
2017-12-10T12:35:43.928232: step 264, loss 1.01709, acc 0.765625, prec 0.0270649, recall 0.73384
2017-12-10T12:35:44.383540: step 265, loss 0.436216, acc 0.890625, prec 0.0270384, recall 0.73384
2017-12-10T12:35:44.829327: step 266, loss 0.74294, acc 0.828125, prec 0.0269968, recall 0.73384
2017-12-10T12:35:45.286746: step 267, loss 2.44046, acc 0.84375, prec 0.0272346, recall 0.733083
2017-12-10T12:35:45.742713: step 268, loss 0.561435, acc 0.859375, prec 0.0272004, recall 0.733083
2017-12-10T12:35:46.187758: step 269, loss 3.43259, acc 0.828125, prec 0.0272981, recall 0.731343
2017-12-10T12:35:46.614640: step 270, loss 0.622098, acc 0.875, prec 0.0272677, recall 0.731343
2017-12-10T12:35:47.071164: step 271, loss 1.09492, acc 0.8125, prec 0.0273573, recall 0.732342
2017-12-10T12:35:47.515167: step 272, loss 0.714124, acc 0.84375, prec 0.0273194, recall 0.732342
2017-12-10T12:35:47.960256: step 273, loss 6.02638, acc 0.890625, prec 0.0272967, recall 0.72963
2017-12-10T12:35:48.417414: step 274, loss 6.07865, acc 0.71875, prec 0.0272325, recall 0.726937
2017-12-10T12:35:48.851537: step 275, loss 23.4395, acc 0.703125, prec 0.0271687, recall 0.721612
2017-12-10T12:35:49.301155: step 276, loss 13.5161, acc 0.59375, prec 0.0270753, recall 0.718978
2017-12-10T12:35:49.759873: step 277, loss 2.68232, acc 0.578125, prec 0.0269752, recall 0.718978
2017-12-10T12:35:50.207324: step 278, loss 4.16404, acc 0.53125, prec 0.0269975, recall 0.72
2017-12-10T12:35:50.647921: step 279, loss 5.23017, acc 0.328125, prec 0.0269721, recall 0.721014
2017-12-10T12:35:51.086897: step 280, loss 5.19969, acc 0.390625, prec 0.0270926, recall 0.723022
2017-12-10T12:35:51.543968: step 281, loss 6.31289, acc 0.296875, prec 0.0269293, recall 0.723022
2017-12-10T12:35:51.989789: step 282, loss 5.33956, acc 0.34375, prec 0.0270378, recall 0.725
2017-12-10T12:35:52.447931: step 283, loss 7.47806, acc 0.265625, prec 0.0269984, recall 0.725979
2017-12-10T12:35:52.884509: step 284, loss 5.80888, acc 0.328125, prec 0.0269737, recall 0.72695
2017-12-10T12:35:53.323485: step 285, loss 6.42239, acc 0.296875, prec 0.0268149, recall 0.72695
2017-12-10T12:35:53.773043: step 286, loss 6.16896, acc 0.296875, prec 0.026658, recall 0.72695
2017-12-10T12:35:54.220987: step 287, loss 7.22187, acc 0.40625, prec 0.0265303, recall 0.724382
2017-12-10T12:35:54.653983: step 288, loss 4.93274, acc 0.28125, prec 0.0263733, recall 0.724382
2017-12-10T12:35:55.091971: step 289, loss 4.32521, acc 0.40625, prec 0.026245, recall 0.724382
2017-12-10T12:35:55.526393: step 290, loss 4.13074, acc 0.4375, prec 0.0261246, recall 0.724382
2017-12-10T12:35:55.965869: step 291, loss 3.89805, acc 0.375, prec 0.0259921, recall 0.724382
2017-12-10T12:35:56.415537: step 292, loss 4.22186, acc 0.421875, prec 0.0261166, recall 0.726316
2017-12-10T12:35:56.854315: step 293, loss 2.60881, acc 0.5625, prec 0.0266365, recall 0.731034
2017-12-10T12:35:57.296528: step 294, loss 8.39926, acc 0.71875, prec 0.0265797, recall 0.728522
2017-12-10T12:35:57.743932: step 295, loss 9.09291, acc 0.609375, prec 0.0266217, recall 0.726962
2017-12-10T12:35:58.199180: step 296, loss 2.40609, acc 0.609375, prec 0.02666, recall 0.727891
2017-12-10T12:35:58.634605: step 297, loss 9.90801, acc 0.625, prec 0.0267048, recall 0.726351
2017-12-10T12:35:59.075136: step 298, loss 4.16558, acc 0.71875, prec 0.0266485, recall 0.723906
2017-12-10T12:35:59.516135: step 299, loss 1.53542, acc 0.609375, prec 0.0266864, recall 0.724832
2017-12-10T12:35:59.948099: step 300, loss 2.4102, acc 0.5625, prec 0.0268341, recall 0.726667
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-300

2017-12-10T12:36:01.864689: step 301, loss 2.98093, acc 0.609375, prec 0.0268712, recall 0.727575
2017-12-10T12:36:02.305450: step 302, loss 10.4066, acc 0.53125, prec 0.0267759, recall 0.725166
2017-12-10T12:36:02.758148: step 303, loss 2.71712, acc 0.46875, prec 0.026665, recall 0.725166
2017-12-10T12:36:03.197543: step 304, loss 2.62951, acc 0.609375, prec 0.0268204, recall 0.726974
2017-12-10T12:36:03.641123: step 305, loss 2.28774, acc 0.609375, prec 0.026857, recall 0.727869
2017-12-10T12:36:04.083354: step 306, loss 3.0031, acc 0.546875, prec 0.0267631, recall 0.727869
2017-12-10T12:36:04.529636: step 307, loss 2.19676, acc 0.671875, prec 0.0268126, recall 0.728758
2017-12-10T12:36:04.964340: step 308, loss 2.25731, acc 0.625, prec 0.0268521, recall 0.729642
2017-12-10T12:36:05.402911: step 309, loss 1.82834, acc 0.65625, prec 0.0268978, recall 0.730519
2017-12-10T12:36:05.851533: step 310, loss 1.98223, acc 0.625, prec 0.0268208, recall 0.730519
2017-12-10T12:36:06.300048: step 311, loss 7.23944, acc 0.609375, prec 0.02686, recall 0.729032
2017-12-10T12:36:06.741974: step 312, loss 24.6732, acc 0.625, prec 0.0267868, recall 0.726688
2017-12-10T12:36:07.182393: step 313, loss 2.89491, acc 0.578125, prec 0.0268163, recall 0.727564
2017-12-10T12:36:07.622809: step 314, loss 2.62574, acc 0.625, prec 0.0267405, recall 0.727564
2017-12-10T12:36:08.062694: step 315, loss 2.36033, acc 0.65625, prec 0.0269, recall 0.729299
2017-12-10T12:36:08.510915: step 316, loss 2.00968, acc 0.703125, prec 0.0270682, recall 0.731013
2017-12-10T12:36:08.986666: step 317, loss 1.67902, acc 0.71875, prec 0.027125, recall 0.731861
2017-12-10T12:36:09.445472: step 318, loss 1.43513, acc 0.6875, prec 0.0271752, recall 0.732704
2017-12-10T12:36:09.904249: step 319, loss 2.15109, acc 0.6875, prec 0.0272251, recall 0.733542
2017-12-10T12:36:10.367688: step 320, loss 1.84869, acc 0.59375, prec 0.027143, recall 0.733542
2017-12-10T12:36:10.815119: step 321, loss 1.90896, acc 0.65625, prec 0.0270739, recall 0.733542
2017-12-10T12:36:11.264750: step 322, loss 1.89378, acc 0.6875, prec 0.0270114, recall 0.733542
2017-12-10T12:36:11.706287: step 323, loss 3.42613, acc 0.734375, prec 0.0270737, recall 0.732087
2017-12-10T12:36:12.152279: step 324, loss 0.875354, acc 0.78125, prec 0.027142, recall 0.732919
2017-12-10T12:36:12.593787: step 325, loss 0.875089, acc 0.8125, prec 0.0272164, recall 0.733746
2017-12-10T12:36:13.041210: step 326, loss 2.21947, acc 0.671875, prec 0.0272623, recall 0.734568
2017-12-10T12:36:13.487189: step 327, loss 1.28276, acc 0.703125, prec 0.0272031, recall 0.734568
2017-12-10T12:36:13.932515: step 328, loss 2.02856, acc 0.640625, prec 0.0272427, recall 0.735385
2017-12-10T12:36:14.384138: step 329, loss 3.06848, acc 0.6875, prec 0.0271838, recall 0.733129
2017-12-10T12:36:14.829537: step 330, loss 0.832014, acc 0.734375, prec 0.0271313, recall 0.733129
2017-12-10T12:36:15.275806: step 331, loss 2.5523, acc 0.84375, prec 0.027214, recall 0.731707
2017-12-10T12:36:15.712626: step 332, loss 12.867, acc 0.6875, prec 0.0272686, recall 0.728097
2017-12-10T12:36:16.153530: step 333, loss 1.32002, acc 0.71875, prec 0.027323, recall 0.728916
2017-12-10T12:36:16.593959: step 334, loss 2.7657, acc 0.640625, prec 0.0273618, recall 0.72973
2017-12-10T12:36:17.039561: step 335, loss 1.70335, acc 0.75, prec 0.0274219, recall 0.730539
2017-12-10T12:36:17.483538: step 336, loss 3.23678, acc 0.59375, prec 0.027342, recall 0.730539
2017-12-10T12:36:17.939785: step 337, loss 2.41911, acc 0.53125, prec 0.027359, recall 0.731343
2017-12-10T12:36:18.401332: step 338, loss 2.49176, acc 0.53125, prec 0.0272677, recall 0.731343
2017-12-10T12:36:18.843087: step 339, loss 2.37964, acc 0.546875, prec 0.0272879, recall 0.732143
2017-12-10T12:36:19.284875: step 340, loss 3.517, acc 0.5, prec 0.0271913, recall 0.732143
2017-12-10T12:36:19.723377: step 341, loss 7.72689, acc 0.453125, prec 0.0270895, recall 0.72997
2017-12-10T12:36:20.179122: step 342, loss 3.55956, acc 0.4375, prec 0.0269826, recall 0.72997
2017-12-10T12:36:20.610984: step 343, loss 3.33951, acc 0.484375, prec 0.0268852, recall 0.72997
2017-12-10T12:36:21.044012: step 344, loss 3.27057, acc 0.5, prec 0.0267915, recall 0.72997
2017-12-10T12:36:21.489107: step 345, loss 3.9282, acc 0.46875, prec 0.0266927, recall 0.72997
2017-12-10T12:36:21.918654: step 346, loss 2.63962, acc 0.59375, prec 0.0266176, recall 0.72997
2017-12-10T12:36:22.379857: step 347, loss 3.17295, acc 0.53125, prec 0.0266365, recall 0.730769
2017-12-10T12:36:22.821313: step 348, loss 1.86448, acc 0.578125, prec 0.0265591, recall 0.730769
2017-12-10T12:36:23.266662: step 349, loss 1.71302, acc 0.671875, prec 0.0266037, recall 0.731563
2017-12-10T12:36:23.707382: step 350, loss 1.26299, acc 0.703125, prec 0.0265496, recall 0.731563
2017-12-10T12:36:24.151347: step 351, loss 1.91294, acc 0.703125, prec 0.0267037, recall 0.733138
2017-12-10T12:36:24.594384: step 352, loss 1.75431, acc 0.6875, prec 0.0266468, recall 0.733138
2017-12-10T12:36:25.042966: step 353, loss 10.5327, acc 0.71875, prec 0.0265986, recall 0.730994
2017-12-10T12:36:25.496214: step 354, loss 1.02357, acc 0.71875, prec 0.0266511, recall 0.731778
2017-12-10T12:36:25.956929: step 355, loss 3.4768, acc 0.765625, prec 0.0266115, recall 0.729651
2017-12-10T12:36:26.399520: step 356, loss 0.876933, acc 0.828125, prec 0.0266836, recall 0.730435
2017-12-10T12:36:26.840663: step 357, loss 6.3769, acc 0.78125, prec 0.0266469, recall 0.728324
2017-12-10T12:36:27.283425: step 358, loss 0.345999, acc 0.90625, prec 0.02663, recall 0.728324
2017-12-10T12:36:27.732674: step 359, loss 1.53626, acc 0.75, prec 0.0265851, recall 0.728324
2017-12-10T12:36:28.174962: step 360, loss 1.17833, acc 0.78125, prec 0.0266484, recall 0.729107
2017-12-10T12:36:28.617042: step 361, loss 5.771, acc 0.640625, prec 0.0266891, recall 0.727794
2017-12-10T12:36:29.093830: step 362, loss 1.3961, acc 0.78125, prec 0.0268541, recall 0.729345
2017-12-10T12:36:29.539558: step 363, loss 6.09862, acc 0.671875, prec 0.0268997, recall 0.728045
2017-12-10T12:36:29.976069: step 364, loss 4.6492, acc 0.703125, prec 0.0270524, recall 0.727528
2017-12-10T12:36:30.408503: step 365, loss 1.51586, acc 0.703125, prec 0.0269989, recall 0.727528
2017-12-10T12:36:30.858667: step 366, loss 2.31425, acc 0.625, prec 0.0270326, recall 0.728291
2017-12-10T12:36:31.311091: step 367, loss 3.28471, acc 0.515625, prec 0.0270466, recall 0.72905
2017-12-10T12:36:31.753515: step 368, loss 1.83092, acc 0.640625, prec 0.0271835, recall 0.730556
2017-12-10T12:36:32.207254: step 369, loss 2.4848, acc 0.625, prec 0.027417, recall 0.732782
2017-12-10T12:36:32.679738: step 370, loss 2.63264, acc 0.546875, prec 0.0274353, recall 0.733516
2017-12-10T12:36:33.111525: step 371, loss 2.79259, acc 0.515625, prec 0.0274478, recall 0.734247
2017-12-10T12:36:33.556795: step 372, loss 2.42519, acc 0.59375, prec 0.0273749, recall 0.734247
2017-12-10T12:36:33.997917: step 373, loss 10.2239, acc 0.484375, prec 0.0273847, recall 0.73297
2017-12-10T12:36:34.437216: step 374, loss 2.57444, acc 0.578125, prec 0.0274084, recall 0.733696
2017-12-10T12:36:34.881693: step 375, loss 2.67052, acc 0.578125, prec 0.0275304, recall 0.735135
2017-12-10T12:36:35.334036: step 376, loss 2.69183, acc 0.625, prec 0.0275618, recall 0.735849
2017-12-10T12:36:35.784717: step 377, loss 2.65137, acc 0.625, prec 0.0275932, recall 0.736559
2017-12-10T12:36:36.233659: step 378, loss 10.8072, acc 0.53125, prec 0.0275128, recall 0.734584
2017-12-10T12:36:36.681277: step 379, loss 2.73448, acc 0.5625, prec 0.0274357, recall 0.734584
2017-12-10T12:36:37.125870: step 380, loss 11.8984, acc 0.640625, prec 0.0273754, recall 0.73262
2017-12-10T12:36:37.577716: step 381, loss 2.68573, acc 0.609375, prec 0.0273072, recall 0.73262
2017-12-10T12:36:38.011722: step 382, loss 2.09511, acc 0.625, prec 0.0273387, recall 0.733333
2017-12-10T12:36:38.455576: step 383, loss 2.62996, acc 0.53125, prec 0.0272574, recall 0.733333
2017-12-10T12:36:38.910994: step 384, loss 4.10645, acc 0.59375, prec 0.02719, recall 0.731383
2017-12-10T12:36:39.347376: step 385, loss 2.47377, acc 0.609375, prec 0.027123, recall 0.731383
2017-12-10T12:36:39.794860: step 386, loss 3.31065, acc 0.546875, prec 0.0270456, recall 0.731383
2017-12-10T12:36:40.239117: step 387, loss 2.49724, acc 0.578125, prec 0.0270694, recall 0.732095
2017-12-10T12:36:40.688143: step 388, loss 3.28765, acc 0.546875, prec 0.0270878, recall 0.732804
2017-12-10T12:36:41.131504: step 389, loss 1.87069, acc 0.671875, prec 0.0271272, recall 0.733509
2017-12-10T12:36:41.571640: step 390, loss 2.48553, acc 0.6875, prec 0.0272639, recall 0.734908
2017-12-10T12:36:42.010294: step 391, loss 1.45745, acc 0.75, prec 0.0272215, recall 0.734908
2017-12-10T12:36:42.468155: step 392, loss 2.25455, acc 0.5625, prec 0.0271476, recall 0.734908
2017-12-10T12:36:42.920765: step 393, loss 2.10344, acc 0.75, prec 0.0271055, recall 0.734908
2017-12-10T12:36:43.358014: step 394, loss 1.24648, acc 0.75, prec 0.0271576, recall 0.735602
2017-12-10T12:36:43.808792: step 395, loss 3.43107, acc 0.734375, prec 0.0271157, recall 0.733681
2017-12-10T12:36:44.239514: step 396, loss 1.05709, acc 0.8125, prec 0.0271781, recall 0.734375
2017-12-10T12:36:44.680850: step 397, loss 6.19481, acc 0.765625, prec 0.0271415, recall 0.732468
2017-12-10T12:36:45.120483: step 398, loss 1.89399, acc 0.671875, prec 0.0271802, recall 0.733161
2017-12-10T12:36:45.561534: step 399, loss 0.826039, acc 0.84375, prec 0.0273408, recall 0.734536
2017-12-10T12:36:46.012412: step 400, loss 7.66051, acc 0.828125, prec 0.0273145, recall 0.732648
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-400

2017-12-10T12:36:47.960002: step 401, loss 1.99152, acc 0.671875, prec 0.0273527, recall 0.733333
2017-12-10T12:36:48.402397: step 402, loss 0.620771, acc 0.84375, prec 0.0273266, recall 0.733333
2017-12-10T12:36:48.845455: step 403, loss 0.972537, acc 0.796875, prec 0.0272927, recall 0.733333
2017-12-10T12:36:49.291298: step 404, loss 1.51244, acc 0.796875, prec 0.0272589, recall 0.733333
2017-12-10T12:36:49.737333: step 405, loss 1.05965, acc 0.796875, prec 0.0273177, recall 0.734015
2017-12-10T12:36:50.186435: step 406, loss 1.8676, acc 0.71875, prec 0.027271, recall 0.734015
2017-12-10T12:36:50.631269: step 407, loss 1.70997, acc 0.671875, prec 0.0272167, recall 0.734015
2017-12-10T12:36:51.081958: step 408, loss 2.27093, acc 0.703125, prec 0.0273519, recall 0.735369
2017-12-10T12:36:51.525742: step 409, loss 5.43743, acc 0.8125, prec 0.0274154, recall 0.734177
2017-12-10T12:36:51.966700: step 410, loss 1.58953, acc 0.703125, prec 0.0273662, recall 0.734177
2017-12-10T12:36:52.408824: step 411, loss 1.82147, acc 0.640625, prec 0.027307, recall 0.734177
2017-12-10T12:36:52.857357: step 412, loss 3.257, acc 0.578125, prec 0.0274204, recall 0.735516
2017-12-10T12:36:53.306149: step 413, loss 2.06898, acc 0.609375, prec 0.0274473, recall 0.736181
2017-12-10T12:36:53.748830: step 414, loss 1.33652, acc 0.671875, prec 0.0273934, recall 0.736181
2017-12-10T12:36:54.201711: step 415, loss 1.55566, acc 0.796875, prec 0.027451, recall 0.736842
2017-12-10T12:36:54.643191: step 416, loss 1.87027, acc 0.65625, prec 0.0273947, recall 0.736842
2017-12-10T12:36:55.091312: step 417, loss 1.47158, acc 0.71875, prec 0.0274393, recall 0.7375
2017-12-10T12:36:55.542506: step 418, loss 1.89441, acc 0.625, prec 0.0274684, recall 0.738155
2017-12-10T12:36:55.983125: step 419, loss 2.02906, acc 0.71875, prec 0.0276028, recall 0.739454
2017-12-10T12:36:56.427468: step 420, loss 2.36162, acc 0.734375, prec 0.0278291, recall 0.741379
2017-12-10T12:36:56.874205: step 421, loss 1.35408, acc 0.75, prec 0.027788, recall 0.741379
2017-12-10T12:36:57.327840: step 422, loss 1.34993, acc 0.75, prec 0.027747, recall 0.741379
2017-12-10T12:36:57.770807: step 423, loss 1.12628, acc 0.75, prec 0.0277062, recall 0.741379
2017-12-10T12:36:58.217770: step 424, loss 1.02905, acc 0.75, prec 0.0277548, recall 0.742015
2017-12-10T12:36:58.670635: step 425, loss 1.58428, acc 0.671875, prec 0.0277013, recall 0.742015
2017-12-10T12:36:59.111455: step 426, loss 10.7115, acc 0.765625, prec 0.0276658, recall 0.740196
2017-12-10T12:36:59.547526: step 427, loss 1.54546, acc 0.765625, prec 0.0277168, recall 0.740831
2017-12-10T12:36:59.982705: step 428, loss 0.913236, acc 0.828125, prec 0.0276889, recall 0.740831
2017-12-10T12:37:00.434473: step 429, loss 21.2896, acc 0.796875, prec 0.0277499, recall 0.737864
2017-12-10T12:37:00.881911: step 430, loss 5.77086, acc 0.78125, prec 0.027717, recall 0.736077
2017-12-10T12:37:01.328864: step 431, loss 11.4899, acc 0.75, prec 0.0277677, recall 0.73494
2017-12-10T12:37:01.776748: step 432, loss 5.52979, acc 0.703125, prec 0.0277222, recall 0.733173
2017-12-10T12:37:02.229157: step 433, loss 18.2386, acc 0.515625, prec 0.0276494, recall 0.729665
2017-12-10T12:37:02.669922: step 434, loss 3.72183, acc 0.515625, prec 0.0275719, recall 0.729665
2017-12-10T12:37:03.128483: step 435, loss 4.43199, acc 0.453125, prec 0.0274849, recall 0.729665
2017-12-10T12:37:03.575560: step 436, loss 4.13875, acc 0.375, prec 0.0273862, recall 0.729665
2017-12-10T12:37:04.015551: step 437, loss 5.71321, acc 0.328125, prec 0.0272809, recall 0.729665
2017-12-10T12:37:04.460450: step 438, loss 5.81033, acc 0.265625, prec 0.0271667, recall 0.729665
2017-12-10T12:37:04.894516: step 439, loss 7.74346, acc 0.21875, prec 0.0271325, recall 0.73031
2017-12-10T12:37:05.320751: step 440, loss 7.64702, acc 0.21875, prec 0.0270986, recall 0.730952
2017-12-10T12:37:05.747852: step 441, loss 6.9741, acc 0.234375, prec 0.0269819, recall 0.730952
2017-12-10T12:37:06.185012: step 442, loss 6.72019, acc 0.265625, prec 0.0269561, recall 0.731591
2017-12-10T12:37:06.636800: step 443, loss 6.57399, acc 0.203125, prec 0.0268363, recall 0.731591
2017-12-10T12:37:07.087569: step 444, loss 5.87513, acc 0.28125, prec 0.0267291, recall 0.731591
2017-12-10T12:37:07.537848: step 445, loss 5.20559, acc 0.359375, prec 0.0266344, recall 0.731591
2017-12-10T12:37:07.985772: step 446, loss 6.00405, acc 0.265625, prec 0.0266104, recall 0.732228
2017-12-10T12:37:08.418007: step 447, loss 2.60883, acc 0.609375, prec 0.0265532, recall 0.732228
2017-12-10T12:37:08.855534: step 448, loss 3.81332, acc 0.5, prec 0.0264804, recall 0.732228
2017-12-10T12:37:09.290317: step 449, loss 3.25966, acc 0.4375, prec 0.0264821, recall 0.732861
2017-12-10T12:37:09.732286: step 450, loss 2.38685, acc 0.625, prec 0.026428, recall 0.732861
2017-12-10T12:37:10.189669: step 451, loss 1.67251, acc 0.640625, prec 0.0264591, recall 0.733491
2017-12-10T12:37:10.630197: step 452, loss 1.51905, acc 0.765625, prec 0.0265081, recall 0.734118
2017-12-10T12:37:11.077089: step 453, loss 1.37078, acc 0.671875, prec 0.0264609, recall 0.734118
2017-12-10T12:37:11.523911: step 454, loss 1.02353, acc 0.796875, prec 0.0265142, recall 0.734742
2017-12-10T12:37:11.966637: step 455, loss 1.20077, acc 0.78125, prec 0.0266475, recall 0.735981
2017-12-10T12:37:12.416070: step 456, loss 14.4009, acc 0.84375, prec 0.0267918, recall 0.735499
2017-12-10T12:37:12.864757: step 457, loss 0.785143, acc 0.859375, prec 0.0269357, recall 0.736721
2017-12-10T12:37:13.326650: step 458, loss 2.27522, acc 0.84375, prec 0.0269153, recall 0.735023
2017-12-10T12:37:13.780776: step 459, loss 0.723072, acc 0.875, prec 0.0268971, recall 0.735023
2017-12-10T12:37:14.241281: step 460, loss 0.373851, acc 0.90625, prec 0.0268835, recall 0.735023
2017-12-10T12:37:14.676862: step 461, loss 0.795995, acc 0.890625, prec 0.0269496, recall 0.735632
2017-12-10T12:37:15.130563: step 462, loss 8.2458, acc 0.890625, prec 0.026936, recall 0.733945
2017-12-10T12:37:15.578155: step 463, loss 16.0581, acc 0.890625, prec 0.0269224, recall 0.732265
2017-12-10T12:37:16.026558: step 464, loss 1.20101, acc 0.796875, prec 0.0269748, recall 0.732877
2017-12-10T12:37:16.475982: step 465, loss 7.55156, acc 0.9375, prec 0.026968, recall 0.731207
2017-12-10T12:37:16.916336: step 466, loss 0.384135, acc 0.9375, prec 0.0270406, recall 0.731818
2017-12-10T12:37:17.370170: step 467, loss 3.25818, acc 0.75, prec 0.0270066, recall 0.730159
2017-12-10T12:37:17.811546: step 468, loss 16.412, acc 0.828125, prec 0.0270678, recall 0.727477
2017-12-10T12:37:18.255339: step 469, loss 1.18624, acc 0.75, prec 0.0270316, recall 0.727477
2017-12-10T12:37:18.702047: step 470, loss 1.77726, acc 0.796875, prec 0.0270835, recall 0.72809
2017-12-10T12:37:19.140392: step 471, loss 2.81384, acc 0.5625, prec 0.0271014, recall 0.7287
2017-12-10T12:37:19.587696: step 472, loss 2.92168, acc 0.609375, prec 0.027045, recall 0.7287
2017-12-10T12:37:20.033702: step 473, loss 3.40257, acc 0.546875, prec 0.0270607, recall 0.729306
2017-12-10T12:37:20.501342: step 474, loss 3.74047, acc 0.4375, prec 0.0271411, recall 0.730512
2017-12-10T12:37:20.941156: step 475, loss 3.32466, acc 0.59375, prec 0.0271631, recall 0.731111
2017-12-10T12:37:21.392257: step 476, loss 4.05309, acc 0.484375, prec 0.0270893, recall 0.731111
2017-12-10T12:37:21.847687: step 477, loss 4.52212, acc 0.453125, prec 0.0271712, recall 0.732301
2017-12-10T12:37:22.288547: step 478, loss 2.28678, acc 0.5625, prec 0.0272683, recall 0.73348
2017-12-10T12:37:22.732065: step 479, loss 3.19766, acc 0.421875, prec 0.0271859, recall 0.73348
2017-12-10T12:37:23.182135: step 480, loss 4.01046, acc 0.375, prec 0.0270974, recall 0.73348
2017-12-10T12:37:23.631179: step 481, loss 3.13349, acc 0.515625, prec 0.0270292, recall 0.73348
2017-12-10T12:37:24.064057: step 482, loss 2.65081, acc 0.5625, prec 0.0269679, recall 0.73348
2017-12-10T12:37:24.520926: step 483, loss 2.08063, acc 0.640625, prec 0.0269178, recall 0.73348
2017-12-10T12:37:24.977569: step 484, loss 2.99351, acc 0.640625, prec 0.0270248, recall 0.734649
2017-12-10T12:37:25.431249: step 485, loss 1.96896, acc 0.6875, prec 0.027138, recall 0.735808
2017-12-10T12:37:25.867133: step 486, loss 20.7326, acc 0.65625, prec 0.0270944, recall 0.732609
2017-12-10T12:37:26.318100: step 487, loss 1.59971, acc 0.734375, prec 0.0271355, recall 0.733189
2017-12-10T12:37:26.755905: step 488, loss 1.86908, acc 0.640625, prec 0.0270855, recall 0.733189
2017-12-10T12:37:27.196637: step 489, loss 2.23564, acc 0.65625, prec 0.0271157, recall 0.733766
2017-12-10T12:37:27.649217: step 490, loss 8.10882, acc 0.65625, prec 0.0271479, recall 0.732759
2017-12-10T12:37:28.102819: step 491, loss 1.65547, acc 0.6875, prec 0.0271821, recall 0.733333
2017-12-10T12:37:28.549458: step 492, loss 1.24388, acc 0.8125, prec 0.0271562, recall 0.733333
2017-12-10T12:37:28.991824: step 493, loss 3.98521, acc 0.765625, prec 0.0272033, recall 0.732334
2017-12-10T12:37:29.448226: step 494, loss 2.28828, acc 0.578125, prec 0.027145, recall 0.732334
2017-12-10T12:37:29.891702: step 495, loss 1.92716, acc 0.6875, prec 0.0271791, recall 0.732906
2017-12-10T12:37:30.338524: step 496, loss 6.12804, acc 0.671875, prec 0.027213, recall 0.731915
2017-12-10T12:37:30.754703: step 497, loss 2.44213, acc 0.519231, prec 0.0271593, recall 0.731915
2017-12-10T12:37:31.224738: step 498, loss 1.65673, acc 0.640625, prec 0.0273401, recall 0.733615
2017-12-10T12:37:31.670757: step 499, loss 1.71951, acc 0.65625, prec 0.0274457, recall 0.734737
2017-12-10T12:37:32.119549: step 500, loss 2.23123, acc 0.609375, prec 0.0273919, recall 0.734737
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-500

2017-12-10T12:37:33.999148: step 501, loss 3.11771, acc 0.515625, prec 0.0273254, recall 0.734737
2017-12-10T12:37:34.438797: step 502, loss 2.48846, acc 0.640625, prec 0.0273523, recall 0.735294
2017-12-10T12:37:34.877908: step 503, loss 2.7389, acc 0.59375, prec 0.0274485, recall 0.736402
2017-12-10T12:37:35.335998: step 504, loss 2.81924, acc 0.59375, prec 0.0274687, recall 0.736952
2017-12-10T12:37:35.768705: step 505, loss 2.63665, acc 0.59375, prec 0.0274132, recall 0.736952
2017-12-10T12:37:36.214492: step 506, loss 2.39768, acc 0.625, prec 0.0273622, recall 0.736952
2017-12-10T12:37:36.663872: step 507, loss 1.9151, acc 0.609375, prec 0.0273093, recall 0.736952
2017-12-10T12:37:37.100965: step 508, loss 2.14776, acc 0.703125, prec 0.0272692, recall 0.736952
2017-12-10T12:37:37.538497: step 509, loss 1.66807, acc 0.75, prec 0.0273856, recall 0.738046
2017-12-10T12:37:37.975089: step 510, loss 2.38107, acc 0.6875, prec 0.0274933, recall 0.73913
2017-12-10T12:37:38.419421: step 511, loss 1.62473, acc 0.734375, prec 0.0274573, recall 0.73913
2017-12-10T12:37:38.867431: step 512, loss 0.968212, acc 0.859375, prec 0.0274383, recall 0.73913
2017-12-10T12:37:39.313637: step 513, loss 1.35936, acc 0.78125, prec 0.0274088, recall 0.73913
2017-12-10T12:37:39.751333: step 514, loss 14.0462, acc 0.8125, prec 0.0274603, recall 0.738144
2017-12-10T12:37:40.212339: step 515, loss 0.783536, acc 0.828125, prec 0.0275117, recall 0.738683
2017-12-10T12:37:40.655410: step 516, loss 0.763541, acc 0.84375, prec 0.0275651, recall 0.73922
2017-12-10T12:37:41.099583: step 517, loss 0.729331, acc 0.8125, prec 0.0276142, recall 0.739754
2017-12-10T12:37:41.559220: step 518, loss 0.435151, acc 0.875, prec 0.0275973, recall 0.739754
2017-12-10T12:37:41.992676: step 519, loss 0.55468, acc 0.875, prec 0.0276547, recall 0.740286
2017-12-10T12:37:42.441097: step 520, loss 0.765594, acc 0.84375, prec 0.0276336, recall 0.740286
2017-12-10T12:37:42.899696: step 521, loss 2.72702, acc 0.859375, prec 0.0276909, recall 0.739308
2017-12-10T12:37:43.336586: step 522, loss 2.5443, acc 0.859375, prec 0.0277481, recall 0.738337
2017-12-10T12:37:43.784803: step 523, loss 0.73, acc 0.84375, prec 0.027727, recall 0.738337
2017-12-10T12:37:44.223734: step 524, loss 9.84904, acc 0.84375, prec 0.027708, recall 0.736842
2017-12-10T12:37:44.680807: step 525, loss 0.900639, acc 0.8125, prec 0.0276827, recall 0.736842
2017-12-10T12:37:45.137610: step 526, loss 1.62882, acc 0.8125, prec 0.0278052, recall 0.737903
2017-12-10T12:37:45.576866: step 527, loss 1.25395, acc 0.734375, prec 0.0277693, recall 0.737903
2017-12-10T12:37:46.030489: step 528, loss 1.14798, acc 0.796875, prec 0.0278894, recall 0.738956
2017-12-10T12:37:46.482079: step 529, loss 0.963085, acc 0.765625, prec 0.0279313, recall 0.739479
2017-12-10T12:37:46.930526: step 530, loss 1.2581, acc 0.8125, prec 0.0279794, recall 0.74
2017-12-10T12:37:47.378280: step 531, loss 1.30443, acc 0.703125, prec 0.0279393, recall 0.74
2017-12-10T12:37:47.825373: step 532, loss 1.6763, acc 0.703125, prec 0.0279726, recall 0.740519
2017-12-10T12:37:48.270768: step 533, loss 1.43192, acc 0.703125, prec 0.0280057, recall 0.741036
2017-12-10T12:37:48.712992: step 534, loss 1.07221, acc 0.828125, prec 0.0279825, recall 0.741036
2017-12-10T12:37:49.173333: step 535, loss 0.618889, acc 0.796875, prec 0.0280283, recall 0.741551
2017-12-10T12:37:49.597008: step 536, loss 2.01221, acc 0.6875, prec 0.0279862, recall 0.741551
2017-12-10T12:37:50.046963: step 537, loss 1.92511, acc 0.6875, prec 0.0279443, recall 0.741551
2017-12-10T12:37:50.477559: step 538, loss 1.22488, acc 0.828125, prec 0.0279213, recall 0.741551
2017-12-10T12:37:50.926075: step 539, loss 1.21836, acc 0.765625, prec 0.0278899, recall 0.741551
2017-12-10T12:37:51.371252: step 540, loss 1.03422, acc 0.828125, prec 0.0280123, recall 0.742574
2017-12-10T12:37:51.816128: step 541, loss 0.841337, acc 0.8125, prec 0.0280597, recall 0.743083
2017-12-10T12:37:52.261602: step 542, loss 0.707549, acc 0.8125, prec 0.0281071, recall 0.74359
2017-12-10T12:37:52.681437: step 543, loss 10.5106, acc 0.796875, prec 0.0281543, recall 0.742633
2017-12-10T12:37:53.134882: step 544, loss 0.527938, acc 0.890625, prec 0.0281397, recall 0.742633
2017-12-10T12:37:53.581709: step 545, loss 3.44655, acc 0.796875, prec 0.0281868, recall 0.741683
2017-12-10T12:37:54.044230: step 546, loss 0.572568, acc 0.890625, prec 0.0281722, recall 0.741683
2017-12-10T12:37:54.478948: step 547, loss 1.95354, acc 0.828125, prec 0.0282213, recall 0.742188
2017-12-10T12:37:54.928379: step 548, loss 0.718813, acc 0.8125, prec 0.0282683, recall 0.74269
2017-12-10T12:37:55.404425: step 549, loss 1.14194, acc 0.71875, prec 0.0283746, recall 0.743689
2017-12-10T12:37:55.845751: step 550, loss 1.27248, acc 0.734375, prec 0.0283389, recall 0.743689
2017-12-10T12:37:56.286988: step 551, loss 2.24761, acc 0.75, prec 0.0283772, recall 0.744186
2017-12-10T12:37:56.730246: step 552, loss 1.34878, acc 0.734375, prec 0.0284133, recall 0.744681
2017-12-10T12:37:57.179408: step 553, loss 1.19599, acc 0.765625, prec 0.0284535, recall 0.745174
2017-12-10T12:37:57.623392: step 554, loss 1.01926, acc 0.765625, prec 0.0284936, recall 0.745665
2017-12-10T12:37:58.076167: step 555, loss 9.34451, acc 0.84375, prec 0.0284747, recall 0.744231
2017-12-10T12:37:58.526668: step 556, loss 1.18908, acc 0.71875, prec 0.0284371, recall 0.744231
2017-12-10T12:37:58.970713: step 557, loss 1.2355, acc 0.8125, prec 0.0284833, recall 0.744722
2017-12-10T12:37:59.418340: step 558, loss 1.07035, acc 0.8125, prec 0.0285295, recall 0.745211
2017-12-10T12:37:59.849423: step 559, loss 1.33186, acc 0.703125, prec 0.0286321, recall 0.746183
2017-12-10T12:38:00.296612: step 560, loss 1.76403, acc 0.640625, prec 0.028655, recall 0.746667
2017-12-10T12:38:00.761967: step 561, loss 1.17848, acc 0.765625, prec 0.0287654, recall 0.747628
2017-12-10T12:38:01.205976: step 562, loss 1.13986, acc 0.78125, prec 0.0288069, recall 0.748106
2017-12-10T12:38:01.671011: step 563, loss 1.02975, acc 0.765625, prec 0.0287754, recall 0.748106
2017-12-10T12:38:02.109384: step 564, loss 1.00354, acc 0.78125, prec 0.0288874, recall 0.749057
2017-12-10T12:38:02.557698: step 565, loss 8.59311, acc 0.78125, prec 0.0291424, recall 0.749533
2017-12-10T12:38:03.008637: step 566, loss 1.33923, acc 0.71875, prec 0.0291044, recall 0.749533
2017-12-10T12:38:03.455278: step 567, loss 1.77905, acc 0.71875, prec 0.0290664, recall 0.749533
2017-12-10T12:38:03.897335: step 568, loss 1.76875, acc 0.6875, prec 0.0290946, recall 0.75
2017-12-10T12:38:04.344902: step 569, loss 2.51713, acc 0.703125, prec 0.0291269, recall 0.749071
2017-12-10T12:38:04.784169: step 570, loss 1.64412, acc 0.78125, prec 0.0292377, recall 0.75
2017-12-10T12:38:05.236190: step 571, loss 0.980683, acc 0.734375, prec 0.0292018, recall 0.75
2017-12-10T12:38:05.678404: step 572, loss 1.32223, acc 0.671875, prec 0.0291577, recall 0.75
2017-12-10T12:38:06.109683: step 573, loss 2.11259, acc 0.671875, prec 0.0291137, recall 0.75
2017-12-10T12:38:06.538795: step 574, loss 2.14492, acc 0.734375, prec 0.0292175, recall 0.750923
2017-12-10T12:38:06.985035: step 575, loss 1.39914, acc 0.671875, prec 0.0292431, recall 0.751381
2017-12-10T12:38:07.423884: step 576, loss 0.928991, acc 0.75, prec 0.0292791, recall 0.751838
2017-12-10T12:38:07.859986: step 577, loss 4.26637, acc 0.625, prec 0.029231, recall 0.750459
2017-12-10T12:38:08.300142: step 578, loss 1.66778, acc 0.703125, prec 0.0291914, recall 0.750459
2017-12-10T12:38:08.728706: step 579, loss 1.17494, acc 0.75, prec 0.0292965, recall 0.751371
2017-12-10T12:38:09.169173: step 580, loss 1.09614, acc 0.734375, prec 0.029261, recall 0.751371
2017-12-10T12:38:09.609993: step 581, loss 1.47389, acc 0.765625, prec 0.0292988, recall 0.751825
2017-12-10T12:38:10.051389: step 582, loss 1.8307, acc 0.6875, prec 0.0293261, recall 0.752277
2017-12-10T12:38:10.497956: step 583, loss 0.870097, acc 0.828125, prec 0.0295098, recall 0.753623
2017-12-10T12:38:10.942519: step 584, loss 2.02319, acc 0.6875, prec 0.029468, recall 0.753623
2017-12-10T12:38:11.387497: step 585, loss 1.64648, acc 0.71875, prec 0.0294305, recall 0.753623
2017-12-10T12:38:11.819816: step 586, loss 1.60146, acc 0.734375, prec 0.0293951, recall 0.753623
2017-12-10T12:38:12.259082: step 587, loss 1.46526, acc 0.734375, prec 0.0294284, recall 0.754069
2017-12-10T12:38:12.708535: step 588, loss 0.772046, acc 0.796875, prec 0.0294698, recall 0.754513
2017-12-10T12:38:13.150120: step 589, loss 0.658266, acc 0.8125, prec 0.0294449, recall 0.754513
2017-12-10T12:38:13.600339: step 590, loss 1.22663, acc 0.78125, prec 0.0294159, recall 0.754513
2017-12-10T12:38:14.043801: step 591, loss 0.759159, acc 0.8125, prec 0.0293911, recall 0.754513
2017-12-10T12:38:14.492553: step 592, loss 0.376634, acc 0.859375, prec 0.0295089, recall 0.755396
2017-12-10T12:38:14.960045: step 593, loss 6.83592, acc 0.84375, prec 0.0294902, recall 0.75404
2017-12-10T12:38:15.402443: step 594, loss 0.305645, acc 0.84375, prec 0.0294695, recall 0.75404
2017-12-10T12:38:15.838625: step 595, loss 0.802392, acc 0.84375, prec 0.0294489, recall 0.75404
2017-12-10T12:38:16.272730: step 596, loss 0.9543, acc 0.859375, prec 0.0294983, recall 0.75448
2017-12-10T12:38:16.721838: step 597, loss 0.678507, acc 0.828125, prec 0.0295435, recall 0.754919
2017-12-10T12:38:17.159842: step 598, loss 1.23539, acc 0.828125, prec 0.0297244, recall 0.756228
2017-12-10T12:38:17.610080: step 599, loss 9.74792, acc 0.8125, prec 0.0297694, recall 0.755319
2017-12-10T12:38:18.063581: step 600, loss 0.953659, acc 0.84375, prec 0.0298164, recall 0.755752
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-600

2017-12-10T12:38:19.949666: step 601, loss 0.44497, acc 0.921875, prec 0.0298737, recall 0.756184
2017-12-10T12:38:20.394726: step 602, loss 1.26143, acc 0.75, prec 0.029908, recall 0.756614
2017-12-10T12:38:20.835188: step 603, loss 0.713509, acc 0.828125, prec 0.0299526, recall 0.757042
2017-12-10T12:38:21.271746: step 604, loss 0.367359, acc 0.84375, prec 0.0301343, recall 0.758319
2017-12-10T12:38:21.714608: step 605, loss 1.41468, acc 0.796875, prec 0.0301071, recall 0.758319
2017-12-10T12:38:22.149066: step 606, loss 1.63019, acc 0.796875, prec 0.030282, recall 0.759582
2017-12-10T12:38:22.595662: step 607, loss 0.82149, acc 0.75, prec 0.0303156, recall 0.76
2017-12-10T12:38:23.044400: step 608, loss 0.452774, acc 0.875, prec 0.0303661, recall 0.760417
2017-12-10T12:38:23.474012: step 609, loss 0.930675, acc 0.84375, prec 0.0304122, recall 0.760832
2017-12-10T12:38:23.919326: step 610, loss 1.41111, acc 0.796875, prec 0.0305861, recall 0.762069
2017-12-10T12:38:24.360282: step 611, loss 0.543418, acc 0.890625, prec 0.0307054, recall 0.762887
2017-12-10T12:38:24.809392: step 612, loss 2.08765, acc 0.8125, prec 0.0306821, recall 0.761578
2017-12-10T12:38:25.284788: step 613, loss 1.04841, acc 0.828125, prec 0.0306587, recall 0.761578
2017-12-10T12:38:25.725563: step 614, loss 1.31802, acc 0.75, prec 0.0306918, recall 0.761986
2017-12-10T12:38:26.177057: step 615, loss 0.525058, acc 0.890625, prec 0.0308106, recall 0.762799
2017-12-10T12:38:26.632285: step 616, loss 1.05512, acc 0.8125, prec 0.0309853, recall 0.764007
2017-12-10T12:38:27.072533: step 617, loss 2.05364, acc 0.640625, prec 0.0309363, recall 0.764007
2017-12-10T12:38:27.517285: step 618, loss 9.10169, acc 0.765625, prec 0.0309731, recall 0.763113
2017-12-10T12:38:27.958783: step 619, loss 1.1651, acc 0.796875, prec 0.031012, recall 0.763514
2017-12-10T12:38:28.412116: step 620, loss 1.46179, acc 0.75, prec 0.0311108, recall 0.76431
2017-12-10T12:38:28.865807: step 621, loss 1.27132, acc 0.796875, prec 0.0312158, recall 0.765101
2017-12-10T12:38:29.304080: step 622, loss 1.07832, acc 0.765625, prec 0.0311838, recall 0.765101
2017-12-10T12:38:29.758560: step 623, loss 1.02827, acc 0.8125, prec 0.0312244, recall 0.765494
2017-12-10T12:38:30.211179: step 624, loss 1.12252, acc 0.75, prec 0.0312564, recall 0.765886
2017-12-10T12:38:30.657338: step 625, loss 0.547803, acc 0.828125, prec 0.031233, recall 0.765886
2017-12-10T12:38:31.102209: step 626, loss 0.691657, acc 0.84375, prec 0.0312117, recall 0.765886
2017-12-10T12:38:31.550713: step 627, loss 1.01653, acc 0.765625, prec 0.0313117, recall 0.766667
2017-12-10T12:38:31.989235: step 628, loss 0.541796, acc 0.796875, prec 0.0313499, recall 0.767055
2017-12-10T12:38:32.436785: step 629, loss 0.834618, acc 0.875, prec 0.0313328, recall 0.767055
2017-12-10T12:38:32.867384: step 630, loss 0.888439, acc 0.828125, prec 0.0313094, recall 0.767055
2017-12-10T12:38:33.322778: step 631, loss 0.926284, acc 0.796875, prec 0.0312818, recall 0.767055
2017-12-10T12:38:33.768740: step 632, loss 0.627486, acc 0.859375, prec 0.0312627, recall 0.767055
2017-12-10T12:38:34.212998: step 633, loss 0.803218, acc 0.8125, prec 0.0312373, recall 0.767055
2017-12-10T12:38:34.648668: step 634, loss 0.946368, acc 0.8125, prec 0.0312119, recall 0.767055
2017-12-10T12:38:35.088910: step 635, loss 0.851412, acc 0.78125, prec 0.0311824, recall 0.767055
2017-12-10T12:38:35.534501: step 636, loss 0.322462, acc 0.90625, prec 0.0311697, recall 0.767055
2017-12-10T12:38:35.980886: step 637, loss 0.266655, acc 0.921875, prec 0.0311592, recall 0.767055
2017-12-10T12:38:36.429317: step 638, loss 26.0791, acc 0.875, prec 0.0312796, recall 0.764026
2017-12-10T12:38:36.885234: step 639, loss 0.572986, acc 0.859375, prec 0.0312606, recall 0.764026
2017-12-10T12:38:37.333046: step 640, loss 0.261488, acc 0.90625, prec 0.0312479, recall 0.764026
2017-12-10T12:38:37.767501: step 641, loss 1.36954, acc 0.734375, prec 0.031408, recall 0.765189
2017-12-10T12:38:38.212142: step 642, loss 0.326288, acc 0.875, prec 0.0314563, recall 0.765574
2017-12-10T12:38:38.662696: step 643, loss 1.10595, acc 0.8125, prec 0.0316264, recall 0.766721
2017-12-10T12:38:39.100945: step 644, loss 0.813341, acc 0.78125, prec 0.0315966, recall 0.766721
2017-12-10T12:38:39.559211: step 645, loss 1.37899, acc 0.75, prec 0.0315627, recall 0.766721
2017-12-10T12:38:40.012541: step 646, loss 1.264, acc 0.75, prec 0.0315288, recall 0.766721
2017-12-10T12:38:40.465219: step 647, loss 0.89342, acc 0.78125, prec 0.031629, recall 0.76748
2017-12-10T12:38:40.919591: step 648, loss 0.891639, acc 0.796875, prec 0.0316663, recall 0.767857
2017-12-10T12:38:41.364094: step 649, loss 0.727278, acc 0.8125, prec 0.0316409, recall 0.767857
2017-12-10T12:38:41.803834: step 650, loss 1.33372, acc 0.8125, prec 0.0316803, recall 0.768233
2017-12-10T12:38:42.253588: step 651, loss 1.1424, acc 0.828125, prec 0.0317217, recall 0.768608
2017-12-10T12:38:42.696467: step 652, loss 1.3965, acc 0.765625, prec 0.0318191, recall 0.769355
2017-12-10T12:38:43.142558: step 653, loss 0.634758, acc 0.796875, prec 0.0317915, recall 0.769355
2017-12-10T12:38:43.578793: step 654, loss 0.822314, acc 0.796875, prec 0.031764, recall 0.769355
2017-12-10T12:38:44.019047: step 655, loss 0.890104, acc 0.859375, prec 0.0318738, recall 0.770096
2017-12-10T12:38:44.455160: step 656, loss 1.0305, acc 0.796875, prec 0.0318463, recall 0.770096
2017-12-10T12:38:44.890586: step 657, loss 1.26504, acc 0.734375, prec 0.0318103, recall 0.770096
2017-12-10T12:38:45.324585: step 658, loss 0.666897, acc 0.84375, prec 0.0319177, recall 0.770833
2017-12-10T12:38:45.765638: step 659, loss 1.00666, acc 0.796875, prec 0.0320186, recall 0.771565
2017-12-10T12:38:46.212890: step 660, loss 2.36394, acc 0.90625, prec 0.0320721, recall 0.770701
2017-12-10T12:38:46.663304: step 661, loss 0.904824, acc 0.828125, prec 0.0320487, recall 0.770701
2017-12-10T12:38:47.107652: step 662, loss 0.534647, acc 0.8125, prec 0.0320873, recall 0.771065
2017-12-10T12:38:47.549547: step 663, loss 0.352902, acc 0.890625, prec 0.0320725, recall 0.771065
2017-12-10T12:38:47.995367: step 664, loss 1.66917, acc 0.890625, prec 0.0321856, recall 0.771791
2017-12-10T12:38:48.444010: step 665, loss 0.712404, acc 0.90625, prec 0.0323007, recall 0.772512
2017-12-10T12:38:48.886639: step 666, loss 1.11524, acc 0.734375, prec 0.0323283, recall 0.772871
2017-12-10T12:38:49.327112: step 667, loss 0.346246, acc 0.921875, prec 0.0323176, recall 0.772871
2017-12-10T12:38:49.768518: step 668, loss 0.635043, acc 0.796875, prec 0.03229, recall 0.772871
2017-12-10T12:38:50.209376: step 669, loss 1.22789, acc 0.84375, prec 0.0323324, recall 0.773228
2017-12-10T12:38:50.655967: step 670, loss 1.18714, acc 0.75, prec 0.0324257, recall 0.77394
2017-12-10T12:38:51.108005: step 671, loss 0.451346, acc 0.84375, prec 0.0324044, recall 0.77394
2017-12-10T12:38:51.543384: step 672, loss 0.710561, acc 0.796875, prec 0.0323767, recall 0.77394
2017-12-10T12:38:51.980426: step 673, loss 0.587706, acc 0.890625, prec 0.0324888, recall 0.774648
2017-12-10T12:38:52.435377: step 674, loss 27.2819, acc 0.96875, prec 0.0326137, recall 0.774143
2017-12-10T12:38:52.885532: step 675, loss 0.520187, acc 0.828125, prec 0.0326536, recall 0.774495
2017-12-10T12:38:53.325483: step 676, loss 2.00587, acc 0.90625, prec 0.0326429, recall 0.773292
2017-12-10T12:38:53.780834: step 677, loss 0.63382, acc 0.796875, prec 0.0326785, recall 0.773643
2017-12-10T12:38:54.223076: step 678, loss 1.02708, acc 0.765625, prec 0.0327097, recall 0.773994
2017-12-10T12:38:54.672077: step 679, loss 1.04258, acc 0.734375, prec 0.0326733, recall 0.773994
2017-12-10T12:38:55.118569: step 680, loss 1.54209, acc 0.703125, prec 0.0326959, recall 0.774343
2017-12-10T12:38:55.565520: step 681, loss 1.24873, acc 0.75, prec 0.032788, recall 0.775039
2017-12-10T12:38:56.013938: step 682, loss 1.4091, acc 0.71875, prec 0.0327495, recall 0.775039
2017-12-10T12:38:56.464677: step 683, loss 1.82888, acc 0.609375, prec 0.0327592, recall 0.775385
2017-12-10T12:38:56.908428: step 684, loss 1.23031, acc 0.75, prec 0.0327879, recall 0.77573
2017-12-10T12:38:57.362546: step 685, loss 1.34349, acc 0.796875, prec 0.0328858, recall 0.776417
2017-12-10T12:38:57.813693: step 686, loss 0.438574, acc 0.859375, prec 0.0328666, recall 0.776417
2017-12-10T12:38:58.271291: step 687, loss 0.73513, acc 0.859375, prec 0.0328474, recall 0.776417
2017-12-10T12:38:58.719283: step 688, loss 2.40033, acc 0.8125, prec 0.032824, recall 0.775229
2017-12-10T12:38:59.157440: step 689, loss 0.857922, acc 0.78125, prec 0.0327943, recall 0.775229
2017-12-10T12:38:59.611536: step 690, loss 0.750567, acc 0.765625, prec 0.0327625, recall 0.775229
2017-12-10T12:39:00.053289: step 691, loss 1.7197, acc 0.78125, prec 0.0327954, recall 0.775573
2017-12-10T12:39:00.491319: step 692, loss 0.810401, acc 0.84375, prec 0.0328366, recall 0.775915
2017-12-10T12:39:00.936569: step 693, loss 2.22768, acc 0.78125, prec 0.0328091, recall 0.774734
2017-12-10T12:39:01.402111: step 694, loss 0.724521, acc 0.828125, prec 0.0328481, recall 0.775076
2017-12-10T12:39:01.852572: step 695, loss 0.732771, acc 0.828125, prec 0.0328249, recall 0.775076
2017-12-10T12:39:02.307013: step 696, loss 1.53031, acc 0.796875, prec 0.0328596, recall 0.775417
2017-12-10T12:39:02.746714: step 697, loss 1.85012, acc 0.78125, prec 0.0329543, recall 0.776097
2017-12-10T12:39:03.190225: step 698, loss 1.0022, acc 0.796875, prec 0.033051, recall 0.776772
2017-12-10T12:39:03.643717: step 699, loss 2.13324, acc 0.78125, prec 0.0330234, recall 0.775602
2017-12-10T12:39:04.083572: step 700, loss 1.01473, acc 0.78125, prec 0.0329938, recall 0.775602
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-700

2017-12-10T12:39:06.117016: step 701, loss 2.20428, acc 0.78125, prec 0.033088, recall 0.776276
2017-12-10T12:39:06.549848: step 702, loss 1.1089, acc 0.765625, prec 0.0331181, recall 0.776612
2017-12-10T12:39:06.996072: step 703, loss 0.771035, acc 0.828125, prec 0.0330948, recall 0.776612
2017-12-10T12:39:07.467114: step 704, loss 1.51342, acc 0.71875, prec 0.0330568, recall 0.776612
2017-12-10T12:39:07.912011: step 705, loss 1.34484, acc 0.796875, prec 0.0330294, recall 0.776612
2017-12-10T12:39:08.369367: step 706, loss 9.7978, acc 0.703125, prec 0.0329915, recall 0.775449
2017-12-10T12:39:08.821868: step 707, loss 1.66826, acc 0.703125, prec 0.0330132, recall 0.775785
2017-12-10T12:39:09.267397: step 708, loss 1.33855, acc 0.703125, prec 0.0330347, recall 0.776119
2017-12-10T12:39:09.713681: step 709, loss 1.49659, acc 0.703125, prec 0.0330563, recall 0.776453
2017-12-10T12:39:10.150069: step 710, loss 0.958655, acc 0.765625, prec 0.0330861, recall 0.776786
2017-12-10T12:39:10.594990: step 711, loss 2.20894, acc 0.625, prec 0.0330359, recall 0.776786
2017-12-10T12:39:11.037132: step 712, loss 1.74449, acc 0.625, prec 0.0329858, recall 0.776786
2017-12-10T12:39:11.481124: step 713, loss 1.79945, acc 0.625, prec 0.0329968, recall 0.777117
2017-12-10T12:39:11.915398: step 714, loss 1.20341, acc 0.71875, prec 0.0329594, recall 0.777117
2017-12-10T12:39:12.360220: step 715, loss 1.12079, acc 0.78125, prec 0.0329304, recall 0.777117
2017-12-10T12:39:12.799516: step 716, loss 1.05818, acc 0.71875, prec 0.0329539, recall 0.777448
2017-12-10T12:39:13.250903: step 717, loss 0.957082, acc 0.75, prec 0.0329208, recall 0.777448
2017-12-10T12:39:13.684284: step 718, loss 0.782929, acc 0.8125, prec 0.032896, recall 0.777448
2017-12-10T12:39:14.127554: step 719, loss 0.859319, acc 0.78125, prec 0.0328671, recall 0.777448
2017-12-10T12:39:14.585788: step 720, loss 0.44196, acc 0.90625, prec 0.0328547, recall 0.777448
2017-12-10T12:39:15.048707: step 721, loss 0.993589, acc 0.875, prec 0.0328989, recall 0.777778
2017-12-10T12:39:15.485998: step 722, loss 1.30349, acc 0.890625, prec 0.0330056, recall 0.778434
2017-12-10T12:39:15.938272: step 723, loss 0.526184, acc 0.859375, prec 0.0330475, recall 0.778761
2017-12-10T12:39:16.387641: step 724, loss 0.623334, acc 0.84375, prec 0.0330268, recall 0.778761
2017-12-10T12:39:16.829477: step 725, loss 0.362659, acc 0.90625, prec 0.0330144, recall 0.778761
2017-12-10T12:39:17.281532: step 726, loss 0.626553, acc 0.859375, prec 0.0331167, recall 0.779412
2017-12-10T12:39:17.725616: step 727, loss 0.418381, acc 0.921875, prec 0.0331064, recall 0.779412
2017-12-10T12:39:18.171033: step 728, loss 10.202, acc 0.84375, prec 0.0331481, recall 0.778592
2017-12-10T12:39:18.606595: step 729, loss 4.61366, acc 0.859375, prec 0.0331316, recall 0.777452
2017-12-10T12:39:19.070991: step 730, loss 0.461917, acc 0.9375, prec 0.0331836, recall 0.777778
2017-12-10T12:39:19.512483: step 731, loss 0.695199, acc 0.828125, prec 0.0332814, recall 0.778426
2017-12-10T12:39:19.960771: step 732, loss 5.57238, acc 0.890625, prec 0.0333292, recall 0.777616
2017-12-10T12:39:20.417607: step 733, loss 0.813015, acc 0.75, prec 0.0334163, recall 0.778261
2017-12-10T12:39:20.854587: step 734, loss 0.483085, acc 0.859375, prec 0.0334577, recall 0.778582
2017-12-10T12:39:21.295072: step 735, loss 3.56469, acc 0.796875, prec 0.0336129, recall 0.778417
2017-12-10T12:39:21.744621: step 736, loss 1.12801, acc 0.703125, prec 0.0336932, recall 0.779053
2017-12-10T12:39:22.201895: step 737, loss 1.32534, acc 0.71875, prec 0.0337155, recall 0.77937
2017-12-10T12:39:22.667366: step 738, loss 1.5364, acc 0.71875, prec 0.0337378, recall 0.779685
2017-12-10T12:39:23.103716: step 739, loss 6.89367, acc 0.703125, prec 0.0337002, recall 0.778571
2017-12-10T12:39:23.545070: step 740, loss 2.32609, acc 0.578125, prec 0.0336441, recall 0.778571
2017-12-10T12:39:23.977627: step 741, loss 3.03795, acc 0.609375, prec 0.0336518, recall 0.778887
2017-12-10T12:39:24.422529: step 742, loss 2.7836, acc 0.5625, prec 0.0337721, recall 0.77983
2017-12-10T12:39:24.871046: step 743, loss 2.15595, acc 0.625, prec 0.0338411, recall 0.780453
2017-12-10T12:39:25.311601: step 744, loss 3.68083, acc 0.5, prec 0.0340115, recall 0.78169
2017-12-10T12:39:25.759411: step 745, loss 2.66988, acc 0.515625, prec 0.033947, recall 0.78169
2017-12-10T12:39:26.191804: step 746, loss 2.80159, acc 0.5, prec 0.0338807, recall 0.78169
2017-12-10T12:39:26.631288: step 747, loss 2.59391, acc 0.59375, prec 0.033827, recall 0.78169
2017-12-10T12:39:27.075442: step 748, loss 2.03268, acc 0.609375, prec 0.0338344, recall 0.781997
2017-12-10T12:39:27.525542: step 749, loss 1.84758, acc 0.65625, prec 0.0337891, recall 0.781997
2017-12-10T12:39:27.982076: step 750, loss 1.96028, acc 0.625, prec 0.0338572, recall 0.782609
2017-12-10T12:39:28.421184: step 751, loss 11.383, acc 0.71875, prec 0.0338808, recall 0.781818
2017-12-10T12:39:28.877352: step 752, loss 0.926146, acc 0.8125, prec 0.0339732, recall 0.782427
2017-12-10T12:39:29.326015: step 753, loss 1.49481, acc 0.734375, prec 0.0340552, recall 0.783032
2017-12-10T12:39:29.759477: step 754, loss 1.01606, acc 0.734375, prec 0.0340202, recall 0.783032
2017-12-10T12:39:30.230222: step 755, loss 0.99834, acc 0.796875, prec 0.0341101, recall 0.783634
2017-12-10T12:39:30.681128: step 756, loss 1.2283, acc 0.75, prec 0.0341355, recall 0.783934
2017-12-10T12:39:31.134533: step 757, loss 0.512768, acc 0.890625, prec 0.0342375, recall 0.78453
2017-12-10T12:39:31.575009: step 758, loss 1.19662, acc 0.75, prec 0.0342045, recall 0.78453
2017-12-10T12:39:32.020323: step 759, loss 9.4952, acc 0.765625, prec 0.0342338, recall 0.783747
2017-12-10T12:39:32.473261: step 760, loss 0.617018, acc 0.890625, prec 0.0342775, recall 0.784044
2017-12-10T12:39:32.919997: step 761, loss 0.92401, acc 0.8125, prec 0.0343108, recall 0.784341
2017-12-10T12:39:33.359539: step 762, loss 1.68326, acc 0.859375, prec 0.0344662, recall 0.785226
2017-12-10T12:39:33.794437: step 763, loss 1.49783, acc 0.859375, prec 0.0344496, recall 0.784153
2017-12-10T12:39:34.242362: step 764, loss 0.981572, acc 0.796875, prec 0.0344228, recall 0.784153
2017-12-10T12:39:34.669559: step 765, loss 1.48149, acc 0.8125, prec 0.0345138, recall 0.784741
2017-12-10T12:39:35.109566: step 766, loss 4.36923, acc 0.796875, prec 0.0345468, recall 0.783967
2017-12-10T12:39:35.556160: step 767, loss 0.65794, acc 0.84375, prec 0.0345261, recall 0.783967
2017-12-10T12:39:35.990234: step 768, loss 1.16584, acc 0.78125, prec 0.0344972, recall 0.783967
2017-12-10T12:39:36.443848: step 769, loss 1.12516, acc 0.828125, prec 0.0344745, recall 0.783967
2017-12-10T12:39:36.893595: step 770, loss 0.901292, acc 0.734375, prec 0.0345548, recall 0.784553
2017-12-10T12:39:37.332230: step 771, loss 1.23849, acc 0.734375, prec 0.0346349, recall 0.785135
2017-12-10T12:39:37.785819: step 772, loss 0.849081, acc 0.796875, prec 0.0346656, recall 0.785425
2017-12-10T12:39:38.240197: step 773, loss 0.900249, acc 0.8125, prec 0.0346408, recall 0.785425
2017-12-10T12:39:38.704673: step 774, loss 8.25082, acc 0.765625, prec 0.034612, recall 0.784367
2017-12-10T12:39:39.165994: step 775, loss 1.61486, acc 0.609375, prec 0.0346179, recall 0.784657
2017-12-10T12:39:39.615177: step 776, loss 0.965739, acc 0.734375, prec 0.0346403, recall 0.784946
2017-12-10T12:39:40.072525: step 777, loss 10.4707, acc 0.765625, prec 0.0346687, recall 0.784182
2017-12-10T12:39:40.518461: step 778, loss 1.83845, acc 0.671875, prec 0.0347399, recall 0.784759
2017-12-10T12:39:40.956817: step 779, loss 4.20969, acc 0.53125, prec 0.0347924, recall 0.785333
2017-12-10T12:39:41.402109: step 780, loss 2.39408, acc 0.59375, prec 0.034739, recall 0.785333
2017-12-10T12:39:41.857234: step 781, loss 10.0317, acc 0.6875, prec 0.034757, recall 0.784574
2017-12-10T12:39:42.299175: step 782, loss 3.14021, acc 0.578125, prec 0.0348721, recall 0.78543
2017-12-10T12:39:42.735977: step 783, loss 3.4338, acc 0.53125, prec 0.0348107, recall 0.78543
2017-12-10T12:39:43.178126: step 784, loss 2.11226, acc 0.578125, prec 0.0348687, recall 0.785997
2017-12-10T12:39:43.617055: step 785, loss 2.47866, acc 0.5, prec 0.0349728, recall 0.786842
2017-12-10T12:39:44.063787: step 786, loss 2.52741, acc 0.515625, prec 0.0349659, recall 0.787122
2017-12-10T12:39:44.512870: step 787, loss 3.14191, acc 0.46875, prec 0.0349528, recall 0.787402
2017-12-10T12:39:44.965362: step 788, loss 3.57027, acc 0.46875, prec 0.0348837, recall 0.787402
2017-12-10T12:39:45.416336: step 789, loss 5.40359, acc 0.5, prec 0.034821, recall 0.78637
2017-12-10T12:39:45.875457: step 790, loss 7.70886, acc 0.546875, prec 0.0347645, recall 0.78534
2017-12-10T12:39:46.339232: step 791, loss 3.55132, acc 0.421875, prec 0.0348575, recall 0.78618
2017-12-10T12:39:46.785588: step 792, loss 3.41009, acc 0.453125, prec 0.0347871, recall 0.78618
2017-12-10T12:39:47.241821: step 793, loss 2.89759, acc 0.515625, prec 0.0348918, recall 0.787013
2017-12-10T12:39:47.691064: step 794, loss 3.3505, acc 0.453125, prec 0.0348216, recall 0.787013
2017-12-10T12:39:48.144648: step 795, loss 2.47815, acc 0.5625, prec 0.0347656, recall 0.787013
2017-12-10T12:39:48.590561: step 796, loss 2.7954, acc 0.484375, prec 0.0347552, recall 0.787289
2017-12-10T12:39:49.031957: step 797, loss 1.64019, acc 0.5625, prec 0.0347548, recall 0.787565
2017-12-10T12:39:49.475461: step 798, loss 1.83, acc 0.578125, prec 0.0348114, recall 0.788114
2017-12-10T12:39:49.913072: step 799, loss 1.81997, acc 0.640625, prec 0.0349308, recall 0.788932
2017-12-10T12:39:50.355007: step 800, loss 1.80613, acc 0.546875, prec 0.0348731, recall 0.788932
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-800

2017-12-10T12:39:52.265677: step 801, loss 0.904526, acc 0.8125, prec 0.0349042, recall 0.789203
2017-12-10T12:39:52.711247: step 802, loss 1.26065, acc 0.6875, prec 0.0348646, recall 0.789203
2017-12-10T12:39:53.164701: step 803, loss 1.20705, acc 0.75, prec 0.0348877, recall 0.789474
2017-12-10T12:39:53.601963: step 804, loss 0.627148, acc 0.84375, prec 0.0348679, recall 0.789474
2017-12-10T12:39:54.054146: step 805, loss 0.54464, acc 0.859375, prec 0.0349048, recall 0.789744
2017-12-10T12:39:54.488651: step 806, loss 0.394691, acc 0.90625, prec 0.0349476, recall 0.790013
2017-12-10T12:39:54.925752: step 807, loss 0.153981, acc 0.9375, prec 0.0349397, recall 0.790013
2017-12-10T12:39:55.370974: step 808, loss 5.9246, acc 0.859375, prec 0.0349239, recall 0.789003
2017-12-10T12:39:55.828180: step 809, loss 0.0808996, acc 0.953125, prec 0.0349179, recall 0.789003
2017-12-10T12:39:56.268629: step 810, loss 0.577824, acc 0.859375, prec 0.0349548, recall 0.789272
2017-12-10T12:39:56.697964: step 811, loss 0.689792, acc 0.90625, prec 0.0349429, recall 0.789272
2017-12-10T12:39:57.139159: step 812, loss 0.429443, acc 0.859375, prec 0.0349251, recall 0.789272
2017-12-10T12:39:57.604921: step 813, loss 0.203919, acc 0.921875, prec 0.0349698, recall 0.789541
2017-12-10T12:39:58.056597: step 814, loss 3.19158, acc 0.890625, prec 0.0349579, recall 0.788535
2017-12-10T12:39:58.500555: step 815, loss 0.400821, acc 0.890625, prec 0.0349441, recall 0.788535
2017-12-10T12:39:58.944095: step 816, loss 0.442422, acc 0.890625, prec 0.0349848, recall 0.788804
2017-12-10T12:39:59.387413: step 817, loss 0.234022, acc 0.921875, prec 0.0349749, recall 0.788804
2017-12-10T12:39:59.823937: step 818, loss 0.259417, acc 0.90625, prec 0.0349631, recall 0.788804
2017-12-10T12:40:00.272480: step 819, loss 13.9561, acc 0.859375, prec 0.0349493, recall 0.786802
2017-12-10T12:40:00.725576: step 820, loss 2.8587, acc 0.921875, prec 0.0349414, recall 0.785805
2017-12-10T12:40:01.165919: step 821, loss 0.872181, acc 0.828125, prec 0.0349741, recall 0.786076
2017-12-10T12:40:01.606686: step 822, loss 0.647234, acc 0.84375, prec 0.0349544, recall 0.786076
2017-12-10T12:40:02.050629: step 823, loss 1.08483, acc 0.78125, prec 0.0349269, recall 0.786076
2017-12-10T12:40:02.490865: step 824, loss 1.50149, acc 0.734375, prec 0.035002, recall 0.786616
2017-12-10T12:40:02.946310: step 825, loss 4.96563, acc 0.609375, prec 0.035009, recall 0.785894
2017-12-10T12:40:03.413535: step 826, loss 1.91972, acc 0.609375, prec 0.035014, recall 0.786164
2017-12-10T12:40:03.855567: step 827, loss 2.10816, acc 0.625, prec 0.035021, recall 0.786432
2017-12-10T12:40:04.311937: step 828, loss 2.373, acc 0.46875, prec 0.0350084, recall 0.7867
2017-12-10T12:40:04.763858: step 829, loss 2.0147, acc 0.640625, prec 0.0349635, recall 0.7867
2017-12-10T12:40:05.199707: step 830, loss 1.77622, acc 0.625, prec 0.0349167, recall 0.7867
2017-12-10T12:40:05.637826: step 831, loss 1.58187, acc 0.625, prec 0.0349238, recall 0.786967
2017-12-10T12:40:06.092256: step 832, loss 2.58699, acc 0.53125, prec 0.0349192, recall 0.787234
2017-12-10T12:40:06.534815: step 833, loss 2.21319, acc 0.5625, prec 0.0349185, recall 0.7875
2017-12-10T12:40:06.975295: step 834, loss 2.50681, acc 0.4375, prec 0.034849, recall 0.7875
2017-12-10T12:40:07.430011: step 835, loss 1.84128, acc 0.734375, prec 0.0349229, recall 0.78803
2017-12-10T12:40:07.868222: step 836, loss 1.54948, acc 0.671875, prec 0.0348824, recall 0.78803
2017-12-10T12:40:08.313943: step 837, loss 1.09168, acc 0.796875, prec 0.0349107, recall 0.788294
2017-12-10T12:40:08.755647: step 838, loss 1.03689, acc 0.75, prec 0.0348799, recall 0.788294
2017-12-10T12:40:09.210881: step 839, loss 1.39804, acc 0.734375, prec 0.0348472, recall 0.788294
2017-12-10T12:40:09.649421: step 840, loss 3.7028, acc 0.78125, prec 0.0348754, recall 0.787578
2017-12-10T12:40:10.112949: step 841, loss 1.42677, acc 0.734375, prec 0.0348428, recall 0.787578
2017-12-10T12:40:10.547640: step 842, loss 3.42971, acc 0.765625, prec 0.034922, recall 0.787129
2017-12-10T12:40:10.999094: step 843, loss 0.514426, acc 0.90625, prec 0.0349105, recall 0.787129
2017-12-10T12:40:11.454056: step 844, loss 0.779446, acc 0.84375, prec 0.0349443, recall 0.787392
2017-12-10T12:40:11.894922: step 845, loss 0.766462, acc 0.796875, prec 0.0349194, recall 0.787392
2017-12-10T12:40:12.346662: step 846, loss 0.722086, acc 0.828125, prec 0.0349512, recall 0.787654
2017-12-10T12:40:12.791367: step 847, loss 1.47119, acc 0.765625, prec 0.0350282, recall 0.788177
2017-12-10T12:40:13.228001: step 848, loss 0.751471, acc 0.859375, prec 0.0350637, recall 0.788438
2017-12-10T12:40:13.670033: step 849, loss 0.602346, acc 0.828125, prec 0.0350426, recall 0.788438
2017-12-10T12:40:14.112914: step 850, loss 0.806459, acc 0.828125, prec 0.0350216, recall 0.788438
2017-12-10T12:40:14.555144: step 851, loss 0.701877, acc 0.796875, prec 0.0349967, recall 0.788438
2017-12-10T12:40:15.009868: step 852, loss 15.8572, acc 0.875, prec 0.0350398, recall 0.785802
2017-12-10T12:40:15.461699: step 853, loss 1.76518, acc 0.75, prec 0.0350619, recall 0.786064
2017-12-10T12:40:15.889233: step 854, loss 0.681421, acc 0.828125, prec 0.0350409, recall 0.786064
2017-12-10T12:40:16.332772: step 855, loss 6.15282, acc 0.734375, prec 0.0350103, recall 0.785104
2017-12-10T12:40:16.776328: step 856, loss 1.82673, acc 0.625, prec 0.0350696, recall 0.785627
2017-12-10T12:40:17.221835: step 857, loss 2.08402, acc 0.5625, prec 0.0350163, recall 0.785627
2017-12-10T12:40:17.682302: step 858, loss 1.96217, acc 0.578125, prec 0.0350697, recall 0.786148
2017-12-10T12:40:18.132634: step 859, loss 2.1046, acc 0.640625, prec 0.035026, recall 0.786148
2017-12-10T12:40:18.598380: step 860, loss 3.20969, acc 0.59375, prec 0.0350289, recall 0.786408
2017-12-10T12:40:19.044316: step 861, loss 3.10426, acc 0.484375, prec 0.0349665, recall 0.786408
2017-12-10T12:40:19.495377: step 862, loss 3.29887, acc 0.453125, prec 0.0349006, recall 0.786408
2017-12-10T12:40:19.929131: step 863, loss 2.13795, acc 0.578125, prec 0.03485, recall 0.786408
2017-12-10T12:40:20.374005: step 864, loss 2.49057, acc 0.5625, prec 0.0347976, recall 0.786408
2017-12-10T12:40:20.817121: step 865, loss 2.224, acc 0.5625, prec 0.0347971, recall 0.786667
2017-12-10T12:40:21.261326: step 866, loss 7.80882, acc 0.625, prec 0.0348059, recall 0.785973
2017-12-10T12:40:21.719632: step 867, loss 1.95051, acc 0.578125, prec 0.0348072, recall 0.786232
2017-12-10T12:40:22.164558: step 868, loss 2.36412, acc 0.609375, prec 0.0349154, recall 0.787004
2017-12-10T12:40:22.603702: step 869, loss 1.68428, acc 0.625, prec 0.0349222, recall 0.78726
2017-12-10T12:40:23.062255: step 870, loss 1.90494, acc 0.609375, prec 0.0348757, recall 0.78726
2017-12-10T12:40:23.507173: step 871, loss 1.9632, acc 0.59375, prec 0.0348275, recall 0.78726
2017-12-10T12:40:23.942886: step 872, loss 3.83156, acc 0.734375, prec 0.0347979, recall 0.786315
2017-12-10T12:40:24.394013: step 873, loss 1.4164, acc 0.71875, prec 0.0347646, recall 0.786315
2017-12-10T12:40:24.836060: step 874, loss 1.20109, acc 0.734375, prec 0.0347333, recall 0.786315
2017-12-10T12:40:25.279797: step 875, loss 1.39155, acc 0.71875, prec 0.0348535, recall 0.787081
2017-12-10T12:40:25.730192: step 876, loss 0.972431, acc 0.75, prec 0.034824, recall 0.787081
2017-12-10T12:40:26.182867: step 877, loss 0.967481, acc 0.796875, prec 0.0348001, recall 0.787081
2017-12-10T12:40:26.633694: step 878, loss 1.35522, acc 0.734375, prec 0.0348198, recall 0.787336
2017-12-10T12:40:27.100780: step 879, loss 0.975711, acc 0.828125, prec 0.0347996, recall 0.787336
2017-12-10T12:40:27.543615: step 880, loss 4.41796, acc 0.828125, prec 0.0348322, recall 0.786651
2017-12-10T12:40:27.989451: step 881, loss 0.548539, acc 0.890625, prec 0.0348702, recall 0.786905
2017-12-10T12:40:28.436983: step 882, loss 0.451146, acc 0.875, prec 0.0348555, recall 0.786905
2017-12-10T12:40:28.870836: step 883, loss 6.67171, acc 0.796875, prec 0.0348335, recall 0.785969
2017-12-10T12:40:29.321743: step 884, loss 0.82556, acc 0.8125, prec 0.0348115, recall 0.785969
2017-12-10T12:40:29.762487: step 885, loss 0.64399, acc 0.78125, prec 0.0347858, recall 0.785969
2017-12-10T12:40:30.211100: step 886, loss 0.763747, acc 0.8125, prec 0.0348654, recall 0.786477
2017-12-10T12:40:30.644392: step 887, loss 1.12091, acc 0.765625, prec 0.0348379, recall 0.786477
2017-12-10T12:40:31.078242: step 888, loss 0.826337, acc 0.765625, prec 0.0348105, recall 0.786477
2017-12-10T12:40:31.527184: step 889, loss 0.189381, acc 0.90625, prec 0.0347995, recall 0.786477
2017-12-10T12:40:31.985335: step 890, loss 0.700299, acc 0.828125, prec 0.03483, recall 0.78673
2017-12-10T12:40:32.427481: step 891, loss 0.995904, acc 0.71875, prec 0.0347972, recall 0.78673
2017-12-10T12:40:32.866675: step 892, loss 5.3534, acc 0.828125, prec 0.0348295, recall 0.786052
2017-12-10T12:40:33.312519: step 893, loss 0.948847, acc 0.8125, prec 0.0348582, recall 0.786305
2017-12-10T12:40:33.762359: step 894, loss 0.850373, acc 0.796875, prec 0.0348345, recall 0.786305
2017-12-10T12:40:34.203963: step 895, loss 1.29557, acc 0.8125, prec 0.0348126, recall 0.786305
2017-12-10T12:40:34.654748: step 896, loss 5.35167, acc 0.765625, prec 0.0348376, recall 0.78563
2017-12-10T12:40:35.098156: step 897, loss 5.99867, acc 0.84375, prec 0.0348716, recall 0.784959
2017-12-10T12:40:35.546763: step 898, loss 1.03819, acc 0.75, prec 0.0348928, recall 0.785211
2017-12-10T12:40:36.003489: step 899, loss 2.45101, acc 0.828125, prec 0.0349249, recall 0.784543
2017-12-10T12:40:36.460252: step 900, loss 1.41589, acc 0.6875, prec 0.0348886, recall 0.784543
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-900

2017-12-10T12:40:38.302402: step 901, loss 2.4759, acc 0.515625, prec 0.0348825, recall 0.784795
2017-12-10T12:40:38.745875: step 902, loss 1.42735, acc 0.703125, prec 0.0348481, recall 0.784795
2017-12-10T12:40:39.201971: step 903, loss 1.6102, acc 0.671875, prec 0.0348101, recall 0.784795
2017-12-10T12:40:39.648639: step 904, loss 1.53438, acc 0.578125, prec 0.0347614, recall 0.784795
2017-12-10T12:40:40.101242: step 905, loss 2.23466, acc 0.5625, prec 0.0347111, recall 0.784795
2017-12-10T12:40:40.552013: step 906, loss 1.48111, acc 0.671875, prec 0.0347732, recall 0.785298
2017-12-10T12:40:40.995553: step 907, loss 1.90583, acc 0.59375, prec 0.0347265, recall 0.785298
2017-12-10T12:40:41.463931: step 908, loss 2.04632, acc 0.625, prec 0.0347333, recall 0.785548
2017-12-10T12:40:41.904964: step 909, loss 2.105, acc 0.59375, prec 0.0347365, recall 0.785797
2017-12-10T12:40:42.341298: step 910, loss 1.56029, acc 0.671875, prec 0.034699, recall 0.785797
2017-12-10T12:40:42.783691: step 911, loss 2.72091, acc 0.53125, prec 0.0346951, recall 0.786047
2017-12-10T12:40:43.228750: step 912, loss 0.873391, acc 0.734375, prec 0.0347144, recall 0.786295
2017-12-10T12:40:43.670891: step 913, loss 1.47886, acc 0.671875, prec 0.034677, recall 0.786295
2017-12-10T12:40:44.114965: step 914, loss 0.842476, acc 0.78125, prec 0.034751, recall 0.78679
2017-12-10T12:40:44.565557: step 915, loss 0.941763, acc 0.75, prec 0.0347226, recall 0.78679
2017-12-10T12:40:45.002530: step 916, loss 0.553874, acc 0.84375, prec 0.0347048, recall 0.78679
2017-12-10T12:40:45.434466: step 917, loss 0.77342, acc 0.796875, prec 0.0346818, recall 0.78679
2017-12-10T12:40:45.870895: step 918, loss 0.721628, acc 0.8125, prec 0.0347098, recall 0.787037
2017-12-10T12:40:46.318273: step 919, loss 0.35858, acc 0.859375, prec 0.0347431, recall 0.787283
2017-12-10T12:40:46.775836: step 920, loss 3.86918, acc 0.890625, prec 0.0347325, recall 0.786374
2017-12-10T12:40:47.225329: step 921, loss 0.157322, acc 0.9375, prec 0.0347254, recall 0.786374
2017-12-10T12:40:47.685573: step 922, loss 0.305236, acc 0.953125, prec 0.0348185, recall 0.786866
2017-12-10T12:40:48.126083: step 923, loss 0.262854, acc 0.890625, prec 0.0348061, recall 0.786866
2017-12-10T12:40:48.570486: step 924, loss 0.221557, acc 0.984375, prec 0.0348535, recall 0.787112
2017-12-10T12:40:49.004951: step 925, loss 2.88427, acc 0.90625, prec 0.0348446, recall 0.786207
2017-12-10T12:40:49.451068: step 926, loss 0.586335, acc 0.8125, prec 0.0348233, recall 0.786207
2017-12-10T12:40:49.892813: step 927, loss 0.392454, acc 0.96875, prec 0.0348689, recall 0.786452
2017-12-10T12:40:50.342235: step 928, loss 9.02691, acc 0.875, prec 0.0348565, recall 0.78555
2017-12-10T12:40:50.798495: step 929, loss 6.92864, acc 0.859375, prec 0.0348914, recall 0.784897
2017-12-10T12:40:51.248345: step 930, loss 0.732826, acc 0.78125, prec 0.0348666, recall 0.784897
2017-12-10T12:40:51.694954: step 931, loss 1.26786, acc 0.703125, prec 0.0348819, recall 0.785143
2017-12-10T12:40:52.134530: step 932, loss 0.486952, acc 0.8125, prec 0.0349097, recall 0.785388
2017-12-10T12:40:52.580049: step 933, loss 0.359559, acc 0.875, prec 0.0349445, recall 0.785633
2017-12-10T12:40:53.017725: step 934, loss 1.26552, acc 0.671875, prec 0.0349562, recall 0.785877
2017-12-10T12:40:53.461405: step 935, loss 0.917561, acc 0.78125, prec 0.0349314, recall 0.785877
2017-12-10T12:40:53.902712: step 936, loss 1.139, acc 0.671875, prec 0.0348943, recall 0.785877
2017-12-10T12:40:54.352414: step 937, loss 1.45855, acc 0.6875, prec 0.0349078, recall 0.786121
2017-12-10T12:40:54.793673: step 938, loss 1.34006, acc 0.6875, prec 0.0349213, recall 0.786364
2017-12-10T12:40:55.230219: step 939, loss 5.20772, acc 0.625, prec 0.0348808, recall 0.785471
2017-12-10T12:40:55.673424: step 940, loss 0.986075, acc 0.71875, prec 0.0349464, recall 0.785957
2017-12-10T12:40:56.117281: step 941, loss 1.75574, acc 0.75, prec 0.0349182, recall 0.785957
2017-12-10T12:40:56.565710: step 942, loss 1.70294, acc 0.75, prec 0.0350357, recall 0.786682
2017-12-10T12:40:57.008430: step 943, loss 1.29892, acc 0.75, prec 0.0351045, recall 0.787162
2017-12-10T12:40:57.464332: step 944, loss 1.19107, acc 0.671875, prec 0.0350675, recall 0.787162
2017-12-10T12:40:57.931508: step 945, loss 1.31391, acc 0.734375, prec 0.0351827, recall 0.787879
2017-12-10T12:40:58.382683: step 946, loss 3.59509, acc 0.609375, prec 0.0351404, recall 0.786996
2017-12-10T12:40:58.829737: step 947, loss 7.53676, acc 0.796875, prec 0.0351676, recall 0.786353
2017-12-10T12:40:59.274985: step 948, loss 2.03665, acc 0.78125, prec 0.0352394, recall 0.78683
2017-12-10T12:40:59.731052: step 949, loss 1.53309, acc 0.71875, prec 0.0352559, recall 0.787068
2017-12-10T12:41:00.170384: step 950, loss 1.57364, acc 0.703125, prec 0.0352225, recall 0.787068
2017-12-10T12:41:00.627301: step 951, loss 3.76104, acc 0.703125, prec 0.035239, recall 0.786429
2017-12-10T12:41:01.070517: step 952, loss 1.53593, acc 0.6875, prec 0.0352519, recall 0.786667
2017-12-10T12:41:01.530329: step 953, loss 1.91416, acc 0.609375, prec 0.0352081, recall 0.786667
2017-12-10T12:41:01.987232: step 954, loss 1.26519, acc 0.6875, prec 0.0352211, recall 0.786903
2017-12-10T12:41:02.428401: step 955, loss 1.28535, acc 0.625, prec 0.035227, recall 0.78714
2017-12-10T12:41:02.888727: step 956, loss 1.81006, acc 0.671875, prec 0.0351903, recall 0.78714
2017-12-10T12:41:03.333684: step 957, loss 1.13713, acc 0.65625, prec 0.0351998, recall 0.787375
2017-12-10T12:41:03.783432: step 958, loss 1.48808, acc 0.765625, prec 0.0353168, recall 0.788079
2017-12-10T12:41:04.237102: step 959, loss 0.917275, acc 0.734375, prec 0.0352871, recall 0.788079
2017-12-10T12:41:04.677464: step 960, loss 1.28897, acc 0.734375, prec 0.0352575, recall 0.788079
2017-12-10T12:41:05.123689: step 961, loss 1.25903, acc 0.6875, prec 0.0352227, recall 0.788079
2017-12-10T12:41:05.575637: step 962, loss 1.52103, acc 0.703125, prec 0.0352373, recall 0.788313
2017-12-10T12:41:06.008045: step 963, loss 0.701826, acc 0.828125, prec 0.0352657, recall 0.788546
2017-12-10T12:41:06.441405: step 964, loss 1.01801, acc 0.75, prec 0.0352854, recall 0.788779
2017-12-10T12:41:06.887342: step 965, loss 10.352, acc 0.859375, prec 0.035319, recall 0.788145
2017-12-10T12:41:07.332872: step 966, loss 0.425376, acc 0.859375, prec 0.0353508, recall 0.788377
2017-12-10T12:41:07.788133: step 967, loss 0.224323, acc 0.890625, prec 0.0353861, recall 0.788609
2017-12-10T12:41:08.235241: step 968, loss 0.488291, acc 0.890625, prec 0.0354213, recall 0.78884
2017-12-10T12:41:08.674330: step 969, loss 1.05224, acc 0.78125, prec 0.0354916, recall 0.789301
2017-12-10T12:41:09.126623: step 970, loss 1.39446, acc 0.828125, prec 0.0355671, recall 0.78976
2017-12-10T12:41:09.592865: step 971, loss 0.549594, acc 0.84375, prec 0.0355497, recall 0.78976
2017-12-10T12:41:10.038382: step 972, loss 0.413171, acc 0.859375, prec 0.035534, recall 0.78976
2017-12-10T12:41:10.493707: step 973, loss 0.498212, acc 0.875, prec 0.0355201, recall 0.78976
2017-12-10T12:41:10.936536: step 974, loss 0.957432, acc 0.765625, prec 0.0355884, recall 0.790217
2017-12-10T12:41:11.380971: step 975, loss 0.75072, acc 0.84375, prec 0.035571, recall 0.790217
2017-12-10T12:41:11.816838: step 976, loss 0.479674, acc 0.859375, prec 0.0355553, recall 0.790217
2017-12-10T12:41:12.265557: step 977, loss 0.642108, acc 0.875, prec 0.0355886, recall 0.790445
2017-12-10T12:41:12.720561: step 978, loss 16.6063, acc 0.890625, prec 0.0356724, recall 0.790043
2017-12-10T12:41:13.160690: step 979, loss 0.577918, acc 0.8125, prec 0.0357457, recall 0.790497
2017-12-10T12:41:13.601916: step 980, loss 0.453129, acc 0.859375, prec 0.0358241, recall 0.790948
2017-12-10T12:41:14.044781: step 981, loss 0.327112, acc 0.90625, prec 0.0359077, recall 0.791398
2017-12-10T12:41:14.492747: step 982, loss 1.61885, acc 0.8125, prec 0.0359337, recall 0.791622
2017-12-10T12:41:14.933334: step 983, loss 0.524231, acc 0.859375, prec 0.0359649, recall 0.791846
2017-12-10T12:41:15.377036: step 984, loss 0.901756, acc 0.78125, prec 0.0359404, recall 0.791846
2017-12-10T12:41:15.822678: step 985, loss 0.657621, acc 0.78125, prec 0.0359628, recall 0.792069
2017-12-10T12:41:16.261285: step 986, loss 7.28957, acc 0.875, prec 0.0360444, recall 0.791667
2017-12-10T12:41:16.719855: step 987, loss 2.70629, acc 0.828125, prec 0.0360268, recall 0.790822
2017-12-10T12:41:17.154466: step 988, loss 0.963574, acc 0.71875, prec 0.0359953, recall 0.790822
2017-12-10T12:41:17.603315: step 989, loss 1.43293, acc 0.6875, prec 0.0360072, recall 0.791045
2017-12-10T12:41:18.042796: step 990, loss 1.70062, acc 0.703125, prec 0.0360207, recall 0.791267
2017-12-10T12:41:18.463348: step 991, loss 1.168, acc 0.640625, prec 0.0359806, recall 0.791267
2017-12-10T12:41:18.907648: step 992, loss 1.23392, acc 0.75, prec 0.0359528, recall 0.791267
2017-12-10T12:41:19.353710: step 993, loss 1.57421, acc 0.609375, prec 0.0359093, recall 0.791267
2017-12-10T12:41:19.752405: step 994, loss 1.51126, acc 0.673077, prec 0.0359264, recall 0.791489
2017-12-10T12:41:20.221047: step 995, loss 1.73385, acc 0.609375, prec 0.0358831, recall 0.791489
2017-12-10T12:41:20.656361: step 996, loss 0.764696, acc 0.734375, prec 0.0359002, recall 0.791711
2017-12-10T12:41:21.091039: step 997, loss 1.45608, acc 0.671875, prec 0.0358639, recall 0.791711
2017-12-10T12:41:21.530797: step 998, loss 0.879005, acc 0.765625, prec 0.035838, recall 0.791711
2017-12-10T12:41:21.965753: step 999, loss 1.40218, acc 0.734375, prec 0.0358087, recall 0.791711
2017-12-10T12:41:22.402321: step 1000, loss 1.18528, acc 0.765625, prec 0.0358292, recall 0.791932
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1000

2017-12-10T12:41:24.394342: step 1001, loss 0.948497, acc 0.796875, prec 0.0358531, recall 0.792153
2017-12-10T12:41:24.837944: step 1002, loss 1.20822, acc 0.75, prec 0.0358256, recall 0.792153
2017-12-10T12:41:25.287475: step 1003, loss 0.851526, acc 0.78125, prec 0.035894, recall 0.792593
2017-12-10T12:41:25.729088: step 1004, loss 1.20961, acc 0.796875, prec 0.0358716, recall 0.792593
2017-12-10T12:41:26.176572: step 1005, loss 4.61288, acc 0.875, prec 0.0359519, recall 0.792194
2017-12-10T12:41:26.614771: step 1006, loss 0.241752, acc 0.875, prec 0.0359382, recall 0.792194
2017-12-10T12:41:27.061522: step 1007, loss 1.00077, acc 0.828125, prec 0.0359654, recall 0.792413
2017-12-10T12:41:27.509157: step 1008, loss 11.2927, acc 0.828125, prec 0.0359482, recall 0.791579
2017-12-10T12:41:27.978090: step 1009, loss 0.594887, acc 0.828125, prec 0.0359753, recall 0.791798
2017-12-10T12:41:28.419289: step 1010, loss 0.686493, acc 0.84375, prec 0.0359582, recall 0.791798
2017-12-10T12:41:28.860892: step 1011, loss 1.01486, acc 0.875, prec 0.0360365, recall 0.792235
2017-12-10T12:41:29.301026: step 1012, loss 0.557506, acc 0.828125, prec 0.0360635, recall 0.792453
2017-12-10T12:41:29.748660: step 1013, loss 0.848231, acc 0.75, prec 0.036082, recall 0.79267
2017-12-10T12:41:30.200732: step 1014, loss 0.615927, acc 0.859375, prec 0.0360665, recall 0.79267
2017-12-10T12:41:30.646877: step 1015, loss 1.14239, acc 0.75, prec 0.036039, recall 0.79267
2017-12-10T12:41:31.090367: step 1016, loss 1.28198, acc 0.734375, prec 0.0360099, recall 0.79267
2017-12-10T12:41:31.542588: step 1017, loss 17.7121, acc 0.8125, prec 0.0359928, recall 0.791014
2017-12-10T12:41:31.989651: step 1018, loss 0.883841, acc 0.78125, prec 0.0360604, recall 0.791449
2017-12-10T12:41:32.432839: step 1019, loss 0.945107, acc 0.671875, prec 0.0360245, recall 0.791449
2017-12-10T12:41:32.866765: step 1020, loss 0.674654, acc 0.78125, prec 0.036092, recall 0.791883
2017-12-10T12:41:33.326728: step 1021, loss 1.38431, acc 0.671875, prec 0.0361474, recall 0.792316
2017-12-10T12:41:33.763323: step 1022, loss 1.55509, acc 0.609375, prec 0.0361047, recall 0.792316
2017-12-10T12:41:34.210029: step 1023, loss 1.64885, acc 0.625, prec 0.0361093, recall 0.792531
2017-12-10T12:41:34.653416: step 1024, loss 1.36815, acc 0.6875, prec 0.0361207, recall 0.792746
2017-12-10T12:41:35.110416: step 1025, loss 1.81206, acc 0.65625, prec 0.0360832, recall 0.792746
2017-12-10T12:41:35.556706: step 1026, loss 1.00448, acc 0.734375, prec 0.0361451, recall 0.793175
2017-12-10T12:41:35.993675: step 1027, loss 1.34884, acc 0.703125, prec 0.0361582, recall 0.793388
2017-12-10T12:41:36.451053: step 1028, loss 1.13968, acc 0.796875, prec 0.0361814, recall 0.793602
2017-12-10T12:41:36.901510: step 1029, loss 1.04688, acc 0.796875, prec 0.0362046, recall 0.793814
2017-12-10T12:41:37.350959: step 1030, loss 0.671061, acc 0.84375, prec 0.0362329, recall 0.794027
2017-12-10T12:41:37.795388: step 1031, loss 0.565539, acc 0.84375, prec 0.0363517, recall 0.794661
2017-12-10T12:41:38.241447: step 1032, loss 1.48332, acc 0.84375, prec 0.0363799, recall 0.794872
2017-12-10T12:41:38.680539: step 1033, loss 0.739068, acc 0.84375, prec 0.0363628, recall 0.794872
2017-12-10T12:41:39.122445: step 1034, loss 1.06254, acc 0.734375, prec 0.0363338, recall 0.794872
2017-12-10T12:41:39.572402: step 1035, loss 0.535089, acc 0.84375, prec 0.0363619, recall 0.795082
2017-12-10T12:41:40.021714: step 1036, loss 0.554884, acc 0.859375, prec 0.036482, recall 0.79571
2017-12-10T12:41:40.470553: step 1037, loss 0.659462, acc 0.84375, prec 0.0364649, recall 0.79571
2017-12-10T12:41:40.926854: step 1038, loss 5.60718, acc 0.875, prec 0.036453, recall 0.794898
2017-12-10T12:41:41.377757: step 1039, loss 0.543067, acc 0.953125, prec 0.0366282, recall 0.795732
2017-12-10T12:41:41.817602: step 1040, loss 0.312058, acc 0.90625, prec 0.0366629, recall 0.795939
2017-12-10T12:41:42.252800: step 1041, loss 0.374761, acc 0.890625, prec 0.036696, recall 0.796146
2017-12-10T12:41:42.699313: step 1042, loss 0.659224, acc 0.84375, prec 0.0367688, recall 0.796559
2017-12-10T12:41:43.160433: step 1043, loss 0.276656, acc 0.921875, prec 0.0367602, recall 0.796559
2017-12-10T12:41:43.604870: step 1044, loss 0.402399, acc 0.859375, prec 0.0367448, recall 0.796559
2017-12-10T12:41:44.055046: step 1045, loss 0.537947, acc 0.796875, prec 0.0367225, recall 0.796559
2017-12-10T12:41:44.500326: step 1046, loss 0.441365, acc 0.875, prec 0.0367088, recall 0.796559
2017-12-10T12:41:44.933781: step 1047, loss 0.523087, acc 0.890625, prec 0.0367417, recall 0.796764
2017-12-10T12:41:45.377074: step 1048, loss 0.455473, acc 0.84375, prec 0.0367246, recall 0.796764
2017-12-10T12:41:45.837310: step 1049, loss 0.810325, acc 0.859375, prec 0.0367541, recall 0.79697
2017-12-10T12:41:46.280395: step 1050, loss 0.309865, acc 0.90625, prec 0.0367887, recall 0.797175
2017-12-10T12:41:46.721913: step 1051, loss 0.453774, acc 0.84375, prec 0.0368164, recall 0.797379
2017-12-10T12:41:47.163420: step 1052, loss 0.343492, acc 0.953125, prec 0.0368561, recall 0.797583
2017-12-10T12:41:47.620447: step 1053, loss 0.554, acc 0.859375, prec 0.0368854, recall 0.797787
2017-12-10T12:41:48.069523: step 1054, loss 0.447939, acc 0.84375, prec 0.0368683, recall 0.797787
2017-12-10T12:41:48.518533: step 1055, loss 0.337971, acc 0.90625, prec 0.0369028, recall 0.79799
2017-12-10T12:41:48.965371: step 1056, loss 0.378173, acc 0.90625, prec 0.0369372, recall 0.798193
2017-12-10T12:41:49.414800: step 1057, loss 0.413879, acc 0.859375, prec 0.0369218, recall 0.798193
2017-12-10T12:41:49.860925: step 1058, loss 0.238592, acc 0.90625, prec 0.0369562, recall 0.798395
2017-12-10T12:41:50.298137: step 1059, loss 4.12954, acc 0.921875, prec 0.0369941, recall 0.797798
2017-12-10T12:41:50.749986: step 1060, loss 2.16324, acc 0.890625, prec 0.0370731, recall 0.797405
2017-12-10T12:41:51.194044: step 1061, loss 0.246776, acc 0.9375, prec 0.0370662, recall 0.797405
2017-12-10T12:41:51.642216: step 1062, loss 0.31366, acc 0.90625, prec 0.0370559, recall 0.797405
2017-12-10T12:41:52.089341: step 1063, loss 0.386812, acc 0.859375, prec 0.0370851, recall 0.797607
2017-12-10T12:41:52.542956: step 1064, loss 0.227023, acc 0.890625, prec 0.0370731, recall 0.797607
2017-12-10T12:41:52.982179: step 1065, loss 1.04542, acc 0.921875, prec 0.0371091, recall 0.797809
2017-12-10T12:41:53.425795: step 1066, loss 0.573282, acc 0.875, prec 0.0370954, recall 0.797809
2017-12-10T12:41:53.858585: step 1067, loss 0.628918, acc 0.859375, prec 0.0371245, recall 0.79801
2017-12-10T12:41:54.321398: step 1068, loss 0.556863, acc 0.875, prec 0.0371553, recall 0.798211
2017-12-10T12:41:54.766430: step 1069, loss 0.337114, acc 0.90625, prec 0.037234, recall 0.798611
2017-12-10T12:41:55.221771: step 1070, loss 0.989237, acc 0.890625, prec 0.037311, recall 0.79901
2017-12-10T12:41:55.678919: step 1071, loss 0.625754, acc 0.8125, prec 0.0373348, recall 0.799209
2017-12-10T12:41:56.131153: step 1072, loss 0.684468, acc 0.875, prec 0.0373655, recall 0.799407
2017-12-10T12:41:56.566431: step 1073, loss 1.21139, acc 0.765625, prec 0.0373396, recall 0.799407
2017-12-10T12:41:57.018402: step 1074, loss 0.808555, acc 0.828125, prec 0.0373207, recall 0.799407
2017-12-10T12:41:57.461474: step 1075, loss 0.83094, acc 0.8125, prec 0.0373444, recall 0.799605
2017-12-10T12:41:57.907101: step 1076, loss 0.936251, acc 0.828125, prec 0.0373255, recall 0.799605
2017-12-10T12:41:58.346417: step 1077, loss 0.745673, acc 0.859375, prec 0.03731, recall 0.799605
2017-12-10T12:41:58.792417: step 1078, loss 0.978584, acc 0.734375, prec 0.0373251, recall 0.799803
2017-12-10T12:41:59.227507: step 1079, loss 0.484096, acc 0.890625, prec 0.0373131, recall 0.799803
2017-12-10T12:41:59.664664: step 1080, loss 4.78943, acc 0.796875, prec 0.037381, recall 0.79941
2017-12-10T12:42:00.098719: step 1081, loss 0.727494, acc 0.796875, prec 0.0374029, recall 0.799607
2017-12-10T12:42:00.531128: step 1082, loss 0.299962, acc 0.890625, prec 0.0373909, recall 0.799607
2017-12-10T12:42:00.970255: step 1083, loss 0.651208, acc 0.796875, prec 0.0373686, recall 0.799607
2017-12-10T12:42:01.407596: step 1084, loss 0.354571, acc 0.921875, prec 0.0374042, recall 0.799804
2017-12-10T12:42:01.852584: step 1085, loss 0.846206, acc 0.78125, prec 0.0374243, recall 0.8
2017-12-10T12:42:02.302324: step 1086, loss 0.332935, acc 0.875, prec 0.0374106, recall 0.8
2017-12-10T12:42:02.747010: step 1087, loss 4.97632, acc 0.875, prec 0.0374427, recall 0.799413
2017-12-10T12:42:03.192940: step 1088, loss 0.550746, acc 0.796875, prec 0.0374645, recall 0.799609
2017-12-10T12:42:03.639153: step 1089, loss 1.03935, acc 0.859375, prec 0.0376694, recall 0.800584
2017-12-10T12:42:04.096131: step 1090, loss 1.1703, acc 0.703125, prec 0.0376806, recall 0.800777
2017-12-10T12:42:04.541178: step 1091, loss 0.952142, acc 0.8125, prec 0.0377039, recall 0.800971
2017-12-10T12:42:04.995384: step 1092, loss 1.42922, acc 0.671875, prec 0.0377117, recall 0.801164
2017-12-10T12:42:05.456659: step 1093, loss 0.524271, acc 0.890625, prec 0.0377875, recall 0.801549
2017-12-10T12:42:05.905656: step 1094, loss 0.960179, acc 0.71875, prec 0.0377565, recall 0.801549
2017-12-10T12:42:06.353079: step 1095, loss 0.879539, acc 0.828125, prec 0.0377814, recall 0.801741
2017-12-10T12:42:06.784747: step 1096, loss 0.481691, acc 0.828125, prec 0.0377625, recall 0.801741
2017-12-10T12:42:07.226417: step 1097, loss 0.631723, acc 0.765625, prec 0.0377367, recall 0.801741
2017-12-10T12:42:07.664799: step 1098, loss 0.901381, acc 0.796875, prec 0.0377582, recall 0.801932
2017-12-10T12:42:08.108156: step 1099, loss 2.67389, acc 0.8125, prec 0.037783, recall 0.80135
2017-12-10T12:42:08.565806: step 1100, loss 2.33218, acc 0.84375, prec 0.0377676, recall 0.800578
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1100

2017-12-10T12:42:10.546195: step 1101, loss 0.768655, acc 0.796875, prec 0.0378764, recall 0.801153
2017-12-10T12:42:10.975491: step 1102, loss 0.719495, acc 0.765625, prec 0.0379379, recall 0.801534
2017-12-10T12:42:11.417754: step 1103, loss 0.889026, acc 0.8125, prec 0.0380482, recall 0.802103
2017-12-10T12:42:11.878473: step 1104, loss 0.764228, acc 0.765625, prec 0.0380659, recall 0.802292
2017-12-10T12:42:12.329872: step 1105, loss 0.912724, acc 0.75, prec 0.0380383, recall 0.802292
2017-12-10T12:42:12.777226: step 1106, loss 0.723708, acc 0.84375, prec 0.0380646, recall 0.802481
2017-12-10T12:42:13.213262: step 1107, loss 0.340294, acc 0.890625, prec 0.0380526, recall 0.802481
2017-12-10T12:42:13.663582: step 1108, loss 0.565384, acc 0.84375, prec 0.0380789, recall 0.802669
2017-12-10T12:42:14.099106: step 1109, loss 0.462765, acc 0.875, prec 0.0381086, recall 0.802857
2017-12-10T12:42:14.547340: step 1110, loss 1.36608, acc 0.859375, prec 0.03818, recall 0.803232
2017-12-10T12:42:14.990957: step 1111, loss 0.633427, acc 0.796875, prec 0.038201, recall 0.803419
2017-12-10T12:42:15.424432: step 1112, loss 0.738963, acc 0.828125, prec 0.0382255, recall 0.803605
2017-12-10T12:42:15.860225: step 1113, loss 0.625288, acc 0.78125, prec 0.0382447, recall 0.803791
2017-12-10T12:42:16.305485: step 1114, loss 1.14364, acc 0.71875, prec 0.0382137, recall 0.803791
2017-12-10T12:42:16.748252: step 1115, loss 0.478407, acc 0.84375, prec 0.0381965, recall 0.803791
2017-12-10T12:42:17.194045: step 1116, loss 0.426827, acc 0.84375, prec 0.0381793, recall 0.803791
2017-12-10T12:42:17.635254: step 1117, loss 0.172457, acc 0.9375, prec 0.0382157, recall 0.803977
2017-12-10T12:42:18.083611: step 1118, loss 0.666604, acc 0.84375, prec 0.0381985, recall 0.803977
2017-12-10T12:42:18.521545: step 1119, loss 2.234, acc 0.875, prec 0.0383578, recall 0.804717
2017-12-10T12:42:18.964068: step 1120, loss 0.249903, acc 0.90625, prec 0.0383474, recall 0.804717
2017-12-10T12:42:19.424117: step 1121, loss 0.551834, acc 0.890625, prec 0.0383354, recall 0.804717
2017-12-10T12:42:19.862099: step 1122, loss 0.40383, acc 0.890625, prec 0.0383665, recall 0.804901
2017-12-10T12:42:20.318953: step 1123, loss 0.506107, acc 0.84375, prec 0.0383493, recall 0.804901
2017-12-10T12:42:20.758250: step 1124, loss 0.240944, acc 0.90625, prec 0.0383389, recall 0.804901
2017-12-10T12:42:21.202029: step 1125, loss 0.593957, acc 0.859375, prec 0.0383235, recall 0.804901
2017-12-10T12:42:21.649633: step 1126, loss 0.814669, acc 0.84375, prec 0.0383063, recall 0.804901
2017-12-10T12:42:22.103722: step 1127, loss 0.225675, acc 0.921875, prec 0.0382977, recall 0.804901
2017-12-10T12:42:22.544618: step 1128, loss 0.492207, acc 0.875, prec 0.0383271, recall 0.805085
2017-12-10T12:42:22.985690: step 1129, loss 0.599978, acc 0.859375, prec 0.0383116, recall 0.805085
2017-12-10T12:42:23.433461: step 1130, loss 0.535952, acc 0.828125, prec 0.0383358, recall 0.805268
2017-12-10T12:42:23.873401: step 1131, loss 0.164233, acc 0.953125, prec 0.0383306, recall 0.805268
2017-12-10T12:42:24.307390: step 1132, loss 3.5854, acc 0.90625, prec 0.0383651, recall 0.804695
2017-12-10T12:42:24.741386: step 1133, loss 11.5952, acc 0.890625, prec 0.0383548, recall 0.80394
2017-12-10T12:42:25.174814: step 1134, loss 0.63192, acc 0.859375, prec 0.0383394, recall 0.80394
2017-12-10T12:42:25.626672: step 1135, loss 0.334113, acc 0.90625, prec 0.0383291, recall 0.80394
2017-12-10T12:42:26.075509: step 1136, loss 0.723926, acc 0.890625, prec 0.0383601, recall 0.804124
2017-12-10T12:42:26.527460: step 1137, loss 0.512428, acc 0.828125, prec 0.0383412, recall 0.804124
2017-12-10T12:42:26.975114: step 1138, loss 0.660649, acc 0.765625, prec 0.0384014, recall 0.80449
2017-12-10T12:42:27.422408: step 1139, loss 0.634194, acc 0.875, prec 0.0383877, recall 0.80449
2017-12-10T12:42:27.867459: step 1140, loss 0.370428, acc 0.875, prec 0.038374, recall 0.80449
2017-12-10T12:42:28.303303: step 1141, loss 0.987837, acc 0.796875, prec 0.0385233, recall 0.805219
2017-12-10T12:42:28.751386: step 1142, loss 0.863887, acc 0.75, prec 0.0384958, recall 0.805219
2017-12-10T12:42:29.196242: step 1143, loss 0.947205, acc 0.734375, prec 0.0385095, recall 0.8054
2017-12-10T12:42:29.657650: step 1144, loss 0.846722, acc 0.78125, prec 0.038571, recall 0.805762
2017-12-10T12:42:30.113842: step 1145, loss 0.623306, acc 0.875, prec 0.0386001, recall 0.805942
2017-12-10T12:42:30.566333: step 1146, loss 0.501669, acc 0.8125, prec 0.0385795, recall 0.805942
2017-12-10T12:42:31.022882: step 1147, loss 1.02597, acc 0.796875, prec 0.0385572, recall 0.805942
2017-12-10T12:42:31.469582: step 1148, loss 0.490299, acc 0.890625, prec 0.0385452, recall 0.805942
2017-12-10T12:42:31.916030: step 1149, loss 0.851531, acc 0.765625, prec 0.0385196, recall 0.805942
2017-12-10T12:42:32.359701: step 1150, loss 0.739339, acc 0.8125, prec 0.0384991, recall 0.805942
2017-12-10T12:42:32.802422: step 1151, loss 0.504277, acc 0.890625, prec 0.0384871, recall 0.805942
2017-12-10T12:42:33.248863: step 1152, loss 0.444845, acc 0.859375, prec 0.0384718, recall 0.805942
2017-12-10T12:42:33.701095: step 1153, loss 0.207543, acc 0.984375, prec 0.0385553, recall 0.806302
2017-12-10T12:42:34.156522: step 1154, loss 2.53097, acc 0.796875, prec 0.0385774, recall 0.805735
2017-12-10T12:42:34.596562: step 1155, loss 0.373165, acc 0.921875, prec 0.0386114, recall 0.805915
2017-12-10T12:42:35.035823: step 1156, loss 8.15243, acc 0.828125, prec 0.0385943, recall 0.805171
2017-12-10T12:42:35.496107: step 1157, loss 0.533674, acc 0.84375, prec 0.0386198, recall 0.805351
2017-12-10T12:42:35.952143: step 1158, loss 0.573435, acc 0.859375, prec 0.0387744, recall 0.806066
2017-12-10T12:42:36.409867: step 1159, loss 0.533947, acc 0.75, prec 0.038747, recall 0.806066
2017-12-10T12:42:36.850013: step 1160, loss 1.54637, acc 0.765625, prec 0.0388062, recall 0.806422
2017-12-10T12:42:37.284538: step 1161, loss 0.589181, acc 0.8125, prec 0.0388705, recall 0.806777
2017-12-10T12:42:37.729260: step 1162, loss 0.879857, acc 0.796875, prec 0.038933, recall 0.80713
2017-12-10T12:42:38.176917: step 1163, loss 0.881999, acc 0.859375, prec 0.0390869, recall 0.807832
2017-12-10T12:42:38.619663: step 1164, loss 1.05224, acc 0.65625, prec 0.0390914, recall 0.808007
2017-12-10T12:42:39.062890: step 1165, loss 1.18973, acc 0.78125, prec 0.0391096, recall 0.808182
2017-12-10T12:42:39.513569: step 1166, loss 1.25336, acc 0.734375, prec 0.0391648, recall 0.80853
2017-12-10T12:42:39.967401: step 1167, loss 5.80285, acc 0.8125, prec 0.0392303, recall 0.808145
2017-12-10T12:42:40.425621: step 1168, loss 1.52376, acc 0.78125, prec 0.0393327, recall 0.808664
2017-12-10T12:42:40.875965: step 1169, loss 1.46882, acc 0.640625, prec 0.0392931, recall 0.808664
2017-12-10T12:42:41.318730: step 1170, loss 1.47912, acc 0.671875, prec 0.0393832, recall 0.809181
2017-12-10T12:42:41.755221: step 1171, loss 0.847979, acc 0.78125, prec 0.039359, recall 0.809181
2017-12-10T12:42:42.204201: step 1172, loss 0.990695, acc 0.78125, prec 0.039377, recall 0.809353
2017-12-10T12:42:42.641524: step 1173, loss 1.29077, acc 0.65625, prec 0.0393811, recall 0.809524
2017-12-10T12:42:43.078563: step 1174, loss 4.20342, acc 0.65625, prec 0.039345, recall 0.808797
2017-12-10T12:42:43.530490: step 1175, loss 1.41622, acc 0.640625, prec 0.0394312, recall 0.809311
2017-12-10T12:42:43.969818: step 1176, loss 1.43624, acc 0.71875, prec 0.0394421, recall 0.809481
2017-12-10T12:42:44.421352: step 1177, loss 0.731526, acc 0.8125, prec 0.0394634, recall 0.809651
2017-12-10T12:42:44.866249: step 1178, loss 0.930615, acc 0.78125, prec 0.0394811, recall 0.809821
2017-12-10T12:42:45.314227: step 1179, loss 1.42548, acc 0.71875, prec 0.039492, recall 0.809991
2017-12-10T12:42:45.757565: step 1180, loss 1.39806, acc 0.734375, prec 0.0394628, recall 0.809991
2017-12-10T12:42:46.211392: step 1181, loss 1.25808, acc 0.703125, prec 0.039472, recall 0.81016
2017-12-10T12:42:46.690568: step 1182, loss 1.42504, acc 0.734375, prec 0.0394429, recall 0.81016
2017-12-10T12:42:47.130936: step 1183, loss 0.464876, acc 0.84375, prec 0.0394674, recall 0.810329
2017-12-10T12:42:47.572551: step 1184, loss 1.0061, acc 0.75, prec 0.0395649, recall 0.810835
2017-12-10T12:42:48.019599: step 1185, loss 0.977568, acc 0.8125, prec 0.0395444, recall 0.810835
2017-12-10T12:42:48.466741: step 1186, loss 0.867595, acc 0.8125, prec 0.039607, recall 0.81117
2017-12-10T12:42:48.907613: step 1187, loss 0.999303, acc 0.78125, prec 0.039583, recall 0.81117
2017-12-10T12:42:49.352454: step 1188, loss 0.68389, acc 0.828125, prec 0.0396057, recall 0.811337
2017-12-10T12:42:49.803671: step 1189, loss 0.372642, acc 0.875, prec 0.0396335, recall 0.811504
2017-12-10T12:42:50.249025: step 1190, loss 0.190695, acc 0.921875, prec 0.0397079, recall 0.811837
2017-12-10T12:42:50.705212: step 1191, loss 0.498313, acc 0.859375, prec 0.0396925, recall 0.811837
2017-12-10T12:42:51.157341: step 1192, loss 2.67503, acc 0.859375, prec 0.0397617, recall 0.811454
2017-12-10T12:42:51.615806: step 1193, loss 0.495263, acc 0.875, prec 0.0398308, recall 0.811785
2017-12-10T12:42:52.066263: step 1194, loss 0.79573, acc 0.90625, prec 0.039862, recall 0.811951
2017-12-10T12:42:52.511255: step 1195, loss 0.257972, acc 0.921875, prec 0.0398534, recall 0.811951
2017-12-10T12:42:52.943706: step 1196, loss 0.311324, acc 0.9375, prec 0.0398879, recall 0.812116
2017-12-10T12:42:53.382898: step 1197, loss 9.97445, acc 0.84375, prec 0.0398724, recall 0.811404
2017-12-10T12:42:53.843341: step 1198, loss 0.41961, acc 0.890625, prec 0.0399018, recall 0.811569
2017-12-10T12:42:54.294873: step 1199, loss 0.291811, acc 0.921875, prec 0.0398932, recall 0.811569
2017-12-10T12:42:54.743639: step 1200, loss 0.367137, acc 0.90625, prec 0.0399655, recall 0.811899
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1200

2017-12-10T12:42:56.716333: step 1201, loss 1.8359, acc 0.875, prec 0.0399535, recall 0.811189
2017-12-10T12:42:57.187627: step 1202, loss 0.542135, acc 0.859375, prec 0.0400207, recall 0.811518
2017-12-10T12:42:57.635374: step 1203, loss 0.643808, acc 0.890625, prec 0.0400499, recall 0.811683
2017-12-10T12:42:58.085399: step 1204, loss 0.257718, acc 0.90625, prec 0.0400396, recall 0.811683
2017-12-10T12:42:58.523988: step 1205, loss 0.405481, acc 0.921875, prec 0.040031, recall 0.811683
2017-12-10T12:42:58.960501: step 1206, loss 0.580542, acc 0.875, prec 0.0400585, recall 0.811847
2017-12-10T12:42:59.413658: step 1207, loss 0.624047, acc 0.84375, prec 0.0400412, recall 0.811847
2017-12-10T12:42:59.860099: step 1208, loss 0.457299, acc 0.84375, prec 0.0400653, recall 0.81201
2017-12-10T12:43:00.320364: step 1209, loss 0.685331, acc 0.828125, prec 0.0400876, recall 0.812174
2017-12-10T12:43:00.766670: step 1210, loss 0.474838, acc 0.8125, prec 0.0401081, recall 0.812337
2017-12-10T12:43:01.219472: step 1211, loss 0.559353, acc 0.859375, prec 0.0400926, recall 0.812337
2017-12-10T12:43:01.655031: step 1212, loss 0.762641, acc 0.828125, prec 0.0400737, recall 0.812337
2017-12-10T12:43:02.104822: step 1213, loss 0.404942, acc 0.859375, prec 0.0400994, recall 0.8125
2017-12-10T12:43:02.546478: step 1214, loss 0.70442, acc 0.796875, prec 0.0400771, recall 0.8125
2017-12-10T12:43:02.999391: step 1215, loss 1.09636, acc 0.875, prec 0.0401866, recall 0.812987
2017-12-10T12:43:03.449413: step 1216, loss 0.437626, acc 0.859375, prec 0.0402122, recall 0.813149
2017-12-10T12:43:03.891567: step 1217, loss 1.01417, acc 0.875, prec 0.0402805, recall 0.813471
2017-12-10T12:43:04.342895: step 1218, loss 0.560277, acc 0.8125, prec 0.0402599, recall 0.813471
2017-12-10T12:43:04.774714: step 1219, loss 2.09788, acc 0.875, prec 0.0402478, recall 0.81277
2017-12-10T12:43:05.220129: step 1220, loss 0.49435, acc 0.875, prec 0.040275, recall 0.812931
2017-12-10T12:43:05.664995: step 1221, loss 0.586745, acc 0.875, prec 0.0403023, recall 0.813092
2017-12-10T12:43:06.115115: step 1222, loss 0.802491, acc 0.8125, prec 0.0402816, recall 0.813092
2017-12-10T12:43:06.559844: step 1223, loss 3.33128, acc 0.828125, prec 0.0402645, recall 0.812392
2017-12-10T12:43:07.005469: step 1224, loss 0.743659, acc 0.8125, prec 0.0402438, recall 0.812392
2017-12-10T12:43:07.457889: step 1225, loss 0.588044, acc 0.84375, prec 0.0403085, recall 0.812715
2017-12-10T12:43:07.908925: step 1226, loss 0.677917, acc 0.75, prec 0.0403219, recall 0.812876
2017-12-10T12:43:08.346836: step 1227, loss 0.571632, acc 0.859375, prec 0.0403064, recall 0.812876
2017-12-10T12:43:08.788995: step 1228, loss 0.485799, acc 0.75, prec 0.040279, recall 0.812876
2017-12-10T12:43:09.236630: step 1229, loss 0.751902, acc 0.828125, prec 0.0402602, recall 0.812876
2017-12-10T12:43:09.674133: step 1230, loss 0.568983, acc 0.84375, prec 0.0402431, recall 0.812876
2017-12-10T12:43:10.111003: step 1231, loss 0.734522, acc 0.8125, prec 0.0402633, recall 0.813036
2017-12-10T12:43:10.559703: step 1232, loss 0.789655, acc 0.78125, prec 0.0402801, recall 0.813196
2017-12-10T12:43:11.002701: step 1233, loss 0.573736, acc 0.90625, prec 0.0402699, recall 0.813196
2017-12-10T12:43:11.456502: step 1234, loss 6.00728, acc 0.828125, prec 0.0402935, recall 0.81266
2017-12-10T12:43:11.910492: step 1235, loss 1.40348, acc 0.796875, prec 0.0403527, recall 0.81298
2017-12-10T12:43:12.356769: step 1236, loss 0.285417, acc 0.875, prec 0.0403796, recall 0.81314
2017-12-10T12:43:12.809128: step 1237, loss 0.809208, acc 0.796875, prec 0.0403981, recall 0.813299
2017-12-10T12:43:13.267546: step 1238, loss 0.850005, acc 0.765625, prec 0.0403724, recall 0.813299
2017-12-10T12:43:13.715893: step 1239, loss 2.1548, acc 0.75, prec 0.0403874, recall 0.812766
2017-12-10T12:43:14.174855: step 1240, loss 0.649882, acc 0.8125, prec 0.0404075, recall 0.812925
2017-12-10T12:43:14.608389: step 1241, loss 1.35122, acc 0.640625, prec 0.0403682, recall 0.812925
2017-12-10T12:43:15.041297: step 1242, loss 1.28469, acc 0.765625, prec 0.0404641, recall 0.813401
2017-12-10T12:43:15.487458: step 1243, loss 0.586822, acc 0.828125, prec 0.0405263, recall 0.813717
2017-12-10T12:43:15.926062: step 1244, loss 0.893761, acc 0.75, prec 0.0405394, recall 0.813875
2017-12-10T12:43:16.358311: step 1245, loss 1.06046, acc 0.75, prec 0.0405525, recall 0.814032
2017-12-10T12:43:16.798380: step 1246, loss 1.14011, acc 0.734375, prec 0.0405235, recall 0.814032
2017-12-10T12:43:17.228790: step 1247, loss 0.666146, acc 0.84375, prec 0.0405871, recall 0.814346
2017-12-10T12:43:17.673268: step 1248, loss 0.694858, acc 0.890625, prec 0.0406559, recall 0.814659
2017-12-10T12:43:18.105624: step 1249, loss 0.754024, acc 0.84375, prec 0.0406791, recall 0.814815
2017-12-10T12:43:18.548049: step 1250, loss 0.252425, acc 0.890625, prec 0.0406671, recall 0.814815
2017-12-10T12:43:18.992653: step 1251, loss 0.62744, acc 0.8125, prec 0.0407272, recall 0.815126
2017-12-10T12:43:19.442644: step 1252, loss 6.42531, acc 0.90625, prec 0.0407187, recall 0.814442
2017-12-10T12:43:19.899821: step 1253, loss 1.58539, acc 0.8125, prec 0.0407384, recall 0.814597
2017-12-10T12:43:20.338205: step 1254, loss 1.10171, acc 0.75, prec 0.0408317, recall 0.815063
2017-12-10T12:43:20.770637: step 1255, loss 0.737392, acc 0.8125, prec 0.0409318, recall 0.815526
2017-12-10T12:43:21.211288: step 1256, loss 0.881, acc 0.796875, prec 0.0409095, recall 0.815526
2017-12-10T12:43:21.674702: step 1257, loss 0.851429, acc 0.78125, prec 0.0408855, recall 0.815526
2017-12-10T12:43:22.133263: step 1258, loss 0.721381, acc 0.828125, prec 0.0409469, recall 0.815833
2017-12-10T12:43:22.579151: step 1259, loss 0.659031, acc 0.78125, prec 0.040963, recall 0.815987
2017-12-10T12:43:23.029367: step 1260, loss 1.30779, acc 0.796875, prec 0.0409408, recall 0.815987
2017-12-10T12:43:23.469968: step 1261, loss 0.94563, acc 0.8125, prec 0.0409203, recall 0.815987
2017-12-10T12:43:23.908088: step 1262, loss 0.64067, acc 0.828125, prec 0.0409415, recall 0.81614
2017-12-10T12:43:24.348631: step 1263, loss 0.803713, acc 0.78125, prec 0.0409176, recall 0.81614
2017-12-10T12:43:24.799033: step 1264, loss 0.762029, acc 0.75, prec 0.0409703, recall 0.816445
2017-12-10T12:43:25.236330: step 1265, loss 0.588304, acc 0.84375, prec 0.0409932, recall 0.816598
2017-12-10T12:43:25.687522: step 1266, loss 0.531382, acc 0.84375, prec 0.041016, recall 0.81675
2017-12-10T12:43:26.119063: step 1267, loss 0.348937, acc 0.859375, prec 0.0410007, recall 0.81675
2017-12-10T12:43:26.565500: step 1268, loss 0.364945, acc 0.890625, prec 0.0409887, recall 0.81675
2017-12-10T12:43:27.002896: step 1269, loss 2.45774, acc 0.828125, prec 0.0409717, recall 0.816073
2017-12-10T12:43:27.440702: step 1270, loss 3.20612, acc 0.875, prec 0.0410794, recall 0.815855
2017-12-10T12:43:27.882351: step 1271, loss 0.63057, acc 0.828125, prec 0.0410606, recall 0.815855
2017-12-10T12:43:28.325608: step 1272, loss 0.903374, acc 0.90625, prec 0.0410902, recall 0.816007
2017-12-10T12:43:28.766165: step 1273, loss 0.414284, acc 0.8125, prec 0.0411095, recall 0.816158
2017-12-10T12:43:29.213221: step 1274, loss 1.47173, acc 0.890625, prec 0.0411772, recall 0.816461
2017-12-10T12:43:29.655007: step 1275, loss 0.827086, acc 0.859375, prec 0.0412016, recall 0.816612
2017-12-10T12:43:30.095307: step 1276, loss 0.151567, acc 0.921875, prec 0.0411931, recall 0.816612
2017-12-10T12:43:30.537557: step 1277, loss 0.912719, acc 0.890625, prec 0.0412606, recall 0.816913
2017-12-10T12:43:30.976833: step 1278, loss 0.483027, acc 0.84375, prec 0.0412435, recall 0.816913
2017-12-10T12:43:31.451812: step 1279, loss 0.952602, acc 0.71875, prec 0.0412525, recall 0.817063
2017-12-10T12:43:31.891688: step 1280, loss 1.60153, acc 0.828125, prec 0.0413148, recall 0.816694
2017-12-10T12:43:32.348215: step 1281, loss 0.773489, acc 0.859375, prec 0.0412994, recall 0.816694
2017-12-10T12:43:32.802533: step 1282, loss 2.72164, acc 0.78125, prec 0.0412772, recall 0.816026
2017-12-10T12:43:33.262866: step 1283, loss 0.69445, acc 0.765625, prec 0.0413309, recall 0.816327
2017-12-10T12:43:33.713506: step 1284, loss 0.967041, acc 0.75, prec 0.0413035, recall 0.816327
2017-12-10T12:43:34.151068: step 1285, loss 0.834928, acc 0.796875, prec 0.0413209, recall 0.816476
2017-12-10T12:43:34.602462: step 1286, loss 0.744004, acc 0.75, prec 0.0413728, recall 0.816775
2017-12-10T12:43:35.044119: step 1287, loss 0.850521, acc 0.734375, prec 0.0413438, recall 0.816775
2017-12-10T12:43:35.486304: step 1288, loss 0.5241, acc 0.796875, prec 0.0413216, recall 0.816775
2017-12-10T12:43:35.923315: step 1289, loss 0.679252, acc 0.75, prec 0.0412944, recall 0.816775
2017-12-10T12:43:36.363580: step 1290, loss 1.0126, acc 0.75, prec 0.0412672, recall 0.816775
2017-12-10T12:43:36.821687: step 1291, loss 1.40903, acc 0.8125, prec 0.0412863, recall 0.816924
2017-12-10T12:43:37.258238: step 1292, loss 0.655587, acc 0.78125, prec 0.0412625, recall 0.816924
2017-12-10T12:43:37.716671: step 1293, loss 0.684259, acc 0.828125, prec 0.0412439, recall 0.816924
2017-12-10T12:43:38.155787: step 1294, loss 1.0695, acc 0.765625, prec 0.0412185, recall 0.816924
2017-12-10T12:43:38.598234: step 1295, loss 0.681498, acc 0.828125, prec 0.0412786, recall 0.817222
2017-12-10T12:43:39.055068: step 1296, loss 0.461009, acc 0.828125, prec 0.0412993, recall 0.81737
2017-12-10T12:43:39.508810: step 1297, loss 0.216495, acc 0.90625, prec 0.041407, recall 0.817814
2017-12-10T12:43:39.946995: step 1298, loss 0.474892, acc 0.875, prec 0.0413934, recall 0.817814
2017-12-10T12:43:40.378736: step 1299, loss 2.03659, acc 0.84375, prec 0.0414175, recall 0.8173
2017-12-10T12:43:40.827033: step 1300, loss 0.416641, acc 0.84375, prec 0.0414005, recall 0.8173
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1300

2017-12-10T12:43:42.780364: step 1301, loss 7.26501, acc 0.84375, prec 0.0413852, recall 0.81664
2017-12-10T12:43:43.235522: step 1302, loss 0.371683, acc 0.921875, prec 0.0414552, recall 0.816935
2017-12-10T12:43:43.691718: step 1303, loss 2.88701, acc 0.890625, prec 0.0416019, recall 0.816867
2017-12-10T12:43:44.141030: step 1304, loss 0.448633, acc 0.875, prec 0.0416667, recall 0.817161
2017-12-10T12:43:44.579355: step 1305, loss 0.858473, acc 0.765625, prec 0.0416803, recall 0.817308
2017-12-10T12:43:45.031313: step 1306, loss 0.676608, acc 0.8125, prec 0.0417773, recall 0.817746
2017-12-10T12:43:45.473036: step 1307, loss 1.01194, acc 0.765625, prec 0.0417517, recall 0.817746
2017-12-10T12:43:45.923732: step 1308, loss 1.04772, acc 0.703125, prec 0.0417975, recall 0.818037
2017-12-10T12:43:46.363703: step 1309, loss 1.20754, acc 0.703125, prec 0.0417651, recall 0.818037
2017-12-10T12:43:46.800545: step 1310, loss 1.1727, acc 0.640625, prec 0.041726, recall 0.818037
2017-12-10T12:43:47.253949: step 1311, loss 1.11883, acc 0.703125, prec 0.0417328, recall 0.818182
2017-12-10T12:43:47.695184: step 1312, loss 0.953751, acc 0.6875, prec 0.0416988, recall 0.818182
2017-12-10T12:43:48.136128: step 1313, loss 1.0188, acc 0.734375, prec 0.041709, recall 0.818327
2017-12-10T12:43:48.591881: step 1314, loss 1.09451, acc 0.75, prec 0.0417208, recall 0.818471
2017-12-10T12:43:49.033383: step 1315, loss 0.794796, acc 0.734375, prec 0.041692, recall 0.818471
2017-12-10T12:43:49.482681: step 1316, loss 0.629424, acc 0.78125, prec 0.0417072, recall 0.818616
2017-12-10T12:43:49.947430: step 1317, loss 0.930468, acc 0.75, prec 0.041719, recall 0.81876
2017-12-10T12:43:50.389812: step 1318, loss 0.671766, acc 0.828125, prec 0.0417392, recall 0.818904
2017-12-10T12:43:50.831082: step 1319, loss 1.44784, acc 0.765625, prec 0.0417526, recall 0.819048
2017-12-10T12:43:51.278594: step 1320, loss 0.676459, acc 0.796875, prec 0.0417694, recall 0.819191
2017-12-10T12:43:51.726035: step 1321, loss 0.460599, acc 0.859375, prec 0.041793, recall 0.819334
2017-12-10T12:43:52.163631: step 1322, loss 0.519937, acc 0.84375, prec 0.0417761, recall 0.819334
2017-12-10T12:43:52.608251: step 1323, loss 1.66643, acc 0.921875, prec 0.0417693, recall 0.818686
2017-12-10T12:43:53.065716: step 1324, loss 0.505057, acc 0.828125, prec 0.0417508, recall 0.818686
2017-12-10T12:43:53.508310: step 1325, loss 0.537547, acc 0.84375, prec 0.0417726, recall 0.818829
2017-12-10T12:43:53.952864: step 1326, loss 0.341941, acc 0.90625, prec 0.0418012, recall 0.818972
2017-12-10T12:43:54.397986: step 1327, loss 0.952635, acc 0.875, prec 0.0419036, recall 0.819401
2017-12-10T12:43:54.837768: step 1328, loss 0.360046, acc 0.90625, prec 0.0418935, recall 0.819401
2017-12-10T12:43:55.283468: step 1329, loss 0.462866, acc 0.84375, prec 0.0419152, recall 0.819543
2017-12-10T12:43:55.726246: step 1330, loss 0.0944343, acc 0.96875, prec 0.0419504, recall 0.819685
2017-12-10T12:43:56.153887: step 1331, loss 0.468733, acc 0.875, prec 0.0420527, recall 0.82011
2017-12-10T12:43:56.601587: step 1332, loss 0.233468, acc 0.90625, prec 0.0420425, recall 0.82011
2017-12-10T12:43:57.052134: step 1333, loss 7.83807, acc 0.90625, prec 0.0421112, recall 0.819749
2017-12-10T12:43:57.511871: step 1334, loss 0.89938, acc 0.875, prec 0.0421362, recall 0.81989
2017-12-10T12:43:57.948195: step 1335, loss 0.406154, acc 0.875, prec 0.0421226, recall 0.81989
2017-12-10T12:43:58.392888: step 1336, loss 0.462206, acc 0.84375, prec 0.0421057, recall 0.81989
2017-12-10T12:43:58.838894: step 1337, loss 0.533325, acc 0.859375, prec 0.0420905, recall 0.81989
2017-12-10T12:43:59.282069: step 1338, loss 0.138042, acc 0.9375, prec 0.0420837, recall 0.81989
2017-12-10T12:43:59.728190: step 1339, loss 0.856169, acc 0.84375, prec 0.0421437, recall 0.820172
2017-12-10T12:44:00.170701: step 1340, loss 0.895323, acc 0.890625, prec 0.0422088, recall 0.820453
2017-12-10T12:44:00.611371: step 1341, loss 0.781287, acc 0.765625, prec 0.0422603, recall 0.820733
2017-12-10T12:44:01.053525: step 1342, loss 0.7989, acc 0.828125, prec 0.0423185, recall 0.821012
2017-12-10T12:44:01.486972: step 1343, loss 0.464922, acc 0.890625, prec 0.0423066, recall 0.821012
2017-12-10T12:44:01.939917: step 1344, loss 0.398896, acc 0.84375, prec 0.0422897, recall 0.821012
2017-12-10T12:44:02.387267: step 1345, loss 0.507315, acc 0.84375, prec 0.0423111, recall 0.821151
2017-12-10T12:44:02.820392: step 1346, loss 0.286444, acc 0.84375, prec 0.0422941, recall 0.821151
2017-12-10T12:44:03.261669: step 1347, loss 0.80299, acc 0.84375, prec 0.0422772, recall 0.821151
2017-12-10T12:44:03.714106: step 1348, loss 0.250424, acc 0.890625, prec 0.0422654, recall 0.821151
2017-12-10T12:44:04.162703: step 1349, loss 1.61707, acc 0.875, prec 0.0423302, recall 0.820791
2017-12-10T12:44:04.605332: step 1350, loss 7.73868, acc 0.859375, prec 0.0423549, recall 0.820294
2017-12-10T12:44:05.059318: step 1351, loss 0.826241, acc 0.828125, prec 0.0423363, recall 0.820294
2017-12-10T12:44:05.508166: step 1352, loss 0.389907, acc 0.84375, prec 0.0423959, recall 0.820572
2017-12-10T12:44:05.956711: step 1353, loss 0.564901, acc 0.796875, prec 0.0423739, recall 0.820572
2017-12-10T12:44:06.404354: step 1354, loss 0.706109, acc 0.84375, prec 0.0424334, recall 0.820849
2017-12-10T12:44:06.846539: step 1355, loss 0.696958, acc 0.828125, prec 0.042453, recall 0.820988
2017-12-10T12:44:07.287584: step 1356, loss 0.494379, acc 0.859375, prec 0.0424378, recall 0.820988
2017-12-10T12:44:07.743092: step 1357, loss 0.645257, acc 0.828125, prec 0.0424573, recall 0.821126
2017-12-10T12:44:08.201064: step 1358, loss 1.1785, acc 0.75, prec 0.0424684, recall 0.821263
2017-12-10T12:44:08.641142: step 1359, loss 0.96306, acc 0.734375, prec 0.0424778, recall 0.821401
2017-12-10T12:44:09.067093: step 1360, loss 1.04349, acc 0.78125, prec 0.0425303, recall 0.821676
2017-12-10T12:44:09.513904: step 1361, loss 0.548591, acc 0.84375, prec 0.0425134, recall 0.821676
2017-12-10T12:44:09.967618: step 1362, loss 0.47322, acc 0.78125, prec 0.0424898, recall 0.821676
2017-12-10T12:44:10.413617: step 1363, loss 0.339927, acc 0.90625, prec 0.0425177, recall 0.821813
2017-12-10T12:44:10.851927: step 1364, loss 0.729746, acc 0.796875, prec 0.0425338, recall 0.821949
2017-12-10T12:44:11.295618: step 1365, loss 0.823633, acc 0.765625, prec 0.0425084, recall 0.821949
2017-12-10T12:44:11.746565: step 1366, loss 0.317931, acc 0.921875, prec 0.042538, recall 0.822086
2017-12-10T12:44:12.208262: step 1367, loss 1.08724, acc 0.859375, prec 0.0425608, recall 0.822222
2017-12-10T12:44:12.656565: step 1368, loss 13.7786, acc 0.921875, prec 0.0425937, recall 0.821101
2017-12-10T12:44:13.082672: step 1369, loss 0.468505, acc 0.90625, prec 0.0426595, recall 0.821374
2017-12-10T12:44:13.526804: step 1370, loss 0.609224, acc 0.828125, prec 0.0427168, recall 0.821646
2017-12-10T12:44:13.979988: step 1371, loss 0.488789, acc 0.875, prec 0.0427032, recall 0.821646
2017-12-10T12:44:14.425300: step 1372, loss 1.09027, acc 0.765625, prec 0.0427158, recall 0.821782
2017-12-10T12:44:14.876205: step 1373, loss 0.691941, acc 0.78125, prec 0.0426921, recall 0.821782
2017-12-10T12:44:15.326108: step 1374, loss 0.816103, acc 0.734375, prec 0.0427769, recall 0.822188
2017-12-10T12:44:15.773117: step 1375, loss 0.410069, acc 0.828125, prec 0.0427583, recall 0.822188
2017-12-10T12:44:16.226321: step 1376, loss 0.35684, acc 0.90625, prec 0.0428238, recall 0.822458
2017-12-10T12:44:16.669142: step 1377, loss 0.842993, acc 0.765625, prec 0.0427985, recall 0.822458
2017-12-10T12:44:17.109854: step 1378, loss 0.721726, acc 0.796875, prec 0.0427765, recall 0.822458
2017-12-10T12:44:17.546794: step 1379, loss 1.34928, acc 0.71875, prec 0.0427839, recall 0.822593
2017-12-10T12:44:18.000646: step 1380, loss 0.767904, acc 0.765625, prec 0.0428341, recall 0.822861
2017-12-10T12:44:18.468362: step 1381, loss 0.882299, acc 0.765625, prec 0.0428465, recall 0.822995
2017-12-10T12:44:18.909785: step 1382, loss 0.977388, acc 0.71875, prec 0.0428161, recall 0.822995
2017-12-10T12:44:19.353297: step 1383, loss 0.508361, acc 0.828125, prec 0.0427976, recall 0.822995
2017-12-10T12:44:19.803646: step 1384, loss 0.400148, acc 0.84375, prec 0.0427807, recall 0.822995
2017-12-10T12:44:20.236243: step 1385, loss 0.558699, acc 0.828125, prec 0.0427623, recall 0.822995
2017-12-10T12:44:20.681158: step 1386, loss 0.567234, acc 0.765625, prec 0.0427747, recall 0.823129
2017-12-10T12:44:21.126386: step 1387, loss 0.516982, acc 0.84375, prec 0.0427579, recall 0.823129
2017-12-10T12:44:21.562970: step 1388, loss 0.722208, acc 0.828125, prec 0.042777, recall 0.823263
2017-12-10T12:44:22.000881: step 1389, loss 0.689094, acc 0.828125, prec 0.0427585, recall 0.823263
2017-12-10T12:44:22.440325: step 1390, loss 3.20367, acc 0.8125, prec 0.0428151, recall 0.822909
2017-12-10T12:44:22.895505: step 1391, loss 0.555769, acc 0.859375, prec 0.0428, recall 0.822909
2017-12-10T12:44:23.360107: step 1392, loss 0.255719, acc 0.890625, prec 0.0427883, recall 0.822909
2017-12-10T12:44:23.813116: step 1393, loss 0.328543, acc 0.921875, prec 0.0427799, recall 0.822909
2017-12-10T12:44:24.273432: step 1394, loss 0.600538, acc 0.8125, prec 0.0427598, recall 0.822909
2017-12-10T12:44:24.724640: step 1395, loss 0.458003, acc 0.921875, prec 0.0427514, recall 0.822909
2017-12-10T12:44:25.171074: step 1396, loss 0.350724, acc 0.890625, prec 0.0427772, recall 0.823042
2017-12-10T12:44:25.619220: step 1397, loss 0.251003, acc 0.90625, prec 0.0428046, recall 0.823175
2017-12-10T12:44:26.066986: step 1398, loss 0.188787, acc 0.953125, prec 0.0427996, recall 0.823175
2017-12-10T12:44:26.514489: step 1399, loss 0.33424, acc 0.890625, prec 0.0428627, recall 0.823441
2017-12-10T12:44:26.967533: step 1400, loss 0.226595, acc 0.890625, prec 0.0428884, recall 0.823574
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1400

2017-12-10T12:44:29.048552: step 1401, loss 0.223923, acc 0.9375, prec 0.0429191, recall 0.823706
2017-12-10T12:44:29.491401: step 1402, loss 0.182086, acc 0.9375, prec 0.0429124, recall 0.823706
2017-12-10T12:44:29.943619: step 1403, loss 0.313696, acc 0.921875, prec 0.0429414, recall 0.823838
2017-12-10T12:44:30.385402: step 1404, loss 0.0992852, acc 0.953125, prec 0.0429364, recall 0.823838
2017-12-10T12:44:30.831848: step 1405, loss 5.63919, acc 0.9375, prec 0.0430435, recall 0.823617
2017-12-10T12:44:31.284377: step 1406, loss 1.81046, acc 0.9375, prec 0.0431115, recall 0.823881
2017-12-10T12:44:31.729558: step 1407, loss 0.140211, acc 0.953125, prec 0.0431065, recall 0.823881
2017-12-10T12:44:32.168482: step 1408, loss 0.247874, acc 0.9375, prec 0.0431371, recall 0.824012
2017-12-10T12:44:32.618132: step 1409, loss 1.20976, acc 0.953125, prec 0.0432441, recall 0.824405
2017-12-10T12:44:33.064395: step 1410, loss 1.46507, acc 0.84375, prec 0.0432662, recall 0.823923
2017-12-10T12:44:33.509495: step 1411, loss 6.32699, acc 0.859375, prec 0.0433274, recall 0.823573
2017-12-10T12:44:33.958127: step 1412, loss 0.809985, acc 0.65625, prec 0.0432902, recall 0.823573
2017-12-10T12:44:34.409708: step 1413, loss 1.22716, acc 0.65625, prec 0.0432531, recall 0.823573
2017-12-10T12:44:34.855520: step 1414, loss 0.924365, acc 0.78125, prec 0.0432668, recall 0.823704
2017-12-10T12:44:35.296186: step 1415, loss 0.657087, acc 0.765625, prec 0.0432416, recall 0.823704
2017-12-10T12:44:35.743282: step 1416, loss 1.09033, acc 0.703125, prec 0.043284, recall 0.823964
2017-12-10T12:44:36.193978: step 1417, loss 1.77127, acc 0.59375, prec 0.0432774, recall 0.824095
2017-12-10T12:44:36.635029: step 1418, loss 1.19708, acc 0.625, prec 0.0432372, recall 0.824095
2017-12-10T12:44:37.077482: step 1419, loss 1.36998, acc 0.5625, prec 0.0431903, recall 0.824095
2017-12-10T12:44:37.523991: step 1420, loss 0.966778, acc 0.734375, prec 0.0431618, recall 0.824095
2017-12-10T12:44:37.974955: step 1421, loss 0.93791, acc 0.765625, prec 0.0431738, recall 0.824225
2017-12-10T12:44:38.419330: step 1422, loss 1.68138, acc 0.625, prec 0.0431708, recall 0.824354
2017-12-10T12:44:38.865415: step 1423, loss 4.44179, acc 0.703125, prec 0.0431777, recall 0.823876
2017-12-10T12:44:39.310824: step 1424, loss 0.977747, acc 0.6875, prec 0.0431444, recall 0.823876
2017-12-10T12:44:39.747283: step 1425, loss 2.78033, acc 0.640625, prec 0.0431447, recall 0.8234
2017-12-10T12:44:40.200810: step 1426, loss 0.835985, acc 0.78125, prec 0.0431214, recall 0.8234
2017-12-10T12:44:40.630005: step 1427, loss 0.468263, acc 0.828125, prec 0.04314, recall 0.823529
2017-12-10T12:44:41.075595: step 1428, loss 1.00015, acc 0.671875, prec 0.0431051, recall 0.823529
2017-12-10T12:44:41.520882: step 1429, loss 0.764989, acc 0.71875, prec 0.0431121, recall 0.823659
2017-12-10T12:44:41.973270: step 1430, loss 0.873178, acc 0.78125, prec 0.0431256, recall 0.823789
2017-12-10T12:44:42.442442: step 1431, loss 1.20234, acc 0.75, prec 0.0430991, recall 0.823789
2017-12-10T12:44:42.882618: step 1432, loss 0.596839, acc 0.8125, prec 0.043116, recall 0.823918
2017-12-10T12:44:43.321799: step 1433, loss 0.952495, acc 0.796875, prec 0.0430945, recall 0.823918
2017-12-10T12:44:43.766628: step 1434, loss 0.616474, acc 0.78125, prec 0.0431081, recall 0.824047
2017-12-10T12:44:44.202692: step 1435, loss 0.787981, acc 0.75, prec 0.0430816, recall 0.824047
2017-12-10T12:44:44.648677: step 1436, loss 0.484201, acc 0.828125, prec 0.0430635, recall 0.824047
2017-12-10T12:44:45.092523: step 1437, loss 0.583912, acc 0.765625, prec 0.0430754, recall 0.824176
2017-12-10T12:44:45.541275: step 1438, loss 0.499134, acc 0.875, prec 0.0430988, recall 0.824305
2017-12-10T12:44:45.982833: step 1439, loss 2.37913, acc 0.875, prec 0.0431239, recall 0.82383
2017-12-10T12:44:46.416007: step 1440, loss 0.557101, acc 0.765625, prec 0.0430992, recall 0.82383
2017-12-10T12:44:46.864132: step 1441, loss 0.341612, acc 0.890625, prec 0.0431242, recall 0.823959
2017-12-10T12:44:47.310862: step 1442, loss 0.355356, acc 0.890625, prec 0.0431492, recall 0.824088
2017-12-10T12:44:47.753998: step 1443, loss 0.25189, acc 0.90625, prec 0.0431759, recall 0.824216
2017-12-10T12:44:48.200661: step 1444, loss 0.280519, acc 0.90625, prec 0.043166, recall 0.824216
2017-12-10T12:44:48.654933: step 1445, loss 0.533572, acc 0.84375, prec 0.0431495, recall 0.824216
2017-12-10T12:44:49.099377: step 1446, loss 0.412417, acc 0.90625, prec 0.0431762, recall 0.824344
2017-12-10T12:44:49.541755: step 1447, loss 1.13121, acc 0.90625, prec 0.0432393, recall 0.8246
2017-12-10T12:44:49.984938: step 1448, loss 3.84368, acc 0.859375, prec 0.0432261, recall 0.824
2017-12-10T12:44:50.440929: step 1449, loss 0.519789, acc 0.90625, prec 0.0432527, recall 0.824128
2017-12-10T12:44:50.901284: step 1450, loss 0.266419, acc 0.921875, prec 0.0432445, recall 0.824128
2017-12-10T12:44:51.369151: step 1451, loss 0.483316, acc 0.859375, prec 0.0432296, recall 0.824128
2017-12-10T12:44:51.816471: step 1452, loss 0.646763, acc 0.8125, prec 0.0432099, recall 0.824128
2017-12-10T12:44:52.269367: step 1453, loss 0.398862, acc 0.921875, prec 0.0432745, recall 0.824383
2017-12-10T12:44:52.715274: step 1454, loss 0.32917, acc 0.890625, prec 0.0432994, recall 0.824511
2017-12-10T12:44:53.155417: step 1455, loss 0.283705, acc 0.875, prec 0.0432863, recall 0.824511
2017-12-10T12:44:53.584836: step 1456, loss 0.284525, acc 0.921875, prec 0.0433508, recall 0.824765
2017-12-10T12:44:54.021364: step 1457, loss 0.50155, acc 0.90625, prec 0.0433409, recall 0.824765
2017-12-10T12:44:54.462747: step 1458, loss 0.278036, acc 0.9375, prec 0.0433343, recall 0.824765
2017-12-10T12:44:54.908107: step 1459, loss 0.506097, acc 0.828125, prec 0.0433162, recall 0.824765
2017-12-10T12:44:55.330924: step 1460, loss 0.307408, acc 0.890625, prec 0.0433411, recall 0.824891
2017-12-10T12:44:55.774046: step 1461, loss 0.124477, acc 0.984375, prec 0.0433758, recall 0.825018
2017-12-10T12:44:56.214381: step 1462, loss 0.219783, acc 0.890625, prec 0.0433642, recall 0.825018
2017-12-10T12:44:56.657937: step 1463, loss 0.205467, acc 0.921875, prec 0.043356, recall 0.825018
2017-12-10T12:44:57.108855: step 1464, loss 0.153953, acc 0.9375, prec 0.0433858, recall 0.825145
2017-12-10T12:44:57.551537: step 1465, loss 0.404233, acc 0.890625, prec 0.0434469, recall 0.825397
2017-12-10T12:44:57.998096: step 1466, loss 0.256853, acc 0.9375, prec 0.0434403, recall 0.825397
2017-12-10T12:44:58.442561: step 1467, loss 1.21709, acc 0.921875, prec 0.043541, recall 0.825774
2017-12-10T12:44:58.900730: step 1468, loss 0.357073, acc 0.90625, prec 0.0435311, recall 0.825774
2017-12-10T12:44:59.345101: step 1469, loss 1.80157, acc 0.90625, prec 0.0435591, recall 0.825306
2017-12-10T12:44:59.787548: step 1470, loss 0.324373, acc 0.90625, prec 0.0436217, recall 0.825556
2017-12-10T12:45:00.227771: step 1471, loss 0.34734, acc 0.96875, prec 0.043691, recall 0.825806
2017-12-10T12:45:00.669500: step 1472, loss 0.365916, acc 0.90625, prec 0.043681, recall 0.825806
2017-12-10T12:45:01.126497: step 1473, loss 1.46952, acc 0.84375, prec 0.043737, recall 0.826056
2017-12-10T12:45:01.584284: step 1474, loss 0.658401, acc 0.84375, prec 0.0437929, recall 0.826304
2017-12-10T12:45:02.056234: step 1475, loss 0.197522, acc 0.90625, prec 0.0437829, recall 0.826304
2017-12-10T12:45:02.507197: step 1476, loss 0.381173, acc 0.921875, prec 0.0438108, recall 0.826429
2017-12-10T12:45:02.950072: step 1477, loss 0.21426, acc 0.921875, prec 0.0438025, recall 0.826429
2017-12-10T12:45:03.406722: step 1478, loss 0.538935, acc 0.84375, prec 0.0438221, recall 0.826552
2017-12-10T12:45:03.846894: step 1479, loss 0.525023, acc 0.828125, prec 0.0438401, recall 0.826676
2017-12-10T12:45:04.293480: step 1480, loss 0.461994, acc 0.875, prec 0.043863, recall 0.8268
2017-12-10T12:45:04.747043: step 1481, loss 0.838788, acc 0.796875, prec 0.0438775, recall 0.826923
2017-12-10T12:45:05.188523: step 1482, loss 0.463196, acc 0.859375, prec 0.0438988, recall 0.827046
2017-12-10T12:45:05.631642: step 1483, loss 5.72232, acc 0.859375, prec 0.0438855, recall 0.826458
2017-12-10T12:45:06.069334: step 1484, loss 0.592588, acc 0.84375, prec 0.043905, recall 0.826581
2017-12-10T12:45:06.510361: step 1485, loss 0.290354, acc 0.890625, prec 0.0439295, recall 0.826705
2017-12-10T12:45:06.952107: step 1486, loss 1.23278, acc 0.640625, prec 0.0438914, recall 0.826705
2017-12-10T12:45:07.401344: step 1487, loss 0.699822, acc 0.8125, prec 0.0439076, recall 0.826828
2017-12-10T12:45:07.841330: step 1488, loss 0.527183, acc 0.8125, prec 0.0439238, recall 0.82695
2017-12-10T12:45:08.275072: step 1489, loss 0.630506, acc 0.78125, prec 0.0439006, recall 0.82695
2017-12-10T12:45:08.731405: step 1490, loss 0.628852, acc 0.84375, prec 0.0438841, recall 0.82695
2017-12-10T12:45:09.138145: step 1491, loss 0.468724, acc 0.865385, prec 0.0438725, recall 0.82695
2017-12-10T12:45:09.591438: step 1492, loss 1.01665, acc 0.859375, prec 0.0439296, recall 0.827195
2017-12-10T12:45:10.042174: step 1493, loss 0.279875, acc 0.90625, prec 0.0439197, recall 0.827195
2017-12-10T12:45:10.495459: step 1494, loss 0.566427, acc 0.828125, prec 0.0439375, recall 0.827318
2017-12-10T12:45:10.956781: step 1495, loss 0.492039, acc 0.859375, prec 0.0439585, recall 0.82744
2017-12-10T12:45:11.416673: step 1496, loss 0.350253, acc 0.859375, prec 0.0439796, recall 0.827562
2017-12-10T12:45:11.877320: step 1497, loss 0.63487, acc 0.8125, prec 0.0439956, recall 0.827684
2017-12-10T12:45:12.330154: step 1498, loss 0.242102, acc 0.90625, prec 0.0439857, recall 0.827684
2017-12-10T12:45:12.782565: step 1499, loss 0.383171, acc 0.859375, prec 0.0440068, recall 0.827805
2017-12-10T12:45:13.229488: step 1500, loss 0.305241, acc 0.875, prec 0.0440294, recall 0.827927
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1500

2017-12-10T12:45:15.152533: step 1501, loss 0.211061, acc 0.9375, prec 0.0440586, recall 0.828048
2017-12-10T12:45:15.597260: step 1502, loss 0.713414, acc 0.8125, prec 0.0441463, recall 0.828411
2017-12-10T12:45:16.036192: step 1503, loss 0.268329, acc 0.90625, prec 0.0441722, recall 0.828531
2017-12-10T12:45:16.485311: step 1504, loss 0.354888, acc 0.875, prec 0.0442306, recall 0.828772
2017-12-10T12:45:16.964110: step 1505, loss 0.763717, acc 0.875, prec 0.0442889, recall 0.829012
2017-12-10T12:45:17.421960: step 1506, loss 0.505887, acc 0.890625, prec 0.0442773, recall 0.829012
2017-12-10T12:45:17.863801: step 1507, loss 0.14761, acc 0.90625, prec 0.0442673, recall 0.829012
2017-12-10T12:45:18.319240: step 1508, loss 1.69278, acc 0.875, prec 0.0442915, recall 0.828551
2017-12-10T12:45:18.771834: step 1509, loss 1.65121, acc 0.921875, prec 0.0442849, recall 0.827972
2017-12-10T12:45:19.218710: step 1510, loss 0.398674, acc 0.9375, prec 0.044314, recall 0.828092
2017-12-10T12:45:19.677699: step 1511, loss 0.552403, acc 0.859375, prec 0.0443348, recall 0.828212
2017-12-10T12:45:20.105056: step 1512, loss 0.660672, acc 0.84375, prec 0.0443539, recall 0.828332
2017-12-10T12:45:20.556969: step 1513, loss 0.272991, acc 0.921875, prec 0.0443814, recall 0.828452
2017-12-10T12:45:21.006271: step 1514, loss 0.349945, acc 0.859375, prec 0.0444021, recall 0.828571
2017-12-10T12:45:21.457718: step 1515, loss 0.342505, acc 0.875, prec 0.0443889, recall 0.828571
2017-12-10T12:45:21.905140: step 1516, loss 0.524309, acc 0.875, prec 0.0444113, recall 0.828691
2017-12-10T12:45:22.344573: step 1517, loss 0.418481, acc 0.84375, prec 0.0443947, recall 0.828691
2017-12-10T12:45:22.798706: step 1518, loss 0.459128, acc 0.875, prec 0.0444171, recall 0.82881
2017-12-10T12:45:23.236759: step 1519, loss 0.184178, acc 0.921875, prec 0.0444444, recall 0.828929
2017-12-10T12:45:23.680259: step 1520, loss 0.738526, acc 0.8125, prec 0.0444246, recall 0.828929
2017-12-10T12:45:24.132304: step 1521, loss 0.20865, acc 0.90625, prec 0.0444146, recall 0.828929
2017-12-10T12:45:24.584584: step 1522, loss 0.20395, acc 0.90625, prec 0.0444047, recall 0.828929
2017-12-10T12:45:25.021569: step 1523, loss 0.331612, acc 0.9375, prec 0.0443981, recall 0.828929
2017-12-10T12:45:25.467983: step 1524, loss 0.392441, acc 0.921875, prec 0.0443898, recall 0.828929
2017-12-10T12:45:25.910547: step 1525, loss 0.381121, acc 0.890625, prec 0.0444138, recall 0.829048
2017-12-10T12:45:26.361316: step 1526, loss 0.38558, acc 0.921875, prec 0.0444056, recall 0.829048
2017-12-10T12:45:26.807285: step 1527, loss 0.12111, acc 0.96875, prec 0.0444023, recall 0.829048
2017-12-10T12:45:27.276539: step 1528, loss 0.186287, acc 0.9375, prec 0.0443957, recall 0.829048
2017-12-10T12:45:27.720581: step 1529, loss 0.140598, acc 0.953125, prec 0.0444263, recall 0.829167
2017-12-10T12:45:28.164444: step 1530, loss 0.246122, acc 0.921875, prec 0.0444535, recall 0.829285
2017-12-10T12:45:28.606602: step 1531, loss 0.24238, acc 0.953125, prec 0.0444486, recall 0.829285
2017-12-10T12:45:29.058760: step 1532, loss 5.57419, acc 0.9375, prec 0.0444436, recall 0.82871
2017-12-10T12:45:29.494315: step 1533, loss 2.6304, acc 0.921875, prec 0.044437, recall 0.828136
2017-12-10T12:45:29.927352: step 1534, loss 0.255277, acc 0.921875, prec 0.0444998, recall 0.828374
2017-12-10T12:45:30.371833: step 1535, loss 0.307318, acc 0.9375, prec 0.0444932, recall 0.828374
2017-12-10T12:45:30.821279: step 1536, loss 0.358422, acc 0.90625, prec 0.0444833, recall 0.828374
2017-12-10T12:45:31.274661: step 1537, loss 0.499724, acc 0.875, prec 0.0445055, recall 0.828492
2017-12-10T12:45:31.702099: step 1538, loss 0.691366, acc 0.875, prec 0.0445633, recall 0.828729
2017-12-10T12:45:32.154397: step 1539, loss 0.485491, acc 0.890625, prec 0.0445872, recall 0.828847
2017-12-10T12:45:32.598793: step 1540, loss 0.573803, acc 0.8125, prec 0.0445673, recall 0.828847
2017-12-10T12:45:33.052195: step 1541, loss 0.375919, acc 0.875, prec 0.044625, recall 0.829083
2017-12-10T12:45:33.516843: step 1542, loss 0.809827, acc 0.78125, prec 0.0446018, recall 0.829083
2017-12-10T12:45:33.958469: step 1543, loss 0.610863, acc 0.859375, prec 0.0445869, recall 0.829083
2017-12-10T12:45:34.404598: step 1544, loss 0.75256, acc 0.78125, prec 0.0445992, recall 0.829201
2017-12-10T12:45:34.859351: step 1545, loss 0.564256, acc 0.875, prec 0.0446214, recall 0.829319
2017-12-10T12:45:35.297469: step 1546, loss 0.592736, acc 0.828125, prec 0.0446032, recall 0.829319
2017-12-10T12:45:35.740543: step 1547, loss 0.441139, acc 0.84375, prec 0.0445867, recall 0.829319
2017-12-10T12:45:36.189485: step 1548, loss 0.594175, acc 0.90625, prec 0.0446121, recall 0.829436
2017-12-10T12:45:36.621852: step 1549, loss 0.317126, acc 0.90625, prec 0.0446729, recall 0.82967
2017-12-10T12:45:37.066517: step 1550, loss 0.292428, acc 0.890625, prec 0.0446967, recall 0.829787
2017-12-10T12:45:37.505161: step 1551, loss 0.264121, acc 0.921875, prec 0.0447237, recall 0.829904
2017-12-10T12:45:37.951094: step 1552, loss 1.77133, acc 0.921875, prec 0.0447524, recall 0.829452
2017-12-10T12:45:38.398610: step 1553, loss 0.539973, acc 0.875, prec 0.0447392, recall 0.829452
2017-12-10T12:45:38.853626: step 1554, loss 0.26951, acc 0.953125, prec 0.0447342, recall 0.829452
2017-12-10T12:45:39.299421: step 1555, loss 0.260162, acc 0.9375, prec 0.0447629, recall 0.829569
2017-12-10T12:45:39.749411: step 1556, loss 0.499025, acc 0.875, prec 0.0447497, recall 0.829569
2017-12-10T12:45:40.191029: step 1557, loss 0.23886, acc 0.90625, prec 0.0447398, recall 0.829569
2017-12-10T12:45:40.625348: step 1558, loss 0.262966, acc 0.921875, prec 0.0447668, recall 0.829685
2017-12-10T12:45:41.066349: step 1559, loss 0.161948, acc 0.921875, prec 0.0447937, recall 0.829802
2017-12-10T12:45:41.518818: step 1560, loss 0.169568, acc 0.921875, prec 0.0448207, recall 0.829918
2017-12-10T12:45:41.950396: step 1561, loss 0.262353, acc 0.9375, prec 0.0448846, recall 0.83015
2017-12-10T12:45:42.396475: step 1562, loss 0.268942, acc 0.9375, prec 0.0449132, recall 0.830266
2017-12-10T12:45:42.835583: step 1563, loss 4.74071, acc 0.875, prec 0.0449016, recall 0.8297
2017-12-10T12:45:43.287318: step 1564, loss 0.0545004, acc 0.984375, prec 0.0448999, recall 0.8297
2017-12-10T12:45:43.748752: step 1565, loss 0.310987, acc 0.953125, prec 0.0450357, recall 0.830163
2017-12-10T12:45:44.188046: step 1566, loss 0.505876, acc 0.890625, prec 0.0450241, recall 0.830163
2017-12-10T12:45:44.634675: step 1567, loss 0.314823, acc 0.890625, prec 0.0450477, recall 0.830278
2017-12-10T12:45:45.064570: step 1568, loss 0.819548, acc 0.84375, prec 0.0450311, recall 0.830278
2017-12-10T12:45:45.509177: step 1569, loss 0.278428, acc 0.921875, prec 0.0450931, recall 0.830508
2017-12-10T12:45:45.946733: step 1570, loss 0.345851, acc 0.90625, prec 0.0451183, recall 0.830623
2017-12-10T12:45:46.384103: step 1571, loss 0.510593, acc 0.84375, prec 0.0451368, recall 0.830738
2017-12-10T12:45:46.817488: step 1572, loss 0.317896, acc 0.890625, prec 0.0452306, recall 0.831081
2017-12-10T12:45:47.252115: step 1573, loss 0.348706, acc 0.921875, prec 0.0452574, recall 0.831195
2017-12-10T12:45:47.717187: step 1574, loss 0.428414, acc 0.84375, prec 0.0452407, recall 0.831195
2017-12-10T12:45:48.168540: step 1575, loss 0.139788, acc 0.953125, prec 0.0452708, recall 0.831309
2017-12-10T12:45:48.612897: step 1576, loss 2.94835, acc 0.796875, prec 0.045321, recall 0.830976
2017-12-10T12:45:49.058619: step 1577, loss 0.220833, acc 0.9375, prec 0.0453143, recall 0.830976
2017-12-10T12:45:49.512656: step 1578, loss 2.04102, acc 0.921875, prec 0.0453077, recall 0.830417
2017-12-10T12:45:49.963155: step 1579, loss 0.747217, acc 0.796875, prec 0.0453211, recall 0.830531
2017-12-10T12:45:50.400524: step 1580, loss 0.299283, acc 0.921875, prec 0.0453128, recall 0.830531
2017-12-10T12:45:50.849723: step 1581, loss 0.297602, acc 0.921875, prec 0.0453395, recall 0.830645
2017-12-10T12:45:51.314243: step 1582, loss 0.975364, acc 0.8125, prec 0.0454246, recall 0.830986
2017-12-10T12:45:51.758723: step 1583, loss 0.794519, acc 0.765625, prec 0.0453996, recall 0.830986
2017-12-10T12:45:52.219762: step 1584, loss 0.770158, acc 0.828125, prec 0.0453813, recall 0.830986
2017-12-10T12:45:52.665705: step 1585, loss 0.858685, acc 0.75, prec 0.0453547, recall 0.830986
2017-12-10T12:45:53.115856: step 1586, loss 1.47676, acc 0.859375, prec 0.0454096, recall 0.831212
2017-12-10T12:45:53.556275: step 1587, loss 1.25902, acc 0.78125, prec 0.0454213, recall 0.831325
2017-12-10T12:45:53.994633: step 1588, loss 0.922766, acc 0.734375, prec 0.0453931, recall 0.831325
2017-12-10T12:45:54.434658: step 1589, loss 0.59516, acc 0.78125, prec 0.0453699, recall 0.831325
2017-12-10T12:45:54.870388: step 1590, loss 0.664871, acc 0.828125, prec 0.0453865, recall 0.831438
2017-12-10T12:45:55.306422: step 1591, loss 1.00194, acc 0.765625, prec 0.0454313, recall 0.831663
2017-12-10T12:45:55.740539: step 1592, loss 0.837019, acc 0.84375, prec 0.0454148, recall 0.831663
2017-12-10T12:45:56.183414: step 1593, loss 0.569468, acc 0.8125, prec 0.0453949, recall 0.831663
2017-12-10T12:45:56.632533: step 1594, loss 0.379459, acc 0.828125, prec 0.0453767, recall 0.831663
2017-12-10T12:45:57.078235: step 1595, loss 0.595211, acc 0.859375, prec 0.0453618, recall 0.831663
2017-12-10T12:45:57.513890: step 1596, loss 0.372297, acc 0.859375, prec 0.0453469, recall 0.831663
2017-12-10T12:45:57.965119: step 1597, loss 0.315885, acc 0.890625, prec 0.0453701, recall 0.831776
2017-12-10T12:45:58.404174: step 1598, loss 0.226984, acc 0.953125, prec 0.0453652, recall 0.831776
2017-12-10T12:45:58.843041: step 1599, loss 0.296614, acc 0.90625, prec 0.0453553, recall 0.831776
2017-12-10T12:45:59.291439: step 1600, loss 0.634495, acc 0.890625, prec 0.0453785, recall 0.831888
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1600

2017-12-10T12:46:01.437425: step 1601, loss 0.204438, acc 0.90625, prec 0.0453686, recall 0.831888
2017-12-10T12:46:01.882865: step 1602, loss 0.154842, acc 0.953125, prec 0.0453983, recall 0.832
2017-12-10T12:46:02.338063: step 1603, loss 2.40995, acc 0.90625, prec 0.0454942, recall 0.831782
2017-12-10T12:46:02.776288: step 1604, loss 0.226824, acc 0.953125, prec 0.0454893, recall 0.831782
2017-12-10T12:46:03.227247: step 1605, loss 0.134937, acc 0.984375, prec 0.045557, recall 0.832005
2017-12-10T12:46:03.662226: step 1606, loss 0.302579, acc 0.84375, prec 0.0455405, recall 0.832005
2017-12-10T12:46:04.104070: step 1607, loss 0.313253, acc 0.890625, prec 0.0455289, recall 0.832005
2017-12-10T12:46:04.551509: step 1608, loss 0.20358, acc 0.9375, prec 0.0455223, recall 0.832005
2017-12-10T12:46:05.000102: step 1609, loss 0.0710155, acc 0.96875, prec 0.0455189, recall 0.832005
2017-12-10T12:46:05.447324: step 1610, loss 0.230309, acc 0.953125, prec 0.045514, recall 0.832005
2017-12-10T12:46:05.892652: step 1611, loss 0.348916, acc 0.90625, prec 0.0455734, recall 0.832228
2017-12-10T12:46:06.342084: step 1612, loss 2.36728, acc 0.953125, prec 0.045707, recall 0.832672
2017-12-10T12:46:06.797327: step 1613, loss 0.693095, acc 0.9375, prec 0.045735, recall 0.832783
2017-12-10T12:46:07.244781: step 1614, loss 0.524128, acc 0.90625, prec 0.0457251, recall 0.832783
2017-12-10T12:46:07.693727: step 1615, loss 1.89879, acc 0.96875, prec 0.0457927, recall 0.832454
2017-12-10T12:46:08.142229: step 1616, loss 0.365581, acc 0.828125, prec 0.0457744, recall 0.832454
2017-12-10T12:46:08.591854: step 1617, loss 0.275098, acc 0.890625, prec 0.045832, recall 0.832675
2017-12-10T12:46:09.038462: step 1618, loss 0.341327, acc 0.890625, prec 0.0458549, recall 0.832785
2017-12-10T12:46:09.468878: step 1619, loss 0.666895, acc 0.796875, prec 0.0458333, recall 0.832785
2017-12-10T12:46:09.912382: step 1620, loss 0.627964, acc 0.84375, prec 0.0458167, recall 0.832785
2017-12-10T12:46:10.362213: step 1621, loss 1.0852, acc 0.765625, prec 0.0457919, recall 0.832785
2017-12-10T12:46:10.808968: step 1622, loss 0.621063, acc 0.84375, prec 0.0457753, recall 0.832785
2017-12-10T12:46:11.282061: step 1623, loss 0.524113, acc 0.84375, prec 0.0458278, recall 0.833005
2017-12-10T12:46:11.714623: step 1624, loss 0.663025, acc 0.828125, prec 0.045844, recall 0.833114
2017-12-10T12:46:12.159850: step 1625, loss 0.642126, acc 0.84375, prec 0.0459309, recall 0.833443
2017-12-10T12:46:12.621368: step 1626, loss 0.570731, acc 0.796875, prec 0.0459783, recall 0.833661
2017-12-10T12:46:13.068193: step 1627, loss 0.625971, acc 0.84375, prec 0.0459961, recall 0.83377
2017-12-10T12:46:13.514914: step 1628, loss 0.570918, acc 0.875, prec 0.0460173, recall 0.833878
2017-12-10T12:46:13.962027: step 1629, loss 0.249268, acc 0.96875, prec 0.0460139, recall 0.833878
2017-12-10T12:46:14.403710: step 1630, loss 0.733233, acc 0.921875, prec 0.0461089, recall 0.834204
2017-12-10T12:46:14.849356: step 1631, loss 0.462998, acc 0.875, prec 0.0461644, recall 0.83442
2017-12-10T12:46:15.306046: step 1632, loss 0.260718, acc 0.859375, prec 0.0461494, recall 0.83442
2017-12-10T12:46:15.747491: step 1633, loss 0.314598, acc 0.859375, prec 0.0461344, recall 0.83442
2017-12-10T12:46:16.199516: step 1634, loss 0.15993, acc 0.921875, prec 0.0461605, recall 0.834528
2017-12-10T12:46:16.657959: step 1635, loss 0.132903, acc 0.984375, prec 0.0461932, recall 0.834635
2017-12-10T12:46:17.088146: step 1636, loss 0.345031, acc 0.90625, prec 0.0462519, recall 0.83485
2017-12-10T12:46:17.539506: step 1637, loss 0.494412, acc 0.84375, prec 0.0462696, recall 0.834958
2017-12-10T12:46:17.977127: step 1638, loss 0.103241, acc 0.96875, prec 0.0462663, recall 0.834958
2017-12-10T12:46:18.420606: step 1639, loss 0.339236, acc 0.890625, prec 0.0462546, recall 0.834958
2017-12-10T12:46:18.869245: step 1640, loss 0.314226, acc 0.9375, prec 0.0462823, recall 0.835065
2017-12-10T12:46:19.304716: step 1641, loss 0.311482, acc 0.921875, prec 0.046274, recall 0.835065
2017-12-10T12:46:19.723648: step 1642, loss 0.264582, acc 0.9375, prec 0.0463016, recall 0.835172
2017-12-10T12:46:20.161858: step 1643, loss 0.226526, acc 0.9375, prec 0.0463636, recall 0.835386
2017-12-10T12:46:20.610243: step 1644, loss 0.409335, acc 0.9375, prec 0.0464255, recall 0.835599
2017-12-10T12:46:21.063419: step 1645, loss 0.160321, acc 0.9375, prec 0.0464188, recall 0.835599
2017-12-10T12:46:21.502514: step 1646, loss 0.330254, acc 0.90625, prec 0.0464088, recall 0.835599
2017-12-10T12:46:21.954245: step 1647, loss 0.299219, acc 0.890625, prec 0.0463971, recall 0.835599
2017-12-10T12:46:22.418709: step 1648, loss 0.22874, acc 0.921875, prec 0.0464231, recall 0.835705
2017-12-10T12:46:22.871780: step 1649, loss 0.138307, acc 0.9375, prec 0.0464849, recall 0.835917
2017-12-10T12:46:23.325278: step 1650, loss 0.17931, acc 0.953125, prec 0.0464799, recall 0.835917
2017-12-10T12:46:23.778269: step 1651, loss 0.21208, acc 0.96875, prec 0.0464765, recall 0.835917
2017-12-10T12:46:24.232861: step 1652, loss 0.0531561, acc 0.96875, prec 0.0464732, recall 0.835917
2017-12-10T12:46:24.677729: step 1653, loss 0.169552, acc 0.96875, prec 0.0464699, recall 0.835917
2017-12-10T12:46:25.128870: step 1654, loss 0.382357, acc 0.90625, prec 0.0464941, recall 0.836023
2017-12-10T12:46:25.581934: step 1655, loss 0.136107, acc 0.9375, prec 0.0464874, recall 0.836023
2017-12-10T12:46:26.039740: step 1656, loss 0.0886874, acc 0.984375, prec 0.0464857, recall 0.836023
2017-12-10T12:46:26.478378: step 1657, loss 0.652678, acc 0.96875, prec 0.0465166, recall 0.836129
2017-12-10T12:46:26.935127: step 1658, loss 1.24254, acc 0.984375, prec 0.0465834, recall 0.83634
2017-12-10T12:46:27.393295: step 1659, loss 0.0267106, acc 1, prec 0.0465834, recall 0.83634
2017-12-10T12:46:27.844418: step 1660, loss 0.0408637, acc 0.984375, prec 0.0465817, recall 0.83634
2017-12-10T12:46:28.294695: step 1661, loss 0.0361743, acc 1, prec 0.0466159, recall 0.836446
2017-12-10T12:46:28.736231: step 1662, loss 0.202716, acc 0.96875, prec 0.0466126, recall 0.836446
2017-12-10T12:46:29.182439: step 1663, loss 0.371714, acc 0.90625, prec 0.0466368, recall 0.836551
2017-12-10T12:46:29.628397: step 1664, loss 3.45081, acc 0.9375, prec 0.0467001, recall 0.836223
2017-12-10T12:46:30.079739: step 1665, loss 0.17934, acc 0.9375, prec 0.0466934, recall 0.836223
2017-12-10T12:46:30.522994: step 1666, loss 0.330802, acc 0.9375, prec 0.0466867, recall 0.836223
2017-12-10T12:46:30.967856: step 1667, loss 0.449597, acc 0.90625, prec 0.0466767, recall 0.836223
2017-12-10T12:46:31.416270: step 1668, loss 0.210534, acc 0.890625, prec 0.046665, recall 0.836223
2017-12-10T12:46:31.874964: step 1669, loss 0.530913, acc 0.875, prec 0.0467199, recall 0.836434
2017-12-10T12:46:32.307729: step 1670, loss 0.79532, acc 0.78125, prec 0.0466965, recall 0.836434
2017-12-10T12:46:32.745583: step 1671, loss 0.397928, acc 0.921875, prec 0.0467564, recall 0.836643
2017-12-10T12:46:33.190650: step 1672, loss 0.276923, acc 0.890625, prec 0.0467447, recall 0.836643
2017-12-10T12:46:33.638684: step 1673, loss 0.276103, acc 0.875, prec 0.0467313, recall 0.836643
2017-12-10T12:46:34.069922: step 1674, loss 0.447235, acc 0.84375, prec 0.0467146, recall 0.836643
2017-12-10T12:46:34.525487: step 1675, loss 0.557002, acc 0.875, prec 0.0467353, recall 0.836748
2017-12-10T12:46:34.974870: step 1676, loss 0.577384, acc 0.828125, prec 0.0467851, recall 0.836957
2017-12-10T12:46:35.425553: step 1677, loss 0.868379, acc 0.796875, prec 0.0468315, recall 0.837165
2017-12-10T12:46:35.861757: step 1678, loss 0.567098, acc 0.828125, prec 0.0468131, recall 0.837165
2017-12-10T12:46:36.310276: step 1679, loss 0.420918, acc 0.859375, prec 0.046798, recall 0.837165
2017-12-10T12:46:36.757822: step 1680, loss 0.42857, acc 0.90625, prec 0.046856, recall 0.837372
2017-12-10T12:46:37.200545: step 1681, loss 0.739032, acc 0.859375, prec 0.046875, recall 0.837476
2017-12-10T12:46:37.644710: step 1682, loss 1.02625, acc 0.90625, prec 0.046899, recall 0.83758
2017-12-10T12:46:38.101135: step 1683, loss 0.683053, acc 0.875, prec 0.0469196, recall 0.837683
2017-12-10T12:46:38.561670: step 1684, loss 0.278046, acc 0.890625, prec 0.0469079, recall 0.837683
2017-12-10T12:46:39.007908: step 1685, loss 0.23777, acc 0.890625, prec 0.0468962, recall 0.837683
2017-12-10T12:46:39.463238: step 1686, loss 0.412729, acc 0.90625, prec 0.046988, recall 0.837992
2017-12-10T12:46:39.921344: step 1687, loss 1.59144, acc 0.84375, prec 0.0470069, recall 0.837563
2017-12-10T12:46:40.371007: step 1688, loss 2.30113, acc 0.859375, prec 0.0469935, recall 0.837032
2017-12-10T12:46:40.812654: step 1689, loss 1.27915, acc 0.796875, prec 0.0470057, recall 0.837136
2017-12-10T12:46:41.258676: step 1690, loss 0.624929, acc 0.8125, prec 0.0469856, recall 0.837136
2017-12-10T12:46:41.701561: step 1691, loss 4.07124, acc 0.875, prec 0.0470078, recall 0.836709
2017-12-10T12:46:42.145662: step 1692, loss 0.594029, acc 0.828125, prec 0.0469894, recall 0.836709
2017-12-10T12:46:42.594322: step 1693, loss 0.84659, acc 0.84375, prec 0.0470404, recall 0.836915
2017-12-10T12:46:43.036309: step 1694, loss 0.965666, acc 0.734375, prec 0.047012, recall 0.836915
2017-12-10T12:46:43.481159: step 1695, loss 0.985659, acc 0.71875, prec 0.046982, recall 0.836915
2017-12-10T12:46:43.926342: step 1696, loss 1.30483, acc 0.640625, prec 0.0469437, recall 0.836915
2017-12-10T12:46:44.363254: step 1697, loss 1.06157, acc 0.734375, prec 0.0469154, recall 0.836915
2017-12-10T12:46:44.803734: step 1698, loss 0.449895, acc 0.890625, prec 0.0470051, recall 0.837224
2017-12-10T12:46:45.243049: step 1699, loss 0.874972, acc 0.75, prec 0.0470122, recall 0.837327
2017-12-10T12:46:45.692822: step 1700, loss 0.985518, acc 0.71875, prec 0.0469822, recall 0.837327
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1700

2017-12-10T12:46:47.666972: step 1701, loss 0.403619, acc 0.796875, prec 0.0469943, recall 0.837429
2017-12-10T12:46:48.117422: step 1702, loss 0.454098, acc 0.84375, prec 0.0469777, recall 0.837429
2017-12-10T12:46:48.566587: step 1703, loss 0.698092, acc 0.796875, prec 0.0469898, recall 0.837532
2017-12-10T12:46:49.003811: step 1704, loss 0.957748, acc 0.765625, prec 0.0470322, recall 0.837736
2017-12-10T12:46:49.446995: step 1705, loss 0.509544, acc 0.890625, prec 0.0470879, recall 0.83794
2017-12-10T12:46:49.892076: step 1706, loss 0.510627, acc 0.78125, prec 0.0470646, recall 0.83794
2017-12-10T12:46:50.339666: step 1707, loss 0.721478, acc 0.828125, prec 0.0471136, recall 0.838143
2017-12-10T12:46:50.798199: step 1708, loss 0.881296, acc 0.9375, prec 0.0472413, recall 0.838548
2017-12-10T12:46:51.218864: step 1709, loss 0.278926, acc 0.890625, prec 0.0472632, recall 0.838649
2017-12-10T12:46:51.606401: step 1710, loss 0.290981, acc 0.859375, prec 0.0472818, recall 0.83875
2017-12-10T12:46:52.058876: step 1711, loss 0.403484, acc 0.875, prec 0.0472685, recall 0.83875
2017-12-10T12:46:52.503494: step 1712, loss 0.63002, acc 0.828125, prec 0.0472502, recall 0.83875
2017-12-10T12:46:52.957432: step 1713, loss 0.448543, acc 0.9375, prec 0.0472771, recall 0.838851
2017-12-10T12:46:53.405502: step 1714, loss 0.179268, acc 0.953125, prec 0.0473056, recall 0.838951
2017-12-10T12:46:53.855062: step 1715, loss 0.130546, acc 0.921875, prec 0.0472973, recall 0.838951
2017-12-10T12:46:54.299267: step 1716, loss 0.166471, acc 0.9375, prec 0.0472906, recall 0.838951
2017-12-10T12:46:54.748519: step 1717, loss 0.261463, acc 0.953125, prec 0.0473192, recall 0.839052
2017-12-10T12:46:55.208994: step 1718, loss 0.280039, acc 0.90625, prec 0.0473092, recall 0.839052
2017-12-10T12:46:55.650015: step 1719, loss 0.255962, acc 0.859375, prec 0.0472942, recall 0.839052
2017-12-10T12:46:56.119258: step 1720, loss 0.137326, acc 0.9375, prec 0.0473211, recall 0.839152
2017-12-10T12:46:56.552932: step 1721, loss 0.0404847, acc 0.984375, prec 0.0473194, recall 0.839152
2017-12-10T12:46:56.993430: step 1722, loss 0.153204, acc 0.953125, prec 0.0473479, recall 0.839252
2017-12-10T12:46:57.420493: step 1723, loss 0.0684599, acc 0.984375, prec 0.0473462, recall 0.839252
2017-12-10T12:46:57.866701: step 1724, loss 0.0330107, acc 0.984375, prec 0.0473446, recall 0.839252
2017-12-10T12:46:58.317178: step 1725, loss 0.270891, acc 0.890625, prec 0.0473664, recall 0.839352
2017-12-10T12:46:58.766258: step 1726, loss 1.60037, acc 0.953125, prec 0.0473631, recall 0.83883
2017-12-10T12:46:59.209733: step 1727, loss 0.38051, acc 0.984375, prec 0.0473949, recall 0.83893
2017-12-10T12:46:59.645060: step 1728, loss 0.0645141, acc 0.96875, prec 0.047425, recall 0.83903
2017-12-10T12:47:00.113298: step 1729, loss 0.0588399, acc 0.96875, prec 0.0474217, recall 0.83903
2017-12-10T12:47:00.562307: step 1730, loss 0.872326, acc 0.9375, prec 0.0475823, recall 0.839529
2017-12-10T12:47:01.008828: step 1731, loss 0.942739, acc 0.96875, prec 0.0476124, recall 0.839628
2017-12-10T12:47:01.448498: step 1732, loss 7.48699, acc 0.921875, prec 0.0476057, recall 0.839109
2017-12-10T12:47:01.905631: step 1733, loss 0.0793073, acc 0.953125, prec 0.0476007, recall 0.839109
2017-12-10T12:47:02.342640: step 1734, loss 2.23271, acc 0.90625, prec 0.0476257, recall 0.83869
2017-12-10T12:47:02.790551: step 1735, loss 0.829468, acc 0.921875, prec 0.0476508, recall 0.838789
2017-12-10T12:47:03.241992: step 1736, loss 0.99079, acc 0.78125, prec 0.0476608, recall 0.838889
2017-12-10T12:47:03.684743: step 1737, loss 1.24204, acc 0.71875, prec 0.0476641, recall 0.838988
2017-12-10T12:47:04.131510: step 1738, loss 0.908436, acc 0.796875, prec 0.0477091, recall 0.839187
2017-12-10T12:47:04.574975: step 1739, loss 1.2031, acc 0.78125, prec 0.0476857, recall 0.839187
2017-12-10T12:47:05.019740: step 1740, loss 1.01761, acc 0.78125, prec 0.0476957, recall 0.839286
2017-12-10T12:47:05.453428: step 1741, loss 1.12444, acc 0.703125, prec 0.0477639, recall 0.839582
2017-12-10T12:47:05.882737: step 1742, loss 1.02759, acc 0.734375, prec 0.0477688, recall 0.839681
2017-12-10T12:47:06.315456: step 1743, loss 0.950398, acc 0.671875, prec 0.0477338, recall 0.839681
2017-12-10T12:47:06.770348: step 1744, loss 0.771108, acc 0.734375, prec 0.0477055, recall 0.839681
2017-12-10T12:47:07.222254: step 1745, loss 1.48371, acc 0.703125, prec 0.0477071, recall 0.839779
2017-12-10T12:47:07.662062: step 1746, loss 1.26276, acc 0.65625, prec 0.0477037, recall 0.839877
2017-12-10T12:47:08.105016: step 1747, loss 1.09834, acc 0.703125, prec 0.0477053, recall 0.839975
2017-12-10T12:47:08.545957: step 1748, loss 1.40676, acc 0.640625, prec 0.0476671, recall 0.839975
2017-12-10T12:47:08.993700: step 1749, loss 1.61194, acc 0.6875, prec 0.0476671, recall 0.840074
2017-12-10T12:47:09.434271: step 1750, loss 1.30957, acc 0.703125, prec 0.0476356, recall 0.840074
2017-12-10T12:47:09.877518: step 1751, loss 0.952699, acc 0.78125, prec 0.0476786, recall 0.840269
2017-12-10T12:47:10.336198: step 1752, loss 0.873279, acc 0.78125, prec 0.0476554, recall 0.840269
2017-12-10T12:47:10.792355: step 1753, loss 0.628139, acc 0.859375, prec 0.0476405, recall 0.840269
2017-12-10T12:47:11.234201: step 1754, loss 0.700198, acc 0.765625, prec 0.0476157, recall 0.840269
2017-12-10T12:47:11.676093: step 1755, loss 0.49918, acc 0.828125, prec 0.0475976, recall 0.840269
2017-12-10T12:47:12.121273: step 1756, loss 0.397977, acc 0.828125, prec 0.0476124, recall 0.840367
2017-12-10T12:47:12.561326: step 1757, loss 0.418913, acc 0.890625, prec 0.0476669, recall 0.840562
2017-12-10T12:47:13.008182: step 1758, loss 0.254122, acc 0.953125, prec 0.0476619, recall 0.840562
2017-12-10T12:47:13.458194: step 1759, loss 0.332807, acc 0.890625, prec 0.0476504, recall 0.840562
2017-12-10T12:47:13.911509: step 1760, loss 0.292165, acc 0.9375, prec 0.0476768, recall 0.840659
2017-12-10T12:47:14.349305: step 1761, loss 0.0585348, acc 0.96875, prec 0.0477394, recall 0.840854
2017-12-10T12:47:14.795724: step 1762, loss 0.166984, acc 0.953125, prec 0.0477344, recall 0.840854
2017-12-10T12:47:15.236585: step 1763, loss 1.41381, acc 0.953125, prec 0.0477641, recall 0.840438
2017-12-10T12:47:15.692704: step 1764, loss 0.229321, acc 0.9375, prec 0.0477904, recall 0.840536
2017-12-10T12:47:16.141613: step 1765, loss 2.04837, acc 0.9375, prec 0.0477855, recall 0.840024
2017-12-10T12:47:16.594867: step 1766, loss 0.160531, acc 0.9375, prec 0.0478118, recall 0.840122
2017-12-10T12:47:17.041463: step 1767, loss 0.192853, acc 0.921875, prec 0.0478365, recall 0.840219
2017-12-10T12:47:17.479836: step 1768, loss 0.315013, acc 0.90625, prec 0.0478265, recall 0.840219
2017-12-10T12:47:17.931026: step 1769, loss 0.267075, acc 0.921875, prec 0.0478841, recall 0.840413
2017-12-10T12:47:18.368653: step 1770, loss 0.150644, acc 0.96875, prec 0.0478808, recall 0.840413
2017-12-10T12:47:18.834881: step 1771, loss 0.211284, acc 0.953125, prec 0.0479087, recall 0.840509
2017-12-10T12:47:19.275404: step 1772, loss 0.330968, acc 0.921875, prec 0.0479005, recall 0.840509
2017-12-10T12:47:19.714164: step 1773, loss 0.158804, acc 0.921875, prec 0.0479251, recall 0.840606
2017-12-10T12:47:20.159590: step 1774, loss 0.0964068, acc 0.9375, prec 0.0479185, recall 0.840606
2017-12-10T12:47:20.607863: step 1775, loss 0.449846, acc 0.90625, prec 0.0479414, recall 0.840703
2017-12-10T12:47:21.065638: step 1776, loss 0.927666, acc 0.953125, prec 0.0480351, recall 0.840992
2017-12-10T12:47:21.515086: step 1777, loss 0.0738432, acc 0.96875, prec 0.0480318, recall 0.840992
2017-12-10T12:47:21.964971: step 1778, loss 0.137828, acc 0.9375, prec 0.0480251, recall 0.840992
2017-12-10T12:47:22.405231: step 1779, loss 0.220562, acc 0.921875, prec 0.0480497, recall 0.841088
2017-12-10T12:47:22.852915: step 1780, loss 0.228531, acc 0.953125, prec 0.0480447, recall 0.841088
2017-12-10T12:47:23.289793: step 1781, loss 0.296537, acc 0.90625, prec 0.0480348, recall 0.841088
2017-12-10T12:47:23.745377: step 1782, loss 2.568, acc 0.859375, prec 0.0480544, recall 0.840676
2017-12-10T12:47:24.188582: step 1783, loss 0.594356, acc 0.875, prec 0.0480411, recall 0.840676
2017-12-10T12:47:24.639489: step 1784, loss 0.162742, acc 0.9375, prec 0.0480345, recall 0.840676
2017-12-10T12:47:25.070045: step 1785, loss 1.72566, acc 0.859375, prec 0.0480212, recall 0.840169
2017-12-10T12:47:25.529172: step 1786, loss 0.618319, acc 0.859375, prec 0.0481048, recall 0.840458
2017-12-10T12:47:25.963764: step 1787, loss 1.5739, acc 0.84375, prec 0.0482538, recall 0.840432
2017-12-10T12:47:26.351885: step 1788, loss 1.10208, acc 0.921875, prec 0.0482782, recall 0.840528
2017-12-10T12:47:26.722059: step 1789, loss 0.77501, acc 0.75, prec 0.0482844, recall 0.840623
2017-12-10T12:47:27.110054: step 1790, loss 0.251051, acc 0.875, prec 0.0483039, recall 0.840719
2017-12-10T12:47:27.501594: step 1791, loss 0.824333, acc 0.75, prec 0.0482773, recall 0.840719
2017-12-10T12:47:27.893741: step 1792, loss 0.831448, acc 0.71875, prec 0.0483128, recall 0.840909
2017-12-10T12:47:28.351058: step 1793, loss 1.12526, acc 0.734375, prec 0.04835, recall 0.841099
2017-12-10T12:47:28.795628: step 1794, loss 1.1847, acc 0.71875, prec 0.0483528, recall 0.841194
2017-12-10T12:47:29.245810: step 1795, loss 1.43035, acc 0.71875, prec 0.0483229, recall 0.841194
2017-12-10T12:47:29.695246: step 1796, loss 1.08553, acc 0.765625, prec 0.0483307, recall 0.841289
2017-12-10T12:47:30.133645: step 1797, loss 0.638343, acc 0.796875, prec 0.0483744, recall 0.841478
2017-12-10T12:47:30.574462: step 1798, loss 0.74691, acc 0.78125, prec 0.048449, recall 0.841761
2017-12-10T12:47:31.015055: step 1799, loss 0.477482, acc 0.828125, prec 0.0484307, recall 0.841761
2017-12-10T12:47:31.463340: step 1800, loss 0.963519, acc 0.75, prec 0.0484368, recall 0.841855
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1800

2017-12-10T12:47:33.299879: step 1801, loss 0.740788, acc 0.859375, prec 0.0484544, recall 0.841949
2017-12-10T12:47:33.737485: step 1802, loss 0.681698, acc 0.796875, prec 0.0484329, recall 0.841949
2017-12-10T12:47:34.185531: step 1803, loss 5.81165, acc 0.84375, prec 0.0484505, recall 0.841543
2017-12-10T12:47:34.624085: step 1804, loss 0.382201, acc 0.890625, prec 0.0484389, recall 0.841543
2017-12-10T12:47:35.066477: step 1805, loss 0.37618, acc 0.84375, prec 0.0484548, recall 0.841637
2017-12-10T12:47:35.514311: step 1806, loss 0.862482, acc 0.828125, prec 0.0484366, recall 0.841637
2017-12-10T12:47:35.954735: step 1807, loss 0.368169, acc 0.90625, prec 0.0484917, recall 0.841825
2017-12-10T12:47:36.394832: step 1808, loss 0.737735, acc 0.796875, prec 0.0485026, recall 0.841918
2017-12-10T12:47:36.827415: step 1809, loss 0.433369, acc 0.828125, prec 0.0485493, recall 0.842105
2017-12-10T12:47:37.267659: step 1810, loss 0.447449, acc 0.875, prec 0.0486009, recall 0.842292
2017-12-10T12:47:37.730990: step 1811, loss 0.432782, acc 0.875, prec 0.0485877, recall 0.842292
2017-12-10T12:47:38.183073: step 1812, loss 0.579689, acc 0.859375, prec 0.0485728, recall 0.842292
2017-12-10T12:47:38.632628: step 1813, loss 0.311751, acc 0.890625, prec 0.0485936, recall 0.842385
2017-12-10T12:47:39.079707: step 1814, loss 0.378298, acc 0.890625, prec 0.0486144, recall 0.842478
2017-12-10T12:47:39.530163: step 1815, loss 0.311927, acc 0.890625, prec 0.0486352, recall 0.842571
2017-12-10T12:47:39.986450: step 1816, loss 0.323528, acc 0.90625, prec 0.0486577, recall 0.842664
2017-12-10T12:47:40.433742: step 1817, loss 0.307955, acc 0.890625, prec 0.0486784, recall 0.842756
2017-12-10T12:47:40.883451: step 1818, loss 0.291786, acc 0.84375, prec 0.0486942, recall 0.842849
2017-12-10T12:47:41.332613: step 1819, loss 0.448478, acc 0.90625, prec 0.0486843, recall 0.842849
2017-12-10T12:47:41.781417: step 1820, loss 1.17005, acc 0.921875, prec 0.0487084, recall 0.842941
2017-12-10T12:47:42.207360: step 1821, loss 0.725112, acc 0.828125, prec 0.0487225, recall 0.843033
2017-12-10T12:47:42.637259: step 1822, loss 0.386717, acc 0.9375, prec 0.0487805, recall 0.843218
2017-12-10T12:47:43.074776: step 1823, loss 0.167912, acc 0.9375, prec 0.0487739, recall 0.843218
2017-12-10T12:47:43.530556: step 1824, loss 0.547265, acc 0.890625, prec 0.0487623, recall 0.843218
2017-12-10T12:47:43.997955: step 1825, loss 0.305679, acc 0.953125, prec 0.0488542, recall 0.843494
2017-12-10T12:47:44.444219: step 1826, loss 0.707204, acc 0.953125, prec 0.0488815, recall 0.843585
2017-12-10T12:47:44.892889: step 1827, loss 0.248597, acc 0.9375, prec 0.0489071, recall 0.843677
2017-12-10T12:47:45.330276: step 1828, loss 0.341575, acc 0.90625, prec 0.0488972, recall 0.843677
2017-12-10T12:47:45.772506: step 1829, loss 0.235035, acc 0.890625, prec 0.0488856, recall 0.843677
2017-12-10T12:47:46.231341: step 1830, loss 1.58628, acc 0.9375, prec 0.0489129, recall 0.843275
2017-12-10T12:47:46.673552: step 1831, loss 0.422045, acc 0.875, prec 0.0490286, recall 0.843641
2017-12-10T12:47:47.131576: step 1832, loss 0.21258, acc 0.9375, prec 0.0490542, recall 0.843732
2017-12-10T12:47:47.565596: step 1833, loss 0.500808, acc 0.921875, prec 0.0491425, recall 0.844005
2017-12-10T12:47:48.015346: step 1834, loss 4.53755, acc 0.875, prec 0.0491326, recall 0.843023
2017-12-10T12:47:48.456448: step 1835, loss 0.79634, acc 0.84375, prec 0.0491159, recall 0.843023
2017-12-10T12:47:48.888910: step 1836, loss 0.240801, acc 0.9375, prec 0.0491093, recall 0.843023
2017-12-10T12:47:49.355345: step 1837, loss 0.371019, acc 0.875, prec 0.049096, recall 0.843023
2017-12-10T12:47:49.803176: step 1838, loss 0.416993, acc 0.890625, prec 0.0491809, recall 0.843297
2017-12-10T12:47:50.252494: step 1839, loss 0.370618, acc 0.90625, prec 0.0492031, recall 0.843387
2017-12-10T12:47:50.693568: step 1840, loss 0.358336, acc 0.90625, prec 0.0491931, recall 0.843387
2017-12-10T12:47:51.139603: step 1841, loss 0.558589, acc 0.875, prec 0.0492441, recall 0.843569
2017-12-10T12:47:51.585292: step 1842, loss 0.624376, acc 0.8125, prec 0.0492241, recall 0.843569
2017-12-10T12:47:52.030856: step 1843, loss 0.419254, acc 0.828125, prec 0.0493022, recall 0.84384
2017-12-10T12:47:52.475427: step 1844, loss 0.774657, acc 0.828125, prec 0.0493481, recall 0.844021
2017-12-10T12:47:52.921115: step 1845, loss 0.82043, acc 0.84375, prec 0.0493956, recall 0.844201
2017-12-10T12:47:53.363508: step 1846, loss 0.799566, acc 0.765625, prec 0.0493706, recall 0.844201
2017-12-10T12:47:53.826295: step 1847, loss 0.589731, acc 0.859375, prec 0.0493877, recall 0.844291
2017-12-10T12:47:54.302575: step 1848, loss 1.14153, acc 0.75, prec 0.0493611, recall 0.844291
2017-12-10T12:47:54.741798: step 1849, loss 0.268032, acc 0.890625, prec 0.0494135, recall 0.84447
2017-12-10T12:47:55.191254: step 1850, loss 0.56465, acc 0.796875, prec 0.0494559, recall 0.844649
2017-12-10T12:47:55.658092: step 1851, loss 0.319819, acc 0.828125, prec 0.0494376, recall 0.844649
2017-12-10T12:47:56.098996: step 1852, loss 0.451508, acc 0.890625, prec 0.0494259, recall 0.844649
2017-12-10T12:47:56.537590: step 1853, loss 0.448082, acc 0.9375, prec 0.0494513, recall 0.844738
2017-12-10T12:47:56.995288: step 1854, loss 0.445447, acc 0.890625, prec 0.0494396, recall 0.844738
2017-12-10T12:47:57.445166: step 1855, loss 0.598098, acc 0.828125, prec 0.0494533, recall 0.844828
2017-12-10T12:47:57.894368: step 1856, loss 0.412867, acc 0.890625, prec 0.0494736, recall 0.844917
2017-12-10T12:47:58.348096: step 1857, loss 0.393437, acc 0.890625, prec 0.049494, recall 0.845006
2017-12-10T12:47:58.795398: step 1858, loss 0.394592, acc 0.890625, prec 0.0495143, recall 0.845095
2017-12-10T12:47:59.247625: step 1859, loss 0.705274, acc 0.84375, prec 0.0494976, recall 0.845095
2017-12-10T12:47:59.692208: step 1860, loss 0.271277, acc 0.9375, prec 0.0495548, recall 0.845272
2017-12-10T12:48:00.136712: step 1861, loss 0.479897, acc 0.96875, prec 0.0496154, recall 0.845449
2017-12-10T12:48:00.588964: step 1862, loss 2.83448, acc 0.890625, prec 0.0496054, recall 0.844966
2017-12-10T12:48:01.024350: step 1863, loss 0.268516, acc 0.921875, prec 0.049597, recall 0.844966
2017-12-10T12:48:01.469318: step 1864, loss 0.335432, acc 0.9375, prec 0.0496861, recall 0.845231
2017-12-10T12:48:01.921147: step 1865, loss 0.452234, acc 0.90625, prec 0.049708, recall 0.84532
2017-12-10T12:48:02.369705: step 1866, loss 0.257409, acc 0.875, prec 0.0497265, recall 0.845408
2017-12-10T12:48:02.813760: step 1867, loss 0.167195, acc 0.921875, prec 0.0497182, recall 0.845408
2017-12-10T12:48:03.275910: step 1868, loss 0.773521, acc 0.765625, prec 0.0497251, recall 0.845496
2017-12-10T12:48:03.701330: step 1869, loss 0.241782, acc 0.890625, prec 0.0497452, recall 0.845584
2017-12-10T12:48:04.158376: step 1870, loss 0.255276, acc 0.90625, prec 0.0498308, recall 0.845848
2017-12-10T12:48:04.621568: step 1871, loss 0.178004, acc 0.921875, prec 0.0498224, recall 0.845848
2017-12-10T12:48:05.062342: step 1872, loss 0.185209, acc 0.90625, prec 0.0498124, recall 0.845848
2017-12-10T12:48:05.509119: step 1873, loss 0.515763, acc 0.890625, prec 0.0498326, recall 0.845935
2017-12-10T12:48:05.946638: step 1874, loss 0.72501, acc 0.8125, prec 0.0498761, recall 0.84611
2017-12-10T12:48:06.398604: step 1875, loss 0.157199, acc 0.96875, prec 0.0499364, recall 0.846285
2017-12-10T12:48:06.851500: step 1876, loss 0.501613, acc 0.96875, prec 0.0499967, recall 0.846459
2017-12-10T12:48:07.297377: step 1877, loss 0.279935, acc 0.890625, prec 0.0499849, recall 0.846459
2017-12-10T12:48:07.746095: step 1878, loss 0.35343, acc 0.859375, prec 0.0500334, recall 0.846633
2017-12-10T12:48:08.188434: step 1879, loss 1.12079, acc 0.921875, prec 0.0500886, recall 0.846806
2017-12-10T12:48:08.646576: step 1880, loss 3.58773, acc 0.8125, prec 0.0501337, recall 0.846501
2017-12-10T12:48:09.086576: step 1881, loss 0.287888, acc 0.921875, prec 0.0501253, recall 0.846501
2017-12-10T12:48:09.540044: step 1882, loss 0.661046, acc 0.90625, prec 0.050147, recall 0.846588
2017-12-10T12:48:09.985381: step 1883, loss 0.219636, acc 0.890625, prec 0.0501353, recall 0.846588
2017-12-10T12:48:10.438117: step 1884, loss 0.74353, acc 0.828125, prec 0.0501803, recall 0.846761
2017-12-10T12:48:10.889160: step 1885, loss 0.460567, acc 0.8125, prec 0.0501919, recall 0.846847
2017-12-10T12:48:11.320687: step 1886, loss 0.455082, acc 0.84375, prec 0.0501751, recall 0.846847
2017-12-10T12:48:11.774761: step 1887, loss 0.358416, acc 0.921875, prec 0.0501985, recall 0.846933
2017-12-10T12:48:12.222970: step 1888, loss 0.284333, acc 0.890625, prec 0.0501867, recall 0.846933
2017-12-10T12:48:12.664686: step 1889, loss 0.191046, acc 0.953125, prec 0.0502134, recall 0.847019
2017-12-10T12:48:13.104891: step 1890, loss 0.568888, acc 0.828125, prec 0.0502266, recall 0.847105
2017-12-10T12:48:13.542435: step 1891, loss 0.35913, acc 0.90625, prec 0.0502482, recall 0.847191
2017-12-10T12:48:13.980011: step 1892, loss 0.808218, acc 0.828125, prec 0.0502931, recall 0.847363
2017-12-10T12:48:14.428739: step 1893, loss 0.720414, acc 0.8125, prec 0.0503046, recall 0.847448
2017-12-10T12:48:14.872108: step 1894, loss 0.512831, acc 0.84375, prec 0.0503827, recall 0.847704
2017-12-10T12:48:15.320750: step 1895, loss 0.700689, acc 0.828125, prec 0.0503643, recall 0.847704
2017-12-10T12:48:15.767234: step 1896, loss 0.569723, acc 0.828125, prec 0.050409, recall 0.847875
2017-12-10T12:48:16.238894: step 1897, loss 0.688427, acc 0.84375, prec 0.0503922, recall 0.847875
2017-12-10T12:48:16.672847: step 1898, loss 0.447512, acc 0.953125, prec 0.0504503, recall 0.848045
2017-12-10T12:48:17.121469: step 1899, loss 1.33495, acc 0.859375, prec 0.0504983, recall 0.848214
2017-12-10T12:48:17.558430: step 1900, loss 0.957695, acc 0.8125, prec 0.0505097, recall 0.848299
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-1900

2017-12-10T12:48:19.486492: step 1901, loss 0.164718, acc 0.921875, prec 0.0505014, recall 0.848299
2017-12-10T12:48:19.930911: step 1902, loss 2.45948, acc 0.875, prec 0.0504896, recall 0.847826
2017-12-10T12:48:20.381387: step 1903, loss 0.443047, acc 0.859375, prec 0.0505376, recall 0.847996
2017-12-10T12:48:20.830924: step 1904, loss 0.280118, acc 0.90625, prec 0.050559, recall 0.84808
2017-12-10T12:48:21.264203: step 1905, loss 0.314623, acc 0.90625, prec 0.0506749, recall 0.848418
2017-12-10T12:48:21.697558: step 1906, loss 0.676119, acc 0.84375, prec 0.0506581, recall 0.848418
2017-12-10T12:48:22.160354: step 1907, loss 1.51243, acc 0.90625, prec 0.0506795, recall 0.848502
2017-12-10T12:48:22.599700: step 1908, loss 0.290969, acc 0.90625, prec 0.0506694, recall 0.848502
2017-12-10T12:48:23.051485: step 1909, loss 0.695852, acc 0.828125, prec 0.0506509, recall 0.848502
2017-12-10T12:48:23.503670: step 1910, loss 0.824372, acc 0.734375, prec 0.0506224, recall 0.848502
2017-12-10T12:48:23.964423: step 1911, loss 0.628917, acc 0.890625, prec 0.0506107, recall 0.848502
2017-12-10T12:48:24.407878: step 1912, loss 0.706159, acc 0.859375, prec 0.0505956, recall 0.848502
2017-12-10T12:48:24.854045: step 1913, loss 0.77075, acc 0.84375, prec 0.0506103, recall 0.848586
2017-12-10T12:48:25.287984: step 1914, loss 0.688054, acc 0.890625, prec 0.05063, recall 0.84867
2017-12-10T12:48:25.736399: step 1915, loss 0.294326, acc 0.890625, prec 0.0506183, recall 0.84867
2017-12-10T12:48:26.174989: step 1916, loss 0.670145, acc 0.8125, prec 0.0506296, recall 0.848753
2017-12-10T12:48:26.632909: step 1917, loss 0.427492, acc 0.875, prec 0.0506162, recall 0.848753
2017-12-10T12:48:27.076664: step 1918, loss 0.424147, acc 0.9375, prec 0.0506722, recall 0.848921
2017-12-10T12:48:27.536751: step 1919, loss 0.42944, acc 0.859375, prec 0.0506885, recall 0.849004
2017-12-10T12:48:27.990564: step 1920, loss 0.207295, acc 0.921875, prec 0.0507428, recall 0.849171
2017-12-10T12:48:28.442343: step 1921, loss 0.354744, acc 0.859375, prec 0.0507277, recall 0.849171
2017-12-10T12:48:28.882464: step 1922, loss 0.776053, acc 0.9375, prec 0.0507524, recall 0.849255
2017-12-10T12:48:29.328854: step 1923, loss 0.22733, acc 0.9375, prec 0.0508396, recall 0.849504
2017-12-10T12:48:29.767187: step 1924, loss 0.18869, acc 0.96875, prec 0.0508363, recall 0.849504
2017-12-10T12:48:30.219510: step 1925, loss 0.327735, acc 0.953125, prec 0.0509565, recall 0.849835
2017-12-10T12:48:30.668797: step 1926, loss 0.22806, acc 0.890625, prec 0.0509447, recall 0.849835
2017-12-10T12:48:31.104051: step 1927, loss 0.660762, acc 0.875, prec 0.0509938, recall 0.85
2017-12-10T12:48:31.543777: step 1928, loss 0.465367, acc 0.921875, prec 0.0509854, recall 0.85
2017-12-10T12:48:31.989376: step 1929, loss 0.188941, acc 0.9375, prec 0.0509787, recall 0.85
2017-12-10T12:48:32.425962: step 1930, loss 0.175537, acc 0.953125, prec 0.0510049, recall 0.850082
2017-12-10T12:48:32.855804: step 1931, loss 0.213178, acc 0.890625, prec 0.0509932, recall 0.850082
2017-12-10T12:48:33.308478: step 1932, loss 0.354111, acc 0.890625, prec 0.0510439, recall 0.850247
2017-12-10T12:48:33.750744: step 1933, loss 0.301065, acc 0.90625, prec 0.0510651, recall 0.850329
2017-12-10T12:48:34.206923: step 1934, loss 0.558536, acc 0.953125, prec 0.0511538, recall 0.850575
2017-12-10T12:48:34.653997: step 1935, loss 8.09458, acc 0.9375, prec 0.0511799, recall 0.850191
2017-12-10T12:48:35.117847: step 1936, loss 0.321142, acc 0.90625, prec 0.0511698, recall 0.850191
2017-12-10T12:48:35.573856: step 1937, loss 0.411866, acc 0.90625, prec 0.0511909, recall 0.850273
2017-12-10T12:48:36.031488: step 1938, loss 2.31464, acc 0.859375, prec 0.0512087, recall 0.849891
2017-12-10T12:48:36.481768: step 1939, loss 0.957137, acc 0.765625, prec 0.0512458, recall 0.850055
2017-12-10T12:48:36.934061: step 1940, loss 0.567529, acc 0.84375, prec 0.051229, recall 0.850055
2017-12-10T12:48:37.380699: step 1941, loss 1.8737, acc 0.921875, prec 0.0512534, recall 0.849673
2017-12-10T12:48:37.827910: step 1942, loss 0.908869, acc 0.8125, prec 0.0512644, recall 0.849755
2017-12-10T12:48:38.270586: step 1943, loss 0.855683, acc 0.84375, prec 0.0512475, recall 0.849755
2017-12-10T12:48:38.715910: step 1944, loss 1.06805, acc 0.75, prec 0.051314, recall 0.85
2017-12-10T12:48:39.157773: step 1945, loss 0.721949, acc 0.796875, prec 0.0513544, recall 0.850163
2017-12-10T12:48:39.605358: step 1946, loss 0.810059, acc 0.703125, prec 0.0513224, recall 0.850163
2017-12-10T12:48:40.049057: step 1947, loss 1.1756, acc 0.703125, prec 0.0513215, recall 0.850244
2017-12-10T12:48:40.493700: step 1948, loss 1.3992, acc 0.671875, prec 0.0513173, recall 0.850325
2017-12-10T12:48:40.930315: step 1949, loss 0.678652, acc 0.78125, prec 0.0513869, recall 0.850568
2017-12-10T12:48:41.359509: step 1950, loss 0.957001, acc 0.671875, prec 0.0513516, recall 0.850568
2017-12-10T12:48:41.794676: step 1951, loss 0.807147, acc 0.828125, prec 0.0513642, recall 0.850649
2017-12-10T12:48:42.234305: step 1952, loss 0.576168, acc 0.84375, prec 0.0513474, recall 0.850649
2017-12-10T12:48:42.682711: step 1953, loss 0.605679, acc 0.84375, prec 0.0513616, recall 0.85073
2017-12-10T12:48:43.144621: step 1954, loss 0.846694, acc 0.71875, prec 0.0513314, recall 0.85073
2017-12-10T12:48:43.586007: step 1955, loss 0.707965, acc 0.828125, prec 0.0514058, recall 0.850972
2017-12-10T12:48:44.032456: step 1956, loss 0.532321, acc 0.84375, prec 0.0513891, recall 0.850972
2017-12-10T12:48:44.464333: step 1957, loss 0.758744, acc 0.859375, prec 0.0514049, recall 0.851052
2017-12-10T12:48:44.905508: step 1958, loss 0.548157, acc 0.875, prec 0.0514224, recall 0.851133
2017-12-10T12:48:45.351496: step 1959, loss 0.845411, acc 0.859375, prec 0.0514692, recall 0.851293
2017-12-10T12:48:45.795875: step 1960, loss 0.808015, acc 0.84375, prec 0.0514524, recall 0.851293
2017-12-10T12:48:46.233227: step 1961, loss 0.212721, acc 0.890625, prec 0.0514407, recall 0.851293
2017-12-10T12:48:46.676905: step 1962, loss 0.724726, acc 0.796875, prec 0.0514189, recall 0.851293
2017-12-10T12:48:47.126818: step 1963, loss 0.416068, acc 0.890625, prec 0.0514072, recall 0.851293
2017-12-10T12:48:47.568727: step 1964, loss 0.188538, acc 0.96875, prec 0.0514038, recall 0.851293
2017-12-10T12:48:48.011131: step 1965, loss 0.586317, acc 0.875, prec 0.0513905, recall 0.851293
2017-12-10T12:48:48.458896: step 1966, loss 0.540733, acc 0.859375, prec 0.0513754, recall 0.851293
2017-12-10T12:48:48.901785: step 1967, loss 0.304123, acc 0.921875, prec 0.0513979, recall 0.851373
2017-12-10T12:48:49.340144: step 1968, loss 0.377189, acc 0.96875, prec 0.0514254, recall 0.851453
2017-12-10T12:48:49.798309: step 1969, loss 0.366168, acc 0.90625, prec 0.051477, recall 0.851613
2017-12-10T12:48:50.243129: step 1970, loss 0.15116, acc 0.9375, prec 0.0515012, recall 0.851693
2017-12-10T12:48:50.693937: step 1971, loss 0.0939475, acc 0.9375, prec 0.0514945, recall 0.851693
2017-12-10T12:48:51.135930: step 1972, loss 0.151383, acc 0.953125, prec 0.0514895, recall 0.851693
2017-12-10T12:48:51.575424: step 1973, loss 0.195201, acc 0.921875, prec 0.0515119, recall 0.851772
2017-12-10T12:48:52.014071: step 1974, loss 2.83947, acc 0.921875, prec 0.051536, recall 0.851395
2017-12-10T12:48:52.465074: step 1975, loss 9.23458, acc 0.953125, prec 0.0515651, recall 0.850562
2017-12-10T12:48:52.913922: step 1976, loss 0.83599, acc 0.9375, prec 0.0515892, recall 0.850642
2017-12-10T12:48:53.363252: step 1977, loss 0.136219, acc 0.953125, prec 0.0516458, recall 0.850802
2017-12-10T12:48:53.814572: step 1978, loss 1.46258, acc 0.921875, prec 0.0516699, recall 0.850427
2017-12-10T12:48:54.268648: step 1979, loss 0.364517, acc 0.84375, prec 0.0516531, recall 0.850427
2017-12-10T12:48:54.700597: step 1980, loss 0.778907, acc 0.78125, prec 0.0516296, recall 0.850427
2017-12-10T12:48:55.143123: step 1981, loss 0.695593, acc 0.8125, prec 0.0517018, recall 0.850667
2017-12-10T12:48:55.587668: step 1982, loss 0.53419, acc 0.875, prec 0.0517191, recall 0.850746
2017-12-10T12:48:56.024550: step 1983, loss 0.565363, acc 0.828125, prec 0.0517007, recall 0.850746
2017-12-10T12:48:56.489989: step 1984, loss 1.3344, acc 0.671875, prec 0.0516962, recall 0.850826
2017-12-10T12:48:56.921759: step 1985, loss 1.02161, acc 0.75, prec 0.0516695, recall 0.850826
2017-12-10T12:48:57.351660: step 1986, loss 0.590782, acc 0.875, prec 0.0517174, recall 0.850985
2017-12-10T12:48:57.789385: step 1987, loss 0.932516, acc 0.765625, prec 0.0516924, recall 0.850985
2017-12-10T12:48:58.185285: step 1988, loss 0.736902, acc 0.865385, prec 0.0517113, recall 0.851064
2017-12-10T12:48:58.641566: step 1989, loss 0.950893, acc 0.765625, prec 0.0517169, recall 0.851143
2017-12-10T12:48:59.085175: step 1990, loss 0.85394, acc 0.734375, prec 0.0516885, recall 0.851143
2017-12-10T12:48:59.527087: step 1991, loss 1.14768, acc 0.71875, prec 0.0516585, recall 0.851143
2017-12-10T12:48:59.975143: step 1992, loss 0.899687, acc 0.796875, prec 0.0516368, recall 0.851143
2017-12-10T12:49:00.420660: step 1993, loss 0.785833, acc 0.8125, prec 0.0516169, recall 0.851143
2017-12-10T12:49:00.858405: step 1994, loss 0.670852, acc 0.828125, prec 0.0516597, recall 0.851301
2017-12-10T12:49:01.300513: step 1995, loss 0.472138, acc 0.875, prec 0.0516769, recall 0.85138
2017-12-10T12:49:01.760335: step 1996, loss 0.47561, acc 0.84375, prec 0.0516908, recall 0.851459
2017-12-10T12:49:02.203044: step 1997, loss 0.576564, acc 0.875, prec 0.0516775, recall 0.851459
2017-12-10T12:49:02.631624: step 1998, loss 0.751295, acc 0.75, prec 0.0516509, recall 0.851459
2017-12-10T12:49:03.068794: step 1999, loss 0.580779, acc 0.859375, prec 0.0516359, recall 0.851459
2017-12-10T12:49:03.506463: step 2000, loss 0.354747, acc 0.90625, prec 0.051626, recall 0.851459
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2000

2017-12-10T12:49:05.663765: step 2001, loss 0.229248, acc 0.953125, prec 0.0516515, recall 0.851538
2017-12-10T12:49:06.134046: step 2002, loss 0.149143, acc 0.953125, prec 0.0517075, recall 0.851695
2017-12-10T12:49:06.584821: step 2003, loss 0.205642, acc 0.90625, prec 0.051728, recall 0.851773
2017-12-10T12:49:07.041415: step 2004, loss 0.147014, acc 0.953125, prec 0.051723, recall 0.851773
2017-12-10T12:49:07.501392: step 2005, loss 0.237847, acc 0.90625, prec 0.0517131, recall 0.851773
2017-12-10T12:49:07.946093: step 2006, loss 0.308631, acc 0.921875, prec 0.0517047, recall 0.851773
2017-12-10T12:49:08.388864: step 2007, loss 1.44781, acc 0.953125, prec 0.0517014, recall 0.851323
2017-12-10T12:49:08.849168: step 2008, loss 0.305059, acc 0.96875, prec 0.0516981, recall 0.851323
2017-12-10T12:49:09.279197: step 2009, loss 0.06745, acc 0.984375, prec 0.0517269, recall 0.851401
2017-12-10T12:49:09.719428: step 2010, loss 4.88896, acc 0.921875, prec 0.0517203, recall 0.850951
2017-12-10T12:49:10.168433: step 2011, loss 0.0490904, acc 0.984375, prec 0.05181, recall 0.851187
2017-12-10T12:49:10.619968: step 2012, loss 0.597032, acc 0.921875, prec 0.0518626, recall 0.851344
2017-12-10T12:49:11.065570: step 2013, loss 0.372621, acc 0.890625, prec 0.0519422, recall 0.851579
2017-12-10T12:49:11.516004: step 2014, loss 0.195643, acc 0.90625, prec 0.0519931, recall 0.851735
2017-12-10T12:49:11.951468: step 2015, loss 0.213128, acc 0.90625, prec 0.0519831, recall 0.851735
2017-12-10T12:49:12.388423: step 2016, loss 0.179001, acc 0.921875, prec 0.0520051, recall 0.851813
2017-12-10T12:49:12.834914: step 2017, loss 0.233629, acc 0.90625, prec 0.0520255, recall 0.851891
2017-12-10T12:49:13.277599: step 2018, loss 0.284472, acc 0.9375, prec 0.0520189, recall 0.851891
2017-12-10T12:49:13.722598: step 2019, loss 0.0618749, acc 0.984375, prec 0.0520476, recall 0.851969
2017-12-10T12:49:14.171304: step 2020, loss 0.446764, acc 0.90625, prec 0.052068, recall 0.852046
2017-12-10T12:49:14.619935: step 2021, loss 0.296849, acc 0.921875, prec 0.0521204, recall 0.852201
2017-12-10T12:49:15.065906: step 2022, loss 0.456865, acc 0.921875, prec 0.0521424, recall 0.852279
2017-12-10T12:49:15.515933: step 2023, loss 0.22998, acc 0.90625, prec 0.0521324, recall 0.852279
2017-12-10T12:49:15.971260: step 2024, loss 0.725769, acc 0.828125, prec 0.0522051, recall 0.85251
2017-12-10T12:49:16.411085: step 2025, loss 0.383349, acc 0.875, prec 0.0521917, recall 0.85251
2017-12-10T12:49:16.855741: step 2026, loss 0.534048, acc 0.859375, prec 0.0521767, recall 0.85251
2017-12-10T12:49:17.297041: step 2027, loss 0.38994, acc 0.921875, prec 0.0521987, recall 0.852588
2017-12-10T12:49:17.758610: step 2028, loss 1.15829, acc 0.875, prec 0.0522156, recall 0.852665
2017-12-10T12:49:18.194181: step 2029, loss 0.197097, acc 0.953125, prec 0.0522106, recall 0.852665
2017-12-10T12:49:18.646916: step 2030, loss 7.46804, acc 0.859375, prec 0.0521973, recall 0.852219
2017-12-10T12:49:19.079166: step 2031, loss 0.688085, acc 0.890625, prec 0.0522159, recall 0.852296
2017-12-10T12:49:19.535483: step 2032, loss 0.534903, acc 0.875, prec 0.0522025, recall 0.852296
2017-12-10T12:49:19.981243: step 2033, loss 0.486963, acc 0.921875, prec 0.0522245, recall 0.852373
2017-12-10T12:49:20.428755: step 2034, loss 0.366637, acc 0.859375, prec 0.05227, recall 0.852527
2017-12-10T12:49:20.876328: step 2035, loss 0.600892, acc 0.84375, prec 0.0522836, recall 0.852604
2017-12-10T12:49:21.319926: step 2036, loss 0.13674, acc 0.953125, prec 0.0522786, recall 0.852604
2017-12-10T12:49:21.762496: step 2037, loss 0.912042, acc 0.796875, prec 0.0522569, recall 0.852604
2017-12-10T12:49:22.217441: step 2038, loss 0.692556, acc 0.84375, prec 0.0522705, recall 0.852681
2017-12-10T12:49:22.654180: step 2039, loss 0.504079, acc 0.890625, prec 0.0522588, recall 0.852681
2017-12-10T12:49:23.089478: step 2040, loss 0.519256, acc 0.84375, prec 0.0522724, recall 0.852758
2017-12-10T12:49:23.530718: step 2041, loss 0.441841, acc 0.84375, prec 0.0522859, recall 0.852834
2017-12-10T12:49:23.971979: step 2042, loss 0.434675, acc 0.859375, prec 0.0523615, recall 0.853063
2017-12-10T12:49:24.424779: step 2043, loss 0.428214, acc 0.890625, prec 0.05238, recall 0.85314
2017-12-10T12:49:24.868053: step 2044, loss 0.320828, acc 0.859375, prec 0.0524254, recall 0.853292
2017-12-10T12:49:25.323764: step 2045, loss 0.479515, acc 0.90625, prec 0.0524757, recall 0.853444
2017-12-10T12:49:25.769341: step 2046, loss 0.474783, acc 0.859375, prec 0.0524607, recall 0.853444
2017-12-10T12:49:26.217694: step 2047, loss 0.132289, acc 0.9375, prec 0.0525445, recall 0.853671
2017-12-10T12:49:26.660590: step 2048, loss 0.154691, acc 0.953125, prec 0.0525395, recall 0.853671
2017-12-10T12:49:27.104889: step 2049, loss 0.536907, acc 0.875, prec 0.0525261, recall 0.853671
2017-12-10T12:49:27.552956: step 2050, loss 0.404838, acc 0.953125, prec 0.0525512, recall 0.853747
2017-12-10T12:49:28.001039: step 2051, loss 0.210654, acc 0.9375, prec 0.0525747, recall 0.853822
2017-12-10T12:49:28.436073: step 2052, loss 0.188744, acc 0.953125, prec 0.0526299, recall 0.853973
2017-12-10T12:49:28.879610: step 2053, loss 0.251854, acc 0.90625, prec 0.05265, recall 0.854048
2017-12-10T12:49:29.326227: step 2054, loss 0.164373, acc 0.9375, prec 0.0526734, recall 0.854124
2017-12-10T12:49:29.772446: step 2055, loss 0.104777, acc 0.953125, prec 0.0526985, recall 0.854199
2017-12-10T12:49:30.215976: step 2056, loss 0.166806, acc 0.9375, prec 0.0527219, recall 0.854274
2017-12-10T12:49:30.663106: step 2057, loss 0.0736569, acc 0.953125, prec 0.052747, recall 0.854349
2017-12-10T12:49:31.112065: step 2058, loss 0.0646193, acc 0.984375, prec 0.0527453, recall 0.854349
2017-12-10T12:49:31.561546: step 2059, loss 0.322386, acc 0.921875, prec 0.0527971, recall 0.854499
2017-12-10T12:49:32.017505: step 2060, loss 0.370201, acc 0.90625, prec 0.052787, recall 0.854499
2017-12-10T12:49:32.457857: step 2061, loss 0.0724839, acc 0.96875, prec 0.0528138, recall 0.854573
2017-12-10T12:49:32.903936: step 2062, loss 0.156387, acc 0.9375, prec 0.0528071, recall 0.854573
2017-12-10T12:49:33.358127: step 2063, loss 0.220053, acc 0.953125, prec 0.0528321, recall 0.854648
2017-12-10T12:49:33.832225: step 2064, loss 0.0463504, acc 0.984375, prec 0.0528605, recall 0.854723
2017-12-10T12:49:34.290260: step 2065, loss 0.0596354, acc 0.984375, prec 0.0528588, recall 0.854723
2017-12-10T12:49:34.744413: step 2066, loss 0.0632393, acc 0.984375, prec 0.0528872, recall 0.854797
2017-12-10T12:49:35.179343: step 2067, loss 0.0628746, acc 0.96875, prec 0.0528839, recall 0.854797
2017-12-10T12:49:35.629613: step 2068, loss 0.144037, acc 0.96875, prec 0.0528805, recall 0.854797
2017-12-10T12:49:36.067318: step 2069, loss 0.735234, acc 0.96875, prec 0.0529373, recall 0.854946
2017-12-10T12:49:36.505738: step 2070, loss 0.0668085, acc 0.984375, prec 0.0529957, recall 0.855095
2017-12-10T12:49:36.946513: step 2071, loss 1.36151, acc 0.984375, prec 0.0530541, recall 0.855243
2017-12-10T12:49:37.396028: step 2072, loss 0.138124, acc 0.953125, prec 0.0531091, recall 0.855391
2017-12-10T12:49:37.843675: step 2073, loss 0.285819, acc 0.96875, prec 0.0531358, recall 0.855465
2017-12-10T12:49:38.291440: step 2074, loss 0.0592597, acc 0.953125, prec 0.0531307, recall 0.855465
2017-12-10T12:49:38.744471: step 2075, loss 1.72875, acc 0.9375, prec 0.0531557, recall 0.855102
2017-12-10T12:49:39.189794: step 2076, loss 0.114016, acc 0.953125, prec 0.0531507, recall 0.855102
2017-12-10T12:49:39.627152: step 2077, loss 0.774033, acc 0.984375, prec 0.053209, recall 0.85525
2017-12-10T12:49:40.072401: step 2078, loss 0.217075, acc 0.9375, prec 0.0532623, recall 0.855397
2017-12-10T12:49:40.524815: step 2079, loss 0.253569, acc 0.9375, prec 0.0532556, recall 0.855397
2017-12-10T12:49:40.966510: step 2080, loss 0.273455, acc 0.953125, prec 0.0532505, recall 0.855397
2017-12-10T12:49:41.407150: step 2081, loss 0.167333, acc 0.9375, prec 0.0532437, recall 0.855397
2017-12-10T12:49:41.863477: step 2082, loss 0.116281, acc 0.9375, prec 0.053267, recall 0.855471
2017-12-10T12:49:42.309849: step 2083, loss 2.59838, acc 0.890625, prec 0.0533169, recall 0.855183
2017-12-10T12:49:42.779071: step 2084, loss 0.206234, acc 0.9375, prec 0.0533101, recall 0.855183
2017-12-10T12:49:43.210684: step 2085, loss 0.292636, acc 0.921875, prec 0.0533017, recall 0.855183
2017-12-10T12:49:43.659363: step 2086, loss 0.429448, acc 0.875, prec 0.0533481, recall 0.85533
2017-12-10T12:49:44.106428: step 2087, loss 0.293015, acc 0.890625, prec 0.0533363, recall 0.85533
2017-12-10T12:49:44.535723: step 2088, loss 0.908812, acc 0.8125, prec 0.053316, recall 0.85533
2017-12-10T12:49:44.976616: step 2089, loss 0.337307, acc 0.84375, prec 0.053389, recall 0.85555
2017-12-10T12:49:45.424837: step 2090, loss 1.11592, acc 0.765625, prec 0.0533936, recall 0.855623
2017-12-10T12:49:45.881854: step 2091, loss 1.318, acc 0.78125, prec 0.0534298, recall 0.855769
2017-12-10T12:49:46.335084: step 2092, loss 0.598816, acc 0.8125, prec 0.0534395, recall 0.855842
2017-12-10T12:49:46.772459: step 2093, loss 0.643063, acc 0.78125, prec 0.0534158, recall 0.855842
2017-12-10T12:49:47.225298: step 2094, loss 0.808063, acc 0.8125, prec 0.0534255, recall 0.855915
2017-12-10T12:49:47.666505: step 2095, loss 0.502309, acc 0.828125, prec 0.0534368, recall 0.855988
2017-12-10T12:49:48.130493: step 2096, loss 0.553934, acc 0.828125, prec 0.0534481, recall 0.856061
2017-12-10T12:49:48.598375: step 2097, loss 0.581263, acc 0.859375, prec 0.0534628, recall 0.856133
2017-12-10T12:49:49.047424: step 2098, loss 0.209417, acc 0.9375, prec 0.0534859, recall 0.856206
2017-12-10T12:49:49.504863: step 2099, loss 0.384889, acc 0.90625, prec 0.0535056, recall 0.856278
2017-12-10T12:49:49.951086: step 2100, loss 0.437589, acc 0.875, prec 0.0534921, recall 0.856278
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2100

2017-12-10T12:49:52.104067: step 2101, loss 0.127197, acc 0.9375, prec 0.0535152, recall 0.856351
2017-12-10T12:49:52.554643: step 2102, loss 0.418321, acc 0.890625, prec 0.0535332, recall 0.856423
2017-12-10T12:49:53.001518: step 2103, loss 0.305339, acc 0.890625, prec 0.0535512, recall 0.856495
2017-12-10T12:49:53.442629: step 2104, loss 0.241955, acc 0.921875, prec 0.0535726, recall 0.856568
2017-12-10T12:49:53.884644: step 2105, loss 0.431039, acc 0.859375, prec 0.0535872, recall 0.85664
2017-12-10T12:49:54.329231: step 2106, loss 0.211696, acc 0.90625, prec 0.0536366, recall 0.856784
2017-12-10T12:49:54.784401: step 2107, loss 0.294656, acc 0.875, prec 0.0536231, recall 0.856784
2017-12-10T12:49:55.241682: step 2108, loss 0.240076, acc 0.9375, prec 0.0536164, recall 0.856784
2017-12-10T12:49:55.697315: step 2109, loss 0.0473715, acc 0.984375, prec 0.0536147, recall 0.856784
2017-12-10T12:49:56.158630: step 2110, loss 0.461112, acc 0.953125, prec 0.0536096, recall 0.856784
2017-12-10T12:49:56.602507: step 2111, loss 0.189608, acc 0.9375, prec 0.0536326, recall 0.856856
2017-12-10T12:49:57.046405: step 2112, loss 0.435925, acc 0.953125, prec 0.0536573, recall 0.856928
2017-12-10T12:49:57.486972: step 2113, loss 0.13111, acc 0.9375, prec 0.0536506, recall 0.856928
2017-12-10T12:49:57.923996: step 2114, loss 0.0636901, acc 0.953125, prec 0.0536752, recall 0.857
2017-12-10T12:49:58.360605: step 2115, loss 5.52421, acc 0.9375, prec 0.0536702, recall 0.85657
2017-12-10T12:49:58.802618: step 2116, loss 0.0877365, acc 0.953125, prec 0.0536651, recall 0.85657
2017-12-10T12:49:59.235716: step 2117, loss 0.197886, acc 0.953125, prec 0.0536898, recall 0.856642
2017-12-10T12:49:59.675087: step 2118, loss 0.368663, acc 0.875, prec 0.0537357, recall 0.856785
2017-12-10T12:50:00.139211: step 2119, loss 0.229474, acc 0.90625, prec 0.0537553, recall 0.856857
2017-12-10T12:50:00.581875: step 2120, loss 0.327422, acc 0.90625, prec 0.0537749, recall 0.856928
2017-12-10T12:50:01.034555: step 2121, loss 0.429847, acc 0.890625, prec 0.0537928, recall 0.857
2017-12-10T12:50:01.484083: step 2122, loss 0.435188, acc 0.90625, prec 0.053842, recall 0.857143
2017-12-10T12:50:01.925356: step 2123, loss 0.0850088, acc 0.953125, prec 0.053837, recall 0.857143
2017-12-10T12:50:02.373688: step 2124, loss 0.114561, acc 0.953125, prec 0.0538616, recall 0.857214
2017-12-10T12:50:02.823919: step 2125, loss 0.330774, acc 0.921875, prec 0.0539125, recall 0.857357
2017-12-10T12:50:03.270401: step 2126, loss 0.123614, acc 0.953125, prec 0.0539668, recall 0.857499
2017-12-10T12:50:03.728655: step 2127, loss 0.357495, acc 0.84375, prec 0.0539498, recall 0.857499
2017-12-10T12:50:04.191953: step 2128, loss 0.145777, acc 0.953125, prec 0.0539744, recall 0.85757
2017-12-10T12:50:04.633267: step 2129, loss 0.154151, acc 0.953125, prec 0.053999, recall 0.857641
2017-12-10T12:50:05.081628: step 2130, loss 1.1211, acc 0.90625, prec 0.0540185, recall 0.857711
2017-12-10T12:50:05.533356: step 2131, loss 0.259728, acc 0.921875, prec 0.0540397, recall 0.857782
2017-12-10T12:50:05.968818: step 2132, loss 0.363912, acc 0.875, prec 0.0540261, recall 0.857782
2017-12-10T12:50:06.419857: step 2133, loss 0.597201, acc 0.875, prec 0.0540422, recall 0.857853
2017-12-10T12:50:06.866117: step 2134, loss 0.234176, acc 0.90625, prec 0.0540617, recall 0.857924
2017-12-10T12:50:07.336405: step 2135, loss 0.158727, acc 0.9375, prec 0.0540549, recall 0.857924
2017-12-10T12:50:07.772856: step 2136, loss 0.183467, acc 0.953125, prec 0.0540794, recall 0.857994
2017-12-10T12:50:08.219254: step 2137, loss 0.12288, acc 0.96875, prec 0.054076, recall 0.857994
2017-12-10T12:50:08.664562: step 2138, loss 0.407241, acc 0.90625, prec 0.0540955, recall 0.858065
2017-12-10T12:50:09.108777: step 2139, loss 0.268736, acc 0.921875, prec 0.054087, recall 0.858065
2017-12-10T12:50:09.544256: step 2140, loss 0.349927, acc 0.890625, prec 0.0541344, recall 0.858205
2017-12-10T12:50:09.982203: step 2141, loss 0.195582, acc 0.96875, prec 0.0541605, recall 0.858276
2017-12-10T12:50:10.430476: step 2142, loss 0.094101, acc 0.984375, prec 0.0541884, recall 0.858346
2017-12-10T12:50:10.900551: step 2143, loss 0.289816, acc 0.953125, prec 0.0542425, recall 0.858486
2017-12-10T12:50:11.346047: step 2144, loss 0.114415, acc 0.953125, prec 0.054267, recall 0.858556
2017-12-10T12:50:11.796153: step 2145, loss 0.391062, acc 0.90625, prec 0.0543159, recall 0.858696
2017-12-10T12:50:12.242514: step 2146, loss 0.142091, acc 0.9375, prec 0.0543387, recall 0.858765
2017-12-10T12:50:12.687545: step 2147, loss 0.693337, acc 0.9375, prec 0.054391, recall 0.858905
2017-12-10T12:50:13.129497: step 2148, loss 0.0951271, acc 0.984375, prec 0.0543893, recall 0.858905
2017-12-10T12:50:13.574768: step 2149, loss 0.183872, acc 0.953125, prec 0.0543842, recall 0.858905
2017-12-10T12:50:14.036307: step 2150, loss 0.233342, acc 0.9375, prec 0.0543774, recall 0.858905
2017-12-10T12:50:14.480390: step 2151, loss 0.208103, acc 0.9375, prec 0.0544001, recall 0.858974
2017-12-10T12:50:14.923069: step 2152, loss 0.139485, acc 0.96875, prec 0.0543967, recall 0.858974
2017-12-10T12:50:15.368840: step 2153, loss 0.153035, acc 0.9375, prec 0.0544194, recall 0.859044
2017-12-10T12:50:15.801763: step 2154, loss 0.182236, acc 0.921875, prec 0.0544109, recall 0.859044
2017-12-10T12:50:16.244964: step 2155, loss 1.14851, acc 0.921875, prec 0.054432, recall 0.859113
2017-12-10T12:50:16.686055: step 2156, loss 2.00289, acc 0.890625, prec 0.0545086, recall 0.859321
2017-12-10T12:50:17.138900: step 2157, loss 0.182011, acc 0.9375, prec 0.0545313, recall 0.85939
2017-12-10T12:50:17.585405: step 2158, loss 0.560647, acc 0.953125, prec 0.0546146, recall 0.859597
2017-12-10T12:50:18.024755: step 2159, loss 0.687956, acc 0.9375, prec 0.0546373, recall 0.859666
2017-12-10T12:50:18.473659: step 2160, loss 0.604774, acc 0.953125, prec 0.0546617, recall 0.859735
2017-12-10T12:50:18.920323: step 2161, loss 0.423305, acc 0.875, prec 0.0546775, recall 0.859804
2017-12-10T12:50:19.360221: step 2162, loss 0.329395, acc 0.890625, prec 0.054695, recall 0.859873
2017-12-10T12:50:19.808968: step 2163, loss 0.484014, acc 0.84375, prec 0.0547369, recall 0.86001
2017-12-10T12:50:20.245418: step 2164, loss 0.299914, acc 0.859375, prec 0.054751, recall 0.860078
2017-12-10T12:50:20.686877: step 2165, loss 0.520243, acc 0.875, prec 0.0547374, recall 0.860078
2017-12-10T12:50:21.130943: step 2166, loss 0.715923, acc 0.8125, prec 0.0547758, recall 0.860215
2017-12-10T12:50:21.583305: step 2167, loss 0.576241, acc 0.859375, prec 0.0548192, recall 0.860352
2017-12-10T12:50:22.025715: step 2168, loss 0.401723, acc 0.859375, prec 0.0548333, recall 0.86042
2017-12-10T12:50:22.464318: step 2169, loss 5.06307, acc 0.875, prec 0.0548507, recall 0.860068
2017-12-10T12:50:22.914108: step 2170, loss 0.171239, acc 0.921875, prec 0.0548716, recall 0.860136
2017-12-10T12:50:23.348102: step 2171, loss 0.520254, acc 0.875, prec 0.054858, recall 0.860136
2017-12-10T12:50:23.791625: step 2172, loss 0.480423, acc 0.875, prec 0.0548443, recall 0.860136
2017-12-10T12:50:24.238252: step 2173, loss 0.403105, acc 0.84375, prec 0.0548566, recall 0.860205
2017-12-10T12:50:24.685555: step 2174, loss 0.661757, acc 0.890625, prec 0.0549034, recall 0.860341
2017-12-10T12:50:25.130740: step 2175, loss 0.540467, acc 0.828125, prec 0.054914, recall 0.860409
2017-12-10T12:50:25.571625: step 2176, loss 0.41717, acc 0.8125, prec 0.0548936, recall 0.860409
2017-12-10T12:50:26.024205: step 2177, loss 0.670613, acc 0.8125, prec 0.0549024, recall 0.860476
2017-12-10T12:50:26.456074: step 2178, loss 0.817247, acc 0.78125, prec 0.0549079, recall 0.860544
2017-12-10T12:50:26.897407: step 2179, loss 0.69179, acc 0.828125, prec 0.0549185, recall 0.860612
2017-12-10T12:50:27.336266: step 2180, loss 0.540653, acc 0.84375, prec 0.0549015, recall 0.860612
2017-12-10T12:50:27.769089: step 2181, loss 0.55028, acc 0.828125, prec 0.054912, recall 0.86068
2017-12-10T12:50:28.247793: step 2182, loss 0.548617, acc 0.828125, prec 0.0548933, recall 0.86068
2017-12-10T12:50:28.693241: step 2183, loss 0.74137, acc 0.765625, prec 0.0548679, recall 0.86068
2017-12-10T12:50:29.129973: step 2184, loss 0.451369, acc 0.90625, prec 0.0549162, recall 0.860815
2017-12-10T12:50:29.570009: step 2185, loss 0.422, acc 0.90625, prec 0.0549352, recall 0.860882
2017-12-10T12:50:30.023416: step 2186, loss 0.303668, acc 0.890625, prec 0.0549525, recall 0.86095
2017-12-10T12:50:30.467886: step 2187, loss 0.228286, acc 0.921875, prec 0.0550025, recall 0.861084
2017-12-10T12:50:30.906790: step 2188, loss 0.584676, acc 0.875, prec 0.0549889, recall 0.861084
2017-12-10T12:50:31.351798: step 2189, loss 0.113808, acc 0.953125, prec 0.0549838, recall 0.861084
2017-12-10T12:50:31.796164: step 2190, loss 0.381248, acc 0.84375, prec 0.054996, recall 0.861151
2017-12-10T12:50:32.247613: step 2191, loss 0.356574, acc 0.90625, prec 0.0549858, recall 0.861151
2017-12-10T12:50:32.688037: step 2192, loss 0.222993, acc 0.953125, prec 0.0549807, recall 0.861151
2017-12-10T12:50:33.127113: step 2193, loss 0.136689, acc 0.953125, prec 0.0549756, recall 0.861151
2017-12-10T12:50:33.570698: step 2194, loss 0.0288597, acc 0.984375, prec 0.0549739, recall 0.861151
2017-12-10T12:50:34.011083: step 2195, loss 0.255944, acc 0.921875, prec 0.0549654, recall 0.861151
2017-12-10T12:50:34.451266: step 2196, loss 0.164474, acc 0.96875, prec 0.0550204, recall 0.861286
2017-12-10T12:50:34.895262: step 2197, loss 0.440316, acc 0.875, prec 0.055036, recall 0.861353
2017-12-10T12:50:35.336744: step 2198, loss 1.58824, acc 0.890625, prec 0.0550549, recall 0.861004
2017-12-10T12:50:35.792409: step 2199, loss 0.059718, acc 0.953125, prec 0.0550498, recall 0.861004
2017-12-10T12:50:36.231818: step 2200, loss 0.131589, acc 0.953125, prec 0.0550447, recall 0.861004
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2200

2017-12-10T12:50:38.036469: step 2201, loss 0.205863, acc 0.953125, prec 0.0550688, recall 0.861071
2017-12-10T12:50:38.481706: step 2202, loss 0.126545, acc 0.96875, prec 0.0550945, recall 0.861138
2017-12-10T12:50:38.950188: step 2203, loss 0.117619, acc 0.953125, prec 0.0550895, recall 0.861138
2017-12-10T12:50:39.401992: step 2204, loss 0.181243, acc 0.953125, prec 0.0550844, recall 0.861138
2017-12-10T12:50:39.845647: step 2205, loss 0.0638316, acc 0.984375, prec 0.0551118, recall 0.861205
2017-12-10T12:50:40.299487: step 2206, loss 0.154791, acc 0.953125, prec 0.0551358, recall 0.861272
2017-12-10T12:50:40.758353: step 2207, loss 0.165469, acc 0.9375, prec 0.055129, recall 0.861272
2017-12-10T12:50:41.204731: step 2208, loss 0.0345899, acc 0.984375, prec 0.0551565, recall 0.861338
2017-12-10T12:50:41.636934: step 2209, loss 0.104457, acc 0.96875, prec 0.0551822, recall 0.861405
2017-12-10T12:50:42.083026: step 2210, loss 0.0573793, acc 0.984375, prec 0.0551805, recall 0.861405
2017-12-10T12:50:42.521364: step 2211, loss 0.220799, acc 0.953125, prec 0.0551754, recall 0.861405
2017-12-10T12:50:42.958433: step 2212, loss 0.284474, acc 0.96875, prec 0.0552011, recall 0.861472
2017-12-10T12:50:43.409202: step 2213, loss 0.142137, acc 0.96875, prec 0.0552268, recall 0.861538
2017-12-10T12:50:43.851112: step 2214, loss 0.0630981, acc 0.96875, prec 0.0552234, recall 0.861538
2017-12-10T12:50:44.292500: step 2215, loss 0.030957, acc 0.984375, prec 0.0552217, recall 0.861538
2017-12-10T12:50:44.730320: step 2216, loss 0.0879339, acc 0.984375, prec 0.05522, recall 0.861538
2017-12-10T12:50:45.162321: step 2217, loss 2.53586, acc 0.96875, prec 0.0552183, recall 0.861124
2017-12-10T12:50:45.599075: step 2218, loss 0.0244674, acc 1, prec 0.0552474, recall 0.861191
2017-12-10T12:50:46.037559: step 2219, loss 1.03456, acc 0.96875, prec 0.0552731, recall 0.861258
2017-12-10T12:50:46.492705: step 2220, loss 0.0639081, acc 0.984375, prec 0.0552714, recall 0.861258
2017-12-10T12:50:46.929847: step 2221, loss 0.735345, acc 0.953125, prec 0.0552954, recall 0.861324
2017-12-10T12:50:47.381299: step 2222, loss 0.118286, acc 0.96875, prec 0.055292, recall 0.861324
2017-12-10T12:50:47.827592: step 2223, loss 2.04834, acc 0.921875, prec 0.0553434, recall 0.861045
2017-12-10T12:50:48.279488: step 2224, loss 0.264483, acc 0.953125, prec 0.0553674, recall 0.861111
2017-12-10T12:50:48.722942: step 2225, loss 0.18259, acc 0.953125, prec 0.0554495, recall 0.86131
2017-12-10T12:50:49.172331: step 2226, loss 0.49375, acc 0.890625, prec 0.0554666, recall 0.861377
2017-12-10T12:50:49.617320: step 2227, loss 0.577413, acc 0.828125, prec 0.0554769, recall 0.861443
2017-12-10T12:50:50.063382: step 2228, loss 0.469594, acc 0.84375, prec 0.0554599, recall 0.861443
2017-12-10T12:50:50.511007: step 2229, loss 0.593398, acc 0.84375, prec 0.0554718, recall 0.861509
2017-12-10T12:50:50.956388: step 2230, loss 0.763317, acc 0.796875, prec 0.0554497, recall 0.861509
2017-12-10T12:50:51.391240: step 2231, loss 0.349799, acc 0.890625, prec 0.0554668, recall 0.861575
2017-12-10T12:50:51.836224: step 2232, loss 0.293548, acc 0.921875, prec 0.0554583, recall 0.861575
2017-12-10T12:50:52.291666: step 2233, loss 0.568301, acc 0.84375, prec 0.0554412, recall 0.861575
2017-12-10T12:50:52.726237: step 2234, loss 0.565273, acc 0.828125, prec 0.0554515, recall 0.861641
2017-12-10T12:50:53.171889: step 2235, loss 0.397989, acc 0.90625, prec 0.0554993, recall 0.861773
2017-12-10T12:50:53.609105: step 2236, loss 0.60763, acc 0.796875, prec 0.0555061, recall 0.861839
2017-12-10T12:50:54.049778: step 2237, loss 0.408682, acc 0.875, prec 0.0555215, recall 0.861905
2017-12-10T12:50:54.487515: step 2238, loss 0.403947, acc 0.859375, prec 0.0555061, recall 0.861905
2017-12-10T12:50:54.920940: step 2239, loss 0.542412, acc 0.828125, prec 0.0555164, recall 0.86197
2017-12-10T12:50:55.370177: step 2240, loss 0.451241, acc 0.84375, prec 0.0555283, recall 0.862036
2017-12-10T12:50:55.806492: step 2241, loss 0.701798, acc 0.8125, prec 0.0555658, recall 0.862167
2017-12-10T12:50:56.254279: step 2242, loss 0.208682, acc 0.9375, prec 0.0556168, recall 0.862298
2017-12-10T12:50:56.706940: step 2243, loss 0.246282, acc 0.9375, prec 0.05561, recall 0.862298
2017-12-10T12:50:57.156050: step 2244, loss 0.258682, acc 0.953125, prec 0.0556049, recall 0.862298
2017-12-10T12:50:57.610162: step 2245, loss 0.439375, acc 0.90625, prec 0.0556525, recall 0.862429
2017-12-10T12:50:58.060107: step 2246, loss 0.728038, acc 0.953125, prec 0.0557341, recall 0.862624
2017-12-10T12:50:58.507887: step 2247, loss 0.158632, acc 0.9375, prec 0.0557562, recall 0.862689
2017-12-10T12:50:58.943418: step 2248, loss 0.273388, acc 0.953125, prec 0.0557799, recall 0.862754
2017-12-10T12:50:59.384896: step 2249, loss 0.301481, acc 0.9375, prec 0.055802, recall 0.862819
2017-12-10T12:50:59.816336: step 2250, loss 0.428933, acc 0.921875, prec 0.0558512, recall 0.862949
2017-12-10T12:51:00.270083: step 2251, loss 0.264769, acc 0.9375, prec 0.0559021, recall 0.863078
2017-12-10T12:51:00.716667: step 2252, loss 1.11685, acc 0.921875, prec 0.0559225, recall 0.863143
2017-12-10T12:51:01.166534: step 2253, loss 0.136562, acc 0.9375, prec 0.0559445, recall 0.863208
2017-12-10T12:51:01.599922: step 2254, loss 0.192768, acc 0.9375, prec 0.0559665, recall 0.863272
2017-12-10T12:51:02.041622: step 2255, loss 0.493795, acc 0.890625, prec 0.0559545, recall 0.863272
2017-12-10T12:51:02.479103: step 2256, loss 0.212055, acc 0.90625, prec 0.0559443, recall 0.863272
2017-12-10T12:51:02.930529: step 2257, loss 0.0970657, acc 0.953125, prec 0.0559391, recall 0.863272
2017-12-10T12:51:03.385308: step 2258, loss 0.145285, acc 0.9375, prec 0.0559323, recall 0.863272
2017-12-10T12:51:03.848816: step 2259, loss 0.278272, acc 0.890625, prec 0.0559203, recall 0.863272
2017-12-10T12:51:04.301567: step 2260, loss 0.433368, acc 0.90625, prec 0.0559678, recall 0.863401
2017-12-10T12:51:04.753302: step 2261, loss 0.398642, acc 0.84375, prec 0.0559507, recall 0.863401
2017-12-10T12:51:05.203340: step 2262, loss 0.282404, acc 0.890625, prec 0.0559963, recall 0.863529
2017-12-10T12:51:05.656894: step 2263, loss 0.187809, acc 0.9375, prec 0.0559895, recall 0.863529
2017-12-10T12:51:06.107636: step 2264, loss 0.265869, acc 0.953125, prec 0.0559844, recall 0.863529
2017-12-10T12:51:06.559094: step 2265, loss 0.225033, acc 0.9375, prec 0.0560063, recall 0.863594
2017-12-10T12:51:07.017326: step 2266, loss 0.157353, acc 0.953125, prec 0.0560012, recall 0.863594
2017-12-10T12:51:07.464893: step 2267, loss 0.71604, acc 0.9375, prec 0.0560232, recall 0.863658
2017-12-10T12:51:07.909415: step 2268, loss 0.0883705, acc 0.984375, prec 0.0560215, recall 0.863658
2017-12-10T12:51:08.359009: step 2269, loss 0.341568, acc 0.859375, prec 0.0560349, recall 0.863722
2017-12-10T12:51:08.795395: step 2270, loss 0.35855, acc 0.921875, prec 0.0560551, recall 0.863786
2017-12-10T12:51:09.223810: step 2271, loss 0.619487, acc 0.953125, prec 0.0560788, recall 0.86385
2017-12-10T12:51:09.669951: step 2272, loss 0.079695, acc 0.96875, prec 0.0561329, recall 0.863977
2017-12-10T12:51:10.112404: step 2273, loss 0.467987, acc 0.96875, prec 0.0561582, recall 0.864041
2017-12-10T12:51:10.554591: step 2274, loss 0.0860626, acc 0.96875, prec 0.0561548, recall 0.864041
2017-12-10T12:51:11.020903: step 2275, loss 0.535456, acc 0.890625, prec 0.0561716, recall 0.864105
2017-12-10T12:51:11.463839: step 2276, loss 0.149636, acc 0.9375, prec 0.0561647, recall 0.864105
2017-12-10T12:51:11.898318: step 2277, loss 0.331477, acc 0.921875, prec 0.0561849, recall 0.864169
2017-12-10T12:51:12.348526: step 2278, loss 0.152594, acc 0.953125, prec 0.0562373, recall 0.864296
2017-12-10T12:51:12.803764: step 2279, loss 0.279005, acc 0.90625, prec 0.0562557, recall 0.864359
2017-12-10T12:51:13.248454: step 2280, loss 0.305077, acc 0.875, prec 0.056242, recall 0.864359
2017-12-10T12:51:13.687275: step 2281, loss 0.177358, acc 0.96875, prec 0.0562386, recall 0.864359
2017-12-10T12:51:14.164107: step 2282, loss 0.180719, acc 0.96875, prec 0.0562639, recall 0.864423
2017-12-10T12:51:14.613611: step 2283, loss 0.0376189, acc 0.984375, prec 0.0562622, recall 0.864423
2017-12-10T12:51:15.068331: step 2284, loss 0.305047, acc 0.953125, prec 0.0562858, recall 0.864486
2017-12-10T12:51:15.522935: step 2285, loss 0.23665, acc 0.9375, prec 0.0562789, recall 0.864486
2017-12-10T12:51:15.968775: step 2286, loss 0.324594, acc 0.9375, prec 0.0563582, recall 0.864676
2017-12-10T12:51:16.411099: step 2287, loss 0.0654785, acc 0.96875, prec 0.0563834, recall 0.864739
2017-12-10T12:51:16.852008: step 2288, loss 0.252931, acc 0.96875, prec 0.0564087, recall 0.864802
2017-12-10T12:51:17.294939: step 2289, loss 0.292576, acc 0.921875, prec 0.0564001, recall 0.864802
2017-12-10T12:51:17.737497: step 2290, loss 1.53067, acc 0.96875, prec 0.0563984, recall 0.864399
2017-12-10T12:51:18.187484: step 2291, loss 0.143785, acc 0.953125, prec 0.0563933, recall 0.864399
2017-12-10T12:51:18.629717: step 2292, loss 0.44134, acc 0.90625, prec 0.056383, recall 0.864399
2017-12-10T12:51:19.074655: step 2293, loss 0.114741, acc 0.96875, prec 0.0563796, recall 0.864399
2017-12-10T12:51:19.513099: step 2294, loss 0.426969, acc 0.875, prec 0.0563945, recall 0.864462
2017-12-10T12:51:19.943510: step 2295, loss 0.591997, acc 0.9375, prec 0.0564163, recall 0.864525
2017-12-10T12:51:20.384079: step 2296, loss 0.322794, acc 0.90625, prec 0.0564634, recall 0.864651
2017-12-10T12:51:20.831551: step 2297, loss 0.27592, acc 0.9375, prec 0.0565138, recall 0.864777
2017-12-10T12:51:21.282108: step 2298, loss 0.741893, acc 0.90625, prec 0.0565322, recall 0.86484
2017-12-10T12:51:21.748471: step 2299, loss 0.280602, acc 0.90625, prec 0.0565505, recall 0.864902
2017-12-10T12:51:22.188143: step 2300, loss 0.140964, acc 0.921875, prec 0.0565706, recall 0.864965
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2300

2017-12-10T12:51:24.120519: step 2301, loss 0.126964, acc 0.984375, prec 0.0566547, recall 0.865153
2017-12-10T12:51:24.560561: step 2302, loss 0.370933, acc 0.921875, prec 0.0567034, recall 0.865278
2017-12-10T12:51:25.008878: step 2303, loss 0.831789, acc 0.90625, prec 0.0567503, recall 0.865402
2017-12-10T12:51:25.452159: step 2304, loss 0.274618, acc 0.9375, prec 0.0568006, recall 0.865527
2017-12-10T12:51:25.882470: step 2305, loss 0.243569, acc 0.9375, prec 0.0568223, recall 0.865589
2017-12-10T12:51:26.319466: step 2306, loss 0.319241, acc 0.921875, prec 0.0568137, recall 0.865589
2017-12-10T12:51:26.777688: step 2307, loss 0.292894, acc 0.890625, prec 0.0568302, recall 0.865651
2017-12-10T12:51:27.229347: step 2308, loss 0.881557, acc 0.84375, prec 0.0568416, recall 0.865713
2017-12-10T12:51:27.673478: step 2309, loss 0.289904, acc 0.921875, prec 0.0568616, recall 0.865775
2017-12-10T12:51:28.106157: step 2310, loss 0.562396, acc 0.84375, prec 0.0568443, recall 0.865775
2017-12-10T12:51:28.546858: step 2311, loss 0.252777, acc 0.90625, prec 0.056834, recall 0.865775
2017-12-10T12:51:29.011508: step 2312, loss 0.66981, acc 0.859375, prec 0.0568471, recall 0.865837
2017-12-10T12:51:29.453540: step 2313, loss 0.351725, acc 0.875, prec 0.0568619, recall 0.865899
2017-12-10T12:51:29.898502: step 2314, loss 0.507937, acc 0.890625, prec 0.0568498, recall 0.865899
2017-12-10T12:51:30.348580: step 2315, loss 0.421335, acc 0.859375, prec 0.0568343, recall 0.865899
2017-12-10T12:51:30.800377: step 2316, loss 0.128141, acc 0.9375, prec 0.0568275, recall 0.865899
2017-12-10T12:51:31.239520: step 2317, loss 0.380566, acc 0.875, prec 0.0568137, recall 0.865899
2017-12-10T12:51:31.679305: step 2318, loss 0.514366, acc 0.875, prec 0.0568, recall 0.865899
2017-12-10T12:51:32.132908: step 2319, loss 0.274072, acc 0.875, prec 0.0568433, recall 0.866022
2017-12-10T12:51:32.581454: step 2320, loss 0.19672, acc 0.953125, prec 0.0568951, recall 0.866145
2017-12-10T12:51:33.017547: step 2321, loss 0.36779, acc 0.921875, prec 0.056915, recall 0.866207
2017-12-10T12:51:33.463383: step 2322, loss 0.238291, acc 0.953125, prec 0.0569668, recall 0.86633
2017-12-10T12:51:33.901933: step 2323, loss 0.35639, acc 0.921875, prec 0.0569867, recall 0.866391
2017-12-10T12:51:34.348741: step 2324, loss 0.3115, acc 0.921875, prec 0.0569781, recall 0.866391
2017-12-10T12:51:34.781788: step 2325, loss 0.300144, acc 0.96875, prec 0.0570031, recall 0.866453
2017-12-10T12:51:35.226871: step 2326, loss 0.0450595, acc 0.96875, prec 0.0570281, recall 0.866514
2017-12-10T12:51:35.678169: step 2327, loss 0.168359, acc 0.953125, prec 0.0570514, recall 0.866575
2017-12-10T12:51:36.122633: step 2328, loss 0.140249, acc 0.96875, prec 0.057048, recall 0.866575
2017-12-10T12:51:36.555584: step 2329, loss 0.167121, acc 0.953125, prec 0.0570428, recall 0.866575
2017-12-10T12:51:37.003976: step 2330, loss 0.127789, acc 0.984375, prec 0.0571265, recall 0.866758
2017-12-10T12:51:37.441667: step 2331, loss 0.751664, acc 0.9375, prec 0.0571765, recall 0.86688
2017-12-10T12:51:37.887131: step 2332, loss 2.35341, acc 0.96875, prec 0.0571748, recall 0.866484
2017-12-10T12:51:38.360587: step 2333, loss 0.121214, acc 0.984375, prec 0.057173, recall 0.866484
2017-12-10T12:51:38.803757: step 2334, loss 0.195453, acc 0.953125, prec 0.0571963, recall 0.866545
2017-12-10T12:51:39.246858: step 2335, loss 0.234506, acc 0.90625, prec 0.0572144, recall 0.866606
2017-12-10T12:51:39.688298: step 2336, loss 1.66492, acc 0.96875, prec 0.0572979, recall 0.866393
2017-12-10T12:51:40.130010: step 2337, loss 3.27441, acc 0.84375, prec 0.0572824, recall 0.865998
2017-12-10T12:51:40.571092: step 2338, loss 0.412522, acc 0.875, prec 0.0573254, recall 0.86612
2017-12-10T12:51:41.008080: step 2339, loss 0.518627, acc 0.84375, prec 0.0573365, recall 0.866181
2017-12-10T12:51:41.452069: step 2340, loss 0.600813, acc 0.8125, prec 0.0573158, recall 0.866181
2017-12-10T12:51:41.896434: step 2341, loss 0.940279, acc 0.765625, prec 0.0572899, recall 0.866181
2017-12-10T12:51:42.340533: step 2342, loss 1.39527, acc 0.640625, prec 0.0572503, recall 0.866181
2017-12-10T12:51:42.768932: step 2343, loss 0.971493, acc 0.8125, prec 0.0572296, recall 0.866181
2017-12-10T12:51:43.212144: step 2344, loss 1.06458, acc 0.75, prec 0.0572305, recall 0.866242
2017-12-10T12:51:43.656875: step 2345, loss 1.05621, acc 0.703125, prec 0.0571978, recall 0.866242
2017-12-10T12:51:44.098684: step 2346, loss 0.725032, acc 0.765625, prec 0.0572003, recall 0.866303
2017-12-10T12:51:44.553663: step 2347, loss 0.987978, acc 0.734375, prec 0.0571994, recall 0.866364
2017-12-10T12:51:44.997607: step 2348, loss 0.780803, acc 0.796875, prec 0.0572337, recall 0.866485
2017-12-10T12:51:45.446195: step 2349, loss 0.754902, acc 0.765625, prec 0.0572362, recall 0.866546
2017-12-10T12:51:45.920393: step 2350, loss 0.831154, acc 0.8125, prec 0.0572439, recall 0.866606
2017-12-10T12:51:46.365944: step 2351, loss 1.41924, acc 0.625, prec 0.057231, recall 0.866667
2017-12-10T12:51:46.818738: step 2352, loss 0.935073, acc 0.8125, prec 0.0572104, recall 0.866667
2017-12-10T12:51:47.252499: step 2353, loss 0.657107, acc 0.828125, prec 0.0572762, recall 0.866848
2017-12-10T12:51:47.701311: step 2354, loss 0.597789, acc 0.78125, prec 0.0573086, recall 0.866968
2017-12-10T12:51:48.142376: step 2355, loss 0.655405, acc 0.796875, prec 0.0573427, recall 0.867089
2017-12-10T12:51:48.580117: step 2356, loss 0.412501, acc 0.875, prec 0.0573854, recall 0.867209
2017-12-10T12:51:49.024055: step 2357, loss 0.599906, acc 0.828125, prec 0.0573665, recall 0.867209
2017-12-10T12:51:49.470424: step 2358, loss 0.754143, acc 0.875, prec 0.0574373, recall 0.867388
2017-12-10T12:51:49.924751: step 2359, loss 0.290435, acc 0.90625, prec 0.0574551, recall 0.867448
2017-12-10T12:51:50.378071: step 2360, loss 0.60554, acc 0.78125, prec 0.0574593, recall 0.867508
2017-12-10T12:51:50.834576: step 2361, loss 0.377084, acc 0.875, prec 0.0574737, recall 0.867568
2017-12-10T12:51:51.274071: step 2362, loss 0.15807, acc 0.953125, prec 0.0574685, recall 0.867568
2017-12-10T12:51:51.718316: step 2363, loss 0.305466, acc 0.9375, prec 0.057546, recall 0.867746
2017-12-10T12:51:52.164734: step 2364, loss 0.217676, acc 0.921875, prec 0.0575655, recall 0.867806
2017-12-10T12:51:52.612748: step 2365, loss 0.110799, acc 0.96875, prec 0.0575902, recall 0.867865
2017-12-10T12:51:53.055101: step 2366, loss 0.248093, acc 0.953125, prec 0.0575851, recall 0.867865
2017-12-10T12:51:53.503756: step 2367, loss 0.115891, acc 0.9375, prec 0.0575782, recall 0.867865
2017-12-10T12:51:53.961626: step 2368, loss 0.108494, acc 0.984375, prec 0.0576327, recall 0.867984
2017-12-10T12:51:54.429395: step 2369, loss 0.509756, acc 0.953125, prec 0.0577118, recall 0.868161
2017-12-10T12:51:54.876370: step 2370, loss 1.66669, acc 0.984375, prec 0.0577118, recall 0.867772
2017-12-10T12:51:55.338815: step 2371, loss 2.12675, acc 0.9375, prec 0.0577066, recall 0.867384
2017-12-10T12:51:55.793122: step 2372, loss 0.164031, acc 0.953125, prec 0.0577296, recall 0.867443
2017-12-10T12:51:56.241064: step 2373, loss 0.227775, acc 0.9375, prec 0.0577788, recall 0.867562
2017-12-10T12:51:56.692462: step 2374, loss 0.140772, acc 0.953125, prec 0.0578017, recall 0.867621
2017-12-10T12:51:57.132590: step 2375, loss 0.387632, acc 0.921875, prec 0.0577931, recall 0.867621
2017-12-10T12:51:57.575319: step 2376, loss 0.332655, acc 0.890625, prec 0.0578091, recall 0.86768
2017-12-10T12:51:58.018190: step 2377, loss 0.360975, acc 0.921875, prec 0.0579128, recall 0.867916
2017-12-10T12:51:58.449565: step 2378, loss 0.115059, acc 0.96875, prec 0.0579374, recall 0.867975
2017-12-10T12:51:58.884713: step 2379, loss 0.384283, acc 0.859375, prec 0.0579499, recall 0.868034
2017-12-10T12:51:59.336243: step 2380, loss 0.426263, acc 0.953125, prec 0.0579727, recall 0.868093
2017-12-10T12:51:59.792434: step 2381, loss 0.232174, acc 0.953125, prec 0.0579956, recall 0.868151
2017-12-10T12:52:00.247250: step 2382, loss 0.338827, acc 0.9375, prec 0.0580167, recall 0.86821
2017-12-10T12:52:00.697201: step 2383, loss 0.251961, acc 0.90625, prec 0.0580064, recall 0.86821
2017-12-10T12:52:01.135858: step 2384, loss 0.303374, acc 0.875, prec 0.0580206, recall 0.868269
2017-12-10T12:52:01.575953: step 2385, loss 0.491536, acc 0.90625, prec 0.0580382, recall 0.868327
2017-12-10T12:52:02.021172: step 2386, loss 0.460953, acc 0.890625, prec 0.0580262, recall 0.868327
2017-12-10T12:52:02.465805: step 2387, loss 0.229574, acc 0.921875, prec 0.0580455, recall 0.868386
2017-12-10T12:52:02.912322: step 2388, loss 0.358098, acc 0.921875, prec 0.0580649, recall 0.868444
2017-12-10T12:52:03.359868: step 2389, loss 1.12223, acc 0.890625, prec 0.0580808, recall 0.868503
2017-12-10T12:52:03.814243: step 2390, loss 0.622117, acc 0.84375, prec 0.0581195, recall 0.86862
2017-12-10T12:52:04.254178: step 2391, loss 0.35016, acc 0.921875, prec 0.0581109, recall 0.86862
2017-12-10T12:52:04.697079: step 2392, loss 0.356749, acc 0.875, prec 0.0580971, recall 0.86862
2017-12-10T12:52:05.137313: step 2393, loss 0.182218, acc 0.921875, prec 0.0580885, recall 0.86862
2017-12-10T12:52:05.579138: step 2394, loss 0.418154, acc 0.859375, prec 0.0580729, recall 0.86862
2017-12-10T12:52:06.018973: step 2395, loss 0.343489, acc 0.921875, prec 0.0580923, recall 0.868678
2017-12-10T12:52:06.467221: step 2396, loss 0.297822, acc 0.90625, prec 0.0580819, recall 0.868678
2017-12-10T12:52:06.918145: step 2397, loss 0.0863929, acc 0.953125, prec 0.0580768, recall 0.868678
2017-12-10T12:52:07.367528: step 2398, loss 0.313776, acc 0.9375, prec 0.0581257, recall 0.868794
2017-12-10T12:52:07.814864: step 2399, loss 0.726046, acc 0.90625, prec 0.0581433, recall 0.868852
2017-12-10T12:52:08.255366: step 2400, loss 0.449364, acc 0.859375, prec 0.0581557, recall 0.868911
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2400

2017-12-10T12:52:10.302677: step 2401, loss 0.807106, acc 0.96875, prec 0.0582081, recall 0.869027
2017-12-10T12:52:10.752427: step 2402, loss 1.52444, acc 0.90625, prec 0.0582257, recall 0.869084
2017-12-10T12:52:11.192429: step 2403, loss 0.188199, acc 0.9375, prec 0.0582188, recall 0.869084
2017-12-10T12:52:11.649895: step 2404, loss 0.339443, acc 0.875, prec 0.058205, recall 0.869084
2017-12-10T12:52:12.096794: step 2405, loss 0.594839, acc 0.890625, prec 0.0582208, recall 0.869142
2017-12-10T12:52:12.534706: step 2406, loss 0.0954999, acc 0.984375, prec 0.0583027, recall 0.869316
2017-12-10T12:52:12.984540: step 2407, loss 1.17229, acc 0.96875, prec 0.0583289, recall 0.86899
2017-12-10T12:52:13.433675: step 2408, loss 0.151805, acc 0.9375, prec 0.0583499, recall 0.869048
2017-12-10T12:52:13.885457: step 2409, loss 0.276872, acc 0.921875, prec 0.0584248, recall 0.869221
2017-12-10T12:52:14.333613: step 2410, loss 0.391574, acc 0.953125, prec 0.0584754, recall 0.869336
2017-12-10T12:52:14.780037: step 2411, loss 0.652749, acc 0.921875, prec 0.0584946, recall 0.869393
2017-12-10T12:52:15.245557: step 2412, loss 0.251108, acc 0.9375, prec 0.0585434, recall 0.869508
2017-12-10T12:52:15.690889: step 2413, loss 0.541341, acc 0.8125, prec 0.0585226, recall 0.869508
2017-12-10T12:52:16.131394: step 2414, loss 0.237101, acc 0.921875, prec 0.0585696, recall 0.869622
2017-12-10T12:52:16.577563: step 2415, loss 1.0912, acc 0.796875, prec 0.0585749, recall 0.86968
2017-12-10T12:52:17.018382: step 2416, loss 0.320064, acc 0.890625, prec 0.0586184, recall 0.869794
2017-12-10T12:52:17.471259: step 2417, loss 1.02544, acc 0.859375, prec 0.0586307, recall 0.869851
2017-12-10T12:52:17.910556: step 2418, loss 0.945597, acc 0.828125, prec 0.0586116, recall 0.869851
2017-12-10T12:52:18.353587: step 2419, loss 0.211057, acc 0.90625, prec 0.0586012, recall 0.869851
2017-12-10T12:52:18.811653: step 2420, loss 0.272173, acc 0.90625, prec 0.0585909, recall 0.869851
2017-12-10T12:52:19.266922: step 2421, loss 0.370999, acc 0.875, prec 0.0586048, recall 0.869908
2017-12-10T12:52:19.711349: step 2422, loss 0.47473, acc 0.890625, prec 0.0586205, recall 0.869965
2017-12-10T12:52:20.155176: step 2423, loss 0.293617, acc 0.921875, prec 0.0586396, recall 0.870022
2017-12-10T12:52:20.594888: step 2424, loss 0.351044, acc 0.859375, prec 0.0587073, recall 0.870192
2017-12-10T12:52:21.038533: step 2425, loss 0.939411, acc 0.9375, prec 0.0587559, recall 0.870306
2017-12-10T12:52:21.494712: step 2426, loss 0.0508414, acc 1, prec 0.0588114, recall 0.870419
2017-12-10T12:52:21.945843: step 2427, loss 0.259608, acc 0.9375, prec 0.0588599, recall 0.870532
2017-12-10T12:52:22.402465: step 2428, loss 0.260559, acc 0.90625, prec 0.0588773, recall 0.870588
2017-12-10T12:52:22.851205: step 2429, loss 0.297516, acc 0.9375, prec 0.0588981, recall 0.870645
2017-12-10T12:52:23.299485: step 2430, loss 0.733178, acc 0.921875, prec 0.0589448, recall 0.870757
2017-12-10T12:52:23.746919: step 2431, loss 0.310456, acc 0.921875, prec 0.0589639, recall 0.870813
2017-12-10T12:52:24.199805: step 2432, loss 0.958226, acc 0.90625, prec 0.0589812, recall 0.87087
2017-12-10T12:52:24.647674: step 2433, loss 0.264269, acc 0.875, prec 0.0589673, recall 0.87087
2017-12-10T12:52:25.082824: step 2434, loss 0.222751, acc 0.90625, prec 0.0589845, recall 0.870926
2017-12-10T12:52:25.530989: step 2435, loss 0.272236, acc 0.90625, prec 0.0589741, recall 0.870926
2017-12-10T12:52:25.966275: step 2436, loss 0.311119, acc 0.875, prec 0.0589603, recall 0.870926
2017-12-10T12:52:26.398346: step 2437, loss 0.424471, acc 0.90625, prec 0.0590052, recall 0.871038
2017-12-10T12:52:26.842214: step 2438, loss 0.316182, acc 0.921875, prec 0.0590242, recall 0.871094
2017-12-10T12:52:27.287585: step 2439, loss 0.234233, acc 0.921875, prec 0.0590155, recall 0.871094
2017-12-10T12:52:27.721951: step 2440, loss 0.372972, acc 0.84375, prec 0.0590535, recall 0.871206
2017-12-10T12:52:28.161594: step 2441, loss 1.8343, acc 0.90625, prec 0.0591001, recall 0.87094
2017-12-10T12:52:28.613180: step 2442, loss 0.533307, acc 0.828125, prec 0.0591087, recall 0.870996
2017-12-10T12:52:29.064098: step 2443, loss 0.556738, acc 0.90625, prec 0.0591259, recall 0.871051
2017-12-10T12:52:29.495819: step 2444, loss 0.712305, acc 0.953125, prec 0.0591759, recall 0.871163
2017-12-10T12:52:29.947673: step 2445, loss 0.657776, acc 0.859375, prec 0.0591879, recall 0.871219
2017-12-10T12:52:30.396550: step 2446, loss 0.208815, acc 0.921875, prec 0.0591792, recall 0.871219
2017-12-10T12:52:30.852290: step 2447, loss 0.258225, acc 0.921875, prec 0.0591982, recall 0.871274
2017-12-10T12:52:31.303727: step 2448, loss 0.4581, acc 0.921875, prec 0.0591895, recall 0.871274
2017-12-10T12:52:31.748072: step 2449, loss 0.382357, acc 0.859375, prec 0.0591739, recall 0.871274
2017-12-10T12:52:32.201581: step 2450, loss 2.66144, acc 0.875, prec 0.0591617, recall 0.870898
2017-12-10T12:52:32.642123: step 2451, loss 0.44187, acc 0.859375, prec 0.0591461, recall 0.870898
2017-12-10T12:52:33.078308: step 2452, loss 0.331298, acc 0.90625, prec 0.0591357, recall 0.870898
2017-12-10T12:52:33.522776: step 2453, loss 0.194292, acc 0.9375, prec 0.0591563, recall 0.870954
2017-12-10T12:52:33.962033: step 2454, loss 0.398054, acc 0.875, prec 0.0591425, recall 0.870954
2017-12-10T12:52:34.402396: step 2455, loss 0.281723, acc 0.875, prec 0.0591286, recall 0.870954
2017-12-10T12:52:34.855809: step 2456, loss 0.470798, acc 0.859375, prec 0.0591406, recall 0.871009
2017-12-10T12:52:35.307009: step 2457, loss 0.771308, acc 0.875, prec 0.0592094, recall 0.871176
2017-12-10T12:52:35.761776: step 2458, loss 0.250871, acc 0.890625, prec 0.0592523, recall 0.871287
2017-12-10T12:52:36.200502: step 2459, loss 0.0848662, acc 0.96875, prec 0.0592488, recall 0.871287
2017-12-10T12:52:36.630272: step 2460, loss 0.34504, acc 0.90625, prec 0.0592384, recall 0.871287
2017-12-10T12:52:37.077169: step 2461, loss 0.300925, acc 0.875, prec 0.0592246, recall 0.871287
2017-12-10T12:52:37.522508: step 2462, loss 0.100735, acc 0.953125, prec 0.0592194, recall 0.871287
2017-12-10T12:52:37.965034: step 2463, loss 0.470004, acc 0.90625, prec 0.0592365, recall 0.871343
2017-12-10T12:52:38.410351: step 2464, loss 0.342351, acc 0.90625, prec 0.0592536, recall 0.871398
2017-12-10T12:52:38.849821: step 2465, loss 0.254986, acc 0.953125, prec 0.0593034, recall 0.871508
2017-12-10T12:52:39.286137: step 2466, loss 0.359796, acc 0.890625, prec 0.0592913, recall 0.871508
2017-12-10T12:52:39.719584: step 2467, loss 1.1489, acc 0.828125, prec 0.0593272, recall 0.871619
2017-12-10T12:52:40.155609: step 2468, loss 0.259037, acc 0.953125, prec 0.059322, recall 0.871619
2017-12-10T12:52:40.597581: step 2469, loss 3.42044, acc 0.9375, prec 0.0593168, recall 0.871245
2017-12-10T12:52:41.053572: step 2470, loss 0.140581, acc 0.96875, prec 0.0593409, recall 0.8713
2017-12-10T12:52:41.495564: step 2471, loss 0.172361, acc 0.921875, prec 0.0593871, recall 0.87141
2017-12-10T12:52:41.942951: step 2472, loss 0.295088, acc 0.890625, prec 0.0594025, recall 0.871465
2017-12-10T12:52:42.393884: step 2473, loss 0.451999, acc 0.875, prec 0.0593886, recall 0.871465
2017-12-10T12:52:42.849170: step 2474, loss 0.264641, acc 0.890625, prec 0.0594039, recall 0.87152
2017-12-10T12:52:43.295940: step 2475, loss 0.354043, acc 0.875, prec 0.05939, recall 0.87152
2017-12-10T12:52:43.746597: step 2476, loss 0.425995, acc 0.875, prec 0.0593762, recall 0.87152
2017-12-10T12:52:44.192309: step 2477, loss 0.545792, acc 0.859375, prec 0.059388, recall 0.871575
2017-12-10T12:52:44.636404: step 2478, loss 0.296868, acc 0.9375, prec 0.0594085, recall 0.87163
2017-12-10T12:52:45.079011: step 2479, loss 0.35949, acc 0.890625, prec 0.0594513, recall 0.87174
2017-12-10T12:52:45.524121: step 2480, loss 0.299602, acc 0.875, prec 0.0594648, recall 0.871795
2017-12-10T12:52:45.959051: step 2481, loss 0.445597, acc 0.9375, prec 0.0595127, recall 0.871904
2017-12-10T12:52:46.407551: step 2482, loss 0.25351, acc 0.921875, prec 0.0595588, recall 0.872014
2017-12-10T12:52:46.848752: step 2483, loss 0.484332, acc 0.921875, prec 0.059605, recall 0.872123
2017-12-10T12:52:47.292799: step 2484, loss 0.720954, acc 0.875, prec 0.0596459, recall 0.872232
2017-12-10T12:52:47.701981: step 2485, loss 0.11972, acc 1, prec 0.0597006, recall 0.87234
2017-12-10T12:52:48.161750: step 2486, loss 0.254975, acc 0.9375, prec 0.0596937, recall 0.87234
2017-12-10T12:52:48.600109: step 2487, loss 1.59049, acc 0.875, prec 0.0597089, recall 0.872024
2017-12-10T12:52:49.061112: step 2488, loss 0.450698, acc 0.875, prec 0.0597223, recall 0.872078
2017-12-10T12:52:49.505072: step 2489, loss 0.752637, acc 0.859375, prec 0.0597614, recall 0.872187
2017-12-10T12:52:49.962586: step 2490, loss 0.228759, acc 0.9375, prec 0.0597545, recall 0.872187
2017-12-10T12:52:50.405887: step 2491, loss 0.538364, acc 0.90625, prec 0.0598534, recall 0.872404
2017-12-10T12:52:50.849925: step 2492, loss 0.305238, acc 0.90625, prec 0.0598703, recall 0.872458
2017-12-10T12:52:51.293928: step 2493, loss 0.362633, acc 0.875, prec 0.0598837, recall 0.872512
2017-12-10T12:52:51.733936: step 2494, loss 0.388782, acc 0.90625, prec 0.0598733, recall 0.872512
2017-12-10T12:52:52.173196: step 2495, loss 0.546274, acc 0.8125, prec 0.0598797, recall 0.872566
2017-12-10T12:52:52.631189: step 2496, loss 0.411068, acc 0.90625, prec 0.0599239, recall 0.872673
2017-12-10T12:52:53.077032: step 2497, loss 0.135457, acc 0.921875, prec 0.0599152, recall 0.872673
2017-12-10T12:52:53.520706: step 2498, loss 0.394581, acc 0.890625, prec 0.0599576, recall 0.872781
2017-12-10T12:52:53.971910: step 2499, loss 0.248221, acc 0.90625, prec 0.0599745, recall 0.872835
2017-12-10T12:52:54.423831: step 2500, loss 0.443864, acc 0.890625, prec 0.0600168, recall 0.872942
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2500

2017-12-10T12:52:56.351140: step 2501, loss 0.21558, acc 0.9375, prec 0.0600099, recall 0.872942
2017-12-10T12:52:56.793028: step 2502, loss 0.0843043, acc 0.984375, prec 0.0600354, recall 0.872996
2017-12-10T12:52:57.234204: step 2503, loss 0.479968, acc 0.859375, prec 0.060047, recall 0.873049
2017-12-10T12:52:57.690870: step 2504, loss 0.295984, acc 0.921875, prec 0.0600383, recall 0.873049
2017-12-10T12:52:58.129962: step 2505, loss 1.04705, acc 0.90625, prec 0.0601096, recall 0.87321
2017-12-10T12:52:58.574964: step 2506, loss 0.219604, acc 0.921875, prec 0.0601554, recall 0.873317
2017-12-10T12:52:59.014696: step 2507, loss 0.162403, acc 0.90625, prec 0.0601449, recall 0.873317
2017-12-10T12:52:59.454881: step 2508, loss 0.451801, acc 0.890625, prec 0.06016, recall 0.87337
2017-12-10T12:52:59.900300: step 2509, loss 0.286669, acc 0.953125, prec 0.0601547, recall 0.87337
2017-12-10T12:53:00.337442: step 2510, loss 0.168629, acc 0.953125, prec 0.0601767, recall 0.873423
2017-12-10T12:53:00.777657: step 2511, loss 0.151357, acc 0.921875, prec 0.0602225, recall 0.873529
2017-12-10T12:53:01.220012: step 2512, loss 0.257013, acc 0.90625, prec 0.060212, recall 0.873529
2017-12-10T12:53:01.661594: step 2513, loss 0.111715, acc 0.96875, prec 0.0602629, recall 0.873636
2017-12-10T12:53:02.103986: step 2514, loss 0.225039, acc 0.90625, prec 0.0602797, recall 0.873689
2017-12-10T12:53:02.541355: step 2515, loss 0.365696, acc 0.90625, prec 0.0602692, recall 0.873689
2017-12-10T12:53:02.970094: step 2516, loss 0.0428663, acc 0.984375, prec 0.0602675, recall 0.873689
2017-12-10T12:53:03.414558: step 2517, loss 0.117563, acc 0.984375, prec 0.0602929, recall 0.873742
2017-12-10T12:53:03.847923: step 2518, loss 0.130347, acc 0.9375, prec 0.0602859, recall 0.873742
2017-12-10T12:53:04.291073: step 2519, loss 0.147641, acc 0.9375, prec 0.0603605, recall 0.8739
2017-12-10T12:53:04.727129: step 2520, loss 0.161244, acc 0.9375, prec 0.0603536, recall 0.8739
2017-12-10T12:53:05.166898: step 2521, loss 0.249351, acc 0.9375, prec 0.0603466, recall 0.8739
2017-12-10T12:53:05.603619: step 2522, loss 0.0236138, acc 1, prec 0.0603738, recall 0.873953
2017-12-10T12:53:06.037910: step 2523, loss 0.20409, acc 0.96875, prec 0.0603974, recall 0.874006
2017-12-10T12:53:06.477371: step 2524, loss 1.85815, acc 0.953125, prec 0.0604211, recall 0.873693
2017-12-10T12:53:06.931052: step 2525, loss 0.322617, acc 1, prec 0.0605026, recall 0.873851
2017-12-10T12:53:07.378968: step 2526, loss 0.0630732, acc 1, prec 0.0605298, recall 0.873904
2017-12-10T12:53:07.826657: step 2527, loss 0.606164, acc 0.96875, prec 0.0605806, recall 0.874009
2017-12-10T12:53:08.292352: step 2528, loss 0.0853536, acc 0.953125, prec 0.0606026, recall 0.874062
2017-12-10T12:53:08.745087: step 2529, loss 0.123321, acc 0.953125, prec 0.0605973, recall 0.874062
2017-12-10T12:53:09.202564: step 2530, loss 0.0411122, acc 1, prec 0.0605973, recall 0.874062
2017-12-10T12:53:09.644618: step 2531, loss 1.90661, acc 0.9375, prec 0.0606735, recall 0.873855
2017-12-10T12:53:10.100086: step 2532, loss 0.107556, acc 0.984375, prec 0.0606718, recall 0.873855
2017-12-10T12:53:10.542808: step 2533, loss 0.111261, acc 0.9375, prec 0.0606647, recall 0.873855
2017-12-10T12:53:10.986725: step 2534, loss 0.372647, acc 0.890625, prec 0.0606525, recall 0.873855
2017-12-10T12:53:11.428446: step 2535, loss 0.18985, acc 0.96875, prec 0.0607032, recall 0.87396
2017-12-10T12:53:11.876087: step 2536, loss 0.326632, acc 0.921875, prec 0.0606945, recall 0.87396
2017-12-10T12:53:12.309754: step 2537, loss 0.331247, acc 0.859375, prec 0.0606787, recall 0.87396
2017-12-10T12:53:12.750702: step 2538, loss 1.92334, acc 0.78125, prec 0.060683, recall 0.873649
2017-12-10T12:53:13.182480: step 2539, loss 0.663926, acc 0.921875, prec 0.0607285, recall 0.873754
2017-12-10T12:53:13.624729: step 2540, loss 0.520438, acc 0.859375, prec 0.0607398, recall 0.873807
2017-12-10T12:53:14.074977: step 2541, loss 0.775429, acc 0.796875, prec 0.0607171, recall 0.873807
2017-12-10T12:53:14.525772: step 2542, loss 0.466895, acc 0.859375, prec 0.0607555, recall 0.873911
2017-12-10T12:53:14.974959: step 2543, loss 0.473823, acc 0.859375, prec 0.0607939, recall 0.874016
2017-12-10T12:53:15.418149: step 2544, loss 0.784309, acc 0.84375, prec 0.0608034, recall 0.874068
2017-12-10T12:53:15.847765: step 2545, loss 0.376758, acc 0.84375, prec 0.0607859, recall 0.874068
2017-12-10T12:53:16.283117: step 2546, loss 0.526697, acc 0.828125, prec 0.0607937, recall 0.87412
2017-12-10T12:53:16.725216: step 2547, loss 1.16734, acc 0.875, prec 0.0608338, recall 0.874224
2017-12-10T12:53:17.178476: step 2548, loss 0.95981, acc 0.796875, prec 0.0608651, recall 0.874328
2017-12-10T12:53:17.632170: step 2549, loss 0.660156, acc 0.8125, prec 0.060844, recall 0.874328
2017-12-10T12:53:18.082664: step 2550, loss 0.51988, acc 0.859375, prec 0.0608553, recall 0.87438
2017-12-10T12:53:18.529200: step 2551, loss 0.317579, acc 0.90625, prec 0.0608718, recall 0.874432
2017-12-10T12:53:18.978909: step 2552, loss 0.580623, acc 0.828125, prec 0.0608526, recall 0.874432
2017-12-10T12:53:19.423779: step 2553, loss 0.373702, acc 0.890625, prec 0.0608673, recall 0.874484
2017-12-10T12:53:19.869631: step 2554, loss 0.626401, acc 0.828125, prec 0.0608751, recall 0.874536
2017-12-10T12:53:20.330585: step 2555, loss 0.235161, acc 0.90625, prec 0.0608646, recall 0.874536
2017-12-10T12:53:20.770083: step 2556, loss 0.173625, acc 0.921875, prec 0.0608828, recall 0.874587
2017-12-10T12:53:21.220640: step 2557, loss 0.499548, acc 0.890625, prec 0.0608975, recall 0.874639
2017-12-10T12:53:21.671278: step 2558, loss 0.271858, acc 0.90625, prec 0.060887, recall 0.874639
2017-12-10T12:53:22.122488: step 2559, loss 0.274574, acc 0.890625, prec 0.0608748, recall 0.874639
2017-12-10T12:53:22.564817: step 2560, loss 0.255875, acc 0.953125, prec 0.0608696, recall 0.874639
2017-12-10T12:53:23.012028: step 2561, loss 0.201629, acc 0.921875, prec 0.0608608, recall 0.874639
2017-12-10T12:53:23.457226: step 2562, loss 0.296081, acc 0.921875, prec 0.060879, recall 0.874691
2017-12-10T12:53:23.891996: step 2563, loss 0.146686, acc 0.96875, prec 0.0608756, recall 0.874691
2017-12-10T12:53:24.333794: step 2564, loss 0.186925, acc 0.921875, prec 0.0608938, recall 0.874743
2017-12-10T12:53:24.768572: step 2565, loss 0.0681538, acc 0.984375, prec 0.0609189, recall 0.874794
2017-12-10T12:53:25.216345: step 2566, loss 0.163487, acc 0.96875, prec 0.0609155, recall 0.874794
2017-12-10T12:53:25.661005: step 2567, loss 0.81454, acc 0.9375, prec 0.0609892, recall 0.874949
2017-12-10T12:53:26.113796: step 2568, loss 0.142284, acc 0.96875, prec 0.0610127, recall 0.875
2017-12-10T12:53:26.567178: step 2569, loss 0.205785, acc 0.953125, prec 0.0610074, recall 0.875
2017-12-10T12:53:27.012053: step 2570, loss 0.114956, acc 0.984375, prec 0.0610326, recall 0.875051
2017-12-10T12:53:27.457459: step 2571, loss 0.0870114, acc 0.9375, prec 0.0610256, recall 0.875051
2017-12-10T12:53:27.904651: step 2572, loss 0.0795344, acc 0.984375, prec 0.0610777, recall 0.875154
2017-12-10T12:53:28.348036: step 2573, loss 0.106291, acc 0.9375, prec 0.0610976, recall 0.875205
2017-12-10T12:53:28.793573: step 2574, loss 0.0968824, acc 0.953125, prec 0.0610923, recall 0.875205
2017-12-10T12:53:29.244057: step 2575, loss 0.522625, acc 0.953125, prec 0.061114, recall 0.875256
2017-12-10T12:53:29.693134: step 2576, loss 0.123777, acc 0.9375, prec 0.0611339, recall 0.875308
2017-12-10T12:53:30.139331: step 2577, loss 0.0773022, acc 0.953125, prec 0.0611555, recall 0.875359
2017-12-10T12:53:30.585998: step 2578, loss 0.0965993, acc 0.984375, prec 0.0611806, recall 0.87541
2017-12-10T12:53:31.024397: step 2579, loss 0.0473727, acc 0.984375, prec 0.0612058, recall 0.875461
2017-12-10T12:53:31.484784: step 2580, loss 0.0516165, acc 0.96875, prec 0.0612023, recall 0.875461
2017-12-10T12:53:31.933048: step 2581, loss 0.0394597, acc 0.984375, prec 0.0612274, recall 0.875512
2017-12-10T12:53:32.384126: step 2582, loss 0.085477, acc 0.96875, prec 0.0612508, recall 0.875563
2017-12-10T12:53:32.842535: step 2583, loss 0.0386057, acc 0.984375, prec 0.061249, recall 0.875563
2017-12-10T12:53:33.293523: step 2584, loss 0.0293415, acc 1, prec 0.0612759, recall 0.875614
2017-12-10T12:53:33.729243: step 2585, loss 4.72611, acc 0.96875, prec 0.0612742, recall 0.875256
2017-12-10T12:53:34.178775: step 2586, loss 0.270028, acc 0.890625, prec 0.0612888, recall 0.875307
2017-12-10T12:53:34.636256: step 2587, loss 0.261847, acc 0.96875, prec 0.061339, recall 0.875408
2017-12-10T12:53:35.076567: step 2588, loss 0.329008, acc 0.921875, prec 0.0613302, recall 0.875408
2017-12-10T12:53:35.514071: step 2589, loss 0.0861812, acc 0.953125, prec 0.0613249, recall 0.875408
2017-12-10T12:53:35.972879: step 2590, loss 0.128207, acc 0.953125, prec 0.0613465, recall 0.875459
2017-12-10T12:53:36.414625: step 2591, loss 0.113953, acc 0.984375, prec 0.0613716, recall 0.87551
2017-12-10T12:53:36.864272: step 2592, loss 0.0800517, acc 0.96875, prec 0.0613681, recall 0.87551
2017-12-10T12:53:37.308400: step 2593, loss 0.0935062, acc 0.96875, prec 0.0613915, recall 0.875561
2017-12-10T12:53:37.750184: step 2594, loss 0.271805, acc 0.875, prec 0.0614579, recall 0.875713
2017-12-10T12:53:38.192775: step 2595, loss 0.397843, acc 0.90625, prec 0.0614474, recall 0.875713
2017-12-10T12:53:38.630610: step 2596, loss 0.0944484, acc 0.96875, prec 0.0614976, recall 0.875814
2017-12-10T12:53:39.073507: step 2597, loss 0.146408, acc 0.96875, prec 0.0615745, recall 0.875966
2017-12-10T12:53:39.525117: step 2598, loss 0.144461, acc 0.953125, prec 0.0615961, recall 0.876016
2017-12-10T12:53:39.963818: step 2599, loss 0.192653, acc 0.90625, prec 0.0616123, recall 0.876067
2017-12-10T12:53:40.401493: step 2600, loss 0.10551, acc 0.96875, prec 0.0616356, recall 0.876117
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2600

2017-12-10T12:53:42.325893: step 2601, loss 0.685438, acc 0.890625, prec 0.0616769, recall 0.876218
2017-12-10T12:53:42.757102: step 2602, loss 0.233992, acc 0.9375, prec 0.0616699, recall 0.876218
2017-12-10T12:53:43.203244: step 2603, loss 0.122786, acc 0.953125, prec 0.0617182, recall 0.876318
2017-12-10T12:53:43.641069: step 2604, loss 0.233002, acc 0.9375, prec 0.0617379, recall 0.876368
2017-12-10T12:53:44.085101: step 2605, loss 0.346661, acc 0.90625, prec 0.0617541, recall 0.876418
2017-12-10T12:53:44.554341: step 2606, loss 0.482917, acc 0.859375, prec 0.061765, recall 0.876468
2017-12-10T12:53:45.011763: step 2607, loss 0.127408, acc 0.9375, prec 0.0617848, recall 0.876518
2017-12-10T12:53:45.460229: step 2608, loss 0.1946, acc 0.90625, prec 0.0617742, recall 0.876518
2017-12-10T12:53:45.908508: step 2609, loss 0.266311, acc 0.90625, prec 0.0617636, recall 0.876518
2017-12-10T12:53:46.365386: step 2610, loss 0.0698075, acc 0.984375, prec 0.0617886, recall 0.876568
2017-12-10T12:53:46.801384: step 2611, loss 1.16139, acc 0.9375, prec 0.0618083, recall 0.876618
2017-12-10T12:53:47.254768: step 2612, loss 0.203294, acc 0.9375, prec 0.061828, recall 0.876668
2017-12-10T12:53:47.696298: step 2613, loss 0.110132, acc 0.953125, prec 0.0618495, recall 0.876718
2017-12-10T12:53:48.129133: step 2614, loss 0.258481, acc 0.9375, prec 0.0618959, recall 0.876817
2017-12-10T12:53:48.567194: step 2615, loss 0.0989273, acc 0.953125, prec 0.0619174, recall 0.876867
2017-12-10T12:53:49.016820: step 2616, loss 0.103525, acc 0.96875, prec 0.0619139, recall 0.876867
2017-12-10T12:53:49.462143: step 2617, loss 0.142217, acc 0.953125, prec 0.061962, recall 0.876967
2017-12-10T12:53:49.908407: step 2618, loss 0.0478408, acc 0.984375, prec 0.061987, recall 0.877016
2017-12-10T12:53:50.355883: step 2619, loss 0.172648, acc 0.9375, prec 0.0619799, recall 0.877016
2017-12-10T12:53:50.805463: step 2620, loss 0.345364, acc 0.953125, prec 0.0619746, recall 0.877016
2017-12-10T12:53:51.247727: step 2621, loss 0.355845, acc 0.9375, prec 0.062021, recall 0.877115
2017-12-10T12:53:51.697092: step 2622, loss 0.153907, acc 0.9375, prec 0.062014, recall 0.877115
2017-12-10T12:53:52.145505: step 2623, loss 0.177195, acc 0.953125, prec 0.0620621, recall 0.877214
2017-12-10T12:53:52.581423: step 2624, loss 0.300124, acc 0.921875, prec 0.06208, recall 0.877264
2017-12-10T12:53:53.026553: step 2625, loss 0.209632, acc 0.90625, prec 0.0620961, recall 0.877313
2017-12-10T12:53:53.473877: step 2626, loss 0.072734, acc 0.96875, prec 0.0621459, recall 0.877412
2017-12-10T12:53:53.920215: step 2627, loss 0.643159, acc 0.953125, prec 0.062194, recall 0.87751
2017-12-10T12:53:54.354779: step 2628, loss 0.109659, acc 0.96875, prec 0.0621905, recall 0.87751
2017-12-10T12:53:54.785293: step 2629, loss 0.18207, acc 0.953125, prec 0.0622652, recall 0.877657
2017-12-10T12:53:55.223239: step 2630, loss 0.134776, acc 0.96875, prec 0.0622617, recall 0.877657
2017-12-10T12:53:55.660359: step 2631, loss 0.065037, acc 0.96875, prec 0.0623115, recall 0.877756
2017-12-10T12:53:56.109994: step 2632, loss 0.941105, acc 0.96875, prec 0.0623346, recall 0.877804
2017-12-10T12:53:56.555654: step 2633, loss 0.358879, acc 0.953125, prec 0.0623293, recall 0.877804
2017-12-10T12:53:56.999062: step 2634, loss 0.127568, acc 0.953125, prec 0.062324, recall 0.877804
2017-12-10T12:53:57.449405: step 2635, loss 0.0824714, acc 0.984375, prec 0.0623222, recall 0.877804
2017-12-10T12:53:57.886699: step 2636, loss 0.0962101, acc 0.96875, prec 0.0623187, recall 0.877804
2017-12-10T12:53:58.339982: step 2637, loss 0.0650598, acc 0.96875, prec 0.0623151, recall 0.877804
2017-12-10T12:53:58.783335: step 2638, loss 0.129857, acc 0.9375, prec 0.062308, recall 0.877804
2017-12-10T12:53:59.222498: step 2639, loss 0.094609, acc 0.96875, prec 0.0623578, recall 0.877902
2017-12-10T12:53:59.677600: step 2640, loss 0.794313, acc 0.921875, prec 0.0624023, recall 0.878
2017-12-10T12:54:00.129628: step 2641, loss 0.0445154, acc 0.984375, prec 0.0624538, recall 0.878098
2017-12-10T12:54:00.573676: step 2642, loss 1.35008, acc 0.828125, prec 0.0624876, recall 0.878195
2017-12-10T12:54:01.020742: step 2643, loss 0.170665, acc 0.96875, prec 0.062484, recall 0.878195
2017-12-10T12:54:01.466740: step 2644, loss 0.286279, acc 0.921875, prec 0.0624751, recall 0.878195
2017-12-10T12:54:01.924352: step 2645, loss 0.10863, acc 0.953125, prec 0.0625231, recall 0.878292
2017-12-10T12:54:02.365486: step 2646, loss 0.162326, acc 0.96875, prec 0.0625728, recall 0.878389
2017-12-10T12:54:02.799940: step 2647, loss 3.00051, acc 0.921875, prec 0.0625923, recall 0.878088
2017-12-10T12:54:03.246452: step 2648, loss 0.0678755, acc 0.96875, prec 0.0625887, recall 0.878088
2017-12-10T12:54:03.696805: step 2649, loss 0.184103, acc 0.9375, prec 0.0626082, recall 0.878136
2017-12-10T12:54:04.146026: step 2650, loss 0.65783, acc 0.859375, prec 0.0625923, recall 0.878136
2017-12-10T12:54:04.592941: step 2651, loss 0.202444, acc 0.921875, prec 0.06261, recall 0.878185
2017-12-10T12:54:05.050151: step 2652, loss 0.533434, acc 0.90625, prec 0.0626525, recall 0.878282
2017-12-10T12:54:05.495246: step 2653, loss 0.926336, acc 0.8125, prec 0.0626312, recall 0.878282
2017-12-10T12:54:05.943074: step 2654, loss 0.260116, acc 0.921875, prec 0.0626489, recall 0.87833
2017-12-10T12:54:06.386961: step 2655, loss 0.808445, acc 0.828125, prec 0.0626294, recall 0.87833
2017-12-10T12:54:06.841244: step 2656, loss 0.761401, acc 0.8125, prec 0.0626346, recall 0.878378
2017-12-10T12:54:07.281241: step 2657, loss 0.355795, acc 0.890625, prec 0.0626488, recall 0.878427
2017-12-10T12:54:07.719388: step 2658, loss 0.354696, acc 0.90625, prec 0.0626647, recall 0.878475
2017-12-10T12:54:08.162115: step 2659, loss 0.204606, acc 0.921875, prec 0.0627089, recall 0.878571
2017-12-10T12:54:08.603344: step 2660, loss 0.314949, acc 0.859375, prec 0.0626929, recall 0.878571
2017-12-10T12:54:09.049572: step 2661, loss 0.387001, acc 0.890625, prec 0.062707, recall 0.87862
2017-12-10T12:54:09.489477: step 2662, loss 0.549441, acc 0.90625, prec 0.0627494, recall 0.878716
2017-12-10T12:54:09.939196: step 2663, loss 0.280196, acc 0.875, prec 0.0627617, recall 0.878764
2017-12-10T12:54:10.392550: step 2664, loss 0.167915, acc 0.953125, prec 0.0627564, recall 0.878764
2017-12-10T12:54:10.834004: step 2665, loss 1.42284, acc 0.921875, prec 0.0628006, recall 0.87886
2017-12-10T12:54:11.280806: step 2666, loss 0.298011, acc 0.921875, prec 0.0628447, recall 0.878956
2017-12-10T12:54:11.718248: step 2667, loss 0.401021, acc 0.90625, prec 0.062834, recall 0.878956
2017-12-10T12:54:12.165594: step 2668, loss 0.27282, acc 0.890625, prec 0.0628216, recall 0.878956
2017-12-10T12:54:12.609494: step 2669, loss 0.533437, acc 0.921875, prec 0.0628657, recall 0.879051
2017-12-10T12:54:13.068073: step 2670, loss 0.382194, acc 0.90625, prec 0.062855, recall 0.879051
2017-12-10T12:54:13.516244: step 2671, loss 0.191585, acc 0.9375, prec 0.0628479, recall 0.879051
2017-12-10T12:54:13.965371: step 2672, loss 0.149537, acc 0.953125, prec 0.0628691, recall 0.879099
2017-12-10T12:54:14.422672: step 2673, loss 0.237005, acc 0.921875, prec 0.0628867, recall 0.879147
2017-12-10T12:54:14.870094: step 2674, loss 0.286676, acc 0.921875, prec 0.0628778, recall 0.879147
2017-12-10T12:54:15.307687: step 2675, loss 0.264796, acc 0.921875, prec 0.0628954, recall 0.879195
2017-12-10T12:54:15.750295: step 2676, loss 0.408821, acc 0.90625, prec 0.0629377, recall 0.87929
2017-12-10T12:54:16.188263: step 2677, loss 0.265061, acc 0.9375, prec 0.062957, recall 0.879338
2017-12-10T12:54:16.631812: step 2678, loss 0.170059, acc 0.984375, prec 0.0629552, recall 0.879338
2017-12-10T12:54:17.085549: step 2679, loss 0.17589, acc 0.953125, prec 0.0629499, recall 0.879338
2017-12-10T12:54:17.523449: step 2680, loss 0.0621773, acc 0.984375, prec 0.0629746, recall 0.879385
2017-12-10T12:54:17.964990: step 2681, loss 0.452043, acc 0.9375, prec 0.0629939, recall 0.879433
2017-12-10T12:54:18.410108: step 2682, loss 0.0627636, acc 0.953125, prec 0.0629886, recall 0.879433
2017-12-10T12:54:18.858196: step 2683, loss 0.0436864, acc 0.96875, prec 0.062985, recall 0.879433
2017-12-10T12:54:19.311217: step 2684, loss 0.0942786, acc 0.96875, prec 0.0630079, recall 0.87948
2017-12-10T12:54:19.766838: step 2685, loss 0.14601, acc 0.984375, prec 0.063059, recall 0.879575
2017-12-10T12:54:20.199645: step 2686, loss 0.108312, acc 0.953125, prec 0.0630537, recall 0.879575
2017-12-10T12:54:20.645530: step 2687, loss 4.35261, acc 0.96875, prec 0.0630783, recall 0.879276
2017-12-10T12:54:21.127807: step 2688, loss 0.133187, acc 0.953125, prec 0.063073, recall 0.879276
2017-12-10T12:54:21.581553: step 2689, loss 0.222341, acc 0.953125, prec 0.0630941, recall 0.879324
2017-12-10T12:54:22.030780: step 2690, loss 0.177534, acc 0.96875, prec 0.0630905, recall 0.879324
2017-12-10T12:54:22.479105: step 2691, loss 0.0801423, acc 0.96875, prec 0.0631134, recall 0.879371
2017-12-10T12:54:22.920983: step 2692, loss 0.196731, acc 0.9375, prec 0.0631591, recall 0.879466
2017-12-10T12:54:23.373152: step 2693, loss 0.661246, acc 0.9375, prec 0.0632048, recall 0.879561
2017-12-10T12:54:23.820948: step 2694, loss 0.122853, acc 0.9375, prec 0.0631977, recall 0.879561
2017-12-10T12:54:24.261529: step 2695, loss 0.185157, acc 0.9375, prec 0.0632697, recall 0.879702
2017-12-10T12:54:24.714124: step 2696, loss 0.272663, acc 0.921875, prec 0.0632872, recall 0.879749
2017-12-10T12:54:25.155717: step 2697, loss 0.408526, acc 0.90625, prec 0.0633293, recall 0.879843
2017-12-10T12:54:25.591887: step 2698, loss 0.0731473, acc 0.96875, prec 0.0633521, recall 0.87989
2017-12-10T12:54:26.029628: step 2699, loss 0.128835, acc 0.9375, prec 0.063345, recall 0.87989
2017-12-10T12:54:26.481005: step 2700, loss 0.181352, acc 0.9375, prec 0.0633378, recall 0.87989
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2700

2017-12-10T12:54:28.263380: step 2701, loss 0.559591, acc 0.890625, prec 0.0633781, recall 0.879984
2017-12-10T12:54:28.713130: step 2702, loss 0.527363, acc 0.84375, prec 0.0633866, recall 0.880031
2017-12-10T12:54:29.157419: step 2703, loss 0.734366, acc 0.859375, prec 0.0633706, recall 0.880031
2017-12-10T12:54:29.586477: step 2704, loss 0.641558, acc 0.84375, prec 0.0634318, recall 0.880172
2017-12-10T12:54:30.025356: step 2705, loss 0.477242, acc 0.84375, prec 0.0634139, recall 0.880172
2017-12-10T12:54:30.463321: step 2706, loss 0.305613, acc 0.890625, prec 0.0634015, recall 0.880172
2017-12-10T12:54:30.902893: step 2707, loss 0.106707, acc 0.96875, prec 0.0633979, recall 0.880172
2017-12-10T12:54:31.343091: step 2708, loss 0.793314, acc 0.921875, prec 0.0634416, recall 0.880265
2017-12-10T12:54:31.786320: step 2709, loss 0.474009, acc 0.875, prec 0.0634537, recall 0.880312
2017-12-10T12:54:32.232049: step 2710, loss 0.192665, acc 0.90625, prec 0.063443, recall 0.880312
2017-12-10T12:54:32.670198: step 2711, loss 0.203958, acc 0.890625, prec 0.0634305, recall 0.880312
2017-12-10T12:54:33.117711: step 2712, loss 0.32229, acc 0.890625, prec 0.0634444, recall 0.880359
2017-12-10T12:54:33.556861: step 2713, loss 0.209806, acc 0.9375, prec 0.0634635, recall 0.880405
2017-12-10T12:54:34.001207: step 2714, loss 0.120938, acc 0.921875, prec 0.0634809, recall 0.880452
2017-12-10T12:54:34.442669: step 2715, loss 0.12819, acc 0.984375, prec 0.0635054, recall 0.880498
2017-12-10T12:54:34.895801: step 2716, loss 0.261963, acc 0.953125, prec 0.0635001, recall 0.880498
2017-12-10T12:54:35.340521: step 2717, loss 0.127864, acc 0.953125, prec 0.063521, recall 0.880545
2017-12-10T12:54:35.777749: step 2718, loss 0.194071, acc 0.953125, prec 0.0635157, recall 0.880545
2017-12-10T12:54:36.216154: step 2719, loss 0.427173, acc 0.890625, prec 0.0635558, recall 0.880638
2017-12-10T12:54:36.670884: step 2720, loss 0.225635, acc 0.890625, prec 0.0635433, recall 0.880638
2017-12-10T12:54:37.115118: step 2721, loss 0.0519115, acc 0.96875, prec 0.063566, recall 0.880684
2017-12-10T12:54:37.548004: step 2722, loss 0.044435, acc 0.96875, prec 0.0635624, recall 0.880684
2017-12-10T12:54:37.986727: step 2723, loss 0.0814663, acc 0.984375, prec 0.0635606, recall 0.880684
2017-12-10T12:54:38.438482: step 2724, loss 0.401534, acc 0.921875, prec 0.0636042, recall 0.880777
2017-12-10T12:54:38.890940: step 2725, loss 0.0820564, acc 0.9375, prec 0.0635971, recall 0.880777
2017-12-10T12:54:39.335401: step 2726, loss 0.0697485, acc 0.984375, prec 0.0636216, recall 0.880823
2017-12-10T12:54:39.771008: step 2727, loss 0.0401081, acc 0.984375, prec 0.0636198, recall 0.880823
2017-12-10T12:54:40.207519: step 2728, loss 0.759669, acc 0.984375, prec 0.0636705, recall 0.880915
2017-12-10T12:54:40.668528: step 2729, loss 0.115877, acc 0.953125, prec 0.0636914, recall 0.880962
2017-12-10T12:54:41.122807: step 2730, loss 1.44667, acc 0.9375, prec 0.0637892, recall 0.881146
2017-12-10T12:54:41.572225: step 2731, loss 0.125776, acc 0.984375, prec 0.0638399, recall 0.881238
2017-12-10T12:54:42.021499: step 2732, loss 0.0876735, acc 0.953125, prec 0.063887, recall 0.88133
2017-12-10T12:54:42.460281: step 2733, loss 0.0643316, acc 0.953125, prec 0.0639079, recall 0.881376
2017-12-10T12:54:42.899164: step 2734, loss 0.046771, acc 0.984375, prec 0.0639061, recall 0.881376
2017-12-10T12:54:43.352760: step 2735, loss 0.260034, acc 0.953125, prec 0.0639532, recall 0.881467
2017-12-10T12:54:43.799037: step 2736, loss 0.261909, acc 0.9375, prec 0.0640246, recall 0.881604
2017-12-10T12:54:44.241853: step 2737, loss 0.295023, acc 0.953125, prec 0.0640717, recall 0.881696
2017-12-10T12:54:44.693738: step 2738, loss 0.151283, acc 0.9375, prec 0.0640907, recall 0.881741
2017-12-10T12:54:45.145757: step 2739, loss 0.140778, acc 0.953125, prec 0.0641115, recall 0.881787
2017-12-10T12:54:45.585408: step 2740, loss 0.289625, acc 0.921875, prec 0.0641026, recall 0.881787
2017-12-10T12:54:46.035541: step 2741, loss 0.366221, acc 0.890625, prec 0.0641162, recall 0.881832
2017-12-10T12:54:46.482419: step 2742, loss 0.448278, acc 0.875, prec 0.0641018, recall 0.881832
2017-12-10T12:54:46.923917: step 2743, loss 0.135359, acc 0.953125, prec 0.0641226, recall 0.881878
2017-12-10T12:54:47.355875: step 2744, loss 0.355394, acc 0.90625, prec 0.0641119, recall 0.881878
2017-12-10T12:54:47.816791: step 2745, loss 0.198829, acc 0.921875, prec 0.0641291, recall 0.881923
2017-12-10T12:54:48.254867: step 2746, loss 0.522844, acc 0.984375, prec 0.0641796, recall 0.882014
2017-12-10T12:54:48.711572: step 2747, loss 0.156374, acc 0.953125, prec 0.0641743, recall 0.882014
2017-12-10T12:54:49.151264: step 2748, loss 0.304784, acc 0.90625, prec 0.0641897, recall 0.882059
2017-12-10T12:54:49.596515: step 2749, loss 0.22322, acc 0.953125, prec 0.0642104, recall 0.882104
2017-12-10T12:54:50.036046: step 2750, loss 5.64352, acc 0.953125, prec 0.0642068, recall 0.881766
2017-12-10T12:54:50.490858: step 2751, loss 0.174723, acc 0.9375, prec 0.0642258, recall 0.881811
2017-12-10T12:54:50.923558: step 2752, loss 0.224164, acc 0.9375, prec 0.0642186, recall 0.881811
2017-12-10T12:54:51.366217: step 2753, loss 0.250005, acc 0.921875, prec 0.0642358, recall 0.881857
2017-12-10T12:54:51.797908: step 2754, loss 0.719307, acc 0.875, prec 0.0642476, recall 0.881902
2017-12-10T12:54:52.243899: step 2755, loss 0.4238, acc 0.84375, prec 0.0642297, recall 0.881902
2017-12-10T12:54:52.690128: step 2756, loss 0.648809, acc 0.8125, prec 0.0642343, recall 0.881947
2017-12-10T12:54:53.129263: step 2757, loss 0.20109, acc 0.90625, prec 0.0642235, recall 0.881947
2017-12-10T12:54:53.567995: step 2758, loss 0.579464, acc 0.84375, prec 0.0642317, recall 0.881992
2017-12-10T12:54:54.002468: step 2759, loss 0.462273, acc 0.859375, prec 0.0642156, recall 0.881992
2017-12-10T12:54:54.439151: step 2760, loss 0.184746, acc 0.953125, prec 0.0642102, recall 0.881992
2017-12-10T12:54:54.882013: step 2761, loss 0.408223, acc 0.859375, prec 0.0642202, recall 0.882038
2017-12-10T12:54:55.331832: step 2762, loss 0.38481, acc 0.84375, prec 0.0642023, recall 0.882038
2017-12-10T12:54:55.773005: step 2763, loss 0.182227, acc 0.921875, prec 0.0642194, recall 0.882083
2017-12-10T12:54:56.220767: step 2764, loss 0.320883, acc 0.875, prec 0.0642312, recall 0.882128
2017-12-10T12:54:56.664844: step 2765, loss 0.300588, acc 0.921875, prec 0.0642483, recall 0.882173
2017-12-10T12:54:57.101727: step 2766, loss 0.353175, acc 0.859375, prec 0.0642322, recall 0.882173
2017-12-10T12:54:57.554907: step 2767, loss 0.208857, acc 0.921875, prec 0.0642754, recall 0.882263
2017-12-10T12:54:57.996735: step 2768, loss 0.919309, acc 0.953125, prec 0.0643221, recall 0.882353
2017-12-10T12:54:58.444578: step 2769, loss 0.284208, acc 0.921875, prec 0.0643392, recall 0.882398
2017-12-10T12:54:58.889605: step 2770, loss 0.235399, acc 0.9375, prec 0.0643581, recall 0.882443
2017-12-10T12:54:59.331085: step 2771, loss 0.273331, acc 0.9375, prec 0.064377, recall 0.882488
2017-12-10T12:54:59.784105: step 2772, loss 0.158194, acc 0.921875, prec 0.0644201, recall 0.882577
2017-12-10T12:55:00.220123: step 2773, loss 0.183762, acc 0.96875, prec 0.0644425, recall 0.882622
2017-12-10T12:55:00.662554: step 2774, loss 1.45784, acc 0.875, prec 0.06443, recall 0.882286
2017-12-10T12:55:01.104901: step 2775, loss 0.11669, acc 0.984375, prec 0.0644542, recall 0.882331
2017-12-10T12:55:01.559377: step 2776, loss 0.334152, acc 0.90625, prec 0.0645215, recall 0.882465
2017-12-10T12:55:02.009551: step 2777, loss 0.264644, acc 0.953125, prec 0.0645421, recall 0.88251
2017-12-10T12:55:02.464302: step 2778, loss 0.276337, acc 0.921875, prec 0.0645332, recall 0.88251
2017-12-10T12:55:02.913246: step 2779, loss 0.122225, acc 0.953125, prec 0.0645278, recall 0.88251
2017-12-10T12:55:03.365830: step 2780, loss 0.25951, acc 0.921875, prec 0.0645448, recall 0.882554
2017-12-10T12:55:03.801632: step 2781, loss 0.163515, acc 0.96875, prec 0.0645412, recall 0.882554
2017-12-10T12:55:04.239456: step 2782, loss 0.396822, acc 0.859375, prec 0.0645251, recall 0.882554
2017-12-10T12:55:04.683262: step 2783, loss 0.165229, acc 0.9375, prec 0.0645179, recall 0.882554
2017-12-10T12:55:05.131933: step 2784, loss 0.122358, acc 0.921875, prec 0.0645349, recall 0.882599
2017-12-10T12:55:05.584903: step 2785, loss 0.0296303, acc 1, prec 0.0645349, recall 0.882599
2017-12-10T12:55:06.037818: step 2786, loss 0.379587, acc 0.90625, prec 0.0645502, recall 0.882643
2017-12-10T12:55:06.491547: step 2787, loss 0.0186015, acc 1, prec 0.0645502, recall 0.882643
2017-12-10T12:55:06.934665: step 2788, loss 0.264262, acc 0.953125, prec 0.0645708, recall 0.882688
2017-12-10T12:55:07.387859: step 2789, loss 0.862794, acc 0.984375, prec 0.064595, recall 0.882732
2017-12-10T12:55:07.834947: step 2790, loss 0.168554, acc 0.9375, prec 0.0645878, recall 0.882732
2017-12-10T12:55:08.278832: step 2791, loss 0.0951114, acc 0.984375, prec 0.0646379, recall 0.882821
2017-12-10T12:55:08.720975: step 2792, loss 0.205351, acc 0.9375, prec 0.0646308, recall 0.882821
2017-12-10T12:55:09.160852: step 2793, loss 4.30174, acc 0.9375, prec 0.0646254, recall 0.882487
2017-12-10T12:55:09.616967: step 2794, loss 0.273373, acc 0.84375, prec 0.0646334, recall 0.882531
2017-12-10T12:55:10.058959: step 2795, loss 0.132275, acc 0.953125, prec 0.064628, recall 0.882531
2017-12-10T12:55:10.524634: step 2796, loss 0.20478, acc 0.9375, prec 0.0646728, recall 0.88262
2017-12-10T12:55:10.967247: step 2797, loss 0.283488, acc 0.921875, prec 0.0646638, recall 0.88262
2017-12-10T12:55:11.405841: step 2798, loss 0.234897, acc 0.953125, prec 0.0646843, recall 0.882665
2017-12-10T12:55:11.860741: step 2799, loss 0.600279, acc 0.796875, prec 0.0647129, recall 0.882753
2017-12-10T12:55:12.299193: step 2800, loss 0.622568, acc 0.828125, prec 0.0647191, recall 0.882798
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2800

2017-12-10T12:55:14.291716: step 2801, loss 0.399033, acc 0.875, prec 0.0647307, recall 0.882842
2017-12-10T12:55:14.732590: step 2802, loss 0.171485, acc 0.890625, prec 0.0647181, recall 0.882842
2017-12-10T12:55:15.171488: step 2803, loss 1.32324, acc 0.8125, prec 0.0647225, recall 0.882886
2017-12-10T12:55:15.622799: step 2804, loss 0.429347, acc 0.921875, prec 0.0647394, recall 0.882931
2017-12-10T12:55:16.066251: step 2805, loss 0.491572, acc 0.875, prec 0.0647251, recall 0.882931
2017-12-10T12:55:16.504305: step 2806, loss 1.67829, acc 0.84375, prec 0.0647331, recall 0.882975
2017-12-10T12:55:16.952531: step 2807, loss 0.448777, acc 0.859375, prec 0.0647687, recall 0.883063
2017-12-10T12:55:17.388374: step 2808, loss 0.39851, acc 0.875, prec 0.0647544, recall 0.883063
2017-12-10T12:55:17.823411: step 2809, loss 0.462603, acc 0.890625, prec 0.0647936, recall 0.883151
2017-12-10T12:55:18.258625: step 2810, loss 0.599599, acc 0.828125, prec 0.0647997, recall 0.883195
2017-12-10T12:55:18.705723: step 2811, loss 1.06179, acc 0.78125, prec 0.0648263, recall 0.883283
2017-12-10T12:55:19.156733: step 2812, loss 0.332998, acc 0.890625, prec 0.0648138, recall 0.883283
2017-12-10T12:55:19.617925: step 2813, loss 0.34306, acc 0.9375, prec 0.0648066, recall 0.883283
2017-12-10T12:55:20.058951: step 2814, loss 0.226236, acc 0.890625, prec 0.0647941, recall 0.883283
2017-12-10T12:55:20.511724: step 2815, loss 0.376446, acc 0.875, prec 0.0648056, recall 0.883327
2017-12-10T12:55:20.947555: step 2816, loss 0.186254, acc 0.90625, prec 0.0648207, recall 0.883371
2017-12-10T12:55:21.396468: step 2817, loss 0.600595, acc 0.90625, prec 0.0648358, recall 0.883415
2017-12-10T12:55:21.835070: step 2818, loss 0.468197, acc 0.875, prec 0.0648215, recall 0.883415
2017-12-10T12:55:22.281507: step 2819, loss 0.455886, acc 0.890625, prec 0.0648863, recall 0.883546
2017-12-10T12:55:22.722834: step 2820, loss 0.339944, acc 0.890625, prec 0.0648738, recall 0.883546
2017-12-10T12:55:23.151844: step 2821, loss 0.204486, acc 0.9375, prec 0.0648924, recall 0.88359
2017-12-10T12:55:23.590141: step 2822, loss 0.066841, acc 0.984375, prec 0.0649164, recall 0.883634
2017-12-10T12:55:24.028132: step 2823, loss 0.118644, acc 0.953125, prec 0.0649626, recall 0.883721
2017-12-10T12:55:24.466681: step 2824, loss 0.130356, acc 0.96875, prec 0.0649848, recall 0.883765
2017-12-10T12:55:24.921418: step 2825, loss 0.156347, acc 0.96875, prec 0.065007, recall 0.883808
2017-12-10T12:55:25.365410: step 2826, loss 0.181857, acc 0.90625, prec 0.0649963, recall 0.883808
2017-12-10T12:55:25.807892: step 2827, loss 0.29491, acc 0.9375, prec 0.0650149, recall 0.883852
2017-12-10T12:55:26.246179: step 2828, loss 0.477675, acc 0.953125, prec 0.065061, recall 0.883939
2017-12-10T12:55:26.689972: step 2829, loss 0.176181, acc 0.921875, prec 0.0651036, recall 0.884025
2017-12-10T12:55:27.132113: step 2830, loss 0.0689609, acc 0.96875, prec 0.0651258, recall 0.884069
2017-12-10T12:55:27.575638: step 2831, loss 0.201965, acc 0.953125, prec 0.0651204, recall 0.884069
2017-12-10T12:55:28.024386: step 2832, loss 0.549454, acc 0.953125, prec 0.0651407, recall 0.884112
2017-12-10T12:55:28.484593: step 2833, loss 0.155389, acc 0.921875, prec 0.0651318, recall 0.884112
2017-12-10T12:55:28.948081: step 2834, loss 0.119411, acc 0.953125, prec 0.0651521, recall 0.884155
2017-12-10T12:55:29.386602: step 2835, loss 0.0580171, acc 0.96875, prec 0.0651486, recall 0.884155
2017-12-10T12:55:29.831817: step 2836, loss 0.221038, acc 0.9375, prec 0.0651414, recall 0.884155
2017-12-10T12:55:30.267202: step 2837, loss 0.0777279, acc 0.96875, prec 0.0651378, recall 0.884155
2017-12-10T12:55:30.720176: step 2838, loss 0.164479, acc 0.96875, prec 0.0651857, recall 0.884242
2017-12-10T12:55:31.173841: step 2839, loss 0.112939, acc 0.953125, prec 0.0652317, recall 0.884328
2017-12-10T12:55:31.623052: step 2840, loss 0.0552608, acc 0.96875, prec 0.0652282, recall 0.884328
2017-12-10T12:55:32.067745: step 2841, loss 0.21954, acc 0.9375, prec 0.0652467, recall 0.884372
2017-12-10T12:55:32.512090: step 2842, loss 3.51278, acc 0.96875, prec 0.0652449, recall 0.884042
2017-12-10T12:55:32.970474: step 2843, loss 2.17604, acc 0.9375, prec 0.065291, recall 0.883799
2017-12-10T12:55:33.420765: step 2844, loss 0.109568, acc 0.953125, prec 0.0652856, recall 0.883799
2017-12-10T12:55:33.866152: step 2845, loss 0.115037, acc 0.921875, prec 0.0652766, recall 0.883799
2017-12-10T12:55:34.316297: step 2846, loss 0.0947836, acc 0.953125, prec 0.0652969, recall 0.883842
2017-12-10T12:55:34.767742: step 2847, loss 0.455334, acc 0.890625, prec 0.0652843, recall 0.883842
2017-12-10T12:55:35.215705: step 2848, loss 0.553489, acc 0.890625, prec 0.0653489, recall 0.883972
2017-12-10T12:55:35.670269: step 2849, loss 0.344661, acc 0.828125, prec 0.0653805, recall 0.884058
2017-12-10T12:55:36.114308: step 2850, loss 0.449579, acc 0.890625, prec 0.0653936, recall 0.884101
2017-12-10T12:55:36.567120: step 2851, loss 0.223772, acc 0.921875, prec 0.0653846, recall 0.884101
2017-12-10T12:55:37.008157: step 2852, loss 0.574213, acc 0.84375, prec 0.0653923, recall 0.884144
2017-12-10T12:55:37.452436: step 2853, loss 0.384904, acc 0.890625, prec 0.0653798, recall 0.884144
2017-12-10T12:55:37.897366: step 2854, loss 0.741904, acc 0.8125, prec 0.0654095, recall 0.88423
2017-12-10T12:55:38.348241: step 2855, loss 0.684549, acc 0.8125, prec 0.065388, recall 0.88423
2017-12-10T12:55:38.806292: step 2856, loss 0.480528, acc 0.8125, prec 0.0654177, recall 0.884316
2017-12-10T12:55:39.254143: step 2857, loss 0.565673, acc 0.890625, prec 0.0654308, recall 0.884359
2017-12-10T12:55:39.690433: step 2858, loss 0.546486, acc 0.890625, prec 0.0654951, recall 0.884487
2017-12-10T12:55:40.136700: step 2859, loss 0.305131, acc 0.90625, prec 0.06551, recall 0.88453
2017-12-10T12:55:40.578785: step 2860, loss 0.113704, acc 0.9375, prec 0.0655284, recall 0.884573
2017-12-10T12:55:41.023073: step 2861, loss 0.389658, acc 0.890625, prec 0.0655414, recall 0.884615
2017-12-10T12:55:41.480137: step 2862, loss 0.171304, acc 0.96875, prec 0.0655634, recall 0.884658
2017-12-10T12:55:41.907140: step 2863, loss 0.399001, acc 0.875, prec 0.0655747, recall 0.884701
2017-12-10T12:55:42.351679: step 2864, loss 0.247512, acc 0.90625, prec 0.0656407, recall 0.884828
2017-12-10T12:55:42.791798: step 2865, loss 0.197311, acc 0.9375, prec 0.0656846, recall 0.884913
2017-12-10T12:55:43.240824: step 2866, loss 0.125085, acc 0.984375, prec 0.0657596, recall 0.885041
2017-12-10T12:55:43.692443: step 2867, loss 0.372131, acc 0.90625, prec 0.0657488, recall 0.885041
2017-12-10T12:55:44.154425: step 2868, loss 0.067996, acc 0.96875, prec 0.0658219, recall 0.885167
2017-12-10T12:55:44.588626: step 2869, loss 0.114156, acc 0.96875, prec 0.0658183, recall 0.885167
2017-12-10T12:55:45.038643: step 2870, loss 0.289631, acc 0.96875, prec 0.0658147, recall 0.885167
2017-12-10T12:55:45.483744: step 2871, loss 0.187284, acc 0.921875, prec 0.0658312, recall 0.88521
2017-12-10T12:55:45.928105: step 2872, loss 0.737385, acc 0.921875, prec 0.0658478, recall 0.885252
2017-12-10T12:55:46.376135: step 2873, loss 0.12224, acc 0.953125, prec 0.0658424, recall 0.885252
2017-12-10T12:55:46.830748: step 2874, loss 0.407818, acc 0.9375, prec 0.0658863, recall 0.885336
2017-12-10T12:55:47.267643: step 2875, loss 0.217828, acc 0.921875, prec 0.0659028, recall 0.885378
2017-12-10T12:55:47.715929: step 2876, loss 0.210204, acc 0.953125, prec 0.0659229, recall 0.885421
2017-12-10T12:55:48.161197: step 2877, loss 0.144202, acc 0.96875, prec 0.0659449, recall 0.885463
2017-12-10T12:55:48.600595: step 2878, loss 0.0413871, acc 0.984375, prec 0.0659431, recall 0.885463
2017-12-10T12:55:49.050658: step 2879, loss 0.0935835, acc 0.96875, prec 0.065965, recall 0.885505
2017-12-10T12:55:49.495250: step 2880, loss 0.114861, acc 0.984375, prec 0.0659632, recall 0.885505
2017-12-10T12:55:49.930640: step 2881, loss 0.184101, acc 0.9375, prec 0.0659815, recall 0.885547
2017-12-10T12:55:50.370255: step 2882, loss 0.265958, acc 0.9375, prec 0.0659998, recall 0.885589
2017-12-10T12:55:50.812548: step 2883, loss 0.0603132, acc 0.96875, prec 0.0659962, recall 0.885589
2017-12-10T12:55:51.261377: step 2884, loss 0.187723, acc 0.953125, prec 0.0660163, recall 0.88563
2017-12-10T12:55:51.697538: step 2885, loss 0.23078, acc 0.96875, prec 0.0660638, recall 0.885714
2017-12-10T12:55:52.143772: step 2886, loss 6.01456, acc 0.921875, prec 0.0660821, recall 0.885432
2017-12-10T12:55:52.583648: step 2887, loss 0.156049, acc 0.9375, prec 0.0661004, recall 0.885474
2017-12-10T12:55:53.046398: step 2888, loss 0.285565, acc 0.953125, prec 0.0661459, recall 0.885558
2017-12-10T12:55:53.494243: step 2889, loss 0.189142, acc 0.9375, prec 0.0661897, recall 0.885641
2017-12-10T12:55:53.931042: step 2890, loss 0.0147135, acc 1, prec 0.0662407, recall 0.885725
2017-12-10T12:55:54.382213: step 2891, loss 0.164557, acc 0.953125, prec 0.0662608, recall 0.885766
2017-12-10T12:55:54.832045: step 2892, loss 0.285681, acc 0.890625, prec 0.0662991, recall 0.88585
2017-12-10T12:55:55.289402: step 2893, loss 0.0938075, acc 0.96875, prec 0.0662955, recall 0.88585
2017-12-10T12:55:55.732120: step 2894, loss 0.0865992, acc 0.96875, prec 0.0663173, recall 0.885891
2017-12-10T12:55:56.186521: step 2895, loss 0.329812, acc 0.90625, prec 0.066332, recall 0.885933
2017-12-10T12:55:56.636200: step 2896, loss 0.191318, acc 0.984375, prec 0.0663811, recall 0.886016
2017-12-10T12:55:57.091891: step 2897, loss 2.09983, acc 0.90625, prec 0.066372, recall 0.885693
2017-12-10T12:55:57.528450: step 2898, loss 0.25239, acc 0.921875, prec 0.066363, recall 0.885693
2017-12-10T12:55:57.953487: step 2899, loss 0.371728, acc 0.859375, prec 0.0663467, recall 0.885693
2017-12-10T12:55:58.397305: step 2900, loss 0.10708, acc 0.984375, prec 0.0663704, recall 0.885735
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-2900

2017-12-10T12:56:00.270856: step 2901, loss 0.297848, acc 0.890625, prec 0.0664086, recall 0.885818
2017-12-10T12:56:00.729791: step 2902, loss 0.369182, acc 0.9375, prec 0.0665031, recall 0.885984
2017-12-10T12:56:01.176923: step 2903, loss 0.472904, acc 0.875, prec 0.0665649, recall 0.886108
2017-12-10T12:56:01.607321: step 2904, loss 0.342162, acc 0.859375, prec 0.0665486, recall 0.886108
2017-12-10T12:56:02.040096: step 2905, loss 0.37147, acc 0.875, prec 0.0665595, recall 0.886149
2017-12-10T12:56:02.473814: step 2906, loss 0.0394205, acc 0.984375, prec 0.0665577, recall 0.886149
2017-12-10T12:56:02.918241: step 2907, loss 0.702347, acc 0.859375, prec 0.0665923, recall 0.886232
2017-12-10T12:56:03.363420: step 2908, loss 0.40581, acc 0.890625, prec 0.0665796, recall 0.886232
2017-12-10T12:56:03.810271: step 2909, loss 0.440633, acc 0.875, prec 0.0665905, recall 0.886273
2017-12-10T12:56:04.235397: step 2910, loss 0.259401, acc 0.953125, prec 0.0666104, recall 0.886314
2017-12-10T12:56:04.677926: step 2911, loss 0.26151, acc 0.890625, prec 0.0666485, recall 0.886397
2017-12-10T12:56:05.129535: step 2912, loss 0.205949, acc 0.90625, prec 0.0666377, recall 0.886397
2017-12-10T12:56:05.573347: step 2913, loss 0.961042, acc 0.921875, prec 0.066654, recall 0.886438
2017-12-10T12:56:06.014778: step 2914, loss 0.120268, acc 0.9375, prec 0.0666467, recall 0.886438
2017-12-10T12:56:06.455730: step 2915, loss 0.38247, acc 0.828125, prec 0.0666522, recall 0.886479
2017-12-10T12:56:06.906784: step 2916, loss 0.201555, acc 0.9375, prec 0.0666449, recall 0.886479
2017-12-10T12:56:07.349707: step 2917, loss 0.26203, acc 0.90625, prec 0.0666594, recall 0.88652
2017-12-10T12:56:07.787690: step 2918, loss 0.311205, acc 0.859375, prec 0.0666431, recall 0.88652
2017-12-10T12:56:08.231763: step 2919, loss 0.738666, acc 0.90625, prec 0.0666576, recall 0.886561
2017-12-10T12:56:08.680824: step 2920, loss 0.165449, acc 0.953125, prec 0.0666775, recall 0.886602
2017-12-10T12:56:09.138795: step 2921, loss 0.114571, acc 0.953125, prec 0.0666721, recall 0.886602
2017-12-10T12:56:09.573864: step 2922, loss 0.064804, acc 0.96875, prec 0.0666685, recall 0.886602
2017-12-10T12:56:10.011134: step 2923, loss 0.105293, acc 0.96875, prec 0.0666649, recall 0.886602
2017-12-10T12:56:10.460083: step 2924, loss 0.0848718, acc 0.953125, prec 0.0666594, recall 0.886602
2017-12-10T12:56:10.907217: step 2925, loss 0.553507, acc 0.890625, prec 0.0666721, recall 0.886643
2017-12-10T12:56:11.356903: step 2926, loss 0.0951923, acc 0.96875, prec 0.0666685, recall 0.886643
2017-12-10T12:56:11.789181: step 2927, loss 0.175407, acc 0.984375, prec 0.0667173, recall 0.886724
2017-12-10T12:56:12.228562: step 2928, loss 0.1108, acc 0.921875, prec 0.0667083, recall 0.886724
2017-12-10T12:56:12.680549: step 2929, loss 1.22139, acc 0.953125, prec 0.0667047, recall 0.886405
2017-12-10T12:56:13.129728: step 2930, loss 2.82846, acc 0.9375, prec 0.0667752, recall 0.886208
2017-12-10T12:56:13.570816: step 2931, loss 0.338136, acc 0.9375, prec 0.0667933, recall 0.886249
2017-12-10T12:56:14.009943: step 2932, loss 5.79581, acc 0.875, prec 0.0668818, recall 0.886094
2017-12-10T12:56:14.452484: step 2933, loss 0.547094, acc 0.875, prec 0.0669179, recall 0.886176
2017-12-10T12:56:14.891529: step 2934, loss 0.255109, acc 0.875, prec 0.0669034, recall 0.886176
2017-12-10T12:56:15.338392: step 2935, loss 0.518497, acc 0.8125, prec 0.0669069, recall 0.886217
2017-12-10T12:56:15.776272: step 2936, loss 0.987104, acc 0.765625, prec 0.066905, recall 0.886258
2017-12-10T12:56:16.234147: step 2937, loss 0.790946, acc 0.765625, prec 0.0668779, recall 0.886258
2017-12-10T12:56:16.671295: step 2938, loss 0.335458, acc 0.890625, prec 0.0668652, recall 0.886258
2017-12-10T12:56:17.112239: step 2939, loss 0.522802, acc 0.84375, prec 0.0668723, recall 0.886298
2017-12-10T12:56:17.552427: step 2940, loss 0.835148, acc 0.828125, prec 0.0668777, recall 0.886339
2017-12-10T12:56:18.003759: step 2941, loss 0.353523, acc 0.890625, prec 0.0668903, recall 0.88638
2017-12-10T12:56:18.446354: step 2942, loss 0.812724, acc 0.765625, prec 0.0668631, recall 0.88638
2017-12-10T12:56:18.891621: step 2943, loss 0.847148, acc 0.78125, prec 0.0668883, recall 0.886461
2017-12-10T12:56:19.326674: step 2944, loss 1.39266, acc 0.75, prec 0.0669098, recall 0.886543
2017-12-10T12:56:19.769615: step 2945, loss 0.885715, acc 0.78125, prec 0.0669097, recall 0.886583
2017-12-10T12:56:20.210956: step 2946, loss 0.775598, acc 0.734375, prec 0.066879, recall 0.886583
2017-12-10T12:56:20.643331: step 2947, loss 0.833766, acc 0.84375, prec 0.0669616, recall 0.886745
2017-12-10T12:56:21.089085: step 2948, loss 0.799114, acc 0.796875, prec 0.0669885, recall 0.886826
2017-12-10T12:56:21.529430: step 2949, loss 0.333492, acc 0.890625, prec 0.0670262, recall 0.886907
2017-12-10T12:56:21.969956: step 2950, loss 0.272253, acc 0.890625, prec 0.0670135, recall 0.886907
2017-12-10T12:56:22.412725: step 2951, loss 0.296169, acc 0.921875, prec 0.0670548, recall 0.886988
2017-12-10T12:56:22.858338: step 2952, loss 0.299797, acc 0.921875, prec 0.0670709, recall 0.887028
2017-12-10T12:56:23.298093: step 2953, loss 0.422866, acc 0.875, prec 0.0670564, recall 0.887028
2017-12-10T12:56:23.745751: step 2954, loss 0.155283, acc 0.9375, prec 0.0670492, recall 0.887028
2017-12-10T12:56:24.178080: step 2955, loss 0.251981, acc 0.9375, prec 0.0670922, recall 0.887108
2017-12-10T12:56:24.613417: step 2956, loss 1.09583, acc 0.90625, prec 0.0671065, recall 0.887148
2017-12-10T12:56:25.060771: step 2957, loss 0.0880766, acc 0.953125, prec 0.0671011, recall 0.887148
2017-12-10T12:56:25.496723: step 2958, loss 0.1968, acc 0.90625, prec 0.0671154, recall 0.887189
2017-12-10T12:56:25.936680: step 2959, loss 0.211847, acc 0.953125, prec 0.0671099, recall 0.887189
2017-12-10T12:56:26.377936: step 2960, loss 0.131839, acc 0.96875, prec 0.0671063, recall 0.887189
2017-12-10T12:56:26.827559: step 2961, loss 0.318542, acc 0.90625, prec 0.0670955, recall 0.887189
2017-12-10T12:56:27.277360: step 2962, loss 0.238756, acc 0.90625, prec 0.0671098, recall 0.887229
2017-12-10T12:56:27.716470: step 2963, loss 0.127738, acc 0.953125, prec 0.0671043, recall 0.887229
2017-12-10T12:56:28.140760: step 2964, loss 0.23008, acc 0.96875, prec 0.0671007, recall 0.887229
2017-12-10T12:56:28.581353: step 2965, loss 0.121656, acc 0.96875, prec 0.0670971, recall 0.887229
2017-12-10T12:56:29.030205: step 2966, loss 0.0883416, acc 0.96875, prec 0.0670935, recall 0.887229
2017-12-10T12:56:29.478621: step 2967, loss 0.159378, acc 1, prec 0.0671186, recall 0.887269
2017-12-10T12:56:29.942451: step 2968, loss 1.10578, acc 0.984375, prec 0.0671419, recall 0.887309
2017-12-10T12:56:30.389977: step 2969, loss 0.198238, acc 0.953125, prec 0.0671616, recall 0.887349
2017-12-10T12:56:30.842795: step 2970, loss 0.124672, acc 0.984375, prec 0.0671849, recall 0.887389
2017-12-10T12:56:31.270164: step 2971, loss 0.129463, acc 0.984375, prec 0.067183, recall 0.887389
2017-12-10T12:56:31.718871: step 2972, loss 0.122807, acc 0.96875, prec 0.0672045, recall 0.887429
2017-12-10T12:56:32.161064: step 2973, loss 0.0963902, acc 0.953125, prec 0.0671991, recall 0.887429
2017-12-10T12:56:32.591495: step 2974, loss 0.125874, acc 0.9375, prec 0.0671919, recall 0.887429
2017-12-10T12:56:33.023978: step 2975, loss 0.096433, acc 0.984375, prec 0.0672151, recall 0.887469
2017-12-10T12:56:33.457125: step 2976, loss 0.0754654, acc 0.96875, prec 0.0672366, recall 0.887509
2017-12-10T12:56:33.908063: step 2977, loss 0.30801, acc 0.9375, prec 0.0672294, recall 0.887509
2017-12-10T12:56:34.354810: step 2978, loss 0.204226, acc 0.953125, prec 0.067224, recall 0.887509
2017-12-10T12:56:34.803518: step 2979, loss 0.181646, acc 0.953125, prec 0.0672436, recall 0.887549
2017-12-10T12:56:35.255250: step 2980, loss 0.29833, acc 0.953125, prec 0.0672382, recall 0.887549
2017-12-10T12:56:35.705723: step 2981, loss 5.89197, acc 0.96875, prec 0.0672364, recall 0.887234
2017-12-10T12:56:36.117612: step 2982, loss 0.459267, acc 0.903846, prec 0.0672273, recall 0.887234
2017-12-10T12:56:36.574449: step 2983, loss 0.146052, acc 0.9375, prec 0.0672201, recall 0.887234
2017-12-10T12:56:37.024359: step 2984, loss 0.327195, acc 0.921875, prec 0.0672361, recall 0.887274
2017-12-10T12:56:37.477177: step 2985, loss 0.0714835, acc 0.96875, prec 0.0672576, recall 0.887314
2017-12-10T12:56:37.924155: step 2986, loss 0.321985, acc 0.90625, prec 0.0672467, recall 0.887314
2017-12-10T12:56:38.363780: step 2987, loss 0.0899295, acc 0.96875, prec 0.0672431, recall 0.887314
2017-12-10T12:56:38.807623: step 2988, loss 0.337825, acc 0.9375, prec 0.067286, recall 0.887394
2017-12-10T12:56:39.253114: step 2989, loss 0.229535, acc 0.921875, prec 0.067302, recall 0.887434
2017-12-10T12:56:39.696539: step 2990, loss 0.435294, acc 0.890625, prec 0.0672894, recall 0.887434
2017-12-10T12:56:40.156170: step 2991, loss 0.741392, acc 0.84375, prec 0.0672713, recall 0.887434
2017-12-10T12:56:40.601842: step 2992, loss 0.158339, acc 0.96875, prec 0.0672677, recall 0.887434
2017-12-10T12:56:41.057898: step 2993, loss 0.0921991, acc 0.96875, prec 0.0672641, recall 0.887434
2017-12-10T12:56:41.518207: step 2994, loss 0.389736, acc 0.890625, prec 0.0673515, recall 0.887593
2017-12-10T12:56:41.968511: step 2995, loss 0.190387, acc 0.96875, prec 0.0673479, recall 0.887593
2017-12-10T12:56:42.406778: step 2996, loss 0.510385, acc 0.875, prec 0.0673585, recall 0.887632
2017-12-10T12:56:42.853986: step 2997, loss 0.198226, acc 0.953125, prec 0.0673781, recall 0.887672
2017-12-10T12:56:43.309915: step 2998, loss 0.395609, acc 0.90625, prec 0.0673672, recall 0.887672
2017-12-10T12:56:43.752504: step 2999, loss 0.133003, acc 0.96875, prec 0.0673636, recall 0.887672
2017-12-10T12:56:44.209797: step 3000, loss 0.338228, acc 0.875, prec 0.0673742, recall 0.887712
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3000

2017-12-10T12:56:46.120457: step 3001, loss 0.233126, acc 0.921875, prec 0.0673651, recall 0.887712
2017-12-10T12:56:46.562031: step 3002, loss 0.131377, acc 0.96875, prec 0.0673615, recall 0.887712
2017-12-10T12:56:47.001176: step 3003, loss 0.180838, acc 0.921875, prec 0.0674275, recall 0.887831
2017-12-10T12:56:47.465368: step 3004, loss 0.292565, acc 0.921875, prec 0.0674434, recall 0.88787
2017-12-10T12:56:47.912218: step 3005, loss 0.0633015, acc 0.96875, prec 0.0674648, recall 0.88791
2017-12-10T12:56:48.362820: step 3006, loss 0.153722, acc 1, prec 0.0675147, recall 0.887989
2017-12-10T12:56:48.804152: step 3007, loss 0.265384, acc 0.953125, prec 0.0675093, recall 0.887989
2017-12-10T12:56:49.253127: step 3008, loss 0.130027, acc 0.96875, prec 0.0675057, recall 0.887989
2017-12-10T12:56:49.686077: step 3009, loss 0.0694641, acc 0.96875, prec 0.0675021, recall 0.887989
2017-12-10T12:56:50.121212: step 3010, loss 0.0258935, acc 1, prec 0.067527, recall 0.888028
2017-12-10T12:56:50.576254: step 3011, loss 0.204477, acc 0.953125, prec 0.0675216, recall 0.888028
2017-12-10T12:56:51.043238: step 3012, loss 0.158733, acc 0.96875, prec 0.067543, recall 0.888068
2017-12-10T12:56:51.497197: step 3013, loss 0.188731, acc 0.96875, prec 0.0675393, recall 0.888068
2017-12-10T12:56:51.940469: step 3014, loss 0.312328, acc 0.96875, prec 0.0675607, recall 0.888107
2017-12-10T12:56:52.386968: step 3015, loss 0.0750935, acc 0.984375, prec 0.0676088, recall 0.888186
2017-12-10T12:56:52.832724: step 3016, loss 0.320832, acc 0.9375, prec 0.0676515, recall 0.888264
2017-12-10T12:56:53.292015: step 3017, loss 0.0390911, acc 0.984375, prec 0.0676497, recall 0.888264
2017-12-10T12:56:53.735894: step 3018, loss 0.0745797, acc 1, prec 0.0676746, recall 0.888303
2017-12-10T12:56:54.185600: step 3019, loss 0.0783105, acc 0.96875, prec 0.0676959, recall 0.888343
2017-12-10T12:56:54.632345: step 3020, loss 0.00793447, acc 1, prec 0.0676959, recall 0.888343
2017-12-10T12:56:55.078655: step 3021, loss 0.0139519, acc 1, prec 0.0676959, recall 0.888343
2017-12-10T12:56:55.524720: step 3022, loss 0.541556, acc 0.984375, prec 0.0677191, recall 0.888382
2017-12-10T12:56:55.980026: step 3023, loss 1.93456, acc 0.96875, prec 0.0677422, recall 0.888109
2017-12-10T12:56:56.442709: step 3024, loss 0.0281154, acc 1, prec 0.0677671, recall 0.888149
2017-12-10T12:56:56.890968: step 3025, loss 0.0924315, acc 0.984375, prec 0.0677903, recall 0.888188
2017-12-10T12:56:57.339112: step 3026, loss 0.96082, acc 0.953125, prec 0.0678098, recall 0.888227
2017-12-10T12:56:57.796881: step 3027, loss 1.28619, acc 0.90625, prec 0.0678007, recall 0.887916
2017-12-10T12:56:58.243166: step 3028, loss 0.275288, acc 0.875, prec 0.067836, recall 0.887994
2017-12-10T12:56:58.677226: step 3029, loss 0.079758, acc 0.96875, prec 0.0678573, recall 0.888034
2017-12-10T12:56:59.120186: step 3030, loss 0.512362, acc 0.875, prec 0.0678428, recall 0.888034
2017-12-10T12:56:59.549507: step 3031, loss 0.317354, acc 0.875, prec 0.0678283, recall 0.888034
2017-12-10T12:56:59.994623: step 3032, loss 0.73867, acc 0.796875, prec 0.0678297, recall 0.888073
2017-12-10T12:57:00.440574: step 3033, loss 0.42615, acc 0.8125, prec 0.0678328, recall 0.888112
2017-12-10T12:57:00.886701: step 3034, loss 0.313357, acc 0.859375, prec 0.0678165, recall 0.888112
2017-12-10T12:57:01.335844: step 3035, loss 0.209811, acc 0.90625, prec 0.0678554, recall 0.88819
2017-12-10T12:57:01.773124: step 3036, loss 0.218438, acc 0.90625, prec 0.0678694, recall 0.888229
2017-12-10T12:57:02.226679: step 3037, loss 0.563733, acc 0.796875, prec 0.0678708, recall 0.888268
2017-12-10T12:57:02.674385: step 3038, loss 0.248496, acc 0.96875, prec 0.0679169, recall 0.888346
2017-12-10T12:57:03.108953: step 3039, loss 0.552803, acc 0.84375, prec 0.0679236, recall 0.888385
2017-12-10T12:57:03.554150: step 3040, loss 0.475691, acc 0.828125, prec 0.0679037, recall 0.888385
2017-12-10T12:57:03.991459: step 3041, loss 0.333339, acc 0.875, prec 0.0679141, recall 0.888424
2017-12-10T12:57:04.436668: step 3042, loss 0.507477, acc 0.796875, prec 0.0679154, recall 0.888463
2017-12-10T12:57:04.885919: step 3043, loss 0.618733, acc 0.8125, prec 0.0679185, recall 0.888502
2017-12-10T12:57:05.349187: step 3044, loss 0.25435, acc 0.96875, prec 0.0679149, recall 0.888502
2017-12-10T12:57:05.786894: step 3045, loss 0.329318, acc 0.90625, prec 0.0679289, recall 0.888541
2017-12-10T12:57:06.241418: step 3046, loss 0.0906392, acc 0.96875, prec 0.0679252, recall 0.888541
2017-12-10T12:57:06.691226: step 3047, loss 0.205395, acc 0.9375, prec 0.0679428, recall 0.888579
2017-12-10T12:57:07.146251: step 3048, loss 0.428784, acc 0.875, prec 0.067978, recall 0.888657
2017-12-10T12:57:07.584428: step 3049, loss 0.240792, acc 0.875, prec 0.0679635, recall 0.888657
2017-12-10T12:57:08.021072: step 3050, loss 0.101284, acc 0.96875, prec 0.0680095, recall 0.888734
2017-12-10T12:57:08.449344: step 3051, loss 0.708541, acc 0.921875, prec 0.0680252, recall 0.888773
2017-12-10T12:57:08.893652: step 3052, loss 0.253545, acc 0.921875, prec 0.0680162, recall 0.888773
2017-12-10T12:57:09.337185: step 3053, loss 0.296844, acc 0.890625, prec 0.0680035, recall 0.888773
2017-12-10T12:57:09.772046: step 3054, loss 0.120924, acc 0.921875, prec 0.0680192, recall 0.888812
2017-12-10T12:57:10.228279: step 3055, loss 0.145902, acc 0.96875, prec 0.0680156, recall 0.888812
2017-12-10T12:57:10.671390: step 3056, loss 2.09814, acc 0.921875, prec 0.0680332, recall 0.888542
2017-12-10T12:57:11.115352: step 3057, loss 0.236597, acc 0.953125, prec 0.0680525, recall 0.88858
2017-12-10T12:57:11.559037: step 3058, loss 0.223481, acc 0.9375, prec 0.0680453, recall 0.88858
2017-12-10T12:57:12.004691: step 3059, loss 0.135924, acc 0.96875, prec 0.0680664, recall 0.888619
2017-12-10T12:57:12.450086: step 3060, loss 0.313241, acc 0.9375, prec 0.0680592, recall 0.888619
2017-12-10T12:57:12.908198: step 3061, loss 0.199017, acc 0.953125, prec 0.0680785, recall 0.888658
2017-12-10T12:57:13.351275: step 3062, loss 0.0902917, acc 0.96875, prec 0.0680997, recall 0.888696
2017-12-10T12:57:13.799363: step 3063, loss 0.10345, acc 0.96875, prec 0.0680961, recall 0.888696
2017-12-10T12:57:14.235150: step 3064, loss 0.204886, acc 0.9375, prec 0.0681383, recall 0.888773
2017-12-10T12:57:14.671571: step 3065, loss 0.158996, acc 0.90625, prec 0.0681275, recall 0.888773
2017-12-10T12:57:15.115511: step 3066, loss 0.119688, acc 0.9375, prec 0.068145, recall 0.888812
2017-12-10T12:57:15.572234: step 3067, loss 0.417984, acc 0.921875, prec 0.068136, recall 0.888812
2017-12-10T12:57:16.033453: step 3068, loss 0.45384, acc 0.921875, prec 0.0681516, recall 0.88885
2017-12-10T12:57:16.490210: step 3069, loss 0.169319, acc 0.96875, prec 0.0681975, recall 0.888927
2017-12-10T12:57:16.938716: step 3070, loss 0.135148, acc 0.96875, prec 0.0682186, recall 0.888966
2017-12-10T12:57:17.388140: step 3071, loss 0.304003, acc 0.953125, prec 0.0682379, recall 0.889004
2017-12-10T12:57:17.836460: step 3072, loss 0.265704, acc 0.90625, prec 0.0682765, recall 0.889081
2017-12-10T12:57:18.276259: step 3073, loss 0.042055, acc 0.984375, prec 0.0682747, recall 0.889081
2017-12-10T12:57:18.720095: step 3074, loss 0.133282, acc 0.953125, prec 0.068294, recall 0.889119
2017-12-10T12:57:19.172789: step 3075, loss 0.432608, acc 1, prec 0.0683434, recall 0.889196
2017-12-10T12:57:19.618000: step 3076, loss 0.030567, acc 1, prec 0.0683434, recall 0.889196
2017-12-10T12:57:20.061986: step 3077, loss 0.242329, acc 0.90625, prec 0.0683325, recall 0.889196
2017-12-10T12:57:20.505219: step 3078, loss 0.0736131, acc 0.953125, prec 0.0683271, recall 0.889196
2017-12-10T12:57:20.960087: step 3079, loss 0.0563909, acc 0.984375, prec 0.06835, recall 0.889234
2017-12-10T12:57:21.403236: step 3080, loss 0.129153, acc 0.96875, prec 0.0683711, recall 0.889272
2017-12-10T12:57:21.852610: step 3081, loss 0.147829, acc 0.96875, prec 0.0683675, recall 0.889272
2017-12-10T12:57:22.294561: step 3082, loss 0.0927328, acc 0.96875, prec 0.0683885, recall 0.88931
2017-12-10T12:57:22.751135: step 3083, loss 0.426824, acc 0.921875, prec 0.0684042, recall 0.889349
2017-12-10T12:57:23.189035: step 3084, loss 0.198813, acc 0.984375, prec 0.0684517, recall 0.889425
2017-12-10T12:57:23.637198: step 3085, loss 0.249698, acc 0.9375, prec 0.0684692, recall 0.889463
2017-12-10T12:57:24.101452: step 3086, loss 0.0633456, acc 0.953125, prec 0.0684637, recall 0.889463
2017-12-10T12:57:24.544588: step 3087, loss 0.0658848, acc 0.96875, prec 0.0684848, recall 0.889501
2017-12-10T12:57:24.996934: step 3088, loss 0.0322893, acc 0.984375, prec 0.068483, recall 0.889501
2017-12-10T12:57:25.454441: step 3089, loss 0.166302, acc 0.96875, prec 0.068504, recall 0.889539
2017-12-10T12:57:25.906156: step 3090, loss 0.0169083, acc 1, prec 0.0685534, recall 0.889615
2017-12-10T12:57:26.356705: step 3091, loss 0.0686167, acc 0.96875, prec 0.0685498, recall 0.889615
2017-12-10T12:57:26.810457: step 3092, loss 0.251241, acc 0.96875, prec 0.0686202, recall 0.889729
2017-12-10T12:57:27.257476: step 3093, loss 0.136366, acc 0.953125, prec 0.0686147, recall 0.889729
2017-12-10T12:57:27.705298: step 3094, loss 0.22903, acc 0.953125, prec 0.0686093, recall 0.889729
2017-12-10T12:57:28.150892: step 3095, loss 0.196809, acc 0.984375, prec 0.0686321, recall 0.889766
2017-12-10T12:57:28.595250: step 3096, loss 0.451024, acc 1, prec 0.0686568, recall 0.889804
2017-12-10T12:57:29.050047: step 3097, loss 0.125687, acc 0.953125, prec 0.0686513, recall 0.889804
2017-12-10T12:57:29.491820: step 3098, loss 0.0545519, acc 0.984375, prec 0.0686495, recall 0.889804
2017-12-10T12:57:29.938157: step 3099, loss 0.0500091, acc 0.984375, prec 0.0686477, recall 0.889804
2017-12-10T12:57:30.382345: step 3100, loss 0.534479, acc 0.9375, prec 0.0686651, recall 0.889842
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3100

2017-12-10T12:57:32.286909: step 3101, loss 0.377452, acc 0.90625, prec 0.0686542, recall 0.889842
2017-12-10T12:57:32.724364: step 3102, loss 0.218442, acc 0.96875, prec 0.0686506, recall 0.889842
2017-12-10T12:57:33.171110: step 3103, loss 0.032922, acc 0.984375, prec 0.0686487, recall 0.889842
2017-12-10T12:57:33.618605: step 3104, loss 0.0875025, acc 0.96875, prec 0.0686451, recall 0.889842
2017-12-10T12:57:34.076537: step 3105, loss 0.0667, acc 0.984375, prec 0.0686433, recall 0.889842
2017-12-10T12:57:34.530400: step 3106, loss 0.209707, acc 0.96875, prec 0.0686397, recall 0.889842
2017-12-10T12:57:34.981597: step 3107, loss 0.252144, acc 0.9375, prec 0.068657, recall 0.88988
2017-12-10T12:57:35.435692: step 3108, loss 0.261459, acc 0.984375, prec 0.0687045, recall 0.889955
2017-12-10T12:57:35.887174: step 3109, loss 0.191522, acc 0.96875, prec 0.0687255, recall 0.889993
2017-12-10T12:57:36.321127: step 3110, loss 0.122391, acc 0.96875, prec 0.0687219, recall 0.889993
2017-12-10T12:57:36.766939: step 3111, loss 0.177883, acc 0.953125, prec 0.0687657, recall 0.890068
2017-12-10T12:57:37.216123: step 3112, loss 0.23371, acc 0.90625, prec 0.0687794, recall 0.890106
2017-12-10T12:57:37.657906: step 3113, loss 0.0647159, acc 0.96875, prec 0.0687758, recall 0.890106
2017-12-10T12:57:38.102381: step 3114, loss 0.0748402, acc 0.984375, prec 0.0687986, recall 0.890144
2017-12-10T12:57:38.567303: step 3115, loss 0.112279, acc 0.96875, prec 0.0689181, recall 0.890331
2017-12-10T12:57:39.020809: step 3116, loss 0.0475305, acc 0.96875, prec 0.0689145, recall 0.890331
2017-12-10T12:57:39.469419: step 3117, loss 0.0204803, acc 1, prec 0.0689145, recall 0.890331
2017-12-10T12:57:39.923355: step 3118, loss 0.0942753, acc 0.96875, prec 0.0689108, recall 0.890331
2017-12-10T12:57:40.366883: step 3119, loss 0.176339, acc 0.9375, prec 0.0689528, recall 0.890406
2017-12-10T12:57:40.815883: step 3120, loss 2.46979, acc 0.953125, prec 0.0689491, recall 0.890102
2017-12-10T12:57:41.271915: step 3121, loss 0.261545, acc 0.953125, prec 0.0689436, recall 0.890102
2017-12-10T12:57:41.720621: step 3122, loss 0.286671, acc 0.921875, prec 0.0689591, recall 0.89014
2017-12-10T12:57:42.180598: step 3123, loss 0.201644, acc 0.9375, prec 0.0690011, recall 0.890215
2017-12-10T12:57:42.625880: step 3124, loss 0.883299, acc 0.953125, prec 0.0690448, recall 0.89029
2017-12-10T12:57:43.077779: step 3125, loss 0.275444, acc 0.953125, prec 0.0690885, recall 0.890364
2017-12-10T12:57:43.525842: step 3126, loss 0.215782, acc 0.90625, prec 0.0691021, recall 0.890402
2017-12-10T12:57:43.967970: step 3127, loss 0.228926, acc 0.921875, prec 0.0691176, recall 0.890439
2017-12-10T12:57:44.414012: step 3128, loss 0.0955846, acc 0.96875, prec 0.069114, recall 0.890439
2017-12-10T12:57:44.860301: step 3129, loss 0.290963, acc 0.9375, prec 0.0691804, recall 0.890551
2017-12-10T12:57:45.292347: step 3130, loss 0.630276, acc 0.8125, prec 0.0691831, recall 0.890588
2017-12-10T12:57:45.736618: step 3131, loss 0.515703, acc 0.875, prec 0.069193, recall 0.890625
2017-12-10T12:57:46.170777: step 3132, loss 0.278088, acc 0.90625, prec 0.0691821, recall 0.890625
2017-12-10T12:57:46.610905: step 3133, loss 0.448783, acc 0.859375, prec 0.0691902, recall 0.890662
2017-12-10T12:57:47.059434: step 3134, loss 0.229881, acc 0.921875, prec 0.0691811, recall 0.890662
2017-12-10T12:57:47.508988: step 3135, loss 0.332712, acc 0.890625, prec 0.0691683, recall 0.890662
2017-12-10T12:57:47.950991: step 3136, loss 0.513473, acc 0.875, prec 0.0691537, recall 0.890662
2017-12-10T12:57:48.383227: step 3137, loss 0.156829, acc 0.9375, prec 0.069171, recall 0.890699
2017-12-10T12:57:48.822803: step 3138, loss 0.352959, acc 0.890625, prec 0.0691582, recall 0.890699
2017-12-10T12:57:49.276222: step 3139, loss 0.36072, acc 0.90625, prec 0.0691718, recall 0.890736
2017-12-10T12:57:49.724065: step 3140, loss 0.146733, acc 0.984375, prec 0.0691945, recall 0.890773
2017-12-10T12:57:50.168313: step 3141, loss 0.576808, acc 0.921875, prec 0.0692344, recall 0.890847
2017-12-10T12:57:50.623565: step 3142, loss 0.173995, acc 0.953125, prec 0.0693025, recall 0.890958
2017-12-10T12:57:51.069805: step 3143, loss 0.234664, acc 0.9375, prec 0.0692952, recall 0.890958
2017-12-10T12:57:51.516830: step 3144, loss 0.274744, acc 0.921875, prec 0.0692861, recall 0.890958
2017-12-10T12:57:51.959761: step 3145, loss 0.160435, acc 0.96875, prec 0.0693069, recall 0.890995
2017-12-10T12:57:52.407061: step 3146, loss 0.198364, acc 0.921875, prec 0.0692978, recall 0.890995
2017-12-10T12:57:52.845332: step 3147, loss 0.453948, acc 0.859375, prec 0.0692814, recall 0.890995
2017-12-10T12:57:53.284504: step 3148, loss 0.0745957, acc 0.984375, prec 0.0692796, recall 0.890995
2017-12-10T12:57:53.728693: step 3149, loss 0.0821079, acc 0.953125, prec 0.0692986, recall 0.891032
2017-12-10T12:57:54.176779: step 3150, loss 0.847026, acc 0.921875, prec 0.069314, recall 0.891069
2017-12-10T12:57:54.632790: step 3151, loss 0.198224, acc 0.953125, prec 0.0693085, recall 0.891069
2017-12-10T12:57:55.079900: step 3152, loss 0.0925963, acc 0.96875, prec 0.0693048, recall 0.891069
2017-12-10T12:57:55.538010: step 3153, loss 0.21976, acc 0.9375, prec 0.0692976, recall 0.891069
2017-12-10T12:57:56.003237: step 3154, loss 0.261864, acc 0.9375, prec 0.0693637, recall 0.891179
2017-12-10T12:57:56.455753: step 3155, loss 0.380875, acc 0.953125, prec 0.0694072, recall 0.891253
2017-12-10T12:57:56.896808: step 3156, loss 0.304839, acc 0.9375, prec 0.0693999, recall 0.891253
2017-12-10T12:57:57.347963: step 3157, loss 0.216486, acc 0.953125, prec 0.0694189, recall 0.89129
2017-12-10T12:57:57.798465: step 3158, loss 0.157969, acc 0.96875, prec 0.0694397, recall 0.891326
2017-12-10T12:57:58.236624: step 3159, loss 0.215094, acc 0.953125, prec 0.0694342, recall 0.891326
2017-12-10T12:57:58.679319: step 3160, loss 0.0238508, acc 1, prec 0.0694342, recall 0.891326
2017-12-10T12:57:59.131881: step 3161, loss 0.0614015, acc 0.984375, prec 0.0694324, recall 0.891326
2017-12-10T12:57:59.575144: step 3162, loss 0.406092, acc 0.984375, prec 0.069455, recall 0.891363
2017-12-10T12:58:00.017573: step 3163, loss 0.0124315, acc 1, prec 0.0694795, recall 0.8914
2017-12-10T12:58:00.450525: step 3164, loss 0.0772623, acc 0.984375, prec 0.0695266, recall 0.891473
2017-12-10T12:58:00.897563: step 3165, loss 0.282972, acc 1, prec 0.0695755, recall 0.891546
2017-12-10T12:58:01.348225: step 3166, loss 0.021156, acc 1, prec 0.0696244, recall 0.891619
2017-12-10T12:58:01.787155: step 3167, loss 0.264973, acc 0.9375, prec 0.069666, recall 0.891692
2017-12-10T12:58:02.228550: step 3168, loss 0.201589, acc 0.921875, prec 0.0696813, recall 0.891728
2017-12-10T12:58:02.676365: step 3169, loss 0.448213, acc 0.9375, prec 0.0697228, recall 0.891801
2017-12-10T12:58:03.118446: step 3170, loss 0.480103, acc 1, prec 0.0697962, recall 0.89191
2017-12-10T12:58:03.563804: step 3171, loss 0.0339421, acc 1, prec 0.0697962, recall 0.89191
2017-12-10T12:58:04.016416: step 3172, loss 0.214871, acc 0.90625, prec 0.0697852, recall 0.89191
2017-12-10T12:58:04.462482: step 3173, loss 0.184345, acc 0.9375, prec 0.0698023, recall 0.891946
2017-12-10T12:58:04.915156: step 3174, loss 0.207, acc 0.953125, prec 0.0698944, recall 0.892091
2017-12-10T12:58:05.368174: step 3175, loss 0.0965036, acc 0.96875, prec 0.0699152, recall 0.892127
2017-12-10T12:58:05.824772: step 3176, loss 0.974159, acc 0.9375, prec 0.0699567, recall 0.8922
2017-12-10T12:58:06.292899: step 3177, loss 0.110258, acc 0.96875, prec 0.0700018, recall 0.892272
2017-12-10T12:58:06.738247: step 3178, loss 1.64073, acc 0.953125, prec 0.070047, recall 0.892045
2017-12-10T12:58:07.189938: step 3179, loss 0.184023, acc 0.9375, prec 0.070064, recall 0.892081
2017-12-10T12:58:07.631563: step 3180, loss 0.352751, acc 0.890625, prec 0.0700756, recall 0.892118
2017-12-10T12:58:08.084056: step 3181, loss 0.445468, acc 0.84375, prec 0.0700572, recall 0.892118
2017-12-10T12:58:08.529717: step 3182, loss 0.308115, acc 0.90625, prec 0.0700949, recall 0.89219
2017-12-10T12:58:08.972499: step 3183, loss 0.247832, acc 0.890625, prec 0.0701064, recall 0.892226
2017-12-10T12:58:09.415228: step 3184, loss 0.550153, acc 0.90625, prec 0.0701198, recall 0.892262
2017-12-10T12:58:09.851173: step 3185, loss 0.611907, acc 0.859375, prec 0.0701276, recall 0.892297
2017-12-10T12:58:10.299297: step 3186, loss 0.301715, acc 0.90625, prec 0.070141, recall 0.892333
2017-12-10T12:58:10.742116: step 3187, loss 0.573331, acc 0.828125, prec 0.0701451, recall 0.892369
2017-12-10T12:58:11.195973: step 3188, loss 0.607242, acc 0.84375, prec 0.0701754, recall 0.892441
2017-12-10T12:58:11.640452: step 3189, loss 0.903866, acc 0.796875, prec 0.0702002, recall 0.892513
2017-12-10T12:58:12.084348: step 3190, loss 0.866476, acc 0.8125, prec 0.0702268, recall 0.892584
2017-12-10T12:58:12.536643: step 3191, loss 0.253339, acc 0.890625, prec 0.0702383, recall 0.89262
2017-12-10T12:58:12.988094: step 3192, loss 0.448178, acc 0.875, prec 0.0702236, recall 0.89262
2017-12-10T12:58:13.436267: step 3193, loss 0.359343, acc 0.859375, prec 0.0702314, recall 0.892655
2017-12-10T12:58:13.873713: step 3194, loss 0.491984, acc 0.859375, prec 0.0702878, recall 0.892762
2017-12-10T12:58:14.322011: step 3195, loss 0.651758, acc 0.84375, prec 0.070318, recall 0.892833
2017-12-10T12:58:14.765098: step 3196, loss 0.285295, acc 0.859375, prec 0.0703258, recall 0.892869
2017-12-10T12:58:15.205196: step 3197, loss 0.438341, acc 0.875, prec 0.0703111, recall 0.892869
2017-12-10T12:58:15.649826: step 3198, loss 0.13975, acc 0.96875, prec 0.0703317, recall 0.892905
2017-12-10T12:58:16.075268: step 3199, loss 0.290671, acc 0.90625, prec 0.0703449, recall 0.89294
2017-12-10T12:58:16.512910: step 3200, loss 0.219738, acc 0.921875, prec 0.0703358, recall 0.89294
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3200

2017-12-10T12:58:18.569871: step 3201, loss 0.057076, acc 0.984375, prec 0.0703582, recall 0.892976
2017-12-10T12:58:19.015220: step 3202, loss 0.14446, acc 0.96875, prec 0.0703788, recall 0.893011
2017-12-10T12:58:19.464591: step 3203, loss 0.0982013, acc 0.953125, prec 0.0703975, recall 0.893046
2017-12-10T12:58:19.907255: step 3204, loss 0.117869, acc 0.96875, prec 0.0704181, recall 0.893082
2017-12-10T12:58:20.355232: step 3205, loss 0.161884, acc 0.9375, prec 0.070435, recall 0.893117
2017-12-10T12:58:20.801420: step 3206, loss 0.0470579, acc 0.953125, prec 0.0704295, recall 0.893117
2017-12-10T12:58:21.243677: step 3207, loss 0.242017, acc 0.984375, prec 0.0704762, recall 0.893188
2017-12-10T12:58:21.683815: step 3208, loss 0.0247324, acc 0.984375, prec 0.0704986, recall 0.893223
2017-12-10T12:58:22.122245: step 3209, loss 0.113447, acc 0.984375, prec 0.070521, recall 0.893258
2017-12-10T12:58:22.579937: step 3210, loss 0.0665683, acc 0.96875, prec 0.0705173, recall 0.893258
2017-12-10T12:58:23.022633: step 3211, loss 0.259622, acc 0.984375, prec 0.070564, recall 0.893329
2017-12-10T12:58:23.466217: step 3212, loss 0.142492, acc 0.984375, prec 0.0705621, recall 0.893329
2017-12-10T12:58:23.909994: step 3213, loss 0.021497, acc 0.984375, prec 0.0705603, recall 0.893329
2017-12-10T12:58:24.349080: step 3214, loss 0.0358688, acc 0.984375, prec 0.0705827, recall 0.893364
2017-12-10T12:58:24.806319: step 3215, loss 0.164945, acc 0.9375, prec 0.0706238, recall 0.893435
2017-12-10T12:58:25.237486: step 3216, loss 1.32321, acc 0.96875, prec 0.0706947, recall 0.893245
2017-12-10T12:58:25.684129: step 3217, loss 0.135993, acc 0.953125, prec 0.0706892, recall 0.893245
2017-12-10T12:58:26.128890: step 3218, loss 0.230108, acc 0.9375, prec 0.070706, recall 0.893281
2017-12-10T12:58:26.570220: step 3219, loss 0.0924377, acc 0.984375, prec 0.0707284, recall 0.893316
2017-12-10T12:58:27.031727: step 3220, loss 0.105116, acc 0.96875, prec 0.0707247, recall 0.893316
2017-12-10T12:58:27.476214: step 3221, loss 0.233389, acc 0.953125, prec 0.0707434, recall 0.893351
2017-12-10T12:58:27.916278: step 3222, loss 0.210073, acc 0.953125, prec 0.0707621, recall 0.893386
2017-12-10T12:58:28.369842: step 3223, loss 0.041691, acc 0.96875, prec 0.0707826, recall 0.893421
2017-12-10T12:58:28.815491: step 3224, loss 0.0834899, acc 0.96875, prec 0.0707789, recall 0.893421
2017-12-10T12:58:29.261103: step 3225, loss 0.52334, acc 0.90625, prec 0.0708163, recall 0.893491
2017-12-10T12:58:29.697846: step 3226, loss 0.110893, acc 0.984375, prec 0.0708387, recall 0.893526
2017-12-10T12:58:30.138583: step 3227, loss 0.556877, acc 0.953125, prec 0.0708573, recall 0.893561
2017-12-10T12:58:30.592754: step 3228, loss 0.129437, acc 0.953125, prec 0.0708518, recall 0.893561
2017-12-10T12:58:31.037651: step 3229, loss 0.235234, acc 0.953125, prec 0.0708705, recall 0.893596
2017-12-10T12:58:31.476518: step 3230, loss 0.20911, acc 0.9375, prec 0.0708873, recall 0.893631
2017-12-10T12:58:31.918583: step 3231, loss 0.319337, acc 0.921875, prec 0.0709022, recall 0.893666
2017-12-10T12:58:32.365735: step 3232, loss 0.0778126, acc 0.96875, prec 0.0708985, recall 0.893666
2017-12-10T12:58:32.800872: step 3233, loss 0.135007, acc 0.96875, prec 0.070919, recall 0.893701
2017-12-10T12:58:33.237178: step 3234, loss 0.199899, acc 0.96875, prec 0.0709637, recall 0.893771
2017-12-10T12:58:33.685281: step 3235, loss 0.096514, acc 0.96875, prec 0.0710084, recall 0.89384
2017-12-10T12:58:34.137381: step 3236, loss 0.312911, acc 0.953125, prec 0.0710028, recall 0.89384
2017-12-10T12:58:34.589134: step 3237, loss 0.198745, acc 0.953125, prec 0.0710215, recall 0.893875
2017-12-10T12:58:35.035462: step 3238, loss 0.284228, acc 0.90625, prec 0.0710346, recall 0.89391
2017-12-10T12:58:35.476934: step 3239, loss 0.150388, acc 0.9375, prec 0.0710755, recall 0.893979
2017-12-10T12:58:35.908890: step 3240, loss 0.0829438, acc 0.984375, prec 0.0711461, recall 0.894083
2017-12-10T12:58:36.372051: step 3241, loss 0.218404, acc 0.953125, prec 0.0712131, recall 0.894187
2017-12-10T12:58:36.820998: step 3242, loss 0.404198, acc 0.890625, prec 0.0712001, recall 0.894187
2017-12-10T12:58:37.269366: step 3243, loss 0.162781, acc 0.953125, prec 0.0711946, recall 0.894187
2017-12-10T12:58:37.715513: step 3244, loss 0.238296, acc 0.9375, prec 0.0712113, recall 0.894221
2017-12-10T12:58:38.168184: step 3245, loss 0.174621, acc 0.921875, prec 0.0712262, recall 0.894256
2017-12-10T12:58:38.612798: step 3246, loss 0.17862, acc 0.953125, prec 0.0712206, recall 0.894256
2017-12-10T12:58:39.057307: step 3247, loss 0.0324598, acc 1, prec 0.0712448, recall 0.89429
2017-12-10T12:58:39.501971: step 3248, loss 0.119554, acc 0.96875, prec 0.0712411, recall 0.89429
2017-12-10T12:58:39.947821: step 3249, loss 0.109344, acc 1, prec 0.0712652, recall 0.894325
2017-12-10T12:58:40.389986: step 3250, loss 0.0218567, acc 1, prec 0.0713135, recall 0.894394
2017-12-10T12:58:40.845392: step 3251, loss 0.122352, acc 0.953125, prec 0.0713079, recall 0.894394
2017-12-10T12:58:41.294558: step 3252, loss 0.0116702, acc 1, prec 0.0713079, recall 0.894394
2017-12-10T12:58:41.745996: step 3253, loss 0.378852, acc 0.953125, prec 0.0713265, recall 0.894428
2017-12-10T12:58:42.200724: step 3254, loss 0.0230877, acc 1, prec 0.0713265, recall 0.894428
2017-12-10T12:58:42.642620: step 3255, loss 0.677276, acc 0.984375, prec 0.0713729, recall 0.894497
2017-12-10T12:58:43.092793: step 3256, loss 0.225846, acc 0.984375, prec 0.0713952, recall 0.894531
2017-12-10T12:58:43.545983: step 3257, loss 0.147533, acc 0.953125, prec 0.0714137, recall 0.894566
2017-12-10T12:58:43.989830: step 3258, loss 0.12374, acc 0.96875, prec 0.07141, recall 0.894566
2017-12-10T12:58:44.423105: step 3259, loss 0.0801759, acc 0.96875, prec 0.0714063, recall 0.894566
2017-12-10T12:58:44.884406: step 3260, loss 0.0824872, acc 0.953125, prec 0.0714249, recall 0.8946
2017-12-10T12:58:45.334372: step 3261, loss 0.107777, acc 0.953125, prec 0.0714434, recall 0.894634
2017-12-10T12:58:45.784108: step 3262, loss 0.0186245, acc 1, prec 0.0714434, recall 0.894634
2017-12-10T12:58:46.225420: step 3263, loss 0.652929, acc 0.9375, prec 0.0715083, recall 0.894737
2017-12-10T12:58:46.673598: step 3264, loss 5.61691, acc 0.9375, prec 0.071551, recall 0.894515
2017-12-10T12:58:47.124078: step 3265, loss 0.159243, acc 0.9375, prec 0.0715676, recall 0.894549
2017-12-10T12:58:47.570644: step 3266, loss 0.289197, acc 0.953125, prec 0.0715621, recall 0.894549
2017-12-10T12:58:48.012785: step 3267, loss 0.216305, acc 0.921875, prec 0.0715528, recall 0.894549
2017-12-10T12:58:48.450648: step 3268, loss 0.180272, acc 0.921875, prec 0.0715676, recall 0.894583
2017-12-10T12:58:48.893946: step 3269, loss 0.216297, acc 0.9375, prec 0.0715602, recall 0.894583
2017-12-10T12:58:49.329991: step 3270, loss 0.327659, acc 0.90625, prec 0.0715972, recall 0.894652
2017-12-10T12:58:49.773472: step 3271, loss 0.494582, acc 0.890625, prec 0.0716323, recall 0.89472
2017-12-10T12:58:50.222357: step 3272, loss 0.471389, acc 0.859375, prec 0.0716638, recall 0.894788
2017-12-10T12:58:50.665184: step 3273, loss 0.283241, acc 0.90625, prec 0.0716526, recall 0.894788
2017-12-10T12:58:51.108038: step 3274, loss 0.344254, acc 0.90625, prec 0.0716415, recall 0.894788
2017-12-10T12:58:51.552419: step 3275, loss 0.374459, acc 0.875, prec 0.0716747, recall 0.894856
2017-12-10T12:58:52.001719: step 3276, loss 0.362341, acc 0.90625, prec 0.0716877, recall 0.89489
2017-12-10T12:58:52.437163: step 3277, loss 1.21629, acc 0.859375, prec 0.0717671, recall 0.895026
2017-12-10T12:58:52.889057: step 3278, loss 0.217408, acc 0.90625, prec 0.071756, recall 0.895026
2017-12-10T12:58:53.334941: step 3279, loss 0.518599, acc 0.84375, prec 0.0717374, recall 0.895026
2017-12-10T12:58:53.775688: step 3280, loss 0.414129, acc 0.9375, prec 0.071754, recall 0.89506
2017-12-10T12:58:54.237267: step 3281, loss 0.223826, acc 0.921875, prec 0.0717687, recall 0.895094
2017-12-10T12:58:54.681547: step 3282, loss 0.41001, acc 0.890625, prec 0.0717557, recall 0.895094
2017-12-10T12:58:55.132849: step 3283, loss 0.273122, acc 0.9375, prec 0.0717963, recall 0.895161
2017-12-10T12:58:55.581675: step 3284, loss 0.309009, acc 0.875, prec 0.0718295, recall 0.895229
2017-12-10T12:58:56.024948: step 3285, loss 0.46349, acc 0.875, prec 0.0718146, recall 0.895229
2017-12-10T12:58:56.467340: step 3286, loss 0.256532, acc 0.890625, prec 0.0718256, recall 0.895263
2017-12-10T12:58:56.921651: step 3287, loss 0.197946, acc 0.9375, prec 0.0718422, recall 0.895296
2017-12-10T12:58:57.365800: step 3288, loss 0.271936, acc 0.921875, prec 0.0718569, recall 0.89533
2017-12-10T12:58:57.812490: step 3289, loss 0.37236, acc 0.921875, prec 0.0718716, recall 0.895364
2017-12-10T12:58:58.248697: step 3290, loss 0.306482, acc 0.953125, prec 0.07189, recall 0.895397
2017-12-10T12:58:58.698584: step 3291, loss 0.370275, acc 0.90625, prec 0.0719029, recall 0.895431
2017-12-10T12:58:59.138826: step 3292, loss 0.107775, acc 0.96875, prec 0.0719711, recall 0.895532
2017-12-10T12:58:59.591013: step 3293, loss 0.181979, acc 0.953125, prec 0.0719895, recall 0.895566
2017-12-10T12:59:00.032319: step 3294, loss 0.438011, acc 0.890625, prec 0.0720244, recall 0.895633
2017-12-10T12:59:00.481449: step 3295, loss 0.260708, acc 0.921875, prec 0.072039, recall 0.895666
2017-12-10T12:59:00.925434: step 3296, loss 0.194713, acc 0.953125, prec 0.0720574, recall 0.8957
2017-12-10T12:59:01.372626: step 3297, loss 0.112667, acc 0.953125, prec 0.0720518, recall 0.8957
2017-12-10T12:59:01.807505: step 3298, loss 0.14384, acc 0.953125, prec 0.0720702, recall 0.895733
2017-12-10T12:59:02.245073: step 3299, loss 0.279431, acc 0.96875, prec 0.0720665, recall 0.895733
2017-12-10T12:59:02.699736: step 3300, loss 0.0924161, acc 0.96875, prec 0.0720628, recall 0.895733
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3300

2017-12-10T12:59:04.891434: step 3301, loss 0.119253, acc 0.953125, prec 0.0720811, recall 0.895766
2017-12-10T12:59:05.346954: step 3302, loss 0.0354197, acc 0.984375, prec 0.0720793, recall 0.895766
2017-12-10T12:59:05.806003: step 3303, loss 0.94039, acc 0.9375, prec 0.0720958, recall 0.8958
2017-12-10T12:59:06.261911: step 3304, loss 0.015017, acc 1, prec 0.0720958, recall 0.8958
2017-12-10T12:59:06.701909: step 3305, loss 0.0543566, acc 0.96875, prec 0.0720921, recall 0.8958
2017-12-10T12:59:07.150151: step 3306, loss 0.0276663, acc 0.984375, prec 0.0720902, recall 0.8958
2017-12-10T12:59:07.592487: step 3307, loss 0.027775, acc 1, prec 0.0720902, recall 0.8958
2017-12-10T12:59:08.050695: step 3308, loss 0.101326, acc 0.953125, prec 0.0721564, recall 0.8959
2017-12-10T12:59:08.503263: step 3309, loss 0.0311735, acc 0.984375, prec 0.0721546, recall 0.8959
2017-12-10T12:59:08.953193: step 3310, loss 0.0772173, acc 0.96875, prec 0.0721748, recall 0.895933
2017-12-10T12:59:09.394613: step 3311, loss 0.017602, acc 0.984375, prec 0.0721729, recall 0.895933
2017-12-10T12:59:09.838155: step 3312, loss 0.166309, acc 0.984375, prec 0.072195, recall 0.895967
2017-12-10T12:59:10.280609: step 3313, loss 0.117622, acc 0.96875, prec 0.0722152, recall 0.896
2017-12-10T12:59:10.718406: step 3314, loss 0.0585754, acc 0.984375, prec 0.0722851, recall 0.8961
2017-12-10T12:59:11.179189: step 3315, loss 3.68577, acc 0.921875, prec 0.0722777, recall 0.895813
2017-12-10T12:59:11.634353: step 3316, loss 0.0454055, acc 0.984375, prec 0.0722997, recall 0.895847
2017-12-10T12:59:12.073348: step 3317, loss 0.189041, acc 0.9375, prec 0.0723401, recall 0.895913
2017-12-10T12:59:12.513271: step 3318, loss 0.335845, acc 0.90625, prec 0.0723289, recall 0.895913
2017-12-10T12:59:12.955712: step 3319, loss 0.0515776, acc 0.96875, prec 0.072373, recall 0.89598
2017-12-10T12:59:13.392406: step 3320, loss 0.236179, acc 0.90625, prec 0.0723857, recall 0.896013
2017-12-10T12:59:13.844802: step 3321, loss 0.316174, acc 0.921875, prec 0.0723764, recall 0.896013
2017-12-10T12:59:14.298834: step 3322, loss 0.471151, acc 0.90625, prec 0.0723891, recall 0.896046
2017-12-10T12:59:14.745516: step 3323, loss 1.28721, acc 0.90625, prec 0.0724018, recall 0.896079
2017-12-10T12:59:15.191484: step 3324, loss 0.296798, acc 0.9375, prec 0.0724182, recall 0.896112
2017-12-10T12:59:15.642215: step 3325, loss 0.157129, acc 0.9375, prec 0.0724108, recall 0.896112
2017-12-10T12:59:16.077382: step 3326, loss 0.21747, acc 0.890625, prec 0.0724216, recall 0.896145
2017-12-10T12:59:16.517510: step 3327, loss 0.254072, acc 0.890625, prec 0.0724563, recall 0.896211
2017-12-10T12:59:16.968109: step 3328, loss 0.444567, acc 0.890625, prec 0.0725387, recall 0.896343
2017-12-10T12:59:17.408529: step 3329, loss 0.588276, acc 0.921875, prec 0.0725533, recall 0.896376
2017-12-10T12:59:17.845142: step 3330, loss 0.438291, acc 0.90625, prec 0.0725421, recall 0.896376
2017-12-10T12:59:18.287466: step 3331, loss 0.243574, acc 0.90625, prec 0.0725309, recall 0.896376
2017-12-10T12:59:18.717975: step 3332, loss 0.202438, acc 0.90625, prec 0.0726151, recall 0.896508
2017-12-10T12:59:19.159114: step 3333, loss 0.37267, acc 0.890625, prec 0.072602, recall 0.896508
2017-12-10T12:59:19.608348: step 3334, loss 0.322481, acc 0.875, prec 0.0726347, recall 0.896574
2017-12-10T12:59:20.052375: step 3335, loss 0.245656, acc 0.953125, prec 0.0726291, recall 0.896574
2017-12-10T12:59:20.504592: step 3336, loss 0.633895, acc 0.921875, prec 0.0726675, recall 0.896639
2017-12-10T12:59:20.967482: step 3337, loss 0.200037, acc 0.921875, prec 0.072682, recall 0.896672
2017-12-10T12:59:21.424606: step 3338, loss 0.522092, acc 0.875, prec 0.0726908, recall 0.896705
2017-12-10T12:59:21.857540: step 3339, loss 0.363282, acc 0.890625, prec 0.0727016, recall 0.896737
2017-12-10T12:59:22.300624: step 3340, loss 0.222264, acc 0.890625, prec 0.0727123, recall 0.89677
2017-12-10T12:59:22.748527: step 3341, loss 0.330726, acc 0.890625, prec 0.0727231, recall 0.896803
2017-12-10T12:59:23.196088: step 3342, loss 0.168408, acc 0.953125, prec 0.0727413, recall 0.896835
2017-12-10T12:59:23.637068: step 3343, loss 0.0486916, acc 0.984375, prec 0.0727632, recall 0.896868
2017-12-10T12:59:24.079742: step 3344, loss 0.345522, acc 0.9375, prec 0.0727795, recall 0.896901
2017-12-10T12:59:24.525883: step 3345, loss 0.394126, acc 0.921875, prec 0.0727702, recall 0.896901
2017-12-10T12:59:24.964006: step 3346, loss 0.120188, acc 0.96875, prec 0.072814, recall 0.896966
2017-12-10T12:59:25.416041: step 3347, loss 0.235608, acc 0.953125, prec 0.0728322, recall 0.896998
2017-12-10T12:59:25.863031: step 3348, loss 0.954969, acc 0.9375, prec 0.0728485, recall 0.897031
2017-12-10T12:59:26.310445: step 3349, loss 0.1157, acc 0.96875, prec 0.0728686, recall 0.897063
2017-12-10T12:59:26.756344: step 3350, loss 0.0439219, acc 1, prec 0.0728686, recall 0.897063
2017-12-10T12:59:27.202582: step 3351, loss 0.263151, acc 0.9375, prec 0.0728611, recall 0.897063
2017-12-10T12:59:27.654606: step 3352, loss 0.131987, acc 0.9375, prec 0.0728536, recall 0.897063
2017-12-10T12:59:28.100263: step 3353, loss 0.022376, acc 1, prec 0.0728774, recall 0.897096
2017-12-10T12:59:28.554507: step 3354, loss 0.197595, acc 0.953125, prec 0.0728956, recall 0.897128
2017-12-10T12:59:29.000101: step 3355, loss 0.221352, acc 0.921875, prec 0.07291, recall 0.897161
2017-12-10T12:59:29.449278: step 3356, loss 0.0841874, acc 0.96875, prec 0.0729063, recall 0.897161
2017-12-10T12:59:29.903188: step 3357, loss 0.174065, acc 0.96875, prec 0.0729025, recall 0.897161
2017-12-10T12:59:30.354821: step 3358, loss 0.200115, acc 0.96875, prec 0.0729225, recall 0.897193
2017-12-10T12:59:30.804262: step 3359, loss 0.120456, acc 0.96875, prec 0.0729426, recall 0.897226
2017-12-10T12:59:31.260863: step 3360, loss 0.144259, acc 0.984375, prec 0.0729407, recall 0.897226
2017-12-10T12:59:31.703794: step 3361, loss 9.16616, acc 0.921875, prec 0.072957, recall 0.896975
2017-12-10T12:59:32.158662: step 3362, loss 0.241584, acc 0.90625, prec 0.0729458, recall 0.896975
2017-12-10T12:59:32.595749: step 3363, loss 0.0694003, acc 0.984375, prec 0.0729439, recall 0.896975
2017-12-10T12:59:33.029736: step 3364, loss 0.402762, acc 0.9375, prec 0.0729602, recall 0.897008
2017-12-10T12:59:33.471804: step 3365, loss 0.12247, acc 0.9375, prec 0.0729764, recall 0.89704
2017-12-10T12:59:33.928484: step 3366, loss 0.146261, acc 0.96875, prec 0.0730439, recall 0.897137
2017-12-10T12:59:34.374790: step 3367, loss 0.221172, acc 0.9375, prec 0.0730364, recall 0.897137
2017-12-10T12:59:34.819897: step 3368, loss 0.429285, acc 0.921875, prec 0.0730508, recall 0.89717
2017-12-10T12:59:35.270731: step 3369, loss 0.0967751, acc 0.953125, prec 0.0730927, recall 0.897234
2017-12-10T12:59:35.693702: step 3370, loss 0.36593, acc 0.921875, prec 0.073107, recall 0.897267
2017-12-10T12:59:36.145986: step 3371, loss 0.183369, acc 0.921875, prec 0.0731214, recall 0.897299
2017-12-10T12:59:36.592589: step 3372, loss 0.256066, acc 0.875, prec 0.0731302, recall 0.897331
2017-12-10T12:59:37.037269: step 3373, loss 0.265309, acc 0.90625, prec 0.0731427, recall 0.897363
2017-12-10T12:59:37.489085: step 3374, loss 0.484804, acc 0.890625, prec 0.0731296, recall 0.897363
2017-12-10T12:59:37.953145: step 3375, loss 0.274619, acc 0.9375, prec 0.0731695, recall 0.897428
2017-12-10T12:59:38.405641: step 3376, loss 0.318897, acc 0.953125, prec 0.0731876, recall 0.89746
2017-12-10T12:59:38.856116: step 3377, loss 0.338432, acc 0.84375, prec 0.0731689, recall 0.89746
2017-12-10T12:59:39.295514: step 3378, loss 0.355412, acc 0.90625, prec 0.0731576, recall 0.89746
2017-12-10T12:59:39.740257: step 3379, loss 1.01603, acc 0.859375, prec 0.0732119, recall 0.897556
2017-12-10T12:59:40.185153: step 3380, loss 0.422559, acc 0.921875, prec 0.0732499, recall 0.897621
2017-12-10T12:59:40.635864: step 3381, loss 0.0954498, acc 0.953125, prec 0.0732679, recall 0.897653
2017-12-10T12:59:41.072089: step 3382, loss 0.180922, acc 0.953125, prec 0.073286, recall 0.897685
2017-12-10T12:59:41.509261: step 3383, loss 0.525251, acc 0.9375, prec 0.0733258, recall 0.897749
2017-12-10T12:59:41.962290: step 3384, loss 0.490475, acc 0.90625, prec 0.0733619, recall 0.897812
2017-12-10T12:59:42.400799: step 3385, loss 0.407619, acc 0.921875, prec 0.0733999, recall 0.897876
2017-12-10T12:59:42.844260: step 3386, loss 0.130763, acc 0.96875, prec 0.0734198, recall 0.897908
2017-12-10T12:59:43.288729: step 3387, loss 0.407507, acc 0.890625, prec 0.073454, recall 0.897972
2017-12-10T12:59:43.735495: step 3388, loss 0.126695, acc 0.953125, prec 0.073472, recall 0.898004
2017-12-10T12:59:44.183437: step 3389, loss 0.0711586, acc 0.96875, prec 0.0734919, recall 0.898036
2017-12-10T12:59:44.641271: step 3390, loss 0.40747, acc 0.890625, prec 0.0734788, recall 0.898036
2017-12-10T12:59:45.101625: step 3391, loss 0.0180146, acc 1, prec 0.0734788, recall 0.898036
2017-12-10T12:59:45.558921: step 3392, loss 0.143436, acc 0.953125, prec 0.0734968, recall 0.898067
2017-12-10T12:59:45.994799: step 3393, loss 0.503747, acc 0.875, prec 0.0734818, recall 0.898067
2017-12-10T12:59:46.438942: step 3394, loss 0.168534, acc 0.9375, prec 0.0734979, recall 0.898099
2017-12-10T12:59:46.885004: step 3395, loss 3.20648, acc 0.9375, prec 0.0735159, recall 0.897851
2017-12-10T12:59:47.329676: step 3396, loss 0.0925975, acc 0.953125, prec 0.0735103, recall 0.897851
2017-12-10T12:59:47.786764: step 3397, loss 0.0991071, acc 0.9375, prec 0.0735264, recall 0.897883
2017-12-10T12:59:48.238112: step 3398, loss 0.328354, acc 0.9375, prec 0.0735425, recall 0.897915
2017-12-10T12:59:48.694756: step 3399, loss 0.230024, acc 0.90625, prec 0.0735549, recall 0.897946
2017-12-10T12:59:49.141074: step 3400, loss 0.0751529, acc 0.984375, prec 0.0735766, recall 0.897978
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3400

2017-12-10T12:59:51.331031: step 3401, loss 0.314051, acc 0.90625, prec 0.073589, recall 0.89801
2017-12-10T12:59:51.774819: step 3402, loss 0.384868, acc 0.90625, prec 0.0736249, recall 0.898073
2017-12-10T12:59:52.206583: step 3403, loss 0.221578, acc 0.953125, prec 0.0736429, recall 0.898105
2017-12-10T12:59:52.645209: step 3404, loss 0.466602, acc 0.859375, prec 0.073626, recall 0.898105
2017-12-10T12:59:53.088438: step 3405, loss 0.0809571, acc 0.96875, prec 0.0736223, recall 0.898105
2017-12-10T12:59:53.530708: step 3406, loss 0.342249, acc 0.890625, prec 0.0736092, recall 0.898105
2017-12-10T12:59:53.975226: step 3407, loss 0.460104, acc 0.84375, prec 0.073614, recall 0.898137
2017-12-10T12:59:54.423962: step 3408, loss 0.122613, acc 0.953125, prec 0.0736084, recall 0.898137
2017-12-10T12:59:54.865375: step 3409, loss 0.277392, acc 0.9375, prec 0.0736245, recall 0.898168
2017-12-10T12:59:55.310224: step 3410, loss 0.143423, acc 0.953125, prec 0.0736424, recall 0.8982
2017-12-10T12:59:55.750931: step 3411, loss 0.198346, acc 0.90625, prec 0.0736312, recall 0.8982
2017-12-10T12:59:56.186441: step 3412, loss 0.312379, acc 0.953125, prec 0.0736256, recall 0.8982
2017-12-10T12:59:56.634993: step 3413, loss 0.152648, acc 0.9375, prec 0.0736652, recall 0.898263
2017-12-10T12:59:57.071685: step 3414, loss 0.0983157, acc 0.9375, prec 0.0736577, recall 0.898263
2017-12-10T12:59:57.521735: step 3415, loss 0.2305, acc 0.9375, prec 0.0736502, recall 0.898263
2017-12-10T12:59:57.970618: step 3416, loss 0.406524, acc 0.921875, prec 0.0736644, recall 0.898295
2017-12-10T12:59:58.416800: step 3417, loss 0.313604, acc 0.953125, prec 0.0736823, recall 0.898326
2017-12-10T12:59:58.864988: step 3418, loss 0.389624, acc 0.90625, prec 0.0736946, recall 0.898358
2017-12-10T12:59:59.308740: step 3419, loss 0.0878187, acc 0.953125, prec 0.0737126, recall 0.898389
2017-12-10T12:59:59.753704: step 3420, loss 0.164716, acc 0.953125, prec 0.073754, recall 0.898452
2017-12-10T13:00:00.199185: step 3421, loss 0.870056, acc 0.9375, prec 0.0737701, recall 0.898483
2017-12-10T13:00:00.638578: step 3422, loss 1.5432, acc 0.9375, prec 0.0738096, recall 0.898546
2017-12-10T13:00:01.088734: step 3423, loss 0.205656, acc 0.96875, prec 0.073853, recall 0.898609
2017-12-10T13:00:01.548000: step 3424, loss 0.0656491, acc 0.96875, prec 0.0738492, recall 0.898609
2017-12-10T13:00:02.005224: step 3425, loss 0.0897039, acc 0.984375, prec 0.0738473, recall 0.898609
2017-12-10T13:00:02.457350: step 3426, loss 2.82511, acc 0.9375, prec 0.0738652, recall 0.898363
2017-12-10T13:00:02.908846: step 3427, loss 0.344533, acc 0.921875, prec 0.0738558, recall 0.898363
2017-12-10T13:00:03.357912: step 3428, loss 1.48417, acc 0.875, prec 0.0738427, recall 0.898085
2017-12-10T13:00:03.804446: step 3429, loss 0.467616, acc 0.921875, prec 0.0738569, recall 0.898117
2017-12-10T13:00:04.253728: step 3430, loss 0.360737, acc 0.859375, prec 0.0738635, recall 0.898148
2017-12-10T13:00:04.705284: step 3431, loss 0.315959, acc 0.90625, prec 0.0738993, recall 0.898211
2017-12-10T13:00:05.146510: step 3432, loss 0.457193, acc 0.828125, prec 0.0739021, recall 0.898242
2017-12-10T13:00:05.597522: step 3433, loss 0.255162, acc 0.9375, prec 0.0739651, recall 0.898336
2017-12-10T13:00:06.042040: step 3434, loss 1.39109, acc 0.734375, prec 0.0739802, recall 0.898399
2017-12-10T13:00:06.494878: step 3435, loss 0.339362, acc 0.890625, prec 0.073967, recall 0.898399
2017-12-10T13:00:06.929161: step 3436, loss 0.594455, acc 0.90625, prec 0.0739793, recall 0.89843
2017-12-10T13:00:07.369820: step 3437, loss 1.00863, acc 0.875, prec 0.0740112, recall 0.898493
2017-12-10T13:00:07.814774: step 3438, loss 0.810163, acc 0.875, prec 0.0740197, recall 0.898524
2017-12-10T13:00:08.260336: step 3439, loss 0.563456, acc 0.828125, prec 0.073999, recall 0.898524
2017-12-10T13:00:08.700630: step 3440, loss 0.661118, acc 0.78125, prec 0.0739963, recall 0.898555
2017-12-10T13:00:09.144219: step 3441, loss 0.440264, acc 0.890625, prec 0.07403, recall 0.898618
2017-12-10T13:00:09.592050: step 3442, loss 0.338409, acc 0.921875, prec 0.0740441, recall 0.898649
2017-12-10T13:00:10.042842: step 3443, loss 0.290122, acc 0.890625, prec 0.0740544, recall 0.89868
2017-12-10T13:00:10.476654: step 3444, loss 0.309445, acc 0.90625, prec 0.0740666, recall 0.898711
2017-12-10T13:00:10.927592: step 3445, loss 0.628313, acc 0.859375, prec 0.0740497, recall 0.898711
2017-12-10T13:00:11.379629: step 3446, loss 0.118272, acc 0.9375, prec 0.0740656, recall 0.898742
2017-12-10T13:00:11.829343: step 3447, loss 0.15965, acc 0.9375, prec 0.0740582, recall 0.898742
2017-12-10T13:00:12.276991: step 3448, loss 0.172383, acc 0.953125, prec 0.0740759, recall 0.898773
2017-12-10T13:00:12.711667: step 3449, loss 0.183492, acc 0.953125, prec 0.0740937, recall 0.898804
2017-12-10T13:00:13.154056: step 3450, loss 0.219271, acc 0.9375, prec 0.0741097, recall 0.898835
2017-12-10T13:00:13.605365: step 3451, loss 0.223512, acc 0.921875, prec 0.0741237, recall 0.898866
2017-12-10T13:00:14.048947: step 3452, loss 0.0447187, acc 0.984375, prec 0.0741218, recall 0.898866
2017-12-10T13:00:14.502917: step 3453, loss 0.314794, acc 0.9375, prec 0.0741845, recall 0.898959
2017-12-10T13:00:14.973245: step 3454, loss 0.0610513, acc 0.953125, prec 0.0741789, recall 0.898959
2017-12-10T13:00:15.418137: step 3455, loss 0.244869, acc 0.953125, prec 0.0741966, recall 0.89899
2017-12-10T13:00:15.877066: step 3456, loss 0.052868, acc 0.984375, prec 0.0742182, recall 0.899021
2017-12-10T13:00:16.324660: step 3457, loss 0.00334736, acc 1, prec 0.0742182, recall 0.899021
2017-12-10T13:00:16.777679: step 3458, loss 0.0662835, acc 0.96875, prec 0.0742144, recall 0.899021
2017-12-10T13:00:17.213145: step 3459, loss 0.24118, acc 0.953125, prec 0.0742555, recall 0.899083
2017-12-10T13:00:17.653427: step 3460, loss 0.0401065, acc 0.984375, prec 0.0743004, recall 0.899144
2017-12-10T13:00:18.095910: step 3461, loss 0.0276615, acc 0.984375, prec 0.0743219, recall 0.899175
2017-12-10T13:00:18.546592: step 3462, loss 1.18527, acc 0.9375, prec 0.0743612, recall 0.899237
2017-12-10T13:00:18.995146: step 3463, loss 0.192975, acc 0.96875, prec 0.0743574, recall 0.899237
2017-12-10T13:00:19.429739: step 3464, loss 0.100275, acc 0.96875, prec 0.074377, recall 0.899267
2017-12-10T13:00:19.886531: step 3465, loss 0.0598393, acc 1, prec 0.0744238, recall 0.899329
2017-12-10T13:00:20.330398: step 3466, loss 0.128282, acc 0.96875, prec 0.07442, recall 0.899329
2017-12-10T13:00:20.766122: step 3467, loss 1.20229, acc 0.984375, prec 0.07442, recall 0.899055
2017-12-10T13:00:21.205542: step 3468, loss 0.239096, acc 0.984375, prec 0.0744882, recall 0.899147
2017-12-10T13:00:21.667560: step 3469, loss 0.204435, acc 0.96875, prec 0.0745312, recall 0.899208
2017-12-10T13:00:22.120428: step 3470, loss 0.0180541, acc 1, prec 0.0745545, recall 0.899239
2017-12-10T13:00:22.561291: step 3471, loss 0.24506, acc 0.96875, prec 0.0745508, recall 0.899239
2017-12-10T13:00:23.011771: step 3472, loss 0.506311, acc 1, prec 0.0745975, recall 0.8993
2017-12-10T13:00:23.454719: step 3473, loss 0.165901, acc 0.9375, prec 0.07459, recall 0.8993
2017-12-10T13:00:23.887655: step 3474, loss 0.301639, acc 0.953125, prec 0.0745843, recall 0.8993
2017-12-10T13:00:24.328236: step 3475, loss 0.0291034, acc 0.984375, prec 0.0745824, recall 0.8993
2017-12-10T13:00:24.770965: step 3476, loss 0.0861164, acc 0.984375, prec 0.0746272, recall 0.899361
2017-12-10T13:00:25.207895: step 3477, loss 0.13483, acc 0.984375, prec 0.0746254, recall 0.899361
2017-12-10T13:00:25.650941: step 3478, loss 0.154463, acc 0.9375, prec 0.0746412, recall 0.899392
2017-12-10T13:00:26.050045: step 3479, loss 0.0165881, acc 1, prec 0.0746412, recall 0.899392
2017-12-10T13:00:26.497140: step 3480, loss 0.0568124, acc 0.984375, prec 0.0746626, recall 0.899423
2017-12-10T13:00:26.941339: step 3481, loss 0.110263, acc 1, prec 0.0747326, recall 0.899514
2017-12-10T13:00:27.381635: step 3482, loss 0.0459181, acc 0.984375, prec 0.0747308, recall 0.899514
2017-12-10T13:00:27.814793: step 3483, loss 0.183104, acc 0.984375, prec 0.0747522, recall 0.899545
2017-12-10T13:00:28.256492: step 3484, loss 0.0985501, acc 0.953125, prec 0.0747932, recall 0.899606
2017-12-10T13:00:28.701219: step 3485, loss 0.0397737, acc 0.984375, prec 0.0747913, recall 0.899606
2017-12-10T13:00:29.146547: step 3486, loss 0.144979, acc 0.96875, prec 0.0747876, recall 0.899606
2017-12-10T13:00:29.594579: step 3487, loss 0.0784881, acc 0.96875, prec 0.0747838, recall 0.899606
2017-12-10T13:00:30.024931: step 3488, loss 0.512629, acc 0.921875, prec 0.0747977, recall 0.899636
2017-12-10T13:00:30.465152: step 3489, loss 0.0396165, acc 0.984375, prec 0.0748425, recall 0.899697
2017-12-10T13:00:30.899407: step 3490, loss 0.097867, acc 0.96875, prec 0.0748387, recall 0.899697
2017-12-10T13:00:31.350136: step 3491, loss 0.0899238, acc 0.96875, prec 0.0748349, recall 0.899697
2017-12-10T13:00:31.796128: step 3492, loss 0.227619, acc 0.96875, prec 0.0748544, recall 0.899727
2017-12-10T13:00:32.244835: step 3493, loss 0.101731, acc 0.953125, prec 0.0748721, recall 0.899758
2017-12-10T13:00:32.683945: step 3494, loss 0.259986, acc 0.984375, prec 0.0749168, recall 0.899818
2017-12-10T13:00:33.142641: step 3495, loss 1.96765, acc 0.9375, prec 0.0749112, recall 0.899546
2017-12-10T13:00:33.598989: step 3496, loss 0.188462, acc 0.9375, prec 0.0749036, recall 0.899546
2017-12-10T13:00:34.045791: step 3497, loss 0.311768, acc 0.96875, prec 0.0749465, recall 0.899607
2017-12-10T13:00:34.500781: step 3498, loss 0.0949097, acc 0.953125, prec 0.0749641, recall 0.899637
2017-12-10T13:00:34.949724: step 3499, loss 0.304726, acc 0.921875, prec 0.074978, recall 0.899668
2017-12-10T13:00:35.400437: step 3500, loss 0.0430707, acc 0.96875, prec 0.0749742, recall 0.899668
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3500

2017-12-10T13:00:37.350255: step 3501, loss 0.0510759, acc 0.984375, prec 0.0750655, recall 0.899789
2017-12-10T13:00:37.817620: step 3502, loss 0.382173, acc 0.984375, prec 0.0750869, recall 0.899819
2017-12-10T13:00:38.273460: step 3503, loss 0.524315, acc 0.953125, prec 0.0751278, recall 0.899879
2017-12-10T13:00:38.737217: step 3504, loss 0.0562139, acc 0.96875, prec 0.075124, recall 0.899879
2017-12-10T13:00:39.178506: step 3505, loss 0.176345, acc 0.96875, prec 0.0751202, recall 0.899879
2017-12-10T13:00:39.624791: step 3506, loss 0.193937, acc 0.953125, prec 0.0751378, recall 0.89991
2017-12-10T13:00:40.069175: step 3507, loss 0.852823, acc 0.9375, prec 0.0751535, recall 0.89994
2017-12-10T13:00:40.519077: step 3508, loss 0.065409, acc 0.96875, prec 0.0751497, recall 0.89994
2017-12-10T13:00:40.962827: step 3509, loss 0.0903141, acc 0.96875, prec 0.0751692, recall 0.89997
2017-12-10T13:00:41.420356: step 3510, loss 0.14134, acc 0.953125, prec 0.0751868, recall 0.9
2017-12-10T13:00:41.867623: step 3511, loss 0.147887, acc 0.953125, prec 0.0751812, recall 0.9
2017-12-10T13:00:42.305534: step 3512, loss 0.185272, acc 0.96875, prec 0.0751774, recall 0.9
2017-12-10T13:00:42.745995: step 3513, loss 0.0955546, acc 0.96875, prec 0.0751969, recall 0.90003
2017-12-10T13:00:43.184292: step 3514, loss 0.0999477, acc 0.96875, prec 0.0751931, recall 0.90003
2017-12-10T13:00:43.629936: step 3515, loss 0.0644225, acc 0.984375, prec 0.0751912, recall 0.90003
2017-12-10T13:00:44.074408: step 3516, loss 0.284361, acc 0.96875, prec 0.0751874, recall 0.90003
2017-12-10T13:00:44.522708: step 3517, loss 0.124841, acc 0.953125, prec 0.0751817, recall 0.90003
2017-12-10T13:00:44.963987: step 3518, loss 0.161367, acc 0.9375, prec 0.0752207, recall 0.90009
2017-12-10T13:00:45.412728: step 3519, loss 0.140558, acc 0.984375, prec 0.0752886, recall 0.90018
2017-12-10T13:00:45.862690: step 3520, loss 0.114103, acc 0.953125, prec 0.0753061, recall 0.90021
2017-12-10T13:00:46.307296: step 3521, loss 0.0289922, acc 0.984375, prec 0.0753275, recall 0.90024
2017-12-10T13:00:46.757507: step 3522, loss 0.0456105, acc 0.984375, prec 0.0753256, recall 0.90024
2017-12-10T13:00:47.222957: step 3523, loss 0.154356, acc 0.96875, prec 0.075345, recall 0.90027
2017-12-10T13:00:47.681048: step 3524, loss 0.144303, acc 0.984375, prec 0.0753664, recall 0.9003
2017-12-10T13:00:48.137239: step 3525, loss 0.113826, acc 0.96875, prec 0.0753859, recall 0.90033
2017-12-10T13:00:48.594124: step 3526, loss 0.121248, acc 0.953125, prec 0.0754034, recall 0.90036
2017-12-10T13:00:49.032648: step 3527, loss 0.210772, acc 0.9375, prec 0.0754191, recall 0.90039
2017-12-10T13:00:49.471656: step 3528, loss 0.126446, acc 0.984375, prec 0.0754404, recall 0.90042
2017-12-10T13:00:49.910200: step 3529, loss 0.148006, acc 0.953125, prec 0.0754812, recall 0.90048
2017-12-10T13:00:50.353159: step 3530, loss 0.167918, acc 0.96875, prec 0.0755238, recall 0.900539
2017-12-10T13:00:50.799828: step 3531, loss 0.0586504, acc 0.984375, prec 0.0755219, recall 0.900539
2017-12-10T13:00:51.241992: step 3532, loss 0.0545161, acc 0.984375, prec 0.0755433, recall 0.900569
2017-12-10T13:00:51.698128: step 3533, loss 0.0372299, acc 0.984375, prec 0.0755646, recall 0.900599
2017-12-10T13:00:52.139061: step 3534, loss 1.07156, acc 1, prec 0.0755878, recall 0.900629
2017-12-10T13:00:52.604282: step 3535, loss 0.166616, acc 0.984375, prec 0.0755859, recall 0.900629
2017-12-10T13:00:53.049285: step 3536, loss 0.0312308, acc 1, prec 0.0756324, recall 0.900688
2017-12-10T13:00:53.480218: step 3537, loss 0.139662, acc 0.96875, prec 0.075675, recall 0.900747
2017-12-10T13:00:53.930393: step 3538, loss 0.155376, acc 0.953125, prec 0.0756925, recall 0.900777
2017-12-10T13:00:54.379408: step 3539, loss 0.0878867, acc 0.96875, prec 0.0756887, recall 0.900777
2017-12-10T13:00:54.818822: step 3540, loss 0.0873392, acc 0.96875, prec 0.0757081, recall 0.900807
2017-12-10T13:00:55.270487: step 3541, loss 0.117938, acc 0.953125, prec 0.0757024, recall 0.900807
2017-12-10T13:00:55.717530: step 3542, loss 0.0180195, acc 1, prec 0.0757024, recall 0.900807
2017-12-10T13:00:56.165319: step 3543, loss 0.159054, acc 0.921875, prec 0.0756929, recall 0.900807
2017-12-10T13:00:56.614163: step 3544, loss 0.107404, acc 0.984375, prec 0.075691, recall 0.900807
2017-12-10T13:00:57.060690: step 3545, loss 0.0787818, acc 0.953125, prec 0.0757085, recall 0.900836
2017-12-10T13:00:57.525007: step 3546, loss 0.0407387, acc 0.984375, prec 0.0757066, recall 0.900836
2017-12-10T13:00:57.966268: step 3547, loss 0.142848, acc 0.953125, prec 0.0757009, recall 0.900836
2017-12-10T13:00:58.414509: step 3548, loss 0.0534958, acc 0.953125, prec 0.0756952, recall 0.900836
2017-12-10T13:00:58.862800: step 3549, loss 0.148754, acc 0.96875, prec 0.0757146, recall 0.900866
2017-12-10T13:00:59.304032: step 3550, loss 0.131894, acc 0.984375, prec 0.0757591, recall 0.900925
2017-12-10T13:00:59.745553: step 3551, loss 0.107398, acc 0.96875, prec 0.0757553, recall 0.900925
2017-12-10T13:01:00.191781: step 3552, loss 1.08223, acc 0.953125, prec 0.0757728, recall 0.900955
2017-12-10T13:01:00.630149: step 3553, loss 0.0759813, acc 0.953125, prec 0.0757903, recall 0.900984
2017-12-10T13:01:01.075189: step 3554, loss 0.0162067, acc 1, prec 0.0757903, recall 0.900984
2017-12-10T13:01:01.522069: step 3555, loss 0.379692, acc 0.96875, prec 0.075856, recall 0.901073
2017-12-10T13:01:01.970724: step 3556, loss 0.0623622, acc 0.984375, prec 0.0758773, recall 0.901102
2017-12-10T13:01:02.419786: step 3557, loss 0.0480629, acc 0.984375, prec 0.0758754, recall 0.901102
2017-12-10T13:01:02.858751: step 3558, loss 0.0686042, acc 0.984375, prec 0.0758735, recall 0.901102
2017-12-10T13:01:03.302069: step 3559, loss 0.190114, acc 0.9375, prec 0.0758891, recall 0.901132
2017-12-10T13:01:03.753434: step 3560, loss 0.128674, acc 0.921875, prec 0.0758795, recall 0.901132
2017-12-10T13:01:04.201726: step 3561, loss 0.155466, acc 0.96875, prec 0.0758989, recall 0.901161
2017-12-10T13:01:04.648226: step 3562, loss 0.0873408, acc 0.9375, prec 0.0758913, recall 0.901161
2017-12-10T13:01:05.090819: step 3563, loss 0.14381, acc 0.921875, prec 0.0759049, recall 0.90119
2017-12-10T13:01:05.529677: step 3564, loss 0.236619, acc 0.953125, prec 0.0759224, recall 0.90122
2017-12-10T13:01:05.972686: step 3565, loss 0.143677, acc 0.9375, prec 0.0759611, recall 0.901279
2017-12-10T13:01:06.424076: step 3566, loss 0.335007, acc 0.921875, prec 0.0759747, recall 0.901308
2017-12-10T13:01:06.872790: step 3567, loss 0.0678439, acc 0.984375, prec 0.0759728, recall 0.901308
2017-12-10T13:01:07.323673: step 3568, loss 0.135107, acc 0.953125, prec 0.0759903, recall 0.901337
2017-12-10T13:01:07.774035: step 3569, loss 0.349352, acc 0.890625, prec 0.0760001, recall 0.901367
2017-12-10T13:01:08.214475: step 3570, loss 0.0786945, acc 0.953125, prec 0.0760175, recall 0.901396
2017-12-10T13:01:08.656274: step 3571, loss 0.191345, acc 0.953125, prec 0.076035, recall 0.901425
2017-12-10T13:01:09.096544: step 3572, loss 0.81405, acc 0.96875, prec 0.0760543, recall 0.901454
2017-12-10T13:01:09.544179: step 3573, loss 0.803758, acc 0.984375, prec 0.0761218, recall 0.901542
2017-12-10T13:01:09.990879: step 3574, loss 0.10114, acc 0.953125, prec 0.0761161, recall 0.901542
2017-12-10T13:01:10.437178: step 3575, loss 0.0926721, acc 0.984375, prec 0.0761142, recall 0.901542
2017-12-10T13:01:10.884286: step 3576, loss 0.196799, acc 0.953125, prec 0.0761085, recall 0.901542
2017-12-10T13:01:11.323552: step 3577, loss 0.0663717, acc 0.96875, prec 0.0761278, recall 0.901571
2017-12-10T13:01:11.765378: step 3578, loss 0.23725, acc 0.953125, prec 0.0761221, recall 0.901571
2017-12-10T13:01:12.203864: step 3579, loss 0.390643, acc 0.90625, prec 0.0761569, recall 0.90163
2017-12-10T13:01:12.657677: step 3580, loss 0.173251, acc 0.921875, prec 0.0762167, recall 0.901717
2017-12-10T13:01:13.094735: step 3581, loss 0.36532, acc 0.875, prec 0.0762477, recall 0.901775
2017-12-10T13:01:13.537993: step 3582, loss 0.122868, acc 0.953125, prec 0.076265, recall 0.901804
2017-12-10T13:01:13.983677: step 3583, loss 0.28517, acc 0.875, prec 0.0762498, recall 0.901804
2017-12-10T13:01:14.439964: step 3584, loss 0.254645, acc 0.921875, prec 0.0762402, recall 0.901804
2017-12-10T13:01:14.881105: step 3585, loss 0.0845239, acc 0.96875, prec 0.0762595, recall 0.901833
2017-12-10T13:01:15.343526: step 3586, loss 0.126318, acc 0.96875, prec 0.0762788, recall 0.901862
2017-12-10T13:01:15.793926: step 3587, loss 0.173282, acc 0.96875, prec 0.0762981, recall 0.901891
2017-12-10T13:01:16.256699: step 3588, loss 0.400526, acc 0.953125, prec 0.0763386, recall 0.901949
2017-12-10T13:01:16.710849: step 3589, loss 0.171214, acc 0.953125, prec 0.0763559, recall 0.901978
2017-12-10T13:01:17.160248: step 3590, loss 0.253758, acc 0.9375, prec 0.0763714, recall 0.902007
2017-12-10T13:01:17.606335: step 3591, loss 0.174304, acc 0.984375, prec 0.0764387, recall 0.902094
2017-12-10T13:01:18.055561: step 3592, loss 0.092889, acc 0.953125, prec 0.076456, recall 0.902123
2017-12-10T13:01:18.509751: step 3593, loss 0.104259, acc 0.96875, prec 0.0764984, recall 0.90218
2017-12-10T13:01:18.959843: step 3594, loss 0.161157, acc 0.96875, prec 0.0765407, recall 0.902238
2017-12-10T13:01:19.408286: step 3595, loss 0.0493081, acc 0.96875, prec 0.0765369, recall 0.902238
2017-12-10T13:01:19.848370: step 3596, loss 0.0781319, acc 0.984375, prec 0.0765811, recall 0.902295
2017-12-10T13:01:20.282279: step 3597, loss 0.147375, acc 0.96875, prec 0.0766003, recall 0.902324
2017-12-10T13:01:20.726540: step 3598, loss 0.149742, acc 0.9375, prec 0.0765927, recall 0.902324
2017-12-10T13:01:21.176368: step 3599, loss 0.457478, acc 0.96875, prec 0.076658, recall 0.90241
2017-12-10T13:01:21.629681: step 3600, loss 0.223491, acc 0.9375, prec 0.0767426, recall 0.902525
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3600

2017-12-10T13:01:23.501566: step 3601, loss 0.051229, acc 1, prec 0.0767887, recall 0.902582
2017-12-10T13:01:23.960031: step 3602, loss 0.306324, acc 0.953125, prec 0.0768059, recall 0.902611
2017-12-10T13:01:24.411433: step 3603, loss 0.127812, acc 0.953125, prec 0.0768002, recall 0.902611
2017-12-10T13:01:24.861304: step 3604, loss 0.0858634, acc 0.984375, prec 0.0767983, recall 0.902611
2017-12-10T13:01:25.312784: step 3605, loss 0.20252, acc 0.9375, prec 0.0767906, recall 0.902611
2017-12-10T13:01:25.761529: step 3606, loss 0.0315815, acc 0.984375, prec 0.0767887, recall 0.902611
2017-12-10T13:01:26.209337: step 3607, loss 0.0318221, acc 1, prec 0.0767887, recall 0.902611
2017-12-10T13:01:26.660454: step 3608, loss 0.0608972, acc 0.96875, prec 0.0768079, recall 0.902639
2017-12-10T13:01:27.106695: step 3609, loss 1.85094, acc 0.921875, prec 0.0768693, recall 0.90246
2017-12-10T13:01:27.573116: step 3610, loss 0.17779, acc 0.921875, prec 0.0768828, recall 0.902489
2017-12-10T13:01:28.009627: step 3611, loss 0.0579054, acc 0.984375, prec 0.0769039, recall 0.902518
2017-12-10T13:01:28.450821: step 3612, loss 0.11553, acc 0.921875, prec 0.0769173, recall 0.902546
2017-12-10T13:01:28.889893: step 3613, loss 0.135057, acc 0.90625, prec 0.0769058, recall 0.902546
2017-12-10T13:01:29.345766: step 3614, loss 0.101942, acc 0.953125, prec 0.0769001, recall 0.902546
2017-12-10T13:01:29.796717: step 3615, loss 0.0357851, acc 0.984375, prec 0.0769212, recall 0.902575
2017-12-10T13:01:30.244876: step 3616, loss 0.252233, acc 0.9375, prec 0.0769595, recall 0.902632
2017-12-10T13:01:30.687209: step 3617, loss 0.42231, acc 0.890625, prec 0.0769921, recall 0.902689
2017-12-10T13:01:31.130843: step 3618, loss 0.357592, acc 0.921875, prec 0.0769825, recall 0.902689
2017-12-10T13:01:31.601971: step 3619, loss 0.263188, acc 0.953125, prec 0.0770228, recall 0.902745
2017-12-10T13:01:32.049730: step 3620, loss 0.242347, acc 0.953125, prec 0.077063, recall 0.902802
2017-12-10T13:01:32.501985: step 3621, loss 0.0444421, acc 1, prec 0.077086, recall 0.90283
2017-12-10T13:01:32.954039: step 3622, loss 0.562422, acc 0.859375, prec 0.0770917, recall 0.902859
2017-12-10T13:01:33.399550: step 3623, loss 0.058391, acc 0.96875, prec 0.0770878, recall 0.902859
2017-12-10T13:01:33.839800: step 3624, loss 0.362871, acc 0.890625, prec 0.0770974, recall 0.902887
2017-12-10T13:01:34.286584: step 3625, loss 0.241528, acc 0.90625, prec 0.0771089, recall 0.902915
2017-12-10T13:01:34.734279: step 3626, loss 0.161916, acc 0.9375, prec 0.0771241, recall 0.902944
2017-12-10T13:01:35.169989: step 3627, loss 0.0718103, acc 0.984375, prec 0.0771452, recall 0.902972
2017-12-10T13:01:35.612410: step 3628, loss 0.294466, acc 0.921875, prec 0.0771356, recall 0.902972
2017-12-10T13:01:36.043783: step 3629, loss 0.451738, acc 0.96875, prec 0.0772236, recall 0.903085
2017-12-10T13:01:36.488101: step 3630, loss 0.0343258, acc 1, prec 0.0772466, recall 0.903113
2017-12-10T13:01:36.937143: step 3631, loss 0.144533, acc 0.953125, prec 0.0772868, recall 0.90317
2017-12-10T13:01:37.389378: step 3632, loss 0.176074, acc 0.984375, prec 0.0773307, recall 0.903226
2017-12-10T13:01:37.834168: step 3633, loss 0.0898551, acc 0.953125, prec 0.0773709, recall 0.903282
2017-12-10T13:01:38.287011: step 3634, loss 0.0814803, acc 0.96875, prec 0.07739, recall 0.90331
2017-12-10T13:01:38.727602: step 3635, loss 0.0226433, acc 1, prec 0.0774129, recall 0.903338
2017-12-10T13:01:39.173479: step 3636, loss 0.180231, acc 0.96875, prec 0.077432, recall 0.903366
2017-12-10T13:01:39.628822: step 3637, loss 0.207551, acc 0.9375, prec 0.0774473, recall 0.903394
2017-12-10T13:01:40.083008: step 3638, loss 0.142093, acc 0.9375, prec 0.0774854, recall 0.90345
2017-12-10T13:01:40.535061: step 3639, loss 0.13845, acc 0.9375, prec 0.0774777, recall 0.90345
2017-12-10T13:01:40.985462: step 3640, loss 0.104528, acc 0.96875, prec 0.0774739, recall 0.90345
2017-12-10T13:01:41.435041: step 3641, loss 0.19596, acc 0.96875, prec 0.07747, recall 0.90345
2017-12-10T13:01:41.884224: step 3642, loss 0.203788, acc 0.953125, prec 0.0774872, recall 0.903478
2017-12-10T13:01:42.334006: step 3643, loss 0.0249953, acc 1, prec 0.0775101, recall 0.903506
2017-12-10T13:01:42.781771: step 3644, loss 0.211257, acc 0.984375, prec 0.077577, recall 0.90359
2017-12-10T13:01:43.229788: step 3645, loss 0.0812093, acc 0.96875, prec 0.0775731, recall 0.90359
2017-12-10T13:01:43.676780: step 3646, loss 0.0180659, acc 1, prec 0.0776419, recall 0.903674
2017-12-10T13:01:44.110629: step 3647, loss 0.132402, acc 0.953125, prec 0.077659, recall 0.903702
2017-12-10T13:01:44.562281: step 3648, loss 5.4522, acc 0.984375, prec 0.0777049, recall 0.903496
2017-12-10T13:01:45.009600: step 3649, loss 0.116726, acc 0.984375, prec 0.077703, recall 0.903496
2017-12-10T13:01:45.457033: step 3650, loss 0.47994, acc 0.984375, prec 0.0777239, recall 0.903524
2017-12-10T13:01:45.894494: step 3651, loss 0.133529, acc 0.953125, prec 0.077764, recall 0.90358
2017-12-10T13:01:46.344522: step 3652, loss 0.0930799, acc 0.953125, prec 0.077804, recall 0.903635
2017-12-10T13:01:46.775645: step 3653, loss 0.29491, acc 0.90625, prec 0.0778382, recall 0.903691
2017-12-10T13:01:47.215480: step 3654, loss 0.500205, acc 0.96875, prec 0.0778802, recall 0.903746
2017-12-10T13:01:47.652797: step 3655, loss 0.4722, acc 0.875, prec 0.0778876, recall 0.903774
2017-12-10T13:01:48.106604: step 3656, loss 0.324722, acc 0.9375, prec 0.0779027, recall 0.903802
2017-12-10T13:01:48.545975: step 3657, loss 0.20161, acc 0.9375, prec 0.0779179, recall 0.90383
2017-12-10T13:01:48.991577: step 3658, loss 0.269021, acc 0.96875, prec 0.0780056, recall 0.90394
2017-12-10T13:01:49.442341: step 3659, loss 0.284343, acc 0.921875, prec 0.0779959, recall 0.90394
2017-12-10T13:01:49.891624: step 3660, loss 0.435101, acc 0.84375, prec 0.0779994, recall 0.903968
2017-12-10T13:01:50.341885: step 3661, loss 0.265047, acc 0.90625, prec 0.0779878, recall 0.903968
2017-12-10T13:01:50.776287: step 3662, loss 0.570711, acc 0.84375, prec 0.0779685, recall 0.903968
2017-12-10T13:01:51.225226: step 3663, loss 0.191926, acc 0.921875, prec 0.0780045, recall 0.904023
2017-12-10T13:01:51.662378: step 3664, loss 0.108745, acc 0.953125, prec 0.0780216, recall 0.904051
2017-12-10T13:01:52.099265: step 3665, loss 0.648735, acc 0.828125, prec 0.0780003, recall 0.904051
2017-12-10T13:01:52.531843: step 3666, loss 0.224709, acc 0.90625, prec 0.0780344, recall 0.904106
2017-12-10T13:01:52.969061: step 3667, loss 0.377433, acc 0.921875, prec 0.0780247, recall 0.904106
2017-12-10T13:01:53.415725: step 3668, loss 0.608768, acc 0.859375, prec 0.0780302, recall 0.904133
2017-12-10T13:01:53.858919: step 3669, loss 0.163571, acc 0.953125, prec 0.0780929, recall 0.904216
2017-12-10T13:01:54.300194: step 3670, loss 0.389241, acc 0.859375, prec 0.0780983, recall 0.904243
2017-12-10T13:01:54.751706: step 3671, loss 0.566368, acc 0.890625, prec 0.0781076, recall 0.904271
2017-12-10T13:01:55.202979: step 3672, loss 0.172674, acc 0.9375, prec 0.0781227, recall 0.904298
2017-12-10T13:01:55.655946: step 3673, loss 0.403737, acc 0.890625, prec 0.078132, recall 0.904325
2017-12-10T13:01:56.083125: step 3674, loss 0.259855, acc 0.921875, prec 0.0781451, recall 0.904353
2017-12-10T13:01:56.530189: step 3675, loss 0.245473, acc 0.9375, prec 0.0781374, recall 0.904353
2017-12-10T13:01:56.980142: step 3676, loss 0.0759307, acc 0.953125, prec 0.0781316, recall 0.904353
2017-12-10T13:01:57.427229: step 3677, loss 0.260335, acc 0.9375, prec 0.0781695, recall 0.904408
2017-12-10T13:01:57.874987: step 3678, loss 0.160923, acc 0.9375, prec 0.0781617, recall 0.904408
2017-12-10T13:01:58.310563: step 3679, loss 0.0981491, acc 0.96875, prec 0.0781578, recall 0.904408
2017-12-10T13:01:58.763457: step 3680, loss 1.29689, acc 0.96875, prec 0.0781559, recall 0.904149
2017-12-10T13:01:59.219415: step 3681, loss 0.14153, acc 0.96875, prec 0.0781749, recall 0.904176
2017-12-10T13:01:59.667048: step 3682, loss 0.0903331, acc 0.984375, prec 0.0782185, recall 0.904231
2017-12-10T13:02:00.118223: step 3683, loss 0.241182, acc 0.9375, prec 0.0782336, recall 0.904258
2017-12-10T13:02:00.561321: step 3684, loss 0.685769, acc 0.921875, prec 0.0782695, recall 0.904313
2017-12-10T13:02:01.000119: step 3685, loss 0.237725, acc 0.953125, prec 0.0782637, recall 0.904313
2017-12-10T13:02:01.455656: step 3686, loss 0.0735321, acc 0.96875, prec 0.0782598, recall 0.904313
2017-12-10T13:02:01.908487: step 3687, loss 0.153002, acc 0.9375, prec 0.0782521, recall 0.904313
2017-12-10T13:02:02.364773: step 3688, loss 0.188787, acc 0.921875, prec 0.0782424, recall 0.904313
2017-12-10T13:02:02.817345: step 3689, loss 0.266417, acc 0.953125, prec 0.0782366, recall 0.904313
2017-12-10T13:02:03.283281: step 3690, loss 0.170781, acc 0.9375, prec 0.0782516, recall 0.90434
2017-12-10T13:02:03.725204: step 3691, loss 0.193856, acc 0.921875, prec 0.078242, recall 0.90434
2017-12-10T13:02:04.168806: step 3692, loss 0.114997, acc 0.953125, prec 0.0782589, recall 0.904368
2017-12-10T13:02:04.604509: step 3693, loss 0.0830126, acc 0.96875, prec 0.0782551, recall 0.904368
2017-12-10T13:02:05.045613: step 3694, loss 0.175212, acc 0.9375, prec 0.0782473, recall 0.904368
2017-12-10T13:02:05.481691: step 3695, loss 0.275734, acc 0.9375, prec 0.0782396, recall 0.904368
2017-12-10T13:02:05.922568: step 3696, loss 0.199237, acc 0.890625, prec 0.0782261, recall 0.904368
2017-12-10T13:02:06.370391: step 3697, loss 0.0330356, acc 1, prec 0.0782261, recall 0.904368
2017-12-10T13:02:06.813841: step 3698, loss 0.107779, acc 0.953125, prec 0.0782658, recall 0.904422
2017-12-10T13:02:07.260423: step 3699, loss 0.244587, acc 0.96875, prec 0.0783074, recall 0.904477
2017-12-10T13:02:07.693615: step 3700, loss 0.635272, acc 0.875, prec 0.078292, recall 0.904477
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3700

2017-12-10T13:02:09.697949: step 3701, loss 0.475881, acc 0.9375, prec 0.078307, recall 0.904504
2017-12-10T13:02:10.135542: step 3702, loss 0.241931, acc 0.984375, prec 0.0783733, recall 0.904586
2017-12-10T13:02:10.578144: step 3703, loss 0.0718378, acc 0.96875, prec 0.0783922, recall 0.904613
2017-12-10T13:02:11.034084: step 3704, loss 0.06636, acc 0.953125, prec 0.0783864, recall 0.904613
2017-12-10T13:02:11.473294: step 3705, loss 0.0169201, acc 1, prec 0.0783864, recall 0.904613
2017-12-10T13:02:11.911511: step 3706, loss 0.0329949, acc 0.984375, prec 0.0783844, recall 0.904613
2017-12-10T13:02:12.356015: step 3707, loss 0.17276, acc 0.96875, prec 0.0784033, recall 0.90464
2017-12-10T13:02:12.802126: step 3708, loss 0.050791, acc 0.984375, prec 0.0784014, recall 0.90464
2017-12-10T13:02:13.244387: step 3709, loss 0.171514, acc 0.953125, prec 0.0784183, recall 0.904667
2017-12-10T13:02:13.695251: step 3710, loss 0.440138, acc 0.953125, prec 0.0784352, recall 0.904694
2017-12-10T13:02:14.124578: step 3711, loss 0.0328864, acc 0.984375, prec 0.078456, recall 0.904721
2017-12-10T13:02:14.571536: step 3712, loss 0.0303716, acc 0.984375, prec 0.0784541, recall 0.904721
2017-12-10T13:02:15.013537: step 3713, loss 0.0397137, acc 1, prec 0.0784768, recall 0.904748
2017-12-10T13:02:15.458112: step 3714, loss 0.263263, acc 0.96875, prec 0.0785184, recall 0.904803
2017-12-10T13:02:15.910444: step 3715, loss 0.152437, acc 0.984375, prec 0.0785165, recall 0.904803
2017-12-10T13:02:16.351753: step 3716, loss 0.0927042, acc 0.984375, prec 0.0785145, recall 0.904803
2017-12-10T13:02:16.802810: step 3717, loss 0.329598, acc 0.953125, prec 0.0785542, recall 0.904857
2017-12-10T13:02:17.264897: step 3718, loss 0.18876, acc 0.953125, prec 0.0785711, recall 0.904884
2017-12-10T13:02:17.716120: step 3719, loss 0.350554, acc 0.984375, prec 0.0786373, recall 0.904965
2017-12-10T13:02:18.175264: step 3720, loss 0.0285362, acc 1, prec 0.0786827, recall 0.905018
2017-12-10T13:02:18.622386: step 3721, loss 0.0523111, acc 0.984375, prec 0.0787035, recall 0.905045
2017-12-10T13:02:19.057591: step 3722, loss 0.012619, acc 1, prec 0.0787489, recall 0.905099
2017-12-10T13:02:19.495569: step 3723, loss 0.0110848, acc 1, prec 0.0787716, recall 0.905126
2017-12-10T13:02:19.937629: step 3724, loss 0.121735, acc 0.984375, prec 0.0787697, recall 0.905126
2017-12-10T13:02:20.387297: step 3725, loss 1.44123, acc 0.96875, prec 0.0788131, recall 0.904924
2017-12-10T13:02:20.836374: step 3726, loss 0.401278, acc 1, prec 0.0788358, recall 0.90495
2017-12-10T13:02:21.293100: step 3727, loss 0.211119, acc 0.953125, prec 0.07883, recall 0.90495
2017-12-10T13:02:21.734533: step 3728, loss 0.0603787, acc 0.984375, prec 0.0788281, recall 0.90495
2017-12-10T13:02:22.171027: step 3729, loss 0.460294, acc 0.953125, prec 0.0788449, recall 0.904977
2017-12-10T13:02:22.612773: step 3730, loss 0.19326, acc 0.921875, prec 0.0788352, recall 0.904977
2017-12-10T13:02:23.053061: step 3731, loss 0.0834537, acc 0.953125, prec 0.0788521, recall 0.905004
2017-12-10T13:02:23.495485: step 3732, loss 0.367379, acc 0.9375, prec 0.0788443, recall 0.905004
2017-12-10T13:02:23.946513: step 3733, loss 0.266276, acc 0.921875, prec 0.0788573, recall 0.905031
2017-12-10T13:02:24.402214: step 3734, loss 0.134057, acc 0.953125, prec 0.0788741, recall 0.905058
2017-12-10T13:02:24.868579: step 3735, loss 0.164158, acc 0.921875, prec 0.0788644, recall 0.905058
2017-12-10T13:02:25.312576: step 3736, loss 0.263441, acc 0.9375, prec 0.0789247, recall 0.905138
2017-12-10T13:02:25.768451: step 3737, loss 0.112713, acc 0.953125, prec 0.0789642, recall 0.905192
2017-12-10T13:02:26.224672: step 3738, loss 0.367961, acc 0.921875, prec 0.0789545, recall 0.905192
2017-12-10T13:02:26.674852: step 3739, loss 0.245455, acc 0.921875, prec 0.0789448, recall 0.905192
2017-12-10T13:02:27.119516: step 3740, loss 0.0307328, acc 1, prec 0.0789448, recall 0.905192
2017-12-10T13:02:27.554060: step 3741, loss 0.170444, acc 0.890625, prec 0.0789538, recall 0.905219
2017-12-10T13:02:27.996823: step 3742, loss 0.215198, acc 0.9375, prec 0.0789914, recall 0.905272
2017-12-10T13:02:28.454838: step 3743, loss 0.135469, acc 0.96875, prec 0.0790328, recall 0.905325
2017-12-10T13:02:28.900055: step 3744, loss 0.31198, acc 0.921875, prec 0.0790457, recall 0.905352
2017-12-10T13:02:29.359322: step 3745, loss 0.245401, acc 0.921875, prec 0.0790587, recall 0.905379
2017-12-10T13:02:29.801725: step 3746, loss 0.409283, acc 0.875, prec 0.0790884, recall 0.905432
2017-12-10T13:02:30.245103: step 3747, loss 0.138718, acc 0.96875, prec 0.0791072, recall 0.905459
2017-12-10T13:02:30.692961: step 3748, loss 0.0369281, acc 0.96875, prec 0.0791033, recall 0.905459
2017-12-10T13:02:31.136629: step 3749, loss 0.0447919, acc 0.96875, prec 0.079122, recall 0.905485
2017-12-10T13:02:31.577262: step 3750, loss 0.220163, acc 0.953125, prec 0.0791388, recall 0.905512
2017-12-10T13:02:32.016149: step 3751, loss 0.140124, acc 0.9375, prec 0.079131, recall 0.905512
2017-12-10T13:02:32.451708: step 3752, loss 0.146855, acc 0.953125, prec 0.0791252, recall 0.905512
2017-12-10T13:02:32.911624: step 3753, loss 0.0607313, acc 0.984375, prec 0.0791459, recall 0.905538
2017-12-10T13:02:33.358990: step 3754, loss 0.0631963, acc 0.984375, prec 0.0791439, recall 0.905538
2017-12-10T13:02:33.798524: step 3755, loss 0.17096, acc 0.96875, prec 0.0791401, recall 0.905538
2017-12-10T13:02:34.256494: step 3756, loss 0.0132812, acc 1, prec 0.0791401, recall 0.905538
2017-12-10T13:02:34.705084: step 3757, loss 0.125941, acc 0.96875, prec 0.0791588, recall 0.905565
2017-12-10T13:02:35.152978: step 3758, loss 1.42779, acc 0.984375, prec 0.0791588, recall 0.90531
2017-12-10T13:02:35.605978: step 3759, loss 0.155885, acc 0.953125, prec 0.0791529, recall 0.90531
2017-12-10T13:02:36.046113: step 3760, loss 0.145322, acc 0.9375, prec 0.0791678, recall 0.905337
2017-12-10T13:02:36.502191: step 3761, loss 0.0187687, acc 1, prec 0.0791904, recall 0.905364
2017-12-10T13:02:36.945239: step 3762, loss 0.0411798, acc 0.984375, prec 0.0792337, recall 0.905417
2017-12-10T13:02:37.385727: step 3763, loss 0.305768, acc 0.9375, prec 0.0792259, recall 0.905417
2017-12-10T13:02:37.840616: step 3764, loss 0.353662, acc 0.921875, prec 0.0792388, recall 0.905443
2017-12-10T13:02:38.292110: step 3765, loss 0.186613, acc 0.96875, prec 0.0792575, recall 0.90547
2017-12-10T13:02:38.736827: step 3766, loss 0.04271, acc 1, prec 0.0792575, recall 0.90547
2017-12-10T13:02:39.191634: step 3767, loss 0.0772518, acc 0.984375, prec 0.0792782, recall 0.905496
2017-12-10T13:02:39.644935: step 3768, loss 0.159186, acc 0.953125, prec 0.0792949, recall 0.905523
2017-12-10T13:02:40.083810: step 3769, loss 0.0807619, acc 0.984375, prec 0.079293, recall 0.905523
2017-12-10T13:02:40.525653: step 3770, loss 0.1649, acc 0.9375, prec 0.0792852, recall 0.905523
2017-12-10T13:02:40.964721: step 3771, loss 0.0944572, acc 1, prec 0.0793078, recall 0.905549
2017-12-10T13:02:41.416628: step 3772, loss 0.0173934, acc 1, prec 0.0793078, recall 0.905549
2017-12-10T13:02:41.870288: step 3773, loss 0.448853, acc 0.921875, prec 0.0792981, recall 0.905549
2017-12-10T13:02:42.315807: step 3774, loss 0.0288168, acc 1, prec 0.0792981, recall 0.905549
2017-12-10T13:02:42.752756: step 3775, loss 0.0518779, acc 0.96875, prec 0.0792942, recall 0.905549
2017-12-10T13:02:43.191780: step 3776, loss 0.0952246, acc 0.953125, prec 0.0792883, recall 0.905549
2017-12-10T13:02:43.637442: step 3777, loss 0.0313029, acc 0.984375, prec 0.0792864, recall 0.905549
2017-12-10T13:02:44.075001: step 3778, loss 0.0594696, acc 0.96875, prec 0.0792825, recall 0.905549
2017-12-10T13:02:44.518736: step 3779, loss 0.083126, acc 0.984375, prec 0.0792806, recall 0.905549
2017-12-10T13:02:44.955335: step 3780, loss 0.101876, acc 0.96875, prec 0.0793219, recall 0.905602
2017-12-10T13:02:45.401585: step 3781, loss 2.38016, acc 0.984375, prec 0.0793219, recall 0.905349
2017-12-10T13:02:45.855044: step 3782, loss 0.119308, acc 1, prec 0.0793444, recall 0.905375
2017-12-10T13:02:46.315029: step 3783, loss 0.0564331, acc 0.96875, prec 0.0793405, recall 0.905375
2017-12-10T13:02:46.760340: step 3784, loss 0.203775, acc 0.9375, prec 0.0794005, recall 0.905455
2017-12-10T13:02:47.210529: step 3785, loss 0.208308, acc 0.953125, prec 0.0794172, recall 0.905481
2017-12-10T13:02:47.672212: step 3786, loss 0.135609, acc 0.96875, prec 0.0794359, recall 0.905507
2017-12-10T13:02:48.131040: step 3787, loss 0.0574624, acc 0.984375, prec 0.0794566, recall 0.905534
2017-12-10T13:02:48.587315: step 3788, loss 0.3636, acc 0.90625, prec 0.0794449, recall 0.905534
2017-12-10T13:02:49.042458: step 3789, loss 0.0572024, acc 0.96875, prec 0.0794635, recall 0.90556
2017-12-10T13:02:49.496779: step 3790, loss 0.157755, acc 0.953125, prec 0.0794577, recall 0.90556
2017-12-10T13:02:49.936577: step 3791, loss 0.344599, acc 0.9375, prec 0.0794499, recall 0.90556
2017-12-10T13:02:50.368631: step 3792, loss 0.0608147, acc 0.96875, prec 0.0795137, recall 0.905639
2017-12-10T13:02:50.813969: step 3793, loss 0.217992, acc 0.984375, prec 0.0795343, recall 0.905666
2017-12-10T13:02:51.264169: step 3794, loss 0.186654, acc 0.921875, prec 0.0795471, recall 0.905692
2017-12-10T13:02:51.722595: step 3795, loss 0.435643, acc 0.953125, prec 0.0795413, recall 0.905692
2017-12-10T13:02:52.177610: step 3796, loss 0.328455, acc 0.921875, prec 0.0795541, recall 0.905718
2017-12-10T13:02:52.618303: step 3797, loss 0.118085, acc 0.96875, prec 0.0795953, recall 0.905771
2017-12-10T13:02:53.065025: step 3798, loss 0.0925178, acc 0.96875, prec 0.0795914, recall 0.905771
2017-12-10T13:02:53.510465: step 3799, loss 0.125278, acc 0.96875, prec 0.0796326, recall 0.905823
2017-12-10T13:02:53.956871: step 3800, loss 0.133793, acc 0.9375, prec 0.0796473, recall 0.90585
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3800

2017-12-10T13:02:55.833795: step 3801, loss 0.0764424, acc 0.96875, prec 0.0796885, recall 0.905902
2017-12-10T13:02:56.282147: step 3802, loss 0.0333866, acc 0.984375, prec 0.0796865, recall 0.905902
2017-12-10T13:02:56.747065: step 3803, loss 0.0529896, acc 0.96875, prec 0.0796826, recall 0.905902
2017-12-10T13:02:57.185264: step 3804, loss 0.136263, acc 0.9375, prec 0.0796748, recall 0.905902
2017-12-10T13:02:57.629587: step 3805, loss 0.191552, acc 0.921875, prec 0.0797101, recall 0.905954
2017-12-10T13:02:58.083234: step 3806, loss 0.270567, acc 0.90625, prec 0.0796984, recall 0.905954
2017-12-10T13:02:58.523686: step 3807, loss 0.0635041, acc 0.984375, prec 0.0797415, recall 0.906007
2017-12-10T13:02:58.972130: step 3808, loss 0.294332, acc 0.953125, prec 0.0797582, recall 0.906033
2017-12-10T13:02:59.419553: step 3809, loss 0.115523, acc 0.96875, prec 0.0797993, recall 0.906085
2017-12-10T13:02:59.875539: step 3810, loss 0.333359, acc 0.953125, prec 0.079816, recall 0.906111
2017-12-10T13:03:00.326183: step 3811, loss 0.0717534, acc 0.953125, prec 0.0798101, recall 0.906111
2017-12-10T13:03:00.762384: step 3812, loss 0.135975, acc 0.96875, prec 0.0798287, recall 0.906137
2017-12-10T13:03:01.211516: step 3813, loss 0.0751735, acc 0.96875, prec 0.0798474, recall 0.906163
2017-12-10T13:03:01.654597: step 3814, loss 0.0612067, acc 0.984375, prec 0.0798904, recall 0.906215
2017-12-10T13:03:02.097622: step 3815, loss 0.0899213, acc 0.96875, prec 0.079909, recall 0.906241
2017-12-10T13:03:02.544381: step 3816, loss 0.87187, acc 0.984375, prec 0.0799296, recall 0.906267
2017-12-10T13:03:03.004964: step 3817, loss 0.111414, acc 0.9375, prec 0.0799442, recall 0.906293
2017-12-10T13:03:03.443933: step 3818, loss 0.30252, acc 0.96875, prec 0.0799403, recall 0.906293
2017-12-10T13:03:03.887008: step 3819, loss 0.0312461, acc 0.984375, prec 0.0799609, recall 0.906319
2017-12-10T13:03:04.324275: step 3820, loss 0.0832395, acc 0.96875, prec 0.0799795, recall 0.906345
2017-12-10T13:03:04.781164: step 3821, loss 0.144052, acc 0.96875, prec 0.079998, recall 0.906371
2017-12-10T13:03:05.227149: step 3822, loss 0.232943, acc 0.953125, prec 0.0800596, recall 0.906449
2017-12-10T13:03:05.652033: step 3823, loss 0.0414575, acc 0.984375, prec 0.0800802, recall 0.906475
2017-12-10T13:03:06.090305: step 3824, loss 0.00801181, acc 1, prec 0.0800802, recall 0.906475
2017-12-10T13:03:06.527102: step 3825, loss 0.0793162, acc 0.984375, prec 0.0801007, recall 0.906501
2017-12-10T13:03:06.973184: step 3826, loss 0.238198, acc 0.9375, prec 0.0800929, recall 0.906501
2017-12-10T13:03:07.425829: step 3827, loss 0.109177, acc 0.953125, prec 0.0801095, recall 0.906527
2017-12-10T13:03:07.864519: step 3828, loss 0.0491536, acc 0.984375, prec 0.08013, recall 0.906552
2017-12-10T13:03:08.321095: step 3829, loss 0.255712, acc 1, prec 0.080175, recall 0.906604
2017-12-10T13:03:08.785089: step 3830, loss 0.266961, acc 0.96875, prec 0.080216, recall 0.906656
2017-12-10T13:03:09.227158: step 3831, loss 0.83304, acc 0.953125, prec 0.0802326, recall 0.906681
2017-12-10T13:03:09.675638: step 3832, loss 0.0158641, acc 1, prec 0.0802326, recall 0.906681
2017-12-10T13:03:10.115552: step 3833, loss 0.253176, acc 0.921875, prec 0.0802228, recall 0.906681
2017-12-10T13:03:10.562715: step 3834, loss 0.0206954, acc 1, prec 0.0802453, recall 0.906707
2017-12-10T13:03:11.008565: step 3835, loss 0.406311, acc 0.96875, prec 0.0803087, recall 0.906784
2017-12-10T13:03:11.470402: step 3836, loss 0.0371773, acc 0.984375, prec 0.0803068, recall 0.906784
2017-12-10T13:03:11.921467: step 3837, loss 0.0535325, acc 0.984375, prec 0.0803048, recall 0.906784
2017-12-10T13:03:12.370462: step 3838, loss 0.201172, acc 0.96875, prec 0.0803009, recall 0.906784
2017-12-10T13:03:12.812828: step 3839, loss 0.129292, acc 0.96875, prec 0.0803643, recall 0.906861
2017-12-10T13:03:13.262392: step 3840, loss 0.287492, acc 0.921875, prec 0.080377, recall 0.906887
2017-12-10T13:03:13.716300: step 3841, loss 0.473319, acc 0.984375, prec 0.0803975, recall 0.906913
2017-12-10T13:03:14.172946: step 3842, loss 0.224702, acc 0.921875, prec 0.0803877, recall 0.906913
2017-12-10T13:03:14.607796: step 3843, loss 0.219627, acc 0.953125, prec 0.0803818, recall 0.906913
2017-12-10T13:03:15.054792: step 3844, loss 0.280057, acc 0.859375, prec 0.0803641, recall 0.906913
2017-12-10T13:03:15.497075: step 3845, loss 0.0759295, acc 0.96875, prec 0.0803602, recall 0.906913
2017-12-10T13:03:15.949563: step 3846, loss 0.822322, acc 0.890625, prec 0.0803913, recall 0.906964
2017-12-10T13:03:16.400226: step 3847, loss 0.174207, acc 0.953125, prec 0.0803855, recall 0.906964
2017-12-10T13:03:16.848228: step 3848, loss 0.138911, acc 0.984375, prec 0.0804284, recall 0.907015
2017-12-10T13:03:17.295670: step 3849, loss 0.191971, acc 0.9375, prec 0.0804654, recall 0.907066
2017-12-10T13:03:17.760992: step 3850, loss 0.157279, acc 0.953125, prec 0.0804595, recall 0.907066
2017-12-10T13:03:18.211913: step 3851, loss 0.318989, acc 0.96875, prec 0.0804556, recall 0.907066
2017-12-10T13:03:18.659272: step 3852, loss 0.867067, acc 0.953125, prec 0.0804945, recall 0.907117
2017-12-10T13:03:19.099269: step 3853, loss 0.187428, acc 0.921875, prec 0.0805071, recall 0.907143
2017-12-10T13:03:19.541419: step 3854, loss 0.23825, acc 0.9375, prec 0.0805665, recall 0.907219
2017-12-10T13:03:19.993628: step 3855, loss 0.449694, acc 0.90625, prec 0.0806892, recall 0.907372
2017-12-10T13:03:20.437395: step 3856, loss 0.174851, acc 0.96875, prec 0.0807301, recall 0.907423
2017-12-10T13:03:20.892879: step 3857, loss 0.263381, acc 0.921875, prec 0.0807426, recall 0.907448
2017-12-10T13:03:21.338177: step 3858, loss 0.315897, acc 0.90625, prec 0.0807532, recall 0.907473
2017-12-10T13:03:21.785578: step 3859, loss 0.0884676, acc 0.9375, prec 0.0808125, recall 0.907549
2017-12-10T13:03:22.235867: step 3860, loss 0.399866, acc 0.921875, prec 0.0808474, recall 0.9076
2017-12-10T13:03:22.679848: step 3861, loss 0.233105, acc 0.953125, prec 0.0808639, recall 0.907625
2017-12-10T13:03:23.131439: step 3862, loss 0.0433845, acc 1, prec 0.0808863, recall 0.90765
2017-12-10T13:03:23.582674: step 3863, loss 0.122353, acc 0.9375, prec 0.0808784, recall 0.90765
2017-12-10T13:03:24.021129: step 3864, loss 0.184543, acc 0.921875, prec 0.0808686, recall 0.90765
2017-12-10T13:03:24.467047: step 3865, loss 0.022443, acc 0.984375, prec 0.0808666, recall 0.90765
2017-12-10T13:03:24.918286: step 3866, loss 0.110751, acc 0.96875, prec 0.0809298, recall 0.907726
2017-12-10T13:03:25.367954: step 3867, loss 0.186482, acc 0.90625, prec 0.0809627, recall 0.907776
2017-12-10T13:03:25.768305: step 3868, loss 0.146029, acc 0.921875, prec 0.0809528, recall 0.907776
2017-12-10T13:03:26.166579: step 3869, loss 0.586542, acc 0.9375, prec 0.081012, recall 0.907852
2017-12-10T13:03:26.549001: step 3870, loss 0.341753, acc 0.9375, prec 0.0810265, recall 0.907877
2017-12-10T13:03:26.926105: step 3871, loss 0.0179772, acc 1, prec 0.0810265, recall 0.907877
2017-12-10T13:03:27.318995: step 3872, loss 0.123364, acc 0.953125, prec 0.0810206, recall 0.907877
2017-12-10T13:03:27.786184: step 3873, loss 0.159712, acc 0.96875, prec 0.0810614, recall 0.907927
2017-12-10T13:03:28.240161: step 3874, loss 2.15618, acc 0.96875, prec 0.0811488, recall 0.90778
2017-12-10T13:03:28.683341: step 3875, loss 0.213584, acc 0.953125, prec 0.0811652, recall 0.907805
2017-12-10T13:03:29.148634: step 3876, loss 0.176397, acc 0.96875, prec 0.0811613, recall 0.907805
2017-12-10T13:03:29.591102: step 3877, loss 0.0709244, acc 0.96875, prec 0.0811796, recall 0.90783
2017-12-10T13:03:30.045556: step 3878, loss 0.137235, acc 0.9375, prec 0.0811718, recall 0.90783
2017-12-10T13:03:30.490706: step 3879, loss 0.408361, acc 0.90625, prec 0.0812046, recall 0.90788
2017-12-10T13:03:30.946904: step 3880, loss 1.0745, acc 0.875, prec 0.0812111, recall 0.907905
2017-12-10T13:03:31.389015: step 3881, loss 0.297038, acc 0.921875, prec 0.0812013, recall 0.907905
2017-12-10T13:03:31.832361: step 3882, loss 0.226206, acc 0.890625, prec 0.0811874, recall 0.907905
2017-12-10T13:03:32.275842: step 3883, loss 0.272243, acc 0.921875, prec 0.0812222, recall 0.907955
2017-12-10T13:03:32.725783: step 3884, loss 0.568491, acc 0.890625, prec 0.0812307, recall 0.90798
2017-12-10T13:03:33.175941: step 3885, loss 0.558747, acc 0.84375, prec 0.081211, recall 0.90798
2017-12-10T13:03:33.621615: step 3886, loss 0.282478, acc 0.875, prec 0.0812175, recall 0.908005
2017-12-10T13:03:34.065165: step 3887, loss 0.40493, acc 0.921875, prec 0.0812077, recall 0.908005
2017-12-10T13:03:34.507053: step 3888, loss 0.369553, acc 0.875, prec 0.0811919, recall 0.908005
2017-12-10T13:03:34.943434: step 3889, loss 0.192871, acc 0.9375, prec 0.0812063, recall 0.90803
2017-12-10T13:03:35.405746: step 3890, loss 0.236928, acc 0.90625, prec 0.0812168, recall 0.908055
2017-12-10T13:03:35.857622: step 3891, loss 0.0963693, acc 0.953125, prec 0.0812555, recall 0.908105
2017-12-10T13:03:36.318339: step 3892, loss 0.411933, acc 0.921875, prec 0.0812902, recall 0.908155
2017-12-10T13:03:36.767137: step 3893, loss 0.391017, acc 0.921875, prec 0.0813249, recall 0.908205
2017-12-10T13:03:37.207133: step 3894, loss 0.208288, acc 0.953125, prec 0.0813412, recall 0.90823
2017-12-10T13:03:37.647129: step 3895, loss 0.220873, acc 0.9375, prec 0.0813779, recall 0.908279
2017-12-10T13:03:38.091499: step 3896, loss 0.0786375, acc 0.96875, prec 0.0813739, recall 0.908279
2017-12-10T13:03:38.540612: step 3897, loss 0.756822, acc 0.90625, prec 0.0813844, recall 0.908304
2017-12-10T13:03:38.987640: step 3898, loss 0.249632, acc 0.953125, prec 0.0814007, recall 0.908329
2017-12-10T13:03:39.427609: step 3899, loss 0.607388, acc 0.9375, prec 0.0814151, recall 0.908354
2017-12-10T13:03:39.875757: step 3900, loss 1.20092, acc 0.9375, prec 0.0814314, recall 0.908133
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-3900

2017-12-10T13:03:41.760876: step 3901, loss 0.162299, acc 0.953125, prec 0.0814255, recall 0.908133
2017-12-10T13:03:42.210379: step 3902, loss 0.150433, acc 0.96875, prec 0.0814438, recall 0.908158
2017-12-10T13:03:42.652485: step 3903, loss 0.139834, acc 0.9375, prec 0.0814582, recall 0.908183
2017-12-10T13:03:43.103460: step 3904, loss 0.136096, acc 0.96875, prec 0.0814987, recall 0.908232
2017-12-10T13:03:43.543918: step 3905, loss 0.0755096, acc 0.96875, prec 0.0815392, recall 0.908282
2017-12-10T13:03:43.987191: step 3906, loss 0.366236, acc 0.9375, prec 0.0815313, recall 0.908282
2017-12-10T13:03:44.429625: step 3907, loss 0.258659, acc 0.96875, prec 0.0815496, recall 0.908306
2017-12-10T13:03:44.876851: step 3908, loss 0.239609, acc 0.9375, prec 0.081564, recall 0.908331
2017-12-10T13:03:45.319302: step 3909, loss 0.251384, acc 0.953125, prec 0.081558, recall 0.908331
2017-12-10T13:03:45.757652: step 3910, loss 0.210257, acc 0.953125, prec 0.0815966, recall 0.908381
2017-12-10T13:03:46.201897: step 3911, loss 0.419106, acc 0.890625, prec 0.0815828, recall 0.908381
2017-12-10T13:03:46.647384: step 3912, loss 0.16296, acc 0.96875, prec 0.081601, recall 0.908405
2017-12-10T13:03:47.097595: step 3913, loss 0.245229, acc 0.96875, prec 0.0816415, recall 0.908454
2017-12-10T13:03:47.545990: step 3914, loss 0.147649, acc 0.96875, prec 0.0816376, recall 0.908454
2017-12-10T13:03:47.987795: step 3915, loss 0.164752, acc 0.953125, prec 0.0816539, recall 0.908479
2017-12-10T13:03:48.434979: step 3916, loss 0.109786, acc 0.984375, prec 0.0816741, recall 0.908504
2017-12-10T13:03:48.885320: step 3917, loss 0.0244923, acc 1, prec 0.0817186, recall 0.908553
2017-12-10T13:03:49.326284: step 3918, loss 0.0975151, acc 0.953125, prec 0.0817348, recall 0.908578
2017-12-10T13:03:49.769101: step 3919, loss 0.576551, acc 1, prec 0.0817571, recall 0.908602
2017-12-10T13:03:50.213226: step 3920, loss 0.173598, acc 0.953125, prec 0.0817511, recall 0.908602
2017-12-10T13:03:50.655848: step 3921, loss 0.0779599, acc 0.984375, prec 0.0817491, recall 0.908602
2017-12-10T13:03:51.105005: step 3922, loss 0.0162675, acc 1, prec 0.0817491, recall 0.908602
2017-12-10T13:03:51.550802: step 3923, loss 0.0945101, acc 0.953125, prec 0.0817432, recall 0.908602
2017-12-10T13:03:51.991485: step 3924, loss 0.110175, acc 0.96875, prec 0.0817393, recall 0.908602
2017-12-10T13:03:52.438109: step 3925, loss 0.284451, acc 0.953125, prec 0.0817555, recall 0.908627
2017-12-10T13:03:52.893378: step 3926, loss 0.239434, acc 0.9375, prec 0.0818142, recall 0.9087
2017-12-10T13:03:53.347563: step 3927, loss 0.140439, acc 0.96875, prec 0.0818547, recall 0.908749
2017-12-10T13:03:53.801052: step 3928, loss 0.135017, acc 0.953125, prec 0.0818487, recall 0.908749
2017-12-10T13:03:54.244072: step 3929, loss 0.121575, acc 0.953125, prec 0.081865, recall 0.908774
2017-12-10T13:03:54.686903: step 3930, loss 0.220378, acc 0.921875, prec 0.0818773, recall 0.908798
2017-12-10T13:03:55.148482: step 3931, loss 0.144518, acc 0.953125, prec 0.0818935, recall 0.908823
2017-12-10T13:03:55.606395: step 3932, loss 0.107185, acc 0.96875, prec 0.0819561, recall 0.908896
2017-12-10T13:03:56.048971: step 3933, loss 0.100034, acc 0.984375, prec 0.0819541, recall 0.908896
2017-12-10T13:03:56.499489: step 3934, loss 0.0566384, acc 0.984375, prec 0.0819522, recall 0.908896
2017-12-10T13:03:56.949134: step 3935, loss 0.218181, acc 0.9375, prec 0.0819442, recall 0.908896
2017-12-10T13:03:57.386471: step 3936, loss 0.0568156, acc 0.96875, prec 0.0819403, recall 0.908896
2017-12-10T13:03:57.820363: step 3937, loss 0.243736, acc 0.953125, prec 0.0819343, recall 0.908896
2017-12-10T13:03:58.270526: step 3938, loss 0.0303668, acc 0.984375, prec 0.0819767, recall 0.908945
2017-12-10T13:03:58.708819: step 3939, loss 0.0610143, acc 0.984375, prec 0.0819747, recall 0.908945
2017-12-10T13:03:59.158178: step 3940, loss 0.0513398, acc 0.984375, prec 0.0819728, recall 0.908945
2017-12-10T13:03:59.605787: step 3941, loss 0.133266, acc 0.96875, prec 0.0820131, recall 0.908994
2017-12-10T13:04:00.044585: step 3942, loss 0.0219854, acc 1, prec 0.0820131, recall 0.908994
2017-12-10T13:04:00.498799: step 3943, loss 0.173303, acc 0.96875, prec 0.0820092, recall 0.908994
2017-12-10T13:04:00.947405: step 3944, loss 0.100899, acc 0.96875, prec 0.0820052, recall 0.908994
2017-12-10T13:04:01.395270: step 3945, loss 0.0214451, acc 1, prec 0.0820274, recall 0.909018
2017-12-10T13:04:01.845121: step 3946, loss 5.73419, acc 0.984375, prec 0.0820495, recall 0.908799
2017-12-10T13:04:02.293941: step 3947, loss 0.017479, acc 1, prec 0.0820717, recall 0.908824
2017-12-10T13:04:02.746039: step 3948, loss 1.41468, acc 0.96875, prec 0.0820697, recall 0.908581
2017-12-10T13:04:03.196668: step 3949, loss 0.0412685, acc 0.984375, prec 0.0820678, recall 0.908581
2017-12-10T13:04:03.637466: step 3950, loss 0.0872589, acc 0.984375, prec 0.0821101, recall 0.908629
2017-12-10T13:04:04.074793: step 3951, loss 0.149621, acc 0.9375, prec 0.0821022, recall 0.908629
2017-12-10T13:04:04.523100: step 3952, loss 0.133703, acc 0.953125, prec 0.0821184, recall 0.908654
2017-12-10T13:04:04.961085: step 3953, loss 0.24753, acc 0.890625, prec 0.0821045, recall 0.908654
2017-12-10T13:04:05.432763: step 3954, loss 0.268208, acc 0.921875, prec 0.0821167, recall 0.908678
2017-12-10T13:04:05.868115: step 3955, loss 0.355992, acc 0.9375, prec 0.082131, recall 0.908703
2017-12-10T13:04:06.317529: step 3956, loss 0.484143, acc 0.890625, prec 0.0821614, recall 0.908751
2017-12-10T13:04:06.755906: step 3957, loss 0.283017, acc 0.921875, prec 0.0821515, recall 0.908751
2017-12-10T13:04:07.212676: step 3958, loss 0.290279, acc 0.890625, prec 0.0821376, recall 0.908751
2017-12-10T13:04:07.651770: step 3959, loss 0.311441, acc 0.90625, prec 0.0821479, recall 0.908776
2017-12-10T13:04:08.095033: step 3960, loss 0.185853, acc 0.953125, prec 0.082164, recall 0.9088
2017-12-10T13:04:08.547705: step 3961, loss 0.207903, acc 0.921875, prec 0.0821541, recall 0.9088
2017-12-10T13:04:08.996387: step 3962, loss 0.265668, acc 0.921875, prec 0.0821442, recall 0.9088
2017-12-10T13:04:09.445354: step 3963, loss 0.435269, acc 0.921875, prec 0.0821343, recall 0.9088
2017-12-10T13:04:09.886298: step 3964, loss 0.216881, acc 0.953125, prec 0.0821505, recall 0.908824
2017-12-10T13:04:10.329651: step 3965, loss 0.204286, acc 0.9375, prec 0.0821426, recall 0.908824
2017-12-10T13:04:10.768110: step 3966, loss 0.1898, acc 0.921875, prec 0.0821548, recall 0.908849
2017-12-10T13:04:11.213383: step 3967, loss 0.4214, acc 0.859375, prec 0.0822033, recall 0.908921
2017-12-10T13:04:11.659861: step 3968, loss 0.334059, acc 0.921875, prec 0.0822376, recall 0.90897
2017-12-10T13:04:12.099712: step 3969, loss 0.368458, acc 0.953125, prec 0.0822538, recall 0.908994
2017-12-10T13:04:12.545675: step 3970, loss 0.358899, acc 0.90625, prec 0.0822419, recall 0.908994
2017-12-10T13:04:13.014641: step 3971, loss 0.444254, acc 0.90625, prec 0.08223, recall 0.908994
2017-12-10T13:04:13.462618: step 3972, loss 0.0499804, acc 0.96875, prec 0.0822261, recall 0.908994
2017-12-10T13:04:13.912703: step 3973, loss 0.44646, acc 0.875, prec 0.0822102, recall 0.908994
2017-12-10T13:04:14.345265: step 3974, loss 0.319567, acc 0.9375, prec 0.0822244, recall 0.909018
2017-12-10T13:04:14.800475: step 3975, loss 0.125323, acc 0.953125, prec 0.0822185, recall 0.909018
2017-12-10T13:04:15.208475: step 3976, loss 0.290879, acc 0.923077, prec 0.0822326, recall 0.909043
2017-12-10T13:04:15.663355: step 3977, loss 0.332684, acc 0.921875, prec 0.0822669, recall 0.909091
2017-12-10T13:04:16.113627: step 3978, loss 0.0917458, acc 0.984375, prec 0.082287, recall 0.909115
2017-12-10T13:04:16.549107: step 3979, loss 0.277926, acc 0.90625, prec 0.0823193, recall 0.909163
2017-12-10T13:04:16.987393: step 3980, loss 0.150599, acc 0.96875, prec 0.0823374, recall 0.909187
2017-12-10T13:04:17.456639: step 3981, loss 0.660484, acc 1, prec 0.0823815, recall 0.909236
2017-12-10T13:04:17.903061: step 3982, loss 0.095208, acc 0.953125, prec 0.0823756, recall 0.909236
2017-12-10T13:04:18.352049: step 3983, loss 0.0466625, acc 0.984375, prec 0.0823957, recall 0.90926
2017-12-10T13:04:18.795938: step 3984, loss 0.560661, acc 0.984375, prec 0.0824599, recall 0.909332
2017-12-10T13:04:19.241150: step 3985, loss 0.168606, acc 0.953125, prec 0.0824539, recall 0.909332
2017-12-10T13:04:19.685824: step 3986, loss 0.189736, acc 0.890625, prec 0.08244, recall 0.909332
2017-12-10T13:04:20.132089: step 3987, loss 0.0344936, acc 0.96875, prec 0.0824581, recall 0.909356
2017-12-10T13:04:20.580318: step 3988, loss 0.0666131, acc 0.96875, prec 0.0824762, recall 0.90938
2017-12-10T13:04:21.019633: step 3989, loss 0.129856, acc 0.96875, prec 0.0824943, recall 0.909404
2017-12-10T13:04:21.451823: step 3990, loss 0.234112, acc 0.9375, prec 0.0825305, recall 0.909452
2017-12-10T13:04:21.895145: step 3991, loss 0.0630683, acc 0.96875, prec 0.0825265, recall 0.909452
2017-12-10T13:04:22.340531: step 3992, loss 0.0159008, acc 1, prec 0.0825485, recall 0.909476
2017-12-10T13:04:22.784766: step 3993, loss 0.0247773, acc 0.984375, prec 0.0825465, recall 0.909476
2017-12-10T13:04:23.223334: step 3994, loss 0.928252, acc 0.953125, prec 0.0825847, recall 0.909524
2017-12-10T13:04:23.665947: step 3995, loss 0.147584, acc 0.9375, prec 0.0825767, recall 0.909524
2017-12-10T13:04:24.120542: step 3996, loss 0.314492, acc 0.96875, prec 0.0825948, recall 0.909548
2017-12-10T13:04:24.566892: step 3997, loss 0.153378, acc 0.96875, prec 0.0826349, recall 0.909596
2017-12-10T13:04:25.009644: step 3998, loss 0.29094, acc 0.921875, prec 0.082647, recall 0.909619
2017-12-10T13:04:25.454848: step 3999, loss 0.157665, acc 0.96875, prec 0.082643, recall 0.909619
2017-12-10T13:04:25.900333: step 4000, loss 0.228029, acc 0.96875, prec 0.0826611, recall 0.909643

Evaluation:
2017-12-10T13:04:35.626622: step 4000, loss 3.06525, acc 0.947537, prec 0.0831674, recall 0.892194

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4000

2017-12-10T13:04:37.639495: step 4001, loss 0.167376, acc 0.953125, prec 0.0831615, recall 0.892194
2017-12-10T13:04:38.084311: step 4002, loss 0.206343, acc 0.953125, prec 0.0831556, recall 0.892194
2017-12-10T13:04:38.541014: step 4003, loss 0.54347, acc 0.90625, prec 0.0831655, recall 0.892222
2017-12-10T13:04:38.994812: step 4004, loss 0.0735681, acc 0.96875, prec 0.0831833, recall 0.892249
2017-12-10T13:04:39.448650: step 4005, loss 0.0465105, acc 0.96875, prec 0.0831793, recall 0.892249
2017-12-10T13:04:39.888355: step 4006, loss 0.342833, acc 0.9375, prec 0.0832149, recall 0.892304
2017-12-10T13:04:40.335095: step 4007, loss 0.172353, acc 0.9375, prec 0.083207, recall 0.892304
2017-12-10T13:04:40.785224: step 4008, loss 0.0541338, acc 0.96875, prec 0.0832248, recall 0.892331
2017-12-10T13:04:41.219665: step 4009, loss 0.214065, acc 0.984375, prec 0.0832662, recall 0.892386
2017-12-10T13:04:41.668409: step 4010, loss 0.111132, acc 0.96875, prec 0.0833057, recall 0.89244
2017-12-10T13:04:42.104050: step 4011, loss 0.10378, acc 0.96875, prec 0.0833018, recall 0.89244
2017-12-10T13:04:42.561339: step 4012, loss 0.0173651, acc 1, prec 0.0833235, recall 0.892468
2017-12-10T13:04:43.012880: step 4013, loss 0.116741, acc 0.953125, prec 0.0833175, recall 0.892468
2017-12-10T13:04:43.461862: step 4014, loss 0.0681585, acc 0.96875, prec 0.0833136, recall 0.892468
2017-12-10T13:04:43.906695: step 4015, loss 0.0238571, acc 0.984375, prec 0.0833333, recall 0.892495
2017-12-10T13:04:44.360397: step 4016, loss 0.111847, acc 0.96875, prec 0.0833511, recall 0.892522
2017-12-10T13:04:44.814757: step 4017, loss 0.0132312, acc 1, prec 0.0833511, recall 0.892522
2017-12-10T13:04:45.271154: step 4018, loss 0.129318, acc 0.96875, prec 0.0833688, recall 0.892549
2017-12-10T13:04:45.725325: step 4019, loss 0.0371277, acc 0.984375, prec 0.0833886, recall 0.892577
2017-12-10T13:04:46.172921: step 4020, loss 0.370593, acc 0.984375, prec 0.08343, recall 0.892631
2017-12-10T13:04:46.628483: step 4021, loss 0.0732302, acc 1, prec 0.0834734, recall 0.892685
2017-12-10T13:04:47.069085: step 4022, loss 0.14158, acc 0.96875, prec 0.0834694, recall 0.892685
2017-12-10T13:04:47.517899: step 4023, loss 1.0971, acc 0.984375, prec 0.0835108, recall 0.89274
2017-12-10T13:04:47.971448: step 4024, loss 0.0253572, acc 0.984375, prec 0.0835305, recall 0.892767
2017-12-10T13:04:48.419942: step 4025, loss 0.809642, acc 0.953125, prec 0.0835463, recall 0.892794
2017-12-10T13:04:48.883587: step 4026, loss 0.0309812, acc 0.984375, prec 0.0835877, recall 0.892848
2017-12-10T13:04:49.330891: step 4027, loss 0.204085, acc 0.96875, prec 0.0836054, recall 0.892875
2017-12-10T13:04:49.776131: step 4028, loss 0.0721679, acc 0.96875, prec 0.0836231, recall 0.892902
2017-12-10T13:04:50.243196: step 4029, loss 0.207171, acc 0.96875, prec 0.0836408, recall 0.892929
2017-12-10T13:04:50.692031: step 4030, loss 0.202643, acc 0.953125, prec 0.0836566, recall 0.892956
2017-12-10T13:04:51.130128: step 4031, loss 0.259704, acc 0.921875, prec 0.0836467, recall 0.892956
2017-12-10T13:04:51.582815: step 4032, loss 0.163309, acc 0.953125, prec 0.0836407, recall 0.892956
2017-12-10T13:04:52.033635: step 4033, loss 0.296312, acc 0.953125, prec 0.0836348, recall 0.892956
2017-12-10T13:04:52.479159: step 4034, loss 0.210051, acc 0.890625, prec 0.083621, recall 0.892956
2017-12-10T13:04:52.923707: step 4035, loss 0.249622, acc 0.9375, prec 0.0836347, recall 0.892983
2017-12-10T13:04:53.375088: step 4036, loss 0.381437, acc 0.90625, prec 0.0836445, recall 0.89301
2017-12-10T13:04:53.827340: step 4037, loss 0.142633, acc 0.9375, prec 0.0836366, recall 0.89301
2017-12-10T13:04:54.270560: step 4038, loss 0.0833177, acc 0.96875, prec 0.083676, recall 0.893064
2017-12-10T13:04:54.718655: step 4039, loss 0.231731, acc 0.921875, prec 0.0836661, recall 0.893064
2017-12-10T13:04:55.159400: step 4040, loss 0.0893808, acc 0.984375, prec 0.0836858, recall 0.893091
2017-12-10T13:04:55.590285: step 4041, loss 0.54402, acc 0.921875, prec 0.0837408, recall 0.893172
2017-12-10T13:04:56.035234: step 4042, loss 0.38157, acc 0.859375, prec 0.0837447, recall 0.893199
2017-12-10T13:04:56.483003: step 4043, loss 0.14139, acc 0.96875, prec 0.0837623, recall 0.893226
2017-12-10T13:04:56.928406: step 4044, loss 0.178023, acc 0.953125, prec 0.0837997, recall 0.89328
2017-12-10T13:04:57.375131: step 4045, loss 0.376981, acc 0.921875, prec 0.0837898, recall 0.89328
2017-12-10T13:04:57.819307: step 4046, loss 0.0407633, acc 0.984375, prec 0.0837878, recall 0.89328
2017-12-10T13:04:58.259664: step 4047, loss 0.0792163, acc 0.96875, prec 0.0837839, recall 0.89328
2017-12-10T13:04:58.704972: step 4048, loss 0.559458, acc 0.90625, prec 0.083772, recall 0.89328
2017-12-10T13:04:59.180456: step 4049, loss 0.56702, acc 0.96875, prec 0.0838329, recall 0.89336
2017-12-10T13:04:59.624459: step 4050, loss 0.084493, acc 0.953125, prec 0.0838486, recall 0.893387
2017-12-10T13:05:00.068585: step 4051, loss 0.271084, acc 1, prec 0.0838702, recall 0.893414
2017-12-10T13:05:00.516209: step 4052, loss 0.595713, acc 0.890625, prec 0.0838564, recall 0.893414
2017-12-10T13:05:00.968285: step 4053, loss 0.352926, acc 0.984375, prec 0.0838976, recall 0.893467
2017-12-10T13:05:01.419273: step 4054, loss 0.170171, acc 0.96875, prec 0.0839369, recall 0.893521
2017-12-10T13:05:01.870124: step 4055, loss 0.129671, acc 0.96875, prec 0.0839545, recall 0.893548
2017-12-10T13:05:02.311756: step 4056, loss 0.228571, acc 0.953125, prec 0.0839918, recall 0.893601
2017-12-10T13:05:02.744163: step 4057, loss 0.394301, acc 0.9375, prec 0.0840271, recall 0.893654
2017-12-10T13:05:03.180350: step 4058, loss 0.208554, acc 0.9375, prec 0.0840407, recall 0.893681
2017-12-10T13:05:03.624983: step 4059, loss 0.411772, acc 0.96875, prec 0.0840368, recall 0.893681
2017-12-10T13:05:04.077070: step 4060, loss 0.0979888, acc 0.984375, prec 0.0840996, recall 0.893761
2017-12-10T13:05:04.533318: step 4061, loss 0.0843069, acc 0.984375, prec 0.0840976, recall 0.893761
2017-12-10T13:05:04.984311: step 4062, loss 0.029404, acc 0.984375, prec 0.0840956, recall 0.893761
2017-12-10T13:05:05.424385: step 4063, loss 0.380298, acc 0.953125, prec 0.0841113, recall 0.893788
2017-12-10T13:05:05.876271: step 4064, loss 0.0608048, acc 0.984375, prec 0.0841093, recall 0.893788
2017-12-10T13:05:06.328809: step 4065, loss 0.0652111, acc 0.984375, prec 0.0841073, recall 0.893788
2017-12-10T13:05:06.776747: step 4066, loss 0.0576886, acc 0.984375, prec 0.0841053, recall 0.893788
2017-12-10T13:05:07.226118: step 4067, loss 0.0853608, acc 0.96875, prec 0.0841014, recall 0.893788
2017-12-10T13:05:07.672058: step 4068, loss 0.211194, acc 1, prec 0.0841229, recall 0.893814
2017-12-10T13:05:08.109888: step 4069, loss 0.0315016, acc 0.984375, prec 0.0841426, recall 0.893841
2017-12-10T13:05:08.554175: step 4070, loss 0.233185, acc 0.96875, prec 0.0841817, recall 0.893894
2017-12-10T13:05:09.006280: step 4071, loss 0.218413, acc 0.96875, prec 0.0842209, recall 0.893947
2017-12-10T13:05:09.452986: step 4072, loss 0.222268, acc 0.9375, prec 0.0842562, recall 0.894
2017-12-10T13:05:09.905729: step 4073, loss 0.258888, acc 0.9375, prec 0.0843129, recall 0.894079
2017-12-10T13:05:10.355201: step 4074, loss 0.323601, acc 0.890625, prec 0.084299, recall 0.894079
2017-12-10T13:05:10.794600: step 4075, loss 0.105043, acc 0.96875, prec 0.0843166, recall 0.894106
2017-12-10T13:05:11.236112: step 4076, loss 0.0321285, acc 0.984375, prec 0.0843147, recall 0.894106
2017-12-10T13:05:11.682321: step 4077, loss 0.0703204, acc 0.984375, prec 0.0843342, recall 0.894132
2017-12-10T13:05:12.129272: step 4078, loss 0.0963033, acc 1, prec 0.0843558, recall 0.894159
2017-12-10T13:05:12.590981: step 4079, loss 0.370142, acc 0.984375, prec 0.0843754, recall 0.894185
2017-12-10T13:05:13.038457: step 4080, loss 0.0958134, acc 0.96875, prec 0.0843714, recall 0.894185
2017-12-10T13:05:13.494901: step 4081, loss 0.0345175, acc 0.96875, prec 0.0843674, recall 0.894185
2017-12-10T13:05:13.933538: step 4082, loss 1.83088, acc 0.96875, prec 0.0844086, recall 0.894015
2017-12-10T13:05:14.371963: step 4083, loss 0.433545, acc 0.921875, prec 0.0844417, recall 0.894068
2017-12-10T13:05:14.815303: step 4084, loss 0.103863, acc 0.96875, prec 0.0844378, recall 0.894068
2017-12-10T13:05:15.253924: step 4085, loss 0.15379, acc 0.953125, prec 0.0844318, recall 0.894068
2017-12-10T13:05:15.710205: step 4086, loss 0.300666, acc 0.90625, prec 0.0844414, recall 0.894094
2017-12-10T13:05:16.174865: step 4087, loss 0.171783, acc 0.9375, prec 0.084455, recall 0.894121
2017-12-10T13:05:16.627479: step 4088, loss 0.437368, acc 0.90625, prec 0.0845077, recall 0.8942
2017-12-10T13:05:17.077738: step 4089, loss 0.322029, acc 0.90625, prec 0.0844958, recall 0.8942
2017-12-10T13:05:17.521295: step 4090, loss 0.193316, acc 0.9375, prec 0.0844878, recall 0.8942
2017-12-10T13:05:17.968879: step 4091, loss 0.332127, acc 0.90625, prec 0.0844974, recall 0.894226
2017-12-10T13:05:18.412544: step 4092, loss 0.174475, acc 0.90625, prec 0.084507, recall 0.894252
2017-12-10T13:05:18.854938: step 4093, loss 0.320064, acc 0.90625, prec 0.0845166, recall 0.894279
2017-12-10T13:05:19.288558: step 4094, loss 0.129636, acc 0.96875, prec 0.0845342, recall 0.894305
2017-12-10T13:05:19.737127: step 4095, loss 0.211132, acc 0.96875, prec 0.0845302, recall 0.894305
2017-12-10T13:05:20.177202: step 4096, loss 0.179964, acc 0.953125, prec 0.0845458, recall 0.894331
2017-12-10T13:05:20.613024: step 4097, loss 0.304554, acc 0.9375, prec 0.0845809, recall 0.894384
2017-12-10T13:05:21.054701: step 4098, loss 0.183719, acc 0.953125, prec 0.0845964, recall 0.89441
2017-12-10T13:05:21.503277: step 4099, loss 0.485252, acc 0.90625, prec 0.0846275, recall 0.894462
2017-12-10T13:05:21.946722: step 4100, loss 0.214975, acc 0.921875, prec 0.0846606, recall 0.894515
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4100

2017-12-10T13:05:24.093944: step 4101, loss 0.18768, acc 0.921875, prec 0.0846506, recall 0.894515
2017-12-10T13:05:24.538559: step 4102, loss 0.376415, acc 0.890625, prec 0.0846367, recall 0.894515
2017-12-10T13:05:24.980833: step 4103, loss 0.18524, acc 0.96875, prec 0.0846327, recall 0.894515
2017-12-10T13:05:25.421552: step 4104, loss 0.261801, acc 0.9375, prec 0.0846463, recall 0.894541
2017-12-10T13:05:25.867409: step 4105, loss 0.113692, acc 0.953125, prec 0.0846403, recall 0.894541
2017-12-10T13:05:26.311627: step 4106, loss 0.462723, acc 0.953125, prec 0.0846558, recall 0.894567
2017-12-10T13:05:26.770184: step 4107, loss 0.222734, acc 0.953125, prec 0.0846929, recall 0.894619
2017-12-10T13:05:27.216455: step 4108, loss 0.0756605, acc 0.953125, prec 0.0846869, recall 0.894619
2017-12-10T13:05:27.659310: step 4109, loss 0.028629, acc 0.984375, prec 0.0846849, recall 0.894619
2017-12-10T13:05:28.106910: step 4110, loss 0.123384, acc 0.96875, prec 0.0847239, recall 0.894672
2017-12-10T13:05:28.554947: step 4111, loss 0.217153, acc 0.96875, prec 0.0847414, recall 0.894698
2017-12-10T13:05:29.004502: step 4112, loss 0.189814, acc 0.953125, prec 0.0847569, recall 0.894724
2017-12-10T13:05:29.465815: step 4113, loss 0.244362, acc 0.953125, prec 0.0847509, recall 0.894724
2017-12-10T13:05:29.916268: step 4114, loss 0.193901, acc 0.953125, prec 0.084745, recall 0.894724
2017-12-10T13:05:30.359988: step 4115, loss 0.0593028, acc 0.953125, prec 0.084739, recall 0.894724
2017-12-10T13:05:30.797276: step 4116, loss 0.102687, acc 0.984375, prec 0.0847585, recall 0.89475
2017-12-10T13:05:31.240632: step 4117, loss 0.0696992, acc 0.984375, prec 0.084778, recall 0.894776
2017-12-10T13:05:31.697356: step 4118, loss 0.0895636, acc 0.96875, prec 0.084774, recall 0.894776
2017-12-10T13:05:32.131146: step 4119, loss 0.130464, acc 0.9375, prec 0.084766, recall 0.894776
2017-12-10T13:05:32.579434: step 4120, loss 0.319777, acc 0.9375, prec 0.084801, recall 0.894828
2017-12-10T13:05:33.027856: step 4121, loss 0.0677152, acc 0.96875, prec 0.084797, recall 0.894828
2017-12-10T13:05:33.480959: step 4122, loss 0.0919276, acc 0.96875, prec 0.0847931, recall 0.894828
2017-12-10T13:05:33.930480: step 4123, loss 0.145089, acc 0.984375, prec 0.0848125, recall 0.894854
2017-12-10T13:05:34.394059: step 4124, loss 0.0469988, acc 0.984375, prec 0.0848105, recall 0.894854
2017-12-10T13:05:34.840530: step 4125, loss 0.099265, acc 0.96875, prec 0.084828, recall 0.89488
2017-12-10T13:05:35.289443: step 4126, loss 0.0706202, acc 0.96875, prec 0.0848455, recall 0.894906
2017-12-10T13:05:35.734549: step 4127, loss 0.00603095, acc 1, prec 0.084867, recall 0.894932
2017-12-10T13:05:36.192231: step 4128, loss 0.159465, acc 0.953125, prec 0.084861, recall 0.894932
2017-12-10T13:05:36.637278: step 4129, loss 0.0439319, acc 0.96875, prec 0.0848785, recall 0.894958
2017-12-10T13:05:37.082915: step 4130, loss 0.00141918, acc 1, prec 0.0848785, recall 0.894958
2017-12-10T13:05:37.511874: step 4131, loss 0.0523536, acc 0.984375, prec 0.0848765, recall 0.894958
2017-12-10T13:05:37.964010: step 4132, loss 0.0771599, acc 0.984375, prec 0.0848745, recall 0.894958
2017-12-10T13:05:38.410672: step 4133, loss 0.0711494, acc 0.96875, prec 0.084892, recall 0.894984
2017-12-10T13:05:38.866610: step 4134, loss 0.0470186, acc 0.984375, prec 0.08489, recall 0.894984
2017-12-10T13:05:39.313200: step 4135, loss 0.0259785, acc 0.984375, prec 0.084888, recall 0.894984
2017-12-10T13:05:39.754881: step 4136, loss 0.0126016, acc 1, prec 0.084888, recall 0.894984
2017-12-10T13:05:40.207973: step 4137, loss 0.165319, acc 0.953125, prec 0.084882, recall 0.894984
2017-12-10T13:05:40.651603: step 4138, loss 0.0119159, acc 1, prec 0.0849249, recall 0.895036
2017-12-10T13:05:41.101631: step 4139, loss 0.00171802, acc 1, prec 0.0849249, recall 0.895036
2017-12-10T13:05:41.562441: step 4140, loss 0.755603, acc 0.96875, prec 0.0849852, recall 0.895114
2017-12-10T13:05:42.011973: step 4141, loss 0.121601, acc 0.96875, prec 0.0849813, recall 0.895114
2017-12-10T13:05:42.472470: step 4142, loss 0.423465, acc 1, prec 0.085067, recall 0.895217
2017-12-10T13:05:42.927215: step 4143, loss 0.284913, acc 0.921875, prec 0.085057, recall 0.895217
2017-12-10T13:05:43.370856: step 4144, loss 0.00579552, acc 1, prec 0.0850785, recall 0.895243
2017-12-10T13:05:43.818711: step 4145, loss 0.00541152, acc 1, prec 0.0850785, recall 0.895243
2017-12-10T13:05:44.264426: step 4146, loss 0.0326916, acc 0.96875, prec 0.0850959, recall 0.895269
2017-12-10T13:05:44.713264: step 4147, loss 7.96352, acc 0.96875, prec 0.0850939, recall 0.895048
2017-12-10T13:05:45.163111: step 4148, loss 0.0564662, acc 0.96875, prec 0.0851114, recall 0.895074
2017-12-10T13:05:45.609841: step 4149, loss 0.0762923, acc 0.9375, prec 0.0851034, recall 0.895074
2017-12-10T13:05:46.056072: step 4150, loss 0.150176, acc 0.984375, prec 0.0851443, recall 0.895126
2017-12-10T13:05:46.509441: step 4151, loss 0.15062, acc 0.9375, prec 0.0851363, recall 0.895126
2017-12-10T13:05:46.953614: step 4152, loss 0.308218, acc 0.90625, prec 0.0851457, recall 0.895151
2017-12-10T13:05:47.400935: step 4153, loss 0.515743, acc 0.875, prec 0.0851298, recall 0.895151
2017-12-10T13:05:47.841740: step 4154, loss 0.196589, acc 0.90625, prec 0.0851607, recall 0.895203
2017-12-10T13:05:48.311467: step 4155, loss 0.0673443, acc 0.984375, prec 0.0851587, recall 0.895203
2017-12-10T13:05:48.757619: step 4156, loss 0.0600202, acc 0.953125, prec 0.0851741, recall 0.895229
2017-12-10T13:05:49.216666: step 4157, loss 0.120611, acc 0.90625, prec 0.0851621, recall 0.895229
2017-12-10T13:05:49.657350: step 4158, loss 0.484684, acc 0.90625, prec 0.0851716, recall 0.895254
2017-12-10T13:05:50.104715: step 4159, loss 0.131806, acc 0.921875, prec 0.0851616, recall 0.895254
2017-12-10T13:05:50.547112: step 4160, loss 0.675653, acc 0.90625, prec 0.0851711, recall 0.89528
2017-12-10T13:05:50.996401: step 4161, loss 0.408223, acc 0.921875, prec 0.0851611, recall 0.89528
2017-12-10T13:05:51.445371: step 4162, loss 0.342099, acc 0.90625, prec 0.0851492, recall 0.89528
2017-12-10T13:05:51.886388: step 4163, loss 0.226396, acc 0.9375, prec 0.0851626, recall 0.895306
2017-12-10T13:05:52.335979: step 4164, loss 0.503537, acc 0.890625, prec 0.08517, recall 0.895332
2017-12-10T13:05:52.782902: step 4165, loss 0.384665, acc 0.921875, prec 0.0852028, recall 0.895383
2017-12-10T13:05:53.225637: step 4166, loss 0.272911, acc 0.9375, prec 0.0852163, recall 0.895409
2017-12-10T13:05:53.672241: step 4167, loss 0.505212, acc 0.921875, prec 0.0852277, recall 0.895434
2017-12-10T13:05:54.116712: step 4168, loss 0.228853, acc 0.921875, prec 0.0852391, recall 0.89546
2017-12-10T13:05:54.578156: step 4169, loss 1.8678, acc 0.953125, prec 0.0852351, recall 0.89524
2017-12-10T13:05:55.016089: step 4170, loss 0.209626, acc 0.9375, prec 0.0852699, recall 0.895292
2017-12-10T13:05:55.470773: step 4171, loss 0.379476, acc 0.90625, prec 0.0852579, recall 0.895292
2017-12-10T13:05:55.914702: step 4172, loss 0.230195, acc 0.90625, prec 0.085246, recall 0.895292
2017-12-10T13:05:56.365648: step 4173, loss 0.286075, acc 0.875, prec 0.0852301, recall 0.895292
2017-12-10T13:05:56.828709: step 4174, loss 0.440033, acc 0.90625, prec 0.0852395, recall 0.895317
2017-12-10T13:05:57.280955: step 4175, loss 0.319559, acc 0.90625, prec 0.0852489, recall 0.895343
2017-12-10T13:05:57.723771: step 4176, loss 0.0805926, acc 0.953125, prec 0.0852429, recall 0.895343
2017-12-10T13:05:58.169425: step 4177, loss 0.30615, acc 0.921875, prec 0.085233, recall 0.895343
2017-12-10T13:05:58.626380: step 4178, loss 0.163144, acc 0.953125, prec 0.0852484, recall 0.895369
2017-12-10T13:05:59.064763: step 4179, loss 0.195475, acc 0.953125, prec 0.0852637, recall 0.895394
2017-12-10T13:05:59.513594: step 4180, loss 0.481032, acc 0.875, prec 0.0853118, recall 0.895471
2017-12-10T13:05:59.960156: step 4181, loss 0.118459, acc 0.953125, prec 0.0853058, recall 0.895471
2017-12-10T13:06:00.410509: step 4182, loss 0.12016, acc 0.953125, prec 0.0853852, recall 0.895573
2017-12-10T13:06:00.874826: step 4183, loss 0.159734, acc 0.953125, prec 0.0854005, recall 0.895599
2017-12-10T13:06:01.306520: step 4184, loss 0.396041, acc 0.96875, prec 0.0853966, recall 0.895599
2017-12-10T13:06:01.753282: step 4185, loss 0.0261692, acc 1, prec 0.0854179, recall 0.895625
2017-12-10T13:06:02.196116: step 4186, loss 0.207407, acc 0.96875, prec 0.0854352, recall 0.89565
2017-12-10T13:06:02.639632: step 4187, loss 0.0745832, acc 0.984375, prec 0.0854332, recall 0.89565
2017-12-10T13:06:03.091462: step 4188, loss 0.96191, acc 0.96875, prec 0.0855145, recall 0.895752
2017-12-10T13:06:03.555249: step 4189, loss 0.275814, acc 0.921875, prec 0.0855259, recall 0.895777
2017-12-10T13:06:04.001050: step 4190, loss 0.210587, acc 0.9375, prec 0.0855179, recall 0.895777
2017-12-10T13:06:04.455223: step 4191, loss 0.155095, acc 0.953125, prec 0.0855332, recall 0.895803
2017-12-10T13:06:04.906071: step 4192, loss 0.253231, acc 0.953125, prec 0.0855272, recall 0.895803
2017-12-10T13:06:05.376525: step 4193, loss 0.140698, acc 0.984375, prec 0.0855465, recall 0.895828
2017-12-10T13:06:05.819774: step 4194, loss 0.335194, acc 0.9375, prec 0.0855599, recall 0.895854
2017-12-10T13:06:06.267052: step 4195, loss 0.0996036, acc 0.96875, prec 0.0855559, recall 0.895854
2017-12-10T13:06:06.723129: step 4196, loss 0.367299, acc 0.96875, prec 0.0855945, recall 0.895904
2017-12-10T13:06:07.163350: step 4197, loss 0.10796, acc 0.921875, prec 0.0856058, recall 0.89593
2017-12-10T13:06:07.614470: step 4198, loss 0.456719, acc 0.921875, prec 0.0856384, recall 0.895981
2017-12-10T13:06:08.059382: step 4199, loss 0.202925, acc 0.953125, prec 0.0856538, recall 0.896006
2017-12-10T13:06:08.507561: step 4200, loss 0.286413, acc 0.9375, prec 0.0856883, recall 0.896056
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4200

2017-12-10T13:06:10.625857: step 4201, loss 0.0735099, acc 0.984375, prec 0.0856863, recall 0.896056
2017-12-10T13:06:11.085009: step 4202, loss 0.224856, acc 0.96875, prec 0.0857036, recall 0.896082
2017-12-10T13:06:11.533576: step 4203, loss 0.36745, acc 0.953125, prec 0.0857402, recall 0.896132
2017-12-10T13:06:11.977618: step 4204, loss 0.13135, acc 0.953125, prec 0.0857555, recall 0.896158
2017-12-10T13:06:12.427273: step 4205, loss 0.0277963, acc 1, prec 0.0857555, recall 0.896158
2017-12-10T13:06:12.879931: step 4206, loss 0.138489, acc 0.9375, prec 0.0857688, recall 0.896183
2017-12-10T13:06:13.329131: step 4207, loss 0.249949, acc 0.9375, prec 0.0858034, recall 0.896233
2017-12-10T13:06:13.775809: step 4208, loss 0.271723, acc 0.953125, prec 0.0858186, recall 0.896259
2017-12-10T13:06:14.235451: step 4209, loss 0.139835, acc 0.9375, prec 0.0858107, recall 0.896259
2017-12-10T13:06:14.680009: step 4210, loss 0.164341, acc 0.921875, prec 0.0858007, recall 0.896259
2017-12-10T13:06:15.134385: step 4211, loss 0.314512, acc 0.953125, prec 0.0857947, recall 0.896259
2017-12-10T13:06:15.576802: step 4212, loss 0.0489753, acc 1, prec 0.0858372, recall 0.896309
2017-12-10T13:06:16.016124: step 4213, loss 0.0484351, acc 0.984375, prec 0.0858352, recall 0.896309
2017-12-10T13:06:16.456038: step 4214, loss 0.0138219, acc 1, prec 0.085899, recall 0.896384
2017-12-10T13:06:16.904101: step 4215, loss 0.0170665, acc 1, prec 0.085899, recall 0.896384
2017-12-10T13:06:17.347125: step 4216, loss 0.0182632, acc 1, prec 0.0859202, recall 0.89641
2017-12-10T13:06:17.796554: step 4217, loss 0.271515, acc 0.953125, prec 0.0859355, recall 0.896435
2017-12-10T13:06:18.234621: step 4218, loss 0.0416446, acc 0.984375, prec 0.0859335, recall 0.896435
2017-12-10T13:06:18.697959: step 4219, loss 0.0937805, acc 0.96875, prec 0.0859508, recall 0.89646
2017-12-10T13:06:19.160811: step 4220, loss 0.141613, acc 0.96875, prec 0.085968, recall 0.896485
2017-12-10T13:06:19.594286: step 4221, loss 0.00588096, acc 1, prec 0.085968, recall 0.896485
2017-12-10T13:06:20.043107: step 4222, loss 0.110942, acc 0.96875, prec 0.085964, recall 0.896485
2017-12-10T13:06:20.484975: step 4223, loss 0.828307, acc 0.96875, prec 0.0859813, recall 0.89651
2017-12-10T13:06:20.931202: step 4224, loss 0.237167, acc 0.96875, prec 0.0859985, recall 0.896535
2017-12-10T13:06:21.375372: step 4225, loss 0.237803, acc 0.96875, prec 0.0859945, recall 0.896535
2017-12-10T13:06:21.824462: step 4226, loss 0.011472, acc 1, prec 0.0859945, recall 0.896535
2017-12-10T13:06:22.279697: step 4227, loss 0.0416475, acc 0.984375, prec 0.0860138, recall 0.89656
2017-12-10T13:06:22.720732: step 4228, loss 0.132796, acc 0.953125, prec 0.086029, recall 0.896585
2017-12-10T13:06:23.184852: step 4229, loss 0.0209313, acc 1, prec 0.0860502, recall 0.89661
2017-12-10T13:06:23.629380: step 4230, loss 0.0133952, acc 1, prec 0.0860715, recall 0.896635
2017-12-10T13:06:24.065917: step 4231, loss 4.1765, acc 0.96875, prec 0.0860695, recall 0.896418
2017-12-10T13:06:24.516241: step 4232, loss 0.0831595, acc 0.9375, prec 0.0860827, recall 0.896443
2017-12-10T13:06:24.968739: step 4233, loss 0.27774, acc 0.9375, prec 0.0860959, recall 0.896468
2017-12-10T13:06:25.413172: step 4234, loss 0.260641, acc 0.96875, prec 0.0861344, recall 0.896518
2017-12-10T13:06:25.854143: step 4235, loss 0.327866, acc 0.890625, prec 0.0861204, recall 0.896518
2017-12-10T13:06:26.293213: step 4236, loss 0.443495, acc 0.90625, prec 0.0861084, recall 0.896518
2017-12-10T13:06:26.736336: step 4237, loss 0.342549, acc 0.90625, prec 0.0860964, recall 0.896518
2017-12-10T13:06:27.183565: step 4238, loss 0.221763, acc 0.90625, prec 0.0861056, recall 0.896543
2017-12-10T13:06:27.609979: step 4239, loss 0.635308, acc 0.859375, prec 0.0861089, recall 0.896568
2017-12-10T13:06:28.055442: step 4240, loss 0.500086, acc 0.859375, prec 0.0861121, recall 0.896593
2017-12-10T13:06:28.497368: step 4241, loss 0.245401, acc 0.90625, prec 0.0861425, recall 0.896643
2017-12-10T13:06:28.940430: step 4242, loss 0.151674, acc 0.96875, prec 0.0861597, recall 0.896668
2017-12-10T13:06:29.386753: step 4243, loss 0.227313, acc 0.921875, prec 0.0861921, recall 0.896718
2017-12-10T13:06:29.835540: step 4244, loss 0.431691, acc 0.84375, prec 0.0861721, recall 0.896718
2017-12-10T13:06:30.274952: step 4245, loss 0.755491, acc 0.890625, prec 0.0862641, recall 0.896843
2017-12-10T13:06:30.714383: step 4246, loss 0.445409, acc 0.90625, prec 0.0862732, recall 0.896867
2017-12-10T13:06:31.148815: step 4247, loss 0.597618, acc 0.96875, prec 0.0862904, recall 0.896892
2017-12-10T13:06:31.594483: step 4248, loss 0.152676, acc 0.921875, prec 0.0862804, recall 0.896892
2017-12-10T13:06:32.045246: step 4249, loss 0.164274, acc 0.953125, prec 0.0862744, recall 0.896892
2017-12-10T13:06:32.501790: step 4250, loss 0.0844793, acc 0.96875, prec 0.0863128, recall 0.896942
2017-12-10T13:06:32.958078: step 4251, loss 1.25613, acc 0.921875, prec 0.0863451, recall 0.896992
2017-12-10T13:06:33.410382: step 4252, loss 0.278465, acc 0.953125, prec 0.0863603, recall 0.897016
2017-12-10T13:06:33.863665: step 4253, loss 0.566492, acc 0.9375, prec 0.0863523, recall 0.897016
2017-12-10T13:06:34.316859: step 4254, loss 0.394382, acc 0.875, prec 0.0863574, recall 0.897041
2017-12-10T13:06:34.764789: step 4255, loss 0.243008, acc 0.9375, prec 0.0863706, recall 0.897066
2017-12-10T13:06:35.211809: step 4256, loss 0.162102, acc 0.984375, prec 0.0864109, recall 0.897115
2017-12-10T13:06:35.653274: step 4257, loss 0.101404, acc 0.96875, prec 0.0864069, recall 0.897115
2017-12-10T13:06:36.091367: step 4258, loss 0.28978, acc 0.921875, prec 0.0863969, recall 0.897115
2017-12-10T13:06:36.536722: step 4259, loss 0.401732, acc 0.953125, prec 0.0864543, recall 0.897189
2017-12-10T13:06:36.979358: step 4260, loss 0.468587, acc 0.9375, prec 0.0864463, recall 0.897189
2017-12-10T13:06:37.417808: step 4261, loss 0.153871, acc 0.953125, prec 0.0864615, recall 0.897214
2017-12-10T13:06:37.853148: step 4262, loss 0.0713059, acc 0.96875, prec 0.0864575, recall 0.897214
2017-12-10T13:06:38.307778: step 4263, loss 0.316494, acc 0.953125, prec 0.0864726, recall 0.897239
2017-12-10T13:06:38.759792: step 4264, loss 0.14075, acc 0.9375, prec 0.0864857, recall 0.897264
2017-12-10T13:06:39.228658: step 4265, loss 0.0489093, acc 0.984375, prec 0.0865049, recall 0.897288
2017-12-10T13:06:39.671417: step 4266, loss 0.20951, acc 0.921875, prec 0.0864949, recall 0.897288
2017-12-10T13:06:40.118258: step 4267, loss 0.607514, acc 0.90625, prec 0.086504, recall 0.897313
2017-12-10T13:06:40.570486: step 4268, loss 0.301095, acc 0.875, prec 0.0865091, recall 0.897337
2017-12-10T13:06:41.022623: step 4269, loss 0.0740525, acc 0.96875, prec 0.0865262, recall 0.897362
2017-12-10T13:06:41.466502: step 4270, loss 0.0521983, acc 0.984375, prec 0.0865454, recall 0.897387
2017-12-10T13:06:41.918793: step 4271, loss 0.16402, acc 0.9375, prec 0.0865374, recall 0.897387
2017-12-10T13:06:42.358932: step 4272, loss 0.134458, acc 0.953125, prec 0.0865313, recall 0.897387
2017-12-10T13:06:42.804002: step 4273, loss 0.133668, acc 0.96875, prec 0.0865485, recall 0.897411
2017-12-10T13:06:43.258685: step 4274, loss 0.171114, acc 0.953125, prec 0.0865425, recall 0.897411
2017-12-10T13:06:43.705977: step 4275, loss 0.147308, acc 0.9375, prec 0.0865556, recall 0.897436
2017-12-10T13:06:44.160595: step 4276, loss 0.0714528, acc 0.984375, prec 0.0865536, recall 0.897436
2017-12-10T13:06:44.613740: step 4277, loss 0.191342, acc 0.96875, prec 0.0865496, recall 0.897436
2017-12-10T13:06:45.057813: step 4278, loss 0.0582369, acc 0.96875, prec 0.0865456, recall 0.897436
2017-12-10T13:06:45.505713: step 4279, loss 0.0959507, acc 0.96875, prec 0.0865838, recall 0.897485
2017-12-10T13:06:45.957911: step 4280, loss 0.0097619, acc 1, prec 0.0865838, recall 0.897485
2017-12-10T13:06:46.408615: step 4281, loss 0.0577614, acc 0.984375, prec 0.086624, recall 0.897534
2017-12-10T13:06:46.860226: step 4282, loss 0.300473, acc 0.953125, prec 0.0866602, recall 0.897583
2017-12-10T13:06:47.304051: step 4283, loss 0.0154804, acc 0.984375, prec 0.0866582, recall 0.897583
2017-12-10T13:06:47.769127: step 4284, loss 0.230222, acc 0.984375, prec 0.0866562, recall 0.897583
2017-12-10T13:06:48.233188: step 4285, loss 1.23223, acc 0.953125, prec 0.0866522, recall 0.897368
2017-12-10T13:06:48.683666: step 4286, loss 1.30967, acc 0.9375, prec 0.0867075, recall 0.897442
2017-12-10T13:06:49.117538: step 4287, loss 0.0592425, acc 0.96875, prec 0.0867035, recall 0.897442
2017-12-10T13:06:49.547700: step 4288, loss 0.374713, acc 1, prec 0.0867456, recall 0.897491
2017-12-10T13:06:49.993072: step 4289, loss 0.317439, acc 0.9375, prec 0.0867376, recall 0.897491
2017-12-10T13:06:50.427984: step 4290, loss 0.107206, acc 0.953125, prec 0.0867738, recall 0.89754
2017-12-10T13:06:50.882292: step 4291, loss 0.395573, acc 0.9375, prec 0.086829, recall 0.897613
2017-12-10T13:06:51.330178: step 4292, loss 0.0993691, acc 1, prec 0.0868501, recall 0.897638
2017-12-10T13:06:51.793975: step 4293, loss 0.096291, acc 0.96875, prec 0.0868672, recall 0.897662
2017-12-10T13:06:52.254154: step 4294, loss 0.381059, acc 0.921875, prec 0.0868782, recall 0.897687
2017-12-10T13:06:52.700476: step 4295, loss 0.0469452, acc 1, prec 0.0869204, recall 0.897735
2017-12-10T13:06:53.165189: step 4296, loss 0.193003, acc 0.9375, prec 0.0869124, recall 0.897735
2017-12-10T13:06:53.616146: step 4297, loss 0.0980473, acc 0.953125, prec 0.0869485, recall 0.897784
2017-12-10T13:06:54.069326: step 4298, loss 0.303929, acc 0.90625, prec 0.0869575, recall 0.897808
2017-12-10T13:06:54.527353: step 4299, loss 0.548961, acc 0.921875, prec 0.0869475, recall 0.897808
2017-12-10T13:06:54.965969: step 4300, loss 0.140283, acc 0.953125, prec 0.0869415, recall 0.897808
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4300

2017-12-10T13:06:57.057930: step 4301, loss 0.317422, acc 0.953125, prec 0.0869565, recall 0.897833
2017-12-10T13:06:57.506549: step 4302, loss 0.15315, acc 0.96875, prec 0.0869525, recall 0.897833
2017-12-10T13:06:57.955619: step 4303, loss 0.145367, acc 0.984375, prec 0.0869716, recall 0.897857
2017-12-10T13:06:58.387681: step 4304, loss 0.544128, acc 0.953125, prec 0.0870077, recall 0.897906
2017-12-10T13:06:58.829838: step 4305, loss 0.254772, acc 0.921875, prec 0.0870818, recall 0.898003
2017-12-10T13:06:59.277925: step 4306, loss 0.216667, acc 0.9375, prec 0.0870948, recall 0.898027
2017-12-10T13:06:59.718465: step 4307, loss 0.283862, acc 0.921875, prec 0.0871058, recall 0.898051
2017-12-10T13:07:00.167205: step 4308, loss 0.0412838, acc 0.984375, prec 0.0871038, recall 0.898051
2017-12-10T13:07:00.624718: step 4309, loss 0.125972, acc 0.96875, prec 0.0871419, recall 0.8981
2017-12-10T13:07:01.083777: step 4310, loss 0.251616, acc 0.96875, prec 0.08718, recall 0.898148
2017-12-10T13:07:01.529702: step 4311, loss 0.0359941, acc 0.984375, prec 0.0871779, recall 0.898148
2017-12-10T13:07:01.971179: step 4312, loss 0.0234801, acc 0.984375, prec 0.087197, recall 0.898172
2017-12-10T13:07:02.421867: step 4313, loss 0.0697437, acc 0.96875, prec 0.087214, recall 0.898196
2017-12-10T13:07:02.863649: step 4314, loss 0.0947177, acc 0.96875, prec 0.087231, recall 0.898221
2017-12-10T13:07:03.298730: step 4315, loss 0.0905484, acc 0.9375, prec 0.087244, recall 0.898245
2017-12-10T13:07:03.747816: step 4316, loss 0.185118, acc 0.953125, prec 0.087301, recall 0.898317
2017-12-10T13:07:04.194957: step 4317, loss 0.38973, acc 0.90625, prec 0.087331, recall 0.898365
2017-12-10T13:07:04.638898: step 4318, loss 0.207905, acc 0.921875, prec 0.087363, recall 0.898413
2017-12-10T13:07:05.072558: step 4319, loss 0.670501, acc 0.96875, prec 0.08738, recall 0.898438
2017-12-10T13:07:05.518406: step 4320, loss 0.00915213, acc 1, prec 0.08738, recall 0.898438
2017-12-10T13:07:05.971525: step 4321, loss 0.104937, acc 0.953125, prec 0.0873739, recall 0.898438
2017-12-10T13:07:06.420783: step 4322, loss 0.0712947, acc 0.96875, prec 0.0874119, recall 0.898486
2017-12-10T13:07:06.876175: step 4323, loss 0.28105, acc 0.96875, prec 0.0874499, recall 0.898534
2017-12-10T13:07:07.320566: step 4324, loss 0.383302, acc 0.96875, prec 0.0874669, recall 0.898558
2017-12-10T13:07:07.771002: step 4325, loss 0.104136, acc 0.984375, prec 0.0874649, recall 0.898558
2017-12-10T13:07:08.208294: step 4326, loss 0.438998, acc 0.921875, prec 0.0875178, recall 0.898629
2017-12-10T13:07:08.652178: step 4327, loss 0.334225, acc 0.9375, prec 0.0875518, recall 0.898677
2017-12-10T13:07:09.102276: step 4328, loss 0.145935, acc 0.9375, prec 0.0875437, recall 0.898677
2017-12-10T13:07:09.542750: step 4329, loss 0.230485, acc 0.96875, prec 0.0875397, recall 0.898677
2017-12-10T13:07:09.979188: step 4330, loss 0.0323647, acc 0.984375, prec 0.0875377, recall 0.898677
2017-12-10T13:07:10.416379: step 4331, loss 0.0535033, acc 0.984375, prec 0.0876196, recall 0.898773
2017-12-10T13:07:10.854130: step 4332, loss 0.187327, acc 0.953125, prec 0.0876346, recall 0.898797
2017-12-10T13:07:11.320368: step 4333, loss 0.0445907, acc 0.984375, prec 0.0876325, recall 0.898797
2017-12-10T13:07:11.771062: step 4334, loss 0.18321, acc 0.984375, prec 0.0876725, recall 0.898845
2017-12-10T13:07:12.232733: step 4335, loss 0.0470617, acc 0.96875, prec 0.0876685, recall 0.898845
2017-12-10T13:07:12.681405: step 4336, loss 0.13549, acc 0.9375, prec 0.0876604, recall 0.898845
2017-12-10T13:07:13.122895: step 4337, loss 0.0631583, acc 0.984375, prec 0.0876584, recall 0.898845
2017-12-10T13:07:13.565734: step 4338, loss 0.626715, acc 0.984375, prec 0.0877193, recall 0.898916
2017-12-10T13:07:14.016009: step 4339, loss 0.0317796, acc 0.984375, prec 0.0877173, recall 0.898916
2017-12-10T13:07:14.465726: step 4340, loss 0.115683, acc 0.984375, prec 0.0877362, recall 0.89894
2017-12-10T13:07:14.911560: step 4341, loss 0.191128, acc 0.96875, prec 0.0877951, recall 0.899011
2017-12-10T13:07:15.347684: step 4342, loss 0.0438148, acc 0.96875, prec 0.0877911, recall 0.899011
2017-12-10T13:07:15.794087: step 4343, loss 0.331252, acc 0.953125, prec 0.087785, recall 0.899011
2017-12-10T13:07:16.242028: step 4344, loss 0.126192, acc 0.96875, prec 0.087802, recall 0.899035
2017-12-10T13:07:16.691125: step 4345, loss 0.0223543, acc 1, prec 0.0878229, recall 0.899059
2017-12-10T13:07:17.132345: step 4346, loss 0.219567, acc 0.9375, prec 0.0878149, recall 0.899059
2017-12-10T13:07:17.577643: step 4347, loss 0.270568, acc 1, prec 0.0878568, recall 0.899106
2017-12-10T13:07:18.030691: step 4348, loss 0.0939841, acc 0.96875, prec 0.0878737, recall 0.89913
2017-12-10T13:07:18.468394: step 4349, loss 0.0908016, acc 0.9375, prec 0.0878656, recall 0.89913
2017-12-10T13:07:18.907273: step 4350, loss 0.314822, acc 0.953125, prec 0.0878596, recall 0.89913
2017-12-10T13:07:19.348471: step 4351, loss 0.0153945, acc 1, prec 0.0878596, recall 0.89913
2017-12-10T13:07:19.804670: step 4352, loss 0.0729246, acc 0.953125, prec 0.0878535, recall 0.89913
2017-12-10T13:07:20.257480: step 4353, loss 0.0998139, acc 0.96875, prec 0.0878914, recall 0.899177
2017-12-10T13:07:20.725460: step 4354, loss 0.0214017, acc 1, prec 0.0878914, recall 0.899177
2017-12-10T13:07:21.172983: step 4355, loss 0.514277, acc 0.921875, prec 0.0879022, recall 0.899201
2017-12-10T13:07:21.623548: step 4356, loss 4.12744, acc 0.96875, prec 0.0879212, recall 0.899014
2017-12-10T13:07:22.077565: step 4357, loss 0.0222383, acc 1, prec 0.0879212, recall 0.899014
2017-12-10T13:07:22.512456: step 4358, loss 0.163491, acc 0.96875, prec 0.087959, recall 0.899061
2017-12-10T13:07:22.953341: step 4359, loss 0.269357, acc 0.921875, prec 0.0879908, recall 0.899108
2017-12-10T13:07:23.399385: step 4360, loss 0.130454, acc 0.953125, prec 0.0880057, recall 0.899132
2017-12-10T13:07:23.845825: step 4361, loss 0.222149, acc 0.921875, prec 0.0879956, recall 0.899132
2017-12-10T13:07:24.303439: step 4362, loss 0.113995, acc 0.96875, prec 0.0879916, recall 0.899132
2017-12-10T13:07:24.770802: step 4363, loss 0.216957, acc 0.9375, prec 0.0880044, recall 0.899156
2017-12-10T13:07:25.224371: step 4364, loss 0.216551, acc 0.953125, prec 0.0880193, recall 0.899179
2017-12-10T13:07:25.679077: step 4365, loss 0.0626971, acc 0.96875, prec 0.0880152, recall 0.899179
2017-12-10T13:07:26.119599: step 4366, loss 0.749199, acc 0.984375, prec 0.0880551, recall 0.899227
2017-12-10T13:07:26.564829: step 4367, loss 0.213902, acc 0.921875, prec 0.0880659, recall 0.89925
2017-12-10T13:07:27.017495: step 4368, loss 0.304971, acc 0.921875, prec 0.0880767, recall 0.899274
2017-12-10T13:07:27.458576: step 4369, loss 0.10615, acc 0.96875, prec 0.0881145, recall 0.899321
2017-12-10T13:07:27.904555: step 4370, loss 0.34374, acc 0.890625, prec 0.0881213, recall 0.899345
2017-12-10T13:07:28.365461: step 4371, loss 0.112961, acc 0.96875, prec 0.0881382, recall 0.899368
2017-12-10T13:07:28.827243: step 4372, loss 0.181268, acc 0.9375, prec 0.0881719, recall 0.899415
2017-12-10T13:07:29.273261: step 4373, loss 0.186879, acc 0.90625, prec 0.0881807, recall 0.899439
2017-12-10T13:07:29.719303: step 4374, loss 0.375551, acc 0.90625, prec 0.0881685, recall 0.899439
2017-12-10T13:07:30.162612: step 4375, loss 0.550895, acc 0.875, prec 0.0881733, recall 0.899462
2017-12-10T13:07:30.611438: step 4376, loss 0.156013, acc 0.953125, prec 0.088209, recall 0.899509
2017-12-10T13:07:31.058277: step 4377, loss 0.199586, acc 0.96875, prec 0.0882259, recall 0.899533
2017-12-10T13:07:31.499957: step 4378, loss 0.249516, acc 0.953125, prec 0.0882407, recall 0.899556
2017-12-10T13:07:31.950360: step 4379, loss 0.0752404, acc 0.96875, prec 0.0882366, recall 0.899556
2017-12-10T13:07:32.402972: step 4380, loss 0.176438, acc 0.921875, prec 0.0882265, recall 0.899556
2017-12-10T13:07:32.873822: step 4381, loss 0.148966, acc 0.96875, prec 0.0882434, recall 0.89958
2017-12-10T13:07:33.326290: step 4382, loss 0.235451, acc 0.9375, prec 0.0882353, recall 0.89958
2017-12-10T13:07:33.772998: step 4383, loss 0.118986, acc 0.96875, prec 0.0882313, recall 0.89958
2017-12-10T13:07:34.209299: step 4384, loss 0.5202, acc 0.921875, prec 0.088242, recall 0.899603
2017-12-10T13:07:34.653439: step 4385, loss 0.0678148, acc 0.96875, prec 0.088238, recall 0.899603
2017-12-10T13:07:35.098028: step 4386, loss 0.0867779, acc 0.984375, prec 0.088236, recall 0.899603
2017-12-10T13:07:35.546681: step 4387, loss 0.134878, acc 0.96875, prec 0.0882946, recall 0.899673
2017-12-10T13:07:36.017612: step 4388, loss 0.203053, acc 0.984375, prec 0.0883552, recall 0.899744
2017-12-10T13:07:36.479732: step 4389, loss 0.229686, acc 0.96875, prec 0.0883511, recall 0.899744
2017-12-10T13:07:36.928789: step 4390, loss 0.0204035, acc 1, prec 0.0883929, recall 0.89979
2017-12-10T13:07:37.385332: step 4391, loss 0.185768, acc 0.953125, prec 0.0884077, recall 0.899814
2017-12-10T13:07:37.831361: step 4392, loss 0.0678807, acc 0.96875, prec 0.0884245, recall 0.899837
2017-12-10T13:07:38.265809: step 4393, loss 0.303156, acc 0.984375, prec 0.088485, recall 0.899907
2017-12-10T13:07:38.705884: step 4394, loss 0.206298, acc 0.953125, prec 0.088479, recall 0.899907
2017-12-10T13:07:39.152774: step 4395, loss 0.295838, acc 0.9375, prec 0.0885543, recall 0.9
2017-12-10T13:07:39.622894: step 4396, loss 0.125959, acc 0.96875, prec 0.088592, recall 0.900046
2017-12-10T13:07:40.064761: step 4397, loss 0.298782, acc 0.96875, prec 0.0886296, recall 0.900093
2017-12-10T13:07:40.519503: step 4398, loss 0.0223125, acc 0.984375, prec 0.0886276, recall 0.900093
2017-12-10T13:07:40.979432: step 4399, loss 0.0285986, acc 0.984375, prec 0.0886256, recall 0.900093
2017-12-10T13:07:41.433550: step 4400, loss 0.030742, acc 0.984375, prec 0.0886235, recall 0.900093
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4400

2017-12-10T13:07:43.405194: step 4401, loss 0.0917699, acc 0.96875, prec 0.0886403, recall 0.900116
2017-12-10T13:07:43.848541: step 4402, loss 0.649294, acc 0.96875, prec 0.088678, recall 0.900163
2017-12-10T13:07:44.298848: step 4403, loss 0.226829, acc 0.953125, prec 0.0887135, recall 0.900209
2017-12-10T13:07:44.743411: step 4404, loss 1.01957, acc 0.953125, prec 0.0887908, recall 0.900301
2017-12-10T13:07:45.199443: step 4405, loss 0.0445131, acc 0.96875, prec 0.0887868, recall 0.900301
2017-12-10T13:07:45.652730: step 4406, loss 0.185504, acc 0.953125, prec 0.0887807, recall 0.900301
2017-12-10T13:07:46.095058: step 4407, loss 0.537525, acc 0.984375, prec 0.0888203, recall 0.900348
2017-12-10T13:07:46.548117: step 4408, loss 0.150454, acc 0.96875, prec 0.0888162, recall 0.900348
2017-12-10T13:07:47.012078: step 4409, loss 0.496658, acc 0.984375, prec 0.088835, recall 0.900371
2017-12-10T13:07:47.452847: step 4410, loss 0.214675, acc 0.921875, prec 0.0888457, recall 0.900394
2017-12-10T13:07:47.895758: step 4411, loss 0.25238, acc 0.953125, prec 0.0888813, recall 0.90044
2017-12-10T13:07:48.334270: step 4412, loss 0.135875, acc 0.953125, prec 0.0889168, recall 0.900486
2017-12-10T13:07:48.762766: step 4413, loss 0.478217, acc 0.9375, prec 0.0889295, recall 0.900509
2017-12-10T13:07:49.206383: step 4414, loss 0.279974, acc 0.90625, prec 0.0889173, recall 0.900509
2017-12-10T13:07:49.648079: step 4415, loss 0.433742, acc 0.890625, prec 0.0889239, recall 0.900532
2017-12-10T13:07:50.088071: step 4416, loss 0.627743, acc 0.890625, prec 0.0889305, recall 0.900555
2017-12-10T13:07:50.538768: step 4417, loss 0.284107, acc 0.921875, prec 0.0889412, recall 0.900578
2017-12-10T13:07:50.982097: step 4418, loss 0.160472, acc 0.984375, prec 0.0889807, recall 0.900624
2017-12-10T13:07:51.413468: step 4419, loss 0.271306, acc 0.921875, prec 0.0889706, recall 0.900624
2017-12-10T13:07:51.866031: step 4420, loss 0.2189, acc 0.890625, prec 0.0889772, recall 0.900647
2017-12-10T13:07:52.321831: step 4421, loss 0.536222, acc 0.890625, prec 0.0889629, recall 0.900647
2017-12-10T13:07:52.767818: step 4422, loss 0.453069, acc 0.9375, prec 0.0889964, recall 0.900693
2017-12-10T13:07:53.205192: step 4423, loss 0.450194, acc 0.890625, prec 0.0889822, recall 0.900693
2017-12-10T13:07:53.647943: step 4424, loss 0.320229, acc 0.953125, prec 0.0889761, recall 0.900693
2017-12-10T13:07:54.094007: step 4425, loss 0.231803, acc 0.921875, prec 0.0889867, recall 0.900716
2017-12-10T13:07:54.536857: step 4426, loss 0.0759152, acc 0.984375, prec 0.0889847, recall 0.900716
2017-12-10T13:07:54.978640: step 4427, loss 0.0921593, acc 0.953125, prec 0.0889786, recall 0.900716
2017-12-10T13:07:55.415796: step 4428, loss 0.080514, acc 0.984375, prec 0.0889974, recall 0.900739
2017-12-10T13:07:55.873591: step 4429, loss 0.0483711, acc 0.953125, prec 0.089012, recall 0.900762
2017-12-10T13:07:56.316794: step 4430, loss 0.010426, acc 1, prec 0.0890536, recall 0.900807
2017-12-10T13:07:56.759824: step 4431, loss 0.219725, acc 0.953125, prec 0.0890475, recall 0.900807
2017-12-10T13:07:57.212563: step 4432, loss 0.0830413, acc 0.984375, prec 0.0890662, recall 0.90083
2017-12-10T13:07:57.667215: step 4433, loss 0.156122, acc 0.953125, prec 0.0891017, recall 0.900876
2017-12-10T13:07:58.113395: step 4434, loss 0.103556, acc 0.96875, prec 0.0891184, recall 0.900899
2017-12-10T13:07:58.563559: step 4435, loss 0.0121575, acc 1, prec 0.0891392, recall 0.900922
2017-12-10T13:07:59.019696: step 4436, loss 0.0217096, acc 1, prec 0.0891392, recall 0.900922
2017-12-10T13:07:59.479544: step 4437, loss 0.107297, acc 0.96875, prec 0.0891559, recall 0.900944
2017-12-10T13:07:59.929674: step 4438, loss 0.0364705, acc 0.984375, prec 0.0891746, recall 0.900967
2017-12-10T13:08:00.375133: step 4439, loss 0.194072, acc 0.9375, prec 0.089208, recall 0.901013
2017-12-10T13:08:00.828992: step 4440, loss 0.0549506, acc 0.984375, prec 0.0892059, recall 0.901013
2017-12-10T13:08:01.286876: step 4441, loss 0.0692147, acc 1, prec 0.0892267, recall 0.901036
2017-12-10T13:08:01.728151: step 4442, loss 0.0915125, acc 0.96875, prec 0.0892434, recall 0.901058
2017-12-10T13:08:02.169603: step 4443, loss 0.0674233, acc 0.984375, prec 0.0892621, recall 0.901081
2017-12-10T13:08:02.639582: step 4444, loss 0.0317132, acc 0.984375, prec 0.0892601, recall 0.901081
2017-12-10T13:08:03.082399: step 4445, loss 0.0140799, acc 1, prec 0.0892601, recall 0.901081
2017-12-10T13:08:03.533105: step 4446, loss 0.694891, acc 1, prec 0.0893016, recall 0.901127
2017-12-10T13:08:03.988435: step 4447, loss 0.17736, acc 0.96875, prec 0.0892975, recall 0.901127
2017-12-10T13:08:04.446142: step 4448, loss 0.189, acc 0.96875, prec 0.0893142, recall 0.901149
2017-12-10T13:08:04.902680: step 4449, loss 0.0805322, acc 0.96875, prec 0.0893101, recall 0.901149
2017-12-10T13:08:05.346586: step 4450, loss 0.2369, acc 0.9375, prec 0.089302, recall 0.901149
2017-12-10T13:08:05.805416: step 4451, loss 0.124119, acc 0.984375, prec 0.0893414, recall 0.901195
2017-12-10T13:08:06.267582: step 4452, loss 0.211, acc 0.984375, prec 0.0893601, recall 0.901218
2017-12-10T13:08:06.726642: step 4453, loss 2.48601, acc 0.96875, prec 0.0893789, recall 0.901033
2017-12-10T13:08:07.178752: step 4454, loss 0.0942135, acc 0.96875, prec 0.0893748, recall 0.901033
2017-12-10T13:08:07.625907: step 4455, loss 0.0788612, acc 0.953125, prec 0.0893687, recall 0.901033
2017-12-10T13:08:08.079475: step 4456, loss 0.327144, acc 0.9375, prec 0.0894227, recall 0.901101
2017-12-10T13:08:08.519722: step 4457, loss 0.192392, acc 0.9375, prec 0.0894146, recall 0.901101
2017-12-10T13:08:08.958667: step 4458, loss 0.234632, acc 0.90625, prec 0.0894438, recall 0.901147
2017-12-10T13:08:09.398289: step 4459, loss 0.25167, acc 0.953125, prec 0.0894377, recall 0.901147
2017-12-10T13:08:09.853882: step 4460, loss 0.160211, acc 0.9375, prec 0.0894503, recall 0.901169
2017-12-10T13:08:10.299628: step 4461, loss 0.176713, acc 0.9375, prec 0.0894836, recall 0.901215
2017-12-10T13:08:10.738016: step 4462, loss 0.532348, acc 0.84375, prec 0.0894633, recall 0.901215
2017-12-10T13:08:11.180287: step 4463, loss 0.367072, acc 0.90625, prec 0.0894718, recall 0.901237
2017-12-10T13:08:11.625027: step 4464, loss 0.174228, acc 0.921875, prec 0.0895237, recall 0.901305
2017-12-10T13:08:12.077963: step 4465, loss 0.427624, acc 0.890625, prec 0.0895302, recall 0.901328
2017-12-10T13:08:12.540781: step 4466, loss 0.209466, acc 0.953125, prec 0.0895448, recall 0.90135
2017-12-10T13:08:12.989071: step 4467, loss 0.246317, acc 0.890625, prec 0.0895305, recall 0.90135
2017-12-10T13:08:13.424724: step 4468, loss 0.205998, acc 0.96875, prec 0.0895471, recall 0.901373
2017-12-10T13:08:13.881492: step 4469, loss 0.645382, acc 0.890625, prec 0.0896364, recall 0.901486
2017-12-10T13:08:14.333802: step 4470, loss 0.232439, acc 0.9375, prec 0.0896282, recall 0.901486
2017-12-10T13:08:14.784745: step 4471, loss 0.088537, acc 0.953125, prec 0.0896221, recall 0.901486
2017-12-10T13:08:15.229337: step 4472, loss 0.370147, acc 0.875, prec 0.0896472, recall 0.901531
2017-12-10T13:08:15.620288: step 4473, loss 0.248931, acc 0.923077, prec 0.0896597, recall 0.901553
2017-12-10T13:08:16.074830: step 4474, loss 0.273105, acc 0.9375, prec 0.0896516, recall 0.901553
2017-12-10T13:08:16.515336: step 4475, loss 0.276016, acc 0.9375, prec 0.0896641, recall 0.901576
2017-12-10T13:08:16.952846: step 4476, loss 0.0239728, acc 0.984375, prec 0.0896827, recall 0.901598
2017-12-10T13:08:17.401157: step 4477, loss 0.198118, acc 0.953125, prec 0.0896973, recall 0.901621
2017-12-10T13:08:17.844471: step 4478, loss 0.00710069, acc 1, prec 0.0896973, recall 0.901621
2017-12-10T13:08:18.289976: step 4479, loss 0.156185, acc 0.953125, prec 0.0896912, recall 0.901621
2017-12-10T13:08:18.748880: step 4480, loss 0.285951, acc 0.921875, prec 0.0897017, recall 0.901643
2017-12-10T13:08:19.198191: step 4481, loss 0.0391072, acc 0.984375, prec 0.0896996, recall 0.901643
2017-12-10T13:08:19.649812: step 4482, loss 0.136633, acc 0.96875, prec 0.0897576, recall 0.90171
2017-12-10T13:08:20.088380: step 4483, loss 0.1348, acc 0.953125, prec 0.0897514, recall 0.90171
2017-12-10T13:08:20.542010: step 4484, loss 0.062568, acc 0.984375, prec 0.0897494, recall 0.90171
2017-12-10T13:08:20.986630: step 4485, loss 0.14975, acc 0.953125, prec 0.0897433, recall 0.90171
2017-12-10T13:08:21.431613: step 4486, loss 0.021339, acc 1, prec 0.0897433, recall 0.90171
2017-12-10T13:08:21.880918: step 4487, loss 0.0331533, acc 0.984375, prec 0.0897413, recall 0.90171
2017-12-10T13:08:22.321342: step 4488, loss 0.0328522, acc 0.984375, prec 0.0898012, recall 0.901778
2017-12-10T13:08:22.768579: step 4489, loss 0.00215841, acc 1, prec 0.0898012, recall 0.901778
2017-12-10T13:08:23.203981: step 4490, loss 0.0137824, acc 1, prec 0.0898012, recall 0.901778
2017-12-10T13:08:23.646787: step 4491, loss 2.34229, acc 0.953125, prec 0.0897971, recall 0.901572
2017-12-10T13:08:24.111984: step 4492, loss 0.135035, acc 0.984375, prec 0.089857, recall 0.901639
2017-12-10T13:08:24.555464: step 4493, loss 0.0963606, acc 0.953125, prec 0.0898509, recall 0.901639
2017-12-10T13:08:25.006592: step 4494, loss 0.00497445, acc 1, prec 0.0898716, recall 0.901662
2017-12-10T13:08:25.435579: step 4495, loss 0.0985781, acc 0.96875, prec 0.0898675, recall 0.901662
2017-12-10T13:08:25.889310: step 4496, loss 1.79312, acc 0.9375, prec 0.089882, recall 0.901479
2017-12-10T13:08:26.347033: step 4497, loss 0.113168, acc 0.96875, prec 0.0899192, recall 0.901524
2017-12-10T13:08:26.793167: step 4498, loss 0.12959, acc 0.96875, prec 0.0899564, recall 0.901569
2017-12-10T13:08:27.244025: step 4499, loss 0.04491, acc 0.96875, prec 0.0899524, recall 0.901569
2017-12-10T13:08:27.697211: step 4500, loss 0.38729, acc 0.90625, prec 0.0899608, recall 0.901591
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4500

2017-12-10T13:08:29.700117: step 4501, loss 0.110129, acc 0.9375, prec 0.0900145, recall 0.901658
2017-12-10T13:08:30.146298: step 4502, loss 0.0399949, acc 0.984375, prec 0.0900744, recall 0.901725
2017-12-10T13:08:30.584719: step 4503, loss 0.462061, acc 0.9375, prec 0.0901074, recall 0.90177
2017-12-10T13:08:31.026989: step 4504, loss 0.209638, acc 0.921875, prec 0.0900972, recall 0.90177
2017-12-10T13:08:31.473218: step 4505, loss 0.197558, acc 0.890625, prec 0.0900829, recall 0.90177
2017-12-10T13:08:31.926491: step 4506, loss 0.204735, acc 0.921875, prec 0.0900934, recall 0.901792
2017-12-10T13:08:32.380534: step 4507, loss 0.113622, acc 0.96875, prec 0.0901305, recall 0.901836
2017-12-10T13:08:32.830900: step 4508, loss 0.370293, acc 0.890625, prec 0.0901162, recall 0.901836
2017-12-10T13:08:33.274690: step 4509, loss 0.469224, acc 0.9375, prec 0.0901493, recall 0.901881
2017-12-10T13:08:33.725429: step 4510, loss 0.194745, acc 0.90625, prec 0.0902194, recall 0.90197
2017-12-10T13:08:34.168399: step 4511, loss 0.258128, acc 0.90625, prec 0.0902072, recall 0.90197
2017-12-10T13:08:34.610796: step 4512, loss 0.284534, acc 0.96875, prec 0.0902237, recall 0.901992
2017-12-10T13:08:35.052344: step 4513, loss 0.223545, acc 0.921875, prec 0.0902135, recall 0.901992
2017-12-10T13:08:35.519379: step 4514, loss 0.137548, acc 0.9375, prec 0.0902053, recall 0.901992
2017-12-10T13:08:35.974148: step 4515, loss 0.334443, acc 0.953125, prec 0.0901992, recall 0.901992
2017-12-10T13:08:36.425863: step 4516, loss 0.0512454, acc 0.984375, prec 0.0901971, recall 0.901992
2017-12-10T13:08:36.864866: step 4517, loss 0.265491, acc 0.96875, prec 0.0902136, recall 0.902014
2017-12-10T13:08:37.309817: step 4518, loss 0.132764, acc 0.953125, prec 0.0902281, recall 0.902036
2017-12-10T13:08:37.763510: step 4519, loss 0.217019, acc 0.90625, prec 0.0902365, recall 0.902058
2017-12-10T13:08:38.207476: step 4520, loss 0.14541, acc 0.953125, prec 0.0902303, recall 0.902058
2017-12-10T13:08:38.651503: step 4521, loss 0.212357, acc 0.96875, prec 0.0902262, recall 0.902058
2017-12-10T13:08:39.094417: step 4522, loss 0.113698, acc 0.953125, prec 0.0902201, recall 0.902058
2017-12-10T13:08:39.547628: step 4523, loss 0.114692, acc 0.96875, prec 0.0902366, recall 0.902081
2017-12-10T13:08:39.993847: step 4524, loss 0.0627486, acc 0.96875, prec 0.0903149, recall 0.902169
2017-12-10T13:08:40.451135: step 4525, loss 0.121274, acc 0.96875, prec 0.0903931, recall 0.902257
2017-12-10T13:08:40.894542: step 4526, loss 0.0669419, acc 0.984375, prec 0.090391, recall 0.902257
2017-12-10T13:08:41.347619: step 4527, loss 0.180112, acc 0.9375, prec 0.0904034, recall 0.902279
2017-12-10T13:08:41.794698: step 4528, loss 0.181145, acc 0.96875, prec 0.0904199, recall 0.902301
2017-12-10T13:08:42.242248: step 4529, loss 0.0972793, acc 0.96875, prec 0.0904158, recall 0.902301
2017-12-10T13:08:42.680717: step 4530, loss 0.137044, acc 0.96875, prec 0.0904323, recall 0.902323
2017-12-10T13:08:43.129222: step 4531, loss 0.0197547, acc 1, prec 0.0904528, recall 0.902346
2017-12-10T13:08:43.581397: step 4532, loss 0.111941, acc 0.953125, prec 0.0904673, recall 0.902368
2017-12-10T13:08:44.034137: step 4533, loss 0.00324831, acc 1, prec 0.0904673, recall 0.902368
2017-12-10T13:08:44.480224: step 4534, loss 0.453682, acc 0.984375, prec 0.0904858, recall 0.90239
2017-12-10T13:08:44.924873: step 4535, loss 0.0696299, acc 0.96875, prec 0.0904817, recall 0.90239
2017-12-10T13:08:45.366160: step 4536, loss 0.0629861, acc 1, prec 0.0905228, recall 0.902434
2017-12-10T13:08:45.825304: step 4537, loss 0.0866085, acc 0.96875, prec 0.0905187, recall 0.902434
2017-12-10T13:08:46.283597: step 4538, loss 0.03575, acc 0.96875, prec 0.0905146, recall 0.902434
2017-12-10T13:08:46.731603: step 4539, loss 0.0210699, acc 0.984375, prec 0.0905126, recall 0.902434
2017-12-10T13:08:47.198594: step 4540, loss 0.134638, acc 0.96875, prec 0.0905085, recall 0.902434
2017-12-10T13:08:47.631364: step 4541, loss 0.0111221, acc 1, prec 0.090529, recall 0.902456
2017-12-10T13:08:48.073115: step 4542, loss 0.0596486, acc 0.96875, prec 0.0905455, recall 0.902478
2017-12-10T13:08:48.509353: step 4543, loss 0.022925, acc 1, prec 0.0905455, recall 0.902478
2017-12-10T13:08:48.958430: step 4544, loss 0.21774, acc 0.96875, prec 0.0905414, recall 0.902478
2017-12-10T13:08:49.397196: step 4545, loss 0.376508, acc 0.96875, prec 0.0905579, recall 0.902499
2017-12-10T13:08:49.836772: step 4546, loss 0.0147448, acc 1, prec 0.0905579, recall 0.902499
2017-12-10T13:08:50.271380: step 4547, loss 0.00293957, acc 1, prec 0.0905784, recall 0.902521
2017-12-10T13:08:50.714822: step 4548, loss 0.00875218, acc 1, prec 0.0905784, recall 0.902521
2017-12-10T13:08:51.155980: step 4549, loss 0.0278472, acc 0.984375, prec 0.0905764, recall 0.902521
2017-12-10T13:08:51.601096: step 4550, loss 0.396759, acc 0.953125, prec 0.0905908, recall 0.902543
2017-12-10T13:08:52.040252: step 4551, loss 0.103134, acc 0.984375, prec 0.0906093, recall 0.902565
2017-12-10T13:08:52.473752: step 4552, loss 0.161444, acc 0.921875, prec 0.0906196, recall 0.902587
2017-12-10T13:08:52.907998: step 4553, loss 1.99247, acc 0.96875, prec 0.0906175, recall 0.902384
2017-12-10T13:08:53.351828: step 4554, loss 0.042395, acc 0.96875, prec 0.090634, recall 0.902406
2017-12-10T13:08:53.797218: step 4555, loss 0.0468959, acc 0.96875, prec 0.0906709, recall 0.90245
2017-12-10T13:08:54.251190: step 4556, loss 0.224642, acc 0.9375, prec 0.0907243, recall 0.902516
2017-12-10T13:08:54.704041: step 4557, loss 0.465082, acc 0.953125, prec 0.0907798, recall 0.902581
2017-12-10T13:08:55.149313: step 4558, loss 0.26941, acc 0.9375, prec 0.0907716, recall 0.902581
2017-12-10T13:08:55.607599: step 4559, loss 0.0941021, acc 0.96875, prec 0.0907675, recall 0.902581
2017-12-10T13:08:56.057202: step 4560, loss 0.295054, acc 0.921875, prec 0.0907778, recall 0.902603
2017-12-10T13:08:56.498583: step 4561, loss 0.32712, acc 0.90625, prec 0.0907655, recall 0.902603
2017-12-10T13:08:56.935685: step 4562, loss 0.200046, acc 0.9375, prec 0.0907983, recall 0.902647
2017-12-10T13:08:57.379332: step 4563, loss 0.333528, acc 0.953125, prec 0.0908332, recall 0.902691
2017-12-10T13:08:57.832545: step 4564, loss 0.219, acc 0.90625, prec 0.0908414, recall 0.902712
2017-12-10T13:08:58.270390: step 4565, loss 0.444214, acc 0.84375, prec 0.0908209, recall 0.902712
2017-12-10T13:08:58.719818: step 4566, loss 0.320846, acc 0.953125, prec 0.0908148, recall 0.902712
2017-12-10T13:08:59.173343: step 4567, loss 0.307975, acc 0.890625, prec 0.090821, recall 0.902734
2017-12-10T13:08:59.625086: step 4568, loss 0.366896, acc 0.921875, prec 0.0908107, recall 0.902734
2017-12-10T13:09:00.071966: step 4569, loss 0.53681, acc 0.890625, prec 0.0908374, recall 0.902778
2017-12-10T13:09:00.502479: step 4570, loss 0.130695, acc 0.90625, prec 0.0908251, recall 0.902778
2017-12-10T13:09:00.948294: step 4571, loss 0.211274, acc 0.921875, prec 0.0908149, recall 0.902778
2017-12-10T13:09:01.399801: step 4572, loss 0.257115, acc 0.90625, prec 0.0908026, recall 0.902778
2017-12-10T13:09:01.857718: step 4573, loss 0.174454, acc 0.9375, prec 0.0907944, recall 0.902778
2017-12-10T13:09:02.319012: step 4574, loss 0.279804, acc 0.96875, prec 0.0908313, recall 0.902821
2017-12-10T13:09:02.778900: step 4575, loss 0.223843, acc 0.921875, prec 0.090821, recall 0.902821
2017-12-10T13:09:03.236922: step 4576, loss 0.178707, acc 0.953125, prec 0.0908354, recall 0.902843
2017-12-10T13:09:03.665934: step 4577, loss 0.144927, acc 0.9375, prec 0.0908477, recall 0.902865
2017-12-10T13:09:04.111284: step 4578, loss 0.0812973, acc 0.96875, prec 0.0908436, recall 0.902865
2017-12-10T13:09:04.541624: step 4579, loss 0.135051, acc 0.96875, prec 0.0908804, recall 0.902908
2017-12-10T13:09:04.983561: step 4580, loss 0.284705, acc 0.9375, prec 0.0908723, recall 0.902908
2017-12-10T13:09:05.432348: step 4581, loss 0.251121, acc 0.96875, prec 0.0908886, recall 0.90293
2017-12-10T13:09:05.876753: step 4582, loss 0.01689, acc 1, prec 0.0908886, recall 0.90293
2017-12-10T13:09:06.329380: step 4583, loss 0.207741, acc 0.96875, prec 0.0909664, recall 0.903017
2017-12-10T13:09:06.778906: step 4584, loss 0.082641, acc 0.96875, prec 0.0909623, recall 0.903017
2017-12-10T13:09:07.216066: step 4585, loss 0.00757075, acc 1, prec 0.0909623, recall 0.903017
2017-12-10T13:09:07.669996: step 4586, loss 0.0270253, acc 1, prec 0.0910441, recall 0.903103
2017-12-10T13:09:08.131065: step 4587, loss 0.341159, acc 0.921875, prec 0.0911157, recall 0.90319
2017-12-10T13:09:08.591183: step 4588, loss 0.128824, acc 0.96875, prec 0.0911116, recall 0.90319
2017-12-10T13:09:09.050695: step 4589, loss 0.0404848, acc 0.984375, prec 0.09113, recall 0.903211
2017-12-10T13:09:09.507152: step 4590, loss 0.0983682, acc 0.984375, prec 0.0911484, recall 0.903233
2017-12-10T13:09:09.958913: step 4591, loss 0.130176, acc 0.96875, prec 0.0911443, recall 0.903233
2017-12-10T13:09:10.396607: step 4592, loss 0.0708283, acc 0.96875, prec 0.0911402, recall 0.903233
2017-12-10T13:09:10.839934: step 4593, loss 0.0832161, acc 0.96875, prec 0.0911361, recall 0.903233
2017-12-10T13:09:11.295266: step 4594, loss 0.0615614, acc 0.984375, prec 0.0911341, recall 0.903233
2017-12-10T13:09:11.743866: step 4595, loss 0.0778534, acc 0.984375, prec 0.0911525, recall 0.903255
2017-12-10T13:09:12.206152: step 4596, loss 0.235034, acc 0.953125, prec 0.0912281, recall 0.903341
2017-12-10T13:09:12.649184: step 4597, loss 0.148045, acc 1, prec 0.0912485, recall 0.903362
2017-12-10T13:09:13.105473: step 4598, loss 0.0587609, acc 0.96875, prec 0.0912648, recall 0.903384
2017-12-10T13:09:13.551958: step 4599, loss 0.00454699, acc 1, prec 0.0912648, recall 0.903384
2017-12-10T13:09:13.995544: step 4600, loss 0.100895, acc 0.96875, prec 0.0912607, recall 0.903384
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4600

2017-12-10T13:09:16.178326: step 4601, loss 0.185813, acc 0.984375, prec 0.0912791, recall 0.903405
2017-12-10T13:09:16.631814: step 4602, loss 0.101083, acc 0.96875, prec 0.0912955, recall 0.903427
2017-12-10T13:09:17.080656: step 4603, loss 0.0622372, acc 0.96875, prec 0.0913118, recall 0.903448
2017-12-10T13:09:17.529334: step 4604, loss 0.00237493, acc 1, prec 0.0913118, recall 0.903448
2017-12-10T13:09:17.971155: step 4605, loss 1.26347, acc 0.953125, prec 0.0913465, recall 0.903491
2017-12-10T13:09:18.415659: step 4606, loss 0.015151, acc 1, prec 0.0913465, recall 0.903491
2017-12-10T13:09:18.862928: step 4607, loss 0.0514287, acc 0.984375, prec 0.0913444, recall 0.903491
2017-12-10T13:09:19.315744: step 4608, loss 0.154965, acc 0.953125, prec 0.0913383, recall 0.903491
2017-12-10T13:09:19.760141: step 4609, loss 0.0323885, acc 0.96875, prec 0.0913342, recall 0.903491
2017-12-10T13:09:20.212476: step 4610, loss 0.109472, acc 0.953125, prec 0.0913688, recall 0.903534
2017-12-10T13:09:20.658244: step 4611, loss 0.257806, acc 0.953125, prec 0.0913627, recall 0.903534
2017-12-10T13:09:21.098950: step 4612, loss 0.0625992, acc 0.96875, prec 0.0913586, recall 0.903534
2017-12-10T13:09:21.542889: step 4613, loss 0.132322, acc 0.96875, prec 0.0913545, recall 0.903534
2017-12-10T13:09:21.996009: step 4614, loss 0.213087, acc 0.953125, prec 0.0913483, recall 0.903534
2017-12-10T13:09:22.451334: step 4615, loss 0.0611526, acc 0.984375, prec 0.0913667, recall 0.903556
2017-12-10T13:09:22.908681: step 4616, loss 0.319202, acc 0.9375, prec 0.0913789, recall 0.903577
2017-12-10T13:09:23.359778: step 4617, loss 0.207755, acc 0.984375, prec 0.0913768, recall 0.903577
2017-12-10T13:09:23.809900: step 4618, loss 0.109867, acc 0.96875, prec 0.0913931, recall 0.903598
2017-12-10T13:09:24.266769: step 4619, loss 0.06395, acc 0.96875, prec 0.091389, recall 0.903598
2017-12-10T13:09:24.717622: step 4620, loss 0.145282, acc 0.953125, prec 0.0914237, recall 0.903641
2017-12-10T13:09:25.160360: step 4621, loss 0.0548298, acc 0.984375, prec 0.091442, recall 0.903663
2017-12-10T13:09:25.609521: step 4622, loss 0.139566, acc 0.9375, prec 0.0914338, recall 0.903663
2017-12-10T13:09:26.054284: step 4623, loss 0.105697, acc 0.96875, prec 0.0914501, recall 0.903684
2017-12-10T13:09:26.510223: step 4624, loss 0.495343, acc 0.9375, prec 0.0914419, recall 0.903684
2017-12-10T13:09:26.961281: step 4625, loss 0.016171, acc 1, prec 0.0914419, recall 0.903684
2017-12-10T13:09:27.407876: step 4626, loss 0.336157, acc 0.953125, prec 0.091497, recall 0.903748
2017-12-10T13:09:27.861144: step 4627, loss 0.10719, acc 0.984375, prec 0.0914949, recall 0.903748
2017-12-10T13:09:28.309076: step 4628, loss 0.122974, acc 0.984375, prec 0.0914928, recall 0.903748
2017-12-10T13:09:28.765492: step 4629, loss 0.375069, acc 0.96875, prec 0.0915091, recall 0.903769
2017-12-10T13:09:29.223591: step 4630, loss 0.392016, acc 0.921875, prec 0.0915193, recall 0.903791
2017-12-10T13:09:29.673572: step 4631, loss 0.0443228, acc 0.96875, prec 0.0915355, recall 0.903812
2017-12-10T13:09:30.119537: step 4632, loss 0.0662623, acc 0.984375, prec 0.0915539, recall 0.903833
2017-12-10T13:09:30.562683: step 4633, loss 0.144573, acc 0.9375, prec 0.0915864, recall 0.903876
2017-12-10T13:09:31.008421: step 4634, loss 0.0351069, acc 0.984375, prec 0.0916048, recall 0.903897
2017-12-10T13:09:31.457923: step 4635, loss 0.147586, acc 0.953125, prec 0.0915986, recall 0.903897
2017-12-10T13:09:31.892553: step 4636, loss 0.226913, acc 0.953125, prec 0.0915924, recall 0.903897
2017-12-10T13:09:32.337762: step 4637, loss 0.371035, acc 0.9375, prec 0.0916046, recall 0.903919
2017-12-10T13:09:32.778927: step 4638, loss 0.220992, acc 0.953125, prec 0.0916596, recall 0.903982
2017-12-10T13:09:33.222089: step 4639, loss 0.125913, acc 0.9375, prec 0.0916513, recall 0.903982
2017-12-10T13:09:33.685996: step 4640, loss 1.61118, acc 0.9375, prec 0.0916859, recall 0.903825
2017-12-10T13:09:34.138845: step 4641, loss 0.189725, acc 0.921875, prec 0.0916756, recall 0.903825
2017-12-10T13:09:34.586563: step 4642, loss 0.141858, acc 0.96875, prec 0.0916715, recall 0.903825
2017-12-10T13:09:35.024693: step 4643, loss 0.115103, acc 0.953125, prec 0.0916857, recall 0.903846
2017-12-10T13:09:35.490153: step 4644, loss 0.172998, acc 0.96875, prec 0.0917223, recall 0.903889
2017-12-10T13:09:35.941802: step 4645, loss 0.788887, acc 0.96875, prec 0.0917793, recall 0.903952
2017-12-10T13:09:36.388675: step 4646, loss 0.250913, acc 0.953125, prec 0.0917731, recall 0.903952
2017-12-10T13:09:36.844222: step 4647, loss 0.128053, acc 0.953125, prec 0.0917873, recall 0.903974
2017-12-10T13:09:37.278721: step 4648, loss 0.0821135, acc 0.953125, prec 0.0917812, recall 0.903974
2017-12-10T13:09:37.711871: step 4649, loss 0.201126, acc 0.9375, prec 0.0917729, recall 0.903974
2017-12-10T13:09:38.166304: step 4650, loss 0.157446, acc 0.921875, prec 0.0917626, recall 0.903974
2017-12-10T13:09:38.636625: step 4651, loss 0.157503, acc 0.9375, prec 0.0918155, recall 0.904037
2017-12-10T13:09:39.100606: step 4652, loss 0.32349, acc 0.921875, prec 0.0918255, recall 0.904058
2017-12-10T13:09:39.562644: step 4653, loss 0.385282, acc 0.890625, prec 0.0918315, recall 0.904079
2017-12-10T13:09:40.006016: step 4654, loss 0.168845, acc 0.953125, prec 0.0918253, recall 0.904079
2017-12-10T13:09:40.450813: step 4655, loss 0.301901, acc 0.921875, prec 0.091815, recall 0.904079
2017-12-10T13:09:40.896876: step 4656, loss 0.169864, acc 0.953125, prec 0.0918292, recall 0.904101
2017-12-10T13:09:41.353690: step 4657, loss 0.249983, acc 0.96875, prec 0.0918251, recall 0.904101
2017-12-10T13:09:41.820960: step 4658, loss 0.381459, acc 0.875, prec 0.091829, recall 0.904122
2017-12-10T13:09:42.267292: step 4659, loss 0.512174, acc 0.921875, prec 0.091839, recall 0.904143
2017-12-10T13:09:42.729775: step 4660, loss 0.283462, acc 0.90625, prec 0.0918877, recall 0.904206
2017-12-10T13:09:43.169351: step 4661, loss 0.0511355, acc 0.984375, prec 0.0918856, recall 0.904206
2017-12-10T13:09:43.619410: step 4662, loss 0.195935, acc 0.984375, prec 0.0919242, recall 0.904248
2017-12-10T13:09:44.067474: step 4663, loss 0.0215339, acc 1, prec 0.0919445, recall 0.904269
2017-12-10T13:09:44.518325: step 4664, loss 0.360182, acc 0.96875, prec 0.0919404, recall 0.904269
2017-12-10T13:09:44.974803: step 4665, loss 0.266532, acc 0.9375, prec 0.0919322, recall 0.904269
2017-12-10T13:09:45.450645: step 4666, loss 0.965354, acc 1, prec 0.0919525, recall 0.90429
2017-12-10T13:09:45.895877: step 4667, loss 0.0893921, acc 0.96875, prec 0.0919687, recall 0.904311
2017-12-10T13:09:46.347385: step 4668, loss 0.0366723, acc 0.984375, prec 0.0919869, recall 0.904333
2017-12-10T13:09:46.798786: step 4669, loss 0.220923, acc 1, prec 0.0920276, recall 0.904375
2017-12-10T13:09:47.253648: step 4670, loss 0.403163, acc 0.953125, prec 0.092062, recall 0.904417
2017-12-10T13:09:47.695412: step 4671, loss 0.407392, acc 0.953125, prec 0.0920964, recall 0.904459
2017-12-10T13:09:48.154424: step 4672, loss 0.0373304, acc 0.984375, prec 0.0920944, recall 0.904459
2017-12-10T13:09:48.605327: step 4673, loss 0.289956, acc 0.96875, prec 0.0921512, recall 0.904522
2017-12-10T13:09:49.045605: step 4674, loss 0.188756, acc 0.953125, prec 0.092145, recall 0.904522
2017-12-10T13:09:49.484700: step 4675, loss 0.144947, acc 0.96875, prec 0.0922018, recall 0.904584
2017-12-10T13:09:49.932033: step 4676, loss 0.0904888, acc 0.984375, prec 0.0922403, recall 0.904626
2017-12-10T13:09:50.390166: step 4677, loss 0.120012, acc 0.953125, prec 0.0922341, recall 0.904626
2017-12-10T13:09:50.843169: step 4678, loss 0.163833, acc 0.96875, prec 0.09223, recall 0.904626
2017-12-10T13:09:51.322890: step 4679, loss 0.0481163, acc 0.984375, prec 0.0922279, recall 0.904626
2017-12-10T13:09:51.771259: step 4680, loss 0.213143, acc 0.9375, prec 0.0922602, recall 0.904668
2017-12-10T13:09:52.214851: step 4681, loss 0.0632473, acc 0.96875, prec 0.0922561, recall 0.904668
2017-12-10T13:09:52.653620: step 4682, loss 0.4858, acc 0.9375, prec 0.0922884, recall 0.90471
2017-12-10T13:09:53.088382: step 4683, loss 0.191182, acc 0.9375, prec 0.0922802, recall 0.90471
2017-12-10T13:09:53.533132: step 4684, loss 0.125363, acc 0.96875, prec 0.0922761, recall 0.90471
2017-12-10T13:09:53.998755: step 4685, loss 0.15368, acc 0.96875, prec 0.0922922, recall 0.904731
2017-12-10T13:09:54.451102: step 4686, loss 0.181566, acc 0.953125, prec 0.092286, recall 0.904731
2017-12-10T13:09:54.902579: step 4687, loss 0.237893, acc 0.96875, prec 0.0922819, recall 0.904731
2017-12-10T13:09:55.361479: step 4688, loss 0.0929607, acc 0.953125, prec 0.0922757, recall 0.904731
2017-12-10T13:09:55.812007: step 4689, loss 0.426317, acc 0.96875, prec 0.0923324, recall 0.904793
2017-12-10T13:09:56.261811: step 4690, loss 0.0130471, acc 1, prec 0.092373, recall 0.904835
2017-12-10T13:09:56.722808: step 4691, loss 0.086133, acc 0.96875, prec 0.0923688, recall 0.904835
2017-12-10T13:09:57.176471: step 4692, loss 0.178717, acc 0.953125, prec 0.0923627, recall 0.904835
2017-12-10T13:09:57.612350: step 4693, loss 0.0248709, acc 0.984375, prec 0.0923809, recall 0.904856
2017-12-10T13:09:58.051109: step 4694, loss 0.135054, acc 1, prec 0.0924214, recall 0.904897
2017-12-10T13:09:58.492268: step 4695, loss 0.0959732, acc 1, prec 0.0924417, recall 0.904918
2017-12-10T13:09:58.946224: step 4696, loss 0.127112, acc 0.96875, prec 0.0924375, recall 0.904918
2017-12-10T13:09:59.411151: step 4697, loss 0.21427, acc 0.921875, prec 0.0924272, recall 0.904918
2017-12-10T13:09:59.854340: step 4698, loss 0.0317409, acc 0.984375, prec 0.0924454, recall 0.904939
2017-12-10T13:10:00.305344: step 4699, loss 0.143879, acc 0.9375, prec 0.0924372, recall 0.904939
2017-12-10T13:10:00.745924: step 4700, loss 0.170076, acc 0.96875, prec 0.0924533, recall 0.90496
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4700

2017-12-10T13:10:02.682215: step 4701, loss 0.0908458, acc 0.96875, prec 0.0924694, recall 0.90498
2017-12-10T13:10:03.140800: step 4702, loss 0.0604505, acc 0.984375, prec 0.0924876, recall 0.905001
2017-12-10T13:10:03.594513: step 4703, loss 0.122493, acc 0.96875, prec 0.0924835, recall 0.905001
2017-12-10T13:10:04.050035: step 4704, loss 0.00818153, acc 1, prec 0.0924835, recall 0.905001
2017-12-10T13:10:04.505015: step 4705, loss 0.0512534, acc 0.984375, prec 0.0924814, recall 0.905001
2017-12-10T13:10:04.957468: step 4706, loss 2.11817, acc 0.984375, prec 0.0925219, recall 0.904845
2017-12-10T13:10:05.402727: step 4707, loss 0.0802075, acc 0.96875, prec 0.0925583, recall 0.904887
2017-12-10T13:10:05.856905: step 4708, loss 0.334742, acc 0.9375, prec 0.0925905, recall 0.904928
2017-12-10T13:10:06.317351: step 4709, loss 0.132792, acc 0.96875, prec 0.0926066, recall 0.904949
2017-12-10T13:10:06.762357: step 4710, loss 0.180216, acc 0.953125, prec 0.0926004, recall 0.904949
2017-12-10T13:10:07.209922: step 4711, loss 0.0971121, acc 0.984375, prec 0.0925984, recall 0.904949
2017-12-10T13:10:07.665194: step 4712, loss 0.208677, acc 0.90625, prec 0.092586, recall 0.904949
2017-12-10T13:10:08.111419: step 4713, loss 0.120633, acc 0.921875, prec 0.0925959, recall 0.90497
2017-12-10T13:10:08.543547: step 4714, loss 0.190931, acc 0.921875, prec 0.092626, recall 0.905011
2017-12-10T13:10:08.983124: step 4715, loss 0.177905, acc 0.953125, prec 0.0926198, recall 0.905011
2017-12-10T13:10:09.427619: step 4716, loss 0.151465, acc 0.953125, prec 0.0926743, recall 0.905073
2017-12-10T13:10:09.868371: step 4717, loss 0.443138, acc 0.90625, prec 0.0926619, recall 0.905073
2017-12-10T13:10:10.318135: step 4718, loss 0.0428173, acc 0.96875, prec 0.0926983, recall 0.905114
2017-12-10T13:10:10.770954: step 4719, loss 0.168448, acc 0.953125, prec 0.0926921, recall 0.905114
2017-12-10T13:10:11.213090: step 4720, loss 0.0824245, acc 0.953125, prec 0.0926859, recall 0.905114
2017-12-10T13:10:11.652972: step 4721, loss 0.107032, acc 0.9375, prec 0.0926776, recall 0.905114
2017-12-10T13:10:12.092420: step 4722, loss 0.550524, acc 0.90625, prec 0.0926854, recall 0.905135
2017-12-10T13:10:12.550617: step 4723, loss 0.390706, acc 0.90625, prec 0.0926932, recall 0.905156
2017-12-10T13:10:13.000085: step 4724, loss 0.136072, acc 0.96875, prec 0.0926891, recall 0.905156
2017-12-10T13:10:13.432178: step 4725, loss 0.497304, acc 0.953125, prec 0.0927638, recall 0.905238
2017-12-10T13:10:13.882793: step 4726, loss 0.264079, acc 0.90625, prec 0.0927514, recall 0.905238
2017-12-10T13:10:14.332519: step 4727, loss 0.0710616, acc 0.953125, prec 0.0927654, recall 0.905259
2017-12-10T13:10:14.778101: step 4728, loss 0.154601, acc 0.96875, prec 0.0927612, recall 0.905259
2017-12-10T13:10:15.222575: step 4729, loss 0.144536, acc 0.96875, prec 0.0927975, recall 0.9053
2017-12-10T13:10:15.668314: step 4730, loss 0.090281, acc 0.953125, prec 0.0928317, recall 0.905341
2017-12-10T13:10:16.111182: step 4731, loss 0.0385855, acc 0.984375, prec 0.0928498, recall 0.905361
2017-12-10T13:10:16.561963: step 4732, loss 0.0543534, acc 0.984375, prec 0.0928881, recall 0.905402
2017-12-10T13:10:17.017510: step 4733, loss 0.192568, acc 0.921875, prec 0.0928778, recall 0.905402
2017-12-10T13:10:17.453776: step 4734, loss 0.124885, acc 0.96875, prec 0.0928737, recall 0.905402
2017-12-10T13:10:17.905385: step 4735, loss 0.186107, acc 0.96875, prec 0.0928695, recall 0.905402
2017-12-10T13:10:18.339904: step 4736, loss 0.0805177, acc 0.96875, prec 0.0929461, recall 0.905484
2017-12-10T13:10:18.777110: step 4737, loss 0.496974, acc 0.9375, prec 0.0929782, recall 0.905525
2017-12-10T13:10:19.235709: step 4738, loss 0.204654, acc 0.96875, prec 0.0930145, recall 0.905566
2017-12-10T13:10:19.684749: step 4739, loss 0.0277591, acc 0.984375, prec 0.0930124, recall 0.905566
2017-12-10T13:10:20.138375: step 4740, loss 0.831478, acc 0.96875, prec 0.0930688, recall 0.905628
2017-12-10T13:10:20.587167: step 4741, loss 0.0946594, acc 0.984375, prec 0.0931071, recall 0.905669
2017-12-10T13:10:21.041559: step 4742, loss 0.133759, acc 0.953125, prec 0.093121, recall 0.905689
2017-12-10T13:10:21.480199: step 4743, loss 0.149658, acc 0.953125, prec 0.0931753, recall 0.90575
2017-12-10T13:10:21.919212: step 4744, loss 0.164979, acc 0.953125, prec 0.0931892, recall 0.90577
2017-12-10T13:10:22.359975: step 4745, loss 0.159414, acc 0.96875, prec 0.0932053, recall 0.905791
2017-12-10T13:10:22.805647: step 4746, loss 0.144984, acc 0.96875, prec 0.0932011, recall 0.905791
2017-12-10T13:10:23.245524: step 4747, loss 0.0255717, acc 0.984375, prec 0.0931991, recall 0.905791
2017-12-10T13:10:23.687033: step 4748, loss 0.00367743, acc 1, prec 0.0931991, recall 0.905791
2017-12-10T13:10:24.129739: step 4749, loss 0.087053, acc 0.96875, prec 0.0932151, recall 0.905811
2017-12-10T13:10:24.576726: step 4750, loss 0.0141937, acc 1, prec 0.0932151, recall 0.905811
2017-12-10T13:10:25.017250: step 4751, loss 0.186791, acc 0.9375, prec 0.0932269, recall 0.905832
2017-12-10T13:10:25.470632: step 4752, loss 0.0812951, acc 1, prec 0.0932471, recall 0.905852
2017-12-10T13:10:25.918414: step 4753, loss 0.0460556, acc 0.953125, prec 0.0932409, recall 0.905852
2017-12-10T13:10:26.377049: step 4754, loss 1.23342, acc 0.953125, prec 0.0932951, recall 0.905913
2017-12-10T13:10:26.830215: step 4755, loss 0.0232435, acc 1, prec 0.0933153, recall 0.905933
2017-12-10T13:10:27.270590: step 4756, loss 0.158999, acc 0.984375, prec 0.0933333, recall 0.905953
2017-12-10T13:10:27.709934: step 4757, loss 0.0149213, acc 1, prec 0.0933333, recall 0.905953
2017-12-10T13:10:28.148078: step 4758, loss 0.187344, acc 0.953125, prec 0.0933473, recall 0.905974
2017-12-10T13:10:28.591140: step 4759, loss 0.158955, acc 0.96875, prec 0.0933834, recall 0.906014
2017-12-10T13:10:29.038696: step 4760, loss 0.224598, acc 0.953125, prec 0.0933772, recall 0.906014
2017-12-10T13:10:29.481148: step 4761, loss 0.190332, acc 0.953125, prec 0.0934112, recall 0.906055
2017-12-10T13:10:29.928183: step 4762, loss 0.230781, acc 0.96875, prec 0.0934675, recall 0.906115
2017-12-10T13:10:30.377246: step 4763, loss 0.199023, acc 0.921875, prec 0.0934571, recall 0.906115
2017-12-10T13:10:30.833426: step 4764, loss 0.112172, acc 0.953125, prec 0.0934912, recall 0.906156
2017-12-10T13:10:31.289288: step 4765, loss 0.130262, acc 0.984375, prec 0.0934891, recall 0.906156
2017-12-10T13:10:31.737806: step 4766, loss 0.102233, acc 0.984375, prec 0.0935273, recall 0.906196
2017-12-10T13:10:32.175730: step 4767, loss 0.0507669, acc 0.96875, prec 0.0935432, recall 0.906216
2017-12-10T13:10:32.618932: step 4768, loss 0.79258, acc 0.875, prec 0.0935266, recall 0.906216
2017-12-10T13:10:33.058843: step 4769, loss 0.0899434, acc 0.984375, prec 0.0935245, recall 0.906216
2017-12-10T13:10:33.509052: step 4770, loss 0.042199, acc 0.984375, prec 0.0935627, recall 0.906257
2017-12-10T13:10:33.976124: step 4771, loss 0.0887361, acc 0.953125, prec 0.0935967, recall 0.906297
2017-12-10T13:10:34.414533: step 4772, loss 0.183605, acc 0.921875, prec 0.0935863, recall 0.906297
2017-12-10T13:10:34.876714: step 4773, loss 0.0873568, acc 0.953125, prec 0.0936002, recall 0.906317
2017-12-10T13:10:35.327574: step 4774, loss 0.142572, acc 0.921875, prec 0.0935898, recall 0.906317
2017-12-10T13:10:35.763712: step 4775, loss 0.552659, acc 0.953125, prec 0.0936238, recall 0.906357
2017-12-10T13:10:36.205405: step 4776, loss 0.0163142, acc 1, prec 0.0936238, recall 0.906357
2017-12-10T13:10:36.648243: step 4777, loss 0.690454, acc 0.921875, prec 0.0936536, recall 0.906398
2017-12-10T13:10:37.101083: step 4778, loss 0.0859594, acc 0.984375, prec 0.0936918, recall 0.906438
2017-12-10T13:10:37.552616: step 4779, loss 0.232919, acc 0.96875, prec 0.0937077, recall 0.906458
2017-12-10T13:10:37.997727: step 4780, loss 0.00753092, acc 1, prec 0.093768, recall 0.906518
2017-12-10T13:10:38.439409: step 4781, loss 0.126541, acc 0.953125, prec 0.0937618, recall 0.906518
2017-12-10T13:10:38.899467: step 4782, loss 0.00628726, acc 1, prec 0.0937819, recall 0.906538
2017-12-10T13:10:39.346638: step 4783, loss 0.067829, acc 0.953125, prec 0.0937957, recall 0.906558
2017-12-10T13:10:39.787278: step 4784, loss 0.797176, acc 0.96875, prec 0.0938117, recall 0.906578
2017-12-10T13:10:40.236836: step 4785, loss 0.0937845, acc 0.953125, prec 0.0938456, recall 0.906618
2017-12-10T13:10:40.679433: step 4786, loss 0.00597514, acc 1, prec 0.0938456, recall 0.906618
2017-12-10T13:10:41.122324: step 4787, loss 0.0592382, acc 0.96875, prec 0.0938414, recall 0.906618
2017-12-10T13:10:41.561328: step 4788, loss 0.884912, acc 0.953125, prec 0.0938553, recall 0.906638
2017-12-10T13:10:42.016085: step 4789, loss 2.03344, acc 0.953125, prec 0.0938712, recall 0.906464
2017-12-10T13:10:42.485845: step 4790, loss 0.581286, acc 0.953125, prec 0.0939252, recall 0.906524
2017-12-10T13:10:42.934149: step 4791, loss 0.278085, acc 0.953125, prec 0.0939591, recall 0.906564
2017-12-10T13:10:43.379004: step 4792, loss 0.341276, acc 0.921875, prec 0.0939487, recall 0.906564
2017-12-10T13:10:43.826460: step 4793, loss 0.0874032, acc 0.984375, prec 0.0939667, recall 0.906584
2017-12-10T13:10:44.273928: step 4794, loss 0.261757, acc 0.921875, prec 0.0939764, recall 0.906604
2017-12-10T13:10:44.715848: step 4795, loss 0.475982, acc 0.90625, prec 0.094004, recall 0.906644
2017-12-10T13:10:45.155599: step 4796, loss 0.920038, acc 0.78125, prec 0.093995, recall 0.906664
2017-12-10T13:10:45.601877: step 4797, loss 1.07814, acc 0.796875, prec 0.094008, recall 0.906704
2017-12-10T13:10:46.051813: step 4798, loss 0.696159, acc 0.8125, prec 0.093983, recall 0.906704
2017-12-10T13:10:46.491504: step 4799, loss 0.864507, acc 0.875, prec 0.0939865, recall 0.906724
2017-12-10T13:10:46.939311: step 4800, loss 0.605587, acc 0.796875, prec 0.0939995, recall 0.906763
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4800

2017-12-10T13:10:48.843237: step 4801, loss 0.716141, acc 0.796875, prec 0.0939925, recall 0.906783
2017-12-10T13:10:49.299722: step 4802, loss 0.750119, acc 0.796875, prec 0.0939655, recall 0.906783
2017-12-10T13:10:49.744014: step 4803, loss 0.442164, acc 0.890625, prec 0.093951, recall 0.906783
2017-12-10T13:10:50.202102: step 4804, loss 0.429321, acc 0.875, prec 0.0939344, recall 0.906783
2017-12-10T13:10:50.659497: step 4805, loss 0.16193, acc 0.921875, prec 0.093944, recall 0.906803
2017-12-10T13:10:51.110292: step 4806, loss 0.137383, acc 0.953125, prec 0.0939578, recall 0.906823
2017-12-10T13:10:51.563381: step 4807, loss 0.163866, acc 0.9375, prec 0.0939495, recall 0.906823
2017-12-10T13:10:52.022378: step 4808, loss 0.507958, acc 0.9375, prec 0.0939612, recall 0.906843
2017-12-10T13:10:52.469613: step 4809, loss 0.490913, acc 0.9375, prec 0.0939929, recall 0.906883
2017-12-10T13:10:52.917383: step 4810, loss 0.194789, acc 0.9375, prec 0.0940046, recall 0.906902
2017-12-10T13:10:53.363814: step 4811, loss 0.490302, acc 0.90625, prec 0.0939922, recall 0.906902
2017-12-10T13:10:53.810328: step 4812, loss 0.167591, acc 0.921875, prec 0.0940018, recall 0.906922
2017-12-10T13:10:54.250838: step 4813, loss 0.0984153, acc 0.984375, prec 0.0939997, recall 0.906922
2017-12-10T13:10:54.693434: step 4814, loss 0.92481, acc 0.96875, prec 0.0940356, recall 0.906962
2017-12-10T13:10:55.142916: step 4815, loss 0.539571, acc 0.921875, prec 0.0940452, recall 0.906982
2017-12-10T13:10:55.589645: step 4816, loss 0.184539, acc 0.953125, prec 0.094039, recall 0.906982
2017-12-10T13:10:56.038487: step 4817, loss 0.153637, acc 0.953125, prec 0.0940327, recall 0.906982
2017-12-10T13:10:56.478827: step 4818, loss 0.0985973, acc 0.984375, prec 0.0940906, recall 0.907041
2017-12-10T13:10:56.921962: step 4819, loss 0.192194, acc 0.96875, prec 0.0941265, recall 0.907081
2017-12-10T13:10:57.376222: step 4820, loss 0.02924, acc 0.984375, prec 0.0941444, recall 0.9071
2017-12-10T13:10:57.829874: step 4821, loss 0.0423032, acc 0.984375, prec 0.0941623, recall 0.90712
2017-12-10T13:10:58.281793: step 4822, loss 0.0273186, acc 1, prec 0.0942023, recall 0.90716
2017-12-10T13:10:58.727928: step 4823, loss 0.168486, acc 0.953125, prec 0.094196, recall 0.90716
2017-12-10T13:10:59.161939: step 4824, loss 0.0800718, acc 0.96875, prec 0.0942118, recall 0.907179
2017-12-10T13:10:59.610226: step 4825, loss 0.0910435, acc 0.984375, prec 0.0942298, recall 0.907199
2017-12-10T13:11:00.058548: step 4826, loss 0.0585634, acc 0.984375, prec 0.0942477, recall 0.907219
2017-12-10T13:11:00.509530: step 4827, loss 0.854284, acc 0.984375, prec 0.0942855, recall 0.907258
2017-12-10T13:11:00.963042: step 4828, loss 0.0275725, acc 1, prec 0.0943055, recall 0.907278
2017-12-10T13:11:01.409712: step 4829, loss 0.578032, acc 0.953125, prec 0.0943392, recall 0.907317
2017-12-10T13:11:01.853901: step 4830, loss 0.130028, acc 0.984375, prec 0.0943771, recall 0.907356
2017-12-10T13:11:02.305989: step 4831, loss 1.57938, acc 0.96875, prec 0.0944149, recall 0.907203
2017-12-10T13:11:02.748419: step 4832, loss 0.0947538, acc 0.953125, prec 0.0944686, recall 0.907262
2017-12-10T13:11:03.189772: step 4833, loss 0.256914, acc 0.921875, prec 0.0944781, recall 0.907282
2017-12-10T13:11:03.629809: step 4834, loss 0.399128, acc 0.890625, prec 0.0944635, recall 0.907282
2017-12-10T13:11:04.076182: step 4835, loss 0.197198, acc 0.9375, prec 0.0944752, recall 0.907302
2017-12-10T13:11:04.522723: step 4836, loss 0.182682, acc 0.96875, prec 0.094471, recall 0.907302
2017-12-10T13:11:04.977578: step 4837, loss 0.474535, acc 0.875, prec 0.0944743, recall 0.907321
2017-12-10T13:11:05.417219: step 4838, loss 0.4307, acc 0.890625, prec 0.0944597, recall 0.907321
2017-12-10T13:11:05.885486: step 4839, loss 0.235424, acc 0.9375, prec 0.0944514, recall 0.907321
2017-12-10T13:11:06.329817: step 4840, loss 0.462778, acc 0.828125, prec 0.0944485, recall 0.907341
2017-12-10T13:11:06.768087: step 4841, loss 0.351973, acc 0.90625, prec 0.094436, recall 0.907341
2017-12-10T13:11:07.213146: step 4842, loss 0.30141, acc 0.921875, prec 0.0944455, recall 0.90736
2017-12-10T13:11:07.658960: step 4843, loss 0.286809, acc 0.9375, prec 0.0944771, recall 0.9074
2017-12-10T13:11:08.109946: step 4844, loss 0.144712, acc 0.96875, prec 0.0944729, recall 0.9074
2017-12-10T13:11:08.558799: step 4845, loss 0.189424, acc 0.921875, prec 0.0944625, recall 0.9074
2017-12-10T13:11:09.008121: step 4846, loss 0.390957, acc 0.9375, prec 0.0944542, recall 0.9074
2017-12-10T13:11:09.446233: step 4847, loss 0.242109, acc 0.890625, prec 0.0944397, recall 0.9074
2017-12-10T13:11:09.910606: step 4848, loss 0.117184, acc 0.96875, prec 0.0944554, recall 0.907419
2017-12-10T13:11:10.359256: step 4849, loss 0.326582, acc 0.890625, prec 0.0944608, recall 0.907439
2017-12-10T13:11:10.813076: step 4850, loss 0.691341, acc 0.875, prec 0.0944641, recall 0.907458
2017-12-10T13:11:11.262309: step 4851, loss 0.335466, acc 0.953125, prec 0.0944579, recall 0.907458
2017-12-10T13:11:11.695588: step 4852, loss 0.156747, acc 0.953125, prec 0.0944716, recall 0.907478
2017-12-10T13:11:12.130233: step 4853, loss 0.336711, acc 0.96875, prec 0.0944873, recall 0.907497
2017-12-10T13:11:12.570721: step 4854, loss 0.0498139, acc 0.984375, prec 0.0944852, recall 0.907497
2017-12-10T13:11:13.016270: step 4855, loss 0.0610939, acc 0.96875, prec 0.094501, recall 0.907517
2017-12-10T13:11:13.477184: step 4856, loss 0.489956, acc 0.9375, prec 0.0945126, recall 0.907536
2017-12-10T13:11:13.916044: step 4857, loss 0.050607, acc 1, prec 0.0945325, recall 0.907556
2017-12-10T13:11:14.371843: step 4858, loss 0.0734633, acc 0.984375, prec 0.0945304, recall 0.907556
2017-12-10T13:11:14.821734: step 4859, loss 0.0665993, acc 0.984375, prec 0.0945483, recall 0.907575
2017-12-10T13:11:15.267680: step 4860, loss 0.0194526, acc 1, prec 0.0945483, recall 0.907575
2017-12-10T13:11:15.715818: step 4861, loss 0.215135, acc 0.96875, prec 0.0945441, recall 0.907575
2017-12-10T13:11:16.156457: step 4862, loss 0.114353, acc 0.9375, prec 0.0945557, recall 0.907595
2017-12-10T13:11:16.588656: step 4863, loss 0.888362, acc 0.96875, prec 0.0946112, recall 0.907653
2017-12-10T13:11:17.033102: step 4864, loss 0.400795, acc 1, prec 0.0946311, recall 0.907673
2017-12-10T13:11:17.480310: step 4865, loss 0.0125329, acc 1, prec 0.094651, recall 0.907692
2017-12-10T13:11:17.925628: step 4866, loss 0.0105971, acc 1, prec 0.0946709, recall 0.907712
2017-12-10T13:11:18.373350: step 4867, loss 0.110761, acc 0.984375, prec 0.0946688, recall 0.907712
2017-12-10T13:11:18.825095: step 4868, loss 0.0457532, acc 0.96875, prec 0.0946846, recall 0.907731
2017-12-10T13:11:19.282943: step 4869, loss 0.0185169, acc 0.984375, prec 0.0946825, recall 0.907731
2017-12-10T13:11:19.726907: step 4870, loss 0.05918, acc 0.984375, prec 0.0946804, recall 0.907731
2017-12-10T13:11:20.189152: step 4871, loss 0.0526369, acc 0.984375, prec 0.0946783, recall 0.907731
2017-12-10T13:11:20.653861: step 4872, loss 0.079337, acc 0.96875, prec 0.0946941, recall 0.907751
2017-12-10T13:11:21.092909: step 4873, loss 0.0892981, acc 1, prec 0.0947537, recall 0.907809
2017-12-10T13:11:21.554934: step 4874, loss 0.261663, acc 0.9375, prec 0.0947454, recall 0.907809
2017-12-10T13:11:22.004998: step 4875, loss 0.0538851, acc 0.984375, prec 0.0947831, recall 0.907848
2017-12-10T13:11:22.464746: step 4876, loss 0.0994925, acc 0.96875, prec 0.0948187, recall 0.907886
2017-12-10T13:11:22.907797: step 4877, loss 0.456649, acc 0.984375, prec 0.0948762, recall 0.907945
2017-12-10T13:11:23.344080: step 4878, loss 0.379573, acc 1, prec 0.094916, recall 0.907983
2017-12-10T13:11:23.785893: step 4879, loss 0.166751, acc 0.953125, prec 0.0949495, recall 0.908022
2017-12-10T13:11:24.227226: step 4880, loss 0.0207151, acc 0.984375, prec 0.0949474, recall 0.908022
2017-12-10T13:11:24.675285: step 4881, loss 0.135157, acc 0.96875, prec 0.0949432, recall 0.908022
2017-12-10T13:11:25.127837: step 4882, loss 0.0653867, acc 0.984375, prec 0.094961, recall 0.908041
2017-12-10T13:11:25.576886: step 4883, loss 0.0436329, acc 1, prec 0.094961, recall 0.908041
2017-12-10T13:11:26.015659: step 4884, loss 0.108117, acc 0.96875, prec 0.0949569, recall 0.908041
2017-12-10T13:11:26.452698: step 4885, loss 0.13352, acc 0.984375, prec 0.0950144, recall 0.908099
2017-12-10T13:11:26.904132: step 4886, loss 0.227039, acc 1, prec 0.0950541, recall 0.908138
2017-12-10T13:11:27.352355: step 4887, loss 0.0928078, acc 0.96875, prec 0.0950698, recall 0.908157
2017-12-10T13:11:27.801468: step 4888, loss 0.0326528, acc 0.984375, prec 0.0950677, recall 0.908157
2017-12-10T13:11:28.256843: step 4889, loss 0.0762315, acc 0.984375, prec 0.0950656, recall 0.908157
2017-12-10T13:11:28.701816: step 4890, loss 0.0409014, acc 0.984375, prec 0.0950834, recall 0.908176
2017-12-10T13:11:29.143291: step 4891, loss 0.0633919, acc 0.953125, prec 0.0950771, recall 0.908176
2017-12-10T13:11:29.593067: step 4892, loss 0.0636124, acc 0.984375, prec 0.0950949, recall 0.908195
2017-12-10T13:11:30.048507: step 4893, loss 0.0202411, acc 1, prec 0.0951545, recall 0.908253
2017-12-10T13:11:30.514726: step 4894, loss 0.209907, acc 0.9375, prec 0.0951461, recall 0.908253
2017-12-10T13:11:30.972929: step 4895, loss 0.429575, acc 0.9375, prec 0.0951973, recall 0.908311
2017-12-10T13:11:31.431331: step 4896, loss 0.14141, acc 0.921875, prec 0.0952266, recall 0.908349
2017-12-10T13:11:31.887810: step 4897, loss 0.206024, acc 0.9375, prec 0.0952778, recall 0.908406
2017-12-10T13:11:32.331341: step 4898, loss 0.0476091, acc 0.984375, prec 0.0953154, recall 0.908445
2017-12-10T13:11:32.776864: step 4899, loss 0.0689225, acc 0.984375, prec 0.095353, recall 0.908483
2017-12-10T13:11:33.225026: step 4900, loss 0.469074, acc 0.96875, prec 0.0953885, recall 0.908521
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_2/1512927220/checkpoints/model-4900

2017-12-10T13:11:35.122141: step 4901, loss 0.230149, acc 0.96875, prec 0.0953843, recall 0.908521
2017-12-10T13:11:35.576277: step 4902, loss 0.411338, acc 0.9375, prec 0.0953957, recall 0.90854
2017-12-10T13:11:36.037832: step 4903, loss 0.149006, acc 0.96875, prec 0.0954114, recall 0.90856
2017-12-10T13:11:36.487370: step 4904, loss 0.153273, acc 0.953125, prec 0.095425, recall 0.908579
2017-12-10T13:11:36.931957: step 4905, loss 0.163709, acc 0.984375, prec 0.0954823, recall 0.908636
2017-12-10T13:11:37.391525: step 4906, loss 0.0136424, acc 1, prec 0.0954823, recall 0.908636
2017-12-10T13:11:37.832641: step 4907, loss 0.0805092, acc 0.9375, prec 0.0954938, recall 0.908655
2017-12-10T13:11:38.275451: step 4908, loss 0.860827, acc 0.921875, prec 0.095523, recall 0.908693
2017-12-10T13:11:38.715864: step 4909, loss 0.299103, acc 0.9375, prec 0.0955146, recall 0.908693
2017-12-10T13:11:39.167693: step 4910, loss 0.211179, acc 0.9375, prec 0.0955062, recall 0.908693
2017-12-10T13:11:39.623570: step 4911, loss 0.265968, acc 0.90625, prec 0.0954937, recall 0.908693
2017-12-10T13:11:40.054063: step 4912, loss 0.589348, acc 0.953125, prec 0.0955468, recall 0.90875
2017-12-10T13:11:40.497037: step 4913, loss 0.23816, acc 0.921875, prec 0.0955364, recall 0.90875
2017-12-10T13:11:40.934135: step 4914, loss 0.50228, acc 0.96875, prec 0.095552, recall 0.908769
2017-12-10T13:11:41.380050: step 4915, loss 0.166354, acc 0.921875, prec 0.0955415, recall 0.908769
2017-12-10T13:11:41.839694: step 4916, loss 0.208243, acc 0.921875, prec 0.0955509, recall 0.908788
2017-12-10T13:11:42.288496: step 4917, loss 0.0559398, acc 0.984375, prec 0.0955686, recall 0.908807
2017-12-10T13:11:42.729743: step 4918, loss 0.166595, acc 0.953125, prec 0.0955623, recall 0.908807
2017-12-10T13:11:43.188015: step 4919, loss 0.0688736, acc 0.96875, prec 0.0955977, recall 0.908845
2017-12-10T13:11:43.643248: step 4920, loss 0.145496, acc 0.953125, prec 0.0956113, recall 0.908864
2017-12-10T13:11:44.111488: step 4921, loss 0.158929, acc 0.9375, prec 0.0956029, recall 0.908864
2017-12-10T13:11:44.558683: step 4922, loss 0.124892, acc 0.953125, prec 0.0955966, recall 0.908864
2017-12-10T13:11:45.005671: step 4923, loss 0.0979468, acc 0.953125, prec 0.0955903, recall 0.908864
2017-12-10T13:11:45.450272: step 4924, loss 0.239723, acc 0.953125, prec 0.0956038, recall 0.908883
2017-12-10T13:11:45.900612: step 4925, loss 0.138775, acc 0.96875, prec 0.0956194, recall 0.908902
2017-12-10T13:11:46.350065: step 4926, loss 0.17294, acc 0.953125, prec 0.0956132, recall 0.908902
2017-12-10T13:11:46.794804: step 4927, loss 0.0968478, acc 0.96875, prec 0.095609, recall 0.908902
2017-12-10T13:11:47.244654: step 4928, loss 0.0795749, acc 0.984375, prec 0.0956069, recall 0.908902
2017-12-10T13:11:47.702350: step 4929, loss 0.00222031, acc 1, prec 0.0956267, recall 0.908921
2017-12-10T13:11:48.138386: step 4930, loss 0.0525528, acc 0.984375, prec 0.0956642, recall 0.908959
2017-12-10T13:11:48.587441: step 4931, loss 0.084313, acc 0.96875, prec 0.09566, recall 0.908959
2017-12-10T13:11:49.029518: step 4932, loss 0.0885661, acc 0.984375, prec 0.0956777, recall 0.908978
2017-12-10T13:11:49.470405: step 4933, loss 0.0131085, acc 1, prec 0.0956777, recall 0.908978
2017-12-10T13:11:49.914287: step 4934, loss 0.0357497, acc 0.984375, prec 0.0956756, recall 0.908978
2017-12-10T13:11:50.362343: step 4935, loss 0.00549637, acc 1, prec 0.0956953, recall 0.908996
2017-12-10T13:11:50.805874: step 4936, loss 0.114671, acc 0.96875, prec 0.0956912, recall 0.908996
2017-12-10T13:11:51.247648: step 4937, loss 4.52335, acc 0.984375, prec 0.0957109, recall 0.908827
2017-12-10T13:11:51.710881: step 4938, loss 1.13365, acc 0.96875, prec 0.0957463, recall 0.908864
2017-12-10T13:11:52.169390: step 4939, loss 0.525452, acc 0.953125, prec 0.0957796, recall 0.908902
2017-12-10T13:11:52.631038: step 4940, loss 0.045617, acc 0.96875, prec 0.0957952, recall 0.908921
2017-12-10T13:11:53.074429: step 4941, loss 0.336254, acc 0.90625, prec 0.0958024, recall 0.90894
2017-12-10T13:11:53.515968: step 4942, loss 0.318098, acc 0.9375, prec 0.0958533, recall 0.908997
2017-12-10T13:11:53.962769: step 4943, loss 0.261382, acc 0.953125, prec 0.0958668, recall 0.909016
2017-12-10T13:11:54.428496: step 4944, loss 0.659856, acc 0.828125, prec 0.0958437, recall 0.909016
2017-12-10T13:11:54.881629: step 4945, loss 0.451393, acc 0.875, prec 0.0958467, recall 0.909034
2017-12-10T13:11:55.333215: step 4946, loss 0.716983, acc 0.796875, prec 0.095859, recall 0.909072
2017-12-10T13:11:55.772553: step 4947, loss 0.706488, acc 0.828125, prec 0.0958952, recall 0.909129
2017-12-10T13:11:56.217443: step 4948, loss 0.518322, acc 0.796875, prec 0.095868, recall 0.909129
2017-12-10T13:11:56.622825: step 4949, loss 0.674849, acc 0.78125, prec 0.0958387, recall 0.909129
2017-12-10T13:11:57.061427: step 4950, loss 1.00497, acc 0.703125, prec 0.095799, recall 0.909129
2017-12-10T13:11:57.461431: step 4951, loss 0.778885, acc 0.828125, prec 0.095776, recall 0.909129
2017-12-10T13:11:57.905287: step 4952, loss 0.5414, acc 0.8125, prec 0.0957509, recall 0.909129
2017-12-10T13:11:58.344251: step 4953, loss 0.684822, acc 0.84375, prec 0.0957498, recall 0.909147
2017-12-10T13:11:58.780801: step 4954, loss 0.389174, acc 0.890625, prec 0.0957352, recall 0.909147
2017-12-10T13:11:59.219131: step 4955, loss 0.962957, acc 0.84375, prec 0.0957537, recall 0.909185
2017-12-10T13:11:59.660365: step 4956, loss 0.202993, acc 0.9375, prec 0.0957651, recall 0.909204
2017-12-10T13:12:00.110010: step 4957, loss 0.337485, acc 0.921875, prec 0.0957546, recall 0.909204
2017-12-10T13:12:00.580036: step 4958, loss 0.0548003, acc 0.984375, prec 0.0957526, recall 0.909204
2017-12-10T13:12:01.024598: step 4959, loss 0.681823, acc 0.96875, prec 0.0957878, recall 0.909241
2017-12-10T13:12:01.467818: step 4960, loss 0.113532, acc 0.96875, prec 0.0957836, recall 0.909241
2017-12-10T13:12:01.918667: step 4961, loss 0.432347, acc 0.90625, prec 0.0957908, recall 0.90926
2017-12-10T13:12:02.373431: step 4962, loss 0.872405, acc 0.9375, prec 0.0958218, recall 0.909298
2017-12-10T13:12:02.844598: step 4963, loss 0.0418328, acc 0.984375, prec 0.0958591, recall 0.909335
2017-12-10T13:12:03.287809: step 4964, loss 0.0324834, acc 0.984375, prec 0.0958767, recall 0.909354
2017-12-10T13:12:03.718449: step 4965, loss 0.288655, acc 0.953125, prec 0.0958704, recall 0.909354
2017-12-10T13:12:04.160287: step 4966, loss 0.208417, acc 0.953125, prec 0.0958839, recall 0.909372
2017-12-10T13:12:04.613468: step 4967, loss 1.8708, acc 0.9375, prec 0.0958776, recall 0.909185
2017-12-10T13:12:05.067293: step 4968, loss 0.0269337, acc 1, prec 0.0958973, recall 0.909203
2017-12-10T13:12:05.518149: step 4969, loss 0.0701743, acc 0.96875, prec 0.0959324, recall 0.909241
2017-12-10T13:12:05.941498: step 4970, loss 0.229751, acc 0.980769, prec 0.0959304, recall 0.909241
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 256
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR embedding_size_256_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527

Start training
2017-12-10T13:12:11.214799: step 1, loss 7.21846, acc 0.28125, prec 0.0425532, recall 0.666667
2017-12-10T13:12:11.654911: step 2, loss 4.41295, acc 0.34375, prec 0.0333333, recall 0.75
2017-12-10T13:12:12.103588: step 3, loss 8.53031, acc 0.46875, prec 0.0243902, recall 0.6
2017-12-10T13:12:12.551244: step 4, loss 13.0958, acc 0.515625, prec 0.0196078, recall 0.5
2017-12-10T13:12:13.001257: step 5, loss 27.9636, acc 0.546875, prec 0.0166667, recall 0.375
2017-12-10T13:12:13.454664: step 6, loss 3.24126, acc 0.5625, prec 0.0144231, recall 0.375
2017-12-10T13:12:13.901990: step 7, loss 3.47822, acc 0.328125, prec 0.015873, recall 0.444444
2017-12-10T13:12:14.351670: step 8, loss 3.7363, acc 0.40625, prec 0.0171821, recall 0.5
2017-12-10T13:12:14.793207: step 9, loss 3.55171, acc 0.453125, prec 0.0153374, recall 0.5
2017-12-10T13:12:15.250489: step 10, loss 3.18706, acc 0.4375, prec 0.0165289, recall 0.545455
2017-12-10T13:12:15.702318: step 11, loss 2.66182, acc 0.578125, prec 0.0153846, recall 0.545455
2017-12-10T13:12:16.134910: step 12, loss 9.47636, acc 0.609375, prec 0.0144928, recall 0.5
2017-12-10T13:12:16.585995: step 13, loss 2.27798, acc 0.59375, prec 0.0136364, recall 0.5
2017-12-10T13:12:17.035644: step 14, loss 1.81248, acc 0.59375, prec 0.0128755, recall 0.5
2017-12-10T13:12:17.478977: step 15, loss 1.51489, acc 0.6875, prec 0.0184049, recall 0.6
2017-12-10T13:12:17.924196: step 16, loss 4.20118, acc 0.6875, prec 0.0177165, recall 0.5625
2017-12-10T13:12:18.365016: step 17, loss 2.32781, acc 0.640625, prec 0.018797, recall 0.588235
2017-12-10T13:12:18.807574: step 18, loss 6.34159, acc 0.671875, prec 0.0198915, recall 0.578947
2017-12-10T13:12:19.277406: step 19, loss 17.019, acc 0.71875, prec 0.0192982, recall 0.55
2017-12-10T13:12:19.729374: step 20, loss 6.34859, acc 0.703125, prec 0.0203735, recall 0.545455
2017-12-10T13:12:20.184317: step 21, loss 9.16074, acc 0.59375, prec 0.0227273, recall 0.56
2017-12-10T13:12:20.635287: step 22, loss 4.58865, acc 0.484375, prec 0.0215716, recall 0.56
2017-12-10T13:12:21.069002: step 23, loss 4.84208, acc 0.375, prec 0.0203193, recall 0.56
2017-12-10T13:12:21.507194: step 24, loss 5.86825, acc 0.296875, prec 0.0204082, recall 0.576923
2017-12-10T13:12:21.961485: step 25, loss 5.40155, acc 0.375, prec 0.0193548, recall 0.576923
2017-12-10T13:12:22.406078: step 26, loss 5.18932, acc 0.328125, prec 0.0183374, recall 0.576923
2017-12-10T13:12:22.851057: step 27, loss 6.13272, acc 0.265625, prec 0.0184758, recall 0.592593
2017-12-10T13:12:23.284683: step 28, loss 3.24791, acc 0.515625, prec 0.0178372, recall 0.592593
2017-12-10T13:12:23.722847: step 29, loss 4.06976, acc 0.46875, prec 0.0182403, recall 0.607143
2017-12-10T13:12:24.167931: step 30, loss 2.45247, acc 0.640625, prec 0.0188285, recall 0.62069
2017-12-10T13:12:24.608595: step 31, loss 3.77008, acc 0.546875, prec 0.0192698, recall 0.633333
2017-12-10T13:12:25.043931: step 32, loss 7.80062, acc 0.609375, prec 0.0197824, recall 0.625
2017-12-10T13:12:25.490967: step 33, loss 9.74574, acc 0.65625, prec 0.0193798, recall 0.606061
2017-12-10T13:12:25.934719: step 34, loss 13.415, acc 0.625, prec 0.0208136, recall 0.611111
2017-12-10T13:12:26.387594: step 35, loss 4.07822, acc 0.546875, prec 0.0211592, recall 0.621622
2017-12-10T13:12:26.834242: step 36, loss 2.55557, acc 0.609375, prec 0.0206835, recall 0.621622
2017-12-10T13:12:27.278980: step 37, loss 21.2245, acc 0.4375, prec 0.0209059, recall 0.615385
2017-12-10T13:12:27.721556: step 38, loss 2.86135, acc 0.578125, prec 0.0204255, recall 0.615385
2017-12-10T13:12:28.176413: step 39, loss 30.306, acc 0.53125, prec 0.0199336, recall 0.6
2017-12-10T13:12:28.627602: step 40, loss 13.5614, acc 0.46875, prec 0.0194018, recall 0.585366
2017-12-10T13:12:29.061076: step 41, loss 22.8124, acc 0.484375, prec 0.019685, recall 0.581395
2017-12-10T13:12:29.496905: step 42, loss 5.22268, acc 0.4375, prec 0.0206422, recall 0.6
2017-12-10T13:12:29.938715: step 43, loss 2.99666, acc 0.5625, prec 0.0209424, recall 0.608696
2017-12-10T13:12:30.382850: step 44, loss 5.43378, acc 0.375, prec 0.0203341, recall 0.608696
2017-12-10T13:12:30.819813: step 45, loss 6.72732, acc 0.359375, prec 0.0197461, recall 0.608696
2017-12-10T13:12:31.257723: step 46, loss 5.71612, acc 0.375, prec 0.0198766, recall 0.617021
2017-12-10T13:12:31.697953: step 47, loss 6.3101, acc 0.265625, prec 0.0192563, recall 0.617021
2017-12-10T13:12:32.149997: step 48, loss 5.11387, acc 0.328125, prec 0.0187218, recall 0.617021
2017-12-10T13:12:32.604271: step 49, loss 4.43288, acc 0.4375, prec 0.0201511, recall 0.64
2017-12-10T13:12:33.049153: step 50, loss 5.37501, acc 0.453125, prec 0.0197166, recall 0.64
2017-12-10T13:12:33.500524: step 51, loss 4.71128, acc 0.5, prec 0.0199275, recall 0.647059
2017-12-10T13:12:33.947978: step 52, loss 13.5784, acc 0.484375, prec 0.0195498, recall 0.634615
2017-12-10T13:12:34.396895: step 53, loss 3.83059, acc 0.578125, prec 0.0203844, recall 0.648148
2017-12-10T13:12:34.836432: step 54, loss 3.61868, acc 0.609375, prec 0.0200918, recall 0.648148
2017-12-10T13:12:35.274285: step 55, loss 13.5035, acc 0.640625, prec 0.0203966, recall 0.642857
2017-12-10T13:12:35.733838: step 56, loss 2.45092, acc 0.640625, prec 0.0201342, recall 0.642857
2017-12-10T13:12:36.173236: step 57, loss 2.0431, acc 0.703125, prec 0.0204646, recall 0.649123
2017-12-10T13:12:36.612851: step 58, loss 2.28067, acc 0.765625, prec 0.0208333, recall 0.655172
2017-12-10T13:12:37.054898: step 59, loss 2.46423, acc 0.65625, prec 0.0211153, recall 0.661017
2017-12-10T13:12:37.501665: step 60, loss 3.67181, acc 0.703125, prec 0.0209115, recall 0.65
2017-12-10T13:12:37.935155: step 61, loss 32.929, acc 0.6875, prec 0.0212202, recall 0.645161
2017-12-10T13:12:38.381904: step 62, loss 2.51163, acc 0.703125, prec 0.0220357, recall 0.65625
2017-12-10T13:12:38.851838: step 63, loss 1.68286, acc 0.703125, prec 0.0218182, recall 0.65625
2017-12-10T13:12:39.293069: step 64, loss 18.3684, acc 0.59375, prec 0.0230415, recall 0.661765
2017-12-10T13:12:39.732951: step 65, loss 3.02498, acc 0.53125, prec 0.0226929, recall 0.661765
2017-12-10T13:12:40.170472: step 66, loss 2.53161, acc 0.671875, prec 0.0229426, recall 0.666667
2017-12-10T13:12:40.611826: step 67, loss 2.97036, acc 0.578125, prec 0.0226378, recall 0.666667
2017-12-10T13:12:41.047908: step 68, loss 3.12297, acc 0.546875, prec 0.0223193, recall 0.666667
2017-12-10T13:12:41.488784: step 69, loss 3.95971, acc 0.46875, prec 0.021957, recall 0.666667
2017-12-10T13:12:41.935274: step 70, loss 2.52378, acc 0.578125, prec 0.0216777, recall 0.666667
2017-12-10T13:12:42.389799: step 71, loss 2.6045, acc 0.625, prec 0.0214352, recall 0.666667
2017-12-10T13:12:42.832773: step 72, loss 3.40517, acc 0.671875, prec 0.0212373, recall 0.657143
2017-12-10T13:12:43.282084: step 73, loss 3.22299, acc 0.609375, prec 0.020995, recall 0.657143
2017-12-10T13:12:43.731878: step 74, loss 4.06042, acc 0.546875, prec 0.0207207, recall 0.657143
2017-12-10T13:12:44.175326: step 75, loss 2.25719, acc 0.609375, prec 0.02049, recall 0.657143
2017-12-10T13:12:44.605757: step 76, loss 1.5413, acc 0.71875, prec 0.020327, recall 0.657143
2017-12-10T13:12:45.047688: step 77, loss 1.38312, acc 0.75, prec 0.020614, recall 0.661972
2017-12-10T13:12:45.483522: step 78, loss 14.4376, acc 0.78125, prec 0.0213508, recall 0.662162
2017-12-10T13:12:45.922129: step 79, loss 34.9805, acc 0.796875, prec 0.0212489, recall 0.644737
2017-12-10T13:12:46.385497: step 80, loss 15.6979, acc 0.734375, prec 0.0215239, recall 0.641026
2017-12-10T13:12:46.835440: step 81, loss 9.3156, acc 0.5625, prec 0.0216929, recall 0.6375
2017-12-10T13:12:47.290108: step 82, loss 3.59798, acc 0.484375, prec 0.0222129, recall 0.646341
2017-12-10T13:12:47.722918: step 83, loss 3.40801, acc 0.5625, prec 0.0219553, recall 0.646341
2017-12-10T13:12:48.180840: step 84, loss 3.48, acc 0.484375, prec 0.0220588, recall 0.650602
2017-12-10T13:12:48.619287: step 85, loss 5.20395, acc 0.359375, prec 0.0216955, recall 0.650602
2017-12-10T13:12:49.063073: step 86, loss 8.21622, acc 0.5, prec 0.0214286, recall 0.642857
2017-12-10T13:12:49.503887: step 87, loss 6.33642, acc 0.265625, prec 0.0210362, recall 0.642857
2017-12-10T13:12:49.934965: step 88, loss 4.17296, acc 0.375, prec 0.0214642, recall 0.651163
2017-12-10T13:12:50.363920: step 89, loss 6.021, acc 0.265625, prec 0.0210843, recall 0.651163
2017-12-10T13:12:50.806112: step 90, loss 5.01835, acc 0.359375, prec 0.0211268, recall 0.655172
2017-12-10T13:12:51.255016: step 91, loss 5.42284, acc 0.296875, prec 0.0207802, recall 0.655172
2017-12-10T13:12:51.703044: step 92, loss 5.35089, acc 0.390625, prec 0.0208408, recall 0.659091
2017-12-10T13:12:52.145076: step 93, loss 3.80495, acc 0.515625, prec 0.0209591, recall 0.662921
2017-12-10T13:12:52.586546: step 94, loss 4.53005, acc 0.40625, prec 0.0210231, recall 0.666667
2017-12-10T13:12:53.016153: step 95, loss 3.17405, acc 0.515625, prec 0.0207972, recall 0.666667
2017-12-10T13:12:53.446522: step 96, loss 2.84496, acc 0.578125, prec 0.0212766, recall 0.673913
2017-12-10T13:12:53.889076: step 97, loss 1.57091, acc 0.65625, prec 0.0211172, recall 0.673913
2017-12-10T13:12:54.330730: step 98, loss 1.65867, acc 0.71875, prec 0.0209885, recall 0.673913
2017-12-10T13:12:54.794782: step 99, loss 1.63607, acc 0.609375, prec 0.0208124, recall 0.673913
2017-12-10T13:12:55.251247: step 100, loss 8.5624, acc 0.703125, prec 0.0206874, recall 0.666667
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-100

2017-12-10T13:12:57.166166: step 101, loss 0.785569, acc 0.84375, prec 0.0206186, recall 0.666667
2017-12-10T13:12:57.620964: step 102, loss 0.638531, acc 0.859375, prec 0.020557, recall 0.666667
2017-12-10T13:12:58.069525: step 103, loss 0.533698, acc 0.828125, prec 0.0204823, recall 0.666667
2017-12-10T13:12:58.507896: step 104, loss 0.667536, acc 0.84375, prec 0.0204149, recall 0.666667
2017-12-10T13:12:58.958807: step 105, loss 26.5566, acc 0.921875, prec 0.020388, recall 0.659574
2017-12-10T13:12:59.411609: step 106, loss 0.324924, acc 0.90625, prec 0.0203479, recall 0.659574
2017-12-10T13:12:59.859191: step 107, loss 0.529763, acc 0.90625, prec 0.0206287, recall 0.663158
2017-12-10T13:13:00.306621: step 108, loss 17.666, acc 0.875, prec 0.0205882, recall 0.649485
2017-12-10T13:13:00.745968: step 109, loss 3.39521, acc 0.859375, prec 0.0205346, recall 0.642857
2017-12-10T13:13:01.195167: step 110, loss 0.986975, acc 0.859375, prec 0.0204745, recall 0.642857
2017-12-10T13:13:01.635631: step 111, loss 0.977642, acc 0.796875, prec 0.0207053, recall 0.646465
2017-12-10T13:13:02.072858: step 112, loss 1.50865, acc 0.71875, prec 0.0205854, recall 0.646465
2017-12-10T13:13:02.514967: step 113, loss 6.90697, acc 0.65625, prec 0.0207601, recall 0.643564
2017-12-10T13:13:02.956566: step 114, loss 2.4128, acc 0.625, prec 0.0206022, recall 0.643564
2017-12-10T13:13:03.401291: step 115, loss 2.58707, acc 0.625, prec 0.0207547, recall 0.647059
2017-12-10T13:13:03.853593: step 116, loss 5.71719, acc 0.53125, prec 0.0205672, recall 0.640777
2017-12-10T13:13:04.300191: step 117, loss 2.3596, acc 0.53125, prec 0.0203767, recall 0.640777
2017-12-10T13:13:04.752131: step 118, loss 4.33163, acc 0.46875, prec 0.020165, recall 0.640777
2017-12-10T13:13:05.189777: step 119, loss 9.23083, acc 0.40625, prec 0.0202356, recall 0.638095
2017-12-10T13:13:05.634105: step 120, loss 4.24957, acc 0.453125, prec 0.0200239, recall 0.638095
2017-12-10T13:13:06.081132: step 121, loss 3.49564, acc 0.484375, prec 0.0204082, recall 0.64486
2017-12-10T13:13:06.532934: step 122, loss 5.67576, acc 0.46875, prec 0.0207845, recall 0.645455
2017-12-10T13:13:06.986992: step 123, loss 2.9676, acc 0.484375, prec 0.0205857, recall 0.645455
2017-12-10T13:13:07.452758: step 124, loss 4.90872, acc 0.421875, prec 0.0203672, recall 0.645455
2017-12-10T13:13:07.897928: step 125, loss 4.79294, acc 0.421875, prec 0.0201533, recall 0.645455
2017-12-10T13:13:08.335955: step 126, loss 4.03767, acc 0.484375, prec 0.0207924, recall 0.654867
2017-12-10T13:13:08.768187: step 127, loss 2.33655, acc 0.578125, prec 0.0211817, recall 0.66087
2017-12-10T13:13:09.210610: step 128, loss 4.49969, acc 0.40625, prec 0.0212297, recall 0.663793
2017-12-10T13:13:09.652696: step 129, loss 3.16112, acc 0.53125, prec 0.0213231, recall 0.666667
2017-12-10T13:13:10.091485: step 130, loss 2.90852, acc 0.578125, prec 0.0214324, recall 0.669492
2017-12-10T13:13:10.542894: step 131, loss 8.71539, acc 0.703125, prec 0.0213283, recall 0.663866
2017-12-10T13:13:10.989950: step 132, loss 2.2939, acc 0.65625, prec 0.021465, recall 0.666667
2017-12-10T13:13:11.445677: step 133, loss 2.36897, acc 0.53125, prec 0.0212936, recall 0.666667
2017-12-10T13:13:11.881225: step 134, loss 9.93554, acc 0.671875, prec 0.0214399, recall 0.663934
2017-12-10T13:13:12.327336: step 135, loss 1.99817, acc 0.5625, prec 0.0212822, recall 0.663934
2017-12-10T13:13:12.782391: step 136, loss 10.9884, acc 0.671875, prec 0.0214267, recall 0.66129
2017-12-10T13:13:13.232383: step 137, loss 2.96516, acc 0.75, prec 0.0213431, recall 0.656
2017-12-10T13:13:13.679223: step 138, loss 1.77941, acc 0.640625, prec 0.021216, recall 0.656
2017-12-10T13:13:14.123184: step 139, loss 1.68669, acc 0.734375, prec 0.0213752, recall 0.65873
2017-12-10T13:13:14.574656: step 140, loss 2.5156, acc 0.609375, prec 0.0212385, recall 0.65873
2017-12-10T13:13:15.037990: step 141, loss 2.30227, acc 0.5625, prec 0.0215846, recall 0.664062
2017-12-10T13:13:15.477464: step 142, loss 16.2868, acc 0.671875, prec 0.0214755, recall 0.658915
2017-12-10T13:13:15.930874: step 143, loss 16.5197, acc 0.609375, prec 0.0213461, recall 0.653846
2017-12-10T13:13:16.402649: step 144, loss 23.2578, acc 0.625, prec 0.0214732, recall 0.646617
2017-12-10T13:13:16.858856: step 145, loss 3.88574, acc 0.46875, prec 0.0212924, recall 0.646617
2017-12-10T13:13:17.305516: step 146, loss 2.92666, acc 0.609375, prec 0.0214022, recall 0.649254
2017-12-10T13:13:17.764131: step 147, loss 4.4529, acc 0.4375, prec 0.021453, recall 0.651852
2017-12-10T13:13:18.209686: step 148, loss 3.97775, acc 0.515625, prec 0.021292, recall 0.651852
2017-12-10T13:13:18.655039: step 149, loss 6.55428, acc 0.3125, prec 0.0210678, recall 0.651852
2017-12-10T13:13:19.090578: step 150, loss 5.4002, acc 0.421875, prec 0.0211151, recall 0.654412
2017-12-10T13:13:19.526808: step 151, loss 6.23076, acc 0.234375, prec 0.0208724, recall 0.654412
2017-12-10T13:13:19.969647: step 152, loss 6.03531, acc 0.296875, prec 0.0206544, recall 0.654412
2017-12-10T13:13:20.405844: step 153, loss 3.57292, acc 0.453125, prec 0.020488, recall 0.654412
2017-12-10T13:13:20.854680: step 154, loss 4.42812, acc 0.34375, prec 0.0202918, recall 0.654412
2017-12-10T13:13:21.295530: step 155, loss 5.34833, acc 0.390625, prec 0.020113, recall 0.654412
2017-12-10T13:13:21.734078: step 156, loss 2.50269, acc 0.640625, prec 0.020009, recall 0.654412
2017-12-10T13:13:22.171880: step 157, loss 2.25605, acc 0.609375, prec 0.0198972, recall 0.654412
2017-12-10T13:13:22.612397: step 158, loss 20.2365, acc 0.640625, prec 0.0197998, recall 0.649635
2017-12-10T13:13:23.059219: step 159, loss 13.6226, acc 0.671875, prec 0.0199291, recall 0.647482
2017-12-10T13:13:23.514142: step 160, loss 2.77721, acc 0.59375, prec 0.0198151, recall 0.647482
2017-12-10T13:13:23.966443: step 161, loss 3.25464, acc 0.6875, prec 0.0197325, recall 0.642857
2017-12-10T13:13:24.401900: step 162, loss 16.6078, acc 0.5625, prec 0.0196164, recall 0.638298
2017-12-10T13:13:24.843868: step 163, loss 9.62326, acc 0.625, prec 0.0195185, recall 0.633803
2017-12-10T13:13:25.296126: step 164, loss 15.0892, acc 0.484375, prec 0.0198105, recall 0.630137
2017-12-10T13:13:25.740610: step 165, loss 17.8485, acc 0.453125, prec 0.0196665, recall 0.62585
2017-12-10T13:13:26.201797: step 166, loss 3.70022, acc 0.421875, prec 0.0199279, recall 0.630872
2017-12-10T13:13:26.662670: step 167, loss 5.99613, acc 0.28125, prec 0.0199412, recall 0.633333
2017-12-10T13:13:27.099057: step 168, loss 5.75805, acc 0.265625, prec 0.0197464, recall 0.633333
2017-12-10T13:13:27.547010: step 169, loss 5.73846, acc 0.3125, prec 0.0197694, recall 0.635762
2017-12-10T13:13:27.984436: step 170, loss 6.25104, acc 0.328125, prec 0.0197959, recall 0.638158
2017-12-10T13:13:28.427038: step 171, loss 7.36651, acc 0.25, prec 0.019802, recall 0.640523
2017-12-10T13:13:28.878115: step 172, loss 7.98788, acc 0.203125, prec 0.0196, recall 0.640523
2017-12-10T13:13:29.313147: step 173, loss 4.59975, acc 0.296875, prec 0.0194252, recall 0.640523
2017-12-10T13:13:29.749960: step 174, loss 7.25221, acc 0.296875, prec 0.0194461, recall 0.642857
2017-12-10T13:13:30.202008: step 175, loss 5.71002, acc 0.25, prec 0.0192644, recall 0.642857
2017-12-10T13:13:30.648948: step 176, loss 4.46123, acc 0.34375, prec 0.0192976, recall 0.645161
2017-12-10T13:13:31.084594: step 177, loss 23.0085, acc 0.5, prec 0.0191865, recall 0.636943
2017-12-10T13:13:31.555519: step 178, loss 3.06939, acc 0.515625, prec 0.0192601, recall 0.639241
2017-12-10T13:13:31.996237: step 179, loss 3.10394, acc 0.46875, prec 0.019136, recall 0.639241
2017-12-10T13:13:32.446122: step 180, loss 2.86533, acc 0.546875, prec 0.0192163, recall 0.641509
2017-12-10T13:13:32.884410: step 181, loss 8.69419, acc 0.5, prec 0.0192884, recall 0.639752
2017-12-10T13:13:33.329929: step 182, loss 3.85104, acc 0.453125, prec 0.0191628, recall 0.639752
2017-12-10T13:13:33.772636: step 183, loss 2.42803, acc 0.546875, prec 0.0192414, recall 0.641975
2017-12-10T13:13:34.207706: step 184, loss 2.81697, acc 0.609375, prec 0.0191529, recall 0.641975
2017-12-10T13:13:34.648128: step 185, loss 2.31609, acc 0.609375, prec 0.0190651, recall 0.641975
2017-12-10T13:13:35.098327: step 186, loss 1.55932, acc 0.71875, prec 0.0190024, recall 0.641975
2017-12-10T13:13:35.547319: step 187, loss 1.78209, acc 0.71875, prec 0.0191187, recall 0.644172
2017-12-10T13:13:35.998225: step 188, loss 1.41079, acc 0.75, prec 0.0190632, recall 0.644172
2017-12-10T13:13:36.454430: step 189, loss 1.02902, acc 0.8125, prec 0.0190217, recall 0.644172
2017-12-10T13:13:36.889381: step 190, loss 31.5186, acc 0.765625, prec 0.0193315, recall 0.640719
2017-12-10T13:13:37.334241: step 191, loss 1.25094, acc 0.765625, prec 0.019456, recall 0.642857
2017-12-10T13:13:37.779128: step 192, loss 1.1092, acc 0.78125, prec 0.019407, recall 0.642857
2017-12-10T13:13:38.234404: step 193, loss 0.963264, acc 0.84375, prec 0.0195481, recall 0.64497
2017-12-10T13:13:38.677148: step 194, loss 17.2225, acc 0.703125, prec 0.0196604, recall 0.643275
2017-12-10T13:13:39.130354: step 195, loss 1.51332, acc 0.703125, prec 0.0195939, recall 0.643275
2017-12-10T13:13:39.574411: step 196, loss 1.50337, acc 0.703125, prec 0.0195278, recall 0.643275
2017-12-10T13:13:40.033460: step 197, loss 49.6022, acc 0.8125, prec 0.0198441, recall 0.636364
2017-12-10T13:13:40.482744: step 198, loss 18.0796, acc 0.703125, prec 0.0199541, recall 0.634831
2017-12-10T13:13:40.926176: step 199, loss 11.4888, acc 0.6875, prec 0.0198874, recall 0.631285
2017-12-10T13:13:41.369058: step 200, loss 3.44542, acc 0.5, prec 0.019776, recall 0.631285
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-200

2017-12-10T13:13:43.512750: step 201, loss 4.04863, acc 0.453125, prec 0.0196556, recall 0.631285
2017-12-10T13:13:43.966403: step 202, loss 6.23829, acc 0.328125, prec 0.0198481, recall 0.635359
2017-12-10T13:13:44.402422: step 203, loss 7.09051, acc 0.234375, prec 0.0198494, recall 0.637363
2017-12-10T13:13:44.842415: step 204, loss 6.98504, acc 0.296875, prec 0.0198642, recall 0.639344
2017-12-10T13:13:45.282401: step 205, loss 6.38022, acc 0.25, prec 0.0197036, recall 0.639344
2017-12-10T13:13:45.719829: step 206, loss 6.45725, acc 0.296875, prec 0.0197193, recall 0.641304
2017-12-10T13:13:46.158506: step 207, loss 8.32958, acc 0.234375, prec 0.0197216, recall 0.643243
2017-12-10T13:13:46.593372: step 208, loss 7.8533, acc 0.1875, prec 0.0197141, recall 0.645161
2017-12-10T13:13:47.035394: step 209, loss 6.32518, acc 0.265625, prec 0.0197229, recall 0.647059
2017-12-10T13:13:47.483049: step 210, loss 4.58757, acc 0.328125, prec 0.0197443, recall 0.648936
2017-12-10T13:13:47.935086: step 211, loss 5.79969, acc 0.3125, prec 0.0197622, recall 0.650794
2017-12-10T13:13:48.377657: step 212, loss 2.989, acc 0.546875, prec 0.0198273, recall 0.652632
2017-12-10T13:13:48.815076: step 213, loss 3.03257, acc 0.453125, prec 0.0198728, recall 0.65445
2017-12-10T13:13:49.279964: step 214, loss 3.28377, acc 0.515625, prec 0.0199304, recall 0.65625
2017-12-10T13:13:49.721377: step 215, loss 3.28668, acc 0.515625, prec 0.0198331, recall 0.65625
2017-12-10T13:13:50.174288: step 216, loss 1.41996, acc 0.78125, prec 0.0199435, recall 0.658031
2017-12-10T13:13:50.614853: step 217, loss 1.52924, acc 0.765625, prec 0.0198966, recall 0.658031
2017-12-10T13:13:51.068831: step 218, loss 1.16351, acc 0.75, prec 0.02, recall 0.659794
2017-12-10T13:13:51.508707: step 219, loss 11.8132, acc 0.734375, prec 0.0199532, recall 0.653061
2017-12-10T13:13:51.961206: step 220, loss 0.807423, acc 0.828125, prec 0.0199191, recall 0.653061
2017-12-10T13:13:52.404665: step 221, loss 0.685673, acc 0.890625, prec 0.0200497, recall 0.654822
2017-12-10T13:13:52.845477: step 222, loss 0.889857, acc 0.90625, prec 0.0201832, recall 0.656566
2017-12-10T13:13:53.282716: step 223, loss 0.434131, acc 0.90625, prec 0.0203164, recall 0.658291
2017-12-10T13:13:53.737887: step 224, loss 0.804856, acc 0.875, prec 0.0204429, recall 0.66
2017-12-10T13:13:54.191369: step 225, loss 0.849793, acc 0.859375, prec 0.0205659, recall 0.661692
2017-12-10T13:13:54.630115: step 226, loss 29.0202, acc 0.828125, prec 0.0206886, recall 0.656863
2017-12-10T13:13:55.075117: step 227, loss 0.392336, acc 0.90625, prec 0.0206694, recall 0.656863
2017-12-10T13:13:55.516996: step 228, loss 4.51792, acc 0.796875, prec 0.0206313, recall 0.653659
2017-12-10T13:13:55.962621: step 229, loss 0.690723, acc 0.828125, prec 0.0207469, recall 0.65534
2017-12-10T13:13:56.412568: step 230, loss 0.943428, acc 0.765625, prec 0.0206992, recall 0.65534
2017-12-10T13:13:56.844750: step 231, loss 1.30283, acc 0.734375, prec 0.0206454, recall 0.65534
2017-12-10T13:13:57.296597: step 232, loss 1.84368, acc 0.703125, prec 0.0207349, recall 0.657005
2017-12-10T13:13:57.736117: step 233, loss 1.65246, acc 0.734375, prec 0.0208302, recall 0.658654
2017-12-10T13:13:58.203937: step 234, loss 1.58798, acc 0.671875, prec 0.0209123, recall 0.660287
2017-12-10T13:13:58.653086: step 235, loss 1.11834, acc 0.703125, prec 0.0210002, recall 0.661905
2017-12-10T13:13:59.103185: step 236, loss 1.61566, acc 0.65625, prec 0.0209306, recall 0.661905
2017-12-10T13:13:59.546068: step 237, loss 2.3023, acc 0.65625, prec 0.0208615, recall 0.661905
2017-12-10T13:13:59.994627: step 238, loss 3.04415, acc 0.6875, prec 0.0209487, recall 0.660377
2017-12-10T13:14:00.430538: step 239, loss 1.92652, acc 0.703125, prec 0.0210354, recall 0.661972
2017-12-10T13:14:00.874747: step 240, loss 0.762692, acc 0.828125, prec 0.0212924, recall 0.665116
2017-12-10T13:14:01.320095: step 241, loss 1.8123, acc 0.671875, prec 0.0212261, recall 0.665116
2017-12-10T13:14:01.784205: step 242, loss 1.95356, acc 0.703125, prec 0.0211664, recall 0.665116
2017-12-10T13:14:02.236964: step 243, loss 1.49833, acc 0.703125, prec 0.021107, recall 0.665116
2017-12-10T13:14:02.686402: step 244, loss 1.45365, acc 0.75, prec 0.0210573, recall 0.665116
2017-12-10T13:14:03.139846: step 245, loss 24.4044, acc 0.796875, prec 0.0210263, recall 0.655963
2017-12-10T13:14:03.584204: step 246, loss 10.369, acc 0.828125, prec 0.0209954, recall 0.652968
2017-12-10T13:14:04.034691: step 247, loss 17.8949, acc 0.703125, prec 0.0210835, recall 0.651584
2017-12-10T13:14:04.480111: step 248, loss 10.4313, acc 0.65625, prec 0.0211617, recall 0.650224
2017-12-10T13:14:04.953036: step 249, loss 8.58041, acc 0.5, prec 0.0212086, recall 0.648889
2017-12-10T13:14:05.388349: step 250, loss 3.57504, acc 0.4375, prec 0.0213811, recall 0.651982
2017-12-10T13:14:05.838531: step 251, loss 5.43897, acc 0.328125, prec 0.0212491, recall 0.651982
2017-12-10T13:14:06.293860: step 252, loss 8.25718, acc 0.15625, prec 0.0210856, recall 0.651982
2017-12-10T13:14:06.734059: step 253, loss 10.3707, acc 0.21875, prec 0.0212164, recall 0.652174
2017-12-10T13:14:07.174243: step 254, loss 7.25919, acc 0.125, prec 0.0210497, recall 0.652174
2017-12-10T13:14:07.627283: step 255, loss 7.88421, acc 0.21875, prec 0.0210394, recall 0.65368
2017-12-10T13:14:08.058840: step 256, loss 7.54525, acc 0.203125, prec 0.0210264, recall 0.655172
2017-12-10T13:14:08.497119: step 257, loss 9.58437, acc 0.203125, prec 0.0210136, recall 0.656652
2017-12-10T13:14:08.942508: step 258, loss 8.01658, acc 0.09375, prec 0.0212476, recall 0.661017
2017-12-10T13:14:09.394623: step 259, loss 7.80699, acc 0.171875, prec 0.0212277, recall 0.662447
2017-12-10T13:14:09.840755: step 260, loss 6.32915, acc 0.25, prec 0.0213537, recall 0.665272
2017-12-10T13:14:10.273183: step 261, loss 5.86908, acc 0.3125, prec 0.021359, recall 0.666667
2017-12-10T13:14:10.723271: step 262, loss 6.00049, acc 0.296875, prec 0.0213613, recall 0.66805
2017-12-10T13:14:11.165134: step 263, loss 4.6167, acc 0.28125, prec 0.0212317, recall 0.66805
2017-12-10T13:14:11.598552: step 264, loss 3.9261, acc 0.4375, prec 0.0212598, recall 0.669421
2017-12-10T13:14:12.039869: step 265, loss 4.40115, acc 0.328125, prec 0.0212683, recall 0.670782
2017-12-10T13:14:12.478931: step 266, loss 2.51196, acc 0.625, prec 0.0212019, recall 0.670782
2017-12-10T13:14:12.924917: step 267, loss 3.24671, acc 0.546875, prec 0.0213758, recall 0.673469
2017-12-10T13:14:13.390222: step 268, loss 1.99452, acc 0.578125, prec 0.0213013, recall 0.673469
2017-12-10T13:14:13.830108: step 269, loss 1.82064, acc 0.609375, prec 0.0212328, recall 0.673469
2017-12-10T13:14:14.280093: step 270, loss 1.04747, acc 0.78125, prec 0.0213203, recall 0.674797
2017-12-10T13:14:14.720971: step 271, loss 9.68804, acc 0.796875, prec 0.0212875, recall 0.672065
2017-12-10T13:14:15.168258: step 272, loss 0.579978, acc 0.84375, prec 0.0212602, recall 0.672065
2017-12-10T13:14:15.611483: step 273, loss 1.26352, acc 0.828125, prec 0.0213555, recall 0.673387
2017-12-10T13:14:16.062613: step 274, loss 14.9603, acc 0.765625, prec 0.0213173, recall 0.670683
2017-12-10T13:14:16.519207: step 275, loss 0.980569, acc 0.78125, prec 0.0212793, recall 0.670683
2017-12-10T13:14:16.969454: step 276, loss 0.656057, acc 0.828125, prec 0.0212495, recall 0.670683
2017-12-10T13:14:17.423769: step 277, loss 23.087, acc 0.84375, prec 0.0213523, recall 0.666667
2017-12-10T13:14:17.882049: step 278, loss 6.83435, acc 0.796875, prec 0.0215681, recall 0.666667
2017-12-10T13:14:18.336542: step 279, loss 0.971878, acc 0.78125, prec 0.0216538, recall 0.667969
2017-12-10T13:14:18.782505: step 280, loss 1.54779, acc 0.6875, prec 0.0218462, recall 0.670543
2017-12-10T13:14:19.233663: step 281, loss 2.09847, acc 0.640625, prec 0.0221523, recall 0.67433
2017-12-10T13:14:19.662254: step 282, loss 2.47564, acc 0.5625, prec 0.0221971, recall 0.675573
2017-12-10T13:14:20.101128: step 283, loss 2.2988, acc 0.671875, prec 0.0222611, recall 0.676806
2017-12-10T13:14:20.537738: step 284, loss 13.7358, acc 0.59375, prec 0.0221917, recall 0.674242
2017-12-10T13:14:20.987669: step 285, loss 3.27343, acc 0.5, prec 0.0221036, recall 0.674242
2017-12-10T13:14:21.426518: step 286, loss 3.47691, acc 0.46875, prec 0.0222524, recall 0.676692
2017-12-10T13:14:21.892971: step 287, loss 4.14912, acc 0.390625, prec 0.022266, recall 0.677903
2017-12-10T13:14:22.328792: step 288, loss 3.3075, acc 0.375, prec 0.0222766, recall 0.679105
2017-12-10T13:14:22.766371: step 289, loss 3.27827, acc 0.453125, prec 0.0221816, recall 0.679105
2017-12-10T13:14:23.209663: step 290, loss 3.33678, acc 0.484375, prec 0.0220927, recall 0.679105
2017-12-10T13:14:23.657431: step 291, loss 9.41455, acc 0.5, prec 0.0220126, recall 0.674074
2017-12-10T13:14:24.092918: step 292, loss 2.75333, acc 0.515625, prec 0.0219304, recall 0.674074
2017-12-10T13:14:24.531131: step 293, loss 4.14401, acc 0.5625, prec 0.0222115, recall 0.675182
2017-12-10T13:14:24.985370: step 294, loss 4.41939, acc 0.390625, prec 0.0223417, recall 0.677536
2017-12-10T13:14:25.432816: step 295, loss 4.06364, acc 0.375, prec 0.0223517, recall 0.6787
2017-12-10T13:14:25.881299: step 296, loss 4.01025, acc 0.359375, prec 0.0224746, recall 0.681004
2017-12-10T13:14:26.334691: step 297, loss 3.27291, acc 0.515625, prec 0.0223925, recall 0.681004
2017-12-10T13:14:26.777211: step 298, loss 3.65022, acc 0.421875, prec 0.0224099, recall 0.682143
2017-12-10T13:14:27.250581: step 299, loss 2.66737, acc 0.578125, prec 0.0226821, recall 0.685512
2017-12-10T13:14:27.694706: step 300, loss 2.58105, acc 0.546875, prec 0.0226055, recall 0.685512
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-300

2017-12-10T13:14:29.617520: step 301, loss 1.87257, acc 0.609375, prec 0.0225398, recall 0.685512
2017-12-10T13:14:30.059670: step 302, loss 1.86256, acc 0.6875, prec 0.0224875, recall 0.685512
2017-12-10T13:14:30.496398: step 303, loss 1.62984, acc 0.6875, prec 0.0225486, recall 0.68662
2017-12-10T13:14:30.931613: step 304, loss 0.948638, acc 0.734375, prec 0.0225043, recall 0.68662
2017-12-10T13:14:31.387347: step 305, loss 1.75562, acc 0.71875, prec 0.0227953, recall 0.689895
2017-12-10T13:14:31.827914: step 306, loss 1.1986, acc 0.703125, prec 0.0227455, recall 0.689895
2017-12-10T13:14:32.284770: step 307, loss 0.932336, acc 0.796875, prec 0.0227116, recall 0.689895
2017-12-10T13:14:32.734606: step 308, loss 2.73146, acc 0.8125, prec 0.022683, recall 0.6875
2017-12-10T13:14:33.198427: step 309, loss 2.19606, acc 0.796875, prec 0.0226519, recall 0.685121
2017-12-10T13:14:33.632076: step 310, loss 0.56076, acc 0.84375, prec 0.022626, recall 0.685121
2017-12-10T13:14:34.078121: step 311, loss 7.72507, acc 0.8125, prec 0.0227091, recall 0.683849
2017-12-10T13:14:34.517312: step 312, loss 0.623528, acc 0.828125, prec 0.0226806, recall 0.683849
2017-12-10T13:14:34.948145: step 313, loss 1.30819, acc 0.84375, prec 0.0227661, recall 0.684932
2017-12-10T13:14:35.398262: step 314, loss 1.32491, acc 0.703125, prec 0.0227169, recall 0.684932
2017-12-10T13:14:35.841803: step 315, loss 0.623256, acc 0.8125, prec 0.022686, recall 0.684932
2017-12-10T13:14:36.294478: step 316, loss 6.0597, acc 0.6875, prec 0.0226372, recall 0.682594
2017-12-10T13:14:36.748152: step 317, loss 9.48852, acc 0.65625, prec 0.0225836, recall 0.680272
2017-12-10T13:14:37.209931: step 318, loss 1.11964, acc 0.765625, prec 0.0225454, recall 0.680272
2017-12-10T13:14:37.661430: step 319, loss 1.66423, acc 0.65625, prec 0.0225995, recall 0.681356
2017-12-10T13:14:38.113049: step 320, loss 1.55623, acc 0.6875, prec 0.0225488, recall 0.681356
2017-12-10T13:14:38.545381: step 321, loss 1.06427, acc 0.734375, prec 0.0227247, recall 0.683502
2017-12-10T13:14:39.001182: step 322, loss 2.11833, acc 0.5625, prec 0.0227628, recall 0.684564
2017-12-10T13:14:39.446500: step 323, loss 13.9449, acc 0.578125, prec 0.0228082, recall 0.681063
2017-12-10T13:14:39.886963: step 324, loss 2.32472, acc 0.578125, prec 0.0228483, recall 0.682119
2017-12-10T13:14:40.315998: step 325, loss 2.00581, acc 0.671875, prec 0.0231195, recall 0.685246
2017-12-10T13:14:40.751301: step 326, loss 2.02764, acc 0.546875, prec 0.0231533, recall 0.686275
2017-12-10T13:14:41.186883: step 327, loss 2.86561, acc 0.5, prec 0.0231792, recall 0.687296
2017-12-10T13:14:41.620378: step 328, loss 2.66439, acc 0.65625, prec 0.0231233, recall 0.687296
2017-12-10T13:14:42.059066: step 329, loss 2.96329, acc 0.5625, prec 0.0230526, recall 0.687296
2017-12-10T13:14:42.501267: step 330, loss 2.56337, acc 0.609375, prec 0.0229898, recall 0.687296
2017-12-10T13:14:42.955666: step 331, loss 3.14091, acc 0.4375, prec 0.0228999, recall 0.687296
2017-12-10T13:14:43.398659: step 332, loss 2.17258, acc 0.640625, prec 0.0233716, recall 0.692308
2017-12-10T13:14:43.845526: step 333, loss 2.87173, acc 0.609375, prec 0.0235193, recall 0.694268
2017-12-10T13:14:44.291683: step 334, loss 2.21823, acc 0.703125, prec 0.0235763, recall 0.695238
2017-12-10T13:14:44.760993: step 335, loss 4.44269, acc 0.5625, prec 0.0236128, recall 0.694006
2017-12-10T13:14:45.214043: step 336, loss 1.43427, acc 0.75, prec 0.0236769, recall 0.694969
2017-12-10T13:14:45.661696: step 337, loss 2.81277, acc 0.59375, prec 0.0236111, recall 0.694969
2017-12-10T13:14:46.116210: step 338, loss 1.2557, acc 0.78125, prec 0.0235758, recall 0.694969
2017-12-10T13:14:46.566626: step 339, loss 1.83889, acc 0.75, prec 0.0235357, recall 0.694969
2017-12-10T13:14:47.009492: step 340, loss 24.3462, acc 0.671875, prec 0.0235919, recall 0.691589
2017-12-10T13:14:47.453286: step 341, loss 20.6567, acc 0.5625, prec 0.0236279, recall 0.690402
2017-12-10T13:14:47.908251: step 342, loss 1.60071, acc 0.6875, prec 0.0236812, recall 0.691358
2017-12-10T13:14:48.359095: step 343, loss 2.52252, acc 0.625, prec 0.0237242, recall 0.692308
2017-12-10T13:14:48.814700: step 344, loss 2.79664, acc 0.609375, prec 0.0236618, recall 0.692308
2017-12-10T13:14:49.268459: step 345, loss 2.44677, acc 0.609375, prec 0.0235997, recall 0.692308
2017-12-10T13:14:49.716425: step 346, loss 3.32822, acc 0.484375, prec 0.0236204, recall 0.693252
2017-12-10T13:14:50.164756: step 347, loss 1.82192, acc 0.609375, prec 0.0235588, recall 0.693252
2017-12-10T13:14:50.601672: step 348, loss 2.36244, acc 0.609375, prec 0.0234976, recall 0.693252
2017-12-10T13:14:51.042171: step 349, loss 2.06924, acc 0.65625, prec 0.023444, recall 0.693252
2017-12-10T13:14:51.501516: step 350, loss 2.96548, acc 0.546875, prec 0.0233737, recall 0.693252
2017-12-10T13:14:51.947181: step 351, loss 15.5918, acc 0.53125, prec 0.0233038, recall 0.691131
2017-12-10T13:14:52.381389: step 352, loss 2.65721, acc 0.59375, prec 0.0233419, recall 0.692073
2017-12-10T13:14:52.834277: step 353, loss 1.65529, acc 0.671875, prec 0.0232916, recall 0.692073
2017-12-10T13:14:53.282777: step 354, loss 1.06481, acc 0.78125, prec 0.0233583, recall 0.693009
2017-12-10T13:14:53.745080: step 355, loss 2.40827, acc 0.625, prec 0.0234008, recall 0.693939
2017-12-10T13:14:54.134829: step 356, loss 1.31059, acc 0.6875, prec 0.0234526, recall 0.694864
2017-12-10T13:14:54.522979: step 357, loss 1.81432, acc 0.671875, prec 0.0234025, recall 0.694864
2017-12-10T13:14:54.915449: step 358, loss 1.08555, acc 0.796875, prec 0.0233716, recall 0.694864
2017-12-10T13:14:55.306600: step 359, loss 0.8503, acc 0.828125, prec 0.0233455, recall 0.694864
2017-12-10T13:14:55.693662: step 360, loss 1.56705, acc 0.75, prec 0.0234066, recall 0.695783
2017-12-10T13:14:56.147746: step 361, loss 2.99041, acc 0.765625, prec 0.0233735, recall 0.693694
2017-12-10T13:14:56.591993: step 362, loss 16.3891, acc 0.640625, prec 0.0233216, recall 0.691617
2017-12-10T13:14:57.043561: step 363, loss 1.09225, acc 0.8125, prec 0.0234903, recall 0.693452
2017-12-10T13:14:57.491279: step 364, loss 1.57188, acc 0.828125, prec 0.0236609, recall 0.695266
2017-12-10T13:14:57.946165: step 365, loss 1.09051, acc 0.75, prec 0.023721, recall 0.696165
2017-12-10T13:14:58.399297: step 366, loss 1.51774, acc 0.6875, prec 0.0237713, recall 0.697059
2017-12-10T13:14:58.846534: step 367, loss 4.32119, acc 0.796875, prec 0.0237427, recall 0.695015
2017-12-10T13:14:59.291627: step 368, loss 1.45447, acc 0.75, prec 0.0237047, recall 0.695015
2017-12-10T13:14:59.741104: step 369, loss 0.833539, acc 0.765625, prec 0.0237667, recall 0.695906
2017-12-10T13:15:00.193971: step 370, loss 1.24162, acc 0.75, prec 0.0238261, recall 0.696793
2017-12-10T13:15:00.634370: step 371, loss 1.45243, acc 0.671875, prec 0.0237764, recall 0.696793
2017-12-10T13:15:01.067235: step 372, loss 1.42355, acc 0.75, prec 0.0237386, recall 0.696793
2017-12-10T13:15:01.510034: step 373, loss 3.43077, acc 0.765625, prec 0.0238992, recall 0.696532
2017-12-10T13:15:01.959191: step 374, loss 1.88721, acc 0.703125, prec 0.0239509, recall 0.697406
2017-12-10T13:15:02.402823: step 375, loss 2.40927, acc 0.546875, prec 0.024075, recall 0.69914
2017-12-10T13:15:02.847811: step 376, loss 1.80614, acc 0.6875, prec 0.0241237, recall 0.7
2017-12-10T13:15:03.305110: step 377, loss 2.20734, acc 0.640625, prec 0.024165, recall 0.700855
2017-12-10T13:15:03.763347: step 378, loss 1.70591, acc 0.671875, prec 0.0241153, recall 0.700855
2017-12-10T13:15:04.219245: step 379, loss 1.43836, acc 0.6875, prec 0.0240681, recall 0.700855
2017-12-10T13:15:04.665759: step 380, loss 1.38196, acc 0.734375, prec 0.024314, recall 0.70339
2017-12-10T13:15:05.108144: step 381, loss 0.995165, acc 0.75, prec 0.0242761, recall 0.70339
2017-12-10T13:15:05.544194: step 382, loss 1.2228, acc 0.71875, prec 0.0243285, recall 0.704225
2017-12-10T13:15:05.987319: step 383, loss 16.1719, acc 0.75, prec 0.0242931, recall 0.702247
2017-12-10T13:15:06.442535: step 384, loss 1.01367, acc 0.734375, prec 0.0243477, recall 0.703081
2017-12-10T13:15:06.884315: step 385, loss 1.53063, acc 0.6875, prec 0.0243005, recall 0.703081
2017-12-10T13:15:07.333617: step 386, loss 0.553914, acc 0.875, prec 0.0242817, recall 0.703081
2017-12-10T13:15:07.786093: step 387, loss 1.29555, acc 0.796875, prec 0.0243455, recall 0.703911
2017-12-10T13:15:08.230236: step 388, loss 1.05058, acc 0.78125, prec 0.0243126, recall 0.703911
2017-12-10T13:15:08.671945: step 389, loss 1.67409, acc 0.71875, prec 0.0242704, recall 0.703911
2017-12-10T13:15:09.107627: step 390, loss 1.24352, acc 0.71875, prec 0.024416, recall 0.705556
2017-12-10T13:15:09.556666: step 391, loss 0.851356, acc 0.859375, prec 0.0244886, recall 0.706371
2017-12-10T13:15:10.006522: step 392, loss 10.0415, acc 0.78125, prec 0.0245516, recall 0.705234
2017-12-10T13:15:10.456720: step 393, loss 0.677067, acc 0.828125, prec 0.0245258, recall 0.705234
2017-12-10T13:15:10.903803: step 394, loss 0.675433, acc 0.8125, prec 0.0245909, recall 0.706044
2017-12-10T13:15:11.352279: step 395, loss 6.05655, acc 0.78125, prec 0.0247468, recall 0.705722
2017-12-10T13:15:11.793110: step 396, loss 1.27467, acc 0.6875, prec 0.0247926, recall 0.706522
2017-12-10T13:15:12.248034: step 397, loss 0.765523, acc 0.796875, prec 0.0247619, recall 0.706522
2017-12-10T13:15:12.703761: step 398, loss 2.65397, acc 0.640625, prec 0.0248931, recall 0.708108
2017-12-10T13:15:13.141099: step 399, loss 0.679248, acc 0.796875, prec 0.0248624, recall 0.708108
2017-12-10T13:15:13.576197: step 400, loss 1.58243, acc 0.765625, prec 0.0248271, recall 0.708108
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-400

2017-12-10T13:15:15.430836: step 401, loss 0.765914, acc 0.765625, prec 0.0247918, recall 0.708108
2017-12-10T13:15:15.877962: step 402, loss 1.94432, acc 0.640625, prec 0.024738, recall 0.708108
2017-12-10T13:15:16.331152: step 403, loss 0.736358, acc 0.796875, prec 0.0248916, recall 0.709677
2017-12-10T13:15:16.778926: step 404, loss 1.4521, acc 0.625, prec 0.0250188, recall 0.71123
2017-12-10T13:15:17.216139: step 405, loss 1.71623, acc 0.71875, prec 0.0249765, recall 0.71123
2017-12-10T13:15:17.674732: step 406, loss 11.0146, acc 0.6875, prec 0.024932, recall 0.709333
2017-12-10T13:15:18.122314: step 407, loss 0.99928, acc 0.796875, prec 0.0249017, recall 0.709333
2017-12-10T13:15:18.562715: step 408, loss 0.978738, acc 0.78125, prec 0.0250514, recall 0.710875
2017-12-10T13:15:19.005823: step 409, loss 2.05918, acc 0.734375, prec 0.0252846, recall 0.713158
2017-12-10T13:15:19.454532: step 410, loss 1.4877, acc 0.734375, prec 0.0252445, recall 0.713158
2017-12-10T13:15:19.894787: step 411, loss 9.83926, acc 0.734375, prec 0.025207, recall 0.711286
2017-12-10T13:15:20.343264: step 412, loss 1.73253, acc 0.65625, prec 0.0251555, recall 0.711286
2017-12-10T13:15:20.781190: step 413, loss 3.24117, acc 0.703125, prec 0.0252942, recall 0.710938
2017-12-10T13:15:21.249342: step 414, loss 4.54012, acc 0.703125, prec 0.0253422, recall 0.709845
2017-12-10T13:15:21.702952: step 415, loss 1.17932, acc 0.734375, prec 0.0253924, recall 0.710594
2017-12-10T13:15:22.156049: step 416, loss 1.90066, acc 0.625, prec 0.0255158, recall 0.712082
2017-12-10T13:15:22.613560: step 417, loss 2.69353, acc 0.46875, prec 0.0254362, recall 0.712082
2017-12-10T13:15:23.062118: step 418, loss 10.7998, acc 0.546875, prec 0.0253709, recall 0.710256
2017-12-10T13:15:23.494560: step 419, loss 1.9857, acc 0.59375, prec 0.0253997, recall 0.710997
2017-12-10T13:15:23.918544: step 420, loss 2.62204, acc 0.578125, prec 0.0256924, recall 0.713924
2017-12-10T13:15:24.353833: step 421, loss 3.54527, acc 0.46875, prec 0.0256131, recall 0.713924
2017-12-10T13:15:24.788432: step 422, loss 3.08254, acc 0.5, prec 0.0256271, recall 0.714646
2017-12-10T13:15:25.237228: step 423, loss 4.36797, acc 0.484375, prec 0.025729, recall 0.714286
2017-12-10T13:15:25.679319: step 424, loss 2.44735, acc 0.5625, prec 0.0257518, recall 0.715
2017-12-10T13:15:26.126511: step 425, loss 3.16838, acc 0.546875, prec 0.0257723, recall 0.715711
2017-12-10T13:15:26.578569: step 426, loss 2.90548, acc 0.546875, prec 0.0257926, recall 0.716418
2017-12-10T13:15:27.028846: step 427, loss 4.31539, acc 0.40625, prec 0.0257921, recall 0.717122
2017-12-10T13:15:27.476097: step 428, loss 2.1096, acc 0.59375, prec 0.0257323, recall 0.717122
2017-12-10T13:15:27.934779: step 429, loss 3.17508, acc 0.546875, prec 0.0256661, recall 0.717122
2017-12-10T13:15:28.378510: step 430, loss 2.35395, acc 0.5625, prec 0.0256024, recall 0.717122
2017-12-10T13:15:28.811647: step 431, loss 2.17142, acc 0.484375, prec 0.0256138, recall 0.717822
2017-12-10T13:15:29.237367: step 432, loss 1.86573, acc 0.625, prec 0.0255597, recall 0.717822
2017-12-10T13:15:29.691988: step 433, loss 1.83097, acc 0.640625, prec 0.0256794, recall 0.719212
2017-12-10T13:15:30.135827: step 434, loss 0.764663, acc 0.75, prec 0.0257288, recall 0.719902
2017-12-10T13:15:30.579764: step 435, loss 0.867105, acc 0.796875, prec 0.025785, recall 0.720588
2017-12-10T13:15:31.017505: step 436, loss 0.92274, acc 0.828125, prec 0.0257601, recall 0.720588
2017-12-10T13:15:31.465304: step 437, loss 6.17855, acc 0.796875, prec 0.0258183, recall 0.719512
2017-12-10T13:15:31.924203: step 438, loss 2.07323, acc 0.65625, prec 0.0257687, recall 0.719512
2017-12-10T13:15:32.369193: step 439, loss 6.25663, acc 0.84375, prec 0.0257485, recall 0.717762
2017-12-10T13:15:32.805530: step 440, loss 0.795237, acc 0.875, prec 0.0258155, recall 0.718447
2017-12-10T13:15:33.237459: step 441, loss 0.663827, acc 0.859375, prec 0.0257952, recall 0.718447
2017-12-10T13:15:33.677679: step 442, loss 2.48075, acc 0.671875, prec 0.0257503, recall 0.716707
2017-12-10T13:15:34.134715: step 443, loss 1.28823, acc 0.75, prec 0.0257145, recall 0.716707
2017-12-10T13:15:34.590482: step 444, loss 8.57981, acc 0.765625, prec 0.0256833, recall 0.714976
2017-12-10T13:15:35.033998: step 445, loss 1.19383, acc 0.75, prec 0.0256477, recall 0.714976
2017-12-10T13:15:35.469893: step 446, loss 10.446, acc 0.6875, prec 0.0256055, recall 0.713253
2017-12-10T13:15:35.946073: step 447, loss 6.36269, acc 0.671875, prec 0.0257296, recall 0.712919
2017-12-10T13:15:36.390227: step 448, loss 1.6837, acc 0.65625, prec 0.0258487, recall 0.714286
2017-12-10T13:15:36.844840: step 449, loss 2.39918, acc 0.4375, prec 0.0257688, recall 0.714286
2017-12-10T13:15:37.276646: step 450, loss 2.16472, acc 0.671875, prec 0.0258059, recall 0.714964
2017-12-10T13:15:37.722785: step 451, loss 2.44082, acc 0.53125, prec 0.025823, recall 0.71564
2017-12-10T13:15:38.159611: step 452, loss 3.32157, acc 0.46875, prec 0.0258312, recall 0.716312
2017-12-10T13:15:38.605120: step 453, loss 2.96778, acc 0.46875, prec 0.0258394, recall 0.716981
2017-12-10T13:15:39.033822: step 454, loss 1.59033, acc 0.625, prec 0.0260346, recall 0.71897
2017-12-10T13:15:39.477206: step 455, loss 2.44754, acc 0.59375, prec 0.0260597, recall 0.719626
2017-12-10T13:15:39.926524: step 456, loss 4.37004, acc 0.484375, prec 0.0260715, recall 0.718605
2017-12-10T13:15:40.360614: step 457, loss 2.22931, acc 0.5625, prec 0.0260101, recall 0.718605
2017-12-10T13:15:40.811701: step 458, loss 6.91644, acc 0.578125, prec 0.0259533, recall 0.716937
2017-12-10T13:15:41.262784: step 459, loss 2.51101, acc 0.578125, prec 0.0259762, recall 0.717593
2017-12-10T13:15:41.704093: step 460, loss 1.80038, acc 0.6875, prec 0.0259327, recall 0.717593
2017-12-10T13:15:42.161452: step 461, loss 2.8797, acc 0.515625, prec 0.0259469, recall 0.718245
2017-12-10T13:15:42.596926: step 462, loss 1.76293, acc 0.5625, prec 0.0258865, recall 0.718245
2017-12-10T13:15:43.042051: step 463, loss 2.82528, acc 0.5625, prec 0.0259072, recall 0.718894
2017-12-10T13:15:43.496410: step 464, loss 1.68687, acc 0.703125, prec 0.0261086, recall 0.720824
2017-12-10T13:15:43.939181: step 465, loss 8.71967, acc 0.65625, prec 0.0260632, recall 0.719178
2017-12-10T13:15:44.396431: step 466, loss 1.94684, acc 0.625, prec 0.0261724, recall 0.720455
2017-12-10T13:15:44.837541: step 467, loss 1.93391, acc 0.53125, prec 0.0262681, recall 0.721719
2017-12-10T13:15:45.284331: step 468, loss 1.38328, acc 0.734375, prec 0.0263915, recall 0.722973
2017-12-10T13:15:45.717239: step 469, loss 1.44121, acc 0.65625, prec 0.0264238, recall 0.723595
2017-12-10T13:15:46.162332: step 470, loss 2.60449, acc 0.640625, prec 0.0264537, recall 0.724215
2017-12-10T13:15:46.617389: step 471, loss 1.57917, acc 0.640625, prec 0.026404, recall 0.724215
2017-12-10T13:15:47.063837: step 472, loss 1.18079, acc 0.734375, prec 0.0264468, recall 0.724832
2017-12-10T13:15:47.497854: step 473, loss 0.720194, acc 0.796875, prec 0.0264188, recall 0.724832
2017-12-10T13:15:47.934484: step 474, loss 1.18454, acc 0.703125, prec 0.0263779, recall 0.724832
2017-12-10T13:15:48.372950: step 475, loss 0.948407, acc 0.796875, prec 0.0264292, recall 0.725446
2017-12-10T13:15:48.820797: step 476, loss 1.19332, acc 0.71875, prec 0.0264696, recall 0.726058
2017-12-10T13:15:49.279462: step 477, loss 0.568037, acc 0.828125, prec 0.026446, recall 0.726058
2017-12-10T13:15:49.731697: step 478, loss 0.837097, acc 0.8125, prec 0.0264203, recall 0.726058
2017-12-10T13:15:50.174561: step 479, loss 0.364032, acc 0.890625, prec 0.0264053, recall 0.726058
2017-12-10T13:15:50.617157: step 480, loss 0.751597, acc 0.828125, prec 0.0263818, recall 0.726058
2017-12-10T13:15:51.054326: step 481, loss 0.183558, acc 0.921875, prec 0.0263711, recall 0.726058
2017-12-10T13:15:51.495806: step 482, loss 0.352895, acc 0.90625, prec 0.0264371, recall 0.726667
2017-12-10T13:15:51.940852: step 483, loss 0.246165, acc 0.9375, prec 0.0264285, recall 0.726667
2017-12-10T13:15:52.389738: step 484, loss 1.40244, acc 0.859375, prec 0.0265665, recall 0.727876
2017-12-10T13:15:52.838191: step 485, loss 23.8522, acc 0.875, prec 0.0265558, recall 0.723077
2017-12-10T13:15:53.284306: step 486, loss 0.444048, acc 0.890625, prec 0.0265408, recall 0.723077
2017-12-10T13:15:53.736424: step 487, loss 0.122951, acc 0.953125, prec 0.0265344, recall 0.723077
2017-12-10T13:15:54.174178: step 488, loss 0.471292, acc 0.796875, prec 0.026585, recall 0.723684
2017-12-10T13:15:54.641962: step 489, loss 0.55605, acc 0.8125, prec 0.026716, recall 0.724891
2017-12-10T13:15:55.076156: step 490, loss 1.25064, acc 0.703125, prec 0.0267534, recall 0.72549
2017-12-10T13:15:55.532439: step 491, loss 4.5642, acc 0.703125, prec 0.0267148, recall 0.723913
2017-12-10T13:15:55.974770: step 492, loss 1.12103, acc 0.84375, prec 0.0267714, recall 0.724512
2017-12-10T13:15:56.431447: step 493, loss 0.625836, acc 0.84375, prec 0.02675, recall 0.724512
2017-12-10T13:15:56.880478: step 494, loss 1.49518, acc 0.703125, prec 0.0267093, recall 0.724512
2017-12-10T13:15:57.330125: step 495, loss 1.44325, acc 0.703125, prec 0.0267465, recall 0.725108
2017-12-10T13:15:57.773363: step 496, loss 2.48523, acc 0.53125, prec 0.0267601, recall 0.725702
2017-12-10T13:15:58.175263: step 497, loss 1.60366, acc 0.615385, prec 0.0267176, recall 0.725702
2017-12-10T13:15:58.625443: step 498, loss 1.52639, acc 0.6875, prec 0.0268297, recall 0.726882
2017-12-10T13:15:59.072185: step 499, loss 1.41243, acc 0.71875, prec 0.0268685, recall 0.727468
2017-12-10T13:15:59.527088: step 500, loss 1.08131, acc 0.671875, prec 0.0268239, recall 0.727468
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-500

2017-12-10T13:16:01.304487: step 501, loss 1.78992, acc 0.671875, prec 0.0267794, recall 0.727468
2017-12-10T13:16:01.744195: step 502, loss 1.36231, acc 0.671875, prec 0.0268118, recall 0.728051
2017-12-10T13:16:02.176513: step 503, loss 0.608249, acc 0.859375, prec 0.0267927, recall 0.728051
2017-12-10T13:16:02.618120: step 504, loss 1.48653, acc 0.75, prec 0.0268356, recall 0.728632
2017-12-10T13:16:03.073620: step 505, loss 1.56128, acc 0.65625, prec 0.0268657, recall 0.729211
2017-12-10T13:16:03.515439: step 506, loss 1.05483, acc 0.78125, prec 0.0268362, recall 0.729211
2017-12-10T13:16:03.964755: step 507, loss 0.965456, acc 0.765625, prec 0.0268809, recall 0.729787
2017-12-10T13:16:04.413349: step 508, loss 0.641115, acc 0.796875, prec 0.0269297, recall 0.730361
2017-12-10T13:16:04.863199: step 509, loss 0.485985, acc 0.859375, prec 0.027063, recall 0.731501
2017-12-10T13:16:05.307114: step 510, loss 0.880462, acc 0.8125, prec 0.0271896, recall 0.732632
2017-12-10T13:16:05.759839: step 511, loss 0.552635, acc 0.859375, prec 0.0271705, recall 0.732632
2017-12-10T13:16:06.197669: step 512, loss 0.287305, acc 0.90625, prec 0.0271578, recall 0.732632
2017-12-10T13:16:06.648011: step 513, loss 0.342158, acc 0.90625, prec 0.0274485, recall 0.734864
2017-12-10T13:16:07.113438: step 514, loss 0.222103, acc 0.921875, prec 0.0274378, recall 0.734864
2017-12-10T13:16:07.569009: step 515, loss 3.19346, acc 0.90625, prec 0.0275787, recall 0.73444
2017-12-10T13:16:08.025045: step 516, loss 0.501786, acc 0.875, prec 0.0275615, recall 0.73444
2017-12-10T13:16:08.477220: step 517, loss 0.231614, acc 0.90625, prec 0.0275486, recall 0.73444
2017-12-10T13:16:08.929682: step 518, loss 0.323148, acc 0.90625, prec 0.0275358, recall 0.73444
2017-12-10T13:16:09.374618: step 519, loss 0.241539, acc 0.921875, prec 0.0275251, recall 0.73444
2017-12-10T13:16:09.820960: step 520, loss 2.31087, acc 0.875, prec 0.0275101, recall 0.732919
2017-12-10T13:16:10.270791: step 521, loss 0.162677, acc 0.953125, prec 0.0275037, recall 0.732919
2017-12-10T13:16:10.724619: step 522, loss 8.82987, acc 0.890625, prec 0.0275664, recall 0.731959
2017-12-10T13:16:11.192975: step 523, loss 0.455746, acc 0.90625, prec 0.0275536, recall 0.731959
2017-12-10T13:16:11.637368: step 524, loss 0.285289, acc 0.890625, prec 0.0275386, recall 0.731959
2017-12-10T13:16:12.061753: step 525, loss 2.0013, acc 0.828125, prec 0.0275172, recall 0.730453
2017-12-10T13:16:12.497106: step 526, loss 0.467322, acc 0.875, prec 0.0276508, recall 0.731557
2017-12-10T13:16:12.936064: step 527, loss 0.72845, acc 0.84375, prec 0.0276294, recall 0.731557
2017-12-10T13:16:13.378624: step 528, loss 0.68431, acc 0.765625, prec 0.0277477, recall 0.732653
2017-12-10T13:16:13.830614: step 529, loss 0.634996, acc 0.859375, prec 0.0278035, recall 0.733198
2017-12-10T13:16:14.272933: step 530, loss 0.684058, acc 0.859375, prec 0.0278592, recall 0.73374
2017-12-10T13:16:14.724203: step 531, loss 0.474322, acc 0.859375, prec 0.0279149, recall 0.73428
2017-12-10T13:16:15.192963: step 532, loss 13.7631, acc 0.78125, prec 0.0278869, recall 0.732794
2017-12-10T13:16:15.629229: step 533, loss 1.37266, acc 0.71875, prec 0.0278483, recall 0.732794
2017-12-10T13:16:16.083201: step 534, loss 0.941963, acc 0.8125, prec 0.0280467, recall 0.734406
2017-12-10T13:16:16.533951: step 535, loss 10.7481, acc 0.703125, prec 0.0280826, recall 0.733467
2017-12-10T13:16:16.973646: step 536, loss 2.34146, acc 0.734375, prec 0.0281949, recall 0.734531
2017-12-10T13:16:17.417171: step 537, loss 2.12188, acc 0.5625, prec 0.0282089, recall 0.73506
2017-12-10T13:16:17.864521: step 538, loss 2.52671, acc 0.640625, prec 0.0283076, recall 0.736111
2017-12-10T13:16:18.310144: step 539, loss 1.19996, acc 0.640625, prec 0.0284061, recall 0.737154
2017-12-10T13:16:18.748494: step 540, loss 1.79462, acc 0.578125, prec 0.0284216, recall 0.737673
2017-12-10T13:16:19.209573: step 541, loss 1.8956, acc 0.625, prec 0.0285173, recall 0.738703
2017-12-10T13:16:19.647628: step 542, loss 2.05526, acc 0.640625, prec 0.0284676, recall 0.738703
2017-12-10T13:16:20.095406: step 543, loss 16.2372, acc 0.6875, prec 0.0287938, recall 0.739806
2017-12-10T13:16:20.541304: step 544, loss 2.43774, acc 0.546875, prec 0.0288041, recall 0.74031
2017-12-10T13:16:20.984661: step 545, loss 2.30195, acc 0.53125, prec 0.0287391, recall 0.74031
2017-12-10T13:16:21.425855: step 546, loss 2.6363, acc 0.5625, prec 0.0287516, recall 0.740812
2017-12-10T13:16:21.864898: step 547, loss 2.67125, acc 0.546875, prec 0.0286891, recall 0.740812
2017-12-10T13:16:22.307982: step 548, loss 4.73078, acc 0.671875, prec 0.0287915, recall 0.740385
2017-12-10T13:16:22.751683: step 549, loss 1.91656, acc 0.5, prec 0.0287952, recall 0.740883
2017-12-10T13:16:23.201603: step 550, loss 1.95326, acc 0.5625, prec 0.0288798, recall 0.741874
2017-12-10T13:16:23.641774: step 551, loss 2.44742, acc 0.609375, prec 0.0288262, recall 0.741874
2017-12-10T13:16:24.076316: step 552, loss 2.72371, acc 0.5, prec 0.0287578, recall 0.741874
2017-12-10T13:16:24.511949: step 553, loss 1.63805, acc 0.71875, prec 0.0287195, recall 0.741874
2017-12-10T13:16:24.964967: step 554, loss 0.960711, acc 0.765625, prec 0.0287594, recall 0.742366
2017-12-10T13:16:25.414393: step 555, loss 2.23573, acc 0.640625, prec 0.0287106, recall 0.742366
2017-12-10T13:16:25.851310: step 556, loss 1.47452, acc 0.734375, prec 0.0288178, recall 0.743346
2017-12-10T13:16:26.297182: step 557, loss 0.694377, acc 0.84375, prec 0.0288681, recall 0.743833
2017-12-10T13:16:26.753890: step 558, loss 5.36662, acc 0.71875, prec 0.0289034, recall 0.742911
2017-12-10T13:16:27.197313: step 559, loss 1.09056, acc 0.71875, prec 0.0289365, recall 0.743396
2017-12-10T13:16:27.639737: step 560, loss 1.24698, acc 0.796875, prec 0.0291227, recall 0.744841
2017-12-10T13:16:28.078909: step 561, loss 1.35497, acc 0.78125, prec 0.0291639, recall 0.745318
2017-12-10T13:16:28.524186: step 562, loss 1.15543, acc 0.78125, prec 0.0292051, recall 0.745794
2017-12-10T13:16:28.973365: step 563, loss 0.987116, acc 0.8125, prec 0.0292505, recall 0.746269
2017-12-10T13:16:29.413068: step 564, loss 0.481155, acc 0.859375, prec 0.0292312, recall 0.746269
2017-12-10T13:16:29.862738: step 565, loss 0.594921, acc 0.84375, prec 0.0292808, recall 0.746741
2017-12-10T13:16:30.330903: step 566, loss 0.967688, acc 0.8125, prec 0.0293259, recall 0.747212
2017-12-10T13:16:30.774458: step 567, loss 0.566247, acc 0.84375, prec 0.0293046, recall 0.747212
2017-12-10T13:16:31.212439: step 568, loss 0.509089, acc 0.90625, prec 0.0292918, recall 0.747212
2017-12-10T13:16:31.648862: step 569, loss 7.85725, acc 0.8125, prec 0.0292683, recall 0.745826
2017-12-10T13:16:32.106630: step 570, loss 0.662887, acc 0.8125, prec 0.0293134, recall 0.746296
2017-12-10T13:16:32.558549: step 571, loss 0.374908, acc 0.875, prec 0.0292963, recall 0.746296
2017-12-10T13:16:33.008056: step 572, loss 0.314725, acc 0.875, prec 0.0292793, recall 0.746296
2017-12-10T13:16:33.459039: step 573, loss 0.584791, acc 0.84375, prec 0.029258, recall 0.746296
2017-12-10T13:16:33.900697: step 574, loss 0.506964, acc 0.875, prec 0.0293819, recall 0.747232
2017-12-10T13:16:34.353718: step 575, loss 0.510473, acc 0.84375, prec 0.0293606, recall 0.747232
2017-12-10T13:16:34.790219: step 576, loss 1.85877, acc 0.875, prec 0.029416, recall 0.746324
2017-12-10T13:16:35.243423: step 577, loss 0.76242, acc 0.796875, prec 0.0294586, recall 0.746789
2017-12-10T13:16:35.695146: step 578, loss 0.343547, acc 0.921875, prec 0.0294479, recall 0.746789
2017-12-10T13:16:36.149956: step 579, loss 4.58258, acc 0.90625, prec 0.0295075, recall 0.745887
2017-12-10T13:16:36.618386: step 580, loss 1.46297, acc 0.875, prec 0.0296307, recall 0.746812
2017-12-10T13:16:37.070153: step 581, loss 10.0823, acc 0.796875, prec 0.029605, recall 0.745455
2017-12-10T13:16:37.529452: step 582, loss 0.457028, acc 0.828125, prec 0.0295815, recall 0.745455
2017-12-10T13:16:37.980783: step 583, loss 0.716694, acc 0.84375, prec 0.0295602, recall 0.745455
2017-12-10T13:16:38.435813: step 584, loss 0.783371, acc 0.84375, prec 0.0295389, recall 0.745455
2017-12-10T13:16:38.864848: step 585, loss 0.509614, acc 0.875, prec 0.0295918, recall 0.745916
2017-12-10T13:16:39.311115: step 586, loss 1.67141, acc 0.796875, prec 0.0298432, recall 0.747748
2017-12-10T13:16:39.776809: step 587, loss 0.970177, acc 0.734375, prec 0.0298765, recall 0.748201
2017-12-10T13:16:40.226708: step 588, loss 1.11974, acc 0.671875, prec 0.0298315, recall 0.748201
2017-12-10T13:16:40.661892: step 589, loss 1.62594, acc 0.671875, prec 0.0297866, recall 0.748201
2017-12-10T13:16:41.099075: step 590, loss 10.7591, acc 0.671875, prec 0.0298828, recall 0.747764
2017-12-10T13:16:41.534078: step 591, loss 1.40588, acc 0.625, prec 0.0298316, recall 0.747764
2017-12-10T13:16:41.987328: step 592, loss 1.78344, acc 0.734375, prec 0.0297954, recall 0.747764
2017-12-10T13:16:42.432155: step 593, loss 2.32487, acc 0.734375, prec 0.0298975, recall 0.748663
2017-12-10T13:16:42.875074: step 594, loss 1.30697, acc 0.71875, prec 0.0298592, recall 0.748663
2017-12-10T13:16:43.319493: step 595, loss 1.6779, acc 0.6875, prec 0.0298857, recall 0.74911
2017-12-10T13:16:43.767336: step 596, loss 1.88478, acc 0.625, prec 0.0298349, recall 0.74911
2017-12-10T13:16:44.203918: step 597, loss 2.87087, acc 0.703125, prec 0.0297969, recall 0.74778
2017-12-10T13:16:44.672689: step 598, loss 1.95499, acc 0.546875, prec 0.0298044, recall 0.748227
2017-12-10T13:16:45.116091: step 599, loss 1.91298, acc 0.578125, prec 0.0297476, recall 0.748227
2017-12-10T13:16:45.571074: step 600, loss 1.23217, acc 0.6875, prec 0.0297058, recall 0.748227
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-600

2017-12-10T13:16:47.582953: step 601, loss 0.971969, acc 0.703125, prec 0.0296661, recall 0.748227
2017-12-10T13:16:48.025289: step 602, loss 1.44263, acc 0.75, prec 0.0296328, recall 0.748227
2017-12-10T13:16:48.473532: step 603, loss 1.07139, acc 0.75, prec 0.0295995, recall 0.748227
2017-12-10T13:16:48.917581: step 604, loss 1.00902, acc 0.8125, prec 0.0295746, recall 0.748227
2017-12-10T13:16:49.371655: step 605, loss 0.989075, acc 0.828125, prec 0.0296198, recall 0.748673
2017-12-10T13:16:49.812966: step 606, loss 0.818815, acc 0.765625, prec 0.0295887, recall 0.748673
2017-12-10T13:16:50.262098: step 607, loss 0.470303, acc 0.875, prec 0.02964, recall 0.749117
2017-12-10T13:16:50.713703: step 608, loss 0.945989, acc 0.78125, prec 0.029611, recall 0.749117
2017-12-10T13:16:51.165041: step 609, loss 0.935363, acc 0.796875, prec 0.0295841, recall 0.749117
2017-12-10T13:16:51.602602: step 610, loss 0.622434, acc 0.875, prec 0.0295676, recall 0.749117
2017-12-10T13:16:52.056052: step 611, loss 7.74684, acc 0.9375, prec 0.0295615, recall 0.747795
2017-12-10T13:16:52.510748: step 612, loss 2.08907, acc 0.84375, prec 0.0295429, recall 0.746479
2017-12-10T13:16:52.968914: step 613, loss 0.931607, acc 0.890625, prec 0.0297312, recall 0.747811
2017-12-10T13:16:53.412376: step 614, loss 0.595696, acc 0.8125, prec 0.0297064, recall 0.747811
2017-12-10T13:16:53.848060: step 615, loss 0.361644, acc 0.84375, prec 0.0296858, recall 0.747811
2017-12-10T13:16:54.297694: step 616, loss 1.10599, acc 0.78125, prec 0.0297243, recall 0.748252
2017-12-10T13:16:54.739184: step 617, loss 7.78397, acc 0.796875, prec 0.0297669, recall 0.747387
2017-12-10T13:16:55.180958: step 618, loss 5.25955, acc 0.828125, prec 0.0297462, recall 0.746087
2017-12-10T13:16:55.625177: step 619, loss 0.752744, acc 0.84375, prec 0.0298601, recall 0.746967
2017-12-10T13:16:56.085481: step 620, loss 0.974749, acc 0.71875, prec 0.0299571, recall 0.747841
2017-12-10T13:16:56.530863: step 621, loss 1.03736, acc 0.828125, prec 0.0300014, recall 0.748276
2017-12-10T13:16:56.978068: step 622, loss 1.40166, acc 0.703125, prec 0.029962, recall 0.748276
2017-12-10T13:16:57.419732: step 623, loss 13.6775, acc 0.65625, prec 0.0299855, recall 0.747423
2017-12-10T13:16:57.854152: step 624, loss 1.50213, acc 0.703125, prec 0.0300131, recall 0.747856
2017-12-10T13:16:58.297828: step 625, loss 2.28874, acc 0.59375, prec 0.0300261, recall 0.748288
2017-12-10T13:16:58.751996: step 626, loss 1.68063, acc 0.625, prec 0.0301097, recall 0.749147
2017-12-10T13:16:59.189336: step 627, loss 1.19378, acc 0.703125, prec 0.0300706, recall 0.749147
2017-12-10T13:16:59.628117: step 628, loss 1.77555, acc 0.71875, prec 0.0301662, recall 0.75
2017-12-10T13:17:00.092969: step 629, loss 3.00361, acc 0.5, prec 0.0301665, recall 0.750424
2017-12-10T13:17:00.538688: step 630, loss 1.38519, acc 0.703125, prec 0.0301275, recall 0.750424
2017-12-10T13:17:00.995790: step 631, loss 2.10898, acc 0.578125, prec 0.0300721, recall 0.750424
2017-12-10T13:17:01.445433: step 632, loss 1.71394, acc 0.640625, prec 0.030091, recall 0.750847
2017-12-10T13:17:01.888871: step 633, loss 1.69473, acc 0.65625, prec 0.0301119, recall 0.751269
2017-12-10T13:17:02.330882: step 634, loss 1.39463, acc 0.734375, prec 0.0301429, recall 0.751689
2017-12-10T13:17:02.778062: step 635, loss 0.95415, acc 0.765625, prec 0.030178, recall 0.752108
2017-12-10T13:17:03.218260: step 636, loss 1.03877, acc 0.78125, prec 0.0301494, recall 0.752108
2017-12-10T13:17:03.655148: step 637, loss 0.861375, acc 0.796875, prec 0.0301884, recall 0.752525
2017-12-10T13:17:04.103155: step 638, loss 1.12639, acc 0.75, prec 0.0301558, recall 0.752525
2017-12-10T13:17:04.543391: step 639, loss 0.96252, acc 0.8125, prec 0.0301314, recall 0.752525
2017-12-10T13:17:04.992814: step 640, loss 0.553595, acc 0.859375, prec 0.0301132, recall 0.752525
2017-12-10T13:17:05.444053: step 641, loss 8.00171, acc 0.828125, prec 0.0300929, recall 0.751261
2017-12-10T13:17:05.896131: step 642, loss 0.430044, acc 0.875, prec 0.0302072, recall 0.752094
2017-12-10T13:17:06.341195: step 643, loss 0.479157, acc 0.890625, prec 0.030193, recall 0.752094
2017-12-10T13:17:06.783512: step 644, loss 0.487507, acc 0.859375, prec 0.0303051, recall 0.752922
2017-12-10T13:17:07.222682: step 645, loss 0.609849, acc 0.84375, prec 0.0303498, recall 0.753333
2017-12-10T13:17:07.694967: step 646, loss 0.169922, acc 0.9375, prec 0.0303417, recall 0.753333
2017-12-10T13:17:08.132505: step 647, loss 0.427293, acc 0.90625, prec 0.0303295, recall 0.753333
2017-12-10T13:17:08.574985: step 648, loss 0.217184, acc 0.96875, prec 0.0303254, recall 0.753333
2017-12-10T13:17:09.019889: step 649, loss 16.4335, acc 0.84375, prec 0.0303071, recall 0.75208
2017-12-10T13:17:09.468127: step 650, loss 1.06153, acc 0.78125, prec 0.0302787, recall 0.75208
2017-12-10T13:17:09.916533: step 651, loss 0.372837, acc 0.890625, prec 0.0302645, recall 0.75208
2017-12-10T13:17:10.362621: step 652, loss 0.533244, acc 0.890625, prec 0.0303152, recall 0.752492
2017-12-10T13:17:10.814919: step 653, loss 0.813098, acc 0.84375, prec 0.0303598, recall 0.752902
2017-12-10T13:17:11.259747: step 654, loss 0.599105, acc 0.859375, prec 0.0303415, recall 0.752902
2017-12-10T13:17:11.696641: step 655, loss 0.640334, acc 0.859375, prec 0.0303233, recall 0.752902
2017-12-10T13:17:12.141144: step 656, loss 0.846694, acc 0.828125, prec 0.030301, recall 0.752902
2017-12-10T13:17:12.585856: step 657, loss 0.942693, acc 0.765625, prec 0.0303354, recall 0.753311
2017-12-10T13:17:13.034733: step 658, loss 0.449072, acc 0.875, prec 0.0304484, recall 0.754125
2017-12-10T13:17:13.473222: step 659, loss 2.76043, acc 0.8125, prec 0.0304906, recall 0.753289
2017-12-10T13:17:13.920608: step 660, loss 0.611428, acc 0.875, prec 0.0304744, recall 0.753289
2017-12-10T13:17:14.370897: step 661, loss 0.946732, acc 0.796875, prec 0.030577, recall 0.754098
2017-12-10T13:17:14.813119: step 662, loss 1.58269, acc 0.828125, prec 0.0306211, recall 0.753268
2017-12-10T13:17:15.252004: step 663, loss 12.2497, acc 0.828125, prec 0.0306651, recall 0.752443
2017-12-10T13:17:15.702817: step 664, loss 2.99043, acc 0.828125, prec 0.0307733, recall 0.752026
2017-12-10T13:17:16.149585: step 665, loss 0.819855, acc 0.8125, prec 0.0308131, recall 0.752427
2017-12-10T13:17:16.584231: step 666, loss 1.78654, acc 0.75, prec 0.0308446, recall 0.752827
2017-12-10T13:17:17.018584: step 667, loss 1.29423, acc 0.703125, prec 0.0308699, recall 0.753226
2017-12-10T13:17:17.465781: step 668, loss 0.905758, acc 0.75, prec 0.0308373, recall 0.753226
2017-12-10T13:17:17.927514: step 669, loss 11.9454, acc 0.578125, prec 0.0307844, recall 0.752013
2017-12-10T13:17:18.379727: step 670, loss 2.26115, acc 0.5625, prec 0.0307915, recall 0.752412
2017-12-10T13:17:18.817216: step 671, loss 2.23674, acc 0.59375, prec 0.0307389, recall 0.752412
2017-12-10T13:17:19.259827: step 672, loss 2.48579, acc 0.546875, prec 0.030744, recall 0.752809
2017-12-10T13:17:19.704327: step 673, loss 2.37613, acc 0.609375, prec 0.0307571, recall 0.753205
2017-12-10T13:17:20.155367: step 674, loss 2.19214, acc 0.59375, prec 0.0307049, recall 0.753205
2017-12-10T13:17:20.594808: step 675, loss 3.59033, acc 0.421875, prec 0.0306309, recall 0.753205
2017-12-10T13:17:21.041700: step 676, loss 2.47376, acc 0.5625, prec 0.0307642, recall 0.754386
2017-12-10T13:17:21.482171: step 677, loss 2.65257, acc 0.609375, prec 0.0307143, recall 0.754386
2017-12-10T13:17:21.931857: step 678, loss 2.04663, acc 0.546875, prec 0.0307194, recall 0.754777
2017-12-10T13:17:22.366895: step 679, loss 1.96322, acc 0.625, prec 0.0307971, recall 0.755556
2017-12-10T13:17:22.815613: step 680, loss 2.1861, acc 0.625, prec 0.030812, recall 0.755943
2017-12-10T13:17:23.264683: step 681, loss 1.32543, acc 0.75, prec 0.0309052, recall 0.756714
2017-12-10T13:17:23.698418: step 682, loss 1.95477, acc 0.625, prec 0.0310447, recall 0.757862
2017-12-10T13:17:24.152777: step 683, loss 1.76328, acc 0.71875, prec 0.0310088, recall 0.757862
2017-12-10T13:17:24.598114: step 684, loss 4.26122, acc 0.625, prec 0.0311496, recall 0.757812
2017-12-10T13:17:25.050217: step 685, loss 0.940083, acc 0.78125, prec 0.031246, recall 0.758567
2017-12-10T13:17:25.494065: step 686, loss 0.951153, acc 0.84375, prec 0.0314123, recall 0.75969
2017-12-10T13:17:25.942101: step 687, loss 1.05124, acc 0.78125, prec 0.0313841, recall 0.75969
2017-12-10T13:17:26.386555: step 688, loss 3.16036, acc 0.734375, prec 0.031352, recall 0.758514
2017-12-10T13:17:26.831307: step 689, loss 1.14409, acc 0.78125, prec 0.0313239, recall 0.758514
2017-12-10T13:17:27.273940: step 690, loss 0.74046, acc 0.78125, prec 0.0313578, recall 0.758887
2017-12-10T13:17:27.715566: step 691, loss 1.20392, acc 0.6875, prec 0.0313796, recall 0.759259
2017-12-10T13:17:28.159586: step 692, loss 0.577299, acc 0.78125, prec 0.0314133, recall 0.75963
2017-12-10T13:17:28.601730: step 693, loss 1.57625, acc 0.859375, prec 0.0315186, recall 0.760369
2017-12-10T13:17:29.032564: step 694, loss 0.736781, acc 0.78125, prec 0.0315522, recall 0.760736
2017-12-10T13:17:29.486676: step 695, loss 0.890056, acc 0.84375, prec 0.0316552, recall 0.761468
2017-12-10T13:17:29.930736: step 696, loss 4.90369, acc 0.859375, prec 0.0316391, recall 0.760305
2017-12-10T13:17:30.385514: step 697, loss 0.555858, acc 0.859375, prec 0.0316825, recall 0.760671
2017-12-10T13:17:30.837977: step 698, loss 18.016, acc 0.890625, prec 0.0317934, recall 0.760243
2017-12-10T13:17:31.288827: step 699, loss 0.867964, acc 0.78125, prec 0.0317652, recall 0.760243
2017-12-10T13:17:31.729494: step 700, loss 0.900582, acc 0.78125, prec 0.031737, recall 0.760243
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-700

2017-12-10T13:17:33.921962: step 701, loss 1.29039, acc 0.71875, prec 0.0318234, recall 0.760968
2017-12-10T13:17:34.372815: step 702, loss 1.83022, acc 0.65625, prec 0.0318403, recall 0.761329
2017-12-10T13:17:34.819619: step 703, loss 3.61725, acc 0.625, prec 0.0317941, recall 0.760181
2017-12-10T13:17:35.277846: step 704, loss 1.58022, acc 0.75, prec 0.0318231, recall 0.760542
2017-12-10T13:17:35.728438: step 705, loss 1.0265, acc 0.734375, prec 0.031789, recall 0.760542
2017-12-10T13:17:36.177598: step 706, loss 1.71585, acc 0.640625, prec 0.0319256, recall 0.761619
2017-12-10T13:17:36.616553: step 707, loss 2.01519, acc 0.6875, prec 0.0318855, recall 0.761619
2017-12-10T13:17:37.062482: step 708, loss 3.15737, acc 0.59375, prec 0.0318356, recall 0.760479
2017-12-10T13:17:37.505647: step 709, loss 2.70509, acc 0.71875, prec 0.0318623, recall 0.759701
2017-12-10T13:17:37.962545: step 710, loss 1.39713, acc 0.671875, prec 0.032002, recall 0.760773
2017-12-10T13:17:38.434053: step 711, loss 4.56889, acc 0.640625, prec 0.0320185, recall 0.76
2017-12-10T13:17:38.866324: step 712, loss 1.5515, acc 0.65625, prec 0.0319746, recall 0.76
2017-12-10T13:17:39.316884: step 713, loss 2.37178, acc 0.59375, prec 0.0321035, recall 0.761062
2017-12-10T13:17:39.773992: step 714, loss 2.10162, acc 0.578125, prec 0.0321699, recall 0.761765
2017-12-10T13:17:40.220345: step 715, loss 1.52944, acc 0.65625, prec 0.032186, recall 0.762115
2017-12-10T13:17:40.657210: step 716, loss 1.57989, acc 0.609375, prec 0.0322561, recall 0.762811
2017-12-10T13:17:41.108125: step 717, loss 1.95267, acc 0.59375, prec 0.0323239, recall 0.763504
2017-12-10T13:17:41.568846: step 718, loss 2.38147, acc 0.5625, prec 0.032268, recall 0.763504
2017-12-10T13:17:42.000352: step 719, loss 1.74867, acc 0.625, prec 0.0322799, recall 0.763848
2017-12-10T13:17:42.447038: step 720, loss 1.64588, acc 0.671875, prec 0.0322978, recall 0.764192
2017-12-10T13:17:42.900295: step 721, loss 0.903223, acc 0.734375, prec 0.0323829, recall 0.764877
2017-12-10T13:17:43.346974: step 722, loss 0.989719, acc 0.71875, prec 0.0324066, recall 0.765217
2017-12-10T13:17:43.788484: step 723, loss 0.94778, acc 0.765625, prec 0.0323767, recall 0.765217
2017-12-10T13:17:44.245952: step 724, loss 1.05427, acc 0.734375, prec 0.032343, recall 0.765217
2017-12-10T13:17:44.694597: step 725, loss 1.14713, acc 0.828125, prec 0.0324397, recall 0.765896
2017-12-10T13:17:45.149824: step 726, loss 1.06822, acc 0.734375, prec 0.032406, recall 0.765896
2017-12-10T13:17:45.598926: step 727, loss 0.782185, acc 0.78125, prec 0.0323783, recall 0.765896
2017-12-10T13:17:46.045066: step 728, loss 11.9335, acc 0.796875, prec 0.0323565, recall 0.763689
2017-12-10T13:17:46.503868: step 729, loss 0.464632, acc 0.890625, prec 0.0324018, recall 0.764029
2017-12-10T13:17:46.945511: step 730, loss 1.04241, acc 0.71875, prec 0.0323662, recall 0.764029
2017-12-10T13:17:47.400083: step 731, loss 0.791986, acc 0.828125, prec 0.0323445, recall 0.764029
2017-12-10T13:17:47.855510: step 732, loss 0.609928, acc 0.875, prec 0.0323877, recall 0.764368
2017-12-10T13:17:48.309686: step 733, loss 7.6712, acc 0.796875, prec 0.032364, recall 0.763271
2017-12-10T13:17:48.743709: step 734, loss 0.77031, acc 0.84375, prec 0.0324032, recall 0.76361
2017-12-10T13:17:49.195056: step 735, loss 1.34535, acc 0.75, prec 0.0324305, recall 0.763949
2017-12-10T13:17:49.646833: step 736, loss 0.715641, acc 0.78125, prec 0.0324029, recall 0.763949
2017-12-10T13:17:50.094737: step 737, loss 0.629326, acc 0.8125, prec 0.0325553, recall 0.764957
2017-12-10T13:17:50.530057: step 738, loss 0.55034, acc 0.796875, prec 0.0325883, recall 0.765292
2017-12-10T13:17:50.972588: step 739, loss 1.30056, acc 0.734375, prec 0.0325548, recall 0.765292
2017-12-10T13:17:51.411917: step 740, loss 0.740125, acc 0.796875, prec 0.0325292, recall 0.765292
2017-12-10T13:17:51.852387: step 741, loss 0.801475, acc 0.796875, prec 0.0325036, recall 0.765292
2017-12-10T13:17:52.300164: step 742, loss 6.38381, acc 0.734375, prec 0.032589, recall 0.764873
2017-12-10T13:17:52.742678: step 743, loss 0.667738, acc 0.796875, prec 0.0326218, recall 0.765205
2017-12-10T13:17:53.175772: step 744, loss 0.672471, acc 0.765625, prec 0.0325923, recall 0.765205
2017-12-10T13:17:53.616877: step 745, loss 1.40877, acc 0.765625, prec 0.0326211, recall 0.765537
2017-12-10T13:17:54.066095: step 746, loss 1.21784, acc 0.75, prec 0.0325897, recall 0.765537
2017-12-10T13:17:54.514035: step 747, loss 0.895728, acc 0.828125, prec 0.0325682, recall 0.765537
2017-12-10T13:17:54.956927: step 748, loss 0.631114, acc 0.78125, prec 0.0325989, recall 0.765867
2017-12-10T13:17:55.418330: step 749, loss 0.502992, acc 0.8125, prec 0.0325754, recall 0.765867
2017-12-10T13:17:55.866325: step 750, loss 0.658272, acc 0.8125, prec 0.03261, recall 0.766197
2017-12-10T13:17:56.318414: step 751, loss 1.09992, acc 0.796875, prec 0.0325846, recall 0.766197
2017-12-10T13:17:56.767015: step 752, loss 3.06407, acc 0.765625, prec 0.0326152, recall 0.765449
2017-12-10T13:17:57.216063: step 753, loss 0.60624, acc 0.828125, prec 0.0325937, recall 0.765449
2017-12-10T13:17:57.664081: step 754, loss 0.786701, acc 0.796875, prec 0.0326262, recall 0.765778
2017-12-10T13:17:58.106265: step 755, loss 0.72812, acc 0.828125, prec 0.0326626, recall 0.766106
2017-12-10T13:17:58.556455: step 756, loss 0.71957, acc 0.84375, prec 0.0327008, recall 0.766434
2017-12-10T13:17:59.010387: step 757, loss 0.576009, acc 0.84375, prec 0.0326813, recall 0.766434
2017-12-10T13:17:59.445570: step 758, loss 0.746326, acc 0.828125, prec 0.0327175, recall 0.76676
2017-12-10T13:17:59.891282: step 759, loss 2.5331, acc 0.765625, prec 0.0326902, recall 0.76569
2017-12-10T13:18:00.340873: step 760, loss 2.78038, acc 0.828125, prec 0.0327284, recall 0.764951
2017-12-10T13:18:00.788415: step 761, loss 0.579737, acc 0.828125, prec 0.0327069, recall 0.764951
2017-12-10T13:18:01.229609: step 762, loss 1.0316, acc 0.796875, prec 0.0327392, recall 0.765278
2017-12-10T13:18:01.681829: step 763, loss 0.714503, acc 0.84375, prec 0.0327771, recall 0.765603
2017-12-10T13:18:02.128916: step 764, loss 0.382685, acc 0.84375, prec 0.0327577, recall 0.765603
2017-12-10T13:18:02.582027: step 765, loss 0.791844, acc 0.828125, prec 0.032851, recall 0.766252
2017-12-10T13:18:03.033805: step 766, loss 3.74534, acc 0.75, prec 0.0328791, recall 0.765517
2017-12-10T13:18:03.484561: step 767, loss 1.15321, acc 0.71875, prec 0.0329014, recall 0.76584
2017-12-10T13:18:03.936979: step 768, loss 10.1477, acc 0.8125, prec 0.0329371, recall 0.76511
2017-12-10T13:18:04.394225: step 769, loss 1.36483, acc 0.6875, prec 0.0330125, recall 0.765753
2017-12-10T13:18:04.836205: step 770, loss 1.12615, acc 0.640625, prec 0.0329677, recall 0.765753
2017-12-10T13:18:05.284946: step 771, loss 1.64558, acc 0.640625, prec 0.03298, recall 0.766074
2017-12-10T13:18:05.732422: step 772, loss 1.95208, acc 0.609375, prec 0.0329884, recall 0.766393
2017-12-10T13:18:06.177148: step 773, loss 1.66333, acc 0.671875, prec 0.0329477, recall 0.766393
2017-12-10T13:18:06.619322: step 774, loss 1.94073, acc 0.625, prec 0.032958, recall 0.766712
2017-12-10T13:18:07.059981: step 775, loss 1.37165, acc 0.671875, prec 0.0329175, recall 0.766712
2017-12-10T13:18:07.510158: step 776, loss 1.73571, acc 0.65625, prec 0.0330448, recall 0.767663
2017-12-10T13:18:07.954978: step 777, loss 2.02076, acc 0.5625, prec 0.0329908, recall 0.767663
2017-12-10T13:18:08.406571: step 778, loss 1.82924, acc 0.71875, prec 0.0330125, recall 0.767978
2017-12-10T13:18:08.843141: step 779, loss 1.04235, acc 0.734375, prec 0.0329798, recall 0.767978
2017-12-10T13:18:09.293280: step 780, loss 1.19273, acc 0.734375, prec 0.0330035, recall 0.768293
2017-12-10T13:18:09.733143: step 781, loss 1.69772, acc 0.640625, prec 0.0330718, recall 0.768919
2017-12-10T13:18:10.177592: step 782, loss 0.510097, acc 0.859375, prec 0.0331668, recall 0.769542
2017-12-10T13:18:10.623625: step 783, loss 0.639125, acc 0.765625, prec 0.0331941, recall 0.769852
2017-12-10T13:18:11.052619: step 784, loss 0.778978, acc 0.8125, prec 0.033171, recall 0.769852
2017-12-10T13:18:11.499217: step 785, loss 0.882307, acc 0.78125, prec 0.033144, recall 0.769852
2017-12-10T13:18:11.939317: step 786, loss 0.208851, acc 0.90625, prec 0.0331325, recall 0.769852
2017-12-10T13:18:12.388977: step 787, loss 2.66037, acc 0.875, prec 0.0331191, recall 0.768817
2017-12-10T13:18:12.832677: step 788, loss 7.43917, acc 0.859375, prec 0.0331597, recall 0.768097
2017-12-10T13:18:13.285367: step 789, loss 0.285871, acc 0.90625, prec 0.0331482, recall 0.768097
2017-12-10T13:18:13.712696: step 790, loss 0.352459, acc 0.921875, prec 0.0331386, recall 0.768097
2017-12-10T13:18:14.165776: step 791, loss 0.557448, acc 0.875, prec 0.0331792, recall 0.768407
2017-12-10T13:18:14.615150: step 792, loss 0.570895, acc 0.859375, prec 0.0332178, recall 0.768717
2017-12-10T13:18:15.065673: step 793, loss 0.511099, acc 0.796875, prec 0.0331929, recall 0.768717
2017-12-10T13:18:15.518980: step 794, loss 0.469582, acc 0.875, prec 0.0331775, recall 0.768717
2017-12-10T13:18:15.971246: step 795, loss 0.809666, acc 0.765625, prec 0.0331489, recall 0.768717
2017-12-10T13:18:16.422675: step 796, loss 0.457091, acc 0.875, prec 0.033245, recall 0.769333
2017-12-10T13:18:16.871266: step 797, loss 1.36897, acc 0.875, prec 0.0332853, recall 0.769641
2017-12-10T13:18:17.324860: step 798, loss 2.97393, acc 0.875, prec 0.0332719, recall 0.768617
2017-12-10T13:18:17.772587: step 799, loss 0.76814, acc 0.78125, prec 0.0332451, recall 0.768617
2017-12-10T13:18:18.217249: step 800, loss 0.92205, acc 0.8125, prec 0.0332222, recall 0.768617
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-800

2017-12-10T13:18:20.430234: step 801, loss 0.650192, acc 0.796875, prec 0.0332529, recall 0.768924
2017-12-10T13:18:20.876510: step 802, loss 4.34746, acc 0.828125, prec 0.0332338, recall 0.767905
2017-12-10T13:18:21.308029: step 803, loss 0.616449, acc 0.796875, prec 0.0332645, recall 0.768212
2017-12-10T13:18:21.755691: step 804, loss 4.07644, acc 0.765625, prec 0.0332932, recall 0.767503
2017-12-10T13:18:22.215059: step 805, loss 1.05966, acc 0.765625, prec 0.0332646, recall 0.767503
2017-12-10T13:18:22.669371: step 806, loss 3.17096, acc 0.75, prec 0.0332361, recall 0.766491
2017-12-10T13:18:23.130200: step 807, loss 1.26062, acc 0.625, prec 0.0331905, recall 0.766491
2017-12-10T13:18:23.576402: step 808, loss 1.66367, acc 0.625, prec 0.0331451, recall 0.766491
2017-12-10T13:18:24.023091: step 809, loss 1.42276, acc 0.671875, prec 0.0331605, recall 0.766798
2017-12-10T13:18:24.455161: step 810, loss 1.76684, acc 0.578125, prec 0.0332746, recall 0.767717
2017-12-10T13:18:24.895668: step 811, loss 2.38307, acc 0.578125, prec 0.0332784, recall 0.768021
2017-12-10T13:18:25.335267: step 812, loss 2.11345, acc 0.5625, prec 0.0332256, recall 0.768021
2017-12-10T13:18:25.797666: step 813, loss 2.21759, acc 0.578125, prec 0.0332295, recall 0.768325
2017-12-10T13:18:26.237577: step 814, loss 1.22932, acc 0.6875, prec 0.033192, recall 0.768325
2017-12-10T13:18:26.681871: step 815, loss 2.40448, acc 0.578125, prec 0.0332505, recall 0.768929
2017-12-10T13:18:27.134137: step 816, loss 2.0136, acc 0.578125, prec 0.0332544, recall 0.769231
2017-12-10T13:18:27.585349: step 817, loss 2.33091, acc 0.5625, prec 0.033202, recall 0.769231
2017-12-10T13:18:28.031291: step 818, loss 1.25286, acc 0.75, prec 0.0331722, recall 0.769231
2017-12-10T13:18:28.480579: step 819, loss 1.75985, acc 0.59375, prec 0.033178, recall 0.769531
2017-12-10T13:18:28.935794: step 820, loss 0.872906, acc 0.71875, prec 0.033253, recall 0.77013
2017-12-10T13:18:29.391695: step 821, loss 0.844389, acc 0.796875, prec 0.0332829, recall 0.770428
2017-12-10T13:18:29.847257: step 822, loss 2.63377, acc 0.84375, prec 0.0333203, recall 0.769728
2017-12-10T13:18:30.301639: step 823, loss 12.0103, acc 0.8125, prec 0.0333539, recall 0.769032
2017-12-10T13:18:30.754671: step 824, loss 0.989612, acc 0.703125, prec 0.0333725, recall 0.76933
2017-12-10T13:18:31.196059: step 825, loss 0.849386, acc 0.78125, prec 0.0334004, recall 0.769627
2017-12-10T13:18:31.627703: step 826, loss 0.799194, acc 0.765625, prec 0.0334263, recall 0.769923
2017-12-10T13:18:32.072072: step 827, loss 0.928182, acc 0.75, prec 0.0333965, recall 0.769923
2017-12-10T13:18:32.518504: step 828, loss 1.07388, acc 0.71875, prec 0.033363, recall 0.769923
2017-12-10T13:18:32.962275: step 829, loss 6.77984, acc 0.71875, prec 0.0333315, recall 0.768935
2017-12-10T13:18:33.414081: step 830, loss 0.731687, acc 0.859375, prec 0.0334223, recall 0.769526
2017-12-10T13:18:33.852772: step 831, loss 0.960448, acc 0.78125, prec 0.0333963, recall 0.769526
2017-12-10T13:18:34.293533: step 832, loss 0.889704, acc 0.734375, prec 0.0333648, recall 0.769526
2017-12-10T13:18:34.735989: step 833, loss 0.840433, acc 0.6875, prec 0.0333278, recall 0.769526
2017-12-10T13:18:35.175835: step 834, loss 0.634769, acc 0.796875, prec 0.0333573, recall 0.769821
2017-12-10T13:18:35.610412: step 835, loss 1.17037, acc 0.78125, prec 0.033385, recall 0.770115
2017-12-10T13:18:36.070921: step 836, loss 0.824468, acc 0.8125, prec 0.0333628, recall 0.770115
2017-12-10T13:18:36.501281: step 837, loss 0.883846, acc 0.78125, prec 0.033337, recall 0.770115
2017-12-10T13:18:36.955272: step 838, loss 0.698845, acc 0.796875, prec 0.0333665, recall 0.770408
2017-12-10T13:18:37.403664: step 839, loss 1.60009, acc 0.765625, prec 0.0333922, recall 0.770701
2017-12-10T13:18:37.859094: step 840, loss 2.63632, acc 0.78125, prec 0.0333683, recall 0.76972
2017-12-10T13:18:38.294159: step 841, loss 0.616569, acc 0.765625, prec 0.0333407, recall 0.76972
2017-12-10T13:18:38.737833: step 842, loss 1.79793, acc 0.78125, prec 0.0334214, recall 0.770305
2017-12-10T13:18:39.184205: step 843, loss 1.38757, acc 0.921875, prec 0.0335186, recall 0.770886
2017-12-10T13:18:39.622531: step 844, loss 0.945743, acc 0.78125, prec 0.0335991, recall 0.771465
2017-12-10T13:18:40.068170: step 845, loss 0.733198, acc 0.78125, prec 0.0335733, recall 0.771465
2017-12-10T13:18:40.517834: step 846, loss 1.49264, acc 0.625, prec 0.0335291, recall 0.771465
2017-12-10T13:18:40.961129: step 847, loss 1.111, acc 0.796875, prec 0.0335582, recall 0.771753
2017-12-10T13:18:41.409244: step 848, loss 0.802982, acc 0.734375, prec 0.0335798, recall 0.77204
2017-12-10T13:18:41.849835: step 849, loss 3.27691, acc 0.859375, prec 0.033618, recall 0.771357
2017-12-10T13:18:42.324306: step 850, loss 3.03873, acc 0.71875, prec 0.0335868, recall 0.770389
2017-12-10T13:18:42.752081: step 851, loss 0.935452, acc 0.828125, prec 0.0337251, recall 0.77125
2017-12-10T13:18:43.200213: step 852, loss 1.67533, acc 0.625, prec 0.0336809, recall 0.77125
2017-12-10T13:18:43.629549: step 853, loss 0.843271, acc 0.765625, prec 0.033706, recall 0.771536
2017-12-10T13:18:44.071253: step 854, loss 1.75977, acc 0.65625, prec 0.0336656, recall 0.771536
2017-12-10T13:18:44.514998: step 855, loss 1.08263, acc 0.734375, prec 0.0336871, recall 0.77182
2017-12-10T13:18:44.962082: step 856, loss 0.893485, acc 0.75, prec 0.0336578, recall 0.77182
2017-12-10T13:18:45.402947: step 857, loss 1.46994, acc 0.671875, prec 0.0336194, recall 0.77182
2017-12-10T13:18:45.860635: step 858, loss 0.612447, acc 0.78125, prec 0.0335938, recall 0.77182
2017-12-10T13:18:46.301141: step 859, loss 1.25942, acc 0.734375, prec 0.0336153, recall 0.772105
2017-12-10T13:18:46.739817: step 860, loss 1.37967, acc 0.6875, prec 0.0336835, recall 0.772671
2017-12-10T13:18:47.186615: step 861, loss 1.16248, acc 0.671875, prec 0.0336975, recall 0.772953
2017-12-10T13:18:47.625910: step 862, loss 1.47761, acc 0.609375, prec 0.033652, recall 0.772953
2017-12-10T13:18:48.075925: step 863, loss 1.36526, acc 0.734375, prec 0.0336212, recall 0.772953
2017-12-10T13:18:48.542307: step 864, loss 0.468784, acc 0.8125, prec 0.0336515, recall 0.773234
2017-12-10T13:18:48.981517: step 865, loss 1.1582, acc 0.84375, prec 0.0337375, recall 0.773795
2017-12-10T13:18:49.441766: step 866, loss 0.18956, acc 0.890625, prec 0.0337248, recall 0.773795
2017-12-10T13:18:49.877656: step 867, loss 0.570288, acc 0.859375, prec 0.0337605, recall 0.774074
2017-12-10T13:18:50.321993: step 868, loss 0.524951, acc 0.84375, prec 0.0337423, recall 0.774074
2017-12-10T13:18:50.771014: step 869, loss 0.418921, acc 0.859375, prec 0.033778, recall 0.774353
2017-12-10T13:18:51.217394: step 870, loss 2.76348, acc 0.78125, prec 0.0337544, recall 0.773399
2017-12-10T13:18:51.656348: step 871, loss 0.47132, acc 0.90625, prec 0.0337954, recall 0.773678
2017-12-10T13:18:52.090057: step 872, loss 0.419838, acc 0.90625, prec 0.0337845, recall 0.773678
2017-12-10T13:18:52.553160: step 873, loss 0.563969, acc 0.84375, prec 0.0337664, recall 0.773678
2017-12-10T13:18:53.004055: step 874, loss 0.43479, acc 0.859375, prec 0.0337501, recall 0.773678
2017-12-10T13:18:53.450957: step 875, loss 0.279848, acc 0.890625, prec 0.0337374, recall 0.773678
2017-12-10T13:18:53.904140: step 876, loss 0.328509, acc 0.921875, prec 0.0337283, recall 0.773678
2017-12-10T13:18:54.361509: step 877, loss 0.542708, acc 0.90625, prec 0.0337693, recall 0.773956
2017-12-10T13:18:54.804380: step 878, loss 1.14295, acc 0.953125, prec 0.0338156, recall 0.774233
2017-12-10T13:18:55.251159: step 879, loss 0.250222, acc 0.921875, prec 0.0338066, recall 0.774233
2017-12-10T13:18:55.696682: step 880, loss 0.349648, acc 0.9375, prec 0.0338511, recall 0.77451
2017-12-10T13:18:56.129616: step 881, loss 0.360767, acc 0.90625, prec 0.033892, recall 0.774786
2017-12-10T13:18:56.607871: step 882, loss 4.84974, acc 0.859375, prec 0.0339808, recall 0.77439
2017-12-10T13:18:57.052827: step 883, loss 2.94784, acc 0.875, prec 0.0340715, recall 0.773998
2017-12-10T13:18:57.507997: step 884, loss 0.982054, acc 0.796875, prec 0.0340478, recall 0.773998
2017-12-10T13:18:57.947696: step 885, loss 1.1455, acc 0.75, prec 0.0340187, recall 0.773998
2017-12-10T13:18:58.390314: step 886, loss 1.00013, acc 0.765625, prec 0.0339915, recall 0.773998
2017-12-10T13:18:58.840816: step 887, loss 1.03668, acc 0.703125, prec 0.03406, recall 0.774545
2017-12-10T13:18:59.290471: step 888, loss 1.20352, acc 0.78125, prec 0.0340861, recall 0.774818
2017-12-10T13:18:59.736591: step 889, loss 1.3615, acc 0.75, prec 0.034057, recall 0.774818
2017-12-10T13:19:00.197499: step 890, loss 1.46737, acc 0.640625, prec 0.0340668, recall 0.775091
2017-12-10T13:19:00.635997: step 891, loss 1.19793, acc 0.71875, prec 0.0340855, recall 0.775362
2017-12-10T13:19:01.083604: step 892, loss 0.985005, acc 0.6875, prec 0.0340493, recall 0.775362
2017-12-10T13:19:01.532869: step 893, loss 1.2968, acc 0.640625, prec 0.0340078, recall 0.775362
2017-12-10T13:19:01.978507: step 894, loss 1.45722, acc 0.625, prec 0.0339647, recall 0.775362
2017-12-10T13:19:02.425319: step 895, loss 1.40035, acc 0.71875, prec 0.0339834, recall 0.775633
2017-12-10T13:19:02.868979: step 896, loss 11.1209, acc 0.75, prec 0.0340093, recall 0.774038
2017-12-10T13:19:03.337945: step 897, loss 1.18927, acc 0.6875, prec 0.0340244, recall 0.77431
2017-12-10T13:19:03.783579: step 898, loss 1.27008, acc 0.734375, prec 0.0339939, recall 0.77431
2017-12-10T13:19:04.218516: step 899, loss 1.16949, acc 0.6875, prec 0.0339581, recall 0.77431
2017-12-10T13:19:04.668973: step 900, loss 1.66137, acc 0.6875, prec 0.0340748, recall 0.77512
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-900

2017-12-10T13:19:06.711751: step 901, loss 1.28435, acc 0.625, prec 0.034184, recall 0.775924
2017-12-10T13:19:07.160920: step 902, loss 1.23381, acc 0.65625, prec 0.0341446, recall 0.775924
2017-12-10T13:19:07.613647: step 903, loss 1.09293, acc 0.6875, prec 0.0341088, recall 0.775924
2017-12-10T13:19:08.065264: step 904, loss 1.34332, acc 0.671875, prec 0.0341218, recall 0.77619
2017-12-10T13:19:08.511339: step 905, loss 1.13319, acc 0.734375, prec 0.0340915, recall 0.77619
2017-12-10T13:19:08.953956: step 906, loss 1.26552, acc 0.734375, prec 0.0341621, recall 0.776722
2017-12-10T13:19:09.395837: step 907, loss 1.35171, acc 0.640625, prec 0.0341211, recall 0.776722
2017-12-10T13:19:09.849990: step 908, loss 1.4755, acc 0.75, prec 0.034143, recall 0.776987
2017-12-10T13:19:10.309922: step 909, loss 1.5991, acc 0.796875, prec 0.0341702, recall 0.777251
2017-12-10T13:19:10.767267: step 910, loss 0.911985, acc 0.796875, prec 0.0341471, recall 0.777251
2017-12-10T13:19:11.210292: step 911, loss 0.741834, acc 0.84375, prec 0.0341796, recall 0.777515
2017-12-10T13:19:11.657237: step 912, loss 1.0179, acc 0.828125, prec 0.0342605, recall 0.77804
2017-12-10T13:19:12.100820: step 913, loss 0.578147, acc 0.84375, prec 0.0342928, recall 0.778302
2017-12-10T13:19:12.547451: step 914, loss 0.783805, acc 0.875, prec 0.0344792, recall 0.779343
2017-12-10T13:19:12.985590: step 915, loss 0.327721, acc 0.890625, prec 0.0345669, recall 0.779859
2017-12-10T13:19:13.424134: step 916, loss 0.551918, acc 0.84375, prec 0.034599, recall 0.780117
2017-12-10T13:19:13.868789: step 917, loss 0.48099, acc 0.875, prec 0.0345847, recall 0.780117
2017-12-10T13:19:14.345427: step 918, loss 0.37827, acc 0.859375, prec 0.0345685, recall 0.780117
2017-12-10T13:19:14.802249: step 919, loss 0.530534, acc 0.84375, prec 0.0345506, recall 0.780117
2017-12-10T13:19:15.261466: step 920, loss 1.03081, acc 0.890625, prec 0.0345881, recall 0.780374
2017-12-10T13:19:15.719107: step 921, loss 0.268031, acc 0.90625, prec 0.0346773, recall 0.780886
2017-12-10T13:19:16.157571: step 922, loss 0.335376, acc 0.921875, prec 0.0346683, recall 0.780886
2017-12-10T13:19:16.594600: step 923, loss 0.161955, acc 0.921875, prec 0.0346594, recall 0.780886
2017-12-10T13:19:17.027611: step 924, loss 0.359715, acc 0.890625, prec 0.0346468, recall 0.780886
2017-12-10T13:19:17.480772: step 925, loss 0.427471, acc 0.90625, prec 0.034686, recall 0.781141
2017-12-10T13:19:17.927661: step 926, loss 0.1778, acc 0.953125, prec 0.0347804, recall 0.781649
2017-12-10T13:19:18.377103: step 927, loss 0.342198, acc 0.875, prec 0.034766, recall 0.781649
2017-12-10T13:19:18.825891: step 928, loss 5.3528, acc 0.921875, prec 0.0348087, recall 0.780997
2017-12-10T13:19:19.273653: step 929, loss 0.303588, acc 0.9375, prec 0.0348015, recall 0.780997
2017-12-10T13:19:19.710586: step 930, loss 5.58191, acc 0.953125, prec 0.0348975, recall 0.7806
2017-12-10T13:19:20.153008: step 931, loss 0.232899, acc 0.9375, prec 0.0348903, recall 0.7806
2017-12-10T13:19:20.599351: step 932, loss 0.466765, acc 0.84375, prec 0.0348723, recall 0.7806
2017-12-10T13:19:21.045517: step 933, loss 0.446799, acc 0.859375, prec 0.0348561, recall 0.7806
2017-12-10T13:19:21.497729: step 934, loss 2.38504, acc 0.765625, prec 0.034831, recall 0.7797
2017-12-10T13:19:21.955056: step 935, loss 0.485127, acc 0.84375, prec 0.0348131, recall 0.7797
2017-12-10T13:19:22.405158: step 936, loss 1.04997, acc 0.765625, prec 0.0348359, recall 0.779954
2017-12-10T13:19:22.857629: step 937, loss 1.11687, acc 0.828125, prec 0.0348658, recall 0.780207
2017-12-10T13:19:23.304921: step 938, loss 1.41493, acc 0.6875, prec 0.0349291, recall 0.780712
2017-12-10T13:19:23.749736: step 939, loss 1.98605, acc 0.6875, prec 0.0349923, recall 0.781214
2017-12-10T13:19:24.201071: step 940, loss 1.21735, acc 0.75, prec 0.035112, recall 0.781963
2017-12-10T13:19:24.647350: step 941, loss 1.50206, acc 0.65625, prec 0.0350724, recall 0.781963
2017-12-10T13:19:25.091916: step 942, loss 2.21453, acc 0.625, prec 0.0350787, recall 0.782212
2017-12-10T13:19:25.523763: step 943, loss 1.15741, acc 0.703125, prec 0.0351926, recall 0.782955
2017-12-10T13:19:25.972679: step 944, loss 1.48248, acc 0.671875, prec 0.0351549, recall 0.782955
2017-12-10T13:19:26.406798: step 945, loss 1.32129, acc 0.6875, prec 0.0351682, recall 0.783201
2017-12-10T13:19:26.858530: step 946, loss 1.42203, acc 0.6875, prec 0.0352797, recall 0.783937
2017-12-10T13:19:27.300896: step 947, loss 0.951991, acc 0.71875, prec 0.0352965, recall 0.784181
2017-12-10T13:19:27.747344: step 948, loss 0.719805, acc 0.78125, prec 0.0353204, recall 0.784424
2017-12-10T13:19:28.200075: step 949, loss 0.716934, acc 0.765625, prec 0.0353425, recall 0.784667
2017-12-10T13:19:28.642378: step 950, loss 0.446669, acc 0.875, prec 0.0353282, recall 0.784667
2017-12-10T13:19:29.075355: step 951, loss 0.6632, acc 0.8125, prec 0.0353556, recall 0.78491
2017-12-10T13:19:29.521535: step 952, loss 0.622216, acc 0.859375, prec 0.0353884, recall 0.785152
2017-12-10T13:19:29.981526: step 953, loss 0.30542, acc 0.859375, prec 0.0353722, recall 0.785152
2017-12-10T13:19:30.418068: step 954, loss 0.629468, acc 0.8125, prec 0.0353996, recall 0.785393
2017-12-10T13:19:30.875637: step 955, loss 0.457102, acc 0.875, prec 0.0353852, recall 0.785393
2017-12-10T13:19:31.321341: step 956, loss 0.515472, acc 0.84375, prec 0.0353673, recall 0.785393
2017-12-10T13:19:31.764221: step 957, loss 0.612935, acc 0.921875, prec 0.0354072, recall 0.785634
2017-12-10T13:19:32.216672: step 958, loss 0.385863, acc 0.9375, prec 0.0354, recall 0.785634
2017-12-10T13:19:32.642063: step 959, loss 6.34764, acc 0.828125, prec 0.0353821, recall 0.784753
2017-12-10T13:19:33.094233: step 960, loss 22.448, acc 0.890625, prec 0.0353714, recall 0.783875
2017-12-10T13:19:33.553543: step 961, loss 20.6641, acc 0.859375, prec 0.0353589, recall 0.782123
2017-12-10T13:19:34.005699: step 962, loss 0.725804, acc 0.828125, prec 0.035388, recall 0.782366
2017-12-10T13:19:34.451960: step 963, loss 0.970963, acc 0.84375, prec 0.0355161, recall 0.783092
2017-12-10T13:19:34.895573: step 964, loss 1.30716, acc 0.703125, prec 0.0354821, recall 0.783092
2017-12-10T13:19:35.339143: step 965, loss 1.5696, acc 0.578125, prec 0.0354824, recall 0.783333
2017-12-10T13:19:35.791191: step 966, loss 1.28809, acc 0.625, prec 0.0354396, recall 0.783333
2017-12-10T13:19:36.231914: step 967, loss 1.68315, acc 0.578125, prec 0.0354884, recall 0.783814
2017-12-10T13:19:36.684561: step 968, loss 1.46258, acc 0.625, prec 0.0354941, recall 0.784053
2017-12-10T13:19:37.143561: step 969, loss 2.15568, acc 0.609375, prec 0.0355945, recall 0.784768
2017-12-10T13:19:37.597170: step 970, loss 2.25481, acc 0.453125, prec 0.0355322, recall 0.784768
2017-12-10T13:19:38.050889: step 971, loss 2.20962, acc 0.53125, prec 0.0355753, recall 0.785242
2017-12-10T13:19:38.502045: step 972, loss 2.05766, acc 0.46875, prec 0.0356111, recall 0.785714
2017-12-10T13:19:38.956187: step 973, loss 2.59555, acc 0.53125, prec 0.0356059, recall 0.78595
2017-12-10T13:19:39.390993: step 974, loss 2.20547, acc 0.453125, prec 0.035592, recall 0.786184
2017-12-10T13:19:39.836448: step 975, loss 1.8866, acc 0.5625, prec 0.0355426, recall 0.786184
2017-12-10T13:19:40.300764: step 976, loss 1.89688, acc 0.5, prec 0.0355817, recall 0.786652
2017-12-10T13:19:40.729529: step 977, loss 1.60278, acc 0.640625, prec 0.0356366, recall 0.787118
2017-12-10T13:19:41.182276: step 978, loss 2.30117, acc 0.65625, prec 0.0357407, recall 0.787813
2017-12-10T13:19:41.620892: step 979, loss 1.49765, acc 0.53125, prec 0.0356879, recall 0.787813
2017-12-10T13:19:42.070416: step 980, loss 0.732491, acc 0.703125, prec 0.0356545, recall 0.787813
2017-12-10T13:19:42.513507: step 981, loss 1.20725, acc 0.6875, prec 0.0356194, recall 0.787813
2017-12-10T13:19:42.968195: step 982, loss 1.04315, acc 0.640625, prec 0.0355791, recall 0.787813
2017-12-10T13:19:43.418684: step 983, loss 5.6359, acc 0.609375, prec 0.0355372, recall 0.786957
2017-12-10T13:19:43.876893: step 984, loss 0.429262, acc 0.859375, prec 0.0355689, recall 0.787188
2017-12-10T13:19:44.329758: step 985, loss 0.647614, acc 0.796875, prec 0.0355462, recall 0.787188
2017-12-10T13:19:44.791175: step 986, loss 0.542323, acc 0.859375, prec 0.035625, recall 0.787649
2017-12-10T13:19:45.254970: step 987, loss 0.519865, acc 0.828125, prec 0.0356058, recall 0.787649
2017-12-10T13:19:45.703851: step 988, loss 0.383547, acc 0.875, prec 0.0355919, recall 0.787649
2017-12-10T13:19:46.134346: step 989, loss 0.511678, acc 0.8125, prec 0.035571, recall 0.787649
2017-12-10T13:19:46.582595: step 990, loss 0.340187, acc 0.90625, prec 0.0356077, recall 0.787879
2017-12-10T13:19:47.053055: step 991, loss 0.800371, acc 0.828125, prec 0.0355886, recall 0.787879
2017-12-10T13:19:47.499827: step 992, loss 0.159706, acc 0.96875, prec 0.0356322, recall 0.788108
2017-12-10T13:19:47.946670: step 993, loss 0.709406, acc 0.890625, prec 0.0356672, recall 0.788337
2017-12-10T13:19:48.351688: step 994, loss 0.426264, acc 0.903846, prec 0.0356585, recall 0.788337
2017-12-10T13:19:48.815477: step 995, loss 0.0973918, acc 0.953125, prec 0.0356532, recall 0.788337
2017-12-10T13:19:49.267337: step 996, loss 3.40788, acc 0.953125, prec 0.0357439, recall 0.787944
2017-12-10T13:19:49.722172: step 997, loss 0.444536, acc 0.890625, prec 0.0358258, recall 0.7884
2017-12-10T13:19:50.170535: step 998, loss 0.580755, acc 0.84375, prec 0.0358554, recall 0.788627
2017-12-10T13:19:50.617684: step 999, loss 0.397281, acc 0.890625, prec 0.0358432, recall 0.788627
2017-12-10T13:19:51.066553: step 1000, loss 7.39742, acc 0.953125, prec 0.0358397, recall 0.787781
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1000

2017-12-10T13:19:52.974324: step 1001, loss 0.373987, acc 0.890625, prec 0.0358744, recall 0.788009
2017-12-10T13:19:53.411849: step 1002, loss 0.462488, acc 0.828125, prec 0.0358552, recall 0.788009
2017-12-10T13:19:53.864771: step 1003, loss 6.23371, acc 0.859375, prec 0.0358412, recall 0.787166
2017-12-10T13:19:54.291642: step 1004, loss 0.532152, acc 0.8125, prec 0.0358203, recall 0.787166
2017-12-10T13:19:54.739854: step 1005, loss 0.93598, acc 0.75, prec 0.0357924, recall 0.787166
2017-12-10T13:19:55.182814: step 1006, loss 0.685867, acc 0.75, prec 0.0359052, recall 0.787847
2017-12-10T13:19:55.633247: step 1007, loss 0.804241, acc 0.78125, prec 0.0358808, recall 0.787847
2017-12-10T13:19:56.066172: step 1008, loss 0.924181, acc 0.625, prec 0.035839, recall 0.787847
2017-12-10T13:19:56.521599: step 1009, loss 1.77628, acc 0.640625, prec 0.0357991, recall 0.787847
2017-12-10T13:19:56.961540: step 1010, loss 1.19389, acc 0.65625, prec 0.035761, recall 0.787847
2017-12-10T13:19:57.414281: step 1011, loss 1.19057, acc 0.609375, prec 0.0357177, recall 0.787847
2017-12-10T13:19:57.877908: step 1012, loss 0.595879, acc 0.828125, prec 0.0357453, recall 0.788072
2017-12-10T13:19:58.327185: step 1013, loss 0.92967, acc 0.765625, prec 0.0357195, recall 0.788072
2017-12-10T13:19:58.776676: step 1014, loss 1.08195, acc 0.65625, prec 0.0356816, recall 0.788072
2017-12-10T13:19:59.228727: step 1015, loss 0.938689, acc 0.734375, prec 0.0356988, recall 0.788298
2017-12-10T13:19:59.672689: step 1016, loss 0.84247, acc 0.78125, prec 0.0357212, recall 0.788523
2017-12-10T13:20:00.110171: step 1017, loss 0.87533, acc 0.796875, prec 0.0357452, recall 0.788747
2017-12-10T13:20:00.548192: step 1018, loss 1.78181, acc 0.671875, prec 0.0358018, recall 0.789195
2017-12-10T13:20:00.993079: step 1019, loss 0.301768, acc 0.921875, prec 0.0358395, recall 0.789418
2017-12-10T13:20:01.437047: step 1020, loss 0.730593, acc 0.796875, prec 0.0358172, recall 0.789418
2017-12-10T13:20:01.880429: step 1021, loss 0.438083, acc 0.875, prec 0.0358034, recall 0.789418
2017-12-10T13:20:02.325790: step 1022, loss 0.239869, acc 0.875, prec 0.0358822, recall 0.789863
2017-12-10T13:20:02.770606: step 1023, loss 0.415801, acc 0.859375, prec 0.0358667, recall 0.789863
2017-12-10T13:20:03.227195: step 1024, loss 0.201386, acc 0.921875, prec 0.0359505, recall 0.790306
2017-12-10T13:20:03.675371: step 1025, loss 2.22079, acc 0.921875, prec 0.0359436, recall 0.789474
2017-12-10T13:20:04.146234: step 1026, loss 11.0923, acc 0.890625, prec 0.0360719, recall 0.789308
2017-12-10T13:20:04.602738: step 1027, loss 0.514297, acc 0.84375, prec 0.0360546, recall 0.789308
2017-12-10T13:20:05.049474: step 1028, loss 0.435, acc 0.859375, prec 0.0360852, recall 0.789529
2017-12-10T13:20:05.505603: step 1029, loss 0.58302, acc 0.84375, prec 0.0360679, recall 0.789529
2017-12-10T13:20:05.943699: step 1030, loss 0.765346, acc 0.8125, prec 0.0360472, recall 0.789529
2017-12-10T13:20:06.375733: step 1031, loss 0.785775, acc 0.796875, prec 0.0360248, recall 0.789529
2017-12-10T13:20:06.819427: step 1032, loss 1.18688, acc 0.703125, prec 0.0359922, recall 0.789529
2017-12-10T13:20:07.262794: step 1033, loss 0.954897, acc 0.75, prec 0.0360567, recall 0.789969
2017-12-10T13:20:07.711813: step 1034, loss 0.938442, acc 0.765625, prec 0.0360768, recall 0.790188
2017-12-10T13:20:08.181222: step 1035, loss 0.98448, acc 0.78125, prec 0.0360987, recall 0.790407
2017-12-10T13:20:08.641425: step 1036, loss 0.976712, acc 0.734375, prec 0.0361153, recall 0.790625
2017-12-10T13:20:09.095308: step 1037, loss 0.884912, acc 0.765625, prec 0.0362271, recall 0.791277
2017-12-10T13:20:09.530008: step 1038, loss 0.721397, acc 0.8125, prec 0.0362064, recall 0.791277
2017-12-10T13:20:09.986054: step 1039, loss 0.555823, acc 0.828125, prec 0.0361875, recall 0.791277
2017-12-10T13:20:10.429246: step 1040, loss 0.326154, acc 0.875, prec 0.0361737, recall 0.791277
2017-12-10T13:20:10.856451: step 1041, loss 0.728425, acc 0.796875, prec 0.0362429, recall 0.79171
2017-12-10T13:20:11.318234: step 1042, loss 0.519, acc 0.84375, prec 0.0362714, recall 0.791925
2017-12-10T13:20:11.750729: step 1043, loss 0.535367, acc 0.859375, prec 0.0363473, recall 0.792355
2017-12-10T13:20:12.184453: step 1044, loss 0.674092, acc 0.8125, prec 0.0363266, recall 0.792355
2017-12-10T13:20:12.638689: step 1045, loss 0.548893, acc 0.828125, prec 0.0363533, recall 0.79257
2017-12-10T13:20:13.087175: step 1046, loss 0.395162, acc 0.859375, prec 0.0363378, recall 0.79257
2017-12-10T13:20:13.537161: step 1047, loss 0.42034, acc 0.90625, prec 0.0363275, recall 0.79257
2017-12-10T13:20:13.979553: step 1048, loss 9.2645, acc 0.90625, prec 0.0363645, recall 0.791967
2017-12-10T13:20:14.437057: step 1049, loss 2.8634, acc 0.90625, prec 0.0364015, recall 0.791367
2017-12-10T13:20:14.887010: step 1050, loss 1.04283, acc 0.859375, prec 0.0364315, recall 0.791581
2017-12-10T13:20:15.321802: step 1051, loss 0.813796, acc 0.78125, prec 0.0364074, recall 0.791581
2017-12-10T13:20:15.769457: step 1052, loss 1.04145, acc 0.859375, prec 0.0364829, recall 0.792008
2017-12-10T13:20:16.222795: step 1053, loss 0.478317, acc 0.859375, prec 0.0364674, recall 0.792008
2017-12-10T13:20:16.667497: step 1054, loss 0.604278, acc 0.84375, prec 0.0364502, recall 0.792008
2017-12-10T13:20:17.114659: step 1055, loss 0.514551, acc 0.84375, prec 0.036433, recall 0.792008
2017-12-10T13:20:17.558756: step 1056, loss 2.04673, acc 0.8125, prec 0.0364142, recall 0.791198
2017-12-10T13:20:18.019683: step 1057, loss 0.763278, acc 0.796875, prec 0.0364372, recall 0.791411
2017-12-10T13:20:18.474780: step 1058, loss 0.667758, acc 0.796875, prec 0.036415, recall 0.791411
2017-12-10T13:20:18.910245: step 1059, loss 1.3241, acc 0.84375, prec 0.0366244, recall 0.792472
2017-12-10T13:20:19.357000: step 1060, loss 1.103, acc 0.671875, prec 0.0365882, recall 0.792472
2017-12-10T13:20:19.802274: step 1061, loss 1.50102, acc 0.75, prec 0.0365608, recall 0.792472
2017-12-10T13:20:20.256534: step 1062, loss 0.589218, acc 0.796875, prec 0.0365837, recall 0.792683
2017-12-10T13:20:20.700618: step 1063, loss 1.15663, acc 0.65625, prec 0.0365911, recall 0.792893
2017-12-10T13:20:21.151088: step 1064, loss 1.43569, acc 0.640625, prec 0.0365517, recall 0.792893
2017-12-10T13:20:21.607939: step 1065, loss 1.28926, acc 0.765625, prec 0.0365711, recall 0.793103
2017-12-10T13:20:22.049169: step 1066, loss 0.799268, acc 0.78125, prec 0.0365472, recall 0.793103
2017-12-10T13:20:22.492526: step 1067, loss 1.40581, acc 0.6875, prec 0.036558, recall 0.793313
2017-12-10T13:20:22.938582: step 1068, loss 0.46521, acc 0.84375, prec 0.0365859, recall 0.793522
2017-12-10T13:20:23.385794: step 1069, loss 0.616199, acc 0.75, prec 0.0365586, recall 0.793522
2017-12-10T13:20:23.840633: step 1070, loss 0.828859, acc 0.765625, prec 0.0365331, recall 0.793522
2017-12-10T13:20:24.301417: step 1071, loss 0.792377, acc 0.78125, prec 0.0366439, recall 0.794147
2017-12-10T13:20:24.748416: step 1072, loss 1.5075, acc 0.75, prec 0.0367062, recall 0.794562
2017-12-10T13:20:25.186646: step 1073, loss 0.849076, acc 0.8125, prec 0.0367753, recall 0.794975
2017-12-10T13:20:25.625940: step 1074, loss 0.669913, acc 0.796875, prec 0.0367531, recall 0.794975
2017-12-10T13:20:26.073978: step 1075, loss 0.513907, acc 0.890625, prec 0.0367411, recall 0.794975
2017-12-10T13:20:26.518634: step 1076, loss 0.848336, acc 0.734375, prec 0.0368463, recall 0.795591
2017-12-10T13:20:26.961028: step 1077, loss 0.648913, acc 0.875, prec 0.0368773, recall 0.795796
2017-12-10T13:20:27.414368: step 1078, loss 0.47814, acc 0.828125, prec 0.0369478, recall 0.796204
2017-12-10T13:20:27.856389: step 1079, loss 0.777662, acc 0.8125, prec 0.0369272, recall 0.796204
2017-12-10T13:20:28.299721: step 1080, loss 8.39063, acc 0.875, prec 0.0369598, recall 0.795613
2017-12-10T13:20:28.740376: step 1081, loss 0.285527, acc 0.890625, prec 0.0369479, recall 0.795613
2017-12-10T13:20:29.190168: step 1082, loss 6.91098, acc 0.84375, prec 0.0369788, recall 0.794235
2017-12-10T13:20:29.649405: step 1083, loss 0.730123, acc 0.765625, prec 0.0369976, recall 0.794439
2017-12-10T13:20:30.090678: step 1084, loss 0.515323, acc 0.84375, prec 0.0370251, recall 0.794643
2017-12-10T13:20:30.540306: step 1085, loss 0.534814, acc 0.890625, prec 0.0371021, recall 0.795049
2017-12-10T13:20:30.978293: step 1086, loss 0.698169, acc 0.765625, prec 0.0370764, recall 0.795049
2017-12-10T13:20:31.426368: step 1087, loss 1.60526, acc 0.578125, prec 0.0370302, recall 0.795049
2017-12-10T13:20:31.868855: step 1088, loss 0.957983, acc 0.75, prec 0.0370029, recall 0.795049
2017-12-10T13:20:32.311873: step 1089, loss 1.19753, acc 0.734375, prec 0.0369739, recall 0.795049
2017-12-10T13:20:32.746060: step 1090, loss 1.5732, acc 0.671875, prec 0.0369382, recall 0.795049
2017-12-10T13:20:33.179575: step 1091, loss 1.15982, acc 0.71875, prec 0.0369077, recall 0.795049
2017-12-10T13:20:33.617272: step 1092, loss 0.757228, acc 0.765625, prec 0.0369707, recall 0.795455
2017-12-10T13:20:34.073447: step 1093, loss 0.876616, acc 0.765625, prec 0.0369452, recall 0.795455
2017-12-10T13:20:34.531301: step 1094, loss 1.0648, acc 0.71875, prec 0.0369148, recall 0.795455
2017-12-10T13:20:34.974693: step 1095, loss 0.580747, acc 0.796875, prec 0.0369369, recall 0.795656
2017-12-10T13:20:35.421934: step 1096, loss 0.646868, acc 0.8125, prec 0.0369166, recall 0.795656
2017-12-10T13:20:35.866227: step 1097, loss 0.578241, acc 0.8125, prec 0.0369404, recall 0.795858
2017-12-10T13:20:36.317097: step 1098, loss 0.635833, acc 0.828125, prec 0.0369659, recall 0.796059
2017-12-10T13:20:36.766232: step 1099, loss 0.270085, acc 0.90625, prec 0.0369998, recall 0.79626
2017-12-10T13:20:37.211779: step 1100, loss 7.6505, acc 0.8125, prec 0.0369812, recall 0.795477
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1100

2017-12-10T13:20:39.098868: step 1101, loss 0.735756, acc 0.8125, prec 0.0370929, recall 0.796078
2017-12-10T13:20:39.548390: step 1102, loss 0.63249, acc 0.828125, prec 0.0371182, recall 0.796278
2017-12-10T13:20:40.003414: step 1103, loss 0.486091, acc 0.875, prec 0.0371047, recall 0.796278
2017-12-10T13:20:40.448196: step 1104, loss 0.533371, acc 0.859375, prec 0.0371333, recall 0.796477
2017-12-10T13:20:40.888597: step 1105, loss 2.40375, acc 0.796875, prec 0.0371569, recall 0.795898
2017-12-10T13:20:41.338862: step 1106, loss 0.509033, acc 0.84375, prec 0.0371839, recall 0.796098
2017-12-10T13:20:41.778001: step 1107, loss 0.596766, acc 0.8125, prec 0.0371635, recall 0.796098
2017-12-10T13:20:42.221365: step 1108, loss 0.379889, acc 0.828125, prec 0.0371449, recall 0.796098
2017-12-10T13:20:42.663799: step 1109, loss 0.261551, acc 0.953125, prec 0.0371837, recall 0.796296
2017-12-10T13:20:43.125704: step 1110, loss 0.919524, acc 0.875, prec 0.037214, recall 0.796495
2017-12-10T13:20:43.568237: step 1111, loss 0.644738, acc 0.8125, prec 0.037325, recall 0.797087
2017-12-10T13:20:44.013357: step 1112, loss 1.83118, acc 0.765625, prec 0.037345, recall 0.796512
2017-12-10T13:20:44.448276: step 1113, loss 0.887983, acc 0.84375, prec 0.0374154, recall 0.796905
2017-12-10T13:20:44.903063: step 1114, loss 0.433362, acc 0.84375, prec 0.0374421, recall 0.797101
2017-12-10T13:20:45.338907: step 1115, loss 0.499837, acc 0.8125, prec 0.0374218, recall 0.797101
2017-12-10T13:20:45.775780: step 1116, loss 0.303413, acc 0.890625, prec 0.0374972, recall 0.797493
2017-12-10T13:20:46.238560: step 1117, loss 0.517081, acc 0.90625, prec 0.0375306, recall 0.797688
2017-12-10T13:20:46.701061: step 1118, loss 0.591122, acc 0.84375, prec 0.0375136, recall 0.797688
2017-12-10T13:20:47.139200: step 1119, loss 0.70726, acc 0.8125, prec 0.0374932, recall 0.797688
2017-12-10T13:20:47.580406: step 1120, loss 1.09036, acc 0.75, prec 0.0376402, recall 0.798464
2017-12-10T13:20:48.034209: step 1121, loss 0.688501, acc 0.84375, prec 0.0377103, recall 0.798851
2017-12-10T13:20:48.484603: step 1122, loss 0.641653, acc 0.8125, prec 0.0376898, recall 0.798851
2017-12-10T13:20:48.932838: step 1123, loss 0.909691, acc 0.90625, prec 0.0377665, recall 0.799235
2017-12-10T13:20:49.384778: step 1124, loss 1.45018, acc 0.8125, prec 0.0377895, recall 0.799427
2017-12-10T13:20:49.835269: step 1125, loss 0.521544, acc 0.859375, prec 0.0378176, recall 0.799618
2017-12-10T13:20:50.283820: step 1126, loss 1.42818, acc 0.765625, prec 0.0378788, recall 0.8
2017-12-10T13:20:50.723885: step 1127, loss 0.349752, acc 0.875, prec 0.0378651, recall 0.8
2017-12-10T13:20:51.173886: step 1128, loss 0.683697, acc 0.78125, prec 0.0379279, recall 0.80038
2017-12-10T13:20:51.643692: step 1129, loss 0.369559, acc 0.875, prec 0.0379576, recall 0.80057
2017-12-10T13:20:52.093183: step 1130, loss 0.42722, acc 0.875, prec 0.0379872, recall 0.800759
2017-12-10T13:20:52.545135: step 1131, loss 0.31344, acc 0.859375, prec 0.0379718, recall 0.800759
2017-12-10T13:20:52.979056: step 1132, loss 0.655488, acc 0.828125, prec 0.0380396, recall 0.801136
2017-12-10T13:20:53.434518: step 1133, loss 0.44515, acc 0.859375, prec 0.0380674, recall 0.801324
2017-12-10T13:20:53.882489: step 1134, loss 0.490161, acc 0.8125, prec 0.0380469, recall 0.801324
2017-12-10T13:20:54.352148: step 1135, loss 0.558106, acc 0.890625, prec 0.0380349, recall 0.801324
2017-12-10T13:20:54.789694: step 1136, loss 0.437538, acc 0.875, prec 0.0380213, recall 0.801324
2017-12-10T13:20:55.232462: step 1137, loss 0.387325, acc 0.859375, prec 0.0380922, recall 0.8017
2017-12-10T13:20:55.663636: step 1138, loss 0.955149, acc 0.90625, prec 0.0381683, recall 0.802074
2017-12-10T13:20:56.115356: step 1139, loss 0.16587, acc 0.9375, prec 0.0382046, recall 0.80226
2017-12-10T13:20:56.571304: step 1140, loss 0.228202, acc 0.921875, prec 0.0383253, recall 0.802817
2017-12-10T13:20:57.035329: step 1141, loss 0.428881, acc 0.96875, prec 0.038365, recall 0.803002
2017-12-10T13:20:57.485280: step 1142, loss 0.304885, acc 0.953125, prec 0.038446, recall 0.803371
2017-12-10T13:20:57.939469: step 1143, loss 2.82612, acc 0.9375, prec 0.0384839, recall 0.802804
2017-12-10T13:20:58.392827: step 1144, loss 0.193193, acc 0.90625, prec 0.0384736, recall 0.802804
2017-12-10T13:20:58.836629: step 1145, loss 1.52147, acc 0.921875, prec 0.0384667, recall 0.802054
2017-12-10T13:20:59.289788: step 1146, loss 0.14843, acc 0.9375, prec 0.0384598, recall 0.802054
2017-12-10T13:20:59.732657: step 1147, loss 0.323545, acc 0.890625, prec 0.0384478, recall 0.802054
2017-12-10T13:21:00.176187: step 1148, loss 0.440515, acc 0.875, prec 0.038477, recall 0.802239
2017-12-10T13:21:00.617802: step 1149, loss 0.357707, acc 0.921875, prec 0.0385114, recall 0.802423
2017-12-10T13:21:01.062453: step 1150, loss 0.654761, acc 0.90625, prec 0.0385871, recall 0.802791
2017-12-10T13:21:01.501001: step 1151, loss 0.299129, acc 0.859375, prec 0.0386145, recall 0.802974
2017-12-10T13:21:01.951871: step 1152, loss 0.760127, acc 0.78125, prec 0.0385904, recall 0.802974
2017-12-10T13:21:02.401893: step 1153, loss 0.940813, acc 0.8125, prec 0.0386555, recall 0.80334
2017-12-10T13:21:02.853998: step 1154, loss 0.865179, acc 0.78125, prec 0.0386314, recall 0.80334
2017-12-10T13:21:03.325774: step 1155, loss 4.62855, acc 0.875, prec 0.0387051, recall 0.80296
2017-12-10T13:21:03.785618: step 1156, loss 1.0885, acc 0.78125, prec 0.0387238, recall 0.803142
2017-12-10T13:21:04.236349: step 1157, loss 0.409477, acc 0.859375, prec 0.0387082, recall 0.803142
2017-12-10T13:21:04.683759: step 1158, loss 0.308369, acc 0.859375, prec 0.0387355, recall 0.803324
2017-12-10T13:21:05.119819: step 1159, loss 0.509158, acc 0.828125, prec 0.0387166, recall 0.803324
2017-12-10T13:21:05.567714: step 1160, loss 0.616954, acc 0.828125, prec 0.0387404, recall 0.803506
2017-12-10T13:21:06.025318: step 1161, loss 0.861647, acc 0.703125, prec 0.0387931, recall 0.803867
2017-12-10T13:21:06.472603: step 1162, loss 1.13206, acc 0.703125, prec 0.0387604, recall 0.803867
2017-12-10T13:21:06.917328: step 1163, loss 0.868391, acc 0.78125, prec 0.0387363, recall 0.803867
2017-12-10T13:21:07.360137: step 1164, loss 0.680426, acc 0.78125, prec 0.0387549, recall 0.804048
2017-12-10T13:21:07.809840: step 1165, loss 0.636979, acc 0.78125, prec 0.0387308, recall 0.804048
2017-12-10T13:21:08.256029: step 1166, loss 0.611789, acc 0.765625, prec 0.0387477, recall 0.804228
2017-12-10T13:21:08.727823: step 1167, loss 0.796848, acc 0.8125, prec 0.0388122, recall 0.804587
2017-12-10T13:21:09.170472: step 1168, loss 0.310249, acc 0.890625, prec 0.0388002, recall 0.804587
2017-12-10T13:21:09.621050: step 1169, loss 0.57841, acc 0.859375, prec 0.0387847, recall 0.804587
2017-12-10T13:21:10.059195: step 1170, loss 5.53956, acc 0.9375, prec 0.0387796, recall 0.80385
2017-12-10T13:21:10.513053: step 1171, loss 0.231987, acc 0.90625, prec 0.0387693, recall 0.80385
2017-12-10T13:21:10.958334: step 1172, loss 0.534195, acc 0.859375, prec 0.0387963, recall 0.804029
2017-12-10T13:21:11.411480: step 1173, loss 0.233678, acc 0.90625, prec 0.0388285, recall 0.804209
2017-12-10T13:21:11.861556: step 1174, loss 0.335183, acc 0.875, prec 0.0388148, recall 0.804209
2017-12-10T13:21:12.312557: step 1175, loss 0.469381, acc 0.875, prec 0.0388435, recall 0.804388
2017-12-10T13:21:12.781304: step 1176, loss 0.476329, acc 0.875, prec 0.0388298, recall 0.804388
2017-12-10T13:21:13.236301: step 1177, loss 0.268555, acc 0.921875, prec 0.0388636, recall 0.804566
2017-12-10T13:21:13.682139: step 1178, loss 0.227744, acc 0.890625, prec 0.038894, recall 0.804745
2017-12-10T13:21:14.134929: step 1179, loss 1.43806, acc 0.90625, prec 0.0388855, recall 0.804011
2017-12-10T13:21:14.583957: step 1180, loss 0.612933, acc 0.875, prec 0.0389565, recall 0.804368
2017-12-10T13:21:15.028604: step 1181, loss 1.23653, acc 0.875, prec 0.0390274, recall 0.804723
2017-12-10T13:21:15.470662: step 1182, loss 9.73307, acc 0.90625, prec 0.0390188, recall 0.803993
2017-12-10T13:21:15.925177: step 1183, loss 0.912553, acc 0.90625, prec 0.0390508, recall 0.80417
2017-12-10T13:21:16.367371: step 1184, loss 0.521124, acc 0.859375, prec 0.0391622, recall 0.804702
2017-12-10T13:21:16.826205: step 1185, loss 0.364255, acc 0.859375, prec 0.0391467, recall 0.804702
2017-12-10T13:21:17.266371: step 1186, loss 5.2929, acc 0.765625, prec 0.0391648, recall 0.804152
2017-12-10T13:21:17.714174: step 1187, loss 1.13207, acc 0.71875, prec 0.0392183, recall 0.804505
2017-12-10T13:21:18.165184: step 1188, loss 1.39305, acc 0.71875, prec 0.0392295, recall 0.80468
2017-12-10T13:21:18.614184: step 1189, loss 1.33884, acc 0.65625, prec 0.0392337, recall 0.804856
2017-12-10T13:21:19.057365: step 1190, loss 1.6001, acc 0.640625, prec 0.0392363, recall 0.805031
2017-12-10T13:21:19.497413: step 1191, loss 1.36249, acc 0.65625, prec 0.0391985, recall 0.805031
2017-12-10T13:21:19.959614: step 1192, loss 1.46441, acc 0.609375, prec 0.0391977, recall 0.805206
2017-12-10T13:21:20.400328: step 1193, loss 1.45431, acc 0.671875, prec 0.0391618, recall 0.805206
2017-12-10T13:21:20.840495: step 1194, loss 3.88457, acc 0.515625, prec 0.0391943, recall 0.804834
2017-12-10T13:21:21.290864: step 1195, loss 1.6587, acc 0.5625, prec 0.0391884, recall 0.805009
2017-12-10T13:21:21.746346: step 1196, loss 0.976829, acc 0.703125, prec 0.0392396, recall 0.805357
2017-12-10T13:21:22.182479: step 1197, loss 1.34665, acc 0.609375, prec 0.0391969, recall 0.805357
2017-12-10T13:21:22.616757: step 1198, loss 1.30456, acc 0.71875, prec 0.0391663, recall 0.805357
2017-12-10T13:21:23.056268: step 1199, loss 1.43191, acc 0.65625, prec 0.0391289, recall 0.805357
2017-12-10T13:21:23.528561: step 1200, loss 0.929892, acc 0.734375, prec 0.0391417, recall 0.805531
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1200

2017-12-10T13:21:25.604301: step 1201, loss 0.924716, acc 0.75, prec 0.0392395, recall 0.80605
2017-12-10T13:21:26.042505: step 1202, loss 0.666381, acc 0.78125, prec 0.0392157, recall 0.80605
2017-12-10T13:21:26.502545: step 1203, loss 0.652626, acc 0.8125, prec 0.0392369, recall 0.806222
2017-12-10T13:21:26.953791: step 1204, loss 0.836026, acc 0.75, prec 0.0392098, recall 0.806222
2017-12-10T13:21:27.395313: step 1205, loss 0.872648, acc 0.78125, prec 0.0392275, recall 0.806394
2017-12-10T13:21:27.846950: step 1206, loss 0.684615, acc 0.828125, prec 0.0392504, recall 0.806566
2017-12-10T13:21:28.283994: step 1207, loss 0.35004, acc 0.90625, prec 0.0392817, recall 0.806738
2017-12-10T13:21:28.731457: step 1208, loss 0.286013, acc 0.921875, prec 0.0393147, recall 0.806909
2017-12-10T13:21:29.179814: step 1209, loss 18.9725, acc 0.890625, prec 0.039472, recall 0.806167
2017-12-10T13:21:29.649654: step 1210, loss 0.598673, acc 0.859375, prec 0.0394981, recall 0.806338
2017-12-10T13:21:30.103353: step 1211, loss 0.340222, acc 0.875, prec 0.0394845, recall 0.806338
2017-12-10T13:21:30.559012: step 1212, loss 0.549698, acc 0.859375, prec 0.0395519, recall 0.806678
2017-12-10T13:21:31.021422: step 1213, loss 0.300739, acc 0.90625, prec 0.0396244, recall 0.807018
2017-12-10T13:21:31.476905: step 1214, loss 0.492281, acc 0.875, prec 0.0396108, recall 0.807018
2017-12-10T13:21:31.918999: step 1215, loss 0.908216, acc 0.75, prec 0.0395835, recall 0.807018
2017-12-10T13:21:32.347722: step 1216, loss 0.45187, acc 0.8125, prec 0.0395631, recall 0.807018
2017-12-10T13:21:32.793794: step 1217, loss 5.97132, acc 0.796875, prec 0.0395427, recall 0.80631
2017-12-10T13:21:33.247045: step 1218, loss 0.638346, acc 0.828125, prec 0.0395653, recall 0.80648
2017-12-10T13:21:33.700061: step 1219, loss 0.561347, acc 0.8125, prec 0.0395449, recall 0.80648
2017-12-10T13:21:34.141214: step 1220, loss 1.11078, acc 0.671875, prec 0.0395092, recall 0.80648
2017-12-10T13:21:34.588553: step 1221, loss 0.924185, acc 0.8125, prec 0.0395713, recall 0.806818
2017-12-10T13:21:35.041963: step 1222, loss 1.22848, acc 0.734375, prec 0.0395425, recall 0.806818
2017-12-10T13:21:35.487894: step 1223, loss 0.310551, acc 0.859375, prec 0.0395272, recall 0.806818
2017-12-10T13:21:35.940797: step 1224, loss 0.642684, acc 0.8125, prec 0.0395069, recall 0.806818
2017-12-10T13:21:36.395891: step 1225, loss 1.39619, acc 0.828125, prec 0.0395705, recall 0.807155
2017-12-10T13:21:36.838145: step 1226, loss 0.907445, acc 0.796875, prec 0.0395485, recall 0.807155
2017-12-10T13:21:37.275466: step 1227, loss 0.4169, acc 0.90625, prec 0.0395384, recall 0.807155
2017-12-10T13:21:37.714750: step 1228, loss 0.660863, acc 0.796875, prec 0.0395164, recall 0.807155
2017-12-10T13:21:38.149191: step 1229, loss 1.75488, acc 0.8125, prec 0.0394978, recall 0.806452
2017-12-10T13:21:38.593529: step 1230, loss 5.70013, acc 0.671875, prec 0.0394658, recall 0.805048
2017-12-10T13:21:39.058377: step 1231, loss 0.561417, acc 0.828125, prec 0.0394883, recall 0.805217
2017-12-10T13:21:39.510315: step 1232, loss 0.90027, acc 0.75, prec 0.0395432, recall 0.805556
2017-12-10T13:21:39.953454: step 1233, loss 0.986847, acc 0.71875, prec 0.0395538, recall 0.805724
2017-12-10T13:21:40.386042: step 1234, loss 0.816819, acc 0.75, prec 0.0395677, recall 0.805893
2017-12-10T13:21:40.844111: step 1235, loss 1.287, acc 0.703125, prec 0.0396991, recall 0.806563
2017-12-10T13:21:41.284400: step 1236, loss 0.666571, acc 0.78125, prec 0.0397163, recall 0.80673
2017-12-10T13:21:41.744989: step 1237, loss 1.24887, acc 0.6875, prec 0.0396825, recall 0.80673
2017-12-10T13:21:42.194627: step 1238, loss 0.972367, acc 0.75, prec 0.0397371, recall 0.807063
2017-12-10T13:21:42.637330: step 1239, loss 1.1701, acc 0.6875, prec 0.0397034, recall 0.807063
2017-12-10T13:21:43.102555: step 1240, loss 0.686009, acc 0.75, prec 0.0396765, recall 0.807063
2017-12-10T13:21:43.547380: step 1241, loss 1.92692, acc 0.765625, prec 0.0397343, recall 0.806701
2017-12-10T13:21:43.997942: step 1242, loss 1.11665, acc 0.703125, prec 0.0398241, recall 0.807198
2017-12-10T13:21:44.442488: step 1243, loss 0.780037, acc 0.796875, prec 0.0398023, recall 0.807198
2017-12-10T13:21:44.895633: step 1244, loss 0.716514, acc 0.765625, prec 0.0398987, recall 0.807692
2017-12-10T13:21:45.334357: step 1245, loss 0.656488, acc 0.8125, prec 0.0398785, recall 0.807692
2017-12-10T13:21:45.790285: step 1246, loss 0.847816, acc 0.796875, prec 0.0398971, recall 0.807857
2017-12-10T13:21:46.247546: step 1247, loss 1.07859, acc 0.765625, prec 0.0398719, recall 0.807857
2017-12-10T13:21:46.722476: step 1248, loss 0.591099, acc 0.796875, prec 0.03985, recall 0.807857
2017-12-10T13:21:47.164290: step 1249, loss 0.789627, acc 0.828125, prec 0.0398316, recall 0.807857
2017-12-10T13:21:47.608048: step 1250, loss 1.4964, acc 0.9375, prec 0.039867, recall 0.807332
2017-12-10T13:21:48.077859: step 1251, loss 0.425258, acc 0.8125, prec 0.0398468, recall 0.807332
2017-12-10T13:21:48.521498: step 1252, loss 0.215319, acc 0.90625, prec 0.0398772, recall 0.807496
2017-12-10T13:21:48.967599: step 1253, loss 0.331662, acc 0.90625, prec 0.0399075, recall 0.80766
2017-12-10T13:21:49.419958: step 1254, loss 0.808235, acc 0.796875, prec 0.039926, recall 0.807823
2017-12-10T13:21:49.866799: step 1255, loss 0.365149, acc 0.90625, prec 0.0399563, recall 0.807986
2017-12-10T13:21:50.320252: step 1256, loss 0.339776, acc 0.90625, prec 0.0399462, recall 0.807986
2017-12-10T13:21:50.771740: step 1257, loss 0.288892, acc 0.90625, prec 0.0399362, recall 0.807986
2017-12-10T13:21:51.235026: step 1258, loss 0.0871532, acc 0.96875, prec 0.0399328, recall 0.807986
2017-12-10T13:21:51.675616: step 1259, loss 7.77338, acc 0.90625, prec 0.0399244, recall 0.807301
2017-12-10T13:21:52.126120: step 1260, loss 0.342607, acc 0.90625, prec 0.0399144, recall 0.807301
2017-12-10T13:21:52.583764: step 1261, loss 0.455259, acc 0.890625, prec 0.0399027, recall 0.807301
2017-12-10T13:21:53.022777: step 1262, loss 0.618323, acc 0.84375, prec 0.0399262, recall 0.807464
2017-12-10T13:21:53.472133: step 1263, loss 0.449338, acc 0.890625, prec 0.0399547, recall 0.807627
2017-12-10T13:21:53.907818: step 1264, loss 0.379139, acc 0.921875, prec 0.0399866, recall 0.80779
2017-12-10T13:21:54.338733: step 1265, loss 0.142767, acc 0.9375, prec 0.0400201, recall 0.807953
2017-12-10T13:21:54.777639: step 1266, loss 0.332298, acc 0.90625, prec 0.0400503, recall 0.808115
2017-12-10T13:21:55.240624: step 1267, loss 0.559368, acc 0.875, prec 0.0400369, recall 0.808115
2017-12-10T13:21:55.699653: step 1268, loss 0.858801, acc 0.84375, prec 0.0400603, recall 0.808277
2017-12-10T13:21:56.151969: step 1269, loss 1.49565, acc 0.90625, prec 0.0401707, recall 0.808762
2017-12-10T13:21:56.601663: step 1270, loss 1.38925, acc 0.890625, prec 0.0402393, recall 0.809083
2017-12-10T13:21:57.061854: step 1271, loss 0.293927, acc 0.890625, prec 0.0402275, recall 0.809083
2017-12-10T13:21:57.504166: step 1272, loss 4.05209, acc 0.78125, prec 0.0402056, recall 0.808403
2017-12-10T13:21:57.945268: step 1273, loss 0.265194, acc 0.90625, prec 0.0402356, recall 0.808564
2017-12-10T13:21:58.393147: step 1274, loss 0.66318, acc 0.84375, prec 0.040299, recall 0.808885
2017-12-10T13:21:58.842109: step 1275, loss 0.809726, acc 0.78125, prec 0.0402755, recall 0.808885
2017-12-10T13:21:59.287521: step 1276, loss 0.693725, acc 0.765625, prec 0.0402903, recall 0.809045
2017-12-10T13:21:59.732676: step 1277, loss 0.604687, acc 0.84375, prec 0.0403535, recall 0.809365
2017-12-10T13:22:00.179556: step 1278, loss 0.81684, acc 0.828125, prec 0.040415, recall 0.809683
2017-12-10T13:22:00.619244: step 1279, loss 1.11141, acc 0.703125, prec 0.0404629, recall 0.81
2017-12-10T13:22:01.064269: step 1280, loss 2.21479, acc 0.734375, prec 0.0405557, recall 0.809801
2017-12-10T13:22:01.510325: step 1281, loss 1.26088, acc 0.734375, prec 0.0405669, recall 0.809959
2017-12-10T13:22:01.973934: step 1282, loss 0.763255, acc 0.828125, prec 0.0405484, recall 0.809959
2017-12-10T13:22:02.438652: step 1283, loss 1.35399, acc 0.703125, prec 0.0405562, recall 0.810116
2017-12-10T13:22:02.895457: step 1284, loss 2.07203, acc 0.734375, prec 0.0405674, recall 0.810273
2017-12-10T13:22:03.334749: step 1285, loss 1.1633, acc 0.8125, prec 0.040587, recall 0.81043
2017-12-10T13:22:03.783132: step 1286, loss 1.70651, acc 0.78125, prec 0.0406032, recall 0.810587
2017-12-10T13:22:04.236201: step 1287, loss 0.964576, acc 0.8125, prec 0.0406228, recall 0.810744
2017-12-10T13:22:04.670565: step 1288, loss 0.961422, acc 0.71875, prec 0.0405925, recall 0.810744
2017-12-10T13:22:05.123569: step 1289, loss 1.10403, acc 0.75, prec 0.0406847, recall 0.811212
2017-12-10T13:22:05.581947: step 1290, loss 1.00189, acc 0.78125, prec 0.0406612, recall 0.811212
2017-12-10T13:22:06.050176: step 1291, loss 2.39087, acc 0.75, prec 0.040636, recall 0.810544
2017-12-10T13:22:06.496223: step 1292, loss 0.789205, acc 0.734375, prec 0.0406075, recall 0.810544
2017-12-10T13:22:06.934109: step 1293, loss 0.921641, acc 0.796875, prec 0.0405857, recall 0.810544
2017-12-10T13:22:07.381868: step 1294, loss 1.23997, acc 0.703125, prec 0.0405539, recall 0.810544
2017-12-10T13:22:07.824747: step 1295, loss 1.01185, acc 0.75, prec 0.0405667, recall 0.8107
2017-12-10T13:22:08.272335: step 1296, loss 0.765672, acc 0.75, prec 0.04054, recall 0.8107
2017-12-10T13:22:08.714930: step 1297, loss 0.775285, acc 0.796875, prec 0.0405183, recall 0.8107
2017-12-10T13:22:09.161705: step 1298, loss 0.636325, acc 0.828125, prec 0.0405, recall 0.8107
2017-12-10T13:22:09.599597: step 1299, loss 1.07659, acc 0.84375, prec 0.0405622, recall 0.811011
2017-12-10T13:22:10.046835: step 1300, loss 1.19396, acc 0.6875, prec 0.0405683, recall 0.811166
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1300

2017-12-10T13:22:12.011602: step 1301, loss 0.24254, acc 0.890625, prec 0.0406354, recall 0.811475
2017-12-10T13:22:12.468407: step 1302, loss 0.560182, acc 0.84375, prec 0.0406581, recall 0.81163
2017-12-10T13:22:12.908104: step 1303, loss 0.887184, acc 0.828125, prec 0.0406397, recall 0.81163
2017-12-10T13:22:13.348606: step 1304, loss 0.377822, acc 0.890625, prec 0.0406674, recall 0.811784
2017-12-10T13:22:13.786646: step 1305, loss 1.06496, acc 0.921875, prec 0.0407377, recall 0.812092
2017-12-10T13:22:14.238255: step 1306, loss 0.223983, acc 0.890625, prec 0.040726, recall 0.812092
2017-12-10T13:22:14.681920: step 1307, loss 0.69414, acc 0.828125, prec 0.0407862, recall 0.812398
2017-12-10T13:22:15.128552: step 1308, loss 0.691953, acc 0.921875, prec 0.0408957, recall 0.812856
2017-12-10T13:22:15.576800: step 1309, loss 0.157274, acc 0.90625, prec 0.0408856, recall 0.812856
2017-12-10T13:22:16.040123: step 1310, loss 1.49734, acc 0.828125, prec 0.0408689, recall 0.812195
2017-12-10T13:22:16.494161: step 1311, loss 0.445655, acc 0.90625, prec 0.0408589, recall 0.812195
2017-12-10T13:22:16.938295: step 1312, loss 0.841789, acc 0.828125, prec 0.0408797, recall 0.812348
2017-12-10T13:22:17.387307: step 1313, loss 0.488181, acc 0.859375, prec 0.0408647, recall 0.812348
2017-12-10T13:22:17.831124: step 1314, loss 0.524137, acc 0.859375, prec 0.0408497, recall 0.812348
2017-12-10T13:22:18.276963: step 1315, loss 0.23445, acc 0.875, prec 0.0408363, recall 0.812348
2017-12-10T13:22:18.732190: step 1316, loss 1.53519, acc 0.890625, prec 0.0408655, recall 0.811841
2017-12-10T13:22:19.203938: step 1317, loss 1.57491, acc 0.84375, prec 0.0408505, recall 0.811183
2017-12-10T13:22:19.652542: step 1318, loss 3.69966, acc 0.890625, prec 0.0408421, recall 0.809871
2017-12-10T13:22:20.103352: step 1319, loss 0.426852, acc 0.859375, prec 0.0409054, recall 0.810178
2017-12-10T13:22:20.535266: step 1320, loss 3.46497, acc 0.90625, prec 0.040897, recall 0.809524
2017-12-10T13:22:20.986495: step 1321, loss 0.575221, acc 0.875, prec 0.0408837, recall 0.809524
2017-12-10T13:22:21.427576: step 1322, loss 1.16329, acc 0.78125, prec 0.0408995, recall 0.809677
2017-12-10T13:22:21.896947: step 1323, loss 1.03541, acc 0.734375, prec 0.0409102, recall 0.809831
2017-12-10T13:22:22.348420: step 1324, loss 0.864851, acc 0.75, prec 0.0409226, recall 0.809984
2017-12-10T13:22:22.778544: step 1325, loss 1.0148, acc 0.703125, prec 0.04093, recall 0.810137
2017-12-10T13:22:23.217803: step 1326, loss 1.40654, acc 0.703125, prec 0.0409373, recall 0.810289
2017-12-10T13:22:23.653155: step 1327, loss 2.66872, acc 0.703125, prec 0.0409464, recall 0.809791
2017-12-10T13:22:24.079187: step 1328, loss 1.45058, acc 0.609375, prec 0.0409437, recall 0.809944
2017-12-10T13:22:24.536943: step 1329, loss 1.23491, acc 0.703125, prec 0.0409511, recall 0.810096
2017-12-10T13:22:24.974861: step 1330, loss 1.48405, acc 0.671875, prec 0.0409939, recall 0.8104
2017-12-10T13:22:25.425382: step 1331, loss 1.21202, acc 0.734375, prec 0.0409657, recall 0.8104
2017-12-10T13:22:25.873724: step 1332, loss 1.05443, acc 0.75, prec 0.0409392, recall 0.8104
2017-12-10T13:22:26.321756: step 1333, loss 0.644086, acc 0.84375, prec 0.0409614, recall 0.810552
2017-12-10T13:22:26.767192: step 1334, loss 1.40367, acc 0.6875, prec 0.0409671, recall 0.810703
2017-12-10T13:22:27.227437: step 1335, loss 0.910662, acc 0.78125, prec 0.0409826, recall 0.810854
2017-12-10T13:22:27.665878: step 1336, loss 5.00271, acc 0.890625, prec 0.04105, recall 0.81051
2017-12-10T13:22:28.109944: step 1337, loss 0.414629, acc 0.875, prec 0.0410755, recall 0.81066
2017-12-10T13:22:28.540051: step 1338, loss 1.01625, acc 0.75, prec 0.041049, recall 0.81066
2017-12-10T13:22:28.984911: step 1339, loss 1.00888, acc 0.75, prec 0.0410225, recall 0.81066
2017-12-10T13:22:29.432983: step 1340, loss 1.5878, acc 0.765625, prec 0.0410364, recall 0.810811
2017-12-10T13:22:29.872009: step 1341, loss 0.904562, acc 0.78125, prec 0.0410133, recall 0.810811
2017-12-10T13:22:30.316658: step 1342, loss 2.12633, acc 0.734375, prec 0.0410254, recall 0.810317
2017-12-10T13:22:30.758431: step 1343, loss 0.480358, acc 0.84375, prec 0.041086, recall 0.810618
2017-12-10T13:22:31.197718: step 1344, loss 0.860276, acc 0.796875, prec 0.041103, recall 0.810768
2017-12-10T13:22:31.660334: step 1345, loss 0.347884, acc 0.84375, prec 0.0410865, recall 0.810768
2017-12-10T13:22:32.105044: step 1346, loss 0.736229, acc 0.8125, prec 0.0411437, recall 0.811067
2017-12-10T13:22:32.538567: step 1347, loss 0.999468, acc 0.84375, prec 0.0411656, recall 0.811216
2017-12-10T13:22:32.994249: step 1348, loss 0.454742, acc 0.84375, prec 0.0411491, recall 0.811216
2017-12-10T13:22:33.442576: step 1349, loss 0.369071, acc 0.90625, prec 0.0411776, recall 0.811365
2017-12-10T13:22:33.872004: step 1350, loss 0.451642, acc 0.84375, prec 0.0411996, recall 0.811514
2017-12-10T13:22:34.314667: step 1351, loss 0.592362, acc 0.875, prec 0.0411864, recall 0.811514
2017-12-10T13:22:34.751784: step 1352, loss 0.57008, acc 0.890625, prec 0.0411748, recall 0.811514
2017-12-10T13:22:35.187971: step 1353, loss 0.799977, acc 0.796875, prec 0.0411918, recall 0.811663
2017-12-10T13:22:35.641028: step 1354, loss 6.52644, acc 0.875, prec 0.0412952, recall 0.811469
2017-12-10T13:22:36.110740: step 1355, loss 7.02496, acc 0.828125, prec 0.0413553, recall 0.811128
2017-12-10T13:22:36.564514: step 1356, loss 0.592996, acc 0.828125, prec 0.0413372, recall 0.811128
2017-12-10T13:22:37.008189: step 1357, loss 0.315364, acc 0.890625, prec 0.0413256, recall 0.811128
2017-12-10T13:22:37.464457: step 1358, loss 0.330338, acc 0.890625, prec 0.0413523, recall 0.811276
2017-12-10T13:22:37.910314: step 1359, loss 0.649701, acc 0.796875, prec 0.0413691, recall 0.811424
2017-12-10T13:22:38.347419: step 1360, loss 0.990651, acc 0.828125, prec 0.0413892, recall 0.811572
2017-12-10T13:22:38.785006: step 1361, loss 0.284703, acc 0.875, prec 0.0414142, recall 0.811719
2017-12-10T13:22:39.248204: step 1362, loss 0.513731, acc 0.890625, prec 0.0414409, recall 0.811866
2017-12-10T13:22:39.686377: step 1363, loss 1.10824, acc 0.71875, prec 0.0414493, recall 0.812012
2017-12-10T13:22:40.134135: step 1364, loss 0.539566, acc 0.828125, prec 0.0414312, recall 0.812012
2017-12-10T13:22:40.581968: step 1365, loss 0.65266, acc 0.78125, prec 0.0414462, recall 0.812159
2017-12-10T13:22:41.042231: step 1366, loss 0.422209, acc 0.890625, prec 0.0414347, recall 0.812159
2017-12-10T13:22:41.488088: step 1367, loss 0.722575, acc 0.796875, prec 0.0414514, recall 0.812305
2017-12-10T13:22:41.938243: step 1368, loss 0.929966, acc 0.8125, prec 0.0414316, recall 0.812305
2017-12-10T13:22:42.380810: step 1369, loss 0.60013, acc 0.859375, prec 0.0414549, recall 0.812451
2017-12-10T13:22:42.822151: step 1370, loss 0.679089, acc 0.875, prec 0.0414417, recall 0.812451
2017-12-10T13:22:43.207342: step 1371, loss 0.43115, acc 0.859375, prec 0.0414269, recall 0.812451
2017-12-10T13:22:43.585226: step 1372, loss 0.369246, acc 0.84375, prec 0.0414105, recall 0.812451
2017-12-10T13:22:43.980607: step 1373, loss 0.6718, acc 0.796875, prec 0.0413892, recall 0.812451
2017-12-10T13:22:44.375857: step 1374, loss 0.273818, acc 0.875, prec 0.041376, recall 0.812451
2017-12-10T13:22:44.766930: step 1375, loss 6.83954, acc 0.921875, prec 0.0414454, recall 0.812112
2017-12-10T13:22:45.236406: step 1376, loss 0.330314, acc 0.90625, prec 0.0414736, recall 0.812258
2017-12-10T13:22:45.693203: step 1377, loss 0.473894, acc 0.875, prec 0.0414984, recall 0.812403
2017-12-10T13:22:46.143971: step 1378, loss 0.135804, acc 0.9375, prec 0.0414918, recall 0.812403
2017-12-10T13:22:46.593864: step 1379, loss 0.352317, acc 0.90625, prec 0.0414819, recall 0.812403
2017-12-10T13:22:47.043619: step 1380, loss 0.385644, acc 0.890625, prec 0.0414705, recall 0.812403
2017-12-10T13:22:47.476405: step 1381, loss 0.55676, acc 0.8125, prec 0.0414508, recall 0.812403
2017-12-10T13:22:47.929039: step 1382, loss 0.311362, acc 0.90625, prec 0.0414409, recall 0.812403
2017-12-10T13:22:48.389576: step 1383, loss 0.305316, acc 0.90625, prec 0.0414311, recall 0.812403
2017-12-10T13:22:48.841076: step 1384, loss 0.243852, acc 0.890625, prec 0.0414954, recall 0.812693
2017-12-10T13:22:49.290719: step 1385, loss 0.339975, acc 0.859375, prec 0.0414807, recall 0.812693
2017-12-10T13:22:49.735955: step 1386, loss 6.72623, acc 0.84375, prec 0.0415054, recall 0.811583
2017-12-10T13:22:50.177322: step 1387, loss 0.535427, acc 0.890625, prec 0.0415318, recall 0.811728
2017-12-10T13:22:50.618625: step 1388, loss 0.550854, acc 0.84375, prec 0.0415154, recall 0.811728
2017-12-10T13:22:51.062869: step 1389, loss 0.653725, acc 0.828125, prec 0.0414974, recall 0.811728
2017-12-10T13:22:51.514261: step 1390, loss 1.77793, acc 0.859375, prec 0.0414843, recall 0.811103
2017-12-10T13:22:51.977551: step 1391, loss 1.06589, acc 0.734375, prec 0.0414565, recall 0.811103
2017-12-10T13:22:52.426641: step 1392, loss 0.777737, acc 0.78125, prec 0.0414336, recall 0.811103
2017-12-10T13:22:52.885727: step 1393, loss 0.813278, acc 0.703125, prec 0.0414027, recall 0.811103
2017-12-10T13:22:53.331214: step 1394, loss 1.65137, acc 0.6875, prec 0.0414455, recall 0.811393
2017-12-10T13:22:53.779866: step 1395, loss 1.0528, acc 0.65625, prec 0.0414473, recall 0.811538
2017-12-10T13:22:54.211603: step 1396, loss 0.744069, acc 0.78125, prec 0.0414998, recall 0.811828
2017-12-10T13:22:54.656884: step 1397, loss 0.954084, acc 0.78125, prec 0.0415146, recall 0.811972
2017-12-10T13:22:55.131233: step 1398, loss 1.0751, acc 0.734375, prec 0.0415245, recall 0.812117
2017-12-10T13:22:55.577198: step 1399, loss 0.956977, acc 0.765625, prec 0.0415001, recall 0.812117
2017-12-10T13:22:56.023326: step 1400, loss 1.29513, acc 0.6875, prec 0.0414676, recall 0.812117
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1400

2017-12-10T13:22:57.830342: step 1401, loss 1.12278, acc 0.734375, prec 0.04144, recall 0.812117
2017-12-10T13:22:58.278503: step 1402, loss 0.610566, acc 0.828125, prec 0.0414972, recall 0.812404
2017-12-10T13:22:58.724890: step 1403, loss 0.919112, acc 0.78125, prec 0.0415869, recall 0.812834
2017-12-10T13:22:59.173779: step 1404, loss 0.514288, acc 0.78125, prec 0.0415641, recall 0.812834
2017-12-10T13:22:59.611943: step 1405, loss 0.683536, acc 0.828125, prec 0.0415837, recall 0.812977
2017-12-10T13:23:00.055956: step 1406, loss 0.478801, acc 0.78125, prec 0.041561, recall 0.812977
2017-12-10T13:23:00.499515: step 1407, loss 0.625449, acc 0.90625, prec 0.0415886, recall 0.81312
2017-12-10T13:23:00.975757: step 1408, loss 0.785376, acc 0.875, prec 0.0416504, recall 0.813404
2017-12-10T13:23:01.426390: step 1409, loss 0.958709, acc 0.890625, prec 0.0417138, recall 0.813688
2017-12-10T13:23:01.877330: step 1410, loss 0.513142, acc 0.828125, prec 0.0416959, recall 0.813688
2017-12-10T13:23:02.308305: step 1411, loss 2.79263, acc 0.8125, prec 0.0417154, recall 0.813212
2017-12-10T13:23:02.764118: step 1412, loss 2.22144, acc 0.96875, prec 0.0417137, recall 0.812595
2017-12-10T13:23:03.215320: step 1413, loss 0.195359, acc 0.953125, prec 0.0417089, recall 0.812595
2017-12-10T13:23:03.665102: step 1414, loss 0.253873, acc 0.90625, prec 0.0416991, recall 0.812595
2017-12-10T13:23:04.133436: step 1415, loss 0.57229, acc 0.84375, prec 0.0416829, recall 0.812595
2017-12-10T13:23:04.582209: step 1416, loss 0.71819, acc 0.828125, prec 0.0417396, recall 0.812879
2017-12-10T13:23:05.027210: step 1417, loss 0.733315, acc 0.8125, prec 0.0417201, recall 0.812879
2017-12-10T13:23:05.462879: step 1418, loss 0.806707, acc 0.734375, prec 0.0417298, recall 0.81302
2017-12-10T13:23:05.911186: step 1419, loss 0.842216, acc 0.71875, prec 0.0417751, recall 0.813303
2017-12-10T13:23:06.359468: step 1420, loss 2.24222, acc 0.765625, prec 0.0418267, recall 0.812971
2017-12-10T13:23:06.845941: step 1421, loss 0.67431, acc 0.75, prec 0.0418008, recall 0.812971
2017-12-10T13:23:07.288982: step 1422, loss 0.705127, acc 0.78125, prec 0.0418152, recall 0.813112
2017-12-10T13:23:07.734873: step 1423, loss 0.756722, acc 0.75, prec 0.0418264, recall 0.813253
2017-12-10T13:23:08.170464: step 1424, loss 0.511484, acc 0.8125, prec 0.0418441, recall 0.813394
2017-12-10T13:23:08.611111: step 1425, loss 0.568946, acc 0.84375, prec 0.041865, recall 0.813534
2017-12-10T13:23:09.055100: step 1426, loss 0.858076, acc 0.796875, prec 0.041881, recall 0.813674
2017-12-10T13:23:09.512906: step 1427, loss 0.946792, acc 0.859375, prec 0.0420145, recall 0.814232
2017-12-10T13:23:09.951160: step 1428, loss 1.95839, acc 0.84375, prec 0.0420369, recall 0.813762
2017-12-10T13:23:10.396435: step 1429, loss 0.527166, acc 0.796875, prec 0.0420528, recall 0.813901
2017-12-10T13:23:10.840048: step 1430, loss 0.470443, acc 0.84375, prec 0.0420366, recall 0.813901
2017-12-10T13:23:11.284384: step 1431, loss 0.980559, acc 0.78125, prec 0.0420508, recall 0.81404
2017-12-10T13:23:11.731477: step 1432, loss 0.717929, acc 0.75, prec 0.0420249, recall 0.81404
2017-12-10T13:23:12.199077: step 1433, loss 0.574497, acc 0.8125, prec 0.0420055, recall 0.81404
2017-12-10T13:23:12.641562: step 1434, loss 0.416607, acc 0.84375, prec 0.0420262, recall 0.814179
2017-12-10T13:23:13.085408: step 1435, loss 1.09878, acc 0.75, prec 0.0420372, recall 0.814318
2017-12-10T13:23:13.527284: step 1436, loss 0.257125, acc 0.921875, prec 0.042066, recall 0.814456
2017-12-10T13:23:13.976439: step 1437, loss 0.424619, acc 0.921875, prec 0.0421316, recall 0.814732
2017-12-10T13:23:14.429930: step 1438, loss 1.24467, acc 0.84375, prec 0.0421522, recall 0.81487
2017-12-10T13:23:14.900923: step 1439, loss 1.27247, acc 0.875, prec 0.0422497, recall 0.815282
2017-12-10T13:23:15.340141: step 1440, loss 0.254287, acc 0.90625, prec 0.04224, recall 0.815282
2017-12-10T13:23:15.788936: step 1441, loss 0.143739, acc 0.9375, prec 0.0422335, recall 0.815282
2017-12-10T13:23:16.236047: step 1442, loss 0.292897, acc 0.90625, prec 0.0422606, recall 0.815419
2017-12-10T13:23:16.672703: step 1443, loss 0.315484, acc 0.890625, prec 0.0422492, recall 0.815419
2017-12-10T13:23:17.134238: step 1444, loss 0.312851, acc 0.890625, prec 0.0422746, recall 0.815556
2017-12-10T13:23:17.595641: step 1445, loss 0.419041, acc 0.859375, prec 0.0423703, recall 0.815965
2017-12-10T13:23:18.047830: step 1446, loss 2.626, acc 0.859375, prec 0.042394, recall 0.815498
2017-12-10T13:23:18.502888: step 1447, loss 0.336225, acc 0.890625, prec 0.0423826, recall 0.815498
2017-12-10T13:23:18.951072: step 1448, loss 0.317922, acc 0.875, prec 0.0423696, recall 0.815498
2017-12-10T13:23:19.388037: step 1449, loss 0.595883, acc 0.8125, prec 0.0423501, recall 0.815498
2017-12-10T13:23:19.839984: step 1450, loss 0.356524, acc 0.859375, prec 0.0423722, recall 0.815634
2017-12-10T13:23:20.306410: step 1451, loss 0.675841, acc 0.84375, prec 0.0423927, recall 0.81577
2017-12-10T13:23:20.759814: step 1452, loss 1.18729, acc 0.84375, prec 0.0424131, recall 0.815906
2017-12-10T13:23:21.201675: step 1453, loss 0.376451, acc 0.890625, prec 0.0424384, recall 0.816041
2017-12-10T13:23:21.648890: step 1454, loss 0.610321, acc 0.875, prec 0.0424987, recall 0.816312
2017-12-10T13:23:22.095147: step 1455, loss 0.84795, acc 0.828125, prec 0.0425906, recall 0.816716
2017-12-10T13:23:22.557177: step 1456, loss 0.477677, acc 0.859375, prec 0.042576, recall 0.816716
2017-12-10T13:23:23.008362: step 1457, loss 0.4683, acc 0.859375, prec 0.0426345, recall 0.816984
2017-12-10T13:23:23.459473: step 1458, loss 0.774842, acc 0.8125, prec 0.0426149, recall 0.816984
2017-12-10T13:23:23.911721: step 1459, loss 9.49514, acc 0.8125, prec 0.042597, recall 0.816386
2017-12-10T13:23:24.352791: step 1460, loss 0.44565, acc 0.828125, prec 0.0426157, recall 0.81652
2017-12-10T13:23:24.800906: step 1461, loss 2.46649, acc 0.75, prec 0.0426278, recall 0.816058
2017-12-10T13:23:25.261420: step 1462, loss 1.07709, acc 0.765625, prec 0.0426035, recall 0.816058
2017-12-10T13:23:25.706491: step 1463, loss 0.459614, acc 0.765625, prec 0.0425791, recall 0.816058
2017-12-10T13:23:26.140506: step 1464, loss 0.673302, acc 0.8125, prec 0.0425597, recall 0.816058
2017-12-10T13:23:26.589454: step 1465, loss 0.741743, acc 0.84375, prec 0.0425435, recall 0.816058
2017-12-10T13:23:27.033225: step 1466, loss 0.739735, acc 0.8125, prec 0.0425605, recall 0.816193
2017-12-10T13:23:27.475200: step 1467, loss 0.503917, acc 0.828125, prec 0.0425427, recall 0.816193
2017-12-10T13:23:27.928171: step 1468, loss 0.779907, acc 0.765625, prec 0.0425548, recall 0.816327
2017-12-10T13:23:28.379649: step 1469, loss 0.930509, acc 0.703125, prec 0.0425605, recall 0.81646
2017-12-10T13:23:28.824613: step 1470, loss 0.979304, acc 0.765625, prec 0.0425362, recall 0.81646
2017-12-10T13:23:29.271978: step 1471, loss 0.302802, acc 0.890625, prec 0.0425249, recall 0.81646
2017-12-10T13:23:29.720463: step 1472, loss 1.03096, acc 0.84375, prec 0.0425451, recall 0.816594
2017-12-10T13:23:30.158135: step 1473, loss 0.826089, acc 0.8125, prec 0.0425258, recall 0.816594
2017-12-10T13:23:30.616219: step 1474, loss 0.779944, acc 0.796875, prec 0.0425411, recall 0.816727
2017-12-10T13:23:31.059299: step 1475, loss 0.833969, acc 0.78125, prec 0.0425548, recall 0.81686
2017-12-10T13:23:31.505115: step 1476, loss 0.361442, acc 0.90625, prec 0.0425451, recall 0.81686
2017-12-10T13:23:31.956468: step 1477, loss 0.498749, acc 0.890625, prec 0.0425339, recall 0.81686
2017-12-10T13:23:32.411229: step 1478, loss 0.685014, acc 0.828125, prec 0.0425162, recall 0.81686
2017-12-10T13:23:32.857733: step 1479, loss 0.662384, acc 0.84375, prec 0.0425001, recall 0.81686
2017-12-10T13:23:33.310702: step 1480, loss 0.298555, acc 0.921875, prec 0.0424921, recall 0.81686
2017-12-10T13:23:33.763293: step 1481, loss 0.449395, acc 0.875, prec 0.0425154, recall 0.816993
2017-12-10T13:23:34.208767: step 1482, loss 0.216307, acc 0.96875, prec 0.0425122, recall 0.816993
2017-12-10T13:23:34.649311: step 1483, loss 0.12306, acc 0.953125, prec 0.0425435, recall 0.817126
2017-12-10T13:23:35.098091: step 1484, loss 2.70264, acc 0.90625, prec 0.0425717, recall 0.816667
2017-12-10T13:23:35.541521: step 1485, loss 2.55358, acc 0.890625, prec 0.0425982, recall 0.816208
2017-12-10T13:23:36.016253: step 1486, loss 0.812738, acc 0.859375, prec 0.042656, recall 0.816474
2017-12-10T13:23:36.449792: step 1487, loss 0.28992, acc 0.84375, prec 0.0426399, recall 0.816474
2017-12-10T13:23:36.898233: step 1488, loss 0.383841, acc 0.859375, prec 0.0426615, recall 0.816607
2017-12-10T13:23:37.334413: step 1489, loss 0.957407, acc 0.875, prec 0.0426848, recall 0.816739
2017-12-10T13:23:37.786307: step 1490, loss 1.44401, acc 0.859375, prec 0.0427425, recall 0.817003
2017-12-10T13:23:38.190354: step 1491, loss 1.12568, acc 0.730769, prec 0.042756, recall 0.817135
2017-12-10T13:23:38.671337: step 1492, loss 0.599397, acc 0.8125, prec 0.0427727, recall 0.817266
2017-12-10T13:23:39.112303: step 1493, loss 1.21707, acc 0.765625, prec 0.0428206, recall 0.817529
2017-12-10T13:23:39.549992: step 1494, loss 0.7284, acc 0.75, prec 0.0428308, recall 0.81766
2017-12-10T13:23:39.996797: step 1495, loss 1.11747, acc 0.734375, prec 0.0428394, recall 0.817791
2017-12-10T13:23:40.433807: step 1496, loss 0.631789, acc 0.8125, prec 0.0428561, recall 0.817921
2017-12-10T13:23:40.874273: step 1497, loss 0.758697, acc 0.828125, prec 0.0428384, recall 0.817921
2017-12-10T13:23:41.318826: step 1498, loss 0.853316, acc 0.765625, prec 0.0428502, recall 0.818052
2017-12-10T13:23:41.751506: step 1499, loss 1.13792, acc 0.75, prec 0.0428962, recall 0.818312
2017-12-10T13:23:42.196085: step 1500, loss 0.727193, acc 0.75, prec 0.0429423, recall 0.818571
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1500

2017-12-10T13:23:44.112460: step 1501, loss 0.604656, acc 0.8125, prec 0.0429588, recall 0.818701
2017-12-10T13:23:44.565796: step 1502, loss 0.488073, acc 0.859375, prec 0.0429443, recall 0.818701
2017-12-10T13:23:45.019729: step 1503, loss 0.91848, acc 0.78125, prec 0.0429935, recall 0.818959
2017-12-10T13:23:45.468605: step 1504, loss 1.04924, acc 0.734375, prec 0.0430377, recall 0.819217
2017-12-10T13:23:45.917491: step 1505, loss 1.03972, acc 0.78125, prec 0.0430509, recall 0.819346
2017-12-10T13:23:46.368421: step 1506, loss 0.510866, acc 0.875, prec 0.0430738, recall 0.819474
2017-12-10T13:23:46.801500: step 1507, loss 0.571545, acc 0.796875, prec 0.0430886, recall 0.819602
2017-12-10T13:23:47.248377: step 1508, loss 1.08828, acc 0.8125, prec 0.0431408, recall 0.819858
2017-12-10T13:23:47.714681: step 1509, loss 0.410714, acc 0.890625, prec 0.0431652, recall 0.819986
2017-12-10T13:23:48.155795: step 1510, loss 8.08594, acc 0.828125, prec 0.0431491, recall 0.819405
2017-12-10T13:23:48.611108: step 1511, loss 0.451876, acc 0.859375, prec 0.0431346, recall 0.819405
2017-12-10T13:23:49.058858: step 1512, loss 0.556128, acc 0.796875, prec 0.0431494, recall 0.819533
2017-12-10T13:23:49.500200: step 1513, loss 0.38184, acc 0.890625, prec 0.0432094, recall 0.819788
2017-12-10T13:23:49.945721: step 1514, loss 0.656902, acc 0.84375, prec 0.0432646, recall 0.820042
2017-12-10T13:23:50.388610: step 1515, loss 0.583973, acc 0.90625, prec 0.043433, recall 0.820675
2017-12-10T13:23:50.834254: step 1516, loss 0.309009, acc 0.90625, prec 0.0434944, recall 0.820927
2017-12-10T13:23:51.288748: step 1517, loss 0.457784, acc 0.84375, prec 0.0434783, recall 0.820927
2017-12-10T13:23:51.736938: step 1518, loss 3.4385, acc 0.90625, prec 0.0435413, recall 0.820603
2017-12-10T13:23:52.207238: step 1519, loss 0.104222, acc 0.921875, prec 0.0435688, recall 0.820728
2017-12-10T13:23:52.659579: step 1520, loss 0.367767, acc 0.859375, prec 0.0435542, recall 0.820728
2017-12-10T13:23:53.094298: step 1521, loss 0.193926, acc 0.953125, prec 0.0436204, recall 0.820979
2017-12-10T13:23:53.536146: step 1522, loss 0.49479, acc 0.890625, prec 0.0436091, recall 0.820979
2017-12-10T13:23:53.989080: step 1523, loss 0.78811, acc 0.796875, prec 0.043588, recall 0.820979
2017-12-10T13:23:54.429751: step 1524, loss 0.586532, acc 0.796875, prec 0.0436025, recall 0.821104
2017-12-10T13:23:54.879988: step 1525, loss 0.269633, acc 0.890625, prec 0.0436267, recall 0.821229
2017-12-10T13:23:55.331869: step 1526, loss 0.552391, acc 0.796875, prec 0.0436056, recall 0.821229
2017-12-10T13:23:55.782012: step 1527, loss 0.506476, acc 0.84375, prec 0.0436249, recall 0.821354
2017-12-10T13:23:56.257756: step 1528, loss 0.577067, acc 0.796875, prec 0.0436393, recall 0.821478
2017-12-10T13:23:56.709903: step 1529, loss 0.464459, acc 0.828125, prec 0.0436216, recall 0.821478
2017-12-10T13:23:57.163761: step 1530, loss 0.177611, acc 0.9375, prec 0.0436505, recall 0.821603
2017-12-10T13:23:57.596846: step 1531, loss 0.343453, acc 0.875, prec 0.043673, recall 0.821727
2017-12-10T13:23:58.039592: step 1532, loss 0.218853, acc 0.90625, prec 0.0436633, recall 0.821727
2017-12-10T13:23:58.485485: step 1533, loss 0.397244, acc 0.890625, prec 0.043652, recall 0.821727
2017-12-10T13:23:58.925969: step 1534, loss 0.308825, acc 0.921875, prec 0.0436439, recall 0.821727
2017-12-10T13:23:59.368708: step 1535, loss 0.487282, acc 0.890625, prec 0.0437033, recall 0.821975
2017-12-10T13:23:59.814701: step 1536, loss 0.345511, acc 0.890625, prec 0.0437274, recall 0.822099
2017-12-10T13:24:00.271200: step 1537, loss 1.3537, acc 0.890625, prec 0.0437177, recall 0.821528
2017-12-10T13:24:00.709679: step 1538, loss 0.0325456, acc 1, prec 0.0437177, recall 0.821528
2017-12-10T13:24:01.148039: step 1539, loss 1.00986, acc 0.921875, prec 0.0437802, recall 0.821775
2017-12-10T13:24:01.593585: step 1540, loss 0.227675, acc 0.96875, prec 0.0438123, recall 0.821899
2017-12-10T13:24:02.057745: step 1541, loss 0.453984, acc 0.9375, prec 0.0438412, recall 0.822022
2017-12-10T13:24:02.494116: step 1542, loss 0.388319, acc 0.890625, prec 0.0438299, recall 0.822022
2017-12-10T13:24:02.933890: step 1543, loss 0.251402, acc 0.921875, prec 0.0438571, recall 0.822145
2017-12-10T13:24:03.379213: step 1544, loss 0.261463, acc 0.90625, prec 0.0438473, recall 0.822145
2017-12-10T13:24:03.815299: step 1545, loss 1.17495, acc 0.890625, prec 0.0438713, recall 0.822268
2017-12-10T13:24:04.261506: step 1546, loss 0.295015, acc 0.890625, prec 0.04386, recall 0.822268
2017-12-10T13:24:04.707278: step 1547, loss 5.77517, acc 0.953125, prec 0.0438567, recall 0.8217
2017-12-10T13:24:05.146643: step 1548, loss 0.279711, acc 0.921875, prec 0.0439192, recall 0.821946
2017-12-10T13:24:05.591195: step 1549, loss 0.278719, acc 0.921875, prec 0.0439816, recall 0.822192
2017-12-10T13:24:06.034806: step 1550, loss 0.693232, acc 0.75, prec 0.0439909, recall 0.822314
2017-12-10T13:24:06.494597: step 1551, loss 0.427914, acc 0.875, prec 0.0440835, recall 0.82268
2017-12-10T13:24:06.936725: step 1552, loss 0.189149, acc 0.921875, prec 0.0441106, recall 0.822802
2017-12-10T13:24:07.389513: step 1553, loss 0.913163, acc 0.75, prec 0.0440846, recall 0.822802
2017-12-10T13:24:07.822039: step 1554, loss 0.233739, acc 0.921875, prec 0.0440765, recall 0.822802
2017-12-10T13:24:08.271608: step 1555, loss 0.298319, acc 0.875, prec 0.0440987, recall 0.822924
2017-12-10T13:24:08.713113: step 1556, loss 0.446434, acc 0.859375, prec 0.0441544, recall 0.823167
2017-12-10T13:24:09.138191: step 1557, loss 0.329915, acc 0.84375, prec 0.0441382, recall 0.823167
2017-12-10T13:24:09.588415: step 1558, loss 0.272063, acc 0.84375, prec 0.044122, recall 0.823167
2017-12-10T13:24:10.041546: step 1559, loss 0.267883, acc 0.90625, prec 0.0441825, recall 0.823409
2017-12-10T13:24:10.489713: step 1560, loss 0.38118, acc 0.890625, prec 0.0441711, recall 0.823409
2017-12-10T13:24:10.943508: step 1561, loss 0.525307, acc 0.84375, prec 0.0442251, recall 0.82365
2017-12-10T13:24:11.407114: step 1562, loss 0.475787, acc 0.828125, prec 0.0442072, recall 0.82365
2017-12-10T13:24:11.855669: step 1563, loss 0.254009, acc 0.921875, prec 0.0442342, recall 0.82377
2017-12-10T13:24:12.300320: step 1564, loss 0.939963, acc 0.859375, prec 0.0442546, recall 0.823891
2017-12-10T13:24:12.746719: step 1565, loss 0.0567945, acc 0.96875, prec 0.0442514, recall 0.823891
2017-12-10T13:24:13.198083: step 1566, loss 0.329522, acc 0.90625, prec 0.0442416, recall 0.823891
2017-12-10T13:24:13.642150: step 1567, loss 0.473003, acc 0.921875, prec 0.0443036, recall 0.824131
2017-12-10T13:24:14.099610: step 1568, loss 0.240179, acc 0.921875, prec 0.0443305, recall 0.824251
2017-12-10T13:24:14.544968: step 1569, loss 0.495861, acc 0.921875, prec 0.0443924, recall 0.82449
2017-12-10T13:24:14.990324: step 1570, loss 0.293862, acc 0.890625, prec 0.044381, recall 0.82449
2017-12-10T13:24:15.445329: step 1571, loss 0.817291, acc 0.90625, prec 0.0444062, recall 0.824609
2017-12-10T13:24:15.895197: step 1572, loss 0.150972, acc 0.9375, prec 0.0444347, recall 0.824728
2017-12-10T13:24:16.332247: step 1573, loss 5.82664, acc 0.90625, prec 0.0444615, recall 0.824288
2017-12-10T13:24:16.782637: step 1574, loss 5.49856, acc 0.875, prec 0.0444851, recall 0.823848
2017-12-10T13:24:17.239146: step 1575, loss 0.757859, acc 0.8125, prec 0.0445005, recall 0.823968
2017-12-10T13:24:17.697147: step 1576, loss 0.271459, acc 0.890625, prec 0.0444891, recall 0.823968
2017-12-10T13:24:18.164587: step 1577, loss 0.63728, acc 0.84375, prec 0.0444729, recall 0.823968
2017-12-10T13:24:18.619757: step 1578, loss 0.801073, acc 0.75, prec 0.0444469, recall 0.823968
2017-12-10T13:24:19.068177: step 1579, loss 0.918242, acc 0.71875, prec 0.0444526, recall 0.824087
2017-12-10T13:24:19.510870: step 1580, loss 1.65557, acc 0.765625, prec 0.0445676, recall 0.824561
2017-12-10T13:24:19.958806: step 1581, loss 0.565183, acc 0.796875, prec 0.0445813, recall 0.82468
2017-12-10T13:24:20.408378: step 1582, loss 0.659096, acc 0.796875, prec 0.0445602, recall 0.82468
2017-12-10T13:24:20.855683: step 1583, loss 0.671036, acc 0.8125, prec 0.0446451, recall 0.825034
2017-12-10T13:24:21.304832: step 1584, loss 0.772015, acc 0.796875, prec 0.0446588, recall 0.825151
2017-12-10T13:24:21.764184: step 1585, loss 0.620613, acc 0.8125, prec 0.0447088, recall 0.825386
2017-12-10T13:24:22.218795: step 1586, loss 0.556561, acc 0.859375, prec 0.0447289, recall 0.825503
2017-12-10T13:24:22.674768: step 1587, loss 0.255208, acc 0.890625, prec 0.0447522, recall 0.82562
2017-12-10T13:24:23.126867: step 1588, loss 0.621759, acc 0.859375, prec 0.0448417, recall 0.825971
2017-12-10T13:24:23.572126: step 1589, loss 0.497054, acc 0.828125, prec 0.0448585, recall 0.826087
2017-12-10T13:24:24.010352: step 1590, loss 0.237698, acc 0.890625, prec 0.0448471, recall 0.826087
2017-12-10T13:24:24.461489: step 1591, loss 0.319453, acc 0.890625, prec 0.0448357, recall 0.826087
2017-12-10T13:24:24.903234: step 1592, loss 0.180405, acc 0.953125, prec 0.0449002, recall 0.826319
2017-12-10T13:24:25.372400: step 1593, loss 0.230096, acc 0.9375, prec 0.0448937, recall 0.826319
2017-12-10T13:24:25.817351: step 1594, loss 0.271439, acc 0.90625, prec 0.0448839, recall 0.826319
2017-12-10T13:24:26.255171: step 1595, loss 0.24725, acc 0.90625, prec 0.0448741, recall 0.826319
2017-12-10T13:24:26.690570: step 1596, loss 0.155447, acc 0.90625, prec 0.044899, recall 0.826435
2017-12-10T13:24:27.133709: step 1597, loss 0.17199, acc 0.96875, prec 0.0448957, recall 0.826435
2017-12-10T13:24:27.582815: step 1598, loss 3.11011, acc 0.84375, prec 0.0448811, recall 0.825884
2017-12-10T13:24:28.022219: step 1599, loss 0.421243, acc 0.875, prec 0.0449027, recall 0.826
2017-12-10T13:24:28.483848: step 1600, loss 0.527302, acc 0.84375, prec 0.0449556, recall 0.826232
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1600

2017-12-10T13:24:30.463800: step 1601, loss 0.288226, acc 0.890625, prec 0.0449442, recall 0.826232
2017-12-10T13:24:30.907481: step 1602, loss 0.309796, acc 0.890625, prec 0.0449674, recall 0.826347
2017-12-10T13:24:31.355414: step 1603, loss 0.222881, acc 0.953125, prec 0.0449971, recall 0.826463
2017-12-10T13:24:31.805711: step 1604, loss 0.340738, acc 0.875, prec 0.0449841, recall 0.826463
2017-12-10T13:24:32.246986: step 1605, loss 0.167795, acc 0.96875, prec 0.0449808, recall 0.826463
2017-12-10T13:24:32.696287: step 1606, loss 0.236393, acc 0.9375, prec 0.0449743, recall 0.826463
2017-12-10T13:24:33.141972: step 1607, loss 0.2783, acc 0.90625, prec 0.0449991, recall 0.826578
2017-12-10T13:24:33.588946: step 1608, loss 7.08055, acc 0.9375, prec 0.0450288, recall 0.826145
2017-12-10T13:24:34.048901: step 1609, loss 0.550306, acc 0.953125, prec 0.0450929, recall 0.826375
2017-12-10T13:24:34.494492: step 1610, loss 1.76047, acc 0.953125, prec 0.0450897, recall 0.825828
2017-12-10T13:24:34.939540: step 1611, loss 0.472802, acc 0.875, prec 0.0451111, recall 0.825943
2017-12-10T13:24:35.406101: step 1612, loss 0.43086, acc 0.828125, prec 0.0450932, recall 0.825943
2017-12-10T13:24:35.859967: step 1613, loss 0.778548, acc 0.859375, prec 0.0451475, recall 0.826173
2017-12-10T13:24:36.313259: step 1614, loss 0.359698, acc 0.890625, prec 0.0451361, recall 0.826173
2017-12-10T13:24:36.763776: step 1615, loss 0.757809, acc 0.75, prec 0.0451101, recall 0.826173
2017-12-10T13:24:37.212635: step 1616, loss 0.574033, acc 0.796875, prec 0.0450889, recall 0.826173
2017-12-10T13:24:37.670649: step 1617, loss 0.555132, acc 0.765625, prec 0.045099, recall 0.826288
2017-12-10T13:24:38.122982: step 1618, loss 0.35132, acc 0.84375, prec 0.0450827, recall 0.826288
2017-12-10T13:24:38.558554: step 1619, loss 0.759284, acc 0.75, prec 0.0450567, recall 0.826288
2017-12-10T13:24:39.008733: step 1620, loss 0.705554, acc 0.75, prec 0.0450308, recall 0.826288
2017-12-10T13:24:39.458265: step 1621, loss 0.624655, acc 0.78125, prec 0.0450425, recall 0.826403
2017-12-10T13:24:39.910067: step 1622, loss 1.043, acc 0.6875, prec 0.0450101, recall 0.826403
2017-12-10T13:24:40.352728: step 1623, loss 0.917375, acc 0.75, prec 0.0451214, recall 0.82686
2017-12-10T13:24:40.811341: step 1624, loss 0.376625, acc 0.875, prec 0.0451085, recall 0.82686
2017-12-10T13:24:41.258870: step 1625, loss 0.550178, acc 0.78125, prec 0.0451201, recall 0.826974
2017-12-10T13:24:41.704889: step 1626, loss 0.704882, acc 0.796875, prec 0.0451333, recall 0.827087
2017-12-10T13:24:42.145770: step 1627, loss 0.275603, acc 0.875, prec 0.0451203, recall 0.827087
2017-12-10T13:24:42.592486: step 1628, loss 0.353936, acc 0.90625, prec 0.0451449, recall 0.827201
2017-12-10T13:24:43.026434: step 1629, loss 0.346828, acc 0.890625, prec 0.045202, recall 0.827428
2017-12-10T13:24:43.464574: step 1630, loss 0.165154, acc 0.921875, prec 0.0451939, recall 0.827428
2017-12-10T13:24:43.911452: step 1631, loss 0.343027, acc 0.90625, prec 0.0451842, recall 0.827428
2017-12-10T13:24:44.351142: step 1632, loss 0.321561, acc 0.921875, prec 0.0451761, recall 0.827428
2017-12-10T13:24:44.800617: step 1633, loss 0.0793089, acc 0.953125, prec 0.0451712, recall 0.827428
2017-12-10T13:24:45.253268: step 1634, loss 0.341809, acc 0.890625, prec 0.0451941, recall 0.827541
2017-12-10T13:24:45.696080: step 1635, loss 0.674623, acc 0.96875, prec 0.0452251, recall 0.827654
2017-12-10T13:24:46.143646: step 1636, loss 0.11025, acc 0.953125, prec 0.0452202, recall 0.827654
2017-12-10T13:24:46.588633: step 1637, loss 0.353369, acc 0.953125, prec 0.0452837, recall 0.82788
2017-12-10T13:24:47.033465: step 1638, loss 0.729249, acc 0.96875, prec 0.0453488, recall 0.828105
2017-12-10T13:24:47.476792: step 1639, loss 0.268051, acc 0.90625, prec 0.0453391, recall 0.828105
2017-12-10T13:24:47.925254: step 1640, loss 0.234416, acc 0.9375, prec 0.0453326, recall 0.828105
2017-12-10T13:24:48.394042: step 1641, loss 0.0892779, acc 0.96875, prec 0.0453976, recall 0.828329
2017-12-10T13:24:48.822922: step 1642, loss 0.390669, acc 0.859375, prec 0.0454513, recall 0.828553
2017-12-10T13:24:49.271829: step 1643, loss 0.176242, acc 0.9375, prec 0.0454789, recall 0.828664
2017-12-10T13:24:49.702900: step 1644, loss 0.285933, acc 0.890625, prec 0.0455017, recall 0.828776
2017-12-10T13:24:50.144565: step 1645, loss 0.111998, acc 0.953125, prec 0.0455309, recall 0.828887
2017-12-10T13:24:50.593329: step 1646, loss 0.827448, acc 0.953125, prec 0.0455601, recall 0.828999
2017-12-10T13:24:51.049554: step 1647, loss 0.260795, acc 0.984375, prec 0.0456267, recall 0.829221
2017-12-10T13:24:51.509348: step 1648, loss 0.595387, acc 0.984375, prec 0.0456933, recall 0.829442
2017-12-10T13:24:51.964101: step 1649, loss 0.516919, acc 0.921875, prec 0.0457192, recall 0.829553
2017-12-10T13:24:52.414461: step 1650, loss 0.249582, acc 0.90625, prec 0.0457094, recall 0.829553
2017-12-10T13:24:52.848366: step 1651, loss 0.111352, acc 0.953125, prec 0.0457045, recall 0.829553
2017-12-10T13:24:53.300897: step 1652, loss 0.399673, acc 0.90625, prec 0.0457628, recall 0.829773
2017-12-10T13:24:53.739870: step 1653, loss 0.224364, acc 0.90625, prec 0.0457871, recall 0.829884
2017-12-10T13:24:54.191986: step 1654, loss 0.148102, acc 0.921875, prec 0.0457789, recall 0.829884
2017-12-10T13:24:54.657693: step 1655, loss 0.323781, acc 0.875, prec 0.0457999, recall 0.829994
2017-12-10T13:24:55.107585: step 1656, loss 0.072133, acc 0.96875, prec 0.0458307, recall 0.830103
2017-12-10T13:24:55.563504: step 1657, loss 0.11857, acc 0.96875, prec 0.0458274, recall 0.830103
2017-12-10T13:24:56.016355: step 1658, loss 0.262019, acc 0.875, prec 0.0458143, recall 0.830103
2017-12-10T13:24:56.456353: step 1659, loss 0.616847, acc 0.890625, prec 0.0458369, recall 0.830213
2017-12-10T13:24:56.902125: step 1660, loss 0.0978678, acc 0.96875, prec 0.0458676, recall 0.830323
2017-12-10T13:24:57.341220: step 1661, loss 0.139108, acc 0.9375, prec 0.0458611, recall 0.830323
2017-12-10T13:24:57.793943: step 1662, loss 0.184542, acc 0.9375, prec 0.0458886, recall 0.830432
2017-12-10T13:24:58.236065: step 1663, loss 0.224052, acc 0.9375, prec 0.04595, recall 0.83065
2017-12-10T13:24:58.683013: step 1664, loss 0.151796, acc 0.953125, prec 0.0459791, recall 0.830759
2017-12-10T13:24:59.137931: step 1665, loss 0.308167, acc 0.890625, prec 0.0459676, recall 0.830759
2017-12-10T13:24:59.585438: step 1666, loss 0.0775326, acc 0.984375, prec 0.045966, recall 0.830759
2017-12-10T13:25:00.052969: step 1667, loss 0.207844, acc 0.9375, prec 0.0459934, recall 0.830868
2017-12-10T13:25:00.491113: step 1668, loss 0.13325, acc 0.9375, prec 0.0459868, recall 0.830868
2017-12-10T13:25:00.937097: step 1669, loss 0.298578, acc 0.921875, prec 0.0460465, recall 0.831085
2017-12-10T13:25:01.389382: step 1670, loss 0.167124, acc 0.953125, prec 0.0461095, recall 0.831302
2017-12-10T13:25:01.839274: step 1671, loss 0.491896, acc 0.9375, prec 0.0461369, recall 0.83141
2017-12-10T13:25:02.283597: step 1672, loss 0.623835, acc 0.96875, prec 0.0461675, recall 0.831518
2017-12-10T13:25:02.738484: step 1673, loss 0.107873, acc 0.953125, prec 0.0461626, recall 0.831518
2017-12-10T13:25:03.179558: step 1674, loss 0.161618, acc 0.953125, prec 0.0461577, recall 0.831518
2017-12-10T13:25:03.632497: step 1675, loss 2.02046, acc 0.9375, prec 0.0463207, recall 0.832056
2017-12-10T13:25:04.093098: step 1676, loss 0.0749959, acc 0.9375, prec 0.0463141, recall 0.832056
2017-12-10T13:25:04.543776: step 1677, loss 1.04162, acc 0.875, prec 0.0463348, recall 0.832163
2017-12-10T13:25:04.980550: step 1678, loss 0.249514, acc 0.890625, prec 0.0463571, recall 0.83227
2017-12-10T13:25:05.424198: step 1679, loss 0.895525, acc 0.921875, prec 0.0463828, recall 0.832377
2017-12-10T13:25:05.879374: step 1680, loss 0.251024, acc 0.9375, prec 0.0464101, recall 0.832484
2017-12-10T13:25:06.315849: step 1681, loss 0.399348, acc 0.859375, prec 0.0464291, recall 0.832591
2017-12-10T13:25:06.745924: step 1682, loss 0.35538, acc 0.890625, prec 0.0464175, recall 0.832591
2017-12-10T13:25:07.183546: step 1683, loss 0.306172, acc 0.921875, prec 0.0464093, recall 0.832591
2017-12-10T13:25:07.629217: step 1684, loss 0.958464, acc 0.8125, prec 0.0464572, recall 0.832804
2017-12-10T13:25:08.076915: step 1685, loss 0.770619, acc 0.78125, prec 0.0464679, recall 0.83291
2017-12-10T13:25:08.519990: step 1686, loss 0.592084, acc 0.828125, prec 0.0464836, recall 0.833016
2017-12-10T13:25:08.981953: step 1687, loss 0.59697, acc 0.859375, prec 0.0464688, recall 0.833016
2017-12-10T13:25:09.423652: step 1688, loss 0.829439, acc 0.8125, prec 0.0464491, recall 0.833016
2017-12-10T13:25:09.853414: step 1689, loss 0.632711, acc 0.78125, prec 0.0464598, recall 0.833122
2017-12-10T13:25:10.298175: step 1690, loss 0.386819, acc 0.875, prec 0.0465141, recall 0.833333
2017-12-10T13:25:10.730085: step 1691, loss 2.59765, acc 0.828125, prec 0.0465651, recall 0.833017
2017-12-10T13:25:11.179314: step 1692, loss 0.793306, acc 0.75, prec 0.0466061, recall 0.833228
2017-12-10T13:25:11.634303: step 1693, loss 0.303985, acc 0.90625, prec 0.0465962, recall 0.833228
2017-12-10T13:25:12.086362: step 1694, loss 0.961997, acc 0.84375, prec 0.0465798, recall 0.833228
2017-12-10T13:25:12.533263: step 1695, loss 0.315396, acc 0.90625, prec 0.0465699, recall 0.833228
2017-12-10T13:25:12.978978: step 1696, loss 0.813367, acc 0.734375, prec 0.0465756, recall 0.833333
2017-12-10T13:25:13.426122: step 1697, loss 0.536614, acc 0.828125, prec 0.0465576, recall 0.833333
2017-12-10T13:25:13.877510: step 1698, loss 0.508229, acc 0.84375, prec 0.0465411, recall 0.833333
2017-12-10T13:25:14.328957: step 1699, loss 0.381823, acc 0.828125, prec 0.0465231, recall 0.833333
2017-12-10T13:25:14.777223: step 1700, loss 0.284236, acc 0.875, prec 0.0465436, recall 0.833438
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1700

2017-12-10T13:25:16.747094: step 1701, loss 0.375956, acc 0.875, prec 0.0465641, recall 0.833543
2017-12-10T13:25:17.194658: step 1702, loss 0.674322, acc 0.828125, prec 0.046546, recall 0.833543
2017-12-10T13:25:17.630210: step 1703, loss 0.257594, acc 0.921875, prec 0.0465378, recall 0.833543
2017-12-10T13:25:18.069197: step 1704, loss 0.193005, acc 0.9375, prec 0.0465984, recall 0.833753
2017-12-10T13:25:18.502446: step 1705, loss 0.297958, acc 0.890625, prec 0.0466205, recall 0.833858
2017-12-10T13:25:18.945137: step 1706, loss 0.888538, acc 0.9375, prec 0.046681, recall 0.834067
2017-12-10T13:25:19.392929: step 1707, loss 0.0504832, acc 0.984375, prec 0.0467129, recall 0.834171
2017-12-10T13:25:19.844891: step 1708, loss 0.158803, acc 0.9375, prec 0.0467398, recall 0.834275
2017-12-10T13:25:20.285575: step 1709, loss 0.361352, acc 0.984375, prec 0.0468052, recall 0.834483
2017-12-10T13:25:20.743825: step 1710, loss 0.138546, acc 0.96875, prec 0.0468019, recall 0.834483
2017-12-10T13:25:21.191756: step 1711, loss 0.696496, acc 0.90625, prec 0.0468256, recall 0.834586
2017-12-10T13:25:21.635241: step 1712, loss 0.100202, acc 0.96875, prec 0.0468223, recall 0.834586
2017-12-10T13:25:22.080591: step 1713, loss 0.180967, acc 0.921875, prec 0.046814, recall 0.834586
2017-12-10T13:25:22.519739: step 1714, loss 0.332258, acc 0.9375, prec 0.046841, recall 0.83469
2017-12-10T13:25:22.981872: step 1715, loss 0.065525, acc 0.984375, prec 0.0468393, recall 0.83469
2017-12-10T13:25:23.429868: step 1716, loss 0.354229, acc 0.9375, prec 0.0468662, recall 0.834794
2017-12-10T13:25:23.889150: step 1717, loss 5.59416, acc 0.859375, prec 0.046853, recall 0.834271
2017-12-10T13:25:24.345167: step 1718, loss 0.514319, acc 0.984375, prec 0.0468849, recall 0.834375
2017-12-10T13:25:24.783583: step 1719, loss 0.502858, acc 0.84375, prec 0.0468684, recall 0.834375
2017-12-10T13:25:25.224895: step 1720, loss 1.74746, acc 0.796875, prec 0.0468805, recall 0.834478
2017-12-10T13:25:25.656996: step 1721, loss 0.446565, acc 0.796875, prec 0.0468591, recall 0.834478
2017-12-10T13:25:26.099600: step 1722, loss 0.225321, acc 0.921875, prec 0.0468509, recall 0.834478
2017-12-10T13:25:26.537290: step 1723, loss 0.153528, acc 0.953125, prec 0.0469128, recall 0.834685
2017-12-10T13:25:26.987723: step 1724, loss 0.323059, acc 0.875, prec 0.0469331, recall 0.834788
2017-12-10T13:25:27.440591: step 1725, loss 0.861478, acc 0.828125, prec 0.0470151, recall 0.835096
2017-12-10T13:25:27.883057: step 1726, loss 0.411982, acc 0.84375, prec 0.0469987, recall 0.835096
2017-12-10T13:25:28.327306: step 1727, loss 0.509912, acc 0.859375, prec 0.0470172, recall 0.835199
2017-12-10T13:25:28.763760: step 1728, loss 0.530246, acc 0.78125, prec 0.0469942, recall 0.835199
2017-12-10T13:25:29.221279: step 1729, loss 0.277223, acc 0.90625, prec 0.0469843, recall 0.835199
2017-12-10T13:25:29.668762: step 1730, loss 0.508854, acc 0.8125, prec 0.0469979, recall 0.835301
2017-12-10T13:25:30.117509: step 1731, loss 0.65186, acc 0.796875, prec 0.0470099, recall 0.835404
2017-12-10T13:25:30.559601: step 1732, loss 0.398937, acc 0.859375, prec 0.0470284, recall 0.835506
2017-12-10T13:25:30.998593: step 1733, loss 0.431802, acc 0.859375, prec 0.0470136, recall 0.835506
2017-12-10T13:25:31.442637: step 1734, loss 0.727424, acc 0.796875, prec 0.0469923, recall 0.835506
2017-12-10T13:25:31.890012: step 1735, loss 0.531069, acc 0.84375, prec 0.0470424, recall 0.83571
2017-12-10T13:25:32.340248: step 1736, loss 0.596964, acc 0.953125, prec 0.0470707, recall 0.835812
2017-12-10T13:25:32.779840: step 1737, loss 0.489942, acc 0.859375, prec 0.047056, recall 0.835812
2017-12-10T13:25:33.216202: step 1738, loss 0.224842, acc 0.890625, prec 0.0470445, recall 0.835812
2017-12-10T13:25:33.660563: step 1739, loss 0.232207, acc 0.921875, prec 0.0470363, recall 0.835812
2017-12-10T13:25:34.125746: step 1740, loss 0.210452, acc 0.953125, prec 0.0470313, recall 0.835812
2017-12-10T13:25:34.584536: step 1741, loss 0.338051, acc 0.875, prec 0.0470514, recall 0.835913
2017-12-10T13:25:35.034669: step 1742, loss 0.358558, acc 0.90625, prec 0.0471412, recall 0.836218
2017-12-10T13:25:35.485693: step 1743, loss 0.316503, acc 0.90625, prec 0.0471977, recall 0.83642
2017-12-10T13:25:35.931196: step 1744, loss 0.235089, acc 0.921875, prec 0.0472559, recall 0.836621
2017-12-10T13:25:36.375704: step 1745, loss 0.235074, acc 0.875, prec 0.0472427, recall 0.836621
2017-12-10T13:25:36.815808: step 1746, loss 3.78645, acc 0.84375, prec 0.0472279, recall 0.836106
2017-12-10T13:25:37.276918: step 1747, loss 0.134546, acc 0.96875, prec 0.0472578, recall 0.836207
2017-12-10T13:25:37.748881: step 1748, loss 2.87436, acc 0.96875, prec 0.0472561, recall 0.835692
2017-12-10T13:25:38.213893: step 1749, loss 0.269267, acc 0.890625, prec 0.0472778, recall 0.835793
2017-12-10T13:25:38.673068: step 1750, loss 0.374748, acc 0.875, prec 0.0472978, recall 0.835894
2017-12-10T13:25:39.127689: step 1751, loss 0.348028, acc 0.9375, prec 0.0474237, recall 0.836297
2017-12-10T13:25:39.588917: step 1752, loss 1.53514, acc 0.828125, prec 0.0474387, recall 0.836397
2017-12-10T13:25:40.030976: step 1753, loss 0.540061, acc 0.859375, prec 0.0474569, recall 0.836497
2017-12-10T13:25:40.467745: step 1754, loss 0.752378, acc 0.796875, prec 0.0474686, recall 0.836597
2017-12-10T13:25:40.911915: step 1755, loss 0.538035, acc 0.84375, prec 0.0474521, recall 0.836597
2017-12-10T13:25:41.353758: step 1756, loss 0.610279, acc 0.875, prec 0.047472, recall 0.836697
2017-12-10T13:25:41.803119: step 1757, loss 0.258089, acc 0.9375, prec 0.0474654, recall 0.836697
2017-12-10T13:25:42.256878: step 1758, loss 0.40299, acc 0.859375, prec 0.0474836, recall 0.836797
2017-12-10T13:25:42.705697: step 1759, loss 0.716024, acc 0.78125, prec 0.0474936, recall 0.836897
2017-12-10T13:25:43.152645: step 1760, loss 0.502444, acc 0.875, prec 0.0475464, recall 0.837096
2017-12-10T13:25:43.599417: step 1761, loss 0.785211, acc 0.75, prec 0.0475201, recall 0.837096
2017-12-10T13:25:44.042781: step 1762, loss 0.281949, acc 0.90625, prec 0.0475762, recall 0.837294
2017-12-10T13:25:44.486992: step 1763, loss 0.553199, acc 0.859375, prec 0.0476603, recall 0.837591
2017-12-10T13:25:44.929833: step 1764, loss 0.325589, acc 0.90625, prec 0.0476504, recall 0.837591
2017-12-10T13:25:45.366411: step 1765, loss 1.89092, acc 0.8125, prec 0.0476652, recall 0.837181
2017-12-10T13:25:45.813142: step 1766, loss 0.280549, acc 0.921875, prec 0.0476569, recall 0.837181
2017-12-10T13:25:46.278095: step 1767, loss 0.369417, acc 0.90625, prec 0.047647, recall 0.837181
2017-12-10T13:25:46.715651: step 1768, loss 0.55074, acc 0.828125, prec 0.0476618, recall 0.83728
2017-12-10T13:25:47.169928: step 1769, loss 0.156481, acc 0.921875, prec 0.0476865, recall 0.837379
2017-12-10T13:25:47.603824: step 1770, loss 0.223396, acc 0.90625, prec 0.0477424, recall 0.837576
2017-12-10T13:25:48.044148: step 1771, loss 0.664535, acc 0.75, prec 0.0477161, recall 0.837576
2017-12-10T13:25:48.488060: step 1772, loss 0.572456, acc 0.90625, prec 0.0478048, recall 0.837871
2017-12-10T13:25:48.933196: step 1773, loss 0.453303, acc 0.90625, prec 0.0478277, recall 0.837969
2017-12-10T13:25:49.380757: step 1774, loss 0.138411, acc 0.921875, prec 0.0478523, recall 0.838066
2017-12-10T13:25:49.820882: step 1775, loss 0.880133, acc 0.921875, prec 0.0479426, recall 0.838359
2017-12-10T13:25:50.294905: step 1776, loss 0.426641, acc 0.9375, prec 0.0479688, recall 0.838457
2017-12-10T13:25:50.754759: step 1777, loss 0.287575, acc 0.890625, prec 0.0479572, recall 0.838457
2017-12-10T13:25:51.199000: step 1778, loss 0.374758, acc 0.875, prec 0.0479768, recall 0.838554
2017-12-10T13:25:51.653698: step 1779, loss 0.219161, acc 0.9375, prec 0.0479702, recall 0.838554
2017-12-10T13:25:52.100108: step 1780, loss 0.459465, acc 0.921875, prec 0.0479948, recall 0.838651
2017-12-10T13:25:52.563279: step 1781, loss 2.33819, acc 0.90625, prec 0.0480193, recall 0.838244
2017-12-10T13:25:53.010598: step 1782, loss 0.580118, acc 0.921875, prec 0.0480438, recall 0.838341
2017-12-10T13:25:53.465511: step 1783, loss 0.575902, acc 0.921875, prec 0.0481339, recall 0.838632
2017-12-10T13:25:53.920354: step 1784, loss 0.245581, acc 0.921875, prec 0.0481583, recall 0.838729
2017-12-10T13:25:54.370709: step 1785, loss 0.351983, acc 0.875, prec 0.0481451, recall 0.838729
2017-12-10T13:25:54.821826: step 1786, loss 0.522876, acc 0.8125, prec 0.0481252, recall 0.838729
2017-12-10T13:25:55.259858: step 1787, loss 0.654042, acc 0.859375, prec 0.0481431, recall 0.838826
2017-12-10T13:25:55.692663: step 1788, loss 0.748316, acc 0.8125, prec 0.0481232, recall 0.838826
2017-12-10T13:25:56.142387: step 1789, loss 0.544914, acc 0.890625, prec 0.0482097, recall 0.839115
2017-12-10T13:25:56.589580: step 1790, loss 0.243521, acc 0.890625, prec 0.0482635, recall 0.839307
2017-12-10T13:25:57.036832: step 1791, loss 0.74036, acc 0.8125, prec 0.0482437, recall 0.839307
2017-12-10T13:25:57.478506: step 1792, loss 0.621297, acc 0.796875, prec 0.0482548, recall 0.839403
2017-12-10T13:25:57.931601: step 1793, loss 0.536539, acc 0.796875, prec 0.0482659, recall 0.839499
2017-12-10T13:25:58.387635: step 1794, loss 0.696849, acc 0.8125, prec 0.0482461, recall 0.839499
2017-12-10T13:25:58.822422: step 1795, loss 0.589279, acc 0.890625, prec 0.0482671, recall 0.839595
2017-12-10T13:25:59.294370: step 1796, loss 1.11746, acc 0.875, prec 0.0482865, recall 0.83969
2017-12-10T13:25:59.752496: step 1797, loss 11.6643, acc 0.828125, prec 0.0483678, recall 0.839477
2017-12-10T13:26:00.208074: step 1798, loss 1.92586, acc 0.90625, prec 0.0484247, recall 0.839169
2017-12-10T13:26:00.651636: step 1799, loss 0.777989, acc 0.84375, prec 0.0484407, recall 0.839265
2017-12-10T13:26:01.097075: step 1800, loss 0.583992, acc 0.84375, prec 0.0484241, recall 0.839265
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1800

2017-12-10T13:26:02.971307: step 1801, loss 0.511794, acc 0.8125, prec 0.0484042, recall 0.839265
2017-12-10T13:26:03.415299: step 1802, loss 0.967675, acc 0.734375, prec 0.0483761, recall 0.839265
2017-12-10T13:26:03.887826: step 1803, loss 0.628986, acc 0.84375, prec 0.0483921, recall 0.83936
2017-12-10T13:26:04.333892: step 1804, loss 1.28204, acc 0.71875, prec 0.0483948, recall 0.839455
2017-12-10T13:26:04.777628: step 1805, loss 1.34763, acc 0.671875, prec 0.0483601, recall 0.839455
2017-12-10T13:26:05.213523: step 1806, loss 0.773701, acc 0.765625, prec 0.0483354, recall 0.839455
2017-12-10T13:26:05.655940: step 1807, loss 1.0777, acc 0.71875, prec 0.0483057, recall 0.839455
2017-12-10T13:26:06.103423: step 1808, loss 0.667554, acc 0.734375, prec 0.0483102, recall 0.83955
2017-12-10T13:26:06.542552: step 1809, loss 0.660298, acc 0.84375, prec 0.0482937, recall 0.83955
2017-12-10T13:26:06.972800: step 1810, loss 0.463454, acc 0.84375, prec 0.0483421, recall 0.83974
2017-12-10T13:26:07.411598: step 1811, loss 0.702226, acc 0.765625, prec 0.0483822, recall 0.839929
2017-12-10T13:26:07.885497: step 1812, loss 0.556337, acc 0.84375, prec 0.0483981, recall 0.840024
2017-12-10T13:26:08.324525: step 1813, loss 0.418916, acc 0.859375, prec 0.0484156, recall 0.840118
2017-12-10T13:26:08.766029: step 1814, loss 0.327121, acc 0.90625, prec 0.0484057, recall 0.840118
2017-12-10T13:26:09.198864: step 1815, loss 0.240101, acc 0.90625, prec 0.0483959, recall 0.840118
2017-12-10T13:26:09.650699: step 1816, loss 0.45675, acc 0.84375, prec 0.0483794, recall 0.840118
2017-12-10T13:26:10.102576: step 1817, loss 0.318854, acc 0.921875, prec 0.0483712, recall 0.840118
2017-12-10T13:26:10.557037: step 1818, loss 0.163659, acc 0.9375, prec 0.0483646, recall 0.840118
2017-12-10T13:26:11.010261: step 1819, loss 0.143169, acc 0.90625, prec 0.0483548, recall 0.840118
2017-12-10T13:26:11.460941: step 1820, loss 1.02471, acc 0.9375, prec 0.0484128, recall 0.840306
2017-12-10T13:26:11.925104: step 1821, loss 0.254147, acc 0.921875, prec 0.0484046, recall 0.840306
2017-12-10T13:26:12.364834: step 1822, loss 1.2058, acc 0.953125, prec 0.048432, recall 0.8404
2017-12-10T13:26:12.823846: step 1823, loss 0.201926, acc 0.921875, prec 0.0484561, recall 0.840494
2017-12-10T13:26:13.264297: step 1824, loss 0.214895, acc 0.9375, prec 0.0484818, recall 0.840588
2017-12-10T13:26:13.703605: step 1825, loss 0.183913, acc 0.890625, prec 0.0485348, recall 0.840776
2017-12-10T13:26:14.145873: step 1826, loss 0.168468, acc 0.953125, prec 0.0485299, recall 0.840776
2017-12-10T13:26:14.604700: step 1827, loss 0.958823, acc 0.953125, prec 0.0485894, recall 0.840962
2017-12-10T13:26:15.058445: step 1828, loss 0.154562, acc 0.9375, prec 0.0485829, recall 0.840962
2017-12-10T13:26:15.510108: step 1829, loss 4.2124, acc 0.90625, prec 0.0486085, recall 0.84007
2017-12-10T13:26:15.965730: step 1830, loss 0.147277, acc 0.9375, prec 0.0486019, recall 0.84007
2017-12-10T13:26:16.407237: step 1831, loss 0.187741, acc 0.953125, prec 0.0486615, recall 0.840257
2017-12-10T13:26:16.862699: step 1832, loss 0.361805, acc 0.875, prec 0.0486483, recall 0.840257
2017-12-10T13:26:17.308830: step 1833, loss 0.606342, acc 0.828125, prec 0.0486624, recall 0.840351
2017-12-10T13:26:17.751428: step 1834, loss 0.647096, acc 0.828125, prec 0.0486765, recall 0.840444
2017-12-10T13:26:18.191330: step 1835, loss 0.448303, acc 0.890625, prec 0.0486971, recall 0.840537
2017-12-10T13:26:18.651918: step 1836, loss 0.679282, acc 0.796875, prec 0.0487079, recall 0.84063
2017-12-10T13:26:19.080062: step 1837, loss 0.378005, acc 0.890625, prec 0.0487285, recall 0.840723
2017-12-10T13:26:19.529461: step 1838, loss 1.00825, acc 0.765625, prec 0.0487038, recall 0.840723
2017-12-10T13:26:19.961493: step 1839, loss 1.11682, acc 0.75, prec 0.0487418, recall 0.840909
2017-12-10T13:26:20.408722: step 1840, loss 0.640073, acc 0.8125, prec 0.048722, recall 0.840909
2017-12-10T13:26:20.848090: step 1841, loss 0.488471, acc 0.875, prec 0.048741, recall 0.841002
2017-12-10T13:26:21.295941: step 1842, loss 0.614097, acc 0.859375, prec 0.0487583, recall 0.841094
2017-12-10T13:26:21.747332: step 1843, loss 0.496866, acc 0.796875, prec 0.048769, recall 0.841187
2017-12-10T13:26:22.196452: step 1844, loss 0.525405, acc 0.84375, prec 0.0487525, recall 0.841187
2017-12-10T13:26:22.636688: step 1845, loss 0.449302, acc 0.875, prec 0.0487394, recall 0.841187
2017-12-10T13:26:23.080673: step 1846, loss 0.498779, acc 0.890625, prec 0.0487279, recall 0.841187
2017-12-10T13:26:23.530081: step 1847, loss 0.293642, acc 0.84375, prec 0.0487756, recall 0.841371
2017-12-10T13:26:23.963209: step 1848, loss 0.420056, acc 0.90625, prec 0.0487657, recall 0.841371
2017-12-10T13:26:24.409648: step 1849, loss 0.305809, acc 0.890625, prec 0.0487862, recall 0.841463
2017-12-10T13:26:24.857211: step 1850, loss 0.429996, acc 0.890625, prec 0.0488388, recall 0.841647
2017-12-10T13:26:25.297992: step 1851, loss 0.411151, acc 0.9375, prec 0.0488322, recall 0.841647
2017-12-10T13:26:25.734367: step 1852, loss 0.0874766, acc 0.953125, prec 0.0488593, recall 0.841739
2017-12-10T13:26:26.199930: step 1853, loss 0.244021, acc 0.90625, prec 0.0488814, recall 0.841831
2017-12-10T13:26:26.645143: step 1854, loss 0.203476, acc 0.953125, prec 0.0489085, recall 0.841922
2017-12-10T13:26:27.082192: step 1855, loss 0.125673, acc 0.984375, prec 0.0489388, recall 0.842014
2017-12-10T13:26:27.525421: step 1856, loss 0.139989, acc 0.953125, prec 0.0489339, recall 0.842014
2017-12-10T13:26:27.978616: step 1857, loss 0.24286, acc 0.9375, prec 0.0489593, recall 0.842105
2017-12-10T13:26:28.421061: step 1858, loss 0.191788, acc 0.9375, prec 0.0489527, recall 0.842105
2017-12-10T13:26:28.863167: step 1859, loss 0.0608651, acc 0.984375, prec 0.048951, recall 0.842105
2017-12-10T13:26:29.318871: step 1860, loss 0.204947, acc 0.953125, prec 0.0489461, recall 0.842105
2017-12-10T13:26:29.784054: step 1861, loss 0.0902128, acc 0.953125, prec 0.0489731, recall 0.842197
2017-12-10T13:26:30.235134: step 1862, loss 0.0963547, acc 0.984375, prec 0.0490035, recall 0.842288
2017-12-10T13:26:30.698432: step 1863, loss 1.42857, acc 0.9375, prec 0.0489985, recall 0.841801
2017-12-10T13:26:31.157603: step 1864, loss 3.72432, acc 0.96875, prec 0.0489969, recall 0.841316
2017-12-10T13:26:31.600893: step 1865, loss 0.473341, acc 0.984375, prec 0.0490591, recall 0.841499
2017-12-10T13:26:32.042337: step 1866, loss 0.166698, acc 0.953125, prec 0.0490542, recall 0.841499
2017-12-10T13:26:32.494311: step 1867, loss 0.352238, acc 0.890625, prec 0.0490746, recall 0.84159
2017-12-10T13:26:32.944121: step 1868, loss 0.241737, acc 0.953125, prec 0.0490697, recall 0.84159
2017-12-10T13:26:33.390099: step 1869, loss 0.165551, acc 0.9375, prec 0.0491269, recall 0.841772
2017-12-10T13:26:33.833642: step 1870, loss 0.167318, acc 0.9375, prec 0.0491842, recall 0.841954
2017-12-10T13:26:34.270188: step 1871, loss 0.777324, acc 0.90625, prec 0.0492381, recall 0.842135
2017-12-10T13:26:34.723099: step 1872, loss 1.23237, acc 0.921875, prec 0.0492617, recall 0.842226
2017-12-10T13:26:35.178788: step 1873, loss 0.32332, acc 0.90625, prec 0.0492837, recall 0.842317
2017-12-10T13:26:35.625650: step 1874, loss 0.574557, acc 0.859375, prec 0.0493007, recall 0.842407
2017-12-10T13:26:36.081594: step 1875, loss 0.363623, acc 0.84375, prec 0.0492842, recall 0.842407
2017-12-10T13:26:36.524212: step 1876, loss 0.687556, acc 0.796875, prec 0.0492627, recall 0.842407
2017-12-10T13:26:36.958428: step 1877, loss 0.464992, acc 0.875, prec 0.0492814, recall 0.842497
2017-12-10T13:26:37.403831: step 1878, loss 0.603032, acc 0.84375, prec 0.0492967, recall 0.842587
2017-12-10T13:26:37.879450: step 1879, loss 0.549242, acc 0.875, prec 0.0493153, recall 0.842677
2017-12-10T13:26:38.339699: step 1880, loss 0.320884, acc 0.796875, prec 0.0492939, recall 0.842677
2017-12-10T13:26:38.791596: step 1881, loss 0.234746, acc 0.890625, prec 0.0493142, recall 0.842767
2017-12-10T13:26:39.239523: step 1882, loss 0.610069, acc 0.84375, prec 0.0492977, recall 0.842767
2017-12-10T13:26:39.686831: step 1883, loss 0.678564, acc 0.828125, prec 0.0492795, recall 0.842767
2017-12-10T13:26:40.137650: step 1884, loss 0.379818, acc 0.875, prec 0.0492664, recall 0.842767
2017-12-10T13:26:40.570252: step 1885, loss 0.260036, acc 0.875, prec 0.0493167, recall 0.842947
2017-12-10T13:26:41.034838: step 1886, loss 0.421416, acc 0.90625, prec 0.0493703, recall 0.843126
2017-12-10T13:26:41.485155: step 1887, loss 0.411351, acc 0.859375, prec 0.049419, recall 0.843305
2017-12-10T13:26:41.932809: step 1888, loss 0.65604, acc 0.765625, prec 0.0494577, recall 0.843483
2017-12-10T13:26:42.384078: step 1889, loss 0.210784, acc 0.921875, prec 0.0494495, recall 0.843483
2017-12-10T13:26:42.834278: step 1890, loss 0.0715498, acc 0.96875, prec 0.0494462, recall 0.843483
2017-12-10T13:26:43.274624: step 1891, loss 0.144034, acc 0.953125, prec 0.0495363, recall 0.84375
2017-12-10T13:26:43.709937: step 1892, loss 0.248464, acc 0.9375, prec 0.0495614, recall 0.843839
2017-12-10T13:26:44.153061: step 1893, loss 0.425573, acc 0.90625, prec 0.0496149, recall 0.844016
2017-12-10T13:26:44.602230: step 1894, loss 0.18839, acc 0.9375, prec 0.0496083, recall 0.844016
2017-12-10T13:26:45.073133: step 1895, loss 0.844212, acc 0.890625, prec 0.0496284, recall 0.844104
2017-12-10T13:26:45.519125: step 1896, loss 0.0399138, acc 0.984375, prec 0.0496267, recall 0.844104
2017-12-10T13:26:45.964527: step 1897, loss 0.25266, acc 0.921875, prec 0.0496501, recall 0.844193
2017-12-10T13:26:46.413446: step 1898, loss 0.144711, acc 0.9375, prec 0.0496752, recall 0.844281
2017-12-10T13:26:46.865871: step 1899, loss 0.373228, acc 0.921875, prec 0.0496985, recall 0.844369
2017-12-10T13:26:47.316269: step 1900, loss 3.30694, acc 0.921875, prec 0.0497552, recall 0.844068
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-1900

2017-12-10T13:26:49.405279: step 1901, loss 0.141159, acc 0.96875, prec 0.0497519, recall 0.844068
2017-12-10T13:26:49.850164: step 1902, loss 0.401657, acc 0.875, prec 0.0497387, recall 0.844068
2017-12-10T13:26:50.298242: step 1903, loss 0.286584, acc 0.890625, prec 0.0497587, recall 0.844156
2017-12-10T13:26:50.749010: step 1904, loss 0.444787, acc 0.859375, prec 0.0497438, recall 0.844156
2017-12-10T13:26:51.196687: step 1905, loss 0.257774, acc 0.921875, prec 0.0497355, recall 0.844156
2017-12-10T13:26:51.643449: step 1906, loss 0.381765, acc 0.890625, prec 0.0497555, recall 0.844244
2017-12-10T13:26:52.090990: step 1907, loss 0.379948, acc 0.828125, prec 0.0497689, recall 0.844332
2017-12-10T13:26:52.542570: step 1908, loss 0.230277, acc 0.875, prec 0.0497557, recall 0.844332
2017-12-10T13:26:52.987771: step 1909, loss 1.10338, acc 0.953125, prec 0.0498139, recall 0.844507
2017-12-10T13:26:53.445915: step 1910, loss 0.281391, acc 0.890625, prec 0.0498023, recall 0.844507
2017-12-10T13:26:53.898050: step 1911, loss 0.42112, acc 0.921875, prec 0.0498256, recall 0.844595
2017-12-10T13:26:54.333010: step 1912, loss 0.172154, acc 0.953125, prec 0.0498522, recall 0.844682
2017-12-10T13:26:54.787877: step 1913, loss 0.224056, acc 0.96875, prec 0.0498804, recall 0.844769
2017-12-10T13:26:55.243478: step 1914, loss 0.438565, acc 0.859375, prec 0.0498971, recall 0.844857
2017-12-10T13:26:55.692809: step 1915, loss 0.457447, acc 0.875, prec 0.0498838, recall 0.844857
2017-12-10T13:26:56.138544: step 1916, loss 0.114881, acc 0.953125, prec 0.0498789, recall 0.844857
2017-12-10T13:26:56.591204: step 1917, loss 0.242732, acc 0.953125, prec 0.0499054, recall 0.844944
2017-12-10T13:26:57.036499: step 1918, loss 0.568728, acc 0.890625, prec 0.0499569, recall 0.845118
2017-12-10T13:26:57.481338: step 1919, loss 6.77153, acc 0.90625, prec 0.0499502, recall 0.84417
2017-12-10T13:26:57.941569: step 1920, loss 0.558786, acc 0.90625, prec 0.0499718, recall 0.844258
2017-12-10T13:26:58.385012: step 1921, loss 0.403785, acc 0.84375, prec 0.0499552, recall 0.844258
2017-12-10T13:26:58.831643: step 1922, loss 0.263299, acc 0.90625, prec 0.0499453, recall 0.844258
2017-12-10T13:26:59.271469: step 1923, loss 0.436407, acc 0.90625, prec 0.0499669, recall 0.844345
2017-12-10T13:26:59.728575: step 1924, loss 0.831478, acc 0.8125, prec 0.0500099, recall 0.844519
2017-12-10T13:27:00.173395: step 1925, loss 0.519437, acc 0.875, prec 0.0500281, recall 0.844606
2017-12-10T13:27:00.622824: step 1926, loss 0.345352, acc 0.875, prec 0.0500778, recall 0.844779
2017-12-10T13:27:01.066738: step 1927, loss 1.03953, acc 0.8125, prec 0.0501208, recall 0.844953
2017-12-10T13:27:01.527532: step 1928, loss 0.94045, acc 0.734375, prec 0.0501554, recall 0.845125
2017-12-10T13:27:01.996077: step 1929, loss 0.435808, acc 0.859375, prec 0.0501405, recall 0.845125
2017-12-10T13:27:02.454647: step 1930, loss 0.508874, acc 0.75, prec 0.050114, recall 0.845125
2017-12-10T13:27:02.899181: step 1931, loss 0.611469, acc 0.8125, prec 0.0501568, recall 0.845298
2017-12-10T13:27:03.354871: step 1932, loss 0.355268, acc 0.921875, prec 0.0501486, recall 0.845298
2017-12-10T13:27:03.798177: step 1933, loss 0.569035, acc 0.890625, prec 0.050137, recall 0.845298
2017-12-10T13:27:04.233936: step 1934, loss 0.404, acc 0.859375, prec 0.0501848, recall 0.84547
2017-12-10T13:27:04.677046: step 1935, loss 0.735434, acc 0.796875, prec 0.0502572, recall 0.845727
2017-12-10T13:27:05.120939: step 1936, loss 0.281011, acc 0.90625, prec 0.0502473, recall 0.845727
2017-12-10T13:27:05.570089: step 1937, loss 0.34101, acc 0.90625, prec 0.0502687, recall 0.845813
2017-12-10T13:27:06.002295: step 1938, loss 0.467967, acc 0.84375, prec 0.0502521, recall 0.845813
2017-12-10T13:27:06.438803: step 1939, loss 0.330873, acc 0.84375, prec 0.0502668, recall 0.845898
2017-12-10T13:27:06.898942: step 1940, loss 0.547165, acc 0.90625, prec 0.0502882, recall 0.845983
2017-12-10T13:27:07.358727: step 1941, loss 0.268683, acc 0.921875, prec 0.0502799, recall 0.845983
2017-12-10T13:27:07.796342: step 1942, loss 0.11866, acc 0.953125, prec 0.0502749, recall 0.845983
2017-12-10T13:27:08.239602: step 1943, loss 0.145229, acc 0.90625, prec 0.0502962, recall 0.846069
2017-12-10T13:27:08.681444: step 1944, loss 0.311925, acc 0.9375, prec 0.0503209, recall 0.846154
2017-12-10T13:27:09.138071: step 1945, loss 0.354073, acc 0.84375, prec 0.0503043, recall 0.846154
2017-12-10T13:27:09.582322: step 1946, loss 0.28957, acc 0.9375, prec 0.0503289, recall 0.846239
2017-12-10T13:27:10.031940: step 1947, loss 0.205539, acc 0.90625, prec 0.0503502, recall 0.846324
2017-12-10T13:27:10.471668: step 1948, loss 0.825013, acc 0.96875, prec 0.0503782, recall 0.846409
2017-12-10T13:27:10.904955: step 1949, loss 0.0657773, acc 0.96875, prec 0.0504061, recall 0.846494
2017-12-10T13:27:11.357677: step 1950, loss 1.01103, acc 0.984375, prec 0.0504356, recall 0.846578
2017-12-10T13:27:11.814569: step 1951, loss 0.225833, acc 0.90625, prec 0.0504881, recall 0.846748
2017-12-10T13:27:12.259890: step 1952, loss 0.143615, acc 0.953125, prec 0.0505143, recall 0.846832
2017-12-10T13:27:12.713904: step 1953, loss 3.55152, acc 0.90625, prec 0.0505684, recall 0.846535
2017-12-10T13:27:13.163836: step 1954, loss 0.0730887, acc 0.96875, prec 0.0505651, recall 0.846535
2017-12-10T13:27:13.596700: step 1955, loss 0.900475, acc 0.890625, prec 0.050647, recall 0.846787
2017-12-10T13:27:14.053229: step 1956, loss 0.224863, acc 0.90625, prec 0.0506371, recall 0.846787
2017-12-10T13:27:14.496414: step 1957, loss 0.532965, acc 0.859375, prec 0.0506221, recall 0.846787
2017-12-10T13:27:14.948539: step 1958, loss 0.294036, acc 0.859375, prec 0.0506072, recall 0.846787
2017-12-10T13:27:15.395337: step 1959, loss 0.94314, acc 0.828125, prec 0.0505889, recall 0.846787
2017-12-10T13:27:15.841828: step 1960, loss 0.267334, acc 0.90625, prec 0.0506101, recall 0.846872
2017-12-10T13:27:16.292157: step 1961, loss 0.611308, acc 0.75, prec 0.0505835, recall 0.846872
2017-12-10T13:27:16.733625: step 1962, loss 0.663406, acc 0.8125, prec 0.0505948, recall 0.846956
2017-12-10T13:27:17.167658: step 1963, loss 0.74542, acc 0.75, prec 0.0506926, recall 0.847291
2017-12-10T13:27:17.608706: step 1964, loss 0.500497, acc 0.8125, prec 0.0507038, recall 0.847374
2017-12-10T13:27:18.057464: step 1965, loss 0.530839, acc 0.84375, prec 0.0507182, recall 0.847458
2017-12-10T13:27:18.515388: step 1966, loss 0.757228, acc 0.8125, prec 0.0507294, recall 0.847541
2017-12-10T13:27:18.953775: step 1967, loss 0.439131, acc 0.859375, prec 0.0507455, recall 0.847624
2017-12-10T13:27:19.402417: step 1968, loss 0.221342, acc 0.921875, prec 0.0507682, recall 0.847707
2017-12-10T13:27:19.846116: step 1969, loss 2.01722, acc 0.890625, prec 0.0507583, recall 0.847245
2017-12-10T13:27:20.292118: step 1970, loss 0.380878, acc 0.875, prec 0.050807, recall 0.847411
2017-12-10T13:27:20.729591: step 1971, loss 0.261239, acc 0.90625, prec 0.0507971, recall 0.847411
2017-12-10T13:27:21.177497: step 1972, loss 0.955709, acc 0.765625, prec 0.0508342, recall 0.847578
2017-12-10T13:27:21.620973: step 1973, loss 0.634834, acc 0.796875, prec 0.0508436, recall 0.84766
2017-12-10T13:27:22.057933: step 1974, loss 0.448058, acc 0.890625, prec 0.0508629, recall 0.847743
2017-12-10T13:27:22.515702: step 1975, loss 0.471922, acc 0.890625, prec 0.0508513, recall 0.847743
2017-12-10T13:27:22.966245: step 1976, loss 0.460011, acc 0.875, prec 0.0508381, recall 0.847743
2017-12-10T13:27:23.429181: step 1977, loss 0.253471, acc 0.90625, prec 0.0508281, recall 0.847743
2017-12-10T13:27:23.869945: step 1978, loss 0.108731, acc 0.9375, prec 0.0508524, recall 0.847826
2017-12-10T13:27:24.320587: step 1979, loss 3.82033, acc 0.90625, prec 0.050906, recall 0.847531
2017-12-10T13:27:24.772762: step 1980, loss 2.92746, acc 0.890625, prec 0.0508961, recall 0.847072
2017-12-10T13:27:25.220419: step 1981, loss 0.24968, acc 0.90625, prec 0.050917, recall 0.847154
2017-12-10T13:27:25.669770: step 1982, loss 0.370156, acc 0.921875, prec 0.0509396, recall 0.847237
2017-12-10T13:27:26.124095: step 1983, loss 0.431015, acc 0.859375, prec 0.0509247, recall 0.847237
2017-12-10T13:27:26.559851: step 1984, loss 0.359668, acc 0.84375, prec 0.0509081, recall 0.847237
2017-12-10T13:27:26.999957: step 1985, loss 0.897326, acc 0.828125, prec 0.0509208, recall 0.84732
2017-12-10T13:27:27.449214: step 1986, loss 0.795677, acc 0.78125, prec 0.0509902, recall 0.847568
2017-12-10T13:27:27.897765: step 1987, loss 0.772896, acc 0.734375, prec 0.0510237, recall 0.847732
2017-12-10T13:27:28.295253: step 1988, loss 0.362419, acc 0.903846, prec 0.0510154, recall 0.847732
2017-12-10T13:27:28.763156: step 1989, loss 0.86923, acc 0.78125, prec 0.0509922, recall 0.847732
2017-12-10T13:27:29.200901: step 1990, loss 0.695187, acc 0.84375, prec 0.0509757, recall 0.847732
2017-12-10T13:27:29.644404: step 1991, loss 0.350875, acc 0.859375, prec 0.0509608, recall 0.847732
2017-12-10T13:27:30.098184: step 1992, loss 0.55286, acc 0.796875, prec 0.0509393, recall 0.847732
2017-12-10T13:27:30.534679: step 1993, loss 0.45038, acc 0.84375, prec 0.0509228, recall 0.847732
2017-12-10T13:27:30.982278: step 1994, loss 0.386817, acc 0.890625, prec 0.0509728, recall 0.847896
2017-12-10T13:27:31.425905: step 1995, loss 0.56899, acc 0.84375, prec 0.050987, recall 0.847978
2017-12-10T13:27:31.876409: step 1996, loss 0.352582, acc 0.875, prec 0.0510045, recall 0.84806
2017-12-10T13:27:32.322123: step 1997, loss 0.576637, acc 0.890625, prec 0.050993, recall 0.84806
2017-12-10T13:27:32.754469: step 1998, loss 0.430932, acc 0.859375, prec 0.0510396, recall 0.848224
2017-12-10T13:27:33.189225: step 1999, loss 0.426252, acc 0.921875, prec 0.0510313, recall 0.848224
2017-12-10T13:27:33.620175: step 2000, loss 0.290206, acc 0.84375, prec 0.0510762, recall 0.848387
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2000

2017-12-10T13:27:35.490456: step 2001, loss 0.268436, acc 0.921875, prec 0.0510987, recall 0.848469
2017-12-10T13:27:35.944609: step 2002, loss 0.207497, acc 0.921875, prec 0.0510904, recall 0.848469
2017-12-10T13:27:36.397854: step 2003, loss 0.297212, acc 0.890625, prec 0.0510788, recall 0.848469
2017-12-10T13:27:36.835668: step 2004, loss 2.02175, acc 0.9375, prec 0.0511046, recall 0.848094
2017-12-10T13:27:37.290753: step 2005, loss 0.192735, acc 0.921875, prec 0.0510963, recall 0.848094
2017-12-10T13:27:37.738383: step 2006, loss 0.147952, acc 0.96875, prec 0.051093, recall 0.848094
2017-12-10T13:27:38.175870: step 2007, loss 0.181511, acc 0.90625, prec 0.0510831, recall 0.848094
2017-12-10T13:27:38.622012: step 2008, loss 0.203624, acc 0.921875, prec 0.0511055, recall 0.848176
2017-12-10T13:27:39.069726: step 2009, loss 0.307421, acc 0.90625, prec 0.0511263, recall 0.848257
2017-12-10T13:27:39.518514: step 2010, loss 0.191027, acc 0.9375, prec 0.0511197, recall 0.848257
2017-12-10T13:27:39.961742: step 2011, loss 0.822968, acc 0.921875, prec 0.0511421, recall 0.848339
2017-12-10T13:27:40.408250: step 2012, loss 0.0814175, acc 0.96875, prec 0.0511388, recall 0.848339
2017-12-10T13:27:40.851634: step 2013, loss 0.0910247, acc 0.984375, prec 0.0511677, recall 0.84842
2017-12-10T13:27:41.297829: step 2014, loss 2.02223, acc 0.921875, prec 0.0511611, recall 0.847966
2017-12-10T13:27:41.744802: step 2015, loss 0.204399, acc 0.9375, prec 0.0511545, recall 0.847966
2017-12-10T13:27:42.187165: step 2016, loss 0.205093, acc 0.96875, prec 0.0511819, recall 0.848047
2017-12-10T13:27:42.635338: step 2017, loss 0.0995749, acc 0.96875, prec 0.0512092, recall 0.848128
2017-12-10T13:27:43.075679: step 2018, loss 0.150837, acc 0.96875, prec 0.0512365, recall 0.84821
2017-12-10T13:27:43.521693: step 2019, loss 1.24872, acc 0.96875, prec 0.0512961, recall 0.847919
2017-12-10T13:27:43.973853: step 2020, loss 0.195528, acc 0.921875, prec 0.0512878, recall 0.847919
2017-12-10T13:27:44.418828: step 2021, loss 0.409876, acc 0.90625, prec 0.0512779, recall 0.847919
2017-12-10T13:27:44.867341: step 2022, loss 0.235196, acc 0.921875, prec 0.0512696, recall 0.847919
2017-12-10T13:27:45.313827: step 2023, loss 0.416012, acc 0.921875, prec 0.0513226, recall 0.848081
2017-12-10T13:27:45.751565: step 2024, loss 0.241587, acc 0.9375, prec 0.051316, recall 0.848081
2017-12-10T13:27:46.195528: step 2025, loss 0.353847, acc 0.90625, prec 0.0513366, recall 0.848162
2017-12-10T13:27:46.636772: step 2026, loss 0.270541, acc 0.890625, prec 0.051325, recall 0.848162
2017-12-10T13:27:47.095064: step 2027, loss 0.57491, acc 0.875, prec 0.0513118, recall 0.848162
2017-12-10T13:27:47.538870: step 2028, loss 0.183483, acc 0.90625, prec 0.0513019, recall 0.848162
2017-12-10T13:27:47.997958: step 2029, loss 0.160539, acc 0.921875, prec 0.0513242, recall 0.848243
2017-12-10T13:27:48.433004: step 2030, loss 0.169056, acc 0.953125, prec 0.0513803, recall 0.848404
2017-12-10T13:27:48.876735: step 2031, loss 0.192388, acc 0.921875, prec 0.0513721, recall 0.848404
2017-12-10T13:27:49.327870: step 2032, loss 0.283845, acc 0.921875, prec 0.0513638, recall 0.848404
2017-12-10T13:27:49.779878: step 2033, loss 0.23567, acc 0.9375, prec 0.0513572, recall 0.848404
2017-12-10T13:27:50.224434: step 2034, loss 0.780159, acc 0.90625, prec 0.0513778, recall 0.848485
2017-12-10T13:27:50.683437: step 2035, loss 0.237365, acc 0.859375, prec 0.0513934, recall 0.848565
2017-12-10T13:27:51.127984: step 2036, loss 0.447035, acc 0.875, prec 0.0513802, recall 0.848565
2017-12-10T13:27:51.570811: step 2037, loss 0.228764, acc 0.9375, prec 0.0514041, recall 0.848646
2017-12-10T13:27:52.013222: step 2038, loss 0.297933, acc 0.90625, prec 0.0513942, recall 0.848646
2017-12-10T13:27:52.455996: step 2039, loss 0.271277, acc 0.890625, prec 0.0514131, recall 0.848726
2017-12-10T13:27:52.902548: step 2040, loss 0.46853, acc 0.921875, prec 0.0514354, recall 0.848806
2017-12-10T13:27:53.356319: step 2041, loss 0.574941, acc 0.890625, prec 0.0514543, recall 0.848887
2017-12-10T13:27:53.819367: step 2042, loss 0.128378, acc 0.96875, prec 0.051451, recall 0.848887
2017-12-10T13:27:54.272804: step 2043, loss 0.288634, acc 0.9375, prec 0.0514748, recall 0.848967
2017-12-10T13:27:54.728156: step 2044, loss 0.0651052, acc 0.96875, prec 0.051502, recall 0.849047
2017-12-10T13:27:55.188905: step 2045, loss 0.100102, acc 0.96875, prec 0.0515596, recall 0.849206
2017-12-10T13:27:55.632122: step 2046, loss 0.171249, acc 0.9375, prec 0.0515835, recall 0.849286
2017-12-10T13:27:56.067358: step 2047, loss 0.313267, acc 0.9375, prec 0.0515769, recall 0.849286
2017-12-10T13:27:56.497236: step 2048, loss 0.198745, acc 0.953125, prec 0.0516023, recall 0.849366
2017-12-10T13:27:56.933001: step 2049, loss 0.0854835, acc 0.984375, prec 0.0516007, recall 0.849366
2017-12-10T13:27:57.380720: step 2050, loss 0.13387, acc 0.953125, prec 0.0516262, recall 0.849445
2017-12-10T13:27:57.826128: step 2051, loss 0.331441, acc 0.953125, prec 0.0516516, recall 0.849525
2017-12-10T13:27:58.288937: step 2052, loss 0.0606301, acc 0.96875, prec 0.0516483, recall 0.849525
2017-12-10T13:27:58.738121: step 2053, loss 0.159506, acc 0.9375, prec 0.0516417, recall 0.849525
2017-12-10T13:27:59.193355: step 2054, loss 1.2237, acc 0.9375, prec 0.0516655, recall 0.849604
2017-12-10T13:27:59.643384: step 2055, loss 0.14438, acc 0.953125, prec 0.0516909, recall 0.849684
2017-12-10T13:28:00.081540: step 2056, loss 0.255566, acc 0.90625, prec 0.0517114, recall 0.849763
2017-12-10T13:28:00.530881: step 2057, loss 0.31726, acc 0.90625, prec 0.0517015, recall 0.849763
2017-12-10T13:28:00.988452: step 2058, loss 0.613553, acc 0.96875, prec 0.051759, recall 0.849921
2017-12-10T13:28:01.436519: step 2059, loss 0.477904, acc 0.984375, prec 0.0517877, recall 0.85
2017-12-10T13:28:01.889387: step 2060, loss 0.125417, acc 0.9375, prec 0.0517811, recall 0.85
2017-12-10T13:28:02.326774: step 2061, loss 0.141709, acc 0.953125, prec 0.0518369, recall 0.850158
2017-12-10T13:28:02.770831: step 2062, loss 0.106211, acc 0.953125, prec 0.0518319, recall 0.850158
2017-12-10T13:28:03.219259: step 2063, loss 0.0404214, acc 0.984375, prec 0.0518302, recall 0.850158
2017-12-10T13:28:03.662976: step 2064, loss 0.156309, acc 0.96875, prec 0.0518877, recall 0.850315
2017-12-10T13:28:04.102615: step 2065, loss 0.11429, acc 0.9375, prec 0.051881, recall 0.850315
2017-12-10T13:28:04.542271: step 2066, loss 0.121947, acc 0.96875, prec 0.0519081, recall 0.850394
2017-12-10T13:28:04.981787: step 2067, loss 0.465149, acc 0.890625, prec 0.0519268, recall 0.850472
2017-12-10T13:28:05.427862: step 2068, loss 1.53163, acc 0.859375, prec 0.0519439, recall 0.850105
2017-12-10T13:28:05.875340: step 2069, loss 0.189529, acc 0.921875, prec 0.0519356, recall 0.850105
2017-12-10T13:28:06.323832: step 2070, loss 1.06487, acc 0.9375, prec 0.0519896, recall 0.850262
2017-12-10T13:28:06.784341: step 2071, loss 0.145991, acc 0.96875, prec 0.0519863, recall 0.850262
2017-12-10T13:28:07.222645: step 2072, loss 1.47345, acc 0.8125, prec 0.0519967, recall 0.85034
2017-12-10T13:28:07.658362: step 2073, loss 0.362738, acc 0.875, prec 0.0519834, recall 0.85034
2017-12-10T13:28:08.089339: step 2074, loss 4.1773, acc 0.859375, prec 0.0519701, recall 0.849895
2017-12-10T13:28:08.543888: step 2075, loss 0.396088, acc 0.875, prec 0.0519568, recall 0.849895
2017-12-10T13:28:08.979146: step 2076, loss 0.941751, acc 0.8125, prec 0.0519368, recall 0.849895
2017-12-10T13:28:09.424741: step 2077, loss 0.979709, acc 0.703125, prec 0.0519356, recall 0.849974
2017-12-10T13:28:09.853917: step 2078, loss 0.925519, acc 0.734375, prec 0.0519074, recall 0.849974
2017-12-10T13:28:10.292072: step 2079, loss 1.17317, acc 0.734375, prec 0.0519095, recall 0.850052
2017-12-10T13:28:10.733041: step 2080, loss 1.10416, acc 0.71875, prec 0.0519402, recall 0.850209
2017-12-10T13:28:11.169848: step 2081, loss 1.57531, acc 0.734375, prec 0.0519725, recall 0.850365
2017-12-10T13:28:11.616174: step 2082, loss 0.65278, acc 0.828125, prec 0.0519543, recall 0.850365
2017-12-10T13:28:12.050829: step 2083, loss 0.757663, acc 0.765625, prec 0.0519294, recall 0.850365
2017-12-10T13:28:12.496470: step 2084, loss 1.15397, acc 0.71875, prec 0.0518997, recall 0.850365
2017-12-10T13:28:12.943393: step 2085, loss 1.0044, acc 0.765625, prec 0.0518749, recall 0.850365
2017-12-10T13:28:13.384712: step 2086, loss 0.418785, acc 0.859375, prec 0.0518601, recall 0.850365
2017-12-10T13:28:13.823000: step 2087, loss 0.843446, acc 0.8125, prec 0.0518705, recall 0.850443
2017-12-10T13:28:14.278065: step 2088, loss 0.632675, acc 0.8125, prec 0.0518507, recall 0.850443
2017-12-10T13:28:14.715491: step 2089, loss 0.394185, acc 0.890625, prec 0.0518391, recall 0.850443
2017-12-10T13:28:15.166504: step 2090, loss 0.336361, acc 0.875, prec 0.051826, recall 0.850443
2017-12-10T13:28:15.598530: step 2091, loss 0.193024, acc 0.921875, prec 0.0519081, recall 0.850676
2017-12-10T13:28:16.044570: step 2092, loss 0.224175, acc 0.96875, prec 0.0519048, recall 0.850676
2017-12-10T13:28:16.511680: step 2093, loss 0.416375, acc 0.859375, prec 0.0518899, recall 0.850676
2017-12-10T13:28:16.965822: step 2094, loss 0.500253, acc 0.9375, prec 0.0519435, recall 0.850832
2017-12-10T13:28:17.419773: step 2095, loss 0.149258, acc 0.9375, prec 0.0519971, recall 0.850986
2017-12-10T13:28:17.861742: step 2096, loss 0.0542328, acc 0.96875, prec 0.0519938, recall 0.850986
2017-12-10T13:28:18.313473: step 2097, loss 0.173132, acc 0.9375, prec 0.0519872, recall 0.850986
2017-12-10T13:28:18.766927: step 2098, loss 1.41053, acc 0.96875, prec 0.0520156, recall 0.850622
2017-12-10T13:28:19.214770: step 2099, loss 0.228333, acc 0.96875, prec 0.0520424, recall 0.8507
2017-12-10T13:28:19.663614: step 2100, loss 0.0910066, acc 0.953125, prec 0.0520374, recall 0.8507
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2100

2017-12-10T13:28:21.489627: step 2101, loss 0.291717, acc 0.9375, prec 0.0520308, recall 0.8507
2017-12-10T13:28:21.937700: step 2102, loss 0.591059, acc 0.96875, prec 0.0520576, recall 0.850777
2017-12-10T13:28:22.381393: step 2103, loss 0.354196, acc 0.9375, prec 0.052081, recall 0.850854
2017-12-10T13:28:22.832782: step 2104, loss 0.205295, acc 0.921875, prec 0.0521028, recall 0.850932
2017-12-10T13:28:23.296829: step 2105, loss 0.176468, acc 0.921875, prec 0.0520946, recall 0.850932
2017-12-10T13:28:23.740549: step 2106, loss 0.367149, acc 0.890625, prec 0.052083, recall 0.850932
2017-12-10T13:28:24.196684: step 2107, loss 0.206632, acc 0.9375, prec 0.0521064, recall 0.851009
2017-12-10T13:28:24.626838: step 2108, loss 0.187451, acc 0.96875, prec 0.0521332, recall 0.851086
2017-12-10T13:28:25.085285: step 2109, loss 0.293883, acc 0.96875, prec 0.0521599, recall 0.851163
2017-12-10T13:28:25.541718: step 2110, loss 0.149707, acc 0.9375, prec 0.0521833, recall 0.85124
2017-12-10T13:28:26.005138: step 2111, loss 0.0973902, acc 0.953125, prec 0.0521783, recall 0.85124
2017-12-10T13:28:26.445602: step 2112, loss 0.191122, acc 0.953125, prec 0.0522334, recall 0.851393
2017-12-10T13:28:26.905113: step 2113, loss 0.0178646, acc 1, prec 0.0522634, recall 0.85147
2017-12-10T13:28:27.341099: step 2114, loss 2.3866, acc 0.9375, prec 0.0523184, recall 0.851184
2017-12-10T13:28:27.797170: step 2115, loss 0.104804, acc 0.9375, prec 0.0523118, recall 0.851184
2017-12-10T13:28:28.244448: step 2116, loss 0.429384, acc 0.875, prec 0.0523285, recall 0.851261
2017-12-10T13:28:28.689269: step 2117, loss 0.359524, acc 0.9375, prec 0.0523519, recall 0.851337
2017-12-10T13:28:29.140725: step 2118, loss 0.153753, acc 0.9375, prec 0.0523752, recall 0.851414
2017-12-10T13:28:29.593775: step 2119, loss 0.324644, acc 0.9375, prec 0.0523986, recall 0.85149
2017-12-10T13:28:30.039947: step 2120, loss 0.0958446, acc 0.96875, prec 0.0524252, recall 0.851566
2017-12-10T13:28:30.484601: step 2121, loss 0.409823, acc 0.859375, prec 0.0524702, recall 0.851719
2017-12-10T13:28:30.921877: step 2122, loss 2.66483, acc 0.9375, prec 0.0524952, recall 0.851358
2017-12-10T13:28:31.361501: step 2123, loss 0.436176, acc 0.890625, prec 0.0525135, recall 0.851434
2017-12-10T13:28:31.829063: step 2124, loss 0.302545, acc 0.921875, prec 0.0525351, recall 0.851511
2017-12-10T13:28:32.277522: step 2125, loss 0.585267, acc 0.8125, prec 0.052605, recall 0.851738
2017-12-10T13:28:32.726444: step 2126, loss 0.257837, acc 0.953125, prec 0.0526299, recall 0.851814
2017-12-10T13:28:33.164624: step 2127, loss 0.963808, acc 0.890625, prec 0.0527678, recall 0.852192
2017-12-10T13:28:33.618815: step 2128, loss 0.706568, acc 0.84375, prec 0.0527511, recall 0.852192
2017-12-10T13:28:34.057291: step 2129, loss 1.10878, acc 0.828125, prec 0.0527627, recall 0.852267
2017-12-10T13:28:34.506904: step 2130, loss 0.673616, acc 0.84375, prec 0.0528058, recall 0.852417
2017-12-10T13:28:34.948452: step 2131, loss 0.685844, acc 0.765625, prec 0.0528405, recall 0.852567
2017-12-10T13:28:35.417298: step 2132, loss 0.702741, acc 0.765625, prec 0.0528454, recall 0.852642
2017-12-10T13:28:35.867667: step 2133, loss 0.750199, acc 0.765625, prec 0.0528503, recall 0.852717
2017-12-10T13:28:36.314539: step 2134, loss 0.662751, acc 0.828125, prec 0.052832, recall 0.852717
2017-12-10T13:28:36.756287: step 2135, loss 0.741605, acc 0.875, prec 0.0528187, recall 0.852717
2017-12-10T13:28:37.199853: step 2136, loss 0.73954, acc 0.78125, prec 0.052855, recall 0.852867
2017-12-10T13:28:37.642173: step 2137, loss 0.934783, acc 0.78125, prec 0.0528615, recall 0.852941
2017-12-10T13:28:38.070445: step 2138, loss 0.433512, acc 0.84375, prec 0.0528449, recall 0.852941
2017-12-10T13:28:38.516875: step 2139, loss 0.804101, acc 0.796875, prec 0.0528531, recall 0.853016
2017-12-10T13:28:38.945780: step 2140, loss 0.240348, acc 0.90625, prec 0.0528728, recall 0.85309
2017-12-10T13:28:39.400589: step 2141, loss 0.351684, acc 0.921875, prec 0.0528645, recall 0.85309
2017-12-10T13:28:39.854326: step 2142, loss 0.195128, acc 0.90625, prec 0.0528546, recall 0.85309
2017-12-10T13:28:40.309910: step 2143, loss 0.397865, acc 0.921875, prec 0.0528463, recall 0.85309
2017-12-10T13:28:40.747935: step 2144, loss 0.167986, acc 0.9375, prec 0.0528397, recall 0.85309
2017-12-10T13:28:41.191883: step 2145, loss 0.142877, acc 0.921875, prec 0.0528611, recall 0.853165
2017-12-10T13:28:41.637816: step 2146, loss 0.163452, acc 0.953125, prec 0.0528561, recall 0.853165
2017-12-10T13:28:42.087460: step 2147, loss 1.03179, acc 0.90625, prec 0.0528759, recall 0.853239
2017-12-10T13:28:42.539432: step 2148, loss 0.0705273, acc 0.96875, prec 0.0529023, recall 0.853313
2017-12-10T13:28:43.002536: step 2149, loss 0.317011, acc 0.953125, prec 0.0529567, recall 0.853461
2017-12-10T13:28:43.454158: step 2150, loss 0.210082, acc 0.96875, prec 0.052983, recall 0.853535
2017-12-10T13:28:43.907168: step 2151, loss 0.411344, acc 0.890625, prec 0.0530308, recall 0.853683
2017-12-10T13:28:44.347560: step 2152, loss 0.207284, acc 0.921875, prec 0.0530225, recall 0.853683
2017-12-10T13:28:44.795570: step 2153, loss 0.247912, acc 0.921875, prec 0.0530438, recall 0.853757
2017-12-10T13:28:45.235474: step 2154, loss 0.0199232, acc 1, prec 0.0530438, recall 0.853757
2017-12-10T13:28:45.681110: step 2155, loss 0.0957449, acc 0.953125, prec 0.0530685, recall 0.853831
2017-12-10T13:28:46.127499: step 2156, loss 0.106031, acc 0.96875, prec 0.0530948, recall 0.853904
2017-12-10T13:28:46.567740: step 2157, loss 0.272879, acc 0.90625, prec 0.0531145, recall 0.853978
2017-12-10T13:28:47.011875: step 2158, loss 0.291235, acc 0.90625, prec 0.0531638, recall 0.854125
2017-12-10T13:28:47.477192: step 2159, loss 0.157781, acc 0.921875, prec 0.0531852, recall 0.854198
2017-12-10T13:28:47.936800: step 2160, loss 0.75139, acc 0.984375, prec 0.0532428, recall 0.854345
2017-12-10T13:28:48.377653: step 2161, loss 0.0905745, acc 0.96875, prec 0.0532394, recall 0.854345
2017-12-10T13:28:48.834890: step 2162, loss 1.10181, acc 0.9375, prec 0.0532624, recall 0.854418
2017-12-10T13:28:49.276790: step 2163, loss 0.057763, acc 0.984375, prec 0.0532607, recall 0.854418
2017-12-10T13:28:49.725532: step 2164, loss 0.240318, acc 0.9375, prec 0.0532541, recall 0.854418
2017-12-10T13:28:50.173157: step 2165, loss 0.0669996, acc 0.984375, prec 0.0532524, recall 0.854418
2017-12-10T13:28:50.608737: step 2166, loss 0.182658, acc 0.96875, prec 0.0533379, recall 0.854637
2017-12-10T13:28:51.052946: step 2167, loss 0.198983, acc 0.921875, prec 0.0533296, recall 0.854637
2017-12-10T13:28:51.494124: step 2168, loss 0.0639932, acc 1, prec 0.0533592, recall 0.854709
2017-12-10T13:28:51.936259: step 2169, loss 0.141823, acc 0.984375, prec 0.0533575, recall 0.854709
2017-12-10T13:28:52.394978: step 2170, loss 0.958589, acc 0.96875, prec 0.0533838, recall 0.854782
2017-12-10T13:28:52.858744: step 2171, loss 0.121835, acc 0.9375, prec 0.0534363, recall 0.854927
2017-12-10T13:28:53.309219: step 2172, loss 0.284117, acc 0.953125, prec 0.0534905, recall 0.855072
2017-12-10T13:28:53.747742: step 2173, loss 0.119409, acc 0.96875, prec 0.0535463, recall 0.855217
2017-12-10T13:28:54.195326: step 2174, loss 0.28544, acc 0.90625, prec 0.0535658, recall 0.855289
2017-12-10T13:28:54.646247: step 2175, loss 0.357984, acc 0.875, prec 0.0535525, recall 0.855289
2017-12-10T13:28:55.096812: step 2176, loss 0.42672, acc 0.953125, prec 0.053577, recall 0.855362
2017-12-10T13:28:55.536726: step 2177, loss 0.129962, acc 0.953125, prec 0.0536015, recall 0.855434
2017-12-10T13:28:56.004149: step 2178, loss 0.298281, acc 0.890625, prec 0.0536194, recall 0.855506
2017-12-10T13:28:56.443476: step 2179, loss 0.371263, acc 0.921875, prec 0.0536406, recall 0.855578
2017-12-10T13:28:56.895801: step 2180, loss 0.515321, acc 0.859375, prec 0.0536846, recall 0.855721
2017-12-10T13:28:57.333066: step 2181, loss 0.707756, acc 0.953125, prec 0.0537091, recall 0.855793
2017-12-10T13:28:57.793146: step 2182, loss 0.2396, acc 0.921875, prec 0.0537893, recall 0.856008
2017-12-10T13:28:58.230214: step 2183, loss 0.169138, acc 0.9375, prec 0.0538121, recall 0.856079
2017-12-10T13:28:58.668935: step 2184, loss 0.444003, acc 0.890625, prec 0.0538298, recall 0.856151
2017-12-10T13:28:59.120378: step 2185, loss 0.348417, acc 0.890625, prec 0.0538476, recall 0.856222
2017-12-10T13:28:59.585391: step 2186, loss 0.356363, acc 0.890625, prec 0.0538358, recall 0.856222
2017-12-10T13:29:00.027917: step 2187, loss 0.217488, acc 0.9375, prec 0.0538291, recall 0.856222
2017-12-10T13:29:00.483469: step 2188, loss 0.805374, acc 0.8125, prec 0.053809, recall 0.856222
2017-12-10T13:29:00.933872: step 2189, loss 0.344302, acc 0.890625, prec 0.0537973, recall 0.856222
2017-12-10T13:29:01.385632: step 2190, loss 0.468728, acc 0.859375, prec 0.0537822, recall 0.856222
2017-12-10T13:29:01.835825: step 2191, loss 0.151, acc 0.96875, prec 0.0537788, recall 0.856222
2017-12-10T13:29:02.293315: step 2192, loss 0.293065, acc 0.890625, prec 0.0537671, recall 0.856222
2017-12-10T13:29:02.739799: step 2193, loss 0.173001, acc 0.953125, prec 0.0538505, recall 0.856436
2017-12-10T13:29:03.186702: step 2194, loss 0.102113, acc 0.96875, prec 0.0538471, recall 0.856436
2017-12-10T13:29:03.643781: step 2195, loss 0.143823, acc 0.96875, prec 0.0538438, recall 0.856436
2017-12-10T13:29:04.085139: step 2196, loss 0.192586, acc 0.9375, prec 0.0538371, recall 0.856436
2017-12-10T13:29:04.518622: step 2197, loss 1.47512, acc 0.984375, prec 0.0538959, recall 0.856154
2017-12-10T13:29:04.961156: step 2198, loss 2.9663, acc 0.984375, prec 0.0538959, recall 0.855731
2017-12-10T13:29:05.427051: step 2199, loss 0.394034, acc 0.921875, prec 0.053917, recall 0.855802
2017-12-10T13:29:05.885660: step 2200, loss 0.108201, acc 0.953125, prec 0.0539414, recall 0.855874
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2200

2017-12-10T13:29:07.782428: step 2201, loss 0.200822, acc 0.9375, prec 0.054023, recall 0.856087
2017-12-10T13:29:08.244779: step 2202, loss 0.545521, acc 0.9375, prec 0.0540751, recall 0.856228
2017-12-10T13:29:08.698757: step 2203, loss 0.579685, acc 0.84375, prec 0.0540583, recall 0.856228
2017-12-10T13:29:09.150920: step 2204, loss 0.407091, acc 0.90625, prec 0.0540776, recall 0.856299
2017-12-10T13:29:09.585300: step 2205, loss 0.479264, acc 0.921875, prec 0.0540986, recall 0.85637
2017-12-10T13:29:10.050930: step 2206, loss 0.385726, acc 0.890625, prec 0.0541456, recall 0.856511
2017-12-10T13:29:10.500781: step 2207, loss 0.633071, acc 0.84375, prec 0.0542462, recall 0.856793
2017-12-10T13:29:10.950135: step 2208, loss 0.551105, acc 0.8125, prec 0.054226, recall 0.856793
2017-12-10T13:29:11.393152: step 2209, loss 0.913449, acc 0.765625, prec 0.0542301, recall 0.856863
2017-12-10T13:29:11.828141: step 2210, loss 0.856816, acc 0.8125, prec 0.0542393, recall 0.856933
2017-12-10T13:29:12.275689: step 2211, loss 0.637257, acc 0.796875, prec 0.0542174, recall 0.856933
2017-12-10T13:29:12.712682: step 2212, loss 0.224957, acc 0.9375, prec 0.0542107, recall 0.856933
2017-12-10T13:29:13.157133: step 2213, loss 0.289868, acc 0.890625, prec 0.0542283, recall 0.857003
2017-12-10T13:29:13.607813: step 2214, loss 0.233051, acc 0.90625, prec 0.0542182, recall 0.857003
2017-12-10T13:29:14.066549: step 2215, loss 3.91479, acc 0.84375, prec 0.0542324, recall 0.856654
2017-12-10T13:29:14.513681: step 2216, loss 0.156453, acc 0.921875, prec 0.054224, recall 0.856654
2017-12-10T13:29:14.965453: step 2217, loss 0.397626, acc 0.859375, prec 0.0542381, recall 0.856724
2017-12-10T13:29:15.423731: step 2218, loss 0.533477, acc 0.875, prec 0.0542247, recall 0.856724
2017-12-10T13:29:15.871989: step 2219, loss 0.318691, acc 0.9375, prec 0.0542473, recall 0.856794
2017-12-10T13:29:16.329575: step 2220, loss 0.629224, acc 0.796875, prec 0.0542254, recall 0.856794
2017-12-10T13:29:16.773571: step 2221, loss 0.923093, acc 0.78125, prec 0.054202, recall 0.856794
2017-12-10T13:29:17.208392: step 2222, loss 0.838562, acc 0.78125, prec 0.0541785, recall 0.856794
2017-12-10T13:29:17.648045: step 2223, loss 0.785287, acc 0.78125, prec 0.0541551, recall 0.856794
2017-12-10T13:29:18.080696: step 2224, loss 0.801216, acc 0.765625, prec 0.05413, recall 0.856794
2017-12-10T13:29:18.527351: step 2225, loss 0.574217, acc 0.828125, prec 0.0541116, recall 0.856794
2017-12-10T13:29:18.997623: step 2226, loss 0.497152, acc 0.84375, prec 0.0541825, recall 0.857003
2017-12-10T13:29:19.447874: step 2227, loss 0.783686, acc 0.859375, prec 0.0541966, recall 0.857073
2017-12-10T13:29:19.886888: step 2228, loss 0.338473, acc 0.890625, prec 0.0541849, recall 0.857073
2017-12-10T13:29:20.322618: step 2229, loss 0.110009, acc 0.9375, prec 0.0542366, recall 0.857212
2017-12-10T13:29:20.769413: step 2230, loss 0.328843, acc 0.890625, prec 0.054254, recall 0.857282
2017-12-10T13:29:21.210752: step 2231, loss 0.193215, acc 0.953125, prec 0.054249, recall 0.857282
2017-12-10T13:29:21.655159: step 2232, loss 0.135552, acc 0.9375, prec 0.0542423, recall 0.857282
2017-12-10T13:29:22.089426: step 2233, loss 1.96475, acc 0.890625, prec 0.0542614, recall 0.856934
2017-12-10T13:29:22.539762: step 2234, loss 0.391175, acc 0.90625, prec 0.0542805, recall 0.857004
2017-12-10T13:29:22.998781: step 2235, loss 0.35944, acc 0.890625, prec 0.0542979, recall 0.857073
2017-12-10T13:29:23.445726: step 2236, loss 3.55575, acc 0.890625, prec 0.054317, recall 0.856727
2017-12-10T13:29:23.902440: step 2237, loss 0.340046, acc 0.890625, prec 0.0543053, recall 0.856727
2017-12-10T13:29:24.344458: step 2238, loss 0.237584, acc 0.90625, prec 0.0543244, recall 0.856796
2017-12-10T13:29:24.792204: step 2239, loss 0.375209, acc 0.90625, prec 0.0543435, recall 0.856866
2017-12-10T13:29:25.234643: step 2240, loss 0.198934, acc 0.921875, prec 0.0543351, recall 0.856866
2017-12-10T13:29:25.682483: step 2241, loss 0.379684, acc 0.84375, prec 0.0543184, recall 0.856866
2017-12-10T13:29:26.140747: step 2242, loss 0.770158, acc 0.828125, prec 0.0543873, recall 0.857074
2017-12-10T13:29:26.604109: step 2243, loss 0.351372, acc 0.9375, prec 0.0544387, recall 0.857212
2017-12-10T13:29:27.052492: step 2244, loss 0.378183, acc 0.859375, prec 0.0544527, recall 0.857281
2017-12-10T13:29:27.505588: step 2245, loss 0.85789, acc 0.84375, prec 0.054436, recall 0.857281
2017-12-10T13:29:27.959913: step 2246, loss 0.775456, acc 0.8125, prec 0.0544159, recall 0.857281
2017-12-10T13:29:28.401390: step 2247, loss 0.315697, acc 0.90625, prec 0.0544059, recall 0.857281
2017-12-10T13:29:28.841934: step 2248, loss 0.750891, acc 0.8125, prec 0.0543859, recall 0.857281
2017-12-10T13:29:29.282768: step 2249, loss 0.294142, acc 0.90625, prec 0.0543758, recall 0.857281
2017-12-10T13:29:29.732505: step 2250, loss 0.359739, acc 0.859375, prec 0.0544188, recall 0.857419
2017-12-10T13:29:30.172308: step 2251, loss 0.291193, acc 0.921875, prec 0.0544105, recall 0.857419
2017-12-10T13:29:30.617889: step 2252, loss 0.224742, acc 0.9375, prec 0.0544618, recall 0.857557
2017-12-10T13:29:31.065004: step 2253, loss 0.226522, acc 0.9375, prec 0.0545131, recall 0.857694
2017-12-10T13:29:31.497966: step 2254, loss 0.105244, acc 0.953125, prec 0.0545081, recall 0.857694
2017-12-10T13:29:31.940930: step 2255, loss 0.323956, acc 0.953125, prec 0.0545321, recall 0.857763
2017-12-10T13:29:32.391411: step 2256, loss 0.180363, acc 0.921875, prec 0.0545237, recall 0.857763
2017-12-10T13:29:32.837801: step 2257, loss 0.0975611, acc 0.953125, prec 0.0545477, recall 0.857831
2017-12-10T13:29:33.283824: step 2258, loss 0.141705, acc 0.96875, prec 0.0546023, recall 0.857968
2017-12-10T13:29:33.731400: step 2259, loss 0.194246, acc 0.953125, prec 0.0545973, recall 0.857968
2017-12-10T13:29:34.197127: step 2260, loss 0.239787, acc 0.9375, prec 0.0545906, recall 0.857968
2017-12-10T13:29:34.637405: step 2261, loss 0.111828, acc 0.9375, prec 0.0546128, recall 0.858037
2017-12-10T13:29:35.087476: step 2262, loss 1.34827, acc 0.953125, prec 0.0546674, recall 0.857761
2017-12-10T13:29:35.528845: step 2263, loss 0.229416, acc 0.96875, prec 0.0547509, recall 0.857965
2017-12-10T13:29:35.978917: step 2264, loss 0.218128, acc 0.921875, prec 0.0547425, recall 0.857965
2017-12-10T13:29:36.421979: step 2265, loss 0.13639, acc 0.984375, prec 0.0547698, recall 0.858034
2017-12-10T13:29:36.874088: step 2266, loss 0.565023, acc 1, prec 0.0547987, recall 0.858102
2017-12-10T13:29:37.329473: step 2267, loss 0.239703, acc 0.984375, prec 0.0548549, recall 0.858238
2017-12-10T13:29:37.773604: step 2268, loss 0.220442, acc 0.90625, prec 0.0548738, recall 0.858305
2017-12-10T13:29:38.213659: step 2269, loss 0.364681, acc 0.90625, prec 0.0549215, recall 0.858441
2017-12-10T13:29:38.662497: step 2270, loss 0.0845797, acc 0.9375, prec 0.0549148, recall 0.858441
2017-12-10T13:29:39.107433: step 2271, loss 5.45613, acc 0.828125, prec 0.0549286, recall 0.857689
2017-12-10T13:29:39.557261: step 2272, loss 0.146667, acc 0.921875, prec 0.0549202, recall 0.857689
2017-12-10T13:29:39.980410: step 2273, loss 0.809412, acc 0.84375, prec 0.0549323, recall 0.857757
2017-12-10T13:29:40.437911: step 2274, loss 0.402187, acc 0.84375, prec 0.0550021, recall 0.85796
2017-12-10T13:29:40.899722: step 2275, loss 0.423173, acc 0.890625, prec 0.0550192, recall 0.858028
2017-12-10T13:29:41.354797: step 2276, loss 0.527067, acc 0.796875, prec 0.0550263, recall 0.858095
2017-12-10T13:29:41.801417: step 2277, loss 0.649209, acc 0.796875, prec 0.0550044, recall 0.858095
2017-12-10T13:29:42.250762: step 2278, loss 1.41144, acc 0.703125, prec 0.055059, recall 0.858298
2017-12-10T13:29:42.695654: step 2279, loss 1.0344, acc 0.75, prec 0.0550322, recall 0.858298
2017-12-10T13:29:43.139339: step 2280, loss 0.792733, acc 0.78125, prec 0.0550663, recall 0.858432
2017-12-10T13:29:43.579922: step 2281, loss 1.16877, acc 0.671875, prec 0.0550886, recall 0.858567
2017-12-10T13:29:44.025273: step 2282, loss 0.63486, acc 0.8125, prec 0.055126, recall 0.858701
2017-12-10T13:29:44.474139: step 2283, loss 1.71532, acc 0.734375, prec 0.0551837, recall 0.858902
2017-12-10T13:29:44.916353: step 2284, loss 0.699463, acc 0.75, prec 0.0552143, recall 0.859035
2017-12-10T13:29:45.359194: step 2285, loss 0.436518, acc 0.875, prec 0.0552296, recall 0.859102
2017-12-10T13:29:45.797792: step 2286, loss 0.867457, acc 0.734375, prec 0.0552011, recall 0.859102
2017-12-10T13:29:46.254722: step 2287, loss 0.520613, acc 0.828125, prec 0.0552401, recall 0.859235
2017-12-10T13:29:46.715517: step 2288, loss 0.475121, acc 0.78125, prec 0.0552453, recall 0.859301
2017-12-10T13:29:47.170374: step 2289, loss 0.566921, acc 0.796875, prec 0.0552235, recall 0.859301
2017-12-10T13:29:47.627169: step 2290, loss 0.739663, acc 0.796875, prec 0.055259, recall 0.859434
2017-12-10T13:29:48.077373: step 2291, loss 0.594553, acc 0.828125, prec 0.0552406, recall 0.859434
2017-12-10T13:29:48.519847: step 2292, loss 0.554361, acc 0.84375, prec 0.0552525, recall 0.8595
2017-12-10T13:29:48.963488: step 2293, loss 0.387689, acc 0.859375, prec 0.055266, recall 0.859566
2017-12-10T13:29:49.403863: step 2294, loss 0.583878, acc 0.90625, prec 0.055256, recall 0.859566
2017-12-10T13:29:49.856161: step 2295, loss 1.41673, acc 0.859375, prec 0.0553268, recall 0.859765
2017-12-10T13:29:50.295553: step 2296, loss 0.607182, acc 0.859375, prec 0.0553975, recall 0.859962
2017-12-10T13:29:50.735609: step 2297, loss 0.289765, acc 0.90625, prec 0.055416, recall 0.860028
2017-12-10T13:29:51.186870: step 2298, loss 0.344645, acc 0.921875, prec 0.0554362, recall 0.860094
2017-12-10T13:29:51.641856: step 2299, loss 0.479545, acc 0.859375, prec 0.0554782, recall 0.860225
2017-12-10T13:29:52.089499: step 2300, loss 1.46493, acc 0.90625, prec 0.0554984, recall 0.859888
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2300

2017-12-10T13:29:54.417500: step 2301, loss 0.42081, acc 0.890625, prec 0.0555152, recall 0.859953
2017-12-10T13:29:54.872934: step 2302, loss 0.280361, acc 0.84375, prec 0.0554985, recall 0.859953
2017-12-10T13:29:55.346445: step 2303, loss 0.160636, acc 0.921875, prec 0.0554901, recall 0.859953
2017-12-10T13:29:55.798903: step 2304, loss 0.28711, acc 0.921875, prec 0.0555673, recall 0.86015
2017-12-10T13:29:56.264059: step 2305, loss 0.722097, acc 0.828125, prec 0.0556059, recall 0.86028
2017-12-10T13:29:56.718105: step 2306, loss 0.388694, acc 0.9375, prec 0.0555992, recall 0.86028
2017-12-10T13:29:57.157559: step 2307, loss 0.23331, acc 0.90625, prec 0.0555891, recall 0.86028
2017-12-10T13:29:57.592605: step 2308, loss 0.340415, acc 0.953125, prec 0.0555841, recall 0.86028
2017-12-10T13:29:58.040912: step 2309, loss 0.223141, acc 0.921875, prec 0.0555757, recall 0.86028
2017-12-10T13:29:58.497832: step 2310, loss 1.10429, acc 0.90625, prec 0.0555941, recall 0.860346
2017-12-10T13:29:58.945267: step 2311, loss 0.305268, acc 0.90625, prec 0.0556126, recall 0.860411
2017-12-10T13:29:59.395906: step 2312, loss 0.262629, acc 0.9375, prec 0.0556058, recall 0.860411
2017-12-10T13:29:59.831222: step 2313, loss 0.274997, acc 0.921875, prec 0.0555975, recall 0.860411
2017-12-10T13:30:00.277286: step 2314, loss 0.253291, acc 0.921875, prec 0.0555891, recall 0.860411
2017-12-10T13:30:00.717371: step 2315, loss 0.113171, acc 0.96875, prec 0.0556142, recall 0.860476
2017-12-10T13:30:01.167010: step 2316, loss 2.32699, acc 0.921875, prec 0.055636, recall 0.86014
2017-12-10T13:30:01.630049: step 2317, loss 0.284363, acc 0.90625, prec 0.0556259, recall 0.86014
2017-12-10T13:30:02.082406: step 2318, loss 0.113792, acc 0.96875, prec 0.055651, recall 0.860205
2017-12-10T13:30:02.530274: step 2319, loss 0.849059, acc 0.953125, prec 0.0557029, recall 0.860335
2017-12-10T13:30:02.976878: step 2320, loss 0.300913, acc 0.9375, prec 0.0557247, recall 0.8604
2017-12-10T13:30:03.428993: step 2321, loss 0.381899, acc 0.921875, prec 0.0557447, recall 0.860465
2017-12-10T13:30:03.863709: step 2322, loss 0.532312, acc 0.875, prec 0.0557882, recall 0.860595
2017-12-10T13:30:04.321986: step 2323, loss 0.41971, acc 0.890625, prec 0.0558333, recall 0.860724
2017-12-10T13:30:04.770324: step 2324, loss 0.255768, acc 0.953125, prec 0.0558282, recall 0.860724
2017-12-10T13:30:05.228486: step 2325, loss 0.328514, acc 0.90625, prec 0.0558466, recall 0.860789
2017-12-10T13:30:05.671988: step 2326, loss 0.324249, acc 0.84375, prec 0.0558298, recall 0.860789
2017-12-10T13:30:06.116261: step 2327, loss 1.67848, acc 0.875, prec 0.0558464, recall 0.860454
2017-12-10T13:30:06.564968: step 2328, loss 0.403917, acc 0.890625, prec 0.0558347, recall 0.860454
2017-12-10T13:30:06.994081: step 2329, loss 0.354174, acc 0.921875, prec 0.0558263, recall 0.860454
2017-12-10T13:30:07.436000: step 2330, loss 0.169159, acc 0.921875, prec 0.0558463, recall 0.860519
2017-12-10T13:30:07.864179: step 2331, loss 0.462225, acc 0.875, prec 0.0558612, recall 0.860584
2017-12-10T13:30:08.304506: step 2332, loss 0.400031, acc 0.90625, prec 0.0558511, recall 0.860584
2017-12-10T13:30:08.748748: step 2333, loss 0.42077, acc 0.875, prec 0.0558945, recall 0.860713
2017-12-10T13:30:09.179863: step 2334, loss 0.212119, acc 0.890625, prec 0.0558827, recall 0.860713
2017-12-10T13:30:09.612415: step 2335, loss 0.360468, acc 0.921875, prec 0.0559877, recall 0.86097
2017-12-10T13:30:10.048070: step 2336, loss 0.534675, acc 0.921875, prec 0.0560077, recall 0.861034
2017-12-10T13:30:10.491615: step 2337, loss 0.387787, acc 0.890625, prec 0.0560243, recall 0.861098
2017-12-10T13:30:10.932025: step 2338, loss 0.187049, acc 0.96875, prec 0.0560492, recall 0.861162
2017-12-10T13:30:11.385750: step 2339, loss 0.0650559, acc 0.984375, prec 0.0561042, recall 0.86129
2017-12-10T13:30:11.834065: step 2340, loss 0.186044, acc 0.90625, prec 0.0561224, recall 0.861354
2017-12-10T13:30:12.288055: step 2341, loss 0.246142, acc 0.96875, prec 0.0561757, recall 0.861482
2017-12-10T13:30:12.731265: step 2342, loss 0.138076, acc 0.9375, prec 0.0561973, recall 0.861546
2017-12-10T13:30:13.173831: step 2343, loss 1.8222, acc 0.9375, prec 0.0561922, recall 0.861149
2017-12-10T13:30:13.614836: step 2344, loss 0.784392, acc 0.890625, prec 0.0562654, recall 0.861341
2017-12-10T13:30:14.061553: step 2345, loss 0.540395, acc 0.921875, prec 0.0563418, recall 0.861531
2017-12-10T13:30:14.504163: step 2346, loss 0.316527, acc 0.890625, prec 0.05633, recall 0.861531
2017-12-10T13:30:14.958192: step 2347, loss 0.290201, acc 0.9375, prec 0.0563515, recall 0.861595
2017-12-10T13:30:15.404696: step 2348, loss 0.0797202, acc 0.96875, prec 0.0564047, recall 0.861722
2017-12-10T13:30:15.846388: step 2349, loss 0.19213, acc 0.921875, prec 0.0563963, recall 0.861722
2017-12-10T13:30:16.300094: step 2350, loss 0.209907, acc 0.921875, prec 0.0563878, recall 0.861722
2017-12-10T13:30:16.739833: step 2351, loss 0.270733, acc 0.921875, prec 0.0563794, recall 0.861722
2017-12-10T13:30:17.166594: step 2352, loss 0.278582, acc 0.921875, prec 0.0563709, recall 0.861722
2017-12-10T13:30:17.627884: step 2353, loss 0.512219, acc 0.84375, prec 0.0563541, recall 0.861722
2017-12-10T13:30:18.068716: step 2354, loss 0.263421, acc 0.921875, prec 0.0564021, recall 0.861848
2017-12-10T13:30:18.516495: step 2355, loss 0.419634, acc 0.90625, prec 0.0564202, recall 0.861911
2017-12-10T13:30:18.940673: step 2356, loss 0.248429, acc 0.953125, prec 0.0564434, recall 0.861974
2017-12-10T13:30:19.391805: step 2357, loss 0.35389, acc 0.890625, prec 0.0564598, recall 0.862037
2017-12-10T13:30:19.843047: step 2358, loss 0.335526, acc 0.890625, prec 0.0564762, recall 0.8621
2017-12-10T13:30:20.293821: step 2359, loss 0.293331, acc 0.921875, prec 0.0565524, recall 0.862289
2017-12-10T13:30:20.739415: step 2360, loss 0.129815, acc 0.96875, prec 0.0566055, recall 0.862415
2017-12-10T13:30:21.176646: step 2361, loss 0.328501, acc 0.90625, prec 0.0566235, recall 0.862477
2017-12-10T13:30:21.618532: step 2362, loss 0.254334, acc 0.90625, prec 0.0566698, recall 0.862602
2017-12-10T13:30:22.061627: step 2363, loss 0.179381, acc 0.953125, prec 0.0566929, recall 0.862665
2017-12-10T13:30:22.506314: step 2364, loss 0.299647, acc 0.9375, prec 0.0567706, recall 0.862852
2017-12-10T13:30:22.957483: step 2365, loss 4.50032, acc 0.9375, prec 0.0568219, recall 0.862585
2017-12-10T13:30:23.414267: step 2366, loss 0.213926, acc 0.921875, prec 0.0568416, recall 0.862647
2017-12-10T13:30:23.865745: step 2367, loss 0.165492, acc 0.96875, prec 0.0568945, recall 0.862772
2017-12-10T13:30:24.309611: step 2368, loss 0.572667, acc 0.921875, prec 0.056886, recall 0.862772
2017-12-10T13:30:24.753989: step 2369, loss 5.4581, acc 0.84375, prec 0.0568708, recall 0.862381
2017-12-10T13:30:25.197950: step 2370, loss 0.289314, acc 0.921875, prec 0.0568623, recall 0.862381
2017-12-10T13:30:25.631714: step 2371, loss 0.264561, acc 0.875, prec 0.0568487, recall 0.862381
2017-12-10T13:30:26.062545: step 2372, loss 0.276052, acc 0.890625, prec 0.0568368, recall 0.862381
2017-12-10T13:30:26.500503: step 2373, loss 0.541387, acc 0.78125, prec 0.0568412, recall 0.862443
2017-12-10T13:30:26.934436: step 2374, loss 0.692722, acc 0.71875, prec 0.0568107, recall 0.862443
2017-12-10T13:30:27.371871: step 2375, loss 0.567081, acc 0.78125, prec 0.0568151, recall 0.862506
2017-12-10T13:30:27.819314: step 2376, loss 0.985733, acc 0.765625, prec 0.0568178, recall 0.862568
2017-12-10T13:30:28.274167: step 2377, loss 0.663267, acc 0.75, prec 0.0567908, recall 0.862568
2017-12-10T13:30:28.724297: step 2378, loss 0.854271, acc 0.765625, prec 0.0567935, recall 0.86263
2017-12-10T13:30:29.169479: step 2379, loss 0.76591, acc 0.78125, prec 0.0567699, recall 0.86263
2017-12-10T13:30:29.621405: step 2380, loss 0.801274, acc 0.75, prec 0.0567709, recall 0.862692
2017-12-10T13:30:30.055117: step 2381, loss 0.548767, acc 0.828125, prec 0.0567804, recall 0.862754
2017-12-10T13:30:30.502271: step 2382, loss 0.622214, acc 0.8125, prec 0.0567601, recall 0.862754
2017-12-10T13:30:30.944180: step 2383, loss 0.744408, acc 0.765625, prec 0.0567349, recall 0.862754
2017-12-10T13:30:31.400125: step 2384, loss 0.493169, acc 0.828125, prec 0.0567163, recall 0.862754
2017-12-10T13:30:31.839106: step 2385, loss 0.362739, acc 0.890625, prec 0.0567605, recall 0.862878
2017-12-10T13:30:32.293915: step 2386, loss 1.01624, acc 0.921875, prec 0.0568081, recall 0.863001
2017-12-10T13:30:32.754235: step 2387, loss 0.111974, acc 0.96875, prec 0.0568047, recall 0.863001
2017-12-10T13:30:33.198365: step 2388, loss 0.381169, acc 0.859375, prec 0.0567895, recall 0.863001
2017-12-10T13:30:33.646502: step 2389, loss 0.82318, acc 0.875, prec 0.056804, recall 0.863063
2017-12-10T13:30:34.088639: step 2390, loss 0.345302, acc 0.9375, prec 0.0568253, recall 0.863125
2017-12-10T13:30:34.534338: step 2391, loss 0.411743, acc 0.875, prec 0.0568397, recall 0.863186
2017-12-10T13:30:34.982976: step 2392, loss 4.5162, acc 0.859375, prec 0.0568263, recall 0.862798
2017-12-10T13:30:35.421438: step 2393, loss 0.52233, acc 0.8125, prec 0.0568061, recall 0.862798
2017-12-10T13:30:35.865732: step 2394, loss 0.337119, acc 0.890625, prec 0.0568222, recall 0.86286
2017-12-10T13:30:36.315487: step 2395, loss 0.208871, acc 0.953125, prec 0.0568172, recall 0.86286
2017-12-10T13:30:36.757916: step 2396, loss 0.603843, acc 0.875, prec 0.0568596, recall 0.862983
2017-12-10T13:30:37.201461: step 2397, loss 2.95394, acc 0.84375, prec 0.0569002, recall 0.862719
2017-12-10T13:30:37.650846: step 2398, loss 0.467704, acc 0.90625, prec 0.0568901, recall 0.862719
2017-12-10T13:30:38.085458: step 2399, loss 0.457383, acc 0.875, prec 0.0569324, recall 0.862842
2017-12-10T13:30:38.525917: step 2400, loss 0.318071, acc 0.890625, prec 0.0569207, recall 0.862842
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2400

2017-12-10T13:30:40.758170: step 2401, loss 0.573456, acc 0.8125, prec 0.057012, recall 0.863087
2017-12-10T13:30:41.196360: step 2402, loss 0.79142, acc 0.765625, prec 0.0569867, recall 0.863087
2017-12-10T13:30:41.638474: step 2403, loss 0.323415, acc 0.90625, prec 0.0570045, recall 0.863148
2017-12-10T13:30:42.067227: step 2404, loss 0.341144, acc 0.90625, prec 0.0570779, recall 0.863332
2017-12-10T13:30:42.507083: step 2405, loss 0.777725, acc 0.796875, prec 0.057056, recall 0.863332
2017-12-10T13:30:42.960451: step 2406, loss 1.19899, acc 0.734375, prec 0.0570552, recall 0.863393
2017-12-10T13:30:43.399380: step 2407, loss 0.77625, acc 0.78125, prec 0.0570594, recall 0.863454
2017-12-10T13:30:43.835558: step 2408, loss 0.985696, acc 0.734375, prec 0.0570587, recall 0.863515
2017-12-10T13:30:44.275198: step 2409, loss 0.772558, acc 0.796875, prec 0.0570646, recall 0.863576
2017-12-10T13:30:44.717202: step 2410, loss 0.158424, acc 0.953125, prec 0.0570873, recall 0.863636
2017-12-10T13:30:45.143417: step 2411, loss 1.14789, acc 0.8125, prec 0.0571227, recall 0.863758
2017-12-10T13:30:45.534006: step 2412, loss 0.385628, acc 0.84375, prec 0.0571336, recall 0.863818
2017-12-10T13:30:45.900060: step 2413, loss 0.51136, acc 0.796875, prec 0.0571395, recall 0.863879
2017-12-10T13:30:46.294384: step 2414, loss 1.1228, acc 0.796875, prec 0.0572008, recall 0.86406
2017-12-10T13:30:46.682194: step 2415, loss 0.280991, acc 0.890625, prec 0.0571891, recall 0.86406
2017-12-10T13:30:47.092668: step 2416, loss 0.184535, acc 0.9375, prec 0.0571823, recall 0.86406
2017-12-10T13:30:47.542379: step 2417, loss 0.506149, acc 0.828125, prec 0.0571638, recall 0.86406
2017-12-10T13:30:47.977186: step 2418, loss 0.341379, acc 0.828125, prec 0.0571454, recall 0.86406
2017-12-10T13:30:48.419739: step 2419, loss 0.81665, acc 0.828125, prec 0.0571546, recall 0.864121
2017-12-10T13:30:48.880231: step 2420, loss 1.73718, acc 0.765625, prec 0.0571311, recall 0.863737
2017-12-10T13:30:49.328870: step 2421, loss 0.583794, acc 0.84375, prec 0.057142, recall 0.863798
2017-12-10T13:30:49.773466: step 2422, loss 0.513429, acc 0.921875, prec 0.0571613, recall 0.863858
2017-12-10T13:30:50.217258: step 2423, loss 0.334181, acc 0.9375, prec 0.0571823, recall 0.863918
2017-12-10T13:30:50.660781: step 2424, loss 0.369477, acc 0.890625, prec 0.0571705, recall 0.863918
2017-12-10T13:30:51.094657: step 2425, loss 0.0926484, acc 0.953125, prec 0.0571655, recall 0.863918
2017-12-10T13:30:51.538046: step 2426, loss 0.169956, acc 0.9375, prec 0.0571864, recall 0.863979
2017-12-10T13:30:51.975384: step 2427, loss 0.438496, acc 0.890625, prec 0.0571747, recall 0.863979
2017-12-10T13:30:52.426284: step 2428, loss 1.11772, acc 0.90625, prec 0.0571923, recall 0.864039
2017-12-10T13:30:52.859105: step 2429, loss 0.107604, acc 0.96875, prec 0.0572166, recall 0.864099
2017-12-10T13:30:53.304052: step 2430, loss 0.42801, acc 0.90625, prec 0.0572618, recall 0.864219
2017-12-10T13:30:53.758133: step 2431, loss 0.198979, acc 0.9375, prec 0.0572827, recall 0.864279
2017-12-10T13:30:54.208653: step 2432, loss 0.363836, acc 0.890625, prec 0.0572985, recall 0.864339
2017-12-10T13:30:54.663748: step 2433, loss 0.4238, acc 0.875, prec 0.0572851, recall 0.864339
2017-12-10T13:30:55.094187: step 2434, loss 0.149204, acc 0.9375, prec 0.057306, recall 0.864399
2017-12-10T13:30:55.557735: step 2435, loss 4.9655, acc 0.9375, prec 0.0573286, recall 0.864078
2017-12-10T13:30:56.005445: step 2436, loss 0.548544, acc 0.921875, prec 0.0573754, recall 0.864198
2017-12-10T13:30:56.454290: step 2437, loss 0.143547, acc 0.921875, prec 0.0573946, recall 0.864257
2017-12-10T13:30:56.895749: step 2438, loss 0.503382, acc 0.90625, prec 0.0574121, recall 0.864317
2017-12-10T13:30:57.327469: step 2439, loss 0.432567, acc 0.921875, prec 0.057514, recall 0.864556
2017-12-10T13:30:57.756739: step 2440, loss 0.566345, acc 0.875, prec 0.0575281, recall 0.864615
2017-12-10T13:30:58.195575: step 2441, loss 0.388321, acc 0.875, prec 0.0575422, recall 0.864675
2017-12-10T13:30:58.639577: step 2442, loss 0.705306, acc 0.828125, prec 0.0575788, recall 0.864794
2017-12-10T13:30:59.089767: step 2443, loss 0.275994, acc 0.859375, prec 0.0575636, recall 0.864794
2017-12-10T13:30:59.559274: step 2444, loss 0.288875, acc 0.890625, prec 0.0575794, recall 0.864853
2017-12-10T13:30:59.981960: step 2445, loss 0.528385, acc 0.875, prec 0.0575659, recall 0.864853
2017-12-10T13:31:00.420676: step 2446, loss 0.311576, acc 0.859375, prec 0.0575783, recall 0.864912
2017-12-10T13:31:00.858171: step 2447, loss 0.543454, acc 0.828125, prec 0.0575598, recall 0.864912
2017-12-10T13:31:01.303625: step 2448, loss 0.427894, acc 0.859375, prec 0.0575447, recall 0.864912
2017-12-10T13:31:01.747518: step 2449, loss 0.256725, acc 0.890625, prec 0.057588, recall 0.865031
2017-12-10T13:31:02.203000: step 2450, loss 0.670989, acc 0.8125, prec 0.0576502, recall 0.865208
2017-12-10T13:31:02.646511: step 2451, loss 0.390589, acc 0.890625, prec 0.0576385, recall 0.865208
2017-12-10T13:31:03.094724: step 2452, loss 0.168497, acc 0.921875, prec 0.0576576, recall 0.865267
2017-12-10T13:31:03.547718: step 2453, loss 0.498208, acc 0.84375, prec 0.0576407, recall 0.865267
2017-12-10T13:31:04.000665: step 2454, loss 0.640823, acc 0.9375, prec 0.0576889, recall 0.865385
2017-12-10T13:31:04.460131: step 2455, loss 0.409078, acc 0.84375, prec 0.0576996, recall 0.865443
2017-12-10T13:31:04.916046: step 2456, loss 0.253549, acc 0.90625, prec 0.0577718, recall 0.86562
2017-12-10T13:31:05.374809: step 2457, loss 0.254837, acc 0.875, prec 0.0577584, recall 0.86562
2017-12-10T13:31:05.811085: step 2458, loss 0.366406, acc 0.90625, prec 0.0577757, recall 0.865678
2017-12-10T13:31:06.260368: step 2459, loss 1.59325, acc 0.9375, prec 0.0578255, recall 0.865418
2017-12-10T13:31:06.728968: step 2460, loss 0.0892266, acc 0.984375, prec 0.0578512, recall 0.865477
2017-12-10T13:31:07.183580: step 2461, loss 0.612956, acc 0.875, prec 0.0578378, recall 0.865477
2017-12-10T13:31:07.639643: step 2462, loss 0.1432, acc 0.921875, prec 0.0578294, recall 0.865477
2017-12-10T13:31:08.087763: step 2463, loss 4.19038, acc 0.953125, prec 0.0578534, recall 0.865159
2017-12-10T13:31:08.533337: step 2464, loss 0.252909, acc 0.9375, prec 0.0578741, recall 0.865217
2017-12-10T13:31:08.988205: step 2465, loss 0.318995, acc 0.90625, prec 0.0578914, recall 0.865276
2017-12-10T13:31:09.442361: step 2466, loss 0.453728, acc 0.921875, prec 0.0579103, recall 0.865335
2017-12-10T13:31:09.885093: step 2467, loss 0.393705, acc 0.890625, prec 0.0579259, recall 0.865393
2017-12-10T13:31:10.323833: step 2468, loss 0.236635, acc 0.90625, prec 0.0579158, recall 0.865393
2017-12-10T13:31:10.772993: step 2469, loss 0.220318, acc 0.9375, prec 0.0579091, recall 0.865393
2017-12-10T13:31:11.227130: step 2470, loss 0.200979, acc 0.921875, prec 0.0579281, recall 0.865451
2017-12-10T13:31:11.670041: step 2471, loss 0.162575, acc 0.9375, prec 0.0579487, recall 0.86551
2017-12-10T13:31:12.107911: step 2472, loss 0.734856, acc 0.8125, prec 0.0579832, recall 0.865626
2017-12-10T13:31:12.559376: step 2473, loss 0.819312, acc 0.859375, prec 0.0579681, recall 0.865626
2017-12-10T13:31:13.017453: step 2474, loss 0.374893, acc 0.890625, prec 0.0579836, recall 0.865685
2017-12-10T13:31:13.476973: step 2475, loss 0.447455, acc 0.859375, prec 0.0579958, recall 0.865743
2017-12-10T13:31:13.939434: step 2476, loss 0.629315, acc 0.84375, prec 0.057979, recall 0.865743
2017-12-10T13:31:14.390635: step 2477, loss 0.638454, acc 0.84375, prec 0.0579622, recall 0.865743
2017-12-10T13:31:14.844205: step 2478, loss 0.0868338, acc 0.96875, prec 0.0579588, recall 0.865743
2017-12-10T13:31:15.295866: step 2479, loss 0.0637295, acc 0.953125, prec 0.0579538, recall 0.865743
2017-12-10T13:31:15.745669: step 2480, loss 0.178683, acc 0.890625, prec 0.0579693, recall 0.865801
2017-12-10T13:31:16.196305: step 2481, loss 0.470534, acc 0.90625, prec 0.0580411, recall 0.865975
2017-12-10T13:31:16.649231: step 2482, loss 0.0888336, acc 0.96875, prec 0.0580651, recall 0.866033
2017-12-10T13:31:17.109768: step 2483, loss 0.255188, acc 0.890625, prec 0.0580806, recall 0.866091
2017-12-10T13:31:17.553399: step 2484, loss 0.135028, acc 0.9375, prec 0.0580739, recall 0.866091
2017-12-10T13:31:17.956674: step 2485, loss 0.212799, acc 0.903846, prec 0.0580655, recall 0.866091
2017-12-10T13:31:18.409131: step 2486, loss 0.514673, acc 0.96875, prec 0.0580894, recall 0.866149
2017-12-10T13:31:18.865004: step 2487, loss 0.223977, acc 0.953125, prec 0.0581389, recall 0.866264
2017-12-10T13:31:19.313153: step 2488, loss 0.774368, acc 0.953125, prec 0.0581883, recall 0.866379
2017-12-10T13:31:19.761046: step 2489, loss 0.319905, acc 0.9375, prec 0.0582089, recall 0.866437
2017-12-10T13:31:20.223173: step 2490, loss 2.55736, acc 0.9375, prec 0.0582038, recall 0.866064
2017-12-10T13:31:20.672814: step 2491, loss 0.309867, acc 0.96875, prec 0.058255, recall 0.866179
2017-12-10T13:31:21.121252: step 2492, loss 0.0447088, acc 0.984375, prec 0.0582533, recall 0.866179
2017-12-10T13:31:21.573444: step 2493, loss 0.39953, acc 0.921875, prec 0.0582993, recall 0.866294
2017-12-10T13:31:22.031307: step 2494, loss 0.270787, acc 0.890625, prec 0.0582875, recall 0.866294
2017-12-10T13:31:22.478180: step 2495, loss 0.319182, acc 0.953125, prec 0.0583097, recall 0.866352
2017-12-10T13:31:22.914151: step 2496, loss 0.627528, acc 0.84375, prec 0.0583201, recall 0.866409
2017-12-10T13:31:23.365622: step 2497, loss 0.391382, acc 0.84375, prec 0.0583304, recall 0.866466
2017-12-10T13:31:23.818339: step 2498, loss 0.243203, acc 0.890625, prec 0.0583186, recall 0.866466
2017-12-10T13:31:24.260145: step 2499, loss 0.308998, acc 0.90625, prec 0.0583357, recall 0.866524
2017-12-10T13:31:24.695623: step 2500, loss 0.411751, acc 0.859375, prec 0.0583206, recall 0.866524
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2500

2017-12-10T13:31:26.664611: step 2501, loss 0.395425, acc 0.875, prec 0.0583343, recall 0.866581
2017-12-10T13:31:27.099911: step 2502, loss 0.606232, acc 0.890625, prec 0.0583497, recall 0.866638
2017-12-10T13:31:27.541253: step 2503, loss 0.271363, acc 0.921875, prec 0.0583413, recall 0.866638
2017-12-10T13:31:27.976901: step 2504, loss 0.414709, acc 0.90625, prec 0.0583312, recall 0.866638
2017-12-10T13:31:28.420562: step 2505, loss 0.237343, acc 0.921875, prec 0.0583228, recall 0.866638
2017-12-10T13:31:28.867964: step 2506, loss 0.0400299, acc 1, prec 0.0583228, recall 0.866638
2017-12-10T13:31:29.310827: step 2507, loss 0.365374, acc 0.890625, prec 0.0583381, recall 0.866695
2017-12-10T13:31:29.746015: step 2508, loss 0.254367, acc 0.921875, prec 0.0583569, recall 0.866752
2017-12-10T13:31:30.181455: step 2509, loss 0.258294, acc 0.90625, prec 0.058374, recall 0.866809
2017-12-10T13:31:30.623350: step 2510, loss 0.179086, acc 0.90625, prec 0.058391, recall 0.866866
2017-12-10T13:31:31.071341: step 2511, loss 0.261854, acc 0.96875, prec 0.0584419, recall 0.86698
2017-12-10T13:31:31.538565: step 2512, loss 0.232303, acc 0.9375, prec 0.0584623, recall 0.867037
2017-12-10T13:31:31.974870: step 2513, loss 0.316995, acc 0.9375, prec 0.0584556, recall 0.867037
2017-12-10T13:31:32.417152: step 2514, loss 0.165584, acc 0.96875, prec 0.0584794, recall 0.867094
2017-12-10T13:31:32.862117: step 2515, loss 0.448046, acc 0.9375, prec 0.0585269, recall 0.867208
2017-12-10T13:31:33.307090: step 2516, loss 0.270998, acc 0.9375, prec 0.0585201, recall 0.867208
2017-12-10T13:31:33.745177: step 2517, loss 0.421541, acc 0.890625, prec 0.0585355, recall 0.867264
2017-12-10T13:31:34.187552: step 2518, loss 0.172944, acc 0.953125, prec 0.0585575, recall 0.867321
2017-12-10T13:31:34.646777: step 2519, loss 0.699017, acc 0.96875, prec 0.0585813, recall 0.867377
2017-12-10T13:31:35.105222: step 2520, loss 0.10213, acc 0.953125, prec 0.0586033, recall 0.867434
2017-12-10T13:31:35.558010: step 2521, loss 0.0941626, acc 0.953125, prec 0.0585983, recall 0.867434
2017-12-10T13:31:35.999761: step 2522, loss 0.0659428, acc 0.984375, prec 0.0586508, recall 0.867547
2017-12-10T13:31:36.455745: step 2523, loss 0.315308, acc 0.921875, prec 0.0586694, recall 0.867603
2017-12-10T13:31:36.897165: step 2524, loss 0.115444, acc 0.921875, prec 0.0587152, recall 0.867716
2017-12-10T13:31:37.345354: step 2525, loss 0.15787, acc 0.9375, prec 0.0587084, recall 0.867716
2017-12-10T13:31:37.790496: step 2526, loss 0.238513, acc 0.9375, prec 0.0587017, recall 0.867716
2017-12-10T13:31:38.228416: step 2527, loss 0.0909395, acc 0.96875, prec 0.0586983, recall 0.867716
2017-12-10T13:31:38.679068: step 2528, loss 0.0224636, acc 1, prec 0.0586983, recall 0.867716
2017-12-10T13:31:39.124633: step 2529, loss 0.159998, acc 0.96875, prec 0.058722, recall 0.867772
2017-12-10T13:31:39.577533: step 2530, loss 0.11909, acc 0.9375, prec 0.0587152, recall 0.867772
2017-12-10T13:31:40.032768: step 2531, loss 0.295989, acc 0.9375, prec 0.0587355, recall 0.867828
2017-12-10T13:31:40.478936: step 2532, loss 0.118572, acc 0.984375, prec 0.0587609, recall 0.867884
2017-12-10T13:31:40.946659: step 2533, loss 0.246515, acc 0.9375, prec 0.0587542, recall 0.867884
2017-12-10T13:31:41.389271: step 2534, loss 0.213536, acc 0.953125, prec 0.0587762, recall 0.867941
2017-12-10T13:31:41.829685: step 2535, loss 1.82766, acc 0.96875, prec 0.0587745, recall 0.867572
2017-12-10T13:31:42.286461: step 2536, loss 0.081262, acc 0.96875, prec 0.0587711, recall 0.867572
2017-12-10T13:31:42.735347: step 2537, loss 6.33906, acc 0.921875, prec 0.0588185, recall 0.867317
2017-12-10T13:31:43.179761: step 2538, loss 0.183866, acc 0.96875, prec 0.0588962, recall 0.867485
2017-12-10T13:31:43.626711: step 2539, loss 0.309695, acc 0.890625, prec 0.0589114, recall 0.867541
2017-12-10T13:31:44.077029: step 2540, loss 0.179912, acc 0.9375, prec 0.0589858, recall 0.867709
2017-12-10T13:31:44.524148: step 2541, loss 0.159633, acc 0.921875, prec 0.0590043, recall 0.867765
2017-12-10T13:31:44.987722: step 2542, loss 0.328299, acc 0.890625, prec 0.0590465, recall 0.867877
2017-12-10T13:31:45.440503: step 2543, loss 0.120156, acc 0.9375, prec 0.0590938, recall 0.867988
2017-12-10T13:31:45.906849: step 2544, loss 0.629802, acc 0.84375, prec 0.0591038, recall 0.868044
2017-12-10T13:31:46.349106: step 2545, loss 0.394458, acc 0.828125, prec 0.0591122, recall 0.868099
2017-12-10T13:31:46.791315: step 2546, loss 0.728301, acc 0.8125, prec 0.0591458, recall 0.868211
2017-12-10T13:31:47.234292: step 2547, loss 0.372902, acc 0.84375, prec 0.0591288, recall 0.868211
2017-12-10T13:31:47.683152: step 2548, loss 0.218086, acc 0.921875, prec 0.0591473, recall 0.868266
2017-12-10T13:31:48.133479: step 2549, loss 0.194518, acc 0.90625, prec 0.0591641, recall 0.868321
2017-12-10T13:31:48.611832: step 2550, loss 0.229968, acc 0.875, prec 0.0591775, recall 0.868377
2017-12-10T13:31:49.044640: step 2551, loss 0.821834, acc 0.84375, prec 0.0592414, recall 0.868543
2017-12-10T13:31:49.494118: step 2552, loss 0.436486, acc 0.859375, prec 0.0592531, recall 0.868598
2017-12-10T13:31:49.932075: step 2553, loss 0.0706654, acc 0.953125, prec 0.059248, recall 0.868598
2017-12-10T13:31:50.368606: step 2554, loss 0.380539, acc 0.875, prec 0.0592614, recall 0.868653
2017-12-10T13:31:50.814668: step 2555, loss 0.23116, acc 0.875, prec 0.0592747, recall 0.868708
2017-12-10T13:31:51.254520: step 2556, loss 1.42417, acc 0.875, prec 0.0592898, recall 0.868399
2017-12-10T13:31:51.709102: step 2557, loss 0.189936, acc 0.9375, prec 0.0593637, recall 0.868564
2017-12-10T13:31:52.150449: step 2558, loss 0.389191, acc 0.921875, prec 0.0593822, recall 0.868619
2017-12-10T13:31:52.601905: step 2559, loss 0.489303, acc 0.859375, prec 0.0593938, recall 0.868674
2017-12-10T13:31:53.043931: step 2560, loss 0.268531, acc 0.890625, prec 0.0593819, recall 0.868674
2017-12-10T13:31:53.487125: step 2561, loss 0.236123, acc 0.90625, prec 0.0594255, recall 0.868784
2017-12-10T13:31:53.923350: step 2562, loss 0.374384, acc 0.875, prec 0.0594388, recall 0.868839
2017-12-10T13:31:54.370685: step 2563, loss 0.435212, acc 0.875, prec 0.0595058, recall 0.869003
2017-12-10T13:31:54.806012: step 2564, loss 0.278378, acc 0.90625, prec 0.0595224, recall 0.869058
2017-12-10T13:31:55.250438: step 2565, loss 0.156932, acc 0.9375, prec 0.0595157, recall 0.869058
2017-12-10T13:31:55.707888: step 2566, loss 0.268512, acc 0.921875, prec 0.0595072, recall 0.869058
2017-12-10T13:31:56.153455: step 2567, loss 0.101236, acc 0.953125, prec 0.0595021, recall 0.869058
2017-12-10T13:31:56.594682: step 2568, loss 0.752538, acc 0.859375, prec 0.0595136, recall 0.869112
2017-12-10T13:31:57.051176: step 2569, loss 0.364769, acc 0.921875, prec 0.0595588, recall 0.869221
2017-12-10T13:31:57.518345: step 2570, loss 0.259051, acc 0.90625, prec 0.0595754, recall 0.869276
2017-12-10T13:31:57.962127: step 2571, loss 0.300789, acc 0.9375, prec 0.0596223, recall 0.869384
2017-12-10T13:31:58.416526: step 2572, loss 0.270824, acc 0.921875, prec 0.0596138, recall 0.869384
2017-12-10T13:31:58.866809: step 2573, loss 0.314294, acc 0.96875, prec 0.059664, recall 0.869493
2017-12-10T13:31:59.309287: step 2574, loss 0.177221, acc 0.96875, prec 0.0596874, recall 0.869547
2017-12-10T13:31:59.756947: step 2575, loss 0.246406, acc 0.953125, prec 0.059736, recall 0.869655
2017-12-10T13:32:00.204957: step 2576, loss 0.0157749, acc 1, prec 0.059736, recall 0.869655
2017-12-10T13:32:00.643272: step 2577, loss 0.0651215, acc 0.96875, prec 0.0597326, recall 0.869655
2017-12-10T13:32:01.090567: step 2578, loss 0.111304, acc 0.953125, prec 0.0597274, recall 0.869655
2017-12-10T13:32:01.543866: step 2579, loss 0.343308, acc 0.921875, prec 0.0597725, recall 0.869764
2017-12-10T13:32:01.988640: step 2580, loss 0.155892, acc 1, prec 0.0597993, recall 0.869818
2017-12-10T13:32:02.435518: step 2581, loss 0.0970984, acc 0.96875, prec 0.0597959, recall 0.869818
2017-12-10T13:32:02.889784: step 2582, loss 0.267013, acc 0.921875, prec 0.0597874, recall 0.869818
2017-12-10T13:32:03.324080: step 2583, loss 0.0604316, acc 0.984375, prec 0.0598661, recall 0.869979
2017-12-10T13:32:03.762219: step 2584, loss 0.357081, acc 0.921875, prec 0.0598576, recall 0.869979
2017-12-10T13:32:04.212114: step 2585, loss 0.147527, acc 0.96875, prec 0.0598809, recall 0.870033
2017-12-10T13:32:04.678056: step 2586, loss 0.313265, acc 0.9375, prec 0.0599009, recall 0.870087
2017-12-10T13:32:05.121752: step 2587, loss 0.126777, acc 0.984375, prec 0.0599259, recall 0.870141
2017-12-10T13:32:05.572941: step 2588, loss 0.0830703, acc 0.984375, prec 0.0599778, recall 0.870248
2017-12-10T13:32:06.009505: step 2589, loss 0.0317076, acc 0.984375, prec 0.0600028, recall 0.870302
2017-12-10T13:32:06.458433: step 2590, loss 0.0804261, acc 0.96875, prec 0.0599994, recall 0.870302
2017-12-10T13:32:06.900225: step 2591, loss 0.0934979, acc 0.96875, prec 0.059996, recall 0.870302
2017-12-10T13:32:07.344267: step 2592, loss 0.0268489, acc 1, prec 0.0600228, recall 0.870355
2017-12-10T13:32:07.787669: step 2593, loss 0.389178, acc 0.953125, prec 0.0600444, recall 0.870409
2017-12-10T13:32:08.235284: step 2594, loss 0.267749, acc 0.953125, prec 0.060066, recall 0.870462
2017-12-10T13:32:08.680865: step 2595, loss 0.105262, acc 0.96875, prec 0.0600626, recall 0.870462
2017-12-10T13:32:09.126423: step 2596, loss 0.151481, acc 0.921875, prec 0.0600541, recall 0.870462
2017-12-10T13:32:09.597335: step 2597, loss 0.0690817, acc 0.984375, prec 0.0600524, recall 0.870462
2017-12-10T13:32:10.033600: step 2598, loss 0.276376, acc 0.953125, prec 0.0600472, recall 0.870462
2017-12-10T13:32:10.482666: step 2599, loss 0.100951, acc 0.9375, prec 0.0600404, recall 0.870462
2017-12-10T13:32:10.917393: step 2600, loss 0.00770843, acc 1, prec 0.0600404, recall 0.870462
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2600

2017-12-10T13:32:13.253194: step 2601, loss 0.358307, acc 0.9375, prec 0.0601138, recall 0.870622
2017-12-10T13:32:13.707177: step 2602, loss 0.0218049, acc 1, prec 0.0601138, recall 0.870622
2017-12-10T13:32:14.163938: step 2603, loss 0.112115, acc 1, prec 0.0601405, recall 0.870675
2017-12-10T13:32:14.613071: step 2604, loss 0.0895656, acc 0.984375, prec 0.0601656, recall 0.870729
2017-12-10T13:32:15.053927: step 2605, loss 0.0515176, acc 0.984375, prec 0.0601639, recall 0.870729
2017-12-10T13:32:15.485949: step 2606, loss 4.14325, acc 0.9375, prec 0.0601854, recall 0.870424
2017-12-10T13:32:15.948647: step 2607, loss 0.0995172, acc 0.9375, prec 0.0601786, recall 0.870424
2017-12-10T13:32:16.385638: step 2608, loss 0.174979, acc 0.96875, prec 0.0601752, recall 0.870424
2017-12-10T13:32:16.829664: step 2609, loss 0.542846, acc 0.9375, prec 0.0601951, recall 0.870477
2017-12-10T13:32:17.275774: step 2610, loss 0.0815407, acc 0.953125, prec 0.0601899, recall 0.870477
2017-12-10T13:32:17.725932: step 2611, loss 0.326153, acc 0.921875, prec 0.0602081, recall 0.87053
2017-12-10T13:32:18.168144: step 2612, loss 0.69494, acc 0.921875, prec 0.0602262, recall 0.870583
2017-12-10T13:32:18.639725: step 2613, loss 0.251835, acc 0.90625, prec 0.060216, recall 0.870583
2017-12-10T13:32:19.088457: step 2614, loss 0.398753, acc 0.890625, prec 0.0602574, recall 0.87069
2017-12-10T13:32:19.545671: step 2615, loss 0.575388, acc 0.875, prec 0.0602704, recall 0.870743
2017-12-10T13:32:19.970456: step 2616, loss 0.457323, acc 0.890625, prec 0.0602851, recall 0.870796
2017-12-10T13:32:20.412819: step 2617, loss 0.49175, acc 0.90625, prec 0.0603015, recall 0.870849
2017-12-10T13:32:20.854425: step 2618, loss 0.309771, acc 0.921875, prec 0.0603196, recall 0.870902
2017-12-10T13:32:21.300034: step 2619, loss 0.371291, acc 0.90625, prec 0.0603094, recall 0.870902
2017-12-10T13:32:21.745486: step 2620, loss 0.403888, acc 0.890625, prec 0.060324, recall 0.870955
2017-12-10T13:32:22.185515: step 2621, loss 0.508859, acc 0.90625, prec 0.0603138, recall 0.870955
2017-12-10T13:32:22.627773: step 2622, loss 0.239749, acc 0.90625, prec 0.0603568, recall 0.87106
2017-12-10T13:32:23.071174: step 2623, loss 0.175041, acc 0.953125, prec 0.060405, recall 0.871166
2017-12-10T13:32:23.517224: step 2624, loss 0.317097, acc 0.890625, prec 0.060393, recall 0.871166
2017-12-10T13:32:23.958709: step 2625, loss 0.35471, acc 0.921875, prec 0.0603844, recall 0.871166
2017-12-10T13:32:24.397301: step 2626, loss 0.271145, acc 0.90625, prec 0.0604274, recall 0.871271
2017-12-10T13:32:24.866281: step 2627, loss 0.26168, acc 0.90625, prec 0.0604438, recall 0.871324
2017-12-10T13:32:25.329512: step 2628, loss 0.273236, acc 0.890625, prec 0.0604318, recall 0.871324
2017-12-10T13:32:25.777124: step 2629, loss 0.144526, acc 0.9375, prec 0.0604515, recall 0.871376
2017-12-10T13:32:26.221804: step 2630, loss 0.287995, acc 0.90625, prec 0.0604679, recall 0.871429
2017-12-10T13:32:26.668128: step 2631, loss 0.134809, acc 0.9375, prec 0.0605142, recall 0.871533
2017-12-10T13:32:27.110753: step 2632, loss 0.178556, acc 0.9375, prec 0.0605074, recall 0.871533
2017-12-10T13:32:27.558909: step 2633, loss 0.258106, acc 0.90625, prec 0.0604971, recall 0.871533
2017-12-10T13:32:28.001640: step 2634, loss 0.131088, acc 0.953125, prec 0.0605186, recall 0.871586
2017-12-10T13:32:28.451320: step 2635, loss 0.205266, acc 0.9375, prec 0.0605383, recall 0.871638
2017-12-10T13:32:28.906399: step 2636, loss 0.088565, acc 0.953125, prec 0.0605332, recall 0.871638
2017-12-10T13:32:29.372526: step 2637, loss 0.192839, acc 0.890625, prec 0.0605478, recall 0.87169
2017-12-10T13:32:29.822676: step 2638, loss 0.0573534, acc 0.984375, prec 0.060546, recall 0.87169
2017-12-10T13:32:30.252866: step 2639, loss 0.988749, acc 1, prec 0.0605992, recall 0.871795
2017-12-10T13:32:30.690817: step 2640, loss 0.0578199, acc 1, prec 0.0606524, recall 0.871899
2017-12-10T13:32:31.140926: step 2641, loss 0.240545, acc 0.953125, prec 0.0606738, recall 0.871951
2017-12-10T13:32:31.584274: step 2642, loss 0.152789, acc 0.953125, prec 0.0606686, recall 0.871951
2017-12-10T13:32:32.032049: step 2643, loss 0.0845538, acc 0.984375, prec 0.0606669, recall 0.871951
2017-12-10T13:32:32.499945: step 2644, loss 0.0631296, acc 0.984375, prec 0.0606652, recall 0.871951
2017-12-10T13:32:32.934724: step 2645, loss 1.1181, acc 0.9375, prec 0.0607115, recall 0.872055
2017-12-10T13:32:33.385283: step 2646, loss 1.93703, acc 0.921875, prec 0.0607046, recall 0.871701
2017-12-10T13:32:33.830155: step 2647, loss 0.291603, acc 0.96875, prec 0.0607543, recall 0.871805
2017-12-10T13:32:34.280443: step 2648, loss 0.0870607, acc 0.953125, prec 0.0607491, recall 0.871805
2017-12-10T13:32:34.713383: step 2649, loss 0.178804, acc 0.953125, prec 0.0607971, recall 0.871909
2017-12-10T13:32:35.191644: step 2650, loss 0.892659, acc 0.953125, prec 0.0608184, recall 0.871961
2017-12-10T13:32:35.640776: step 2651, loss 0.264304, acc 0.921875, prec 0.0608099, recall 0.871961
2017-12-10T13:32:36.075854: step 2652, loss 0.153718, acc 0.953125, prec 0.0608047, recall 0.871961
2017-12-10T13:32:36.526212: step 2653, loss 0.201732, acc 0.90625, prec 0.0607944, recall 0.871961
2017-12-10T13:32:36.979447: step 2654, loss 0.193499, acc 0.90625, prec 0.0607841, recall 0.871961
2017-12-10T13:32:37.429301: step 2655, loss 0.775241, acc 0.828125, prec 0.0608183, recall 0.872065
2017-12-10T13:32:37.890131: step 2656, loss 0.425505, acc 0.890625, prec 0.0608062, recall 0.872065
2017-12-10T13:32:38.325618: step 2657, loss 0.33917, acc 0.890625, prec 0.0608207, recall 0.872117
2017-12-10T13:32:38.775684: step 2658, loss 0.534373, acc 0.84375, prec 0.0608831, recall 0.872272
2017-12-10T13:32:39.211959: step 2659, loss 0.347238, acc 0.890625, prec 0.0608975, recall 0.872323
2017-12-10T13:32:39.658348: step 2660, loss 0.104775, acc 0.953125, prec 0.0609718, recall 0.872478
2017-12-10T13:32:40.098685: step 2661, loss 0.30068, acc 0.90625, prec 0.060988, recall 0.872529
2017-12-10T13:32:40.565900: step 2662, loss 0.298839, acc 0.875, prec 0.0609742, recall 0.872529
2017-12-10T13:32:41.017455: step 2663, loss 0.322414, acc 0.90625, prec 0.0609904, recall 0.872581
2017-12-10T13:32:41.454740: step 2664, loss 0.0963817, acc 0.953125, prec 0.0610382, recall 0.872683
2017-12-10T13:32:41.887412: step 2665, loss 2.35051, acc 0.921875, prec 0.0610313, recall 0.872332
2017-12-10T13:32:42.342925: step 2666, loss 0.155546, acc 0.953125, prec 0.061079, recall 0.872435
2017-12-10T13:32:42.796910: step 2667, loss 0.197112, acc 0.921875, prec 0.0611498, recall 0.872588
2017-12-10T13:32:43.267286: step 2668, loss 0.215023, acc 0.953125, prec 0.0611446, recall 0.872588
2017-12-10T13:32:43.714780: step 2669, loss 0.833758, acc 0.8125, prec 0.0611504, recall 0.87264
2017-12-10T13:32:44.153915: step 2670, loss 0.164598, acc 0.953125, prec 0.0611981, recall 0.872742
2017-12-10T13:32:44.600662: step 2671, loss 0.264462, acc 0.90625, prec 0.0611877, recall 0.872742
2017-12-10T13:32:45.045551: step 2672, loss 0.332432, acc 0.921875, prec 0.0612055, recall 0.872793
2017-12-10T13:32:45.504111: step 2673, loss 0.240215, acc 0.890625, prec 0.0611935, recall 0.872793
2017-12-10T13:32:45.965781: step 2674, loss 0.222619, acc 0.9375, prec 0.0611866, recall 0.872793
2017-12-10T13:32:46.408974: step 2675, loss 0.624672, acc 0.875, prec 0.0611992, recall 0.872844
2017-12-10T13:32:46.850444: step 2676, loss 0.171827, acc 0.9375, prec 0.0611924, recall 0.872844
2017-12-10T13:32:47.294232: step 2677, loss 0.207238, acc 0.9375, prec 0.0611855, recall 0.872844
2017-12-10T13:32:47.740064: step 2678, loss 0.133958, acc 0.953125, prec 0.0612067, recall 0.872895
2017-12-10T13:32:48.190437: step 2679, loss 0.387601, acc 0.90625, prec 0.0612228, recall 0.872946
2017-12-10T13:32:48.644666: step 2680, loss 0.281198, acc 0.9375, prec 0.0612423, recall 0.872997
2017-12-10T13:32:49.089487: step 2681, loss 0.10966, acc 0.953125, prec 0.0612635, recall 0.873048
2017-12-10T13:32:49.557955: step 2682, loss 0.198507, acc 0.921875, prec 0.0612549, recall 0.873048
2017-12-10T13:32:49.993885: step 2683, loss 0.349398, acc 0.953125, prec 0.0612497, recall 0.873048
2017-12-10T13:32:50.426908: step 2684, loss 0.170338, acc 0.953125, prec 0.0612709, recall 0.873098
2017-12-10T13:32:50.881315: step 2685, loss 0.148648, acc 0.921875, prec 0.0612623, recall 0.873098
2017-12-10T13:32:51.351155: step 2686, loss 0.194359, acc 0.96875, prec 0.0612852, recall 0.873149
2017-12-10T13:32:51.800037: step 2687, loss 0.0845039, acc 0.984375, prec 0.0612835, recall 0.873149
2017-12-10T13:32:52.260895: step 2688, loss 2.8125, acc 0.890625, prec 0.0612732, recall 0.8728
2017-12-10T13:32:52.722837: step 2689, loss 0.0397738, acc 1, prec 0.0612732, recall 0.8728
2017-12-10T13:32:53.170359: step 2690, loss 0.111075, acc 0.984375, prec 0.0612715, recall 0.8728
2017-12-10T13:32:53.624463: step 2691, loss 0.340837, acc 0.875, prec 0.0612577, recall 0.8728
2017-12-10T13:32:54.073625: step 2692, loss 0.138013, acc 0.9375, prec 0.0612772, recall 0.872851
2017-12-10T13:32:54.527770: step 2693, loss 0.12843, acc 0.9375, prec 0.0612703, recall 0.872851
2017-12-10T13:32:54.976499: step 2694, loss 0.189383, acc 0.984375, prec 0.0613213, recall 0.872952
2017-12-10T13:32:55.419394: step 2695, loss 0.138427, acc 0.96875, prec 0.0613178, recall 0.872952
2017-12-10T13:32:55.870232: step 2696, loss 0.11348, acc 0.984375, prec 0.0613688, recall 0.873054
2017-12-10T13:32:56.323707: step 2697, loss 0.211211, acc 0.90625, prec 0.0613585, recall 0.873054
2017-12-10T13:32:56.768157: step 2698, loss 0.0971016, acc 0.96875, prec 0.0613814, recall 0.873105
2017-12-10T13:32:57.219965: step 2699, loss 1.59306, acc 0.921875, prec 0.0614271, recall 0.872858
2017-12-10T13:32:57.660412: step 2700, loss 2.33573, acc 0.9375, prec 0.0614483, recall 0.872561
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2700

2017-12-10T13:32:59.572610: step 2701, loss 0.099936, acc 0.953125, prec 0.0614958, recall 0.872662
2017-12-10T13:33:00.019575: step 2702, loss 0.536108, acc 0.859375, prec 0.0615065, recall 0.872713
2017-12-10T13:33:00.464912: step 2703, loss 0.39873, acc 0.890625, prec 0.0614945, recall 0.872713
2017-12-10T13:33:00.911205: step 2704, loss 0.3973, acc 0.828125, prec 0.0614755, recall 0.872713
2017-12-10T13:33:01.358100: step 2705, loss 0.326642, acc 0.875, prec 0.0614617, recall 0.872713
2017-12-10T13:33:01.807592: step 2706, loss 0.38362, acc 0.875, prec 0.0614743, recall 0.872763
2017-12-10T13:33:02.248109: step 2707, loss 0.306366, acc 0.921875, prec 0.0614919, recall 0.872814
2017-12-10T13:33:02.696219: step 2708, loss 0.49511, acc 0.84375, prec 0.0615273, recall 0.872915
2017-12-10T13:33:03.147071: step 2709, loss 0.470875, acc 0.890625, prec 0.0615152, recall 0.872915
2017-12-10T13:33:03.588635: step 2710, loss 0.345853, acc 0.875, prec 0.0615277, recall 0.872965
2017-12-10T13:33:04.045256: step 2711, loss 0.687464, acc 0.828125, prec 0.0615088, recall 0.872965
2017-12-10T13:33:04.491720: step 2712, loss 0.0978369, acc 0.953125, prec 0.0615036, recall 0.872965
2017-12-10T13:33:04.936260: step 2713, loss 0.552953, acc 0.90625, prec 0.061572, recall 0.873117
2017-12-10T13:33:05.385906: step 2714, loss 0.291414, acc 0.921875, prec 0.0615634, recall 0.873117
2017-12-10T13:33:05.831194: step 2715, loss 0.181136, acc 0.9375, prec 0.0615565, recall 0.873117
2017-12-10T13:33:06.274956: step 2716, loss 0.285946, acc 0.859375, prec 0.0615935, recall 0.873217
2017-12-10T13:33:06.725949: step 2717, loss 0.218464, acc 0.90625, prec 0.0615832, recall 0.873217
2017-12-10T13:33:07.182915: step 2718, loss 0.318282, acc 0.921875, prec 0.0615746, recall 0.873217
2017-12-10T13:33:07.634921: step 2719, loss 0.139242, acc 0.953125, prec 0.0615956, recall 0.873267
2017-12-10T13:33:08.073503: step 2720, loss 1.13532, acc 0.9375, prec 0.0616149, recall 0.873317
2017-12-10T13:33:08.523646: step 2721, loss 0.220005, acc 0.921875, prec 0.0616326, recall 0.873368
2017-12-10T13:33:08.967595: step 2722, loss 0.45102, acc 0.890625, prec 0.0616467, recall 0.873418
2017-12-10T13:33:09.438996: step 2723, loss 0.256893, acc 0.921875, prec 0.0616381, recall 0.873418
2017-12-10T13:33:09.880236: step 2724, loss 0.197271, acc 0.9375, prec 0.0616836, recall 0.873518
2017-12-10T13:33:10.328752: step 2725, loss 0.513881, acc 0.796875, prec 0.0617136, recall 0.873618
2017-12-10T13:33:10.779442: step 2726, loss 0.413736, acc 0.84375, prec 0.0617487, recall 0.873717
2017-12-10T13:33:11.235005: step 2727, loss 0.18451, acc 0.921875, prec 0.0617401, recall 0.873717
2017-12-10T13:33:11.679265: step 2728, loss 0.414113, acc 0.875, prec 0.0617263, recall 0.873717
2017-12-10T13:33:12.128749: step 2729, loss 0.35884, acc 0.921875, prec 0.06177, recall 0.873817
2017-12-10T13:33:12.558570: step 2730, loss 0.250167, acc 0.9375, prec 0.0617893, recall 0.873867
2017-12-10T13:33:12.992829: step 2731, loss 0.138547, acc 0.9375, prec 0.0617824, recall 0.873867
2017-12-10T13:33:13.441268: step 2732, loss 0.0988401, acc 0.96875, prec 0.0618313, recall 0.873966
2017-12-10T13:33:13.874531: step 2733, loss 0.0820772, acc 0.984375, prec 0.0618295, recall 0.873966
2017-12-10T13:33:14.313433: step 2734, loss 0.208377, acc 0.9375, prec 0.0618488, recall 0.874016
2017-12-10T13:33:14.766057: step 2735, loss 0.0564282, acc 0.984375, prec 0.0618471, recall 0.874016
2017-12-10T13:33:15.205432: step 2736, loss 0.227028, acc 0.90625, prec 0.0618367, recall 0.874016
2017-12-10T13:33:15.650894: step 2737, loss 0.143509, acc 0.96875, prec 0.0618594, recall 0.874065
2017-12-10T13:33:16.091531: step 2738, loss 0.808774, acc 1, prec 0.0618855, recall 0.874115
2017-12-10T13:33:16.540440: step 2739, loss 0.0772887, acc 0.9375, prec 0.0618786, recall 0.874115
2017-12-10T13:33:16.987103: step 2740, loss 2.07382, acc 0.96875, prec 0.0619553, recall 0.87392
2017-12-10T13:33:17.451159: step 2741, loss 0.274286, acc 0.953125, prec 0.0620023, recall 0.874019
2017-12-10T13:33:17.895867: step 2742, loss 0.2194, acc 0.984375, prec 0.0620528, recall 0.874118
2017-12-10T13:33:18.343685: step 2743, loss 1.25553, acc 0.96875, prec 0.0621277, recall 0.874266
2017-12-10T13:33:18.786313: step 2744, loss 0.0377392, acc 1, prec 0.0621277, recall 0.874266
2017-12-10T13:33:19.241009: step 2745, loss 0.0910736, acc 0.96875, prec 0.0621504, recall 0.874315
2017-12-10T13:33:19.682068: step 2746, loss 0.269365, acc 0.890625, prec 0.0621382, recall 0.874315
2017-12-10T13:33:20.151605: step 2747, loss 0.28354, acc 0.90625, prec 0.062154, recall 0.874364
2017-12-10T13:33:20.600012: step 2748, loss 0.309102, acc 0.953125, prec 0.062201, recall 0.874462
2017-12-10T13:33:21.044968: step 2749, loss 0.126894, acc 0.953125, prec 0.0622479, recall 0.87456
2017-12-10T13:33:21.495147: step 2750, loss 0.283536, acc 0.90625, prec 0.0622897, recall 0.874658
2017-12-10T13:33:21.928792: step 2751, loss 0.272459, acc 0.953125, prec 0.0622845, recall 0.874658
2017-12-10T13:33:22.375545: step 2752, loss 0.366928, acc 0.890625, prec 0.0622985, recall 0.874707
2017-12-10T13:33:22.848553: step 2753, loss 0.25308, acc 0.953125, prec 0.0622933, recall 0.874707
2017-12-10T13:33:23.298532: step 2754, loss 0.386777, acc 0.859375, prec 0.0622777, recall 0.874707
2017-12-10T13:33:23.745673: step 2755, loss 0.621858, acc 0.8125, prec 0.0622569, recall 0.874707
2017-12-10T13:33:24.180736: step 2756, loss 0.39948, acc 0.84375, prec 0.0622657, recall 0.874756
2017-12-10T13:33:24.635232: step 2757, loss 0.65932, acc 0.84375, prec 0.0622484, recall 0.874756
2017-12-10T13:33:25.078941: step 2758, loss 0.251045, acc 0.90625, prec 0.062264, recall 0.874805
2017-12-10T13:33:25.535150: step 2759, loss 0.556383, acc 0.90625, prec 0.0623318, recall 0.874951
2017-12-10T13:33:25.986214: step 2760, loss 0.6304, acc 0.859375, prec 0.0623422, recall 0.875
2017-12-10T13:33:26.442045: step 2761, loss 0.450177, acc 0.859375, prec 0.0623787, recall 0.875097
2017-12-10T13:33:26.873301: step 2762, loss 0.323055, acc 0.90625, prec 0.0624203, recall 0.875194
2017-12-10T13:33:27.310914: step 2763, loss 0.174356, acc 0.9375, prec 0.0624134, recall 0.875194
2017-12-10T13:33:27.748784: step 2764, loss 0.575077, acc 0.875, prec 0.0624255, recall 0.875243
2017-12-10T13:33:28.191357: step 2765, loss 0.275169, acc 0.921875, prec 0.0624169, recall 0.875243
2017-12-10T13:33:28.632792: step 2766, loss 0.292223, acc 0.921875, prec 0.0624082, recall 0.875243
2017-12-10T13:33:29.079841: step 2767, loss 0.23935, acc 0.90625, prec 0.0623978, recall 0.875243
2017-12-10T13:33:29.530623: step 2768, loss 1.127, acc 0.921875, prec 0.0624411, recall 0.87534
2017-12-10T13:33:29.979296: step 2769, loss 0.677327, acc 0.953125, prec 0.0625398, recall 0.875533
2017-12-10T13:33:30.434999: step 2770, loss 0.212873, acc 0.9375, prec 0.0625588, recall 0.875581
2017-12-10T13:33:30.895232: step 2771, loss 0.281298, acc 0.90625, prec 0.0625485, recall 0.875581
2017-12-10T13:33:31.339224: step 2772, loss 0.144836, acc 0.953125, prec 0.0625433, recall 0.875581
2017-12-10T13:33:31.786030: step 2773, loss 0.161901, acc 0.921875, prec 0.0625346, recall 0.875581
2017-12-10T13:33:32.227156: step 2774, loss 0.280276, acc 0.9375, prec 0.0625536, recall 0.87563
2017-12-10T13:33:32.675612: step 2775, loss 0.222791, acc 0.9375, prec 0.0625467, recall 0.87563
2017-12-10T13:33:33.122317: step 2776, loss 0.0724947, acc 0.953125, prec 0.0625934, recall 0.875726
2017-12-10T13:33:33.584291: step 2777, loss 0.103313, acc 0.9375, prec 0.0625865, recall 0.875726
2017-12-10T13:33:34.021781: step 2778, loss 0.136299, acc 0.953125, prec 0.0625813, recall 0.875726
2017-12-10T13:33:34.459506: step 2779, loss 0.119777, acc 0.953125, prec 0.0626539, recall 0.87587
2017-12-10T13:33:34.910067: step 2780, loss 0.208398, acc 0.953125, prec 0.0626487, recall 0.87587
2017-12-10T13:33:35.360389: step 2781, loss 0.216938, acc 0.921875, prec 0.0626659, recall 0.875918
2017-12-10T13:33:35.813917: step 2782, loss 0.0144049, acc 1, prec 0.0626659, recall 0.875918
2017-12-10T13:33:36.279807: step 2783, loss 0.0288021, acc 1, prec 0.0626659, recall 0.875918
2017-12-10T13:33:36.723995: step 2784, loss 0.187843, acc 0.953125, prec 0.0626867, recall 0.875966
2017-12-10T13:33:37.179740: step 2785, loss 0.423578, acc 0.96875, prec 0.0627091, recall 0.876014
2017-12-10T13:33:37.619012: step 2786, loss 0.0973612, acc 0.953125, prec 0.0627039, recall 0.876014
2017-12-10T13:33:38.072580: step 2787, loss 0.0407348, acc 0.984375, prec 0.0627022, recall 0.876014
2017-12-10T13:33:38.513053: step 2788, loss 0.080848, acc 0.96875, prec 0.0627246, recall 0.876062
2017-12-10T13:33:38.975652: step 2789, loss 0.0343942, acc 1, prec 0.0627246, recall 0.876062
2017-12-10T13:33:39.437955: step 2790, loss 0.147345, acc 0.9375, prec 0.0627177, recall 0.876062
2017-12-10T13:33:39.888883: step 2791, loss 0.0896541, acc 0.96875, prec 0.0627142, recall 0.876062
2017-12-10T13:33:40.342536: step 2792, loss 0.0263898, acc 1, prec 0.0627401, recall 0.87611
2017-12-10T13:33:40.785689: step 2793, loss 0.286691, acc 0.96875, prec 0.0628143, recall 0.876253
2017-12-10T13:33:41.238622: step 2794, loss 0.221029, acc 0.984375, prec 0.0628385, recall 0.876301
2017-12-10T13:33:41.693130: step 2795, loss 0.0901869, acc 0.984375, prec 0.0628886, recall 0.876396
2017-12-10T13:33:42.140437: step 2796, loss 0.188323, acc 1, prec 0.0629145, recall 0.876443
2017-12-10T13:33:42.601680: step 2797, loss 6.60614, acc 0.984375, prec 0.0629145, recall 0.876106
2017-12-10T13:33:43.057181: step 2798, loss 0.0405171, acc 0.984375, prec 0.0629127, recall 0.876106
2017-12-10T13:33:43.505573: step 2799, loss 3.09636, acc 0.953125, prec 0.0629351, recall 0.875817
2017-12-10T13:33:43.972399: step 2800, loss 0.188477, acc 0.96875, prec 0.0629575, recall 0.875865
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2800

2017-12-10T13:33:45.921325: step 2801, loss 0.552845, acc 0.90625, prec 0.062973, recall 0.875912
2017-12-10T13:33:46.393647: step 2802, loss 0.281916, acc 0.90625, prec 0.0629884, recall 0.87596
2017-12-10T13:33:46.819811: step 2803, loss 0.123118, acc 0.9375, prec 0.0629815, recall 0.87596
2017-12-10T13:33:47.267684: step 2804, loss 0.58471, acc 0.8125, prec 0.0630123, recall 0.876055
2017-12-10T13:33:47.701007: step 2805, loss 0.390119, acc 0.859375, prec 0.0630225, recall 0.876103
2017-12-10T13:33:48.150275: step 2806, loss 0.216526, acc 0.90625, prec 0.063038, recall 0.87615
2017-12-10T13:33:48.596708: step 2807, loss 0.174222, acc 0.921875, prec 0.0631068, recall 0.876293
2017-12-10T13:33:49.070298: step 2808, loss 0.440752, acc 0.84375, prec 0.0631152, recall 0.87634
2017-12-10T13:33:49.522013: step 2809, loss 0.295835, acc 0.890625, prec 0.063103, recall 0.87634
2017-12-10T13:33:49.971886: step 2810, loss 0.677392, acc 0.828125, prec 0.0630839, recall 0.87634
2017-12-10T13:33:50.393094: step 2811, loss 0.611903, acc 0.78125, prec 0.0630854, recall 0.876387
2017-12-10T13:33:50.833067: step 2812, loss 0.439021, acc 0.875, prec 0.0630715, recall 0.876387
2017-12-10T13:33:51.275475: step 2813, loss 0.308071, acc 0.875, prec 0.0630576, recall 0.876387
2017-12-10T13:33:51.745316: step 2814, loss 0.415707, acc 0.890625, prec 0.0630713, recall 0.876435
2017-12-10T13:33:52.193973: step 2815, loss 0.274096, acc 0.90625, prec 0.0630608, recall 0.876435
2017-12-10T13:33:52.642158: step 2816, loss 0.317634, acc 0.9375, prec 0.0631055, recall 0.876529
2017-12-10T13:33:53.082532: step 2817, loss 0.347965, acc 0.890625, prec 0.0630933, recall 0.876529
2017-12-10T13:33:53.527486: step 2818, loss 0.318355, acc 0.859375, prec 0.0630777, recall 0.876529
2017-12-10T13:33:53.963215: step 2819, loss 0.077896, acc 0.96875, prec 0.0630742, recall 0.876529
2017-12-10T13:33:54.413828: step 2820, loss 0.736635, acc 0.859375, prec 0.0631101, recall 0.876623
2017-12-10T13:33:54.863889: step 2821, loss 0.313323, acc 0.953125, prec 0.0631049, recall 0.876623
2017-12-10T13:33:55.323317: step 2822, loss 0.439361, acc 0.921875, prec 0.0631735, recall 0.876765
2017-12-10T13:33:55.762585: step 2823, loss 1.94348, acc 0.875, prec 0.0631614, recall 0.87643
2017-12-10T13:33:56.208260: step 2824, loss 0.881139, acc 0.84375, prec 0.0631698, recall 0.876477
2017-12-10T13:33:56.643012: step 2825, loss 1.34584, acc 0.859375, prec 0.0631559, recall 0.876143
2017-12-10T13:33:57.114035: step 2826, loss 0.200218, acc 0.9375, prec 0.0631747, recall 0.87619
2017-12-10T13:33:57.549238: step 2827, loss 0.123425, acc 0.953125, prec 0.0631695, recall 0.87619
2017-12-10T13:33:57.993693: step 2828, loss 0.546588, acc 0.921875, prec 0.0631608, recall 0.87619
2017-12-10T13:33:58.434383: step 2829, loss 0.501557, acc 0.828125, prec 0.0631931, recall 0.876285
2017-12-10T13:33:58.894232: step 2830, loss 0.816133, acc 0.90625, prec 0.0632342, recall 0.876379
2017-12-10T13:33:59.331098: step 2831, loss 0.358575, acc 0.84375, prec 0.0632168, recall 0.876379
2017-12-10T13:33:59.792391: step 2832, loss 0.490291, acc 0.828125, prec 0.0631977, recall 0.876379
2017-12-10T13:34:00.236228: step 2833, loss 0.624845, acc 0.796875, prec 0.0632009, recall 0.876426
2017-12-10T13:34:00.676649: step 2834, loss 0.240761, acc 0.9375, prec 0.063194, recall 0.876426
2017-12-10T13:34:01.122620: step 2835, loss 0.408995, acc 0.921875, prec 0.0632623, recall 0.876567
2017-12-10T13:34:01.559450: step 2836, loss 0.54778, acc 0.78125, prec 0.0632381, recall 0.876567
2017-12-10T13:34:02.012799: step 2837, loss 0.349618, acc 0.875, prec 0.0632499, recall 0.876613
2017-12-10T13:34:02.469480: step 2838, loss 0.320286, acc 0.890625, prec 0.0632377, recall 0.876613
2017-12-10T13:34:02.902257: step 2839, loss 0.500502, acc 0.84375, prec 0.0632204, recall 0.876613
2017-12-10T13:34:03.339906: step 2840, loss 0.43599, acc 0.875, prec 0.0632066, recall 0.876613
2017-12-10T13:34:03.766082: step 2841, loss 0.251584, acc 0.890625, prec 0.0632458, recall 0.876707
2017-12-10T13:34:04.212142: step 2842, loss 0.152455, acc 0.921875, prec 0.0632371, recall 0.876707
2017-12-10T13:34:04.669042: step 2843, loss 0.250757, acc 0.90625, prec 0.0632267, recall 0.876707
2017-12-10T13:34:05.129957: step 2844, loss 0.291145, acc 0.9375, prec 0.0632967, recall 0.876847
2017-12-10T13:34:05.575452: step 2845, loss 0.140393, acc 0.9375, prec 0.0633154, recall 0.876894
2017-12-10T13:34:06.026415: step 2846, loss 0.246298, acc 0.96875, prec 0.0633888, recall 0.877034
2017-12-10T13:34:06.474961: step 2847, loss 0.103131, acc 0.96875, prec 0.0634109, recall 0.87708
2017-12-10T13:34:06.909650: step 2848, loss 0.224543, acc 0.953125, prec 0.0634569, recall 0.877173
2017-12-10T13:34:07.360757: step 2849, loss 0.0647017, acc 0.984375, prec 0.0634808, recall 0.877219
2017-12-10T13:34:07.824181: step 2850, loss 0.197246, acc 0.953125, prec 0.0635012, recall 0.877266
2017-12-10T13:34:08.277513: step 2851, loss 1.74428, acc 0.953125, prec 0.0635745, recall 0.877074
2017-12-10T13:34:08.736636: step 2852, loss 0.140276, acc 0.953125, prec 0.0635949, recall 0.87712
2017-12-10T13:34:09.180360: step 2853, loss 0.154566, acc 0.96875, prec 0.0636426, recall 0.877213
2017-12-10T13:34:09.625105: step 2854, loss 0.116517, acc 0.953125, prec 0.0636629, recall 0.877259
2017-12-10T13:34:10.066068: step 2855, loss 0.148829, acc 0.96875, prec 0.063685, recall 0.877305
2017-12-10T13:34:10.518950: step 2856, loss 0.26889, acc 0.90625, prec 0.0636746, recall 0.877305
2017-12-10T13:34:10.964776: step 2857, loss 0.105994, acc 0.96875, prec 0.0637223, recall 0.877398
2017-12-10T13:34:11.422233: step 2858, loss 0.124311, acc 0.984375, prec 0.0637461, recall 0.877444
2017-12-10T13:34:11.871206: step 2859, loss 3.97392, acc 0.921875, prec 0.0637647, recall 0.87716
2017-12-10T13:34:12.318971: step 2860, loss 0.360099, acc 0.890625, prec 0.0637525, recall 0.87716
2017-12-10T13:34:12.767643: step 2861, loss 0.265499, acc 0.9375, prec 0.0637456, recall 0.87716
2017-12-10T13:34:13.231779: step 2862, loss 0.299236, acc 0.9375, prec 0.0637642, recall 0.877206
2017-12-10T13:34:13.685209: step 2863, loss 0.26596, acc 0.890625, prec 0.063752, recall 0.877206
2017-12-10T13:34:14.125168: step 2864, loss 0.054704, acc 0.96875, prec 0.0637485, recall 0.877206
2017-12-10T13:34:14.572107: step 2865, loss 0.286737, acc 0.90625, prec 0.0637381, recall 0.877206
2017-12-10T13:34:15.022171: step 2866, loss 0.382045, acc 0.859375, prec 0.0637224, recall 0.877206
2017-12-10T13:34:15.487880: step 2867, loss 0.0942403, acc 0.953125, prec 0.0637172, recall 0.877206
2017-12-10T13:34:15.939389: step 2868, loss 0.413402, acc 0.875, prec 0.0637033, recall 0.877206
2017-12-10T13:34:16.397255: step 2869, loss 0.297501, acc 0.921875, prec 0.0636946, recall 0.877206
2017-12-10T13:34:16.842564: step 2870, loss 0.114209, acc 0.96875, prec 0.0637167, recall 0.877252
2017-12-10T13:34:17.303895: step 2871, loss 0.564125, acc 0.859375, prec 0.063701, recall 0.877252
2017-12-10T13:34:17.756218: step 2872, loss 0.664182, acc 0.828125, prec 0.0637075, recall 0.877298
2017-12-10T13:34:18.197122: step 2873, loss 0.247379, acc 0.875, prec 0.0637191, recall 0.877344
2017-12-10T13:34:18.647042: step 2874, loss 0.816159, acc 0.875, prec 0.0637562, recall 0.877436
2017-12-10T13:34:19.102793: step 2875, loss 0.23837, acc 0.9375, prec 0.0638257, recall 0.877574
2017-12-10T13:34:19.556313: step 2876, loss 0.250466, acc 0.859375, prec 0.0638101, recall 0.877574
2017-12-10T13:34:20.005801: step 2877, loss 0.169177, acc 0.96875, prec 0.0639085, recall 0.877757
2017-12-10T13:34:20.463707: step 2878, loss 0.193754, acc 0.921875, prec 0.0638999, recall 0.877757
2017-12-10T13:34:20.926462: step 2879, loss 0.473449, acc 0.90625, prec 0.0638894, recall 0.877757
2017-12-10T13:34:21.376639: step 2880, loss 0.291885, acc 0.890625, prec 0.0639282, recall 0.877848
2017-12-10T13:34:21.824062: step 2881, loss 0.386031, acc 1, prec 0.0639536, recall 0.877894
2017-12-10T13:34:22.275422: step 2882, loss 0.0332766, acc 1, prec 0.0639791, recall 0.87794
2017-12-10T13:34:22.731864: step 2883, loss 0.27144, acc 0.96875, prec 0.0640011, recall 0.877985
2017-12-10T13:34:23.182344: step 2884, loss 0.948886, acc 1, prec 0.0640775, recall 0.878121
2017-12-10T13:34:23.647356: step 2885, loss 0.593753, acc 0.953125, prec 0.0640977, recall 0.878167
2017-12-10T13:34:24.089157: step 2886, loss 0.238479, acc 0.953125, prec 0.0640925, recall 0.878167
2017-12-10T13:34:24.518250: step 2887, loss 0.276196, acc 0.921875, prec 0.0641092, recall 0.878212
2017-12-10T13:34:24.960297: step 2888, loss 0.613387, acc 0.875, prec 0.0640952, recall 0.878212
2017-12-10T13:34:25.399221: step 2889, loss 0.561114, acc 0.859375, prec 0.0641304, recall 0.878303
2017-12-10T13:34:25.835785: step 2890, loss 0.399008, acc 0.859375, prec 0.0641656, recall 0.878393
2017-12-10T13:34:26.294587: step 2891, loss 0.131363, acc 0.953125, prec 0.0641858, recall 0.878439
2017-12-10T13:34:26.722129: step 2892, loss 0.68228, acc 0.828125, prec 0.064192, recall 0.878484
2017-12-10T13:34:27.170839: step 2893, loss 0.245952, acc 0.875, prec 0.0641781, recall 0.878484
2017-12-10T13:34:27.600468: step 2894, loss 0.310116, acc 0.921875, prec 0.0641694, recall 0.878484
2017-12-10T13:34:28.047597: step 2895, loss 0.816234, acc 0.9375, prec 0.0641878, recall 0.878529
2017-12-10T13:34:28.504082: step 2896, loss 0.38412, acc 0.875, prec 0.0641739, recall 0.878529
2017-12-10T13:34:28.972512: step 2897, loss 0.0953645, acc 0.953125, prec 0.0642194, recall 0.878619
2017-12-10T13:34:29.416626: step 2898, loss 0.367671, acc 0.921875, prec 0.0642361, recall 0.878664
2017-12-10T13:34:29.868654: step 2899, loss 0.114755, acc 0.984375, prec 0.0642851, recall 0.878754
2017-12-10T13:34:30.315680: step 2900, loss 0.222067, acc 0.9375, prec 0.0642782, recall 0.878754
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-2900

2017-12-10T13:34:32.213422: step 2901, loss 0.211751, acc 0.921875, prec 0.0643202, recall 0.878844
2017-12-10T13:34:32.651620: step 2902, loss 0.155665, acc 0.953125, prec 0.0643403, recall 0.878889
2017-12-10T13:34:33.089878: step 2903, loss 0.371943, acc 0.90625, prec 0.0643552, recall 0.878934
2017-12-10T13:34:33.544524: step 2904, loss 0.330277, acc 0.90625, prec 0.0643448, recall 0.878934
2017-12-10T13:34:34.016798: step 2905, loss 0.222192, acc 0.953125, prec 0.0643395, recall 0.878934
2017-12-10T13:34:34.449823: step 2906, loss 0.386102, acc 0.875, prec 0.0643256, recall 0.878934
2017-12-10T13:34:34.904343: step 2907, loss 0.296631, acc 0.875, prec 0.0643116, recall 0.878934
2017-12-10T13:34:35.334959: step 2908, loss 0.0355315, acc 1, prec 0.0643116, recall 0.878934
2017-12-10T13:34:35.785432: step 2909, loss 0.212985, acc 0.90625, prec 0.0643012, recall 0.878934
2017-12-10T13:34:36.240313: step 2910, loss 0.157786, acc 0.953125, prec 0.064296, recall 0.878934
2017-12-10T13:34:36.701935: step 2911, loss 0.2677, acc 0.9375, prec 0.0643397, recall 0.879023
2017-12-10T13:34:37.156670: step 2912, loss 0.113699, acc 0.96875, prec 0.0643615, recall 0.879068
2017-12-10T13:34:37.612203: step 2913, loss 4.36704, acc 0.96875, prec 0.0643598, recall 0.878743
2017-12-10T13:34:38.072994: step 2914, loss 0.146669, acc 0.96875, prec 0.0643563, recall 0.878743
2017-12-10T13:34:38.511692: step 2915, loss 0.0299353, acc 1, prec 0.0643816, recall 0.878788
2017-12-10T13:34:38.958289: step 2916, loss 0.102631, acc 0.984375, prec 0.0643799, recall 0.878788
2017-12-10T13:34:39.417200: step 2917, loss 0.0320097, acc 0.984375, prec 0.0643781, recall 0.878788
2017-12-10T13:34:39.862926: step 2918, loss 0.266277, acc 0.9375, prec 0.0644218, recall 0.878877
2017-12-10T13:34:40.309725: step 2919, loss 0.151828, acc 0.96875, prec 0.0644437, recall 0.878922
2017-12-10T13:34:40.758653: step 2920, loss 0.414406, acc 0.90625, prec 0.0644838, recall 0.879011
2017-12-10T13:34:41.199890: step 2921, loss 0.111827, acc 0.921875, prec 0.0644751, recall 0.879011
2017-12-10T13:34:41.659457: step 2922, loss 0.275049, acc 0.921875, prec 0.0644917, recall 0.879056
2017-12-10T13:34:42.132685: step 2923, loss 0.221094, acc 0.953125, prec 0.0645118, recall 0.879101
2017-12-10T13:34:42.586834: step 2924, loss 0.564678, acc 0.921875, prec 0.0645789, recall 0.879234
2017-12-10T13:34:43.036983: step 2925, loss 0.122537, acc 0.9375, prec 0.064572, recall 0.879234
2017-12-10T13:34:43.478047: step 2926, loss 0.335625, acc 0.90625, prec 0.0645868, recall 0.879279
2017-12-10T13:34:43.918207: step 2927, loss 0.625666, acc 0.96875, prec 0.0646338, recall 0.879367
2017-12-10T13:34:44.376152: step 2928, loss 0.217914, acc 0.9375, prec 0.0646521, recall 0.879412
2017-12-10T13:34:44.826927: step 2929, loss 0.118271, acc 0.953125, prec 0.0646469, recall 0.879412
2017-12-10T13:34:45.283845: step 2930, loss 0.321632, acc 0.90625, prec 0.0646364, recall 0.879412
2017-12-10T13:34:45.734213: step 2931, loss 0.298513, acc 0.90625, prec 0.0647018, recall 0.879545
2017-12-10T13:34:46.185583: step 2932, loss 0.933031, acc 0.90625, prec 0.0647165, recall 0.879589
2017-12-10T13:34:46.622844: step 2933, loss 0.202211, acc 0.96875, prec 0.064713, recall 0.879589
2017-12-10T13:34:47.080013: step 2934, loss 0.118046, acc 0.953125, prec 0.0647078, recall 0.879589
2017-12-10T13:34:47.525726: step 2935, loss 0.352524, acc 0.953125, prec 0.0647278, recall 0.879633
2017-12-10T13:34:47.977692: step 2936, loss 0.0273437, acc 1, prec 0.0647278, recall 0.879633
2017-12-10T13:34:48.432035: step 2937, loss 0.322697, acc 0.9375, prec 0.0647966, recall 0.879765
2017-12-10T13:34:48.881150: step 2938, loss 0.12379, acc 0.96875, prec 0.0648183, recall 0.879809
2017-12-10T13:34:49.326085: step 2939, loss 0.21871, acc 0.96875, prec 0.0648905, recall 0.879941
2017-12-10T13:34:49.795215: step 2940, loss 0.136693, acc 1, prec 0.064941, recall 0.880029
2017-12-10T13:34:50.242257: step 2941, loss 0.189162, acc 0.90625, prec 0.0649305, recall 0.880029
2017-12-10T13:34:50.695502: step 2942, loss 0.217904, acc 0.96875, prec 0.0649775, recall 0.880117
2017-12-10T13:34:51.143062: step 2943, loss 0.499602, acc 0.921875, prec 0.0650192, recall 0.880205
2017-12-10T13:34:51.592423: step 2944, loss 0.145507, acc 0.96875, prec 0.0650156, recall 0.880205
2017-12-10T13:34:52.030857: step 2945, loss 0.247427, acc 0.90625, prec 0.0650303, recall 0.880248
2017-12-10T13:34:52.487408: step 2946, loss 0.303623, acc 0.890625, prec 0.0650937, recall 0.880379
2017-12-10T13:34:52.936559: step 2947, loss 0.12326, acc 0.953125, prec 0.0650884, recall 0.880379
2017-12-10T13:34:53.381634: step 2948, loss 0.448433, acc 0.9375, prec 0.0651318, recall 0.880466
2017-12-10T13:34:53.839662: step 2949, loss 0.178079, acc 0.90625, prec 0.0651717, recall 0.880554
2017-12-10T13:34:54.289690: step 2950, loss 0.13689, acc 0.96875, prec 0.0651682, recall 0.880554
2017-12-10T13:34:54.742715: step 2951, loss 0.267713, acc 0.953125, prec 0.0651881, recall 0.880597
2017-12-10T13:34:55.203184: step 2952, loss 0.0932821, acc 0.953125, prec 0.0651828, recall 0.880597
2017-12-10T13:34:55.651063: step 2953, loss 0.351559, acc 0.9375, prec 0.0652262, recall 0.880684
2017-12-10T13:34:56.094199: step 2954, loss 0.032791, acc 1, prec 0.0652765, recall 0.880771
2017-12-10T13:34:56.537746: step 2955, loss 0.143404, acc 0.921875, prec 0.0652677, recall 0.880771
2017-12-10T13:34:56.990950: step 2956, loss 0.0939411, acc 0.984375, prec 0.065266, recall 0.880771
2017-12-10T13:34:57.431440: step 2957, loss 0.189938, acc 0.953125, prec 0.0652859, recall 0.880814
2017-12-10T13:34:57.894638: step 2958, loss 0.0648062, acc 0.96875, prec 0.0652824, recall 0.880814
2017-12-10T13:34:58.337886: step 2959, loss 2.1095, acc 0.921875, prec 0.0653005, recall 0.880537
2017-12-10T13:34:58.789822: step 2960, loss 0.176841, acc 0.953125, prec 0.0653204, recall 0.880581
2017-12-10T13:34:59.240482: step 2961, loss 0.450096, acc 0.9375, prec 0.0653385, recall 0.880624
2017-12-10T13:34:59.694542: step 2962, loss 0.280102, acc 0.921875, prec 0.0653297, recall 0.880624
2017-12-10T13:35:00.149305: step 2963, loss 0.179011, acc 0.96875, prec 0.0653765, recall 0.880711
2017-12-10T13:35:00.619645: step 2964, loss 0.274003, acc 0.890625, prec 0.0653642, recall 0.880711
2017-12-10T13:35:01.057048: step 2965, loss 0.12343, acc 0.921875, prec 0.0653806, recall 0.880754
2017-12-10T13:35:01.501901: step 2966, loss 0.208733, acc 0.9375, prec 0.0653987, recall 0.880797
2017-12-10T13:35:01.972581: step 2967, loss 0.239585, acc 0.9375, prec 0.0653917, recall 0.880797
2017-12-10T13:35:02.411453: step 2968, loss 0.473232, acc 0.921875, prec 0.0654331, recall 0.880883
2017-12-10T13:35:02.868698: step 2969, loss 0.225718, acc 0.9375, prec 0.0654512, recall 0.880927
2017-12-10T13:35:03.332123: step 2970, loss 0.257559, acc 0.984375, prec 0.0654495, recall 0.880927
2017-12-10T13:35:03.779387: step 2971, loss 0.137875, acc 0.9375, prec 0.0654424, recall 0.880927
2017-12-10T13:35:04.232693: step 2972, loss 0.216948, acc 0.96875, prec 0.065464, recall 0.88097
2017-12-10T13:35:04.688457: step 2973, loss 0.403346, acc 0.859375, prec 0.0655235, recall 0.881099
2017-12-10T13:35:05.135264: step 2974, loss 0.426678, acc 0.90625, prec 0.0655632, recall 0.881185
2017-12-10T13:35:05.575280: step 2975, loss 0.301303, acc 0.890625, prec 0.065576, recall 0.881227
2017-12-10T13:35:06.031659: step 2976, loss 0.223336, acc 0.90625, prec 0.0655905, recall 0.88127
2017-12-10T13:35:06.487029: step 2977, loss 0.103612, acc 0.953125, prec 0.0655852, recall 0.88127
2017-12-10T13:35:06.932770: step 2978, loss 0.446632, acc 0.921875, prec 0.0656266, recall 0.881356
2017-12-10T13:35:07.384521: step 2979, loss 0.177229, acc 0.953125, prec 0.0656213, recall 0.881356
2017-12-10T13:35:07.825385: step 2980, loss 0.346105, acc 0.96875, prec 0.065693, recall 0.881484
2017-12-10T13:35:08.269183: step 2981, loss 0.146594, acc 0.9375, prec 0.0657361, recall 0.881569
2017-12-10T13:35:08.681790: step 2982, loss 0.0289135, acc 1, prec 0.0657361, recall 0.881569
2017-12-10T13:35:09.145216: step 2983, loss 0.116135, acc 0.953125, prec 0.0657308, recall 0.881569
2017-12-10T13:35:09.594751: step 2984, loss 0.0563759, acc 0.96875, prec 0.0657273, recall 0.881569
2017-12-10T13:35:10.059530: step 2985, loss 0.0410086, acc 0.984375, prec 0.0657256, recall 0.881569
2017-12-10T13:35:10.512777: step 2986, loss 0.0926734, acc 0.96875, prec 0.0657972, recall 0.881697
2017-12-10T13:35:10.984445: step 2987, loss 0.160892, acc 0.953125, prec 0.065817, recall 0.88174
2017-12-10T13:35:11.441715: step 2988, loss 0.0984913, acc 0.984375, prec 0.0658403, recall 0.881782
2017-12-10T13:35:11.884474: step 2989, loss 3.56968, acc 0.96875, prec 0.0658636, recall 0.881508
2017-12-10T13:35:12.333510: step 2990, loss 0.205199, acc 0.921875, prec 0.0658548, recall 0.881508
2017-12-10T13:35:12.776981: step 2991, loss 0.129919, acc 0.953125, prec 0.0658745, recall 0.881551
2017-12-10T13:35:13.229396: step 2992, loss 0.247646, acc 1, prec 0.0659246, recall 0.881636
2017-12-10T13:35:13.683899: step 2993, loss 0.242262, acc 0.890625, prec 0.0659123, recall 0.881636
2017-12-10T13:35:14.136892: step 2994, loss 0.257219, acc 0.921875, prec 0.0659034, recall 0.881636
2017-12-10T13:35:14.591270: step 2995, loss 0.0919381, acc 0.9375, prec 0.0659214, recall 0.881678
2017-12-10T13:35:15.028255: step 2996, loss 0.341563, acc 0.9375, prec 0.0659644, recall 0.881763
2017-12-10T13:35:15.476310: step 2997, loss 0.218285, acc 0.890625, prec 0.0659771, recall 0.881805
2017-12-10T13:35:15.915841: step 2998, loss 0.120473, acc 0.953125, prec 0.0659968, recall 0.881848
2017-12-10T13:35:16.377107: step 2999, loss 0.298489, acc 0.890625, prec 0.0660094, recall 0.88189
2017-12-10T13:35:16.832013: step 3000, loss 0.17468, acc 0.9375, prec 0.0660274, recall 0.881932
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3000

2017-12-10T13:35:18.927452: step 3001, loss 0.710141, acc 0.765625, prec 0.0660009, recall 0.881932
2017-12-10T13:35:19.364909: step 3002, loss 0.294385, acc 0.9375, prec 0.0660688, recall 0.882059
2017-12-10T13:35:19.814063: step 3003, loss 0.213806, acc 0.90625, prec 0.0660582, recall 0.882059
2017-12-10T13:35:20.253466: step 3004, loss 0.0950774, acc 0.96875, prec 0.0660546, recall 0.882059
2017-12-10T13:35:20.708547: step 3005, loss 0.0684605, acc 0.984375, prec 0.0661029, recall 0.882143
2017-12-10T13:35:21.171337: step 3006, loss 0.0911916, acc 0.953125, prec 0.0661476, recall 0.882227
2017-12-10T13:35:21.632196: step 3007, loss 0.195853, acc 0.9375, prec 0.0661405, recall 0.882227
2017-12-10T13:35:22.080138: step 3008, loss 0.263755, acc 0.9375, prec 0.0661334, recall 0.882227
2017-12-10T13:35:22.519534: step 3009, loss 0.91507, acc 0.96875, prec 0.0661798, recall 0.882311
2017-12-10T13:35:22.979880: step 3010, loss 0.11047, acc 0.96875, prec 0.0661763, recall 0.882311
2017-12-10T13:35:23.434290: step 3011, loss 0.236032, acc 0.90625, prec 0.0661906, recall 0.882353
2017-12-10T13:35:23.900029: step 3012, loss 1.50752, acc 1, prec 0.0662406, recall 0.882437
2017-12-10T13:35:24.351168: step 3013, loss 0.0873078, acc 0.96875, prec 0.066262, recall 0.882479
2017-12-10T13:35:24.804337: step 3014, loss 0.087484, acc 0.96875, prec 0.0662585, recall 0.882479
2017-12-10T13:35:25.256001: step 3015, loss 0.150215, acc 0.9375, prec 0.0662514, recall 0.882479
2017-12-10T13:35:25.704653: step 3016, loss 0.433496, acc 0.90625, prec 0.0662907, recall 0.882562
2017-12-10T13:35:26.142822: step 3017, loss 0.241951, acc 0.953125, prec 0.0662853, recall 0.882562
2017-12-10T13:35:26.596832: step 3018, loss 0.132651, acc 0.953125, prec 0.0663549, recall 0.882688
2017-12-10T13:35:27.042427: step 3019, loss 0.173878, acc 0.9375, prec 0.0663727, recall 0.882729
2017-12-10T13:35:27.492464: step 3020, loss 0.232322, acc 0.921875, prec 0.0663639, recall 0.882729
2017-12-10T13:35:27.937449: step 3021, loss 0.267295, acc 0.9375, prec 0.0664565, recall 0.882896
2017-12-10T13:35:28.372032: step 3022, loss 0.295329, acc 0.921875, prec 0.0664726, recall 0.882937
2017-12-10T13:35:28.808223: step 3023, loss 0.296198, acc 0.9375, prec 0.0664904, recall 0.882979
2017-12-10T13:35:29.269782: step 3024, loss 0.057256, acc 0.984375, prec 0.0664887, recall 0.882979
2017-12-10T13:35:29.711144: step 3025, loss 0.107191, acc 0.984375, prec 0.0665118, recall 0.88302
2017-12-10T13:35:30.174299: step 3026, loss 0.347889, acc 0.9375, prec 0.0665047, recall 0.88302
2017-12-10T13:35:30.624210: step 3027, loss 0.262666, acc 0.875, prec 0.0665154, recall 0.883062
2017-12-10T13:35:31.076927: step 3028, loss 0.320009, acc 0.890625, prec 0.066503, recall 0.883062
2017-12-10T13:35:31.526542: step 3029, loss 0.370157, acc 0.953125, prec 0.0664977, recall 0.883062
2017-12-10T13:35:31.974913: step 3030, loss 0.0707208, acc 0.984375, prec 0.0664959, recall 0.883062
2017-12-10T13:35:32.418309: step 3031, loss 0.0789024, acc 0.953125, prec 0.0664906, recall 0.883062
2017-12-10T13:35:32.867041: step 3032, loss 0.135306, acc 0.953125, prec 0.0664852, recall 0.883062
2017-12-10T13:35:33.310919: step 3033, loss 0.127381, acc 0.96875, prec 0.0665564, recall 0.883186
2017-12-10T13:35:33.757707: step 3034, loss 0.080845, acc 0.953125, prec 0.0665511, recall 0.883186
2017-12-10T13:35:34.202926: step 3035, loss 3.46054, acc 0.953125, prec 0.0665724, recall 0.882915
2017-12-10T13:35:34.671628: step 3036, loss 0.0292299, acc 1, prec 0.0665973, recall 0.882956
2017-12-10T13:35:35.121679: step 3037, loss 0.0799423, acc 0.96875, prec 0.0666187, recall 0.882998
2017-12-10T13:35:35.568170: step 3038, loss 0.172867, acc 0.9375, prec 0.0666364, recall 0.883039
2017-12-10T13:35:36.005598: step 3039, loss 0.25675, acc 0.90625, prec 0.0666258, recall 0.883039
2017-12-10T13:35:36.464060: step 3040, loss 0.244882, acc 0.890625, prec 0.0666382, recall 0.88308
2017-12-10T13:35:36.909782: step 3041, loss 0.136344, acc 0.953125, prec 0.0666329, recall 0.88308
2017-12-10T13:35:37.363066: step 3042, loss 0.448856, acc 0.875, prec 0.0666436, recall 0.883121
2017-12-10T13:35:37.795654: step 3043, loss 0.24209, acc 0.953125, prec 0.066688, recall 0.883204
2017-12-10T13:35:38.241392: step 3044, loss 0.361079, acc 0.9375, prec 0.0667555, recall 0.883327
2017-12-10T13:35:38.683132: step 3045, loss 0.327223, acc 0.921875, prec 0.0667963, recall 0.88341
2017-12-10T13:35:39.123817: step 3046, loss 0.291666, acc 0.921875, prec 0.0668122, recall 0.883451
2017-12-10T13:35:39.559714: step 3047, loss 0.227469, acc 0.921875, prec 0.0668033, recall 0.883451
2017-12-10T13:35:40.017913: step 3048, loss 0.29002, acc 0.9375, prec 0.0668708, recall 0.883574
2017-12-10T13:35:40.456916: step 3049, loss 0.135031, acc 0.953125, prec 0.0669151, recall 0.883656
2017-12-10T13:35:40.905115: step 3050, loss 0.0310037, acc 0.984375, prec 0.0669133, recall 0.883656
2017-12-10T13:35:41.332767: step 3051, loss 0.224618, acc 0.9375, prec 0.066931, recall 0.883696
2017-12-10T13:35:41.787130: step 3052, loss 0.0647043, acc 0.96875, prec 0.0669523, recall 0.883737
2017-12-10T13:35:42.236753: step 3053, loss 0.163603, acc 0.984375, prec 0.0669753, recall 0.883778
2017-12-10T13:35:42.687654: step 3054, loss 0.0882049, acc 0.96875, prec 0.0670214, recall 0.88386
2017-12-10T13:35:43.130417: step 3055, loss 0.0740777, acc 0.984375, prec 0.0670196, recall 0.88386
2017-12-10T13:35:43.575133: step 3056, loss 0.285791, acc 0.953125, prec 0.0670639, recall 0.883941
2017-12-10T13:35:44.017853: step 3057, loss 0.426363, acc 0.9375, prec 0.0670816, recall 0.883982
2017-12-10T13:35:44.464840: step 3058, loss 0.663178, acc 0.9375, prec 0.0671241, recall 0.884063
2017-12-10T13:35:44.915191: step 3059, loss 0.140094, acc 0.96875, prec 0.0672693, recall 0.884306
2017-12-10T13:35:45.385096: step 3060, loss 0.0754545, acc 0.984375, prec 0.0672676, recall 0.884306
2017-12-10T13:35:45.813997: step 3061, loss 0.603525, acc 0.984375, prec 0.0672906, recall 0.884347
2017-12-10T13:35:46.263746: step 3062, loss 0.189328, acc 0.9375, prec 0.067333, recall 0.884427
2017-12-10T13:35:46.695167: step 3063, loss 0.197213, acc 0.9375, prec 0.0673506, recall 0.884468
2017-12-10T13:35:47.150076: step 3064, loss 0.162867, acc 0.9375, prec 0.0673683, recall 0.884508
2017-12-10T13:35:47.605196: step 3065, loss 0.101278, acc 0.96875, prec 0.0674142, recall 0.884589
2017-12-10T13:35:48.070541: step 3066, loss 0.207242, acc 0.9375, prec 0.0674319, recall 0.884629
2017-12-10T13:35:48.509353: step 3067, loss 0.169925, acc 0.921875, prec 0.0674229, recall 0.884629
2017-12-10T13:35:48.956770: step 3068, loss 0.591666, acc 0.953125, prec 0.0674671, recall 0.884709
2017-12-10T13:35:49.416012: step 3069, loss 0.090997, acc 0.953125, prec 0.0674865, recall 0.884749
2017-12-10T13:35:49.854035: step 3070, loss 0.0472954, acc 0.984375, prec 0.0674847, recall 0.884749
2017-12-10T13:35:50.304859: step 3071, loss 0.118805, acc 0.953125, prec 0.0674793, recall 0.884749
2017-12-10T13:35:50.725192: step 3072, loss 0.0680294, acc 0.984375, prec 0.067527, recall 0.88483
2017-12-10T13:35:51.127816: step 3073, loss 0.274591, acc 0.9375, prec 0.0675198, recall 0.88483
2017-12-10T13:35:51.504323: step 3074, loss 0.558176, acc 0.96875, prec 0.067541, recall 0.88487
2017-12-10T13:35:51.901767: step 3075, loss 0.134526, acc 0.984375, prec 0.0675887, recall 0.88495
2017-12-10T13:35:52.296920: step 3076, loss 0.252127, acc 0.90625, prec 0.067578, recall 0.88495
2017-12-10T13:35:52.715261: step 3077, loss 0.260985, acc 0.890625, prec 0.0675902, recall 0.88499
2017-12-10T13:35:53.148359: step 3078, loss 0.254333, acc 0.9375, prec 0.0676077, recall 0.885029
2017-12-10T13:35:53.590177: step 3079, loss 0.117992, acc 0.953125, prec 0.0676518, recall 0.885109
2017-12-10T13:35:54.023066: step 3080, loss 0.28085, acc 0.90625, prec 0.067641, recall 0.885109
2017-12-10T13:35:54.468440: step 3081, loss 0.172543, acc 0.953125, prec 0.0676357, recall 0.885109
2017-12-10T13:35:54.913258: step 3082, loss 0.123055, acc 0.96875, prec 0.0676568, recall 0.885149
2017-12-10T13:35:55.374193: step 3083, loss 0.0656434, acc 0.953125, prec 0.0676761, recall 0.885189
2017-12-10T13:35:55.822198: step 3084, loss 1.0637, acc 0.953125, prec 0.0677202, recall 0.885269
2017-12-10T13:35:56.263628: step 3085, loss 0.142971, acc 0.953125, prec 0.0677148, recall 0.885269
2017-12-10T13:35:56.711851: step 3086, loss 0.164, acc 0.9375, prec 0.0677076, recall 0.885269
2017-12-10T13:35:57.153508: step 3087, loss 0.0649462, acc 0.984375, prec 0.0677058, recall 0.885269
2017-12-10T13:35:57.589997: step 3088, loss 0.242798, acc 0.921875, prec 0.0677216, recall 0.885308
2017-12-10T13:35:58.059520: step 3089, loss 0.0957086, acc 0.953125, prec 0.0677409, recall 0.885348
2017-12-10T13:35:58.494128: step 3090, loss 0.209148, acc 0.9375, prec 0.0677337, recall 0.885348
2017-12-10T13:35:58.948786: step 3091, loss 0.256415, acc 0.921875, prec 0.0677248, recall 0.885348
2017-12-10T13:35:59.397066: step 3092, loss 0.188019, acc 0.96875, prec 0.0677953, recall 0.885467
2017-12-10T13:35:59.828384: step 3093, loss 0.0926565, acc 0.96875, prec 0.0678411, recall 0.885546
2017-12-10T13:36:00.274065: step 3094, loss 1.90457, acc 0.921875, prec 0.0678586, recall 0.88528
2017-12-10T13:36:00.726939: step 3095, loss 0.325497, acc 0.890625, prec 0.0678707, recall 0.88532
2017-12-10T13:36:01.158810: step 3096, loss 0.324131, acc 0.90625, prec 0.0678599, recall 0.88532
2017-12-10T13:36:01.601306: step 3097, loss 0.0521684, acc 0.984375, prec 0.0678581, recall 0.88532
2017-12-10T13:36:02.041798: step 3098, loss 0.194273, acc 0.953125, prec 0.0679021, recall 0.885399
2017-12-10T13:36:02.481587: step 3099, loss 0.0962749, acc 0.96875, prec 0.0679231, recall 0.885438
2017-12-10T13:36:02.919517: step 3100, loss 0.495322, acc 0.890625, prec 0.0679105, recall 0.885438
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3100

2017-12-10T13:36:04.861468: step 3101, loss 0.254397, acc 0.953125, prec 0.0679298, recall 0.885478
2017-12-10T13:36:05.308081: step 3102, loss 0.171725, acc 0.9375, prec 0.0679226, recall 0.885478
2017-12-10T13:36:05.767643: step 3103, loss 0.129644, acc 0.9375, prec 0.0679401, recall 0.885517
2017-12-10T13:36:06.207998: step 3104, loss 0.203628, acc 0.953125, prec 0.067984, recall 0.885596
2017-12-10T13:36:06.648281: step 3105, loss 0.194725, acc 0.921875, prec 0.0679997, recall 0.885636
2017-12-10T13:36:07.093258: step 3106, loss 0.150909, acc 0.921875, prec 0.0680153, recall 0.885675
2017-12-10T13:36:07.550279: step 3107, loss 0.299944, acc 0.921875, prec 0.0680063, recall 0.885675
2017-12-10T13:36:08.001433: step 3108, loss 0.339863, acc 0.953125, prec 0.0680009, recall 0.885675
2017-12-10T13:36:08.462590: step 3109, loss 0.176466, acc 0.9375, prec 0.0680184, recall 0.885714
2017-12-10T13:36:08.909519: step 3110, loss 0.187172, acc 0.90625, prec 0.0680322, recall 0.885754
2017-12-10T13:36:09.349341: step 3111, loss 0.215109, acc 0.921875, prec 0.0680479, recall 0.885793
2017-12-10T13:36:09.784013: step 3112, loss 0.618299, acc 0.9375, prec 0.0680899, recall 0.885871
2017-12-10T13:36:10.240255: step 3113, loss 0.206688, acc 1, prec 0.0681146, recall 0.885911
2017-12-10T13:36:10.692154: step 3114, loss 0.114232, acc 0.953125, prec 0.0681092, recall 0.885911
2017-12-10T13:36:11.149189: step 3115, loss 0.185089, acc 0.96875, prec 0.0681302, recall 0.88595
2017-12-10T13:36:11.599672: step 3116, loss 0.190387, acc 0.96875, prec 0.0681512, recall 0.885989
2017-12-10T13:36:12.051028: step 3117, loss 0.131924, acc 0.9375, prec 0.068144, recall 0.885989
2017-12-10T13:36:12.495129: step 3118, loss 0.121591, acc 0.953125, prec 0.0681386, recall 0.885989
2017-12-10T13:36:12.942983: step 3119, loss 0.100845, acc 0.96875, prec 0.0681596, recall 0.886028
2017-12-10T13:36:13.405790: step 3120, loss 0.197565, acc 0.9375, prec 0.0681524, recall 0.886028
2017-12-10T13:36:13.863149: step 3121, loss 2.5525, acc 0.96875, prec 0.0681752, recall 0.885763
2017-12-10T13:36:14.317000: step 3122, loss 0.569562, acc 0.921875, prec 0.06824, recall 0.885881
2017-12-10T13:36:14.773935: step 3123, loss 0.222522, acc 0.9375, prec 0.068282, recall 0.885959
2017-12-10T13:36:15.231875: step 3124, loss 0.387737, acc 0.890625, prec 0.0682694, recall 0.885959
2017-12-10T13:36:15.678048: step 3125, loss 0.101156, acc 0.921875, prec 0.068285, recall 0.885998
2017-12-10T13:36:16.131458: step 3126, loss 0.308417, acc 0.921875, prec 0.0683005, recall 0.886037
2017-12-10T13:36:16.584675: step 3127, loss 0.266364, acc 0.921875, prec 0.0683407, recall 0.886115
2017-12-10T13:36:17.037061: step 3128, loss 0.298959, acc 0.921875, prec 0.0683317, recall 0.886115
2017-12-10T13:36:17.490029: step 3129, loss 1.19431, acc 0.90625, prec 0.0683454, recall 0.886154
2017-12-10T13:36:17.939510: step 3130, loss 0.387652, acc 0.90625, prec 0.0683592, recall 0.886193
2017-12-10T13:36:18.403104: step 3131, loss 0.268467, acc 0.9375, prec 0.0684011, recall 0.88627
2017-12-10T13:36:18.867576: step 3132, loss 0.376808, acc 0.859375, prec 0.0683849, recall 0.88627
2017-12-10T13:36:19.311312: step 3133, loss 0.205617, acc 0.90625, prec 0.068374, recall 0.88627
2017-12-10T13:36:19.760171: step 3134, loss 0.320481, acc 0.921875, prec 0.0684141, recall 0.886348
2017-12-10T13:36:20.201925: step 3135, loss 0.347956, acc 0.859375, prec 0.0683979, recall 0.886348
2017-12-10T13:36:20.643286: step 3136, loss 0.338666, acc 0.875, prec 0.068408, recall 0.886387
2017-12-10T13:36:21.074275: step 3137, loss 0.610288, acc 0.8125, prec 0.0683864, recall 0.886387
2017-12-10T13:36:21.522712: step 3138, loss 0.742594, acc 0.890625, prec 0.0684229, recall 0.886464
2017-12-10T13:36:21.972732: step 3139, loss 0.265182, acc 0.890625, prec 0.0684102, recall 0.886464
2017-12-10T13:36:22.424489: step 3140, loss 0.367653, acc 0.890625, prec 0.0684222, recall 0.886503
2017-12-10T13:36:22.864734: step 3141, loss 0.45604, acc 0.875, prec 0.0684078, recall 0.886503
2017-12-10T13:36:23.314077: step 3142, loss 1.01256, acc 0.90625, prec 0.0684215, recall 0.886542
2017-12-10T13:36:23.758782: step 3143, loss 0.345908, acc 0.921875, prec 0.0684125, recall 0.886542
2017-12-10T13:36:24.203273: step 3144, loss 0.313899, acc 0.921875, prec 0.0684035, recall 0.886542
2017-12-10T13:36:24.653289: step 3145, loss 0.261229, acc 0.890625, prec 0.0683909, recall 0.886542
2017-12-10T13:36:25.098307: step 3146, loss 0.243454, acc 0.921875, prec 0.0684064, recall 0.88658
2017-12-10T13:36:25.545916: step 3147, loss 0.349709, acc 0.921875, prec 0.0684219, recall 0.886619
2017-12-10T13:36:25.993401: step 3148, loss 0.40546, acc 0.875, prec 0.0684075, recall 0.886619
2017-12-10T13:36:26.446767: step 3149, loss 0.209973, acc 0.90625, prec 0.0683967, recall 0.886619
2017-12-10T13:36:26.914405: step 3150, loss 0.186704, acc 0.921875, prec 0.0683877, recall 0.886619
2017-12-10T13:36:27.368489: step 3151, loss 0.187336, acc 0.96875, prec 0.0684331, recall 0.886696
2017-12-10T13:36:27.810495: step 3152, loss 0.220071, acc 0.921875, prec 0.0684241, recall 0.886696
2017-12-10T13:36:28.255517: step 3153, loss 0.177306, acc 0.96875, prec 0.0684205, recall 0.886696
2017-12-10T13:36:28.705953: step 3154, loss 0.595618, acc 0.921875, prec 0.0684604, recall 0.886773
2017-12-10T13:36:29.141320: step 3155, loss 0.777676, acc 0.890625, prec 0.0684967, recall 0.88685
2017-12-10T13:36:29.602715: step 3156, loss 0.296263, acc 0.9375, prec 0.068514, recall 0.886889
2017-12-10T13:36:30.058268: step 3157, loss 0.278659, acc 0.953125, prec 0.068533, recall 0.886927
2017-12-10T13:36:30.512946: step 3158, loss 0.546403, acc 0.9375, prec 0.0685747, recall 0.887004
2017-12-10T13:36:30.958866: step 3159, loss 0.0695129, acc 0.96875, prec 0.0685711, recall 0.887004
2017-12-10T13:36:31.409745: step 3160, loss 0.404885, acc 0.984375, prec 0.0685938, recall 0.887042
2017-12-10T13:36:31.851083: step 3161, loss 0.401684, acc 0.890625, prec 0.0686056, recall 0.88708
2017-12-10T13:36:32.306382: step 3162, loss 0.113559, acc 0.953125, prec 0.0686002, recall 0.88708
2017-12-10T13:36:32.741673: step 3163, loss 0.140761, acc 0.953125, prec 0.0685948, recall 0.88708
2017-12-10T13:36:33.186884: step 3164, loss 0.673968, acc 0.90625, prec 0.0686084, recall 0.887119
2017-12-10T13:36:33.624713: step 3165, loss 0.234512, acc 0.921875, prec 0.0686483, recall 0.887195
2017-12-10T13:36:34.075793: step 3166, loss 0.133997, acc 0.953125, prec 0.0686673, recall 0.887233
2017-12-10T13:36:34.514908: step 3167, loss 0.245096, acc 0.9375, prec 0.0687089, recall 0.88731
2017-12-10T13:36:34.982567: step 3168, loss 0.397555, acc 0.9375, prec 0.0687261, recall 0.887348
2017-12-10T13:36:35.415193: step 3169, loss 0.146172, acc 0.9375, prec 0.0687189, recall 0.887348
2017-12-10T13:36:35.868317: step 3170, loss 0.0541325, acc 0.96875, prec 0.0687641, recall 0.887424
2017-12-10T13:36:36.314817: step 3171, loss 0.0959231, acc 0.984375, prec 0.0687867, recall 0.887462
2017-12-10T13:36:36.758045: step 3172, loss 0.115763, acc 0.9375, prec 0.0687795, recall 0.887462
2017-12-10T13:36:37.209840: step 3173, loss 0.194339, acc 0.953125, prec 0.0687985, recall 0.8875
2017-12-10T13:36:37.660195: step 3174, loss 0.157345, acc 0.9375, prec 0.0687912, recall 0.8875
2017-12-10T13:36:38.101175: step 3175, loss 0.376237, acc 0.890625, prec 0.068803, recall 0.887538
2017-12-10T13:36:38.552785: step 3176, loss 0.0364365, acc 0.984375, prec 0.0688256, recall 0.887576
2017-12-10T13:36:38.989036: step 3177, loss 0.0558791, acc 0.984375, prec 0.0688482, recall 0.887614
2017-12-10T13:36:39.434461: step 3178, loss 0.127766, acc 0.9375, prec 0.068841, recall 0.887614
2017-12-10T13:36:39.875474: step 3179, loss 0.189999, acc 0.953125, prec 0.0688356, recall 0.887614
2017-12-10T13:36:40.338484: step 3180, loss 0.0499101, acc 0.96875, prec 0.068832, recall 0.887614
2017-12-10T13:36:40.774435: step 3181, loss 0.078444, acc 0.96875, prec 0.0688527, recall 0.887652
2017-12-10T13:36:41.214083: step 3182, loss 0.112283, acc 0.984375, prec 0.0688509, recall 0.887652
2017-12-10T13:36:41.658868: step 3183, loss 0.0555868, acc 0.96875, prec 0.0688473, recall 0.887652
2017-12-10T13:36:42.110721: step 3184, loss 0.0945037, acc 0.984375, prec 0.0688699, recall 0.88769
2017-12-10T13:36:42.544232: step 3185, loss 0.0499053, acc 0.984375, prec 0.0688924, recall 0.887728
2017-12-10T13:36:43.002604: step 3186, loss 0.215243, acc 0.9375, prec 0.0689583, recall 0.887841
2017-12-10T13:36:43.450664: step 3187, loss 0.0535376, acc 1, prec 0.0689827, recall 0.887879
2017-12-10T13:36:43.894287: step 3188, loss 0.164646, acc 0.953125, prec 0.0690016, recall 0.887917
2017-12-10T13:36:44.344546: step 3189, loss 0.258749, acc 0.96875, prec 0.0690467, recall 0.887992
2017-12-10T13:36:44.799484: step 3190, loss 0.0403699, acc 0.96875, prec 0.0690431, recall 0.887992
2017-12-10T13:36:45.256063: step 3191, loss 0.123842, acc 0.984375, prec 0.0690413, recall 0.887992
2017-12-10T13:36:45.725523: step 3192, loss 0.198685, acc 1, prec 0.0691143, recall 0.888105
2017-12-10T13:36:46.179574: step 3193, loss 0.0610292, acc 0.984375, prec 0.0691125, recall 0.888105
2017-12-10T13:36:46.622294: step 3194, loss 0.0821112, acc 0.953125, prec 0.0691071, recall 0.888105
2017-12-10T13:36:47.070197: step 3195, loss 3.47052, acc 0.953125, prec 0.0691035, recall 0.887807
2017-12-10T13:36:47.524151: step 3196, loss 0.110502, acc 0.984375, prec 0.069126, recall 0.887844
2017-12-10T13:36:47.977690: step 3197, loss 0.489846, acc 1, prec 0.0691503, recall 0.887882
2017-12-10T13:36:48.437838: step 3198, loss 0.477542, acc 0.9375, prec 0.0691674, recall 0.887919
2017-12-10T13:36:48.893936: step 3199, loss 0.134865, acc 0.96875, prec 0.0692125, recall 0.887995
2017-12-10T13:36:49.346354: step 3200, loss 0.241614, acc 0.96875, prec 0.0692575, recall 0.88807
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3200

2017-12-10T13:36:51.267243: step 3201, loss 0.359879, acc 0.9375, prec 0.0692746, recall 0.888107
2017-12-10T13:36:51.718706: step 3202, loss 0.302133, acc 0.9375, prec 0.0692673, recall 0.888107
2017-12-10T13:36:52.161149: step 3203, loss 0.274713, acc 0.921875, prec 0.0693069, recall 0.888182
2017-12-10T13:36:52.607253: step 3204, loss 0.25688, acc 0.859375, prec 0.0693149, recall 0.88822
2017-12-10T13:36:53.059949: step 3205, loss 0.40514, acc 0.875, prec 0.0693248, recall 0.888257
2017-12-10T13:36:53.522994: step 3206, loss 0.715108, acc 0.859375, prec 0.0693814, recall 0.888369
2017-12-10T13:36:53.962120: step 3207, loss 0.229074, acc 0.890625, prec 0.0694173, recall 0.888444
2017-12-10T13:36:54.419126: step 3208, loss 0.17061, acc 0.921875, prec 0.0694325, recall 0.888481
2017-12-10T13:36:54.879818: step 3209, loss 0.552348, acc 0.875, prec 0.0694423, recall 0.888518
2017-12-10T13:36:55.323185: step 3210, loss 0.166243, acc 0.953125, prec 0.0694611, recall 0.888555
2017-12-10T13:36:55.785367: step 3211, loss 0.346667, acc 0.875, prec 0.0694466, recall 0.888555
2017-12-10T13:36:56.247320: step 3212, loss 0.334274, acc 0.90625, prec 0.0694358, recall 0.888555
2017-12-10T13:36:56.689963: step 3213, loss 0.265371, acc 0.921875, prec 0.0694752, recall 0.88863
2017-12-10T13:36:57.142522: step 3214, loss 0.239968, acc 0.890625, prec 0.0694868, recall 0.888667
2017-12-10T13:36:57.591147: step 3215, loss 0.304042, acc 0.9375, prec 0.0695038, recall 0.888704
2017-12-10T13:36:58.023787: step 3216, loss 0.217206, acc 0.9375, prec 0.0695208, recall 0.888741
2017-12-10T13:36:58.482803: step 3217, loss 0.379217, acc 0.90625, prec 0.0695099, recall 0.888741
2017-12-10T13:36:58.937209: step 3218, loss 0.354507, acc 0.90625, prec 0.0695475, recall 0.888815
2017-12-10T13:36:59.381629: step 3219, loss 0.128916, acc 0.953125, prec 0.0695906, recall 0.888889
2017-12-10T13:36:59.819491: step 3220, loss 0.145396, acc 0.9375, prec 0.0695833, recall 0.888889
2017-12-10T13:37:00.269622: step 3221, loss 0.0299433, acc 1, prec 0.0695833, recall 0.888889
2017-12-10T13:37:00.709919: step 3222, loss 0.0994947, acc 0.953125, prec 0.0696021, recall 0.888926
2017-12-10T13:37:01.166333: step 3223, loss 0.0569462, acc 0.984375, prec 0.0696003, recall 0.888926
2017-12-10T13:37:01.625101: step 3224, loss 0.241618, acc 0.9375, prec 0.0695931, recall 0.888926
2017-12-10T13:37:02.077747: step 3225, loss 0.110446, acc 0.96875, prec 0.0695894, recall 0.888926
2017-12-10T13:37:02.524696: step 3226, loss 0.993967, acc 0.9375, prec 0.0696064, recall 0.888963
2017-12-10T13:37:02.981261: step 3227, loss 0.702925, acc 0.9375, prec 0.0696718, recall 0.889073
2017-12-10T13:37:03.425135: step 3228, loss 0.165107, acc 0.921875, prec 0.0697112, recall 0.889147
2017-12-10T13:37:03.898931: step 3229, loss 0.0312864, acc 0.984375, prec 0.0697093, recall 0.889147
2017-12-10T13:37:04.341471: step 3230, loss 0.129503, acc 0.96875, prec 0.0697057, recall 0.889147
2017-12-10T13:37:04.788027: step 3231, loss 0.105766, acc 0.96875, prec 0.0697263, recall 0.889184
2017-12-10T13:37:05.233523: step 3232, loss 0.263586, acc 0.96875, prec 0.0697227, recall 0.889184
2017-12-10T13:37:05.674231: step 3233, loss 0.0658361, acc 0.96875, prec 0.069719, recall 0.889184
2017-12-10T13:37:06.127467: step 3234, loss 0.0385741, acc 1, prec 0.0697432, recall 0.889221
2017-12-10T13:37:06.588769: step 3235, loss 0.613514, acc 0.84375, prec 0.0697493, recall 0.889257
2017-12-10T13:37:07.041697: step 3236, loss 0.250841, acc 0.953125, prec 0.0698164, recall 0.889367
2017-12-10T13:37:07.482723: step 3237, loss 0.0966081, acc 0.96875, prec 0.069837, recall 0.889404
2017-12-10T13:37:07.931106: step 3238, loss 0.0167394, acc 1, prec 0.069837, recall 0.889404
2017-12-10T13:37:08.370378: step 3239, loss 0.11127, acc 0.9375, prec 0.0698539, recall 0.889441
2017-12-10T13:37:08.803313: step 3240, loss 0.114654, acc 0.9375, prec 0.0698708, recall 0.889477
2017-12-10T13:37:09.263428: step 3241, loss 0.127372, acc 0.953125, prec 0.0698654, recall 0.889477
2017-12-10T13:37:09.715722: step 3242, loss 0.103874, acc 0.984375, prec 0.0698636, recall 0.889477
2017-12-10T13:37:10.163678: step 3243, loss 0.0540467, acc 0.984375, prec 0.0698617, recall 0.889477
2017-12-10T13:37:10.602819: step 3244, loss 0.165821, acc 0.953125, prec 0.0698563, recall 0.889477
2017-12-10T13:37:11.046665: step 3245, loss 0.246053, acc 0.96875, prec 0.0699252, recall 0.889587
2017-12-10T13:37:11.489999: step 3246, loss 0.023448, acc 1, prec 0.0699252, recall 0.889587
2017-12-10T13:37:11.944280: step 3247, loss 0.0758935, acc 0.96875, prec 0.0699457, recall 0.889623
2017-12-10T13:37:12.407761: step 3248, loss 0.165798, acc 0.953125, prec 0.0699644, recall 0.88966
2017-12-10T13:37:12.849097: step 3249, loss 0.088318, acc 0.984375, prec 0.0700109, recall 0.889733
2017-12-10T13:37:13.307191: step 3250, loss 0.0596139, acc 0.96875, prec 0.0700073, recall 0.889733
2017-12-10T13:37:13.764228: step 3251, loss 0.0599351, acc 0.984375, prec 0.0700055, recall 0.889733
2017-12-10T13:37:14.221889: step 3252, loss 0.0491532, acc 0.984375, prec 0.0700278, recall 0.889769
2017-12-10T13:37:14.667540: step 3253, loss 0.186448, acc 0.9375, prec 0.070093, recall 0.889878
2017-12-10T13:37:15.124869: step 3254, loss 2.40858, acc 0.953125, prec 0.0701376, recall 0.889657
2017-12-10T13:37:15.581532: step 3255, loss 0.038008, acc 0.984375, prec 0.0701358, recall 0.889657
2017-12-10T13:37:16.037069: step 3256, loss 0.0540005, acc 0.984375, prec 0.0701581, recall 0.889694
2017-12-10T13:37:16.482960: step 3257, loss 0.0131794, acc 1, prec 0.0701823, recall 0.88973
2017-12-10T13:37:16.946214: step 3258, loss 0.153198, acc 0.953125, prec 0.0701768, recall 0.88973
2017-12-10T13:37:17.395232: step 3259, loss 0.092919, acc 0.96875, prec 0.0701732, recall 0.88973
2017-12-10T13:37:17.853173: step 3260, loss 0.45666, acc 0.890625, prec 0.0702328, recall 0.889839
2017-12-10T13:37:18.292485: step 3261, loss 0.282401, acc 0.921875, prec 0.0702478, recall 0.889875
2017-12-10T13:37:18.749593: step 3262, loss 0.173197, acc 0.9375, prec 0.0702647, recall 0.889911
2017-12-10T13:37:19.192329: step 3263, loss 0.225809, acc 0.953125, prec 0.0703316, recall 0.89002
2017-12-10T13:37:19.654541: step 3264, loss 0.525436, acc 0.84375, prec 0.0703374, recall 0.890056
2017-12-10T13:37:20.098306: step 3265, loss 0.209463, acc 0.921875, prec 0.0703524, recall 0.890092
2017-12-10T13:37:20.537010: step 3266, loss 0.0386889, acc 0.984375, prec 0.0703506, recall 0.890092
2017-12-10T13:37:20.980188: step 3267, loss 0.102563, acc 0.96875, prec 0.0703951, recall 0.890164
2017-12-10T13:37:21.428724: step 3268, loss 0.268597, acc 0.9375, prec 0.0704119, recall 0.8902
2017-12-10T13:37:21.877135: step 3269, loss 0.362623, acc 0.90625, prec 0.070401, recall 0.8902
2017-12-10T13:37:22.325057: step 3270, loss 0.333015, acc 0.953125, prec 0.0704437, recall 0.890272
2017-12-10T13:37:22.776083: step 3271, loss 0.120585, acc 0.9375, prec 0.0704364, recall 0.890272
2017-12-10T13:37:23.236074: step 3272, loss 0.318621, acc 0.90625, prec 0.0704736, recall 0.890344
2017-12-10T13:37:23.679345: step 3273, loss 0.0534999, acc 0.984375, prec 0.0704718, recall 0.890344
2017-12-10T13:37:24.132711: step 3274, loss 0.124031, acc 0.953125, prec 0.0704663, recall 0.890344
2017-12-10T13:37:24.576959: step 3275, loss 0.155933, acc 0.984375, prec 0.0705367, recall 0.890451
2017-12-10T13:37:25.030644: step 3276, loss 0.154162, acc 0.953125, prec 0.0706035, recall 0.890559
2017-12-10T13:37:25.481329: step 3277, loss 0.238387, acc 0.90625, prec 0.0706166, recall 0.890594
2017-12-10T13:37:25.926851: step 3278, loss 0.128279, acc 0.984375, prec 0.0706388, recall 0.89063
2017-12-10T13:37:26.370607: step 3279, loss 0.221452, acc 0.9375, prec 0.0706556, recall 0.890666
2017-12-10T13:37:26.833194: step 3280, loss 0.272248, acc 0.953125, prec 0.0706741, recall 0.890701
2017-12-10T13:37:27.279230: step 3281, loss 0.215592, acc 0.984375, prec 0.0707204, recall 0.890773
2017-12-10T13:37:27.752245: step 3282, loss 0.359743, acc 0.921875, prec 0.0707113, recall 0.890773
2017-12-10T13:37:28.200875: step 3283, loss 0.301923, acc 0.953125, prec 0.0707539, recall 0.890844
2017-12-10T13:37:28.642915: step 3284, loss 0.0613357, acc 0.984375, prec 0.0707761, recall 0.890879
2017-12-10T13:37:29.080288: step 3285, loss 0.0779805, acc 0.96875, prec 0.0707724, recall 0.890879
2017-12-10T13:37:29.526495: step 3286, loss 0.0155836, acc 1, prec 0.0708205, recall 0.890951
2017-12-10T13:37:29.969476: step 3287, loss 0.121194, acc 0.953125, prec 0.0708631, recall 0.891021
2017-12-10T13:37:30.431004: step 3288, loss 0.159763, acc 1, prec 0.0709112, recall 0.891092
2017-12-10T13:37:30.888739: step 3289, loss 1.01995, acc 0.96875, prec 0.0709556, recall 0.891163
2017-12-10T13:37:31.336859: step 3290, loss 0.0767086, acc 0.984375, prec 0.0709537, recall 0.891163
2017-12-10T13:37:31.778957: step 3291, loss 0.127491, acc 0.96875, prec 0.0709741, recall 0.891198
2017-12-10T13:37:32.241174: step 3292, loss 0.125888, acc 0.984375, prec 0.0710203, recall 0.891269
2017-12-10T13:37:32.683533: step 3293, loss 0.194171, acc 0.953125, prec 0.0710388, recall 0.891304
2017-12-10T13:37:33.134463: step 3294, loss 0.678221, acc 0.921875, prec 0.0710777, recall 0.891375
2017-12-10T13:37:33.577944: step 3295, loss 0.128829, acc 0.96875, prec 0.071098, recall 0.89141
2017-12-10T13:37:34.028783: step 3296, loss 0.119167, acc 0.96875, prec 0.0710943, recall 0.89141
2017-12-10T13:37:34.480997: step 3297, loss 0.281767, acc 0.96875, prec 0.0711147, recall 0.891445
2017-12-10T13:37:34.935523: step 3298, loss 0.0890794, acc 0.953125, prec 0.0711812, recall 0.891551
2017-12-10T13:37:35.378141: step 3299, loss 0.0394557, acc 0.984375, prec 0.0711793, recall 0.891551
2017-12-10T13:37:35.840749: step 3300, loss 0.280354, acc 0.953125, prec 0.0711978, recall 0.891586
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3300

2017-12-10T13:37:37.755334: step 3301, loss 0.178943, acc 0.953125, prec 0.0711923, recall 0.891586
2017-12-10T13:37:38.198845: step 3302, loss 0.0422318, acc 0.984375, prec 0.0711905, recall 0.891586
2017-12-10T13:37:38.632217: step 3303, loss 0.100772, acc 0.96875, prec 0.0711868, recall 0.891586
2017-12-10T13:37:39.080833: step 3304, loss 0.213487, acc 0.90625, prec 0.0712237, recall 0.891656
2017-12-10T13:37:39.528000: step 3305, loss 0.203341, acc 0.96875, prec 0.0712441, recall 0.891691
2017-12-10T13:37:39.982267: step 3306, loss 0.0943584, acc 0.96875, prec 0.0712404, recall 0.891691
2017-12-10T13:37:40.417507: step 3307, loss 0.392663, acc 0.96875, prec 0.0712847, recall 0.891761
2017-12-10T13:37:40.883060: step 3308, loss 0.36614, acc 0.921875, prec 0.0712755, recall 0.891761
2017-12-10T13:37:41.322182: step 3309, loss 0.408248, acc 0.953125, prec 0.0712939, recall 0.891796
2017-12-10T13:37:41.770248: step 3310, loss 0.154238, acc 0.921875, prec 0.0713087, recall 0.891831
2017-12-10T13:37:42.213947: step 3311, loss 0.308813, acc 0.9375, prec 0.0713013, recall 0.891831
2017-12-10T13:37:42.651323: step 3312, loss 0.187325, acc 0.96875, prec 0.0712977, recall 0.891831
2017-12-10T13:37:43.105006: step 3313, loss 0.20148, acc 0.9375, prec 0.0712903, recall 0.891831
2017-12-10T13:37:43.561341: step 3314, loss 1.3282, acc 0.9375, prec 0.0713309, recall 0.891901
2017-12-10T13:37:43.999516: step 3315, loss 0.201483, acc 0.953125, prec 0.0713493, recall 0.891935
2017-12-10T13:37:44.442418: step 3316, loss 0.193705, acc 0.9375, prec 0.071342, recall 0.891935
2017-12-10T13:37:44.896854: step 3317, loss 0.23205, acc 0.953125, prec 0.0713843, recall 0.892005
2017-12-10T13:37:45.346713: step 3318, loss 0.110883, acc 0.953125, prec 0.0713788, recall 0.892005
2017-12-10T13:37:45.798774: step 3319, loss 0.0613695, acc 0.984375, prec 0.0714009, recall 0.89204
2017-12-10T13:37:46.230098: step 3320, loss 0.169005, acc 0.9375, prec 0.0714175, recall 0.892075
2017-12-10T13:37:46.677324: step 3321, loss 0.106874, acc 0.953125, prec 0.0714599, recall 0.892144
2017-12-10T13:37:47.131879: step 3322, loss 0.28822, acc 0.921875, prec 0.0714986, recall 0.892214
2017-12-10T13:37:47.579480: step 3323, loss 0.211841, acc 0.953125, prec 0.071493, recall 0.892214
2017-12-10T13:37:48.031541: step 3324, loss 0.198594, acc 0.9375, prec 0.0715096, recall 0.892248
2017-12-10T13:37:48.480762: step 3325, loss 0.169121, acc 0.9375, prec 0.0715022, recall 0.892248
2017-12-10T13:37:48.927544: step 3326, loss 0.0830181, acc 0.96875, prec 0.0715225, recall 0.892283
2017-12-10T13:37:49.378372: step 3327, loss 0.269386, acc 0.890625, prec 0.0715096, recall 0.892283
2017-12-10T13:37:49.812005: step 3328, loss 0.0482416, acc 0.96875, prec 0.0715059, recall 0.892283
2017-12-10T13:37:50.252955: step 3329, loss 0.0855462, acc 0.953125, prec 0.0715003, recall 0.892283
2017-12-10T13:37:50.710470: step 3330, loss 0.0502429, acc 0.96875, prec 0.0714967, recall 0.892283
2017-12-10T13:37:51.150329: step 3331, loss 0.250454, acc 0.984375, prec 0.0715427, recall 0.892352
2017-12-10T13:37:51.588627: step 3332, loss 0.0312958, acc 0.984375, prec 0.0715408, recall 0.892352
2017-12-10T13:37:52.032420: step 3333, loss 0.185258, acc 0.953125, prec 0.0715353, recall 0.892352
2017-12-10T13:37:52.493438: step 3334, loss 0.585049, acc 0.90625, prec 0.0715481, recall 0.892387
2017-12-10T13:37:52.938092: step 3335, loss 0.0661111, acc 0.9375, prec 0.0715408, recall 0.892387
2017-12-10T13:37:53.384769: step 3336, loss 0.527263, acc 0.9375, prec 0.0715573, recall 0.892421
2017-12-10T13:37:53.841258: step 3337, loss 0.0340333, acc 1, prec 0.0715573, recall 0.892421
2017-12-10T13:37:54.290249: step 3338, loss 0.620819, acc 0.953125, prec 0.0716235, recall 0.892525
2017-12-10T13:37:54.737522: step 3339, loss 0.222734, acc 0.96875, prec 0.0716437, recall 0.892559
2017-12-10T13:37:55.169614: step 3340, loss 0.112133, acc 0.96875, prec 0.0716639, recall 0.892594
2017-12-10T13:37:55.615802: step 3341, loss 0.0688241, acc 0.984375, prec 0.071686, recall 0.892628
2017-12-10T13:37:56.057281: step 3342, loss 0.155702, acc 0.953125, prec 0.0716804, recall 0.892628
2017-12-10T13:37:56.497381: step 3343, loss 0.263015, acc 0.90625, prec 0.0716933, recall 0.892663
2017-12-10T13:37:56.955108: step 3344, loss 0.317909, acc 0.96875, prec 0.0717373, recall 0.892731
2017-12-10T13:37:57.409828: step 3345, loss 0.277871, acc 0.96875, prec 0.0717575, recall 0.892766
2017-12-10T13:37:57.860381: step 3346, loss 0.0612348, acc 0.96875, prec 0.0717777, recall 0.8928
2017-12-10T13:37:58.298858: step 3347, loss 0.0542746, acc 0.96875, prec 0.071774, recall 0.8928
2017-12-10T13:37:58.734392: step 3348, loss 1.04219, acc 1, prec 0.0718218, recall 0.892869
2017-12-10T13:37:59.173658: step 3349, loss 0.107723, acc 0.953125, prec 0.0718162, recall 0.892869
2017-12-10T13:37:59.621282: step 3350, loss 0.0350529, acc 0.984375, prec 0.0718144, recall 0.892869
2017-12-10T13:38:00.077533: step 3351, loss 0.15884, acc 0.921875, prec 0.071829, recall 0.892903
2017-12-10T13:38:00.526862: step 3352, loss 0.220798, acc 0.890625, prec 0.0718161, recall 0.892903
2017-12-10T13:38:00.966882: step 3353, loss 0.549656, acc 0.90625, prec 0.0718289, recall 0.892937
2017-12-10T13:38:01.410905: step 3354, loss 0.211331, acc 0.90625, prec 0.0718417, recall 0.892971
2017-12-10T13:38:01.866792: step 3355, loss 0.249768, acc 0.90625, prec 0.0718783, recall 0.89304
2017-12-10T13:38:02.320423: step 3356, loss 0.147586, acc 0.9375, prec 0.0718709, recall 0.89304
2017-12-10T13:38:02.755339: step 3357, loss 0.393398, acc 0.921875, prec 0.0718617, recall 0.89304
2017-12-10T13:38:03.207798: step 3358, loss 0.321006, acc 0.921875, prec 0.0718763, recall 0.893074
2017-12-10T13:38:03.662099: step 3359, loss 0.172613, acc 0.9375, prec 0.0719166, recall 0.893142
2017-12-10T13:38:04.104872: step 3360, loss 0.33869, acc 0.875, prec 0.0719018, recall 0.893142
2017-12-10T13:38:04.558491: step 3361, loss 0.285882, acc 0.921875, prec 0.0718926, recall 0.893142
2017-12-10T13:38:04.999690: step 3362, loss 0.128634, acc 0.9375, prec 0.0718852, recall 0.893142
2017-12-10T13:38:05.447274: step 3363, loss 0.225327, acc 0.9375, prec 0.0718778, recall 0.893142
2017-12-10T13:38:05.883385: step 3364, loss 0.382435, acc 0.90625, prec 0.0718667, recall 0.893142
2017-12-10T13:38:06.320221: step 3365, loss 0.0813742, acc 0.953125, prec 0.0719088, recall 0.89321
2017-12-10T13:38:06.778007: step 3366, loss 0.275445, acc 0.921875, prec 0.0718996, recall 0.89321
2017-12-10T13:38:07.224069: step 3367, loss 0.165485, acc 0.9375, prec 0.0718922, recall 0.89321
2017-12-10T13:38:07.663735: step 3368, loss 0.160212, acc 0.9375, prec 0.0719087, recall 0.893244
2017-12-10T13:38:08.106102: step 3369, loss 0.0499919, acc 0.953125, prec 0.0719031, recall 0.893244
2017-12-10T13:38:08.540955: step 3370, loss 1.56067, acc 0.96875, prec 0.0719013, recall 0.89296
2017-12-10T13:38:08.985226: step 3371, loss 0.191067, acc 0.96875, prec 0.0719452, recall 0.893028
2017-12-10T13:38:09.429913: step 3372, loss 0.178111, acc 0.921875, prec 0.071936, recall 0.893028
2017-12-10T13:38:09.883467: step 3373, loss 0.23419, acc 0.953125, prec 0.0720018, recall 0.89313
2017-12-10T13:38:10.329553: step 3374, loss 0.249841, acc 0.984375, prec 0.0720476, recall 0.893198
2017-12-10T13:38:10.770636: step 3375, loss 0.377421, acc 0.984375, prec 0.0720695, recall 0.893232
2017-12-10T13:38:11.225442: step 3376, loss 0.0572862, acc 0.96875, prec 0.0720896, recall 0.893266
2017-12-10T13:38:11.662871: step 3377, loss 0.10004, acc 0.9375, prec 0.0720822, recall 0.893266
2017-12-10T13:38:12.109930: step 3378, loss 0.323963, acc 0.90625, prec 0.0720711, recall 0.893266
2017-12-10T13:38:12.564207: step 3379, loss 0.199423, acc 0.921875, prec 0.0720619, recall 0.893266
2017-12-10T13:38:13.008804: step 3380, loss 0.195808, acc 0.9375, prec 0.0720545, recall 0.893266
2017-12-10T13:38:13.451958: step 3381, loss 0.172897, acc 0.90625, prec 0.0721148, recall 0.893367
2017-12-10T13:38:13.911208: step 3382, loss 0.237949, acc 0.890625, prec 0.0721018, recall 0.893367
2017-12-10T13:38:14.370693: step 3383, loss 2.06979, acc 0.921875, prec 0.0721182, recall 0.893118
2017-12-10T13:38:14.834470: step 3384, loss 0.0757238, acc 0.953125, prec 0.0721127, recall 0.893118
2017-12-10T13:38:15.292274: step 3385, loss 0.368217, acc 0.890625, prec 0.072171, recall 0.893219
2017-12-10T13:38:15.732476: step 3386, loss 0.351162, acc 0.921875, prec 0.0721855, recall 0.893253
2017-12-10T13:38:16.184409: step 3387, loss 0.360979, acc 0.828125, prec 0.0721652, recall 0.893253
2017-12-10T13:38:16.629614: step 3388, loss 0.437784, acc 0.875, prec 0.0721504, recall 0.893253
2017-12-10T13:38:17.063967: step 3389, loss 0.212443, acc 0.90625, prec 0.0721868, recall 0.893321
2017-12-10T13:38:17.510265: step 3390, loss 0.287043, acc 0.890625, prec 0.0721976, recall 0.893354
2017-12-10T13:38:17.960193: step 3391, loss 0.174004, acc 0.890625, prec 0.0721847, recall 0.893354
2017-12-10T13:38:18.422784: step 3392, loss 0.156773, acc 0.9375, prec 0.0722248, recall 0.893422
2017-12-10T13:38:18.873331: step 3393, loss 0.288011, acc 0.921875, prec 0.0722155, recall 0.893422
2017-12-10T13:38:19.308057: step 3394, loss 0.311853, acc 0.9375, prec 0.0722556, recall 0.893489
2017-12-10T13:38:19.756702: step 3395, loss 0.170143, acc 0.921875, prec 0.0722701, recall 0.893523
2017-12-10T13:38:20.211367: step 3396, loss 0.2833, acc 0.890625, prec 0.0722571, recall 0.893523
2017-12-10T13:38:20.652147: step 3397, loss 0.113962, acc 0.96875, prec 0.0722772, recall 0.893557
2017-12-10T13:38:21.102619: step 3398, loss 0.330133, acc 0.9375, prec 0.0723172, recall 0.893624
2017-12-10T13:38:21.537176: step 3399, loss 0.288094, acc 0.90625, prec 0.0723298, recall 0.893657
2017-12-10T13:38:21.976824: step 3400, loss 0.342872, acc 0.921875, prec 0.0723205, recall 0.893657
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3400

2017-12-10T13:38:23.912887: step 3401, loss 0.206449, acc 0.984375, prec 0.0723424, recall 0.893691
2017-12-10T13:38:24.352530: step 3402, loss 0.41517, acc 0.90625, prec 0.0723313, recall 0.893691
2017-12-10T13:38:24.802849: step 3403, loss 0.489997, acc 1, prec 0.0724024, recall 0.893791
2017-12-10T13:38:25.239831: step 3404, loss 0.119682, acc 0.9375, prec 0.0724186, recall 0.893825
2017-12-10T13:38:25.687904: step 3405, loss 0.087903, acc 0.96875, prec 0.0724149, recall 0.893825
2017-12-10T13:38:26.135277: step 3406, loss 0.020032, acc 1, prec 0.0724149, recall 0.893825
2017-12-10T13:38:26.573666: step 3407, loss 0.0344088, acc 0.984375, prec 0.0724368, recall 0.893858
2017-12-10T13:38:27.007143: step 3408, loss 0.163794, acc 0.953125, prec 0.0724549, recall 0.893892
2017-12-10T13:38:27.452389: step 3409, loss 0.106645, acc 0.953125, prec 0.0724493, recall 0.893892
2017-12-10T13:38:27.924112: step 3410, loss 0.188257, acc 0.96875, prec 0.072493, recall 0.893958
2017-12-10T13:38:28.363704: step 3411, loss 0.0221323, acc 1, prec 0.0725166, recall 0.893992
2017-12-10T13:38:28.817709: step 3412, loss 0.0427427, acc 0.984375, prec 0.0725385, recall 0.894025
2017-12-10T13:38:29.253493: step 3413, loss 0.0746838, acc 0.96875, prec 0.0725348, recall 0.894025
2017-12-10T13:38:29.699372: step 3414, loss 0.736136, acc 0.984375, prec 0.0725802, recall 0.894092
2017-12-10T13:38:30.150253: step 3415, loss 0.108033, acc 0.96875, prec 0.0726002, recall 0.894125
2017-12-10T13:38:30.604239: step 3416, loss 0.130463, acc 0.953125, prec 0.0725946, recall 0.894125
2017-12-10T13:38:31.049665: step 3417, loss 0.0511734, acc 0.96875, prec 0.0725909, recall 0.894125
2017-12-10T13:38:31.504959: step 3418, loss 0.238027, acc 0.96875, prec 0.0726109, recall 0.894158
2017-12-10T13:38:31.952424: step 3419, loss 0.142738, acc 0.953125, prec 0.072629, recall 0.894192
2017-12-10T13:38:32.393455: step 3420, loss 0.143332, acc 0.953125, prec 0.0726944, recall 0.894291
2017-12-10T13:38:32.838395: step 3421, loss 0.0516299, acc 0.96875, prec 0.0726907, recall 0.894291
2017-12-10T13:38:33.283974: step 3422, loss 0.170849, acc 0.9375, prec 0.0726832, recall 0.894291
2017-12-10T13:38:33.739669: step 3423, loss 0.0876998, acc 0.96875, prec 0.0727032, recall 0.894324
2017-12-10T13:38:34.182275: step 3424, loss 0.208709, acc 0.984375, prec 0.0727486, recall 0.89439
2017-12-10T13:38:34.627598: step 3425, loss 0.0772148, acc 0.953125, prec 0.0727903, recall 0.894457
2017-12-10T13:38:35.069860: step 3426, loss 0.0321108, acc 0.984375, prec 0.0727884, recall 0.894457
2017-12-10T13:38:35.514892: step 3427, loss 0.0452953, acc 0.984375, prec 0.0728102, recall 0.89449
2017-12-10T13:38:35.961294: step 3428, loss 0.165328, acc 0.96875, prec 0.0728301, recall 0.894523
2017-12-10T13:38:36.411050: step 3429, loss 0.150631, acc 0.953125, prec 0.0728482, recall 0.894556
2017-12-10T13:38:36.860135: step 3430, loss 0.180528, acc 0.96875, prec 0.0728445, recall 0.894556
2017-12-10T13:38:37.302662: step 3431, loss 0.237703, acc 0.953125, prec 0.0728625, recall 0.894589
2017-12-10T13:38:37.761191: step 3432, loss 0.448298, acc 0.984375, prec 0.0729315, recall 0.894687
2017-12-10T13:38:38.216333: step 3433, loss 0.00455545, acc 1, prec 0.0730024, recall 0.894786
2017-12-10T13:38:38.678722: step 3434, loss 0.300124, acc 0.96875, prec 0.0730223, recall 0.894819
2017-12-10T13:38:39.133550: step 3435, loss 0.120246, acc 0.953125, prec 0.0730167, recall 0.894819
2017-12-10T13:38:39.572432: step 3436, loss 0.0798927, acc 0.96875, prec 0.0730366, recall 0.894852
2017-12-10T13:38:40.024864: step 3437, loss 0.207369, acc 0.9375, prec 0.0730291, recall 0.894852
2017-12-10T13:38:40.476040: step 3438, loss 0.298927, acc 0.9375, prec 0.0730689, recall 0.894917
2017-12-10T13:38:40.931777: step 3439, loss 0.178667, acc 0.953125, prec 0.0730633, recall 0.894917
2017-12-10T13:38:41.373821: step 3440, loss 0.0820232, acc 0.96875, prec 0.0730596, recall 0.894917
2017-12-10T13:38:41.815492: step 3441, loss 0.179074, acc 0.953125, prec 0.0730776, recall 0.89495
2017-12-10T13:38:42.260951: step 3442, loss 0.138201, acc 0.96875, prec 0.0730739, recall 0.89495
2017-12-10T13:38:42.699498: step 3443, loss 0.170945, acc 0.953125, prec 0.0730683, recall 0.89495
2017-12-10T13:38:43.144633: step 3444, loss 0.038079, acc 0.984375, prec 0.07309, recall 0.894983
2017-12-10T13:38:43.590936: step 3445, loss 0.599978, acc 0.921875, prec 0.0731279, recall 0.895048
2017-12-10T13:38:44.044931: step 3446, loss 0.115948, acc 0.953125, prec 0.0731223, recall 0.895048
2017-12-10T13:38:44.492396: step 3447, loss 0.283296, acc 1, prec 0.0731695, recall 0.895114
2017-12-10T13:38:44.945832: step 3448, loss 0.175157, acc 0.953125, prec 0.0731875, recall 0.895146
2017-12-10T13:38:45.405253: step 3449, loss 0.287129, acc 0.953125, prec 0.073229, recall 0.895211
2017-12-10T13:38:45.846855: step 3450, loss 0.0675474, acc 0.953125, prec 0.0732235, recall 0.895211
2017-12-10T13:38:46.294711: step 3451, loss 0.0741666, acc 0.96875, prec 0.0732433, recall 0.895244
2017-12-10T13:38:46.740046: step 3452, loss 0.0473384, acc 0.96875, prec 0.0732396, recall 0.895244
2017-12-10T13:38:47.177521: step 3453, loss 0.227843, acc 0.90625, prec 0.073252, recall 0.895277
2017-12-10T13:38:47.625854: step 3454, loss 0.144018, acc 0.953125, prec 0.0733171, recall 0.895374
2017-12-10T13:38:48.067171: step 3455, loss 0.174684, acc 0.953125, prec 0.073335, recall 0.895407
2017-12-10T13:38:48.516346: step 3456, loss 0.238304, acc 0.953125, prec 0.073353, recall 0.895439
2017-12-10T13:38:48.952931: step 3457, loss 0.127122, acc 0.9375, prec 0.0733455, recall 0.895439
2017-12-10T13:38:49.391192: step 3458, loss 0.0203367, acc 1, prec 0.0733926, recall 0.895504
2017-12-10T13:38:49.829933: step 3459, loss 0.124604, acc 0.984375, prec 0.0734143, recall 0.895536
2017-12-10T13:38:50.269666: step 3460, loss 0.0963797, acc 0.984375, prec 0.0734124, recall 0.895536
2017-12-10T13:38:50.728699: step 3461, loss 0.0209981, acc 1, prec 0.0734124, recall 0.895536
2017-12-10T13:38:51.181372: step 3462, loss 0.182933, acc 0.9375, prec 0.0734521, recall 0.895601
2017-12-10T13:38:51.632392: step 3463, loss 0.0276768, acc 1, prec 0.0734756, recall 0.895633
2017-12-10T13:38:52.075680: step 3464, loss 0.866922, acc 0.9375, prec 0.0734917, recall 0.895666
2017-12-10T13:38:52.535209: step 3465, loss 0.0313812, acc 0.984375, prec 0.0735134, recall 0.895698
2017-12-10T13:38:52.976162: step 3466, loss 0.0653672, acc 0.984375, prec 0.0735585, recall 0.895762
2017-12-10T13:38:53.422112: step 3467, loss 2.17571, acc 0.9375, prec 0.0735765, recall 0.895518
2017-12-10T13:38:53.876784: step 3468, loss 0.254249, acc 0.96875, prec 0.0735727, recall 0.895518
2017-12-10T13:38:54.326578: step 3469, loss 0.0887595, acc 0.953125, prec 0.0735671, recall 0.895518
2017-12-10T13:38:54.785996: step 3470, loss 0.113378, acc 0.984375, prec 0.0735653, recall 0.895518
2017-12-10T13:38:55.222947: step 3471, loss 0.396175, acc 0.875, prec 0.0735503, recall 0.895518
2017-12-10T13:38:55.670588: step 3472, loss 0.400803, acc 0.875, prec 0.0735354, recall 0.895518
2017-12-10T13:38:56.126493: step 3473, loss 0.381548, acc 0.90625, prec 0.0735477, recall 0.89555
2017-12-10T13:38:56.574966: step 3474, loss 0.182056, acc 0.953125, prec 0.0735656, recall 0.895582
2017-12-10T13:38:57.016664: step 3475, loss 0.213127, acc 0.96875, prec 0.0735854, recall 0.895615
2017-12-10T13:38:57.462926: step 3476, loss 0.253053, acc 0.90625, prec 0.0735977, recall 0.895647
2017-12-10T13:38:57.900016: step 3477, loss 0.344212, acc 0.859375, prec 0.0735809, recall 0.895647
2017-12-10T13:38:58.354070: step 3478, loss 0.243112, acc 0.890625, prec 0.0735913, recall 0.895679
2017-12-10T13:38:58.752214: step 3479, loss 0.591797, acc 0.826923, prec 0.073598, recall 0.895711
2017-12-10T13:38:59.196546: step 3480, loss 0.310151, acc 0.90625, prec 0.0735868, recall 0.895711
2017-12-10T13:38:59.634367: step 3481, loss 0.391992, acc 0.875, prec 0.0735719, recall 0.895711
2017-12-10T13:39:00.077714: step 3482, loss 0.165754, acc 0.9375, prec 0.0735644, recall 0.895711
2017-12-10T13:39:00.509048: step 3483, loss 0.156941, acc 0.9375, prec 0.073557, recall 0.895711
2017-12-10T13:39:00.944598: step 3484, loss 0.224963, acc 0.921875, prec 0.0735946, recall 0.895775
2017-12-10T13:39:01.386705: step 3485, loss 0.117084, acc 0.96875, prec 0.0735909, recall 0.895775
2017-12-10T13:39:01.832684: step 3486, loss 0.0619993, acc 0.96875, prec 0.0736106, recall 0.895808
2017-12-10T13:39:02.292522: step 3487, loss 0.7327, acc 0.96875, prec 0.0736304, recall 0.89584
2017-12-10T13:39:02.745541: step 3488, loss 0.0616123, acc 0.96875, prec 0.0736266, recall 0.89584
2017-12-10T13:39:03.203067: step 3489, loss 1.98013, acc 0.984375, prec 0.073697, recall 0.89566
2017-12-10T13:39:03.664092: step 3490, loss 0.0494375, acc 0.984375, prec 0.0736951, recall 0.89566
2017-12-10T13:39:04.102248: step 3491, loss 0.305945, acc 0.9375, prec 0.0737111, recall 0.895692
2017-12-10T13:39:04.556755: step 3492, loss 0.317047, acc 0.9375, prec 0.0737271, recall 0.895724
2017-12-10T13:39:04.999432: step 3493, loss 0.285571, acc 0.921875, prec 0.0737178, recall 0.895724
2017-12-10T13:39:05.437302: step 3494, loss 0.232401, acc 0.90625, prec 0.07373, recall 0.895756
2017-12-10T13:39:05.883285: step 3495, loss 0.180305, acc 0.953125, prec 0.0737479, recall 0.895788
2017-12-10T13:39:06.323180: step 3496, loss 0.349831, acc 0.890625, prec 0.0737583, recall 0.895821
2017-12-10T13:39:06.771816: step 3497, loss 0.139173, acc 0.953125, prec 0.0737527, recall 0.895821
2017-12-10T13:39:07.220071: step 3498, loss 0.130869, acc 0.953125, prec 0.0737939, recall 0.895885
2017-12-10T13:39:07.657564: step 3499, loss 0.367367, acc 0.921875, prec 0.073808, recall 0.895916
2017-12-10T13:39:08.098594: step 3500, loss 0.324019, acc 0.921875, prec 0.073869, recall 0.896012
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3500

2017-12-10T13:39:09.959483: step 3501, loss 0.25058, acc 0.9375, prec 0.0739551, recall 0.89614
2017-12-10T13:39:10.400422: step 3502, loss 0.353967, acc 0.875, prec 0.073987, recall 0.896203
2017-12-10T13:39:10.846725: step 3503, loss 0.275323, acc 0.953125, prec 0.0740048, recall 0.896235
2017-12-10T13:39:11.290502: step 3504, loss 0.246766, acc 0.9375, prec 0.0740441, recall 0.896299
2017-12-10T13:39:11.727193: step 3505, loss 0.527155, acc 0.9375, prec 0.07406, recall 0.89633
2017-12-10T13:39:12.171814: step 3506, loss 0.0576562, acc 0.96875, prec 0.0740797, recall 0.896362
2017-12-10T13:39:12.616554: step 3507, loss 0.292688, acc 0.9375, prec 0.0740722, recall 0.896362
2017-12-10T13:39:13.064101: step 3508, loss 0.271279, acc 0.953125, prec 0.0741134, recall 0.896425
2017-12-10T13:39:13.512663: step 3509, loss 0.21758, acc 0.953125, prec 0.0741078, recall 0.896425
2017-12-10T13:39:13.972816: step 3510, loss 0.0985308, acc 0.953125, prec 0.0741021, recall 0.896425
2017-12-10T13:39:14.426530: step 3511, loss 0.426661, acc 0.890625, prec 0.074089, recall 0.896425
2017-12-10T13:39:14.861578: step 3512, loss 0.0287493, acc 0.984375, prec 0.0740872, recall 0.896425
2017-12-10T13:39:15.312519: step 3513, loss 0.044929, acc 0.984375, prec 0.0740853, recall 0.896425
2017-12-10T13:39:15.770043: step 3514, loss 0.0305967, acc 1, prec 0.0741087, recall 0.896457
2017-12-10T13:39:16.209647: step 3515, loss 0.0181552, acc 1, prec 0.0741554, recall 0.89652
2017-12-10T13:39:16.674634: step 3516, loss 0.00547893, acc 1, prec 0.0741788, recall 0.896552
2017-12-10T13:39:17.128472: step 3517, loss 0.112322, acc 0.953125, prec 0.0741732, recall 0.896552
2017-12-10T13:39:17.574940: step 3518, loss 0.148046, acc 0.984375, prec 0.0742181, recall 0.896615
2017-12-10T13:39:18.028809: step 3519, loss 0.0648266, acc 0.96875, prec 0.0742143, recall 0.896615
2017-12-10T13:39:18.469520: step 3520, loss 0.0362792, acc 0.984375, prec 0.0742825, recall 0.896709
2017-12-10T13:39:18.922729: step 3521, loss 1.97516, acc 0.96875, prec 0.074304, recall 0.896468
2017-12-10T13:39:19.385726: step 3522, loss 0.00714914, acc 1, prec 0.074304, recall 0.896468
2017-12-10T13:39:19.836982: step 3523, loss 0.137748, acc 0.953125, prec 0.0743218, recall 0.896499
2017-12-10T13:39:20.290329: step 3524, loss 0.0215605, acc 0.984375, prec 0.0743199, recall 0.896499
2017-12-10T13:39:20.736411: step 3525, loss 0.326402, acc 0.96875, prec 0.0743395, recall 0.896531
2017-12-10T13:39:21.175568: step 3526, loss 0.0596824, acc 0.984375, prec 0.0743376, recall 0.896531
2017-12-10T13:39:21.611353: step 3527, loss 0.25919, acc 0.984375, prec 0.0743591, recall 0.896562
2017-12-10T13:39:22.048095: step 3528, loss 0.0735681, acc 0.96875, prec 0.0743787, recall 0.896594
2017-12-10T13:39:22.498845: step 3529, loss 0.0781066, acc 0.984375, prec 0.0743768, recall 0.896594
2017-12-10T13:39:22.931108: step 3530, loss 0.47333, acc 0.921875, prec 0.0743908, recall 0.896625
2017-12-10T13:39:23.389201: step 3531, loss 0.137906, acc 0.9375, prec 0.0743833, recall 0.896625
2017-12-10T13:39:23.835208: step 3532, loss 0.126081, acc 0.96875, prec 0.0744496, recall 0.896719
2017-12-10T13:39:24.278949: step 3533, loss 0.175355, acc 0.921875, prec 0.0745102, recall 0.896813
2017-12-10T13:39:24.734507: step 3534, loss 0.187826, acc 0.921875, prec 0.0745941, recall 0.896938
2017-12-10T13:39:25.176198: step 3535, loss 0.44943, acc 0.890625, prec 0.0746276, recall 0.897001
2017-12-10T13:39:25.622484: step 3536, loss 0.109182, acc 0.96875, prec 0.0746705, recall 0.897063
2017-12-10T13:39:26.066233: step 3537, loss 0.216503, acc 0.9375, prec 0.074663, recall 0.897063
2017-12-10T13:39:26.519490: step 3538, loss 0.176644, acc 0.9375, prec 0.0746554, recall 0.897063
2017-12-10T13:39:26.962952: step 3539, loss 0.231864, acc 0.96875, prec 0.0746983, recall 0.897126
2017-12-10T13:39:27.421686: step 3540, loss 0.256283, acc 0.90625, prec 0.0747103, recall 0.897157
2017-12-10T13:39:27.865034: step 3541, loss 0.130297, acc 0.953125, prec 0.0747047, recall 0.897157
2017-12-10T13:39:28.323458: step 3542, loss 0.266161, acc 0.953125, prec 0.074699, recall 0.897157
2017-12-10T13:39:28.770727: step 3543, loss 0.409976, acc 0.9375, prec 0.0747148, recall 0.897188
2017-12-10T13:39:29.226133: step 3544, loss 0.116267, acc 0.984375, prec 0.0747362, recall 0.897219
2017-12-10T13:39:29.670010: step 3545, loss 0.0996127, acc 0.96875, prec 0.0747558, recall 0.89725
2017-12-10T13:39:30.114140: step 3546, loss 0.255417, acc 0.9375, prec 0.0747482, recall 0.89725
2017-12-10T13:39:30.571522: step 3547, loss 0.10657, acc 0.921875, prec 0.074832, recall 0.897374
2017-12-10T13:39:31.043288: step 3548, loss 0.655459, acc 0.90625, prec 0.0748207, recall 0.897374
2017-12-10T13:39:31.484741: step 3549, loss 0.0585744, acc 0.984375, prec 0.0748188, recall 0.897374
2017-12-10T13:39:31.923653: step 3550, loss 0.0944722, acc 0.953125, prec 0.0748132, recall 0.897374
2017-12-10T13:39:32.369966: step 3551, loss 0.0708345, acc 0.953125, prec 0.0748308, recall 0.897405
2017-12-10T13:39:32.810041: step 3552, loss 0.0797364, acc 0.96875, prec 0.0748736, recall 0.897467
2017-12-10T13:39:33.254850: step 3553, loss 0.0757259, acc 0.96875, prec 0.0748931, recall 0.897498
2017-12-10T13:39:33.702101: step 3554, loss 0.146498, acc 0.953125, prec 0.0749107, recall 0.897529
2017-12-10T13:39:34.147831: step 3555, loss 0.198106, acc 0.9375, prec 0.0749032, recall 0.897529
2017-12-10T13:39:34.592504: step 3556, loss 2.08356, acc 0.953125, prec 0.0749459, recall 0.89732
2017-12-10T13:39:35.055241: step 3557, loss 0.0218224, acc 1, prec 0.0749459, recall 0.89732
2017-12-10T13:39:35.512359: step 3558, loss 0.337769, acc 0.9375, prec 0.0749849, recall 0.897382
2017-12-10T13:39:35.966932: step 3559, loss 0.0329353, acc 0.984375, prec 0.074983, recall 0.897382
2017-12-10T13:39:36.416419: step 3560, loss 0.332767, acc 0.9375, prec 0.0749755, recall 0.897382
2017-12-10T13:39:36.864118: step 3561, loss 0.0364803, acc 1, prec 0.0749987, recall 0.897413
2017-12-10T13:39:37.318016: step 3562, loss 0.138819, acc 0.96875, prec 0.074995, recall 0.897413
2017-12-10T13:39:37.757643: step 3563, loss 0.346571, acc 0.90625, prec 0.0750069, recall 0.897444
2017-12-10T13:39:38.202672: step 3564, loss 0.0470125, acc 0.984375, prec 0.075005, recall 0.897444
2017-12-10T13:39:38.652490: step 3565, loss 0.213592, acc 0.90625, prec 0.0749937, recall 0.897444
2017-12-10T13:39:39.105560: step 3566, loss 0.303102, acc 0.90625, prec 0.0749824, recall 0.897444
2017-12-10T13:39:39.534244: step 3567, loss 0.101738, acc 0.953125, prec 0.075, recall 0.897474
2017-12-10T13:39:39.981218: step 3568, loss 0.0835665, acc 0.96875, prec 0.0750427, recall 0.897536
2017-12-10T13:39:40.433109: step 3569, loss 0.0417455, acc 1, prec 0.0750659, recall 0.897567
2017-12-10T13:39:40.881636: step 3570, loss 0.06025, acc 0.96875, prec 0.0750854, recall 0.897598
2017-12-10T13:39:41.327822: step 3571, loss 0.058005, acc 0.96875, prec 0.0750816, recall 0.897598
2017-12-10T13:39:41.770220: step 3572, loss 0.198553, acc 0.953125, prec 0.075076, recall 0.897598
2017-12-10T13:39:42.221191: step 3573, loss 0.240083, acc 0.90625, prec 0.0750879, recall 0.897628
2017-12-10T13:39:42.680090: step 3574, loss 3.74542, acc 0.953125, prec 0.0751074, recall 0.89739
2017-12-10T13:39:43.132547: step 3575, loss 0.100157, acc 0.96875, prec 0.0751268, recall 0.897421
2017-12-10T13:39:43.574954: step 3576, loss 0.0608344, acc 0.96875, prec 0.0751695, recall 0.897482
2017-12-10T13:39:44.026522: step 3577, loss 0.402769, acc 0.9375, prec 0.0751619, recall 0.897482
2017-12-10T13:39:44.479788: step 3578, loss 0.257767, acc 0.875, prec 0.0751468, recall 0.897482
2017-12-10T13:39:44.946421: step 3579, loss 0.379206, acc 0.9375, prec 0.0751625, recall 0.897513
2017-12-10T13:39:45.404940: step 3580, loss 0.478969, acc 0.8125, prec 0.0751399, recall 0.897513
2017-12-10T13:39:45.854226: step 3581, loss 0.271906, acc 0.921875, prec 0.0751536, recall 0.897543
2017-12-10T13:39:46.307826: step 3582, loss 0.375026, acc 0.859375, prec 0.0751831, recall 0.897605
2017-12-10T13:39:46.762619: step 3583, loss 0.505716, acc 0.875, prec 0.075168, recall 0.897605
2017-12-10T13:39:47.205355: step 3584, loss 0.83728, acc 0.8125, prec 0.0751454, recall 0.897605
2017-12-10T13:39:47.645299: step 3585, loss 0.450771, acc 0.890625, prec 0.0751554, recall 0.897635
2017-12-10T13:39:48.096332: step 3586, loss 0.513247, acc 0.875, prec 0.0751635, recall 0.897666
2017-12-10T13:39:48.544286: step 3587, loss 0.764447, acc 0.875, prec 0.0751947, recall 0.897727
2017-12-10T13:39:48.988266: step 3588, loss 0.536137, acc 0.8125, prec 0.0751722, recall 0.897727
2017-12-10T13:39:49.440652: step 3589, loss 0.455722, acc 0.84375, prec 0.0751533, recall 0.897727
2017-12-10T13:39:49.881829: step 3590, loss 0.146673, acc 0.921875, prec 0.0751671, recall 0.897758
2017-12-10T13:39:50.315396: step 3591, loss 0.163203, acc 0.96875, prec 0.0751633, recall 0.897758
2017-12-10T13:39:50.755765: step 3592, loss 0.286086, acc 0.9375, prec 0.0751789, recall 0.897788
2017-12-10T13:39:51.201660: step 3593, loss 0.119104, acc 0.9375, prec 0.0751714, recall 0.897788
2017-12-10T13:39:51.651390: step 3594, loss 0.266492, acc 0.921875, prec 0.075162, recall 0.897788
2017-12-10T13:39:52.102232: step 3595, loss 0.127483, acc 0.953125, prec 0.0752489, recall 0.89791
2017-12-10T13:39:52.555726: step 3596, loss 0.0950178, acc 0.9375, prec 0.0752645, recall 0.897941
2017-12-10T13:39:53.001214: step 3597, loss 0.0668023, acc 0.984375, prec 0.0752626, recall 0.897941
2017-12-10T13:39:53.450463: step 3598, loss 0.45181, acc 0.953125, prec 0.0752801, recall 0.897971
2017-12-10T13:39:53.897877: step 3599, loss 0.0265158, acc 0.984375, prec 0.0753014, recall 0.898002
2017-12-10T13:39:54.344456: step 3600, loss 0.565446, acc 0.953125, prec 0.075342, recall 0.898063
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3600

2017-12-10T13:39:56.427929: step 3601, loss 0.190856, acc 0.921875, prec 0.0753788, recall 0.898123
2017-12-10T13:39:56.861264: step 3602, loss 0.0903302, acc 0.953125, prec 0.0753731, recall 0.898123
2017-12-10T13:39:57.323140: step 3603, loss 0.0566449, acc 0.96875, prec 0.0754618, recall 0.898245
2017-12-10T13:39:57.775368: step 3604, loss 0.034491, acc 0.984375, prec 0.0754599, recall 0.898245
2017-12-10T13:39:58.229827: step 3605, loss 0.181029, acc 0.9375, prec 0.0754986, recall 0.898305
2017-12-10T13:39:58.680387: step 3606, loss 0.040983, acc 1, prec 0.0755448, recall 0.898366
2017-12-10T13:39:59.135342: step 3607, loss 0.0705159, acc 0.96875, prec 0.0755641, recall 0.898396
2017-12-10T13:39:59.583872: step 3608, loss 0.0792384, acc 0.96875, prec 0.0755834, recall 0.898426
2017-12-10T13:40:00.039843: step 3609, loss 2.58125, acc 0.984375, prec 0.0756065, recall 0.898189
2017-12-10T13:40:00.498075: step 3610, loss 0.192064, acc 0.953125, prec 0.075624, recall 0.89822
2017-12-10T13:40:00.940161: step 3611, loss 0.16792, acc 0.96875, prec 0.0756202, recall 0.89822
2017-12-10T13:40:01.378833: step 3612, loss 0.585582, acc 0.9375, prec 0.0756588, recall 0.89828
2017-12-10T13:40:01.829721: step 3613, loss 0.503445, acc 0.859375, prec 0.0756418, recall 0.89828
2017-12-10T13:40:02.280220: step 3614, loss 0.108816, acc 0.96875, prec 0.075638, recall 0.89828
2017-12-10T13:40:02.725700: step 3615, loss 0.106512, acc 0.984375, prec 0.0756823, recall 0.89834
2017-12-10T13:40:03.181461: step 3616, loss 0.278762, acc 0.953125, prec 0.0756997, recall 0.89837
2017-12-10T13:40:03.627219: step 3617, loss 0.267094, acc 0.890625, prec 0.0756865, recall 0.89837
2017-12-10T13:40:04.073319: step 3618, loss 0.269723, acc 0.890625, prec 0.0756963, recall 0.8984
2017-12-10T13:40:04.523622: step 3619, loss 0.15898, acc 0.9375, prec 0.0756888, recall 0.8984
2017-12-10T13:40:04.969224: step 3620, loss 0.267232, acc 0.953125, prec 0.0757062, recall 0.898431
2017-12-10T13:40:05.437084: step 3621, loss 0.118124, acc 0.953125, prec 0.0757005, recall 0.898431
2017-12-10T13:40:05.885708: step 3622, loss 0.490627, acc 0.953125, prec 0.075764, recall 0.898521
2017-12-10T13:40:06.331635: step 3623, loss 0.234773, acc 0.90625, prec 0.0757527, recall 0.898521
2017-12-10T13:40:06.767737: step 3624, loss 0.320142, acc 0.859375, prec 0.0758048, recall 0.898611
2017-12-10T13:40:07.193407: step 3625, loss 0.112419, acc 0.953125, prec 0.0757991, recall 0.898611
2017-12-10T13:40:07.636972: step 3626, loss 1.21315, acc 0.9375, prec 0.0758146, recall 0.898641
2017-12-10T13:40:08.080194: step 3627, loss 0.369584, acc 0.890625, prec 0.0758014, recall 0.898641
2017-12-10T13:40:08.533208: step 3628, loss 0.598376, acc 0.890625, prec 0.0757882, recall 0.898641
2017-12-10T13:40:08.979680: step 3629, loss 0.213819, acc 0.953125, prec 0.0758055, recall 0.898671
2017-12-10T13:40:09.421008: step 3630, loss 0.557792, acc 0.90625, prec 0.0758633, recall 0.89876
2017-12-10T13:40:09.866400: step 3631, loss 0.24115, acc 0.90625, prec 0.0758519, recall 0.89876
2017-12-10T13:40:10.322785: step 3632, loss 0.145979, acc 0.953125, prec 0.0758923, recall 0.89882
2017-12-10T13:40:10.759222: step 3633, loss 0.316553, acc 0.96875, prec 0.0759115, recall 0.89885
2017-12-10T13:40:11.209069: step 3634, loss 0.220583, acc 0.921875, prec 0.0759021, recall 0.89885
2017-12-10T13:40:11.664575: step 3635, loss 0.411019, acc 0.90625, prec 0.0758907, recall 0.89885
2017-12-10T13:40:12.120635: step 3636, loss 0.281863, acc 0.953125, prec 0.0759311, recall 0.89891
2017-12-10T13:40:12.568289: step 3637, loss 0.147295, acc 0.953125, prec 0.0759254, recall 0.89891
2017-12-10T13:40:13.006236: step 3638, loss 0.213658, acc 0.90625, prec 0.0759141, recall 0.89891
2017-12-10T13:40:13.461485: step 3639, loss 0.237536, acc 0.921875, prec 0.0759046, recall 0.89891
2017-12-10T13:40:13.900992: step 3640, loss 0.37931, acc 0.90625, prec 0.0759163, recall 0.898939
2017-12-10T13:40:14.346207: step 3641, loss 1.27054, acc 0.9375, prec 0.0759547, recall 0.898999
2017-12-10T13:40:14.802478: step 3642, loss 0.445831, acc 0.9375, prec 0.0759931, recall 0.899058
2017-12-10T13:40:15.276975: step 3643, loss 0.163733, acc 0.921875, prec 0.0759837, recall 0.899058
2017-12-10T13:40:15.718977: step 3644, loss 0.378457, acc 0.890625, prec 0.0759934, recall 0.899088
2017-12-10T13:40:16.157427: step 3645, loss 0.21005, acc 0.9375, prec 0.0759859, recall 0.899088
2017-12-10T13:40:16.604470: step 3646, loss 0.0338506, acc 0.984375, prec 0.075984, recall 0.899088
2017-12-10T13:40:17.048889: step 3647, loss 0.248938, acc 0.9375, prec 0.0759764, recall 0.899088
2017-12-10T13:40:17.496049: step 3648, loss 0.542374, acc 0.890625, prec 0.0759862, recall 0.899118
2017-12-10T13:40:17.937525: step 3649, loss 0.0836002, acc 0.953125, prec 0.0760035, recall 0.899147
2017-12-10T13:40:18.385516: step 3650, loss 0.146955, acc 0.984375, prec 0.0760016, recall 0.899147
2017-12-10T13:40:18.831739: step 3651, loss 0.0823994, acc 0.96875, prec 0.0759978, recall 0.899147
2017-12-10T13:40:19.281781: step 3652, loss 0.416565, acc 0.890625, prec 0.0759846, recall 0.899147
2017-12-10T13:40:19.728328: step 3653, loss 0.27426, acc 0.875, prec 0.0760154, recall 0.899207
2017-12-10T13:40:20.181035: step 3654, loss 0.133863, acc 0.96875, prec 0.0760346, recall 0.899236
2017-12-10T13:40:20.629695: step 3655, loss 0.047913, acc 0.984375, prec 0.0760327, recall 0.899236
2017-12-10T13:40:21.073512: step 3656, loss 0.285594, acc 0.921875, prec 0.0760232, recall 0.899236
2017-12-10T13:40:21.516176: step 3657, loss 0.220897, acc 0.9375, prec 0.0760157, recall 0.899236
2017-12-10T13:40:21.949863: step 3658, loss 0.250883, acc 0.96875, prec 0.0760119, recall 0.899236
2017-12-10T13:40:22.395212: step 3659, loss 0.220284, acc 0.90625, prec 0.0760694, recall 0.899325
2017-12-10T13:40:22.840089: step 3660, loss 0.727109, acc 0.953125, prec 0.0760867, recall 0.899354
2017-12-10T13:40:23.289132: step 3661, loss 0.0539525, acc 0.96875, prec 0.0761058, recall 0.899384
2017-12-10T13:40:23.741396: step 3662, loss 0.0247925, acc 0.984375, prec 0.0761269, recall 0.899413
2017-12-10T13:40:24.191278: step 3663, loss 0.142294, acc 0.984375, prec 0.0761479, recall 0.899443
2017-12-10T13:40:24.649305: step 3664, loss 0.386224, acc 0.90625, prec 0.0761366, recall 0.899443
2017-12-10T13:40:25.088507: step 3665, loss 3.99499, acc 0.921875, prec 0.076152, recall 0.899209
2017-12-10T13:40:25.541212: step 3666, loss 0.177742, acc 0.9375, prec 0.0761444, recall 0.899209
2017-12-10T13:40:25.993764: step 3667, loss 0.410459, acc 0.921875, prec 0.0761808, recall 0.899268
2017-12-10T13:40:26.446185: step 3668, loss 0.109609, acc 0.953125, prec 0.0761751, recall 0.899268
2017-12-10T13:40:26.891339: step 3669, loss 0.126807, acc 0.96875, prec 0.0761713, recall 0.899268
2017-12-10T13:40:27.328153: step 3670, loss 0.344419, acc 0.90625, prec 0.0761829, recall 0.899297
2017-12-10T13:40:27.769583: step 3671, loss 0.271953, acc 0.9375, prec 0.0761983, recall 0.899327
2017-12-10T13:40:28.214010: step 3672, loss 0.354041, acc 0.890625, prec 0.076208, recall 0.899356
2017-12-10T13:40:28.664886: step 3673, loss 0.184307, acc 0.890625, prec 0.0761947, recall 0.899356
2017-12-10T13:40:29.116534: step 3674, loss 0.127507, acc 0.953125, prec 0.076212, recall 0.899386
2017-12-10T13:40:29.555398: step 3675, loss 0.221315, acc 0.890625, prec 0.0762445, recall 0.899445
2017-12-10T13:40:30.007535: step 3676, loss 0.227839, acc 0.9375, prec 0.0762599, recall 0.899474
2017-12-10T13:40:30.466529: step 3677, loss 0.193671, acc 0.9375, prec 0.0762752, recall 0.899503
2017-12-10T13:40:30.914226: step 3678, loss 0.211757, acc 0.90625, prec 0.0762638, recall 0.899503
2017-12-10T13:40:31.360890: step 3679, loss 0.0815009, acc 0.96875, prec 0.0762601, recall 0.899503
2017-12-10T13:40:31.804848: step 3680, loss 0.456005, acc 0.875, prec 0.0762907, recall 0.899562
2017-12-10T13:40:32.258875: step 3681, loss 0.24052, acc 0.9375, prec 0.0763289, recall 0.899621
2017-12-10T13:40:32.716962: step 3682, loss 0.311169, acc 0.90625, prec 0.0763175, recall 0.899621
2017-12-10T13:40:33.171852: step 3683, loss 0.114501, acc 0.953125, prec 0.0763347, recall 0.89965
2017-12-10T13:40:33.625309: step 3684, loss 0.0555009, acc 0.96875, prec 0.0763767, recall 0.899708
2017-12-10T13:40:34.077943: step 3685, loss 0.356266, acc 0.890625, prec 0.0763635, recall 0.899708
2017-12-10T13:40:34.521902: step 3686, loss 0.230711, acc 0.953125, prec 0.0763578, recall 0.899708
2017-12-10T13:40:34.964739: step 3687, loss 0.046295, acc 0.984375, prec 0.0763559, recall 0.899708
2017-12-10T13:40:35.413134: step 3688, loss 0.495882, acc 1, prec 0.0763787, recall 0.899738
2017-12-10T13:40:35.863852: step 3689, loss 0.11671, acc 0.96875, prec 0.0763978, recall 0.899767
2017-12-10T13:40:36.307828: step 3690, loss 0.0601711, acc 0.984375, prec 0.0763959, recall 0.899767
2017-12-10T13:40:36.755287: step 3691, loss 0.0455045, acc 1, prec 0.0764188, recall 0.899796
2017-12-10T13:40:37.204313: step 3692, loss 0.0852211, acc 0.953125, prec 0.0764131, recall 0.899796
2017-12-10T13:40:37.661122: step 3693, loss 0.144692, acc 0.953125, prec 0.0764531, recall 0.899854
2017-12-10T13:40:38.112540: step 3694, loss 1.36671, acc 0.953125, prec 0.0765388, recall 0.899971
2017-12-10T13:40:38.567517: step 3695, loss 0.0868382, acc 0.984375, prec 0.0765369, recall 0.899971
2017-12-10T13:40:39.021500: step 3696, loss 0.731329, acc 0.9375, prec 0.0765294, recall 0.899971
2017-12-10T13:40:39.461217: step 3697, loss 0.16725, acc 0.953125, prec 0.0765465, recall 0.9
2017-12-10T13:40:39.912321: step 3698, loss 0.082578, acc 0.96875, prec 0.0766112, recall 0.900087
2017-12-10T13:40:40.363072: step 3699, loss 0.305467, acc 0.9375, prec 0.0766036, recall 0.900087
2017-12-10T13:40:40.807701: step 3700, loss 0.39316, acc 0.9375, prec 0.0766189, recall 0.900116
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3700

2017-12-10T13:40:42.735812: step 3701, loss 0.194429, acc 0.921875, prec 0.0766094, recall 0.900116
2017-12-10T13:40:43.179633: step 3702, loss 0.303915, acc 0.890625, prec 0.0765962, recall 0.900116
2017-12-10T13:40:43.629742: step 3703, loss 0.243209, acc 0.9375, prec 0.0766342, recall 0.900174
2017-12-10T13:40:44.067480: step 3704, loss 0.216879, acc 0.96875, prec 0.0766532, recall 0.900203
2017-12-10T13:40:44.511891: step 3705, loss 0.154872, acc 0.953125, prec 0.0766476, recall 0.900203
2017-12-10T13:40:44.962532: step 3706, loss 0.0804865, acc 0.96875, prec 0.0766438, recall 0.900203
2017-12-10T13:40:45.397768: step 3707, loss 0.0867288, acc 0.96875, prec 0.0767084, recall 0.90029
2017-12-10T13:40:45.852427: step 3708, loss 2.37096, acc 0.953125, prec 0.0767502, recall 0.900087
2017-12-10T13:40:46.301657: step 3709, loss 0.0859717, acc 0.9375, prec 0.0767654, recall 0.900116
2017-12-10T13:40:46.745795: step 3710, loss 0.0598978, acc 0.96875, prec 0.0767844, recall 0.900145
2017-12-10T13:40:47.179980: step 3711, loss 0.136794, acc 0.96875, prec 0.0768034, recall 0.900174
2017-12-10T13:40:47.638517: step 3712, loss 0.224685, acc 0.953125, prec 0.0767977, recall 0.900174
2017-12-10T13:40:48.092551: step 3713, loss 0.279637, acc 0.9375, prec 0.076813, recall 0.900203
2017-12-10T13:40:48.542904: step 3714, loss 0.457507, acc 0.875, prec 0.0767978, recall 0.900203
2017-12-10T13:40:48.986831: step 3715, loss 0.64797, acc 0.859375, prec 0.0768263, recall 0.90026
2017-12-10T13:40:49.438568: step 3716, loss 0.548677, acc 0.859375, prec 0.0768092, recall 0.90026
2017-12-10T13:40:49.876166: step 3717, loss 0.180868, acc 0.90625, prec 0.0767979, recall 0.90026
2017-12-10T13:40:50.336384: step 3718, loss 0.243467, acc 0.953125, prec 0.076815, recall 0.900289
2017-12-10T13:40:50.783416: step 3719, loss 0.276629, acc 0.90625, prec 0.0768263, recall 0.900318
2017-12-10T13:40:51.245213: step 3720, loss 0.119566, acc 0.953125, prec 0.0768207, recall 0.900318
2017-12-10T13:40:51.691726: step 3721, loss 0.268563, acc 0.9375, prec 0.0768358, recall 0.900347
2017-12-10T13:40:52.145759: step 3722, loss 0.274692, acc 0.921875, prec 0.0768264, recall 0.900347
2017-12-10T13:40:52.601102: step 3723, loss 0.415365, acc 0.84375, prec 0.0768302, recall 0.900375
2017-12-10T13:40:53.048122: step 3724, loss 0.232862, acc 0.953125, prec 0.0768473, recall 0.900404
2017-12-10T13:40:53.488091: step 3725, loss 0.0211136, acc 1, prec 0.0768473, recall 0.900404
2017-12-10T13:40:53.931179: step 3726, loss 0.0680025, acc 0.984375, prec 0.0768909, recall 0.900462
2017-12-10T13:40:54.378656: step 3727, loss 0.104019, acc 0.96875, prec 0.076978, recall 0.900576
2017-12-10T13:40:54.821301: step 3728, loss 0.383993, acc 0.9375, prec 0.0769932, recall 0.900605
2017-12-10T13:40:55.265716: step 3729, loss 0.283359, acc 0.921875, prec 0.0770519, recall 0.900691
2017-12-10T13:40:55.696791: step 3730, loss 0.177732, acc 0.9375, prec 0.0770443, recall 0.900691
2017-12-10T13:40:56.134259: step 3731, loss 0.200311, acc 0.96875, prec 0.0770405, recall 0.900691
2017-12-10T13:40:56.580261: step 3732, loss 0.0404486, acc 0.984375, prec 0.0770613, recall 0.900719
2017-12-10T13:40:57.015116: step 3733, loss 0.194778, acc 0.96875, prec 0.077103, recall 0.900777
2017-12-10T13:40:57.446430: step 3734, loss 0.681019, acc 0.9375, prec 0.0771408, recall 0.900834
2017-12-10T13:40:57.896196: step 3735, loss 0.193163, acc 0.953125, prec 0.0771351, recall 0.900834
2017-12-10T13:40:58.346622: step 3736, loss 0.244733, acc 0.953125, prec 0.0771521, recall 0.900862
2017-12-10T13:40:58.792877: step 3737, loss 0.082628, acc 0.96875, prec 0.0771711, recall 0.900891
2017-12-10T13:40:59.244283: step 3738, loss 0.289183, acc 0.90625, prec 0.0771824, recall 0.900919
2017-12-10T13:40:59.699267: step 3739, loss 0.0868497, acc 0.984375, prec 0.0771805, recall 0.900919
2017-12-10T13:41:00.160691: step 3740, loss 0.107411, acc 0.953125, prec 0.0771748, recall 0.900919
2017-12-10T13:41:00.604526: step 3741, loss 0.138576, acc 0.9375, prec 0.0771672, recall 0.900919
2017-12-10T13:41:01.030856: step 3742, loss 0.270049, acc 0.9375, prec 0.0772277, recall 0.901004
2017-12-10T13:41:01.478298: step 3743, loss 0.109334, acc 0.96875, prec 0.0772239, recall 0.901004
2017-12-10T13:41:01.917845: step 3744, loss 0.0117899, acc 1, prec 0.0772466, recall 0.901033
2017-12-10T13:41:02.347335: step 3745, loss 0.108455, acc 0.984375, prec 0.0772447, recall 0.901033
2017-12-10T13:41:02.793205: step 3746, loss 0.43529, acc 0.921875, prec 0.0772352, recall 0.901033
2017-12-10T13:41:03.239203: step 3747, loss 0.0487406, acc 0.984375, prec 0.0772333, recall 0.901033
2017-12-10T13:41:03.688336: step 3748, loss 0.0445242, acc 0.984375, prec 0.0772314, recall 0.901033
2017-12-10T13:41:04.151777: step 3749, loss 0.0265423, acc 0.984375, prec 0.0772295, recall 0.901033
2017-12-10T13:41:04.586868: step 3750, loss 0.677874, acc 0.96875, prec 0.0772484, recall 0.901061
2017-12-10T13:41:05.021090: step 3751, loss 0.231036, acc 0.9375, prec 0.0772408, recall 0.901061
2017-12-10T13:41:05.465534: step 3752, loss 0.0532049, acc 0.984375, prec 0.0772616, recall 0.901089
2017-12-10T13:41:05.906046: step 3753, loss 0.0548803, acc 0.984375, prec 0.0772823, recall 0.901118
2017-12-10T13:41:06.359496: step 3754, loss 0.0693051, acc 0.96875, prec 0.0772785, recall 0.901118
2017-12-10T13:41:06.803767: step 3755, loss 0.00627204, acc 1, prec 0.0773012, recall 0.901146
2017-12-10T13:41:07.258130: step 3756, loss 0.0153872, acc 1, prec 0.0773012, recall 0.901146
2017-12-10T13:41:07.700901: step 3757, loss 0.189266, acc 1, prec 0.0773466, recall 0.901203
2017-12-10T13:41:08.156399: step 3758, loss 1.31914, acc 0.9375, prec 0.0773635, recall 0.900973
2017-12-10T13:41:08.597496: step 3759, loss 0.0345474, acc 0.984375, prec 0.0773616, recall 0.900973
2017-12-10T13:41:09.052427: step 3760, loss 0.0154543, acc 1, prec 0.0773616, recall 0.900973
2017-12-10T13:41:09.496846: step 3761, loss 0.0238684, acc 0.984375, prec 0.0774278, recall 0.901058
2017-12-10T13:41:09.939648: step 3762, loss 0.0291801, acc 0.984375, prec 0.0774485, recall 0.901086
2017-12-10T13:41:10.399304: step 3763, loss 0.0375878, acc 0.984375, prec 0.0774466, recall 0.901086
2017-12-10T13:41:10.841977: step 3764, loss 0.0566874, acc 0.96875, prec 0.0774655, recall 0.901115
2017-12-10T13:41:11.291436: step 3765, loss 0.170866, acc 0.96875, prec 0.077507, recall 0.901171
2017-12-10T13:41:11.743530: step 3766, loss 0.0805013, acc 0.96875, prec 0.0775259, recall 0.901199
2017-12-10T13:41:12.183893: step 3767, loss 0.0437914, acc 0.984375, prec 0.0775466, recall 0.901228
2017-12-10T13:41:12.630373: step 3768, loss 0.0985211, acc 0.96875, prec 0.0775428, recall 0.901228
2017-12-10T13:41:13.079538: step 3769, loss 0.0810714, acc 0.984375, prec 0.0775409, recall 0.901228
2017-12-10T13:41:13.525105: step 3770, loss 0.349836, acc 0.921875, prec 0.0775767, recall 0.901284
2017-12-10T13:41:13.978943: step 3771, loss 0.221165, acc 0.96875, prec 0.0775729, recall 0.901284
2017-12-10T13:41:14.420857: step 3772, loss 0.142132, acc 0.953125, prec 0.0775898, recall 0.901312
2017-12-10T13:41:14.874390: step 3773, loss 0.149302, acc 0.953125, prec 0.0775841, recall 0.901312
2017-12-10T13:41:15.310528: step 3774, loss 0.2526, acc 0.921875, prec 0.0776199, recall 0.901368
2017-12-10T13:41:15.761695: step 3775, loss 0.284135, acc 0.921875, prec 0.0776556, recall 0.901425
2017-12-10T13:41:16.201054: step 3776, loss 0.0123529, acc 1, prec 0.0776556, recall 0.901425
2017-12-10T13:41:16.683440: step 3777, loss 0.501185, acc 0.953125, prec 0.0776725, recall 0.901453
2017-12-10T13:41:17.138164: step 3778, loss 0.274562, acc 0.9375, prec 0.0776649, recall 0.901453
2017-12-10T13:41:17.595868: step 3779, loss 0.197727, acc 0.984375, prec 0.0777083, recall 0.901509
2017-12-10T13:41:18.038134: step 3780, loss 0.137854, acc 0.9375, prec 0.0777233, recall 0.901537
2017-12-10T13:41:18.481137: step 3781, loss 0.170645, acc 0.96875, prec 0.0777194, recall 0.901537
2017-12-10T13:41:18.938127: step 3782, loss 0.0839691, acc 0.953125, prec 0.0777816, recall 0.901621
2017-12-10T13:41:19.384067: step 3783, loss 0.0118909, acc 1, prec 0.0778268, recall 0.901677
2017-12-10T13:41:19.825982: step 3784, loss 0.186963, acc 0.96875, prec 0.0778456, recall 0.901705
2017-12-10T13:41:20.276591: step 3785, loss 0.285995, acc 0.90625, prec 0.077902, recall 0.901788
2017-12-10T13:41:20.722388: step 3786, loss 0.395659, acc 0.96875, prec 0.0779208, recall 0.901816
2017-12-10T13:41:21.174711: step 3787, loss 0.181828, acc 0.96875, prec 0.0779622, recall 0.901872
2017-12-10T13:41:21.618938: step 3788, loss 0.846177, acc 0.9375, prec 0.0779772, recall 0.9019
2017-12-10T13:41:22.057378: step 3789, loss 1.19152, acc 0.953125, prec 0.077994, recall 0.901927
2017-12-10T13:41:22.501888: step 3790, loss 0.10535, acc 0.96875, prec 0.0779902, recall 0.901927
2017-12-10T13:41:22.935284: step 3791, loss 0.0349995, acc 1, prec 0.0779902, recall 0.901927
2017-12-10T13:41:23.377653: step 3792, loss 0.0576928, acc 0.96875, prec 0.0780316, recall 0.901983
2017-12-10T13:41:23.837613: step 3793, loss 0.667169, acc 0.90625, prec 0.0780427, recall 0.902011
2017-12-10T13:41:24.294170: step 3794, loss 0.930513, acc 0.859375, prec 0.0780481, recall 0.902039
2017-12-10T13:41:24.744563: step 3795, loss 0.12726, acc 0.953125, prec 0.0780423, recall 0.902039
2017-12-10T13:41:25.182809: step 3796, loss 0.121179, acc 0.96875, prec 0.0780837, recall 0.902094
2017-12-10T13:41:25.629468: step 3797, loss 0.277907, acc 0.9375, prec 0.0781212, recall 0.902149
2017-12-10T13:41:26.071075: step 3798, loss 0.36856, acc 0.90625, prec 0.0781097, recall 0.902149
2017-12-10T13:41:26.513091: step 3799, loss 0.24113, acc 0.9375, prec 0.078102, recall 0.902149
2017-12-10T13:41:26.976392: step 3800, loss 0.140976, acc 0.953125, prec 0.0781189, recall 0.902177
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3800

2017-12-10T13:41:28.866739: step 3801, loss 0.0325831, acc 1, prec 0.0781415, recall 0.902205
2017-12-10T13:41:29.302053: step 3802, loss 0.148217, acc 0.96875, prec 0.0781376, recall 0.902205
2017-12-10T13:41:29.751867: step 3803, loss 0.170207, acc 0.96875, prec 0.078224, recall 0.902315
2017-12-10T13:41:30.198782: step 3804, loss 0.157737, acc 0.9375, prec 0.0782164, recall 0.902315
2017-12-10T13:41:30.627175: step 3805, loss 0.479689, acc 0.890625, prec 0.078203, recall 0.902315
2017-12-10T13:41:31.076598: step 3806, loss 0.245031, acc 0.9375, prec 0.0781953, recall 0.902315
2017-12-10T13:41:31.519404: step 3807, loss 0.275411, acc 0.859375, prec 0.0782458, recall 0.902398
2017-12-10T13:41:31.965340: step 3808, loss 0.10967, acc 0.953125, prec 0.07824, recall 0.902398
2017-12-10T13:41:32.413979: step 3809, loss 1.35513, acc 0.9375, prec 0.0782568, recall 0.902171
2017-12-10T13:41:32.854257: step 3810, loss 0.365615, acc 0.90625, prec 0.0782679, recall 0.902198
2017-12-10T13:41:33.322071: step 3811, loss 0.118153, acc 0.921875, prec 0.0782809, recall 0.902226
2017-12-10T13:41:33.747807: step 3812, loss 0.211786, acc 0.953125, prec 0.0782976, recall 0.902254
2017-12-10T13:41:34.198720: step 3813, loss 0.445682, acc 0.828125, prec 0.0782766, recall 0.902254
2017-12-10T13:41:34.646710: step 3814, loss 0.168275, acc 0.890625, prec 0.0782632, recall 0.902254
2017-12-10T13:41:35.092693: step 3815, loss 0.194767, acc 0.953125, prec 0.0783025, recall 0.902309
2017-12-10T13:41:35.538923: step 3816, loss 0.265509, acc 0.953125, prec 0.0782968, recall 0.902309
2017-12-10T13:41:35.989451: step 3817, loss 0.313559, acc 0.9375, prec 0.0782891, recall 0.902309
2017-12-10T13:41:36.470386: step 3818, loss 0.139641, acc 0.9375, prec 0.078304, recall 0.902336
2017-12-10T13:41:36.920176: step 3819, loss 0.0686332, acc 0.953125, prec 0.0782982, recall 0.902336
2017-12-10T13:41:37.362391: step 3820, loss 0.398032, acc 0.890625, prec 0.0783299, recall 0.902391
2017-12-10T13:41:37.812658: step 3821, loss 0.130435, acc 0.984375, prec 0.0783505, recall 0.902418
2017-12-10T13:41:38.255940: step 3822, loss 0.304076, acc 0.921875, prec 0.0784084, recall 0.902501
2017-12-10T13:41:38.698272: step 3823, loss 1.76821, acc 0.890625, prec 0.0783969, recall 0.902247
2017-12-10T13:41:39.151495: step 3824, loss 0.271543, acc 0.90625, prec 0.0784079, recall 0.902275
2017-12-10T13:41:39.594524: step 3825, loss 0.115829, acc 0.96875, prec 0.0784041, recall 0.902275
2017-12-10T13:41:40.043008: step 3826, loss 0.754968, acc 0.96875, prec 0.0784228, recall 0.902302
2017-12-10T13:41:40.501110: step 3827, loss 0.22298, acc 0.921875, prec 0.0784132, recall 0.902302
2017-12-10T13:41:40.936544: step 3828, loss 0.264359, acc 0.9375, prec 0.078473, recall 0.902384
2017-12-10T13:41:41.379107: step 3829, loss 0.654057, acc 0.84375, prec 0.0784763, recall 0.902412
2017-12-10T13:41:41.829737: step 3830, loss 0.451717, acc 0.84375, prec 0.0784572, recall 0.902412
2017-12-10T13:41:42.275728: step 3831, loss 0.165455, acc 0.921875, prec 0.0784701, recall 0.902439
2017-12-10T13:41:42.726649: step 3832, loss 0.470868, acc 0.90625, prec 0.0784811, recall 0.902466
2017-12-10T13:41:43.172690: step 3833, loss 0.476591, acc 0.875, prec 0.0784882, recall 0.902494
2017-12-10T13:41:43.617219: step 3834, loss 0.216155, acc 0.90625, prec 0.0784768, recall 0.902494
2017-12-10T13:41:44.065099: step 3835, loss 0.245563, acc 0.890625, prec 0.0784634, recall 0.902494
2017-12-10T13:41:44.510271: step 3836, loss 0.547355, acc 0.859375, prec 0.0784462, recall 0.902494
2017-12-10T13:41:44.959849: step 3837, loss 0.150861, acc 0.96875, prec 0.0784872, recall 0.902548
2017-12-10T13:41:45.413842: step 3838, loss 0.33274, acc 0.90625, prec 0.0784982, recall 0.902576
2017-12-10T13:41:45.859615: step 3839, loss 0.296997, acc 0.9375, prec 0.078513, recall 0.902603
2017-12-10T13:41:46.321818: step 3840, loss 0.262379, acc 0.921875, prec 0.0785034, recall 0.902603
2017-12-10T13:41:46.768766: step 3841, loss 0.351997, acc 0.921875, prec 0.0785612, recall 0.902685
2017-12-10T13:41:47.209847: step 3842, loss 0.126177, acc 0.9375, prec 0.0785535, recall 0.902685
2017-12-10T13:41:47.658420: step 3843, loss 0.0836156, acc 0.953125, prec 0.0785702, recall 0.902712
2017-12-10T13:41:48.103180: step 3844, loss 0.211268, acc 0.984375, prec 0.0785907, recall 0.902739
2017-12-10T13:41:48.549100: step 3845, loss 0.0301122, acc 0.984375, prec 0.0785888, recall 0.902739
2017-12-10T13:41:48.998818: step 3846, loss 0.0350902, acc 0.96875, prec 0.0786074, recall 0.902766
2017-12-10T13:41:49.437511: step 3847, loss 0.291767, acc 0.9375, prec 0.0786446, recall 0.90282
2017-12-10T13:41:49.892766: step 3848, loss 0.113384, acc 0.9375, prec 0.0786369, recall 0.90282
2017-12-10T13:41:50.335286: step 3849, loss 0.0576451, acc 0.984375, prec 0.0786574, recall 0.902848
2017-12-10T13:41:50.777697: step 3850, loss 0.00852767, acc 1, prec 0.0786574, recall 0.902848
2017-12-10T13:41:51.219905: step 3851, loss 0.217613, acc 0.96875, prec 0.0786536, recall 0.902848
2017-12-10T13:41:51.664455: step 3852, loss 0.0358054, acc 0.984375, prec 0.0786517, recall 0.902848
2017-12-10T13:41:52.107771: step 3853, loss 0.0409859, acc 0.984375, prec 0.0786498, recall 0.902848
2017-12-10T13:41:52.550113: step 3854, loss 0.249735, acc 0.96875, prec 0.0786459, recall 0.902848
2017-12-10T13:41:52.996898: step 3855, loss 0.752712, acc 0.984375, prec 0.0786888, recall 0.902902
2017-12-10T13:41:53.451028: step 3856, loss 0.0270969, acc 0.984375, prec 0.0786869, recall 0.902902
2017-12-10T13:41:53.917723: step 3857, loss 0.0869818, acc 0.96875, prec 0.0786831, recall 0.902902
2017-12-10T13:41:54.364496: step 3858, loss 0.0878811, acc 0.953125, prec 0.0787222, recall 0.902956
2017-12-10T13:41:54.807180: step 3859, loss 0.240813, acc 0.953125, prec 0.0787388, recall 0.902983
2017-12-10T13:41:55.250477: step 3860, loss 0.011077, acc 1, prec 0.0787388, recall 0.902983
2017-12-10T13:41:55.699511: step 3861, loss 0.0647147, acc 0.96875, prec 0.078735, recall 0.902983
2017-12-10T13:41:56.147097: step 3862, loss 0.0488194, acc 0.984375, prec 0.0787331, recall 0.902983
2017-12-10T13:41:56.603347: step 3863, loss 0.50192, acc 0.96875, prec 0.078774, recall 0.903037
2017-12-10T13:41:57.053606: step 3864, loss 0.0417877, acc 0.96875, prec 0.0787926, recall 0.903064
2017-12-10T13:41:57.504161: step 3865, loss 0.118813, acc 1, prec 0.0788374, recall 0.903118
2017-12-10T13:41:57.949411: step 3866, loss 0.0525281, acc 0.984375, prec 0.0788355, recall 0.903118
2017-12-10T13:41:58.392589: step 3867, loss 0.108723, acc 0.96875, prec 0.078854, recall 0.903145
2017-12-10T13:41:58.842324: step 3868, loss 0.329133, acc 0.96875, prec 0.0788726, recall 0.903172
2017-12-10T13:41:59.284346: step 3869, loss 0.18665, acc 0.921875, prec 0.0788854, recall 0.903199
2017-12-10T13:41:59.727620: step 3870, loss 0.0507132, acc 0.96875, prec 0.0789039, recall 0.903226
2017-12-10T13:42:00.182761: step 3871, loss 0.140977, acc 0.953125, prec 0.0788981, recall 0.903226
2017-12-10T13:42:00.633344: step 3872, loss 0.129056, acc 0.953125, prec 0.0789371, recall 0.90328
2017-12-10T13:42:01.077349: step 3873, loss 0.0719265, acc 0.984375, prec 0.0789352, recall 0.90328
2017-12-10T13:42:01.523479: step 3874, loss 0.0781104, acc 0.984375, prec 0.0789333, recall 0.90328
2017-12-10T13:42:01.980619: step 3875, loss 0.0946746, acc 0.96875, prec 0.0789742, recall 0.903333
2017-12-10T13:42:02.424034: step 3876, loss 1.04144, acc 0.96875, prec 0.0790375, recall 0.903414
2017-12-10T13:42:02.882630: step 3877, loss 0.106496, acc 0.984375, prec 0.0790803, recall 0.903467
2017-12-10T13:42:03.329444: step 3878, loss 0.0355122, acc 0.984375, prec 0.0790783, recall 0.903467
2017-12-10T13:42:03.764796: step 3879, loss 0.0380279, acc 0.96875, prec 0.0790969, recall 0.903494
2017-12-10T13:42:04.213870: step 3880, loss 0.0193468, acc 0.984375, prec 0.0791173, recall 0.903521
2017-12-10T13:42:04.648995: step 3881, loss 0.348922, acc 0.953125, prec 0.0791786, recall 0.903601
2017-12-10T13:42:05.101538: step 3882, loss 0.03754, acc 0.984375, prec 0.079199, recall 0.903628
2017-12-10T13:42:05.547142: step 3883, loss 0.639325, acc 0.984375, prec 0.0792641, recall 0.903708
2017-12-10T13:42:05.981470: step 3884, loss 0.195087, acc 0.953125, prec 0.0792807, recall 0.903734
2017-12-10T13:42:06.419281: step 3885, loss 0.024742, acc 1, prec 0.0792807, recall 0.903734
2017-12-10T13:42:06.868491: step 3886, loss 0.0308592, acc 0.984375, prec 0.0792788, recall 0.903734
2017-12-10T13:42:07.315280: step 3887, loss 0.0676283, acc 0.96875, prec 0.0793196, recall 0.903788
2017-12-10T13:42:07.754839: step 3888, loss 0.181767, acc 0.953125, prec 0.0793362, recall 0.903814
2017-12-10T13:42:08.189240: step 3889, loss 0.0788026, acc 0.953125, prec 0.0793751, recall 0.903867
2017-12-10T13:42:08.637333: step 3890, loss 0.2472, acc 0.9375, prec 0.0793674, recall 0.903867
2017-12-10T13:42:09.097838: step 3891, loss 0.587282, acc 0.90625, prec 0.0793782, recall 0.903894
2017-12-10T13:42:09.547623: step 3892, loss 0.356087, acc 0.953125, prec 0.079417, recall 0.903947
2017-12-10T13:42:09.998229: step 3893, loss 0.211639, acc 0.96875, prec 0.0794355, recall 0.903974
2017-12-10T13:42:10.441992: step 3894, loss 0.451504, acc 0.921875, prec 0.0794259, recall 0.903974
2017-12-10T13:42:10.886656: step 3895, loss 0.0852064, acc 0.96875, prec 0.079422, recall 0.903974
2017-12-10T13:42:11.334823: step 3896, loss 0.0921058, acc 0.96875, prec 0.0794405, recall 0.904
2017-12-10T13:42:11.788472: step 3897, loss 0.0377889, acc 0.984375, prec 0.0794386, recall 0.904
2017-12-10T13:42:12.221387: step 3898, loss 0.0830265, acc 0.953125, prec 0.0794551, recall 0.904026
2017-12-10T13:42:12.654447: step 3899, loss 0.152619, acc 0.953125, prec 0.0794716, recall 0.904053
2017-12-10T13:42:13.093135: step 3900, loss 0.0825286, acc 0.96875, prec 0.0794678, recall 0.904053
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-3900

2017-12-10T13:42:15.116418: step 3901, loss 0.0916689, acc 0.96875, prec 0.0794639, recall 0.904053
2017-12-10T13:42:15.569167: step 3902, loss 0.130456, acc 0.96875, prec 0.0794601, recall 0.904053
2017-12-10T13:42:16.024296: step 3903, loss 0.0324216, acc 1, prec 0.0794824, recall 0.904079
2017-12-10T13:42:16.466485: step 3904, loss 0.512512, acc 0.9375, prec 0.0795193, recall 0.904132
2017-12-10T13:42:16.920616: step 3905, loss 0.00920563, acc 1, prec 0.0795416, recall 0.904159
2017-12-10T13:42:17.361806: step 3906, loss 0.1022, acc 0.953125, prec 0.0795358, recall 0.904159
2017-12-10T13:42:17.806324: step 3907, loss 0.101006, acc 0.953125, prec 0.0795523, recall 0.904185
2017-12-10T13:42:18.241667: step 3908, loss 0.212323, acc 0.953125, prec 0.0795688, recall 0.904211
2017-12-10T13:42:18.697037: step 3909, loss 0.180106, acc 0.96875, prec 0.079565, recall 0.904211
2017-12-10T13:42:19.148323: step 3910, loss 0.22668, acc 0.90625, prec 0.0795534, recall 0.904211
2017-12-10T13:42:19.588606: step 3911, loss 0.0288128, acc 0.984375, prec 0.0795515, recall 0.904211
2017-12-10T13:42:20.042837: step 3912, loss 0.0049727, acc 1, prec 0.0795738, recall 0.904238
2017-12-10T13:42:20.497427: step 3913, loss 0.0290188, acc 0.984375, prec 0.0795942, recall 0.904264
2017-12-10T13:42:20.949607: step 3914, loss 0.0878618, acc 1, prec 0.0796164, recall 0.90429
2017-12-10T13:42:21.400410: step 3915, loss 0.0026443, acc 1, prec 0.0796387, recall 0.904317
2017-12-10T13:42:21.831764: step 3916, loss 0.0982491, acc 0.96875, prec 0.0796794, recall 0.904369
2017-12-10T13:42:22.270765: step 3917, loss 0.00889973, acc 1, prec 0.0796794, recall 0.904369
2017-12-10T13:42:22.708166: step 3918, loss 0.00920771, acc 1, prec 0.0796794, recall 0.904369
2017-12-10T13:42:23.148890: step 3919, loss 0.0341209, acc 0.984375, prec 0.0796775, recall 0.904369
2017-12-10T13:42:23.617352: step 3920, loss 0.00517, acc 1, prec 0.0796775, recall 0.904369
2017-12-10T13:42:24.067178: step 3921, loss 0.118676, acc 0.984375, prec 0.0796979, recall 0.904396
2017-12-10T13:42:24.518847: step 3922, loss 0.123077, acc 1, prec 0.0797201, recall 0.904422
2017-12-10T13:42:24.959075: step 3923, loss 0.0579214, acc 0.984375, prec 0.0797405, recall 0.904448
2017-12-10T13:42:25.407458: step 3924, loss 5.76806, acc 0.953125, prec 0.0797831, recall 0.904004
2017-12-10T13:42:25.850568: step 3925, loss 0.620799, acc 0.96875, prec 0.0798015, recall 0.904031
2017-12-10T13:42:26.292599: step 3926, loss 0.141376, acc 0.96875, prec 0.0798199, recall 0.904057
2017-12-10T13:42:26.740466: step 3927, loss 0.124018, acc 0.96875, prec 0.0798161, recall 0.904057
2017-12-10T13:42:27.187007: step 3928, loss 0.265702, acc 0.921875, prec 0.0798287, recall 0.904083
2017-12-10T13:42:27.635383: step 3929, loss 0.222065, acc 0.9375, prec 0.0798432, recall 0.90411
2017-12-10T13:42:28.078621: step 3930, loss 0.246993, acc 0.953125, prec 0.0798819, recall 0.904162
2017-12-10T13:42:28.522468: step 3931, loss 0.335965, acc 0.875, prec 0.0798887, recall 0.904188
2017-12-10T13:42:28.964808: step 3932, loss 0.374919, acc 0.875, prec 0.07994, recall 0.904267
2017-12-10T13:42:29.404010: step 3933, loss 0.410793, acc 0.859375, prec 0.0799226, recall 0.904267
2017-12-10T13:42:29.840774: step 3934, loss 0.710005, acc 0.8125, prec 0.0798995, recall 0.904267
2017-12-10T13:42:30.280436: step 3935, loss 0.382217, acc 0.875, prec 0.079884, recall 0.904267
2017-12-10T13:42:30.723971: step 3936, loss 0.358152, acc 0.921875, prec 0.0798744, recall 0.904267
2017-12-10T13:42:31.172645: step 3937, loss 0.291576, acc 0.921875, prec 0.0798647, recall 0.904267
2017-12-10T13:42:31.610873: step 3938, loss 0.411798, acc 0.859375, prec 0.0798474, recall 0.904267
2017-12-10T13:42:32.064870: step 3939, loss 0.52731, acc 0.875, prec 0.0798986, recall 0.904345
2017-12-10T13:42:32.511092: step 3940, loss 0.4566, acc 0.890625, prec 0.0798851, recall 0.904345
2017-12-10T13:42:32.952428: step 3941, loss 0.128917, acc 0.9375, prec 0.0798996, recall 0.904372
2017-12-10T13:42:33.408836: step 3942, loss 0.223367, acc 0.96875, prec 0.0799179, recall 0.904398
2017-12-10T13:42:33.863270: step 3943, loss 0.300803, acc 0.875, prec 0.0799247, recall 0.904424
2017-12-10T13:42:34.307746: step 3944, loss 0.471012, acc 0.90625, prec 0.0799353, recall 0.90445
2017-12-10T13:42:34.755365: step 3945, loss 1.40433, acc 0.890625, prec 0.080055, recall 0.904606
2017-12-10T13:42:35.202502: step 3946, loss 0.275777, acc 0.859375, prec 0.0800376, recall 0.904606
2017-12-10T13:42:35.654232: step 3947, loss 0.463606, acc 0.90625, prec 0.0800926, recall 0.904684
2017-12-10T13:42:36.102638: step 3948, loss 0.32316, acc 0.90625, prec 0.0801032, recall 0.90471
2017-12-10T13:42:36.541896: step 3949, loss 0.137541, acc 0.9375, prec 0.0800954, recall 0.90471
2017-12-10T13:42:36.991466: step 3950, loss 0.382662, acc 0.921875, prec 0.0800858, recall 0.90471
2017-12-10T13:42:37.425271: step 3951, loss 0.0355675, acc 1, prec 0.0800858, recall 0.90471
2017-12-10T13:42:37.868849: step 3952, loss 0.506538, acc 0.921875, prec 0.0800983, recall 0.904736
2017-12-10T13:42:38.300802: step 3953, loss 0.0792515, acc 0.984375, prec 0.0800964, recall 0.904736
2017-12-10T13:42:38.747111: step 3954, loss 0.346151, acc 0.9375, prec 0.0800887, recall 0.904736
2017-12-10T13:42:39.198464: step 3955, loss 0.489836, acc 0.9375, prec 0.0801031, recall 0.904762
2017-12-10T13:42:39.629713: step 3956, loss 0.0719706, acc 0.96875, prec 0.0801214, recall 0.904788
2017-12-10T13:42:40.080842: step 3957, loss 0.476733, acc 0.890625, prec 0.0801079, recall 0.904788
2017-12-10T13:42:40.524029: step 3958, loss 0.161166, acc 0.953125, prec 0.0801243, recall 0.904814
2017-12-10T13:42:40.972748: step 3959, loss 0.782712, acc 0.859375, prec 0.0801955, recall 0.904917
2017-12-10T13:42:41.421805: step 3960, loss 0.521924, acc 0.921875, prec 0.0802523, recall 0.904995
2017-12-10T13:42:41.872463: step 3961, loss 0.189703, acc 0.953125, prec 0.0802465, recall 0.904995
2017-12-10T13:42:42.313562: step 3962, loss 0.337915, acc 0.953125, prec 0.0802849, recall 0.905046
2017-12-10T13:42:42.777703: step 3963, loss 0.137422, acc 0.96875, prec 0.0802811, recall 0.905046
2017-12-10T13:42:43.224236: step 3964, loss 0.241622, acc 0.9375, prec 0.0802955, recall 0.905072
2017-12-10T13:42:43.674787: step 3965, loss 0.453229, acc 0.9375, prec 0.080332, recall 0.905123
2017-12-10T13:42:44.132573: step 3966, loss 0.0793733, acc 0.96875, prec 0.0803945, recall 0.9052
2017-12-10T13:42:44.568053: step 3967, loss 0.0155616, acc 1, prec 0.0803945, recall 0.9052
2017-12-10T13:42:45.021007: step 3968, loss 0.121286, acc 0.96875, prec 0.0803906, recall 0.9052
2017-12-10T13:42:45.465532: step 3969, loss 0.199303, acc 0.90625, prec 0.0804233, recall 0.905252
2017-12-10T13:42:45.919839: step 3970, loss 0.227671, acc 0.953125, prec 0.0804617, recall 0.905303
2017-12-10T13:42:46.364531: step 3971, loss 0.0637283, acc 0.953125, prec 0.0805001, recall 0.905354
2017-12-10T13:42:46.809032: step 3972, loss 0.0856339, acc 0.9375, prec 0.0804924, recall 0.905354
2017-12-10T13:42:47.262419: step 3973, loss 0.372027, acc 0.96875, prec 0.0805327, recall 0.905405
2017-12-10T13:42:47.709484: step 3974, loss 0.252023, acc 0.953125, prec 0.0805269, recall 0.905405
2017-12-10T13:42:48.161842: step 3975, loss 0.261409, acc 0.953125, prec 0.0805432, recall 0.905431
2017-12-10T13:42:48.555671: step 3976, loss 0.167457, acc 0.980769, prec 0.0806076, recall 0.905508
2017-12-10T13:42:49.007143: step 3977, loss 0.0136032, acc 1, prec 0.0806076, recall 0.905508
2017-12-10T13:42:49.451199: step 3978, loss 0.414087, acc 0.9375, prec 0.0806661, recall 0.905584
2017-12-10T13:42:49.907617: step 3979, loss 0.128589, acc 0.96875, prec 0.0807064, recall 0.905635
2017-12-10T13:42:50.344856: step 3980, loss 0.314628, acc 0.90625, prec 0.0807389, recall 0.905686
2017-12-10T13:42:50.783069: step 3981, loss 0.161994, acc 0.953125, prec 0.0807331, recall 0.905686
2017-12-10T13:42:51.227353: step 3982, loss 0.0438801, acc 0.984375, prec 0.0807533, recall 0.905711
2017-12-10T13:42:51.673870: step 3983, loss 0.235765, acc 0.9375, prec 0.0807455, recall 0.905711
2017-12-10T13:42:52.124919: step 3984, loss 0.0713169, acc 0.96875, prec 0.0807416, recall 0.905711
2017-12-10T13:42:52.582872: step 3985, loss 0.0291592, acc 0.984375, prec 0.0807397, recall 0.905711
2017-12-10T13:42:53.018601: step 3986, loss 0.0458716, acc 0.984375, prec 0.0807819, recall 0.905762
2017-12-10T13:42:53.464337: step 3987, loss 0.0628412, acc 0.984375, prec 0.080802, recall 0.905787
2017-12-10T13:42:53.916166: step 3988, loss 0.0170104, acc 0.984375, prec 0.0808001, recall 0.905787
2017-12-10T13:42:54.356282: step 3989, loss 0.152932, acc 0.953125, prec 0.0808163, recall 0.905813
2017-12-10T13:42:54.812883: step 3990, loss 0.256655, acc 0.9375, prec 0.0808527, recall 0.905863
2017-12-10T13:42:55.251573: step 3991, loss 0.00568494, acc 1, prec 0.0808527, recall 0.905863
2017-12-10T13:42:55.685128: step 3992, loss 0.178756, acc 0.953125, prec 0.0808469, recall 0.905863
2017-12-10T13:42:56.130898: step 3993, loss 0.08937, acc 1, prec 0.080891, recall 0.905914
2017-12-10T13:42:56.571588: step 3994, loss 0.154588, acc 0.984375, prec 0.0809332, recall 0.905965
2017-12-10T13:42:57.008144: step 3995, loss 0.0848784, acc 0.953125, prec 0.0809273, recall 0.905965
2017-12-10T13:42:57.459637: step 3996, loss 0.194885, acc 0.953125, prec 0.0809215, recall 0.905965
2017-12-10T13:42:57.907087: step 3997, loss 0.063057, acc 0.984375, prec 0.0809637, recall 0.906015
2017-12-10T13:42:58.350538: step 3998, loss 0.103774, acc 0.96875, prec 0.0810039, recall 0.906065
2017-12-10T13:42:58.800238: step 3999, loss 0.123705, acc 0.96875, prec 0.0810441, recall 0.906116
2017-12-10T13:42:59.244560: step 4000, loss 0.0544348, acc 1, prec 0.0810662, recall 0.906141

Evaluation:
2017-12-10T13:43:08.673923: step 4000, loss 4.60202, acc 0.964993, prec 0.081567, recall 0.881823

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4000

2017-12-10T13:43:10.527649: step 4001, loss 0.0299927, acc 1, prec 0.0816327, recall 0.881914
2017-12-10T13:43:10.963764: step 4002, loss 0.0517647, acc 0.984375, prec 0.0816744, recall 0.881975
2017-12-10T13:43:11.398242: step 4003, loss 0.0373364, acc 0.96875, prec 0.0816924, recall 0.882005
2017-12-10T13:43:11.839793: step 4004, loss 0.0570823, acc 0.96875, prec 0.0816885, recall 0.882005
2017-12-10T13:43:12.287489: step 4005, loss 0.0919285, acc 1, prec 0.0817104, recall 0.882035
2017-12-10T13:43:12.734763: step 4006, loss 0.247955, acc 0.984375, prec 0.0817084, recall 0.882035
2017-12-10T13:43:13.181689: step 4007, loss 0.0409408, acc 0.984375, prec 0.0817065, recall 0.882035
2017-12-10T13:43:13.631550: step 4008, loss 0.0407825, acc 0.984375, prec 0.0817046, recall 0.882035
2017-12-10T13:43:14.077950: step 4009, loss 0.0544972, acc 0.984375, prec 0.0817245, recall 0.882066
2017-12-10T13:43:14.505499: step 4010, loss 0.0375428, acc 0.984375, prec 0.0817225, recall 0.882066
2017-12-10T13:43:14.948957: step 4011, loss 0.0203607, acc 0.984375, prec 0.0817424, recall 0.882096
2017-12-10T13:43:15.400695: step 4012, loss 0.370667, acc 0.953125, prec 0.0817585, recall 0.882126
2017-12-10T13:43:15.841114: step 4013, loss 0.15084, acc 0.96875, prec 0.0817983, recall 0.882187
2017-12-10T13:43:16.287292: step 4014, loss 0.0266584, acc 0.984375, prec 0.0817963, recall 0.882187
2017-12-10T13:43:16.740361: step 4015, loss 0.082998, acc 0.984375, prec 0.0818162, recall 0.882217
2017-12-10T13:43:17.193814: step 4016, loss 0.0255602, acc 1, prec 0.0818381, recall 0.882247
2017-12-10T13:43:17.653722: step 4017, loss 0.0425227, acc 1, prec 0.0818599, recall 0.882277
2017-12-10T13:43:18.102015: step 4018, loss 0.166202, acc 0.984375, prec 0.0819017, recall 0.882338
2017-12-10T13:43:18.545351: step 4019, loss 0.0203193, acc 1, prec 0.0819017, recall 0.882338
2017-12-10T13:43:18.984647: step 4020, loss 0.0295197, acc 1, prec 0.0819235, recall 0.882368
2017-12-10T13:43:19.428767: step 4021, loss 0.0163718, acc 1, prec 0.0819235, recall 0.882368
2017-12-10T13:43:19.887168: step 4022, loss 0.170945, acc 0.96875, prec 0.0819196, recall 0.882368
2017-12-10T13:43:20.338991: step 4023, loss 0.108108, acc 0.96875, prec 0.0819157, recall 0.882368
2017-12-10T13:43:20.795109: step 4024, loss 0.0164726, acc 1, prec 0.0819594, recall 0.882428
2017-12-10T13:43:21.243324: step 4025, loss 0.216123, acc 0.984375, prec 0.0819793, recall 0.882458
2017-12-10T13:43:21.688994: step 4026, loss 0.0888321, acc 0.984375, prec 0.0819774, recall 0.882458
2017-12-10T13:43:22.137532: step 4027, loss 0.0519278, acc 1, prec 0.0820429, recall 0.882549
2017-12-10T13:43:22.590194: step 4028, loss 1.92359, acc 0.96875, prec 0.0820409, recall 0.882323
2017-12-10T13:43:23.029103: step 4029, loss 0.32111, acc 0.984375, prec 0.0820608, recall 0.882353
2017-12-10T13:43:23.465412: step 4030, loss 0.106927, acc 0.96875, prec 0.0820569, recall 0.882353
2017-12-10T13:43:23.916392: step 4031, loss 0.160437, acc 0.96875, prec 0.082053, recall 0.882353
2017-12-10T13:43:24.363759: step 4032, loss 0.065877, acc 0.984375, prec 0.0820729, recall 0.882383
2017-12-10T13:43:24.825921: step 4033, loss 0.107031, acc 0.953125, prec 0.082067, recall 0.882383
2017-12-10T13:43:25.275802: step 4034, loss 0.165083, acc 0.921875, prec 0.0820573, recall 0.882383
2017-12-10T13:43:25.723929: step 4035, loss 0.0641803, acc 0.96875, prec 0.0820534, recall 0.882383
2017-12-10T13:43:26.172341: step 4036, loss 0.202352, acc 0.9375, prec 0.0820674, recall 0.882413
2017-12-10T13:43:26.617576: step 4037, loss 0.26322, acc 0.9375, prec 0.0820814, recall 0.882443
2017-12-10T13:43:27.062614: step 4038, loss 0.360913, acc 0.875, prec 0.0820876, recall 0.882473
2017-12-10T13:43:27.514011: step 4039, loss 0.0955251, acc 0.953125, prec 0.0820818, recall 0.882473
2017-12-10T13:43:27.966952: step 4040, loss 0.512219, acc 0.875, prec 0.082088, recall 0.882503
2017-12-10T13:43:28.426619: step 4041, loss 0.36526, acc 0.921875, prec 0.0820782, recall 0.882503
2017-12-10T13:43:28.874886: step 4042, loss 0.305136, acc 0.875, prec 0.0820626, recall 0.882503
2017-12-10T13:43:29.318953: step 4043, loss 0.621879, acc 0.875, prec 0.0820688, recall 0.882533
2017-12-10T13:43:29.775816: step 4044, loss 0.237011, acc 0.9375, prec 0.082061, recall 0.882533
2017-12-10T13:43:30.214954: step 4045, loss 0.24914, acc 0.9375, prec 0.0820532, recall 0.882533
2017-12-10T13:43:30.647344: step 4046, loss 0.24551, acc 0.953125, prec 0.0820692, recall 0.882563
2017-12-10T13:43:31.090064: step 4047, loss 0.0472532, acc 0.96875, prec 0.0820871, recall 0.882593
2017-12-10T13:43:31.542741: step 4048, loss 0.0870631, acc 0.96875, prec 0.0820832, recall 0.882593
2017-12-10T13:43:31.992087: step 4049, loss 0.188733, acc 0.9375, prec 0.0820754, recall 0.882593
2017-12-10T13:43:32.430036: step 4050, loss 0.0743562, acc 0.96875, prec 0.0820715, recall 0.882593
2017-12-10T13:43:32.861771: step 4051, loss 0.117093, acc 0.96875, prec 0.0820894, recall 0.882623
2017-12-10T13:43:33.306999: step 4052, loss 0.00456013, acc 1, prec 0.0820894, recall 0.882623
2017-12-10T13:43:33.758136: step 4053, loss 5.86206, acc 0.921875, prec 0.0821034, recall 0.882428
2017-12-10T13:43:34.222541: step 4054, loss 0.0927315, acc 0.9375, prec 0.0821173, recall 0.882458
2017-12-10T13:43:34.672096: step 4055, loss 0.19773, acc 0.96875, prec 0.082157, recall 0.882518
2017-12-10T13:43:35.123987: step 4056, loss 0.0501809, acc 0.984375, prec 0.0821551, recall 0.882518
2017-12-10T13:43:35.570556: step 4057, loss 0.263209, acc 0.9375, prec 0.0821473, recall 0.882518
2017-12-10T13:43:36.015010: step 4058, loss 0.342112, acc 0.921875, prec 0.0821593, recall 0.882548
2017-12-10T13:43:36.468787: step 4059, loss 0.527002, acc 0.875, prec 0.0821655, recall 0.882578
2017-12-10T13:43:36.910183: step 4060, loss 0.53823, acc 0.890625, prec 0.0821736, recall 0.882608
2017-12-10T13:43:37.348213: step 4061, loss 0.369846, acc 0.90625, prec 0.0821837, recall 0.882638
2017-12-10T13:43:37.788492: step 4062, loss 0.283953, acc 0.953125, prec 0.0821996, recall 0.882667
2017-12-10T13:43:38.233995: step 4063, loss 0.318425, acc 0.953125, prec 0.0822155, recall 0.882697
2017-12-10T13:43:38.683153: step 4064, loss 0.217871, acc 0.921875, prec 0.0822057, recall 0.882697
2017-12-10T13:43:39.130547: step 4065, loss 0.217467, acc 0.90625, prec 0.0821941, recall 0.882697
2017-12-10T13:43:39.572962: step 4066, loss 0.27306, acc 0.96875, prec 0.0821902, recall 0.882697
2017-12-10T13:43:40.019530: step 4067, loss 1.77218, acc 0.9375, prec 0.0822278, recall 0.882532
2017-12-10T13:43:40.475372: step 4068, loss 0.0744744, acc 0.953125, prec 0.0822437, recall 0.882562
2017-12-10T13:43:40.903337: step 4069, loss 0.081437, acc 0.984375, prec 0.0822418, recall 0.882562
2017-12-10T13:43:41.343665: step 4070, loss 0.230734, acc 0.9375, prec 0.0822557, recall 0.882592
2017-12-10T13:43:41.790430: step 4071, loss 0.151625, acc 0.921875, prec 0.082246, recall 0.882592
2017-12-10T13:43:42.248745: step 4072, loss 0.229367, acc 0.953125, prec 0.0822401, recall 0.882592
2017-12-10T13:43:42.693336: step 4073, loss 0.522216, acc 0.890625, prec 0.0822265, recall 0.882592
2017-12-10T13:43:43.146143: step 4074, loss 0.414598, acc 0.9375, prec 0.0822187, recall 0.882592
2017-12-10T13:43:43.598433: step 4075, loss 0.175076, acc 0.9375, prec 0.0822761, recall 0.882682
2017-12-10T13:43:44.045525: step 4076, loss 0.737482, acc 0.90625, prec 0.0822861, recall 0.882711
2017-12-10T13:43:44.494315: step 4077, loss 0.392447, acc 0.921875, prec 0.0822981, recall 0.882741
2017-12-10T13:43:44.938577: step 4078, loss 0.319363, acc 0.9375, prec 0.082312, recall 0.882771
2017-12-10T13:43:45.383308: step 4079, loss 0.141687, acc 0.9375, prec 0.0823477, recall 0.88283
2017-12-10T13:43:45.817844: step 4080, loss 0.455995, acc 0.875, prec 0.0823321, recall 0.88283
2017-12-10T13:43:46.262720: step 4081, loss 0.374222, acc 0.90625, prec 0.0823638, recall 0.88289
2017-12-10T13:43:46.700714: step 4082, loss 0.405037, acc 0.890625, prec 0.0823935, recall 0.882949
2017-12-10T13:43:47.141741: step 4083, loss 0.172835, acc 0.9375, prec 0.0823858, recall 0.882949
2017-12-10T13:43:47.598414: step 4084, loss 0.218019, acc 0.96875, prec 0.0824252, recall 0.883008
2017-12-10T13:43:48.053142: step 4085, loss 0.0839658, acc 0.96875, prec 0.0824213, recall 0.883008
2017-12-10T13:43:48.503950: step 4086, loss 0.056825, acc 0.984375, prec 0.0824411, recall 0.883038
2017-12-10T13:43:48.945670: step 4087, loss 0.145656, acc 0.96875, prec 0.0824372, recall 0.883038
2017-12-10T13:43:49.387829: step 4088, loss 0.0486237, acc 0.984375, prec 0.0824569, recall 0.883068
2017-12-10T13:43:49.833331: step 4089, loss 0.264225, acc 0.953125, prec 0.0824728, recall 0.883097
2017-12-10T13:43:50.291998: step 4090, loss 0.0618621, acc 0.984375, prec 0.0824708, recall 0.883097
2017-12-10T13:43:50.756929: step 4091, loss 0.115165, acc 0.953125, prec 0.082465, recall 0.883097
2017-12-10T13:43:51.202838: step 4092, loss 0.880454, acc 0.984375, prec 0.0825281, recall 0.883186
2017-12-10T13:43:51.647025: step 4093, loss 0.164668, acc 0.984375, prec 0.0825478, recall 0.883215
2017-12-10T13:43:52.094560: step 4094, loss 0.0354235, acc 0.984375, prec 0.0825675, recall 0.883245
2017-12-10T13:43:52.547288: step 4095, loss 0.0603974, acc 0.984375, prec 0.0825872, recall 0.883274
2017-12-10T13:43:52.987675: step 4096, loss 0.400571, acc 0.953125, prec 0.0826247, recall 0.883333
2017-12-10T13:43:53.434497: step 4097, loss 0.26122, acc 0.953125, prec 0.0826189, recall 0.883333
2017-12-10T13:43:53.890123: step 4098, loss 0.139103, acc 1, prec 0.0826622, recall 0.883392
2017-12-10T13:43:54.327483: step 4099, loss 0.100051, acc 0.953125, prec 0.0826563, recall 0.883392
2017-12-10T13:43:54.761670: step 4100, loss 0.00459637, acc 1, prec 0.0826563, recall 0.883392
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4100

2017-12-10T13:43:56.658923: step 4101, loss 0.117154, acc 0.96875, prec 0.0827174, recall 0.88348
2017-12-10T13:43:57.118995: step 4102, loss 4.50122, acc 0.984375, prec 0.0827391, recall 0.883287
2017-12-10T13:43:57.571616: step 4103, loss 0.218098, acc 0.984375, prec 0.0827588, recall 0.883317
2017-12-10T13:43:58.010719: step 4104, loss 0.0780282, acc 0.96875, prec 0.0827765, recall 0.883346
2017-12-10T13:43:58.472683: step 4105, loss 0.356147, acc 0.921875, prec 0.0827668, recall 0.883346
2017-12-10T13:43:58.921790: step 4106, loss 0.232165, acc 0.9375, prec 0.0828239, recall 0.883434
2017-12-10T13:43:59.363222: step 4107, loss 0.627191, acc 0.8125, prec 0.0828004, recall 0.883434
2017-12-10T13:43:59.817167: step 4108, loss 0.14482, acc 0.96875, prec 0.0828615, recall 0.883522
2017-12-10T13:44:00.264366: step 4109, loss 0.618997, acc 0.859375, prec 0.0828871, recall 0.883581
2017-12-10T13:44:00.708903: step 4110, loss 0.289933, acc 0.921875, prec 0.082899, recall 0.88361
2017-12-10T13:44:01.159591: step 4111, loss 0.420162, acc 0.859375, prec 0.0828814, recall 0.88361
2017-12-10T13:44:01.614770: step 4112, loss 0.789315, acc 0.84375, prec 0.0828835, recall 0.883639
2017-12-10T13:44:02.084696: step 4113, loss 0.376932, acc 0.875, prec 0.0828679, recall 0.883639
2017-12-10T13:44:02.534205: step 4114, loss 0.310175, acc 0.9375, prec 0.0828817, recall 0.883668
2017-12-10T13:44:02.981513: step 4115, loss 0.315477, acc 0.890625, prec 0.0829112, recall 0.883727
2017-12-10T13:44:03.419546: step 4116, loss 0.117427, acc 0.9375, prec 0.0829034, recall 0.883727
2017-12-10T13:44:03.884403: step 4117, loss 0.481153, acc 0.84375, prec 0.0829055, recall 0.883756
2017-12-10T13:44:04.332364: step 4118, loss 0.312171, acc 0.875, prec 0.0828898, recall 0.883756
2017-12-10T13:44:04.786538: step 4119, loss 0.368636, acc 0.890625, prec 0.0828762, recall 0.883756
2017-12-10T13:44:05.235031: step 4120, loss 0.242873, acc 0.96875, prec 0.0828723, recall 0.883756
2017-12-10T13:44:05.686607: step 4121, loss 0.24082, acc 0.9375, prec 0.0828645, recall 0.883756
2017-12-10T13:44:06.136580: step 4122, loss 0.100316, acc 0.953125, prec 0.0829234, recall 0.883843
2017-12-10T13:44:06.576781: step 4123, loss 0.127228, acc 0.96875, prec 0.0829195, recall 0.883843
2017-12-10T13:44:07.011973: step 4124, loss 0.0864387, acc 0.9375, prec 0.0829333, recall 0.883873
2017-12-10T13:44:07.453051: step 4125, loss 0.242706, acc 0.96875, prec 0.0829294, recall 0.883873
2017-12-10T13:44:07.908930: step 4126, loss 1.49303, acc 0.953125, prec 0.0829255, recall 0.883651
2017-12-10T13:44:08.370212: step 4127, loss 0.15033, acc 0.953125, prec 0.0829196, recall 0.883651
2017-12-10T13:44:08.817075: step 4128, loss 0.352012, acc 0.9375, prec 0.0829765, recall 0.883738
2017-12-10T13:44:09.263601: step 4129, loss 0.156317, acc 0.953125, prec 0.0829922, recall 0.883768
2017-12-10T13:44:09.714504: step 4130, loss 0.419025, acc 0.9375, prec 0.0830276, recall 0.883826
2017-12-10T13:44:10.159031: step 4131, loss 0.187561, acc 0.921875, prec 0.0830178, recall 0.883826
2017-12-10T13:44:10.607904: step 4132, loss 0.119534, acc 0.953125, prec 0.0830119, recall 0.883826
2017-12-10T13:44:11.045693: step 4133, loss 0.268259, acc 0.90625, prec 0.0830002, recall 0.883826
2017-12-10T13:44:11.486627: step 4134, loss 0.0631549, acc 0.984375, prec 0.0829983, recall 0.883826
2017-12-10T13:44:11.945282: step 4135, loss 0.270831, acc 0.921875, prec 0.0829885, recall 0.883826
2017-12-10T13:44:12.391927: step 4136, loss 0.551838, acc 0.9375, prec 0.0830023, recall 0.883855
2017-12-10T13:44:12.844339: step 4137, loss 0.0631425, acc 0.984375, prec 0.0830003, recall 0.883855
2017-12-10T13:44:13.288160: step 4138, loss 0.189039, acc 0.96875, prec 0.0830395, recall 0.883913
2017-12-10T13:44:13.739860: step 4139, loss 0.084745, acc 0.953125, prec 0.0830337, recall 0.883913
2017-12-10T13:44:14.179038: step 4140, loss 0.0394665, acc 0.984375, prec 0.0830533, recall 0.883942
2017-12-10T13:44:14.630036: step 4141, loss 0.0682397, acc 0.984375, prec 0.0830944, recall 0.884
2017-12-10T13:44:15.077215: step 4142, loss 0.0301226, acc 0.984375, prec 0.083114, recall 0.884029
2017-12-10T13:44:15.512687: step 4143, loss 1.61114, acc 0.96875, prec 0.0831336, recall 0.883837
2017-12-10T13:44:15.974794: step 4144, loss 0.147616, acc 0.96875, prec 0.0831728, recall 0.883895
2017-12-10T13:44:16.416728: step 4145, loss 0.236077, acc 0.921875, prec 0.0832276, recall 0.883982
2017-12-10T13:44:16.875942: step 4146, loss 0.230146, acc 1, prec 0.0832492, recall 0.884011
2017-12-10T13:44:17.349989: step 4147, loss 0.173343, acc 0.953125, prec 0.0832864, recall 0.884069
2017-12-10T13:44:17.804348: step 4148, loss 0.0376231, acc 0.984375, prec 0.0832844, recall 0.884069
2017-12-10T13:44:18.248074: step 4149, loss 0.0849444, acc 0.96875, prec 0.0832805, recall 0.884069
2017-12-10T13:44:18.692706: step 4150, loss 0.202902, acc 0.90625, prec 0.0833333, recall 0.884155
2017-12-10T13:44:19.138285: step 4151, loss 1.0989, acc 0.921875, prec 0.0833881, recall 0.884242
2017-12-10T13:44:19.599986: step 4152, loss 0.346037, acc 0.953125, prec 0.0833822, recall 0.884242
2017-12-10T13:44:20.038744: step 4153, loss 0.281587, acc 0.890625, prec 0.0834116, recall 0.8843
2017-12-10T13:44:20.497907: step 4154, loss 0.23055, acc 0.90625, prec 0.0834428, recall 0.884357
2017-12-10T13:44:20.941940: step 4155, loss 0.278831, acc 0.921875, prec 0.0834546, recall 0.884386
2017-12-10T13:44:21.388903: step 4156, loss 0.439925, acc 0.859375, prec 0.0834369, recall 0.884386
2017-12-10T13:44:21.842671: step 4157, loss 0.236427, acc 0.921875, prec 0.0834486, recall 0.884415
2017-12-10T13:44:22.286245: step 4158, loss 0.320702, acc 0.90625, prec 0.0834584, recall 0.884443
2017-12-10T13:44:22.730101: step 4159, loss 0.681316, acc 0.875, prec 0.0834427, recall 0.884443
2017-12-10T13:44:23.162738: step 4160, loss 0.127433, acc 0.984375, prec 0.0834838, recall 0.884501
2017-12-10T13:44:23.605581: step 4161, loss 0.244921, acc 0.9375, prec 0.0834759, recall 0.884501
2017-12-10T13:44:24.047801: step 4162, loss 0.243844, acc 0.9375, prec 0.0835111, recall 0.884558
2017-12-10T13:44:24.500074: step 4163, loss 0.53945, acc 0.875, prec 0.0834954, recall 0.884558
2017-12-10T13:44:24.945842: step 4164, loss 0.207334, acc 0.921875, prec 0.0835071, recall 0.884587
2017-12-10T13:44:25.396724: step 4165, loss 0.215059, acc 0.9375, prec 0.0835208, recall 0.884615
2017-12-10T13:44:25.846433: step 4166, loss 1.68789, acc 0.921875, prec 0.0835773, recall 0.884482
2017-12-10T13:44:26.297639: step 4167, loss 0.333794, acc 0.953125, prec 0.0836359, recall 0.884568
2017-12-10T13:44:26.754657: step 4168, loss 0.31419, acc 0.9375, prec 0.083628, recall 0.884568
2017-12-10T13:44:27.191976: step 4169, loss 0.217627, acc 0.9375, prec 0.0836631, recall 0.884625
2017-12-10T13:44:27.643791: step 4170, loss 0.39904, acc 0.890625, prec 0.0836708, recall 0.884653
2017-12-10T13:44:28.096884: step 4171, loss 0.3915, acc 0.9375, prec 0.0836845, recall 0.884682
2017-12-10T13:44:28.550219: step 4172, loss 0.530581, acc 0.9375, prec 0.0836766, recall 0.884682
2017-12-10T13:44:28.998976: step 4173, loss 0.222686, acc 0.953125, prec 0.0836922, recall 0.884711
2017-12-10T13:44:29.441603: step 4174, loss 0.386855, acc 0.953125, prec 0.0836863, recall 0.884711
2017-12-10T13:44:29.885417: step 4175, loss 0.106181, acc 0.9375, prec 0.0837214, recall 0.884768
2017-12-10T13:44:30.332893: step 4176, loss 0.136678, acc 0.953125, prec 0.0837369, recall 0.884796
2017-12-10T13:44:30.770948: step 4177, loss 1.37387, acc 0.921875, prec 0.083772, recall 0.884634
2017-12-10T13:44:31.219015: step 4178, loss 0.0924095, acc 0.953125, prec 0.0837661, recall 0.884634
2017-12-10T13:44:31.667309: step 4179, loss 1.06016, acc 0.953125, prec 0.0838031, recall 0.884691
2017-12-10T13:44:32.136821: step 4180, loss 0.37388, acc 0.953125, prec 0.08384, recall 0.884748
2017-12-10T13:44:32.622109: step 4181, loss 0.193671, acc 0.921875, prec 0.0838302, recall 0.884748
2017-12-10T13:44:33.060431: step 4182, loss 0.40829, acc 0.875, prec 0.083836, recall 0.884777
2017-12-10T13:44:33.512896: step 4183, loss 0.633216, acc 0.90625, prec 0.0838456, recall 0.884805
2017-12-10T13:44:33.955104: step 4184, loss 0.166843, acc 0.9375, prec 0.0838806, recall 0.884862
2017-12-10T13:44:34.400404: step 4185, loss 0.24244, acc 0.90625, prec 0.0838689, recall 0.884862
2017-12-10T13:44:34.836796: step 4186, loss 0.354397, acc 0.890625, prec 0.0838551, recall 0.884862
2017-12-10T13:44:35.280063: step 4187, loss 0.184136, acc 0.9375, prec 0.0838473, recall 0.884862
2017-12-10T13:44:35.712048: step 4188, loss 0.200649, acc 0.90625, prec 0.0838355, recall 0.884862
2017-12-10T13:44:36.157276: step 4189, loss 0.457636, acc 0.890625, prec 0.0838218, recall 0.884862
2017-12-10T13:44:36.589759: step 4190, loss 0.348201, acc 0.859375, prec 0.0838042, recall 0.884862
2017-12-10T13:44:37.033166: step 4191, loss 0.408272, acc 0.890625, prec 0.0837905, recall 0.884862
2017-12-10T13:44:37.490053: step 4192, loss 0.23742, acc 0.921875, prec 0.0838449, recall 0.884947
2017-12-10T13:44:37.923742: step 4193, loss 0.0866353, acc 0.953125, prec 0.083839, recall 0.884947
2017-12-10T13:44:38.358550: step 4194, loss 0.459891, acc 0.90625, prec 0.0838487, recall 0.884975
2017-12-10T13:44:38.790753: step 4195, loss 0.448195, acc 0.875, prec 0.0838544, recall 0.885004
2017-12-10T13:44:39.225930: step 4196, loss 0.231328, acc 0.953125, prec 0.0838699, recall 0.885032
2017-12-10T13:44:39.662372: step 4197, loss 0.073145, acc 0.953125, prec 0.083864, recall 0.885032
2017-12-10T13:44:40.129750: step 4198, loss 0.113021, acc 0.96875, prec 0.0838815, recall 0.88506
2017-12-10T13:44:40.578715: step 4199, loss 0.294526, acc 0.9375, prec 0.0838737, recall 0.88506
2017-12-10T13:44:41.023353: step 4200, loss 0.232741, acc 0.96875, prec 0.0838911, recall 0.885089
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4200

2017-12-10T13:44:43.131043: step 4201, loss 0.0753451, acc 0.984375, prec 0.0838892, recall 0.885089
2017-12-10T13:44:43.580655: step 4202, loss 0.0728769, acc 0.96875, prec 0.0838853, recall 0.885089
2017-12-10T13:44:44.024698: step 4203, loss 0.885116, acc 0.984375, prec 0.0839047, recall 0.885117
2017-12-10T13:44:44.481468: step 4204, loss 0.163333, acc 0.953125, prec 0.0839629, recall 0.885202
2017-12-10T13:44:44.948471: step 4205, loss 0.065873, acc 0.984375, prec 0.0839609, recall 0.885202
2017-12-10T13:44:45.382480: step 4206, loss 0.0720554, acc 0.96875, prec 0.083957, recall 0.885202
2017-12-10T13:44:45.834767: step 4207, loss 0.0959304, acc 0.953125, prec 0.0839725, recall 0.88523
2017-12-10T13:44:46.271264: step 4208, loss 0.0545409, acc 0.984375, prec 0.0839705, recall 0.88523
2017-12-10T13:44:46.712084: step 4209, loss 0.266383, acc 0.953125, prec 0.0840074, recall 0.885286
2017-12-10T13:44:47.157696: step 4210, loss 0.115806, acc 0.953125, prec 0.0840015, recall 0.885286
2017-12-10T13:44:47.596855: step 4211, loss 0.0718198, acc 0.984375, prec 0.0840209, recall 0.885314
2017-12-10T13:44:48.049993: step 4212, loss 0.260563, acc 0.953125, prec 0.084015, recall 0.885314
2017-12-10T13:44:48.496725: step 4213, loss 0.308484, acc 0.921875, prec 0.0840266, recall 0.885342
2017-12-10T13:44:48.927676: step 4214, loss 0.105006, acc 0.984375, prec 0.0840246, recall 0.885342
2017-12-10T13:44:49.380006: step 4215, loss 0.336504, acc 0.953125, prec 0.0840401, recall 0.885371
2017-12-10T13:44:49.840864: step 4216, loss 0.197502, acc 0.96875, prec 0.0840788, recall 0.885427
2017-12-10T13:44:50.302602: step 4217, loss 0.199865, acc 0.984375, prec 0.0840982, recall 0.885455
2017-12-10T13:44:50.747252: step 4218, loss 0.039978, acc 0.96875, prec 0.084137, recall 0.885511
2017-12-10T13:44:51.199415: step 4219, loss 0.0357219, acc 0.984375, prec 0.084135, recall 0.885511
2017-12-10T13:44:51.645181: step 4220, loss 0.0965858, acc 0.96875, prec 0.0841524, recall 0.885539
2017-12-10T13:44:52.079781: step 4221, loss 0.0234148, acc 1, prec 0.0841738, recall 0.885567
2017-12-10T13:44:52.521465: step 4222, loss 0.136929, acc 0.953125, prec 0.0841679, recall 0.885567
2017-12-10T13:44:52.975708: step 4223, loss 0.0417085, acc 0.96875, prec 0.0841639, recall 0.885567
2017-12-10T13:44:53.433669: step 4224, loss 0.182325, acc 0.953125, prec 0.0841794, recall 0.885595
2017-12-10T13:44:53.879769: step 4225, loss 0.25601, acc 0.96875, prec 0.0841755, recall 0.885595
2017-12-10T13:44:54.318981: step 4226, loss 0.249784, acc 0.96875, prec 0.0841716, recall 0.885595
2017-12-10T13:44:54.765785: step 4227, loss 1.5681, acc 0.984375, prec 0.0841929, recall 0.885406
2017-12-10T13:44:55.230329: step 4228, loss 0.703296, acc 0.984375, prec 0.0842549, recall 0.885491
2017-12-10T13:44:55.688519: step 4229, loss 0.184038, acc 0.953125, prec 0.0842703, recall 0.885519
2017-12-10T13:44:56.137416: step 4230, loss 0.105366, acc 0.953125, prec 0.0843071, recall 0.885575
2017-12-10T13:44:56.582203: step 4231, loss 0.0355177, acc 1, prec 0.0843284, recall 0.885603
2017-12-10T13:44:57.037003: step 4232, loss 0.542356, acc 0.953125, prec 0.0843438, recall 0.88563
2017-12-10T13:44:57.498570: step 4233, loss 0.15515, acc 0.921875, prec 0.084334, recall 0.88563
2017-12-10T13:44:57.957785: step 4234, loss 0.160178, acc 0.953125, prec 0.0843281, recall 0.88563
2017-12-10T13:44:58.389505: step 4235, loss 0.0557814, acc 0.984375, prec 0.0843261, recall 0.88563
2017-12-10T13:44:58.835874: step 4236, loss 0.478532, acc 0.90625, prec 0.084357, recall 0.885686
2017-12-10T13:44:59.277842: step 4237, loss 0.101167, acc 0.96875, prec 0.084353, recall 0.885686
2017-12-10T13:44:59.723710: step 4238, loss 0.16813, acc 0.96875, prec 0.0843704, recall 0.885714
2017-12-10T13:45:00.174592: step 4239, loss 0.13443, acc 0.953125, prec 0.0843645, recall 0.885714
2017-12-10T13:45:00.621338: step 4240, loss 0.152482, acc 0.953125, prec 0.0844012, recall 0.88577
2017-12-10T13:45:01.068935: step 4241, loss 0.321231, acc 0.890625, prec 0.0844301, recall 0.885826
2017-12-10T13:45:01.520456: step 4242, loss 0.290955, acc 0.953125, prec 0.084488, recall 0.885909
2017-12-10T13:45:01.974972: step 4243, loss 0.325087, acc 0.90625, prec 0.0844975, recall 0.885937
2017-12-10T13:45:02.425301: step 4244, loss 0.0777307, acc 0.96875, prec 0.0845149, recall 0.885965
2017-12-10T13:45:02.871515: step 4245, loss 0.169161, acc 0.984375, prec 0.0845555, recall 0.88602
2017-12-10T13:45:03.328125: step 4246, loss 0.249337, acc 0.9375, prec 0.0845476, recall 0.88602
2017-12-10T13:45:03.771505: step 4247, loss 0.189873, acc 0.9375, prec 0.0845398, recall 0.88602
2017-12-10T13:45:04.215655: step 4248, loss 0.13032, acc 0.96875, prec 0.0845784, recall 0.886076
2017-12-10T13:45:04.668122: step 4249, loss 0.0581147, acc 0.984375, prec 0.0845977, recall 0.886104
2017-12-10T13:45:05.125262: step 4250, loss 0.215389, acc 0.953125, prec 0.0845918, recall 0.886104
2017-12-10T13:45:05.567496: step 4251, loss 0.00729625, acc 1, prec 0.0846131, recall 0.886131
2017-12-10T13:45:06.019691: step 4252, loss 0.0411244, acc 0.984375, prec 0.0846111, recall 0.886131
2017-12-10T13:45:06.476152: step 4253, loss 0.12488, acc 0.984375, prec 0.0846304, recall 0.886159
2017-12-10T13:45:06.934366: step 4254, loss 5.8474, acc 0.921875, prec 0.0846225, recall 0.885944
2017-12-10T13:45:07.381559: step 4255, loss 0.0980361, acc 0.96875, prec 0.0846186, recall 0.885944
2017-12-10T13:45:07.815357: step 4256, loss 0.369538, acc 0.9375, prec 0.0846107, recall 0.885944
2017-12-10T13:45:08.271448: step 4257, loss 0.349936, acc 0.953125, prec 0.0846474, recall 0.885999
2017-12-10T13:45:08.723525: step 4258, loss 0.282293, acc 0.96875, prec 0.0846434, recall 0.885999
2017-12-10T13:45:09.170296: step 4259, loss 0.106531, acc 0.953125, prec 0.0846375, recall 0.885999
2017-12-10T13:45:09.613866: step 4260, loss 0.132108, acc 0.953125, prec 0.0846741, recall 0.886054
2017-12-10T13:45:10.062338: step 4261, loss 0.0796204, acc 0.96875, prec 0.0846702, recall 0.886054
2017-12-10T13:45:10.506398: step 4262, loss 0.218204, acc 0.9375, prec 0.0846623, recall 0.886054
2017-12-10T13:45:10.958396: step 4263, loss 0.31006, acc 0.9375, prec 0.0846757, recall 0.886082
2017-12-10T13:45:11.399640: step 4264, loss 0.36416, acc 0.859375, prec 0.084658, recall 0.886082
2017-12-10T13:45:11.824905: step 4265, loss 0.416685, acc 0.921875, prec 0.0846695, recall 0.88611
2017-12-10T13:45:12.281777: step 4266, loss 0.327894, acc 0.90625, prec 0.0846789, recall 0.886137
2017-12-10T13:45:12.724795: step 4267, loss 0.250414, acc 0.890625, prec 0.0847076, recall 0.886193
2017-12-10T13:45:13.169656: step 4268, loss 0.10416, acc 0.953125, prec 0.0847442, recall 0.886248
2017-12-10T13:45:13.627876: step 4269, loss 0.141765, acc 0.90625, prec 0.0847536, recall 0.886275
2017-12-10T13:45:14.075770: step 4270, loss 0.155497, acc 0.921875, prec 0.0847862, recall 0.886331
2017-12-10T13:45:14.517449: step 4271, loss 0.169581, acc 0.9375, prec 0.0847996, recall 0.886358
2017-12-10T13:45:14.965639: step 4272, loss 0.0309842, acc 1, prec 0.0848208, recall 0.886386
2017-12-10T13:45:15.402385: step 4273, loss 0.133784, acc 0.953125, prec 0.0848149, recall 0.886386
2017-12-10T13:45:15.847570: step 4274, loss 0.229789, acc 0.953125, prec 0.084809, recall 0.886386
2017-12-10T13:45:16.295450: step 4275, loss 0.0748375, acc 0.953125, prec 0.0848243, recall 0.886413
2017-12-10T13:45:16.743393: step 4276, loss 0.213726, acc 0.9375, prec 0.0848589, recall 0.886468
2017-12-10T13:45:17.184084: step 4277, loss 0.323865, acc 0.890625, prec 0.0848663, recall 0.886496
2017-12-10T13:45:17.626786: step 4278, loss 0.295483, acc 0.9375, prec 0.0849009, recall 0.886551
2017-12-10T13:45:18.080166: step 4279, loss 0.098888, acc 0.96875, prec 0.0848969, recall 0.886551
2017-12-10T13:45:18.519978: step 4280, loss 0.359937, acc 0.90625, prec 0.0848851, recall 0.886551
2017-12-10T13:45:18.960223: step 4281, loss 0.0530056, acc 0.984375, prec 0.0848832, recall 0.886551
2017-12-10T13:45:19.399865: step 4282, loss 0.0903084, acc 0.96875, prec 0.0849216, recall 0.886605
2017-12-10T13:45:19.845868: step 4283, loss 0.117816, acc 0.953125, prec 0.0849369, recall 0.886633
2017-12-10T13:45:20.311931: step 4284, loss 0.0312933, acc 0.984375, prec 0.0849561, recall 0.88666
2017-12-10T13:45:20.758637: step 4285, loss 0.157052, acc 0.96875, prec 0.0849734, recall 0.886688
2017-12-10T13:45:21.195085: step 4286, loss 0.191317, acc 0.96875, prec 0.0849694, recall 0.886688
2017-12-10T13:45:21.643384: step 4287, loss 0.882194, acc 0.9375, prec 0.0850463, recall 0.886797
2017-12-10T13:45:22.090204: step 4288, loss 0.306431, acc 0.921875, prec 0.0850576, recall 0.886824
2017-12-10T13:45:22.534397: step 4289, loss 0.0665088, acc 0.96875, prec 0.0850537, recall 0.886824
2017-12-10T13:45:22.993843: step 4290, loss 6.77321, acc 0.984375, prec 0.0850749, recall 0.886638
2017-12-10T13:45:23.455691: step 4291, loss 0.0907436, acc 0.96875, prec 0.0850921, recall 0.886665
2017-12-10T13:45:23.909846: step 4292, loss 0.118447, acc 0.953125, prec 0.0851285, recall 0.88672
2017-12-10T13:45:24.355379: step 4293, loss 0.259439, acc 0.953125, prec 0.0851438, recall 0.886747
2017-12-10T13:45:24.809232: step 4294, loss 0.100735, acc 0.96875, prec 0.0851822, recall 0.886802
2017-12-10T13:45:25.254649: step 4295, loss 0.287731, acc 0.90625, prec 0.0851704, recall 0.886802
2017-12-10T13:45:25.703823: step 4296, loss 0.344352, acc 0.859375, prec 0.0851738, recall 0.886829
2017-12-10T13:45:26.156929: step 4297, loss 0.396628, acc 0.921875, prec 0.0852063, recall 0.886883
2017-12-10T13:45:26.609678: step 4298, loss 0.711446, acc 0.84375, prec 0.0851866, recall 0.886883
2017-12-10T13:45:27.048954: step 4299, loss 0.408786, acc 0.84375, prec 0.085188, recall 0.88691
2017-12-10T13:45:27.495303: step 4300, loss 0.249338, acc 0.921875, prec 0.0852204, recall 0.886965
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4300

2017-12-10T13:45:29.777418: step 4301, loss 0.735222, acc 0.78125, prec 0.0852351, recall 0.887019
2017-12-10T13:45:30.213998: step 4302, loss 0.436436, acc 0.90625, prec 0.0852445, recall 0.887046
2017-12-10T13:45:30.650770: step 4303, loss 0.522559, acc 0.859375, prec 0.0852267, recall 0.887046
2017-12-10T13:45:31.107192: step 4304, loss 0.309086, acc 0.890625, prec 0.0852341, recall 0.887074
2017-12-10T13:45:31.558151: step 4305, loss 0.345503, acc 0.90625, prec 0.0852223, recall 0.887074
2017-12-10T13:45:32.015350: step 4306, loss 0.692804, acc 0.859375, prec 0.0852046, recall 0.887074
2017-12-10T13:45:32.458223: step 4307, loss 0.167816, acc 0.921875, prec 0.0852159, recall 0.887101
2017-12-10T13:45:32.899213: step 4308, loss 0.0731732, acc 0.953125, prec 0.08521, recall 0.887101
2017-12-10T13:45:33.343244: step 4309, loss 0.391426, acc 0.890625, prec 0.0851962, recall 0.887101
2017-12-10T13:45:33.786432: step 4310, loss 0.217975, acc 0.890625, prec 0.0852247, recall 0.887155
2017-12-10T13:45:34.234140: step 4311, loss 0.467992, acc 0.90625, prec 0.085234, recall 0.887182
2017-12-10T13:45:34.692477: step 4312, loss 0.42977, acc 0.890625, prec 0.0852413, recall 0.887209
2017-12-10T13:45:35.136527: step 4313, loss 0.130294, acc 0.96875, prec 0.0852374, recall 0.887209
2017-12-10T13:45:35.579073: step 4314, loss 0.0644508, acc 0.953125, prec 0.0852315, recall 0.887209
2017-12-10T13:45:36.046671: step 4315, loss 0.071986, acc 1, prec 0.0852526, recall 0.887236
2017-12-10T13:45:36.501184: step 4316, loss 0.0367885, acc 0.984375, prec 0.0852928, recall 0.88729
2017-12-10T13:45:36.941365: step 4317, loss 0.0363229, acc 0.984375, prec 0.0852908, recall 0.88729
2017-12-10T13:45:37.400176: step 4318, loss 0.103038, acc 0.96875, prec 0.0852869, recall 0.88729
2017-12-10T13:45:37.847641: step 4319, loss 0.0445855, acc 0.984375, prec 0.085306, recall 0.887317
2017-12-10T13:45:38.294599: step 4320, loss 0.127104, acc 0.96875, prec 0.085302, recall 0.887317
2017-12-10T13:45:38.746844: step 4321, loss 0.0903733, acc 0.96875, prec 0.0853192, recall 0.887344
2017-12-10T13:45:39.193539: step 4322, loss 0.0467804, acc 0.984375, prec 0.0853594, recall 0.887398
2017-12-10T13:45:39.631269: step 4323, loss 0.0697793, acc 0.953125, prec 0.0853746, recall 0.887425
2017-12-10T13:45:40.071638: step 4324, loss 0.0264002, acc 0.984375, prec 0.0854147, recall 0.887479
2017-12-10T13:45:40.519780: step 4325, loss 2.12545, acc 0.96875, prec 0.085476, recall 0.887348
2017-12-10T13:45:40.963709: step 4326, loss 1.30949, acc 0.96875, prec 0.0854951, recall 0.887162
2017-12-10T13:45:41.402143: step 4327, loss 0.0423798, acc 0.96875, prec 0.0854912, recall 0.887162
2017-12-10T13:45:41.847102: step 4328, loss 2.1351, acc 0.953125, prec 0.0855083, recall 0.886977
2017-12-10T13:45:42.293385: step 4329, loss 0.254104, acc 0.859375, prec 0.0855116, recall 0.887004
2017-12-10T13:45:42.721838: step 4330, loss 0.267576, acc 0.9375, prec 0.0855248, recall 0.887031
2017-12-10T13:45:43.165339: step 4331, loss 0.613844, acc 0.859375, prec 0.0855702, recall 0.887112
2017-12-10T13:45:43.626858: step 4332, loss 0.414172, acc 0.90625, prec 0.0855795, recall 0.887139
2017-12-10T13:45:44.077901: step 4333, loss 0.716617, acc 0.765625, prec 0.0855499, recall 0.887139
2017-12-10T13:45:44.533069: step 4334, loss 0.480158, acc 0.875, prec 0.0855552, recall 0.887166
2017-12-10T13:45:44.983229: step 4335, loss 0.68691, acc 0.828125, prec 0.0855756, recall 0.88722
2017-12-10T13:45:45.419876: step 4336, loss 0.595178, acc 0.828125, prec 0.085554, recall 0.88722
2017-12-10T13:45:45.862179: step 4337, loss 0.727894, acc 0.84375, prec 0.0855554, recall 0.887247
2017-12-10T13:45:46.321517: step 4338, loss 1.4382, acc 0.734375, prec 0.0855429, recall 0.887274
2017-12-10T13:45:46.757235: step 4339, loss 0.59527, acc 0.796875, prec 0.0855174, recall 0.887274
2017-12-10T13:45:47.199933: step 4340, loss 0.634434, acc 0.84375, prec 0.0855188, recall 0.8873
2017-12-10T13:45:47.639816: step 4341, loss 0.710331, acc 0.84375, prec 0.0855411, recall 0.887354
2017-12-10T13:45:48.083986: step 4342, loss 0.637694, acc 0.8125, prec 0.0855176, recall 0.887354
2017-12-10T13:45:48.519850: step 4343, loss 0.545287, acc 0.875, prec 0.0855438, recall 0.887408
2017-12-10T13:45:48.953163: step 4344, loss 1.06177, acc 0.796875, prec 0.0855183, recall 0.887408
2017-12-10T13:45:49.396316: step 4345, loss 0.316052, acc 0.921875, prec 0.0855295, recall 0.887435
2017-12-10T13:45:49.833446: step 4346, loss 0.411875, acc 0.859375, prec 0.0855328, recall 0.887461
2017-12-10T13:45:50.272570: step 4347, loss 0.599099, acc 0.890625, prec 0.085561, recall 0.887515
2017-12-10T13:45:50.729095: step 4348, loss 0.317911, acc 0.890625, prec 0.0855682, recall 0.887542
2017-12-10T13:45:51.178904: step 4349, loss 0.149294, acc 0.96875, prec 0.0856481, recall 0.887648
2017-12-10T13:45:51.623854: step 4350, loss 0.217278, acc 0.9375, prec 0.0856403, recall 0.887648
2017-12-10T13:45:52.072674: step 4351, loss 0.256406, acc 0.9375, prec 0.0856325, recall 0.887648
2017-12-10T13:45:52.524760: step 4352, loss 0.116695, acc 0.953125, prec 0.0856475, recall 0.887675
2017-12-10T13:45:52.969711: step 4353, loss 0.312357, acc 0.921875, prec 0.0856377, recall 0.887675
2017-12-10T13:45:53.416692: step 4354, loss 0.0805723, acc 0.96875, prec 0.0856338, recall 0.887675
2017-12-10T13:45:53.859888: step 4355, loss 0.241906, acc 0.96875, prec 0.0856508, recall 0.887702
2017-12-10T13:45:54.308012: step 4356, loss 0.128205, acc 0.984375, prec 0.0856488, recall 0.887702
2017-12-10T13:45:54.767058: step 4357, loss 0.342358, acc 0.96875, prec 0.0856659, recall 0.887728
2017-12-10T13:45:55.205117: step 4358, loss 0.0483938, acc 0.96875, prec 0.0856829, recall 0.887755
2017-12-10T13:45:55.655883: step 4359, loss 0.121722, acc 1, prec 0.0857038, recall 0.887782
2017-12-10T13:45:56.108915: step 4360, loss 0.454165, acc 0.96875, prec 0.0857208, recall 0.887808
2017-12-10T13:45:56.556485: step 4361, loss 0.134433, acc 1, prec 0.0857418, recall 0.887835
2017-12-10T13:45:57.010380: step 4362, loss 3.89434, acc 0.9375, prec 0.0857568, recall 0.887651
2017-12-10T13:45:57.471602: step 4363, loss 0.0730496, acc 1, prec 0.0857777, recall 0.887678
2017-12-10T13:45:57.935958: step 4364, loss 0.0826533, acc 0.96875, prec 0.0857948, recall 0.887704
2017-12-10T13:45:58.383450: step 4365, loss 0.0279573, acc 0.984375, prec 0.0857928, recall 0.887704
2017-12-10T13:45:58.829809: step 4366, loss 0.0972217, acc 0.96875, prec 0.0857889, recall 0.887704
2017-12-10T13:45:59.279459: step 4367, loss 4.01823, acc 0.921875, prec 0.0858229, recall 0.887547
2017-12-10T13:45:59.733956: step 4368, loss 0.54968, acc 0.90625, prec 0.0858111, recall 0.887547
2017-12-10T13:46:00.183475: step 4369, loss 0.518123, acc 0.875, prec 0.0857954, recall 0.887547
2017-12-10T13:46:00.630737: step 4370, loss 0.360842, acc 0.84375, prec 0.0857966, recall 0.887574
2017-12-10T13:46:01.087008: step 4371, loss 0.724386, acc 0.765625, prec 0.085809, recall 0.887627
2017-12-10T13:46:01.530621: step 4372, loss 0.644614, acc 0.734375, prec 0.0857757, recall 0.887627
2017-12-10T13:46:01.976261: step 4373, loss 0.787558, acc 0.8125, prec 0.0857939, recall 0.88768
2017-12-10T13:46:02.420358: step 4374, loss 1.08054, acc 0.8125, prec 0.0857913, recall 0.887707
2017-12-10T13:46:02.855113: step 4375, loss 0.724087, acc 0.75, prec 0.0857808, recall 0.887733
2017-12-10T13:46:03.299294: step 4376, loss 1.28435, acc 0.8125, prec 0.0857782, recall 0.88776
2017-12-10T13:46:03.755740: step 4377, loss 1.28817, acc 0.6875, prec 0.0857599, recall 0.887786
2017-12-10T13:46:04.203863: step 4378, loss 1.35928, acc 0.71875, prec 0.0857247, recall 0.887786
2017-12-10T13:46:04.646724: step 4379, loss 1.16296, acc 0.78125, prec 0.0857807, recall 0.887892
2017-12-10T13:46:05.093298: step 4380, loss 1.45971, acc 0.75, prec 0.0857703, recall 0.887919
2017-12-10T13:46:05.545987: step 4381, loss 1.44755, acc 0.671875, prec 0.0857293, recall 0.887919
2017-12-10T13:46:05.992753: step 4382, loss 1.74937, acc 0.703125, prec 0.085713, recall 0.887945
2017-12-10T13:46:06.422743: step 4383, loss 0.865149, acc 0.765625, prec 0.0857045, recall 0.887972
2017-12-10T13:46:06.879323: step 4384, loss 1.16891, acc 0.78125, prec 0.085698, recall 0.887998
2017-12-10T13:46:07.329735: step 4385, loss 0.504514, acc 0.890625, prec 0.0857052, recall 0.888025
2017-12-10T13:46:07.779726: step 4386, loss 0.175218, acc 0.9375, prec 0.0857182, recall 0.888051
2017-12-10T13:46:08.233514: step 4387, loss 0.556016, acc 0.859375, prec 0.0857214, recall 0.888077
2017-12-10T13:46:08.699800: step 4388, loss 0.0481088, acc 0.984375, prec 0.0857403, recall 0.888104
2017-12-10T13:46:09.153383: step 4389, loss 0.0128809, acc 1, prec 0.0857611, recall 0.88813
2017-12-10T13:46:09.586920: step 4390, loss 0.0398891, acc 0.96875, prec 0.085778, recall 0.888156
2017-12-10T13:46:10.026172: step 4391, loss 0.161614, acc 0.953125, prec 0.0857929, recall 0.888183
2017-12-10T13:46:10.473171: step 4392, loss 0.141542, acc 0.9375, prec 0.0857851, recall 0.888183
2017-12-10T13:46:10.914279: step 4393, loss 0.15796, acc 0.96875, prec 0.0858228, recall 0.888235
2017-12-10T13:46:11.369859: step 4394, loss 0.172814, acc 0.96875, prec 0.0858396, recall 0.888262
2017-12-10T13:46:11.816091: step 4395, loss 1.32342, acc 0.96875, prec 0.0858585, recall 0.888079
2017-12-10T13:46:12.251104: step 4396, loss 0.0937854, acc 0.984375, prec 0.0858773, recall 0.888105
2017-12-10T13:46:12.706538: step 4397, loss 0.123749, acc 0.984375, prec 0.0858753, recall 0.888105
2017-12-10T13:46:13.153756: step 4398, loss 0.0130406, acc 1, prec 0.0858753, recall 0.888105
2017-12-10T13:46:13.596921: step 4399, loss 0.0737154, acc 0.984375, prec 0.0858734, recall 0.888105
2017-12-10T13:46:14.050085: step 4400, loss 0.0225533, acc 0.984375, prec 0.0858714, recall 0.888105
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4400

2017-12-10T13:46:15.934123: step 4401, loss 0.101112, acc 0.984375, prec 0.0858903, recall 0.888132
2017-12-10T13:46:16.377405: step 4402, loss 0.0301699, acc 0.984375, prec 0.0858883, recall 0.888132
2017-12-10T13:46:16.830609: step 4403, loss 0.114059, acc 0.953125, prec 0.0859032, recall 0.888158
2017-12-10T13:46:17.265566: step 4404, loss 0.0924153, acc 0.9375, prec 0.0859162, recall 0.888184
2017-12-10T13:46:17.737239: step 4405, loss 0.167944, acc 0.96875, prec 0.0859538, recall 0.888237
2017-12-10T13:46:18.202846: step 4406, loss 0.380793, acc 0.953125, prec 0.0859687, recall 0.888263
2017-12-10T13:46:18.662638: step 4407, loss 0.107927, acc 0.96875, prec 0.0859648, recall 0.888263
2017-12-10T13:46:19.111104: step 4408, loss 0.837671, acc 0.984375, prec 0.0859836, recall 0.888289
2017-12-10T13:46:19.559834: step 4409, loss 0.0673406, acc 0.984375, prec 0.0860025, recall 0.888315
2017-12-10T13:46:20.007082: step 4410, loss 0.0159274, acc 1, prec 0.0860025, recall 0.888315
2017-12-10T13:46:20.444795: step 4411, loss 0.212067, acc 0.96875, prec 0.0860193, recall 0.888342
2017-12-10T13:46:20.888530: step 4412, loss 0.166186, acc 0.953125, prec 0.0860342, recall 0.888368
2017-12-10T13:46:21.326566: step 4413, loss 0.0454406, acc 0.96875, prec 0.0860511, recall 0.888394
2017-12-10T13:46:21.758918: step 4414, loss 0.113308, acc 0.9375, prec 0.086064, recall 0.88842
2017-12-10T13:46:22.201237: step 4415, loss 0.300321, acc 0.90625, prec 0.086073, recall 0.888446
2017-12-10T13:46:22.656469: step 4416, loss 0.426735, acc 0.921875, prec 0.086084, recall 0.888472
2017-12-10T13:46:23.105059: step 4417, loss 1.63101, acc 0.953125, prec 0.0861008, recall 0.88829
2017-12-10T13:46:23.560991: step 4418, loss 0.156474, acc 0.96875, prec 0.0860969, recall 0.88829
2017-12-10T13:46:24.008575: step 4419, loss 0.108616, acc 0.953125, prec 0.0860911, recall 0.88829
2017-12-10T13:46:24.442036: step 4420, loss 0.238455, acc 0.96875, prec 0.0861286, recall 0.888343
2017-12-10T13:46:24.889786: step 4421, loss 0.389708, acc 0.875, prec 0.086113, recall 0.888343
2017-12-10T13:46:25.325533: step 4422, loss 0.630306, acc 0.875, prec 0.0861181, recall 0.888369
2017-12-10T13:46:25.780365: step 4423, loss 0.160798, acc 0.921875, prec 0.0861083, recall 0.888369
2017-12-10T13:46:26.228865: step 4424, loss 0.516639, acc 0.921875, prec 0.08614, recall 0.888421
2017-12-10T13:46:26.684257: step 4425, loss 0.225973, acc 0.921875, prec 0.0861717, recall 0.888473
2017-12-10T13:46:27.132299: step 4426, loss 0.239041, acc 0.890625, prec 0.0861787, recall 0.888499
2017-12-10T13:46:27.590996: step 4427, loss 0.588349, acc 0.859375, prec 0.0861819, recall 0.888525
2017-12-10T13:46:28.030350: step 4428, loss 0.162593, acc 0.953125, prec 0.086176, recall 0.888525
2017-12-10T13:46:28.465844: step 4429, loss 0.559992, acc 0.859375, prec 0.0861584, recall 0.888525
2017-12-10T13:46:28.906060: step 4430, loss 0.801793, acc 0.875, prec 0.0861635, recall 0.888551
2017-12-10T13:46:29.348921: step 4431, loss 0.372729, acc 0.875, prec 0.0861893, recall 0.888603
2017-12-10T13:46:29.785203: step 4432, loss 0.286319, acc 0.90625, prec 0.0861983, recall 0.888629
2017-12-10T13:46:30.233848: step 4433, loss 0.257784, acc 0.921875, prec 0.0862506, recall 0.888707
2017-12-10T13:46:30.679757: step 4434, loss 0.136738, acc 0.921875, prec 0.0862615, recall 0.888733
2017-12-10T13:46:31.126309: step 4435, loss 0.189274, acc 0.921875, prec 0.0862932, recall 0.888785
2017-12-10T13:46:31.574221: step 4436, loss 0.315286, acc 0.953125, prec 0.086308, recall 0.888811
2017-12-10T13:46:32.027103: step 4437, loss 0.213623, acc 0.9375, prec 0.0863002, recall 0.888811
2017-12-10T13:46:32.480431: step 4438, loss 0.213363, acc 0.921875, prec 0.0862904, recall 0.888811
2017-12-10T13:46:32.915779: step 4439, loss 0.258053, acc 0.953125, prec 0.0863052, recall 0.888837
2017-12-10T13:46:33.356369: step 4440, loss 0.0753328, acc 0.96875, prec 0.0863013, recall 0.888837
2017-12-10T13:46:33.808180: step 4441, loss 0.272927, acc 0.96875, prec 0.0863388, recall 0.888889
2017-12-10T13:46:34.251104: step 4442, loss 0.194852, acc 0.953125, prec 0.0863536, recall 0.888915
2017-12-10T13:46:34.711884: step 4443, loss 0.90001, acc 0.984375, prec 0.0863929, recall 0.888967
2017-12-10T13:46:35.154889: step 4444, loss 1.38492, acc 0.9375, prec 0.0864471, recall 0.889044
2017-12-10T13:46:35.609479: step 4445, loss 0.0267425, acc 0.984375, prec 0.0864452, recall 0.889044
2017-12-10T13:46:36.084369: step 4446, loss 0.141337, acc 0.9375, prec 0.0864787, recall 0.889096
2017-12-10T13:46:36.528445: step 4447, loss 0.34532, acc 0.921875, prec 0.0864895, recall 0.889121
2017-12-10T13:46:36.993541: step 4448, loss 0.165977, acc 0.953125, prec 0.0864837, recall 0.889121
2017-12-10T13:46:37.445202: step 4449, loss 0.232125, acc 0.90625, prec 0.0864719, recall 0.889121
2017-12-10T13:46:37.896563: step 4450, loss 0.0972206, acc 0.953125, prec 0.0865074, recall 0.889173
2017-12-10T13:46:38.350914: step 4451, loss 0.516495, acc 0.90625, prec 0.0864957, recall 0.889173
2017-12-10T13:46:38.804253: step 4452, loss 0.121826, acc 0.96875, prec 0.0865124, recall 0.889199
2017-12-10T13:46:39.251898: step 4453, loss 0.0585834, acc 0.984375, prec 0.0865517, recall 0.88925
2017-12-10T13:46:39.696435: step 4454, loss 0.0542062, acc 0.96875, prec 0.0865684, recall 0.889276
2017-12-10T13:46:40.143882: step 4455, loss 0.130172, acc 0.953125, prec 0.0865626, recall 0.889276
2017-12-10T13:46:40.591146: step 4456, loss 0.23506, acc 0.9375, prec 0.0865754, recall 0.889301
2017-12-10T13:46:41.036550: step 4457, loss 0.190172, acc 0.984375, prec 0.0866353, recall 0.889378
2017-12-10T13:46:41.484980: step 4458, loss 0.096918, acc 0.96875, prec 0.0866314, recall 0.889378
2017-12-10T13:46:41.929338: step 4459, loss 0.195218, acc 0.953125, prec 0.0866462, recall 0.889404
2017-12-10T13:46:42.365825: step 4460, loss 0.209505, acc 0.953125, prec 0.0866609, recall 0.88943
2017-12-10T13:46:42.804077: step 4461, loss 0.112376, acc 0.984375, prec 0.086659, recall 0.88943
2017-12-10T13:46:43.251334: step 4462, loss 0.224513, acc 0.96875, prec 0.0866551, recall 0.88943
2017-12-10T13:46:43.700919: step 4463, loss 0.0577006, acc 0.984375, prec 0.0866531, recall 0.88943
2017-12-10T13:46:44.137545: step 4464, loss 0.325025, acc 0.96875, prec 0.0866492, recall 0.88943
2017-12-10T13:46:44.580653: step 4465, loss 0.310118, acc 0.9375, prec 0.086662, recall 0.889455
2017-12-10T13:46:45.036078: step 4466, loss 0.295574, acc 0.921875, prec 0.0866935, recall 0.889507
2017-12-10T13:46:45.478796: step 4467, loss 0.135765, acc 0.953125, prec 0.0867288, recall 0.889558
2017-12-10T13:46:45.911171: step 4468, loss 0.323577, acc 0.953125, prec 0.0867229, recall 0.889558
2017-12-10T13:46:46.349082: step 4469, loss 0.0992941, acc 0.984375, prec 0.086721, recall 0.889558
2017-12-10T13:46:46.791207: step 4470, loss 0.0773812, acc 0.953125, prec 0.0867357, recall 0.889583
2017-12-10T13:46:47.232527: step 4471, loss 0.144455, acc 0.96875, prec 0.0867524, recall 0.889609
2017-12-10T13:46:47.676940: step 4472, loss 0.0352888, acc 0.96875, prec 0.0867485, recall 0.889609
2017-12-10T13:46:48.077797: step 4473, loss 0.378399, acc 0.942308, prec 0.0867632, recall 0.889634
2017-12-10T13:46:48.540330: step 4474, loss 0.181804, acc 0.953125, prec 0.086778, recall 0.88966
2017-12-10T13:46:48.992683: step 4475, loss 0.309201, acc 0.953125, prec 0.0867721, recall 0.88966
2017-12-10T13:46:49.434622: step 4476, loss 0.0422986, acc 0.984375, prec 0.0867701, recall 0.88966
2017-12-10T13:46:49.874192: step 4477, loss 0.141377, acc 0.96875, prec 0.0867662, recall 0.88966
2017-12-10T13:46:50.323122: step 4478, loss 0.0849037, acc 0.96875, prec 0.0867829, recall 0.889685
2017-12-10T13:46:50.783655: step 4479, loss 0.76187, acc 0.96875, prec 0.0868408, recall 0.889762
2017-12-10T13:46:51.221664: step 4480, loss 0.0248274, acc 1, prec 0.0868408, recall 0.889762
2017-12-10T13:46:51.669830: step 4481, loss 0.060846, acc 0.984375, prec 0.0868594, recall 0.889787
2017-12-10T13:46:52.144107: step 4482, loss 0.245871, acc 0.96875, prec 0.0868967, recall 0.889838
2017-12-10T13:46:52.601324: step 4483, loss 0.0887889, acc 0.96875, prec 0.0868928, recall 0.889838
2017-12-10T13:46:53.049123: step 4484, loss 0.148365, acc 0.953125, prec 0.0868869, recall 0.889838
2017-12-10T13:46:53.506912: step 4485, loss 0.0264484, acc 0.984375, prec 0.0869055, recall 0.889864
2017-12-10T13:46:53.967249: step 4486, loss 0.403465, acc 0.953125, prec 0.0869202, recall 0.889889
2017-12-10T13:46:54.421223: step 4487, loss 0.085783, acc 1, prec 0.0869614, recall 0.88994
2017-12-10T13:46:54.877311: step 4488, loss 0.0457115, acc 0.984375, prec 0.0869595, recall 0.88994
2017-12-10T13:46:55.324442: step 4489, loss 0.193875, acc 0.9375, prec 0.0869516, recall 0.88994
2017-12-10T13:46:55.766949: step 4490, loss 0.13631, acc 0.96875, prec 0.0869477, recall 0.88994
2017-12-10T13:46:56.219865: step 4491, loss 0.0507946, acc 0.984375, prec 0.0869663, recall 0.889965
2017-12-10T13:46:56.666286: step 4492, loss 0.0321581, acc 0.984375, prec 0.0869849, recall 0.889991
2017-12-10T13:46:57.122086: step 4493, loss 0.161731, acc 0.953125, prec 0.0869996, recall 0.890016
2017-12-10T13:46:57.564290: step 4494, loss 0.0267282, acc 0.984375, prec 0.0869977, recall 0.890016
2017-12-10T13:46:58.005521: step 4495, loss 0.0131347, acc 1, prec 0.0870183, recall 0.890041
2017-12-10T13:46:58.459720: step 4496, loss 0.042787, acc 0.984375, prec 0.0870163, recall 0.890041
2017-12-10T13:46:58.901871: step 4497, loss 0.0686182, acc 0.96875, prec 0.0870329, recall 0.890067
2017-12-10T13:46:59.351214: step 4498, loss 0.053047, acc 0.96875, prec 0.087029, recall 0.890067
2017-12-10T13:46:59.805247: step 4499, loss 0.146974, acc 0.953125, prec 0.0870231, recall 0.890067
2017-12-10T13:47:00.258466: step 4500, loss 0.11752, acc 0.953125, prec 0.0870173, recall 0.890067
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4500

2017-12-10T13:47:02.297982: step 4501, loss 0.117577, acc 0.96875, prec 0.0870133, recall 0.890067
2017-12-10T13:47:02.745811: step 4502, loss 0.21557, acc 0.96875, prec 0.0870506, recall 0.890117
2017-12-10T13:47:03.200009: step 4503, loss 0.011186, acc 1, prec 0.0870506, recall 0.890117
2017-12-10T13:47:03.648076: step 4504, loss 0.0272687, acc 0.984375, prec 0.0870486, recall 0.890117
2017-12-10T13:47:04.095815: step 4505, loss 0.764803, acc 0.9375, prec 0.0870819, recall 0.890168
2017-12-10T13:47:04.537935: step 4506, loss 2.14864, acc 0.984375, prec 0.0871024, recall 0.889988
2017-12-10T13:47:04.987254: step 4507, loss 0.135518, acc 0.96875, prec 0.0871191, recall 0.890014
2017-12-10T13:47:05.439778: step 4508, loss 0.845267, acc 0.984375, prec 0.0871582, recall 0.890064
2017-12-10T13:47:05.888838: step 4509, loss 0.115312, acc 0.96875, prec 0.0872365, recall 0.890165
2017-12-10T13:47:06.344611: step 4510, loss 0.0803772, acc 0.96875, prec 0.0872326, recall 0.890165
2017-12-10T13:47:06.785838: step 4511, loss 0.0607202, acc 0.984375, prec 0.0872306, recall 0.890165
2017-12-10T13:47:07.237455: step 4512, loss 0.276039, acc 0.9375, prec 0.0872433, recall 0.890191
2017-12-10T13:47:07.710126: step 4513, loss 0.45156, acc 0.859375, prec 0.0872668, recall 0.890241
2017-12-10T13:47:08.158495: step 4514, loss 0.300865, acc 0.9375, prec 0.0872794, recall 0.890266
2017-12-10T13:47:08.604034: step 4515, loss 0.195844, acc 0.96875, prec 0.0872755, recall 0.890266
2017-12-10T13:47:09.045178: step 4516, loss 0.379406, acc 0.890625, prec 0.0872823, recall 0.890292
2017-12-10T13:47:09.496136: step 4517, loss 0.120567, acc 0.96875, prec 0.0872784, recall 0.890292
2017-12-10T13:47:09.947062: step 4518, loss 0.473125, acc 0.921875, prec 0.0873917, recall 0.890442
2017-12-10T13:47:10.402713: step 4519, loss 0.114225, acc 0.984375, prec 0.0874103, recall 0.890467
2017-12-10T13:47:10.841051: step 4520, loss 0.219678, acc 0.921875, prec 0.087421, recall 0.890493
2017-12-10T13:47:11.275972: step 4521, loss 0.320231, acc 0.90625, prec 0.0874503, recall 0.890543
2017-12-10T13:47:11.713518: step 4522, loss 0.18439, acc 0.921875, prec 0.0874404, recall 0.890543
2017-12-10T13:47:12.150926: step 4523, loss 0.451731, acc 0.90625, prec 0.0874697, recall 0.890593
2017-12-10T13:47:12.592240: step 4524, loss 0.162829, acc 0.90625, prec 0.0874784, recall 0.890618
2017-12-10T13:47:13.048172: step 4525, loss 0.232096, acc 0.9375, prec 0.087532, recall 0.890693
2017-12-10T13:47:13.489178: step 4526, loss 0.311269, acc 0.90625, prec 0.0875612, recall 0.890743
2017-12-10T13:47:13.955367: step 4527, loss 0.377827, acc 0.953125, prec 0.0875758, recall 0.890768
2017-12-10T13:47:14.390435: step 4528, loss 0.0590109, acc 0.984375, prec 0.0875739, recall 0.890768
2017-12-10T13:47:14.836414: step 4529, loss 0.0372122, acc 0.984375, prec 0.0875924, recall 0.890793
2017-12-10T13:47:15.275521: step 4530, loss 0.065681, acc 0.96875, prec 0.0875885, recall 0.890793
2017-12-10T13:47:15.722648: step 4531, loss 0.0932556, acc 0.9375, prec 0.0875806, recall 0.890793
2017-12-10T13:47:16.169083: step 4532, loss 0.222438, acc 0.984375, prec 0.0875991, recall 0.890818
2017-12-10T13:47:16.617163: step 4533, loss 0.123583, acc 0.921875, prec 0.0876303, recall 0.890868
2017-12-10T13:47:17.055592: step 4534, loss 0.0417936, acc 1, prec 0.0876507, recall 0.890893
2017-12-10T13:47:17.513544: step 4535, loss 0.121777, acc 0.953125, prec 0.0876653, recall 0.890917
2017-12-10T13:47:17.959513: step 4536, loss 0.0791922, acc 0.984375, prec 0.0877043, recall 0.890967
2017-12-10T13:47:18.397335: step 4537, loss 2.2973, acc 0.96875, prec 0.0877228, recall 0.890789
2017-12-10T13:47:18.852518: step 4538, loss 0.337127, acc 0.90625, prec 0.087711, recall 0.890789
2017-12-10T13:47:19.301156: step 4539, loss 0.120337, acc 0.953125, prec 0.0877051, recall 0.890789
2017-12-10T13:47:19.744364: step 4540, loss 0.306203, acc 0.953125, prec 0.0877197, recall 0.890814
2017-12-10T13:47:20.194697: step 4541, loss 0.0316396, acc 0.984375, prec 0.0877382, recall 0.890839
2017-12-10T13:47:20.637792: step 4542, loss 0.151332, acc 0.96875, prec 0.0877547, recall 0.890864
2017-12-10T13:47:21.082444: step 4543, loss 0.323733, acc 0.953125, prec 0.0877693, recall 0.890888
2017-12-10T13:47:21.541607: step 4544, loss 0.408623, acc 0.921875, prec 0.0877595, recall 0.890888
2017-12-10T13:47:21.992734: step 4545, loss 0.15175, acc 0.9375, prec 0.087772, recall 0.890913
2017-12-10T13:47:22.440647: step 4546, loss 0.265413, acc 0.953125, prec 0.0877661, recall 0.890913
2017-12-10T13:47:22.881799: step 4547, loss 0.772997, acc 0.890625, prec 0.0877524, recall 0.890913
2017-12-10T13:47:23.336859: step 4548, loss 0.691627, acc 0.90625, prec 0.0877405, recall 0.890913
2017-12-10T13:47:23.774677: step 4549, loss 0.266817, acc 0.921875, prec 0.0877512, recall 0.890938
2017-12-10T13:47:24.221158: step 4550, loss 0.183116, acc 0.90625, prec 0.0877394, recall 0.890938
2017-12-10T13:47:24.664332: step 4551, loss 0.199393, acc 0.921875, prec 0.08775, recall 0.890963
2017-12-10T13:47:25.115319: step 4552, loss 0.462051, acc 0.90625, prec 0.0877382, recall 0.890963
2017-12-10T13:47:25.558575: step 4553, loss 0.043462, acc 0.984375, prec 0.0877362, recall 0.890963
2017-12-10T13:47:26.007248: step 4554, loss 0.143212, acc 0.953125, prec 0.0877508, recall 0.890988
2017-12-10T13:47:26.449783: step 4555, loss 0.369015, acc 0.921875, prec 0.0877818, recall 0.891037
2017-12-10T13:47:26.899576: step 4556, loss 0.118284, acc 0.9375, prec 0.0877739, recall 0.891037
2017-12-10T13:47:27.353367: step 4557, loss 0.256156, acc 0.9375, prec 0.0877661, recall 0.891037
2017-12-10T13:47:27.819486: step 4558, loss 0.0218903, acc 1, prec 0.0877661, recall 0.891037
2017-12-10T13:47:28.271658: step 4559, loss 0.12692, acc 0.984375, prec 0.087805, recall 0.891087
2017-12-10T13:47:28.727584: step 4560, loss 0.300753, acc 0.9375, prec 0.0877971, recall 0.891087
2017-12-10T13:47:29.172687: step 4561, loss 0.0238666, acc 1, prec 0.0877971, recall 0.891087
2017-12-10T13:47:29.625551: step 4562, loss 0.0486694, acc 0.96875, prec 0.0878341, recall 0.891136
2017-12-10T13:47:30.063230: step 4563, loss 0.246124, acc 0.953125, prec 0.087869, recall 0.891186
2017-12-10T13:47:30.503918: step 4564, loss 0.0904723, acc 0.984375, prec 0.087867, recall 0.891186
2017-12-10T13:47:30.942132: step 4565, loss 0.0764923, acc 0.984375, prec 0.0878855, recall 0.891211
2017-12-10T13:47:31.379209: step 4566, loss 0.0433084, acc 0.984375, prec 0.087904, recall 0.891235
2017-12-10T13:47:31.822293: step 4567, loss 0.0684874, acc 0.96875, prec 0.0879205, recall 0.89126
2017-12-10T13:47:32.261395: step 4568, loss 0.0842381, acc 0.96875, prec 0.0879574, recall 0.891309
2017-12-10T13:47:32.691653: step 4569, loss 0.0670963, acc 0.984375, prec 0.0879554, recall 0.891309
2017-12-10T13:47:33.139784: step 4570, loss 0.0587093, acc 0.984375, prec 0.0879943, recall 0.891359
2017-12-10T13:47:33.589798: step 4571, loss 0.0126033, acc 1, prec 0.0880147, recall 0.891383
2017-12-10T13:47:34.029617: step 4572, loss 0.0193024, acc 1, prec 0.0880351, recall 0.891408
2017-12-10T13:47:34.468075: step 4573, loss 0.00421719, acc 1, prec 0.0880351, recall 0.891408
2017-12-10T13:47:34.913723: step 4574, loss 0.0974484, acc 0.96875, prec 0.088072, recall 0.891457
2017-12-10T13:47:35.361194: step 4575, loss 0.00688644, acc 1, prec 0.0881128, recall 0.891506
2017-12-10T13:47:35.815916: step 4576, loss 0.00914964, acc 1, prec 0.0881128, recall 0.891506
2017-12-10T13:47:36.288490: step 4577, loss 0.025853, acc 1, prec 0.0881128, recall 0.891506
2017-12-10T13:47:36.734539: step 4578, loss 1.53133, acc 0.984375, prec 0.0882129, recall 0.891629
2017-12-10T13:47:37.190209: step 4579, loss 0.150517, acc 0.9375, prec 0.088205, recall 0.891629
2017-12-10T13:47:37.656068: step 4580, loss 2.55406, acc 0.953125, prec 0.0882011, recall 0.891427
2017-12-10T13:47:38.105057: step 4581, loss 0.0384875, acc 0.984375, prec 0.0882399, recall 0.891476
2017-12-10T13:47:38.546860: step 4582, loss 2.35503, acc 0.9375, prec 0.088234, recall 0.891275
2017-12-10T13:47:39.004348: step 4583, loss 0.192384, acc 0.9375, prec 0.0882465, recall 0.891299
2017-12-10T13:47:39.451185: step 4584, loss 0.175209, acc 0.9375, prec 0.088259, recall 0.891324
2017-12-10T13:47:39.898249: step 4585, loss 0.485686, acc 0.875, prec 0.0882432, recall 0.891324
2017-12-10T13:47:40.350208: step 4586, loss 0.614456, acc 0.84375, prec 0.0882235, recall 0.891324
2017-12-10T13:47:40.796722: step 4587, loss 0.44982, acc 0.890625, prec 0.08823, recall 0.891349
2017-12-10T13:47:41.233387: step 4588, loss 0.766475, acc 0.828125, prec 0.0882287, recall 0.891373
2017-12-10T13:47:41.679781: step 4589, loss 0.884383, acc 0.75, prec 0.0881972, recall 0.891373
2017-12-10T13:47:42.123388: step 4590, loss 0.696581, acc 0.828125, prec 0.0882162, recall 0.891422
2017-12-10T13:47:42.557463: step 4591, loss 0.647204, acc 0.84375, prec 0.0881965, recall 0.891422
2017-12-10T13:47:42.994135: step 4592, loss 0.514737, acc 0.8125, prec 0.0881933, recall 0.891447
2017-12-10T13:47:43.452586: step 4593, loss 0.951802, acc 0.734375, prec 0.0881598, recall 0.891447
2017-12-10T13:47:43.889421: step 4594, loss 1.08164, acc 0.703125, prec 0.0881428, recall 0.891471
2017-12-10T13:47:44.325633: step 4595, loss 1.03479, acc 0.75, prec 0.0881113, recall 0.891471
2017-12-10T13:47:44.763892: step 4596, loss 0.486086, acc 0.84375, prec 0.0880917, recall 0.891471
2017-12-10T13:47:45.214204: step 4597, loss 0.401732, acc 0.828125, prec 0.0880701, recall 0.891471
2017-12-10T13:47:45.655545: step 4598, loss 0.378734, acc 0.90625, prec 0.0880583, recall 0.891471
2017-12-10T13:47:46.106890: step 4599, loss 0.348227, acc 0.890625, prec 0.0880446, recall 0.891471
2017-12-10T13:47:46.553060: step 4600, loss 0.514984, acc 0.859375, prec 0.0880879, recall 0.891545
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4600

2017-12-10T13:47:48.425157: step 4601, loss 0.502487, acc 0.875, prec 0.0880722, recall 0.891545
2017-12-10T13:47:48.877482: step 4602, loss 0.191702, acc 0.890625, prec 0.0880787, recall 0.891569
2017-12-10T13:47:49.317657: step 4603, loss 0.215216, acc 0.9375, prec 0.0881724, recall 0.891691
2017-12-10T13:47:49.757530: step 4604, loss 0.10079, acc 0.96875, prec 0.0881685, recall 0.891691
2017-12-10T13:47:50.191645: step 4605, loss 0.364621, acc 0.921875, prec 0.088179, recall 0.891715
2017-12-10T13:47:50.641884: step 4606, loss 0.170855, acc 0.9375, prec 0.0881914, recall 0.89174
2017-12-10T13:47:51.085351: step 4607, loss 0.0293138, acc 0.984375, prec 0.0881895, recall 0.89174
2017-12-10T13:47:51.549099: step 4608, loss 0.0302049, acc 1, prec 0.0881895, recall 0.89174
2017-12-10T13:47:51.982349: step 4609, loss 0.0842362, acc 0.96875, prec 0.0881855, recall 0.89174
2017-12-10T13:47:52.420274: step 4610, loss 0.0580665, acc 0.96875, prec 0.0881816, recall 0.89174
2017-12-10T13:47:52.863925: step 4611, loss 0.00947608, acc 1, prec 0.0881816, recall 0.89174
2017-12-10T13:47:53.316994: step 4612, loss 0.00458987, acc 1, prec 0.0881816, recall 0.89174
2017-12-10T13:47:53.764680: step 4613, loss 0.00924753, acc 1, prec 0.0882019, recall 0.891764
2017-12-10T13:47:54.211094: step 4614, loss 0.0632544, acc 0.96875, prec 0.0882183, recall 0.891789
2017-12-10T13:47:54.658529: step 4615, loss 0.383297, acc 0.953125, prec 0.0882124, recall 0.891789
2017-12-10T13:47:55.115535: step 4616, loss 9.38782, acc 0.953125, prec 0.0882287, recall 0.891612
2017-12-10T13:47:55.571334: step 4617, loss 0.0888931, acc 0.96875, prec 0.0882248, recall 0.891612
2017-12-10T13:47:56.025091: step 4618, loss 0.274386, acc 0.9375, prec 0.088217, recall 0.891612
2017-12-10T13:47:56.461152: step 4619, loss 0.0408978, acc 1, prec 0.0882778, recall 0.891685
2017-12-10T13:47:56.911432: step 4620, loss 0.0561882, acc 0.96875, prec 0.0882739, recall 0.891685
2017-12-10T13:47:57.361534: step 4621, loss 0.29234, acc 0.96875, prec 0.0883105, recall 0.891734
2017-12-10T13:47:57.813356: step 4622, loss 0.0502137, acc 0.96875, prec 0.0883472, recall 0.891783
2017-12-10T13:47:58.264770: step 4623, loss 0.0830223, acc 0.96875, prec 0.0883432, recall 0.891783
2017-12-10T13:47:58.718074: step 4624, loss 0.214407, acc 0.953125, prec 0.0883373, recall 0.891783
2017-12-10T13:47:59.159122: step 4625, loss 0.16615, acc 0.953125, prec 0.0883517, recall 0.891807
2017-12-10T13:47:59.596599: step 4626, loss 0.233109, acc 0.9375, prec 0.0883439, recall 0.891807
2017-12-10T13:48:00.039819: step 4627, loss 0.320903, acc 0.953125, prec 0.0883785, recall 0.891856
2017-12-10T13:48:00.483563: step 4628, loss 0.362961, acc 0.9375, prec 0.0883706, recall 0.891856
2017-12-10T13:48:00.924774: step 4629, loss 0.369063, acc 0.953125, prec 0.0883648, recall 0.891856
2017-12-10T13:48:01.372268: step 4630, loss 0.175059, acc 0.953125, prec 0.0883791, recall 0.89188
2017-12-10T13:48:01.809005: step 4631, loss 0.088385, acc 0.96875, prec 0.0883955, recall 0.891904
2017-12-10T13:48:02.250674: step 4632, loss 0.170505, acc 0.953125, prec 0.0883896, recall 0.891904
2017-12-10T13:48:02.684611: step 4633, loss 0.235233, acc 0.96875, prec 0.0883856, recall 0.891904
2017-12-10T13:48:03.119895: step 4634, loss 0.226808, acc 0.90625, prec 0.0883738, recall 0.891904
2017-12-10T13:48:03.566309: step 4635, loss 0.116805, acc 0.96875, prec 0.0883699, recall 0.891904
2017-12-10T13:48:04.015308: step 4636, loss 0.184562, acc 0.90625, prec 0.0884392, recall 0.892001
2017-12-10T13:48:04.457510: step 4637, loss 0.233692, acc 0.9375, prec 0.0884515, recall 0.892025
2017-12-10T13:48:04.901285: step 4638, loss 0.229794, acc 0.9375, prec 0.0884639, recall 0.892049
2017-12-10T13:48:05.355137: step 4639, loss 0.0239215, acc 1, prec 0.0884639, recall 0.892049
2017-12-10T13:48:05.799061: step 4640, loss 0.119764, acc 0.953125, prec 0.088458, recall 0.892049
2017-12-10T13:48:06.256502: step 4641, loss 0.0413535, acc 0.984375, prec 0.0884561, recall 0.892049
2017-12-10T13:48:06.695666: step 4642, loss 0.329947, acc 0.90625, prec 0.0884443, recall 0.892049
2017-12-10T13:48:07.142641: step 4643, loss 0.122159, acc 0.96875, prec 0.0884606, recall 0.892073
2017-12-10T13:48:07.589455: step 4644, loss 0.322101, acc 0.96875, prec 0.0884971, recall 0.892122
2017-12-10T13:48:08.034885: step 4645, loss 0.076199, acc 0.96875, prec 0.0884932, recall 0.892122
2017-12-10T13:48:08.498903: step 4646, loss 0.0820659, acc 0.96875, prec 0.0884893, recall 0.892122
2017-12-10T13:48:08.952663: step 4647, loss 0.0340847, acc 0.984375, prec 0.0885076, recall 0.892146
2017-12-10T13:48:09.400043: step 4648, loss 0.0538511, acc 0.984375, prec 0.0885056, recall 0.892146
2017-12-10T13:48:09.849564: step 4649, loss 0.0183947, acc 1, prec 0.0885056, recall 0.892146
2017-12-10T13:48:10.297795: step 4650, loss 0.0593201, acc 0.96875, prec 0.0885017, recall 0.892146
2017-12-10T13:48:10.751027: step 4651, loss 0.174366, acc 0.953125, prec 0.0885362, recall 0.892194
2017-12-10T13:48:11.182662: step 4652, loss 0.160077, acc 0.984375, prec 0.0885343, recall 0.892194
2017-12-10T13:48:11.622298: step 4653, loss 0.274112, acc 0.953125, prec 0.0885486, recall 0.892218
2017-12-10T13:48:12.081169: step 4654, loss 0.0610666, acc 0.96875, prec 0.0885447, recall 0.892218
2017-12-10T13:48:12.546621: step 4655, loss 0.0517997, acc 0.984375, prec 0.0885629, recall 0.892242
2017-12-10T13:48:12.996273: step 4656, loss 0.233545, acc 0.953125, prec 0.088557, recall 0.892242
2017-12-10T13:48:13.440390: step 4657, loss 1.32806, acc 0.953125, prec 0.0885531, recall 0.892043
2017-12-10T13:48:13.897410: step 4658, loss 0.0105124, acc 1, prec 0.0885531, recall 0.892043
2017-12-10T13:48:14.343620: step 4659, loss 1.79576, acc 0.984375, prec 0.0885531, recall 0.891844
2017-12-10T13:48:14.794608: step 4660, loss 6.66977, acc 0.921875, prec 0.0885857, recall 0.891693
2017-12-10T13:48:15.251753: step 4661, loss 0.260958, acc 0.953125, prec 0.0886, recall 0.891717
2017-12-10T13:48:15.696112: step 4662, loss 0.114847, acc 0.953125, prec 0.0886346, recall 0.891765
2017-12-10T13:48:16.155382: step 4663, loss 0.203754, acc 0.953125, prec 0.0886489, recall 0.891789
2017-12-10T13:48:16.603631: step 4664, loss 0.60104, acc 0.828125, prec 0.0886677, recall 0.891838
2017-12-10T13:48:17.048364: step 4665, loss 0.734261, acc 0.84375, prec 0.0886682, recall 0.891862
2017-12-10T13:48:17.511006: step 4666, loss 0.433156, acc 0.8125, prec 0.0886446, recall 0.891862
2017-12-10T13:48:17.963069: step 4667, loss 1.04381, acc 0.65625, prec 0.0886014, recall 0.891862
2017-12-10T13:48:18.420568: step 4668, loss 1.20571, acc 0.734375, prec 0.0885681, recall 0.891862
2017-12-10T13:48:18.864631: step 4669, loss 1.02259, acc 0.71875, prec 0.0885731, recall 0.89191
2017-12-10T13:48:19.308809: step 4670, loss 1.11951, acc 0.703125, prec 0.0885561, recall 0.891934
2017-12-10T13:48:19.759935: step 4671, loss 0.628753, acc 0.859375, prec 0.0885586, recall 0.891958
2017-12-10T13:48:20.212932: step 4672, loss 0.844568, acc 0.8125, prec 0.0885351, recall 0.891958
2017-12-10T13:48:20.647908: step 4673, loss 1.46216, acc 0.703125, prec 0.0885584, recall 0.89203
2017-12-10T13:48:21.098284: step 4674, loss 0.649132, acc 0.828125, prec 0.0885972, recall 0.892102
2017-12-10T13:48:21.539498: step 4675, loss 1.15656, acc 0.765625, prec 0.088588, recall 0.892126
2017-12-10T13:48:21.981907: step 4676, loss 0.42241, acc 0.875, prec 0.0886327, recall 0.892198
2017-12-10T13:48:22.444876: step 4677, loss 0.442184, acc 0.84375, prec 0.0886132, recall 0.892198
2017-12-10T13:48:22.895759: step 4678, loss 0.336359, acc 0.890625, prec 0.0886598, recall 0.89227
2017-12-10T13:48:23.354152: step 4679, loss 0.217195, acc 0.890625, prec 0.0886461, recall 0.89227
2017-12-10T13:48:23.790709: step 4680, loss 0.545161, acc 0.890625, prec 0.0886526, recall 0.892294
2017-12-10T13:48:24.244829: step 4681, loss 0.291821, acc 0.921875, prec 0.088683, recall 0.892342
2017-12-10T13:48:24.680949: step 4682, loss 0.431838, acc 0.875, prec 0.0886673, recall 0.892342
2017-12-10T13:48:25.127948: step 4683, loss 0.10937, acc 0.96875, prec 0.0886835, recall 0.892366
2017-12-10T13:48:25.562941: step 4684, loss 0.0533041, acc 0.984375, prec 0.0886816, recall 0.892366
2017-12-10T13:48:26.005361: step 4685, loss 0.425637, acc 0.90625, prec 0.0886899, recall 0.89239
2017-12-10T13:48:26.466566: step 4686, loss 0.407402, acc 0.9375, prec 0.0887022, recall 0.892413
2017-12-10T13:48:26.912194: step 4687, loss 0.0604193, acc 0.96875, prec 0.0887184, recall 0.892437
2017-12-10T13:48:27.363494: step 4688, loss 0.0153227, acc 0.984375, prec 0.0887164, recall 0.892437
2017-12-10T13:48:27.807105: step 4689, loss 0.0251603, acc 1, prec 0.0887365, recall 0.892461
2017-12-10T13:48:28.259993: step 4690, loss 0.0604991, acc 0.96875, prec 0.0887326, recall 0.892461
2017-12-10T13:48:28.697652: step 4691, loss 0.0736183, acc 0.953125, prec 0.0887267, recall 0.892461
2017-12-10T13:48:29.141733: step 4692, loss 1.90417, acc 0.953125, prec 0.088763, recall 0.892311
2017-12-10T13:48:29.586550: step 4693, loss 0.0255051, acc 1, prec 0.088763, recall 0.892311
2017-12-10T13:48:30.038294: step 4694, loss 0.00843526, acc 1, prec 0.0887831, recall 0.892335
2017-12-10T13:48:30.477218: step 4695, loss 0.0347008, acc 0.984375, prec 0.0888414, recall 0.892406
2017-12-10T13:48:30.927277: step 4696, loss 0.0282437, acc 1, prec 0.0888615, recall 0.89243
2017-12-10T13:48:31.381187: step 4697, loss 0.550647, acc 0.953125, prec 0.0888757, recall 0.892454
2017-12-10T13:48:31.828141: step 4698, loss 0.195401, acc 0.953125, prec 0.0888899, recall 0.892478
2017-12-10T13:48:32.276835: step 4699, loss 0.178514, acc 0.9375, prec 0.088882, recall 0.892478
2017-12-10T13:48:32.712032: step 4700, loss 0.187636, acc 0.953125, prec 0.0888962, recall 0.892502
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4700

2017-12-10T13:48:34.687476: step 4701, loss 0.238796, acc 0.9375, prec 0.0888884, recall 0.892502
2017-12-10T13:48:35.045677: step 4702, loss 0.0560758, acc 0.96875, prec 0.0889246, recall 0.892549
2017-12-10T13:48:35.444996: step 4703, loss 0.126916, acc 0.953125, prec 0.0889187, recall 0.892549
2017-12-10T13:48:35.841056: step 4704, loss 0.0450966, acc 0.96875, prec 0.0889148, recall 0.892549
2017-12-10T13:48:36.235980: step 4705, loss 0.143737, acc 0.953125, prec 0.088909, recall 0.892549
2017-12-10T13:48:36.631056: step 4706, loss 0.0877967, acc 0.96875, prec 0.0889452, recall 0.892597
2017-12-10T13:48:37.026113: step 4707, loss 0.76889, acc 0.984375, prec 0.0889833, recall 0.892644
2017-12-10T13:48:37.489302: step 4708, loss 0.0786022, acc 0.984375, prec 0.0890014, recall 0.892668
2017-12-10T13:48:37.942818: step 4709, loss 0.213713, acc 0.9375, prec 0.0889936, recall 0.892668
2017-12-10T13:48:38.405905: step 4710, loss 0.0182755, acc 1, prec 0.0890137, recall 0.892692
2017-12-10T13:48:38.858634: step 4711, loss 0.168482, acc 0.953125, prec 0.0890078, recall 0.892692
2017-12-10T13:48:39.311265: step 4712, loss 0.205848, acc 0.96875, prec 0.0890039, recall 0.892692
2017-12-10T13:48:39.743449: step 4713, loss 0.0936616, acc 0.96875, prec 0.0889999, recall 0.892692
2017-12-10T13:48:40.194927: step 4714, loss 0.338199, acc 0.921875, prec 0.0889901, recall 0.892692
2017-12-10T13:48:40.651061: step 4715, loss 0.108027, acc 0.984375, prec 0.0890082, recall 0.892715
2017-12-10T13:48:41.095606: step 4716, loss 0.085965, acc 0.96875, prec 0.0890043, recall 0.892715
2017-12-10T13:48:41.557198: step 4717, loss 0.0119992, acc 1, prec 0.0890043, recall 0.892715
2017-12-10T13:48:42.012311: step 4718, loss 0.164826, acc 0.9375, prec 0.0890165, recall 0.892739
2017-12-10T13:48:42.484566: step 4719, loss 0.285477, acc 0.984375, prec 0.0890547, recall 0.892786
2017-12-10T13:48:42.941404: step 4720, loss 0.168343, acc 0.953125, prec 0.0890488, recall 0.892786
2017-12-10T13:48:43.378185: step 4721, loss 0.658152, acc 0.921875, prec 0.089059, recall 0.89281
2017-12-10T13:48:43.811947: step 4722, loss 0.23944, acc 0.9375, prec 0.0890512, recall 0.89281
2017-12-10T13:48:44.256598: step 4723, loss 0.0909868, acc 0.96875, prec 0.0890874, recall 0.892857
2017-12-10T13:48:44.685553: step 4724, loss 0.157983, acc 0.9375, prec 0.0891196, recall 0.892904
2017-12-10T13:48:45.136094: step 4725, loss 0.329935, acc 0.96875, prec 0.0891958, recall 0.892999
2017-12-10T13:48:45.592299: step 4726, loss 0.358819, acc 0.953125, prec 0.0892099, recall 0.893022
2017-12-10T13:48:46.037794: step 4727, loss 0.133176, acc 0.984375, prec 0.089248, recall 0.893069
2017-12-10T13:48:46.494711: step 4728, loss 0.234583, acc 0.953125, prec 0.0892622, recall 0.893093
2017-12-10T13:48:46.937213: step 4729, loss 0.0836369, acc 0.953125, prec 0.0892563, recall 0.893093
2017-12-10T13:48:47.372833: step 4730, loss 0.212495, acc 0.96875, prec 0.0892924, recall 0.89314
2017-12-10T13:48:47.812747: step 4731, loss 0.24911, acc 0.96875, prec 0.0893085, recall 0.893163
2017-12-10T13:48:48.269187: step 4732, loss 0.000593438, acc 1, prec 0.0893485, recall 0.89321
2017-12-10T13:48:48.721573: step 4733, loss 1.06111, acc 0.984375, prec 0.0893866, recall 0.893257
2017-12-10T13:48:49.188385: step 4734, loss 0.245585, acc 0.921875, prec 0.0893768, recall 0.893257
2017-12-10T13:48:49.633027: step 4735, loss 0.0218311, acc 1, prec 0.0893968, recall 0.893281
2017-12-10T13:48:50.094736: step 4736, loss 0.0451161, acc 0.984375, prec 0.0894148, recall 0.893304
2017-12-10T13:48:50.550148: step 4737, loss 0.0520078, acc 0.984375, prec 0.0894529, recall 0.893351
2017-12-10T13:48:51.005719: step 4738, loss 0.689338, acc 0.921875, prec 0.089463, recall 0.893374
2017-12-10T13:48:51.463307: step 4739, loss 0.0685024, acc 0.96875, prec 0.0894791, recall 0.893398
2017-12-10T13:48:51.911056: step 4740, loss 0.168972, acc 0.953125, prec 0.0894932, recall 0.893421
2017-12-10T13:48:52.371569: step 4741, loss 0.104699, acc 0.953125, prec 0.0895473, recall 0.893491
2017-12-10T13:48:52.821677: step 4742, loss 0.0753059, acc 0.96875, prec 0.0895434, recall 0.893491
2017-12-10T13:48:53.262519: step 4743, loss 0.0975979, acc 0.953125, prec 0.0895575, recall 0.893514
2017-12-10T13:48:53.705909: step 4744, loss 0.0267471, acc 0.984375, prec 0.0895955, recall 0.893561
2017-12-10T13:48:54.154763: step 4745, loss 0.240685, acc 0.890625, prec 0.0896017, recall 0.893584
2017-12-10T13:48:54.600677: step 4746, loss 0.103971, acc 0.953125, prec 0.0896358, recall 0.893631
2017-12-10T13:48:55.059713: step 4747, loss 0.232215, acc 0.90625, prec 0.089644, recall 0.893654
2017-12-10T13:48:55.501061: step 4748, loss 0.259316, acc 0.953125, prec 0.0896381, recall 0.893654
2017-12-10T13:48:55.940193: step 4749, loss 0.277841, acc 0.96875, prec 0.0896341, recall 0.893654
2017-12-10T13:48:56.396732: step 4750, loss 0.154421, acc 0.953125, prec 0.0896282, recall 0.893654
2017-12-10T13:48:56.843424: step 4751, loss 0.153768, acc 0.953125, prec 0.0896423, recall 0.893678
2017-12-10T13:48:57.297670: step 4752, loss 0.253948, acc 0.921875, prec 0.0896524, recall 0.893701
2017-12-10T13:48:57.742371: step 4753, loss 0.459295, acc 0.921875, prec 0.0896426, recall 0.893701
2017-12-10T13:48:58.183614: step 4754, loss 0.151239, acc 0.96875, prec 0.0896786, recall 0.893747
2017-12-10T13:48:58.638823: step 4755, loss 0.186937, acc 0.9375, prec 0.0897107, recall 0.893794
2017-12-10T13:48:59.081988: step 4756, loss 0.0459457, acc 0.96875, prec 0.0897267, recall 0.893817
2017-12-10T13:48:59.520469: step 4757, loss 0.309175, acc 0.921875, prec 0.0897169, recall 0.893817
2017-12-10T13:48:59.968388: step 4758, loss 0.283516, acc 0.96875, prec 0.0897728, recall 0.893886
2017-12-10T13:49:00.403251: step 4759, loss 0.0981832, acc 0.96875, prec 0.0898088, recall 0.893933
2017-12-10T13:49:00.850512: step 4760, loss 0.515561, acc 0.96875, prec 0.0898248, recall 0.893956
2017-12-10T13:49:01.305862: step 4761, loss 0.194169, acc 0.9375, prec 0.0898169, recall 0.893956
2017-12-10T13:49:01.757743: step 4762, loss 0.399991, acc 0.921875, prec 0.089867, recall 0.894025
2017-12-10T13:49:02.214274: step 4763, loss 0.067105, acc 0.96875, prec 0.089863, recall 0.894025
2017-12-10T13:49:02.658845: step 4764, loss 0.0873733, acc 0.984375, prec 0.089881, recall 0.894048
2017-12-10T13:49:03.096724: step 4765, loss 0.0395159, acc 0.984375, prec 0.089899, recall 0.894072
2017-12-10T13:49:03.539844: step 4766, loss 0.166032, acc 0.953125, prec 0.089913, recall 0.894095
2017-12-10T13:49:03.986759: step 4767, loss 0.223275, acc 0.921875, prec 0.089943, recall 0.894141
2017-12-10T13:49:04.440987: step 4768, loss 0.535007, acc 0.953125, prec 0.089977, recall 0.894187
2017-12-10T13:49:04.876149: step 4769, loss 0.145838, acc 0.953125, prec 0.090011, recall 0.894233
2017-12-10T13:49:05.326106: step 4770, loss 0.00581377, acc 1, prec 0.0900309, recall 0.894256
2017-12-10T13:49:05.766275: step 4771, loss 0.0975805, acc 0.96875, prec 0.0900269, recall 0.894256
2017-12-10T13:49:06.215473: step 4772, loss 0.105945, acc 0.96875, prec 0.0900629, recall 0.894302
2017-12-10T13:49:06.659268: step 4773, loss 0.0903879, acc 0.96875, prec 0.0900589, recall 0.894302
2017-12-10T13:49:07.108835: step 4774, loss 0.168433, acc 0.96875, prec 0.090055, recall 0.894302
2017-12-10T13:49:07.559135: step 4775, loss 0.472964, acc 0.96875, prec 0.090051, recall 0.894302
2017-12-10T13:49:08.006274: step 4776, loss 0.454746, acc 0.96875, prec 0.0900869, recall 0.894348
2017-12-10T13:49:08.450764: step 4777, loss 0.0342825, acc 0.984375, prec 0.0901049, recall 0.894371
2017-12-10T13:49:08.899588: step 4778, loss 0.00991156, acc 1, prec 0.0901049, recall 0.894371
2017-12-10T13:49:09.354830: step 4779, loss 0.069289, acc 1, prec 0.0901248, recall 0.894394
2017-12-10T13:49:09.802446: step 4780, loss 0.182766, acc 0.9375, prec 0.0901169, recall 0.894394
2017-12-10T13:49:10.242156: step 4781, loss 0.18728, acc 0.984375, prec 0.0901349, recall 0.894417
2017-12-10T13:49:10.688479: step 4782, loss 0.218866, acc 0.953125, prec 0.0901489, recall 0.89444
2017-12-10T13:49:11.150634: step 4783, loss 0.0429911, acc 0.984375, prec 0.0901469, recall 0.89444
2017-12-10T13:49:11.599260: step 4784, loss 0.00637954, acc 1, prec 0.0901469, recall 0.89444
2017-12-10T13:49:12.049280: step 4785, loss 0.0237885, acc 0.984375, prec 0.0901648, recall 0.894463
2017-12-10T13:49:12.499007: step 4786, loss 0.0127194, acc 1, prec 0.0901648, recall 0.894463
2017-12-10T13:49:12.942137: step 4787, loss 0.0401678, acc 0.984375, prec 0.0901828, recall 0.894485
2017-12-10T13:49:13.393517: step 4788, loss 0.203547, acc 0.984375, prec 0.0902007, recall 0.894508
2017-12-10T13:49:13.835546: step 4789, loss 0.153318, acc 0.96875, prec 0.0902167, recall 0.894531
2017-12-10T13:49:14.267641: step 4790, loss 0.749888, acc 0.953125, prec 0.0902307, recall 0.894554
2017-12-10T13:49:14.716461: step 4791, loss 0.0234363, acc 0.984375, prec 0.0902287, recall 0.894554
2017-12-10T13:49:15.163167: step 4792, loss 0.103173, acc 0.953125, prec 0.0902427, recall 0.894577
2017-12-10T13:49:15.620493: step 4793, loss 0.0481813, acc 0.984375, prec 0.0902606, recall 0.8946
2017-12-10T13:49:16.089641: step 4794, loss 0.0921666, acc 0.96875, prec 0.0902766, recall 0.894623
2017-12-10T13:49:16.540749: step 4795, loss 0.0613253, acc 0.984375, prec 0.0903144, recall 0.894668
2017-12-10T13:49:16.991258: step 4796, loss 0.00362408, acc 1, prec 0.0903144, recall 0.894668
2017-12-10T13:49:17.433813: step 4797, loss 0.238236, acc 0.953125, prec 0.0903085, recall 0.894668
2017-12-10T13:49:17.881756: step 4798, loss 0.539952, acc 0.953125, prec 0.0903025, recall 0.894668
2017-12-10T13:49:18.325090: step 4799, loss 0.0163468, acc 1, prec 0.0903025, recall 0.894668
2017-12-10T13:49:18.765003: step 4800, loss 0.0506007, acc 0.984375, prec 0.0903006, recall 0.894668
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4800

2017-12-10T13:49:20.587315: step 4801, loss 0.437268, acc 0.9375, prec 0.0903126, recall 0.894691
2017-12-10T13:49:21.038455: step 4802, loss 0.00940647, acc 1, prec 0.0903325, recall 0.894714
2017-12-10T13:49:21.490074: step 4803, loss 0.229439, acc 0.9375, prec 0.0903246, recall 0.894714
2017-12-10T13:49:21.933175: step 4804, loss 0.0879404, acc 0.984375, prec 0.0903226, recall 0.894714
2017-12-10T13:49:22.382517: step 4805, loss 0.0309433, acc 0.984375, prec 0.0903206, recall 0.894714
2017-12-10T13:49:22.836146: step 4806, loss 0.17797, acc 0.9375, prec 0.0903326, recall 0.894737
2017-12-10T13:49:23.278640: step 4807, loss 0.254555, acc 0.921875, prec 0.0903227, recall 0.894737
2017-12-10T13:49:23.729818: step 4808, loss 0.263407, acc 0.96875, prec 0.0903188, recall 0.894737
2017-12-10T13:49:24.167190: step 4809, loss 0.0783546, acc 0.984375, prec 0.0903168, recall 0.894737
2017-12-10T13:49:24.625702: step 4810, loss 0.220726, acc 0.953125, prec 0.0903507, recall 0.894782
2017-12-10T13:49:25.082347: step 4811, loss 0.0418695, acc 0.984375, prec 0.0903884, recall 0.894828
2017-12-10T13:49:25.541836: step 4812, loss 0.0366969, acc 1, prec 0.0904083, recall 0.894851
2017-12-10T13:49:25.993894: step 4813, loss 0.0952178, acc 0.953125, prec 0.0904024, recall 0.894851
2017-12-10T13:49:26.446714: step 4814, loss 0.0551396, acc 0.984375, prec 0.0904203, recall 0.894873
2017-12-10T13:49:26.895903: step 4815, loss 0.168485, acc 0.953125, prec 0.0904343, recall 0.894896
2017-12-10T13:49:27.350760: step 4816, loss 5.04327, acc 0.96875, prec 0.0904521, recall 0.894725
2017-12-10T13:49:27.792546: step 4817, loss 0.0750977, acc 0.984375, prec 0.0905098, recall 0.894794
2017-12-10T13:49:28.242353: step 4818, loss 0.0354142, acc 0.96875, prec 0.0905257, recall 0.894816
2017-12-10T13:49:28.692112: step 4819, loss 0.225246, acc 0.9375, prec 0.0905377, recall 0.894839
2017-12-10T13:49:29.142808: step 4820, loss 0.238748, acc 0.921875, prec 0.0905477, recall 0.894862
2017-12-10T13:49:29.599731: step 4821, loss 0.0379265, acc 1, prec 0.0905675, recall 0.894885
2017-12-10T13:49:30.043077: step 4822, loss 0.0319092, acc 0.984375, prec 0.0905655, recall 0.894885
2017-12-10T13:49:30.489920: step 4823, loss 0.25611, acc 0.921875, prec 0.0905755, recall 0.894907
2017-12-10T13:49:30.937151: step 4824, loss 0.308555, acc 0.9375, prec 0.0905676, recall 0.894907
2017-12-10T13:49:31.385259: step 4825, loss 0.111964, acc 0.96875, prec 0.0906034, recall 0.894953
2017-12-10T13:49:31.845223: step 4826, loss 0.0916421, acc 0.96875, prec 0.0906193, recall 0.894975
2017-12-10T13:49:32.296085: step 4827, loss 0.311566, acc 0.921875, prec 0.0906689, recall 0.895043
2017-12-10T13:49:32.740643: step 4828, loss 0.563465, acc 0.90625, prec 0.0906769, recall 0.895066
2017-12-10T13:49:33.181911: step 4829, loss 0.103994, acc 0.96875, prec 0.0906928, recall 0.895088
2017-12-10T13:49:33.630344: step 4830, loss 0.091627, acc 0.96875, prec 0.0906888, recall 0.895088
2017-12-10T13:49:34.078440: step 4831, loss 0.198568, acc 0.953125, prec 0.0907027, recall 0.895111
2017-12-10T13:49:34.530809: step 4832, loss 0.431679, acc 0.90625, prec 0.0907702, recall 0.895201
2017-12-10T13:49:34.972645: step 4833, loss 0.0777956, acc 0.96875, prec 0.0907663, recall 0.895201
2017-12-10T13:49:35.424292: step 4834, loss 0.560346, acc 0.875, prec 0.0907703, recall 0.895224
2017-12-10T13:49:35.891109: step 4835, loss 0.279929, acc 0.953125, prec 0.0908238, recall 0.895291
2017-12-10T13:49:36.336069: step 4836, loss 0.235872, acc 0.96875, prec 0.0908199, recall 0.895291
2017-12-10T13:49:36.787585: step 4837, loss 0.137574, acc 0.96875, prec 0.0908159, recall 0.895291
2017-12-10T13:49:37.240273: step 4838, loss 0.0304659, acc 0.984375, prec 0.0908536, recall 0.895336
2017-12-10T13:49:37.672665: step 4839, loss 0.0952949, acc 0.96875, prec 0.0908893, recall 0.895381
2017-12-10T13:49:38.116337: step 4840, loss 0.378548, acc 0.90625, prec 0.0908774, recall 0.895381
2017-12-10T13:49:38.559463: step 4841, loss 0.111905, acc 0.96875, prec 0.0908932, recall 0.895404
2017-12-10T13:49:39.008931: step 4842, loss 0.274693, acc 0.921875, prec 0.0908833, recall 0.895404
2017-12-10T13:49:39.459142: step 4843, loss 0.0864001, acc 0.953125, prec 0.0908774, recall 0.895404
2017-12-10T13:49:39.910844: step 4844, loss 0.202779, acc 0.9375, prec 0.0908695, recall 0.895404
2017-12-10T13:49:40.352400: step 4845, loss 0.146148, acc 0.984375, prec 0.0909071, recall 0.895449
2017-12-10T13:49:40.806583: step 4846, loss 0.182202, acc 0.96875, prec 0.090923, recall 0.895471
2017-12-10T13:49:41.250642: step 4847, loss 0.13471, acc 0.984375, prec 0.0909606, recall 0.895516
2017-12-10T13:49:41.702921: step 4848, loss 0.159296, acc 0.953125, prec 0.0909745, recall 0.895538
2017-12-10T13:49:42.150811: step 4849, loss 0.214302, acc 0.921875, prec 0.0909844, recall 0.895561
2017-12-10T13:49:42.593185: step 4850, loss 0.171719, acc 0.9375, prec 0.0909764, recall 0.895561
2017-12-10T13:49:43.038106: step 4851, loss 0.0439814, acc 0.984375, prec 0.0909744, recall 0.895561
2017-12-10T13:49:43.495550: step 4852, loss 0.447599, acc 0.96875, prec 0.0910101, recall 0.895606
2017-12-10T13:49:43.947218: step 4853, loss 0.228979, acc 0.953125, prec 0.0910437, recall 0.89565
2017-12-10T13:49:44.396202: step 4854, loss 0.569259, acc 0.9375, prec 0.0910754, recall 0.895695
2017-12-10T13:49:44.836373: step 4855, loss 0.787574, acc 1, prec 0.091115, recall 0.89574
2017-12-10T13:49:45.285107: step 4856, loss 0.184925, acc 0.96875, prec 0.0911308, recall 0.895762
2017-12-10T13:49:45.730060: step 4857, loss 0.0259606, acc 0.984375, prec 0.0911288, recall 0.895762
2017-12-10T13:49:46.183838: step 4858, loss 0.125039, acc 0.96875, prec 0.0911249, recall 0.895762
2017-12-10T13:49:46.642998: step 4859, loss 0.20273, acc 0.921875, prec 0.0911743, recall 0.895829
2017-12-10T13:49:47.088924: step 4860, loss 0.114607, acc 0.953125, prec 0.0911683, recall 0.895829
2017-12-10T13:49:47.536599: step 4861, loss 0.0984557, acc 0.9375, prec 0.0911802, recall 0.895851
2017-12-10T13:49:47.986240: step 4862, loss 0.172836, acc 0.9375, prec 0.0912118, recall 0.895896
2017-12-10T13:49:48.429615: step 4863, loss 0.0426474, acc 1, prec 0.0912316, recall 0.895918
2017-12-10T13:49:48.878929: step 4864, loss 0.366347, acc 0.9375, prec 0.0912236, recall 0.895918
2017-12-10T13:49:49.325143: step 4865, loss 0.049589, acc 0.984375, prec 0.0912414, recall 0.89594
2017-12-10T13:49:49.779709: step 4866, loss 0.362157, acc 0.953125, prec 0.0912355, recall 0.89594
2017-12-10T13:49:50.232594: step 4867, loss 0.110314, acc 0.984375, prec 0.0912335, recall 0.89594
2017-12-10T13:49:50.678424: step 4868, loss 1.99152, acc 0.9375, prec 0.0912473, recall 0.895771
2017-12-10T13:49:51.135087: step 4869, loss 0.12859, acc 0.953125, prec 0.0912809, recall 0.895816
2017-12-10T13:49:51.575500: step 4870, loss 0.656196, acc 0.96875, prec 0.0912967, recall 0.895838
2017-12-10T13:49:52.027258: step 4871, loss 0.116547, acc 0.96875, prec 0.0913125, recall 0.89586
2017-12-10T13:49:52.480058: step 4872, loss 0.564562, acc 0.9375, prec 0.0913243, recall 0.895882
2017-12-10T13:49:52.932175: step 4873, loss 0.0902715, acc 0.96875, prec 0.0913599, recall 0.895927
2017-12-10T13:49:53.389968: step 4874, loss 0.280412, acc 0.921875, prec 0.0913697, recall 0.895949
2017-12-10T13:49:53.842626: step 4875, loss 0.444775, acc 0.9375, prec 0.0913815, recall 0.895971
2017-12-10T13:49:54.313182: step 4876, loss 0.187149, acc 0.953125, prec 0.091415, recall 0.896015
2017-12-10T13:49:54.752016: step 4877, loss 0.0594073, acc 0.953125, prec 0.0914486, recall 0.89606
2017-12-10T13:49:55.201172: step 4878, loss 0.341742, acc 0.890625, prec 0.0914347, recall 0.89606
2017-12-10T13:49:55.653592: step 4879, loss 2.42211, acc 0.90625, prec 0.0914445, recall 0.895891
2017-12-10T13:49:56.091653: step 4880, loss 0.333793, acc 0.90625, prec 0.0914523, recall 0.895913
2017-12-10T13:49:56.520817: step 4881, loss 0.730764, acc 0.890625, prec 0.0914384, recall 0.895913
2017-12-10T13:49:56.966823: step 4882, loss 0.637174, acc 0.875, prec 0.0914422, recall 0.895935
2017-12-10T13:49:57.419789: step 4883, loss 0.388901, acc 0.890625, prec 0.0914481, recall 0.895957
2017-12-10T13:49:57.865530: step 4884, loss 0.607569, acc 0.828125, prec 0.0914459, recall 0.89598
2017-12-10T13:49:58.318350: step 4885, loss 0.790972, acc 0.84375, prec 0.0914853, recall 0.896046
2017-12-10T13:49:58.758072: step 4886, loss 0.633773, acc 0.828125, prec 0.0914831, recall 0.896068
2017-12-10T13:49:59.201785: step 4887, loss 0.17841, acc 0.953125, prec 0.0914969, recall 0.89609
2017-12-10T13:49:59.641962: step 4888, loss 0.293096, acc 0.90625, prec 0.091485, recall 0.89609
2017-12-10T13:50:00.090348: step 4889, loss 0.367424, acc 0.875, prec 0.0915085, recall 0.896134
2017-12-10T13:50:00.531273: step 4890, loss 0.21737, acc 0.890625, prec 0.0914946, recall 0.896134
2017-12-10T13:50:00.979735: step 4891, loss 0.906971, acc 0.890625, prec 0.0914807, recall 0.896134
2017-12-10T13:50:01.437373: step 4892, loss 0.60239, acc 0.921875, prec 0.0914708, recall 0.896134
2017-12-10T13:50:01.875645: step 4893, loss 0.891152, acc 0.859375, prec 0.091453, recall 0.896134
2017-12-10T13:50:02.325826: step 4894, loss 0.0826325, acc 0.96875, prec 0.0914687, recall 0.896156
2017-12-10T13:50:02.770308: step 4895, loss 0.163787, acc 0.921875, prec 0.0914588, recall 0.896156
2017-12-10T13:50:03.226481: step 4896, loss 0.113497, acc 0.953125, prec 0.0914725, recall 0.896178
2017-12-10T13:50:03.657183: step 4897, loss 0.0326321, acc 1, prec 0.0915119, recall 0.896222
2017-12-10T13:50:04.105110: step 4898, loss 0.598357, acc 0.9375, prec 0.0915237, recall 0.896244
2017-12-10T13:50:04.569618: step 4899, loss 0.396258, acc 0.90625, prec 0.0915118, recall 0.896244
2017-12-10T13:50:05.035489: step 4900, loss 0.154942, acc 0.984375, prec 0.0915295, recall 0.896266
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_256_fold_3/1512929527/checkpoints/model-4900

2017-12-10T13:50:07.240403: step 4901, loss 0.0877584, acc 0.953125, prec 0.0915432, recall 0.896288
2017-12-10T13:50:07.679340: step 4902, loss 0.140465, acc 0.953125, prec 0.0915372, recall 0.896288
2017-12-10T13:50:08.133476: step 4903, loss 0.241115, acc 0.953125, prec 0.091551, recall 0.89631
2017-12-10T13:50:08.593187: step 4904, loss 0.0149354, acc 1, prec 0.091551, recall 0.89631
2017-12-10T13:50:09.040134: step 4905, loss 0.139734, acc 0.953125, prec 0.091545, recall 0.89631
2017-12-10T13:50:09.490486: step 4906, loss 0.106693, acc 0.984375, prec 0.091543, recall 0.89631
2017-12-10T13:50:09.949544: step 4907, loss 1.42123, acc 0.984375, prec 0.0915627, recall 0.896142
2017-12-10T13:50:10.397978: step 4908, loss 0.141103, acc 0.96875, prec 0.0915981, recall 0.896186
2017-12-10T13:50:10.835815: step 4909, loss 3.35518, acc 0.953125, prec 0.0916138, recall 0.896019
2017-12-10T13:50:11.289939: step 4910, loss 0.317981, acc 0.9375, prec 0.0916059, recall 0.896019
2017-12-10T13:50:11.741508: step 4911, loss 0.619401, acc 0.90625, prec 0.0916333, recall 0.896063
2017-12-10T13:50:12.188493: step 4912, loss 0.270755, acc 0.921875, prec 0.0916627, recall 0.896107
2017-12-10T13:50:12.641497: step 4913, loss 0.2358, acc 0.96875, prec 0.0916587, recall 0.896107
2017-12-10T13:50:13.085634: step 4914, loss 0.338883, acc 0.953125, prec 0.0916724, recall 0.896129
2017-12-10T13:50:13.537815: step 4915, loss 0.174948, acc 0.9375, prec 0.0916842, recall 0.896151
2017-12-10T13:50:13.980205: step 4916, loss 0.274197, acc 0.921875, prec 0.0916742, recall 0.896151
2017-12-10T13:50:14.430865: step 4917, loss 0.540066, acc 0.875, prec 0.091678, recall 0.896173
2017-12-10T13:50:14.903762: step 4918, loss 0.351081, acc 0.921875, prec 0.0916681, recall 0.896173
2017-12-10T13:50:15.357295: step 4919, loss 0.190734, acc 0.921875, prec 0.0916582, recall 0.896173
2017-12-10T13:50:15.816501: step 4920, loss 0.314961, acc 0.90625, prec 0.0917249, recall 0.89626
2017-12-10T13:50:16.274231: step 4921, loss 0.464495, acc 0.890625, prec 0.0917306, recall 0.896282
2017-12-10T13:50:16.718083: step 4922, loss 0.963247, acc 0.828125, prec 0.0917088, recall 0.896282
2017-12-10T13:50:17.173007: step 4923, loss 0.430074, acc 0.921875, prec 0.0916989, recall 0.896282
2017-12-10T13:50:17.615418: step 4924, loss 0.178429, acc 0.9375, prec 0.0917499, recall 0.896348
2017-12-10T13:50:18.051960: step 4925, loss 0.246534, acc 0.90625, prec 0.0917772, recall 0.896392
2017-12-10T13:50:18.494157: step 4926, loss 0.552379, acc 0.90625, prec 0.0917849, recall 0.896414
2017-12-10T13:50:18.942317: step 4927, loss 0.248365, acc 0.890625, prec 0.0917907, recall 0.896435
2017-12-10T13:50:19.388771: step 4928, loss 0.15064, acc 0.9375, prec 0.091822, recall 0.896479
2017-12-10T13:50:19.832346: step 4929, loss 0.24514, acc 0.9375, prec 0.0918337, recall 0.896501
2017-12-10T13:50:20.287669: step 4930, loss 0.201962, acc 0.921875, prec 0.0918433, recall 0.896523
2017-12-10T13:50:20.747153: step 4931, loss 0.50391, acc 0.875, prec 0.0918275, recall 0.896523
2017-12-10T13:50:21.189118: step 4932, loss 0.195146, acc 0.9375, prec 0.0918392, recall 0.896544
2017-12-10T13:50:21.635379: step 4933, loss 0.0919358, acc 0.953125, prec 0.091892, recall 0.89661
2017-12-10T13:50:22.089127: step 4934, loss 0.268805, acc 0.9375, prec 0.0919037, recall 0.896632
2017-12-10T13:50:22.534996: step 4935, loss 0.146909, acc 0.921875, prec 0.0918938, recall 0.896632
2017-12-10T13:50:22.979102: step 4936, loss 0.279483, acc 0.921875, prec 0.0919034, recall 0.896653
2017-12-10T13:50:23.421935: step 4937, loss 0.314186, acc 0.953125, prec 0.0918975, recall 0.896653
2017-12-10T13:50:23.869639: step 4938, loss 0.44053, acc 0.953125, prec 0.0919503, recall 0.896719
2017-12-10T13:50:24.331133: step 4939, loss 0.306616, acc 1, prec 0.0919895, recall 0.896762
2017-12-10T13:50:24.801698: step 4940, loss 0.111861, acc 0.984375, prec 0.0920071, recall 0.896784
2017-12-10T13:50:25.253978: step 4941, loss 0.0413777, acc 0.984375, prec 0.0920638, recall 0.896849
2017-12-10T13:50:25.701404: step 4942, loss 0.175276, acc 0.9375, prec 0.0920755, recall 0.89687
2017-12-10T13:50:26.139629: step 4943, loss 0.139203, acc 0.984375, prec 0.0921126, recall 0.896914
2017-12-10T13:50:26.582551: step 4944, loss 0.169903, acc 0.921875, prec 0.0921223, recall 0.896935
2017-12-10T13:50:27.033856: step 4945, loss 0.193166, acc 0.953125, prec 0.0921163, recall 0.896935
2017-12-10T13:50:27.483681: step 4946, loss 0.165792, acc 0.984375, prec 0.0921143, recall 0.896935
2017-12-10T13:50:27.930387: step 4947, loss 0.168664, acc 0.9375, prec 0.0921064, recall 0.896935
2017-12-10T13:50:28.389833: step 4948, loss 0.15587, acc 0.96875, prec 0.092122, recall 0.896957
2017-12-10T13:50:28.845394: step 4949, loss 1.27782, acc 0.96875, prec 0.0921396, recall 0.89679
2017-12-10T13:50:29.295260: step 4950, loss 0.205374, acc 0.953125, prec 0.0921727, recall 0.896834
2017-12-10T13:50:29.739789: step 4951, loss 0.080732, acc 0.984375, prec 0.0921708, recall 0.896834
2017-12-10T13:50:30.188286: step 4952, loss 0.0211367, acc 1, prec 0.0921708, recall 0.896834
2017-12-10T13:50:30.633745: step 4953, loss 0.0217783, acc 1, prec 0.0921708, recall 0.896834
2017-12-10T13:50:31.067987: step 4954, loss 0.102182, acc 0.96875, prec 0.0922059, recall 0.896877
2017-12-10T13:50:31.506211: step 4955, loss 4.10288, acc 0.9375, prec 0.0922391, recall 0.896732
2017-12-10T13:50:31.956183: step 4956, loss 0.167165, acc 0.953125, prec 0.0922331, recall 0.896732
2017-12-10T13:50:32.395622: step 4957, loss 0.339218, acc 0.90625, prec 0.0922212, recall 0.896732
2017-12-10T13:50:32.850354: step 4958, loss 0.175502, acc 0.921875, prec 0.0922113, recall 0.896732
2017-12-10T13:50:33.308670: step 4959, loss 0.513054, acc 0.890625, prec 0.0921974, recall 0.896732
2017-12-10T13:50:33.757781: step 4960, loss 0.130558, acc 0.953125, prec 0.0921914, recall 0.896732
2017-12-10T13:50:34.204310: step 4961, loss 0.656254, acc 0.921875, prec 0.0922206, recall 0.896776
2017-12-10T13:50:34.660905: step 4962, loss 0.391386, acc 0.890625, prec 0.0922067, recall 0.896776
2017-12-10T13:50:35.103994: step 4963, loss 0.255641, acc 0.9375, prec 0.0922378, recall 0.896819
2017-12-10T13:50:35.558160: step 4964, loss 0.231025, acc 0.90625, prec 0.092265, recall 0.896862
2017-12-10T13:50:36.007835: step 4965, loss 0.857056, acc 0.765625, prec 0.0922547, recall 0.896883
2017-12-10T13:50:36.446064: step 4966, loss 0.314213, acc 0.921875, prec 0.0922448, recall 0.896883
2017-12-10T13:50:36.885781: step 4967, loss 0.191645, acc 0.90625, prec 0.0922329, recall 0.896883
2017-12-10T13:50:37.328890: step 4968, loss 0.193193, acc 0.90625, prec 0.0922405, recall 0.896905
2017-12-10T13:50:37.777579: step 4969, loss 0.259697, acc 0.90625, prec 0.0922286, recall 0.896905
2017-12-10T13:50:38.187362: step 4970, loss 0.277535, acc 0.884615, prec 0.0922753, recall 0.89697
Training finished
Starting Experiment - embedding_size_512 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 512
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR embedding_size_512_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839

Start training
2017-12-10T13:50:43.717638: step 1, loss 7.48071, acc 0.71875, prec 0.0555556, recall 0.5
2017-12-10T13:50:44.548528: step 2, loss 0.91831, acc 0.84375, prec 0.0357143, recall 0.5
2017-12-10T13:50:45.374276: step 3, loss 0.621454, acc 0.828125, prec 0.025641, recall 0.5
2017-12-10T13:50:46.199361: step 4, loss 0.509197, acc 0.90625, prec 0.0222222, recall 0.5
2017-12-10T13:50:47.004183: step 5, loss 12.196, acc 0.875, prec 0.0192308, recall 0.333333
2017-12-10T13:50:47.822155: step 6, loss 59.5646, acc 0.9375, prec 0.0185185, recall 0.2
2017-12-10T13:50:48.643706: step 7, loss 7.08333, acc 0.90625, prec 0.0169492, recall 0.166667
2017-12-10T13:50:49.471786: step 8, loss 16.2041, acc 0.859375, prec 0.0151515, recall 0.125
2017-12-10T13:50:50.307139: step 9, loss 20.082, acc 0.734375, prec 0.0121951, recall 0.111111
2017-12-10T13:50:51.131849: step 10, loss 2.88951, acc 0.53125, prec 0.00892857, recall 0.111111
2017-12-10T13:50:51.962749: step 11, loss 5.64704, acc 0.40625, prec 0.00666667, recall 0.111111
2017-12-10T13:50:52.794244: step 12, loss 7.45546, acc 0.15625, prec 0.0097561, recall 0.2
2017-12-10T13:50:53.607490: step 13, loss 8.91698, acc 0.25, prec 0.00790514, recall 0.2
2017-12-10T13:50:54.432531: step 14, loss 10.0763, acc 0.15625, prec 0.00651466, recall 0.2
2017-12-10T13:50:55.251443: step 15, loss 11.5879, acc 0.15625, prec 0.00554017, recall 0.2
2017-12-10T13:50:56.071317: step 16, loss 10.4626, acc 0.109375, prec 0.0071599, recall 0.272727
2017-12-10T13:50:56.881603: step 17, loss 8.43267, acc 0.25, prec 0.00642398, recall 0.272727
2017-12-10T13:50:57.700415: step 18, loss 8.71549, acc 0.171875, prec 0.00767754, recall 0.333333
2017-12-10T13:50:58.521920: step 19, loss 7.16466, acc 0.328125, prec 0.0106007, recall 0.428571
2017-12-10T13:50:59.337924: step 20, loss 7.25177, acc 0.25, prec 0.00977199, recall 0.428571
2017-12-10T13:51:00.159005: step 21, loss 5.22564, acc 0.359375, prec 0.00916031, recall 0.428571
2017-12-10T13:51:00.985952: step 22, loss 2.92818, acc 0.59375, prec 0.011713, recall 0.5
2017-12-10T13:51:01.787107: step 23, loss 8.9499, acc 0.65625, prec 0.0113636, recall 0.470588
2017-12-10T13:51:02.615075: step 24, loss 48.356, acc 0.765625, prec 0.0111421, recall 0.444444
2017-12-10T13:51:03.425323: step 25, loss 21.5387, acc 0.671875, prec 0.0108401, recall 0.421053
2017-12-10T13:51:04.239092: step 26, loss 1.88571, acc 0.734375, prec 0.010596, recall 0.421053
2017-12-10T13:51:05.071839: step 27, loss 24.5447, acc 0.71875, prec 0.0103627, recall 0.4
2017-12-10T13:51:05.887157: step 28, loss 1.8108, acc 0.71875, prec 0.011378, recall 0.428571
2017-12-10T13:51:06.699483: step 29, loss 1.88204, acc 0.703125, prec 0.0111111, recall 0.428571
2017-12-10T13:51:07.500109: step 30, loss 9.6256, acc 0.625, prec 0.0119904, recall 0.434783
2017-12-10T13:51:08.325142: step 31, loss 4.21107, acc 0.5625, prec 0.0116009, recall 0.434783
2017-12-10T13:51:09.153099: step 32, loss 26.8865, acc 0.625, prec 0.0124153, recall 0.44
2017-12-10T13:51:10.007481: step 33, loss 2.85271, acc 0.625, prec 0.0131723, recall 0.461538
2017-12-10T13:51:10.822057: step 34, loss 56.0863, acc 0.53125, prec 0.0127796, recall 0.428571
2017-12-10T13:51:11.638320: step 35, loss 5.66711, acc 0.46875, prec 0.012333, recall 0.428571
2017-12-10T13:51:12.463906: step 36, loss 6.84159, acc 0.40625, prec 0.0118694, recall 0.428571
2017-12-10T13:51:13.283852: step 37, loss 6.38452, acc 0.484375, prec 0.0115053, recall 0.413793
2017-12-10T13:51:14.103097: step 38, loss 7.49443, acc 0.3125, prec 0.0110396, recall 0.413793
2017-12-10T13:51:14.928743: step 39, loss 5.19095, acc 0.40625, prec 0.0115453, recall 0.433333
2017-12-10T13:51:15.754647: step 40, loss 7.19831, acc 0.3125, prec 0.0119556, recall 0.451613
2017-12-10T13:51:16.581082: step 41, loss 5.96253, acc 0.359375, prec 0.0115512, recall 0.451613
2017-12-10T13:51:17.397343: step 42, loss 6.43781, acc 0.390625, prec 0.0127694, recall 0.484848
2017-12-10T13:51:18.212538: step 43, loss 4.64028, acc 0.4375, prec 0.0131783, recall 0.5
2017-12-10T13:51:19.036284: step 44, loss 5.95073, acc 0.28125, prec 0.013463, recall 0.514286
2017-12-10T13:51:19.847459: step 45, loss 4.5043, acc 0.53125, prec 0.0138889, recall 0.527778
2017-12-10T13:51:20.662763: step 46, loss 48.1183, acc 0.5625, prec 0.0143369, recall 0.512821
2017-12-10T13:51:21.471576: step 47, loss 3.6513, acc 0.546875, prec 0.0140449, recall 0.512821
2017-12-10T13:51:22.283463: step 48, loss 17.1546, acc 0.46875, prec 0.0150788, recall 0.52381
2017-12-10T13:51:23.113446: step 49, loss 4.89123, acc 0.5, prec 0.0154155, recall 0.534884
2017-12-10T13:51:23.931919: step 50, loss 2.66654, acc 0.65625, prec 0.0151915, recall 0.534884
2017-12-10T13:51:24.759129: step 51, loss 3.75367, acc 0.515625, prec 0.0155239, recall 0.545455
2017-12-10T13:51:25.580179: step 52, loss 4.23204, acc 0.46875, prec 0.0158128, recall 0.555556
2017-12-10T13:51:26.394071: step 53, loss 25.0255, acc 0.484375, prec 0.0154991, recall 0.543478
2017-12-10T13:51:27.208609: step 54, loss 2.62835, acc 0.625, prec 0.0152718, recall 0.543478
2017-12-10T13:51:28.017316: step 55, loss 3.38131, acc 0.59375, prec 0.015625, recall 0.553191
2017-12-10T13:51:28.849418: step 56, loss 2.41202, acc 0.640625, prec 0.015412, recall 0.553191
2017-12-10T13:51:29.667146: step 57, loss 2.60828, acc 0.578125, prec 0.0151692, recall 0.553191
2017-12-10T13:51:30.471312: step 58, loss 28.226, acc 0.625, prec 0.0161105, recall 0.54902
2017-12-10T13:51:31.284401: step 59, loss 16.1349, acc 0.4375, prec 0.0163472, recall 0.54717
2017-12-10T13:51:32.113176: step 60, loss 4.10321, acc 0.484375, prec 0.0160487, recall 0.54717
2017-12-10T13:51:32.929901: step 61, loss 11.0771, acc 0.53125, prec 0.016331, recall 0.545455
2017-12-10T13:51:33.734229: step 62, loss 4.73615, acc 0.453125, prec 0.0160256, recall 0.545455
2017-12-10T13:51:34.551069: step 63, loss 6.53806, acc 0.375, prec 0.0162049, recall 0.553571
2017-12-10T13:51:35.362168: step 64, loss 7.30363, acc 0.34375, prec 0.0163599, recall 0.561404
2017-12-10T13:51:36.166896: step 65, loss 7.70925, acc 0.296875, prec 0.015992, recall 0.561404
2017-12-10T13:51:36.977846: step 66, loss 6.69926, acc 0.34375, prec 0.0161448, recall 0.568965
2017-12-10T13:51:37.800014: step 67, loss 8.95595, acc 0.28125, prec 0.0162602, recall 0.576271
2017-12-10T13:51:38.620968: step 68, loss 7.02265, acc 0.265625, prec 0.0168224, recall 0.590164
2017-12-10T13:51:39.440221: step 69, loss 4.42438, acc 0.46875, prec 0.0165593, recall 0.590164
2017-12-10T13:51:40.261799: step 70, loss 3.93908, acc 0.484375, prec 0.0163117, recall 0.590164
2017-12-10T13:51:41.077620: step 71, loss 3.92161, acc 0.5625, prec 0.0165474, recall 0.596774
2017-12-10T13:51:41.890267: step 72, loss 3.0353, acc 0.578125, prec 0.01635, recall 0.596774
2017-12-10T13:51:42.717906: step 73, loss 2.40639, acc 0.71875, prec 0.016221, recall 0.596774
2017-12-10T13:51:43.566768: step 74, loss 29.1281, acc 0.59375, prec 0.016059, recall 0.569231
2017-12-10T13:51:44.407691: step 75, loss 2.06421, acc 0.71875, prec 0.0163582, recall 0.575758
2017-12-10T13:51:45.240659: step 76, loss 2.52514, acc 0.609375, prec 0.0166028, recall 0.58209
2017-12-10T13:51:46.040615: step 77, loss 2.84643, acc 0.625, prec 0.0168492, recall 0.588235
2017-12-10T13:51:46.871996: step 78, loss 3.13453, acc 0.625, prec 0.0170905, recall 0.594203
2017-12-10T13:51:47.716355: step 79, loss 2.96756, acc 0.640625, prec 0.0169282, recall 0.594203
2017-12-10T13:51:48.540045: step 80, loss 3.27445, acc 0.59375, prec 0.0171499, recall 0.6
2017-12-10T13:51:49.381481: step 81, loss 14.8501, acc 0.6875, prec 0.0170178, recall 0.591549
2017-12-10T13:51:50.207761: step 82, loss 2.51922, acc 0.640625, prec 0.0172552, recall 0.597222
2017-12-10T13:51:51.034903: step 83, loss 42.2045, acc 0.671875, prec 0.0171247, recall 0.581081
2017-12-10T13:51:51.836382: step 84, loss 2.76856, acc 0.625, prec 0.0169625, recall 0.581081
2017-12-10T13:51:52.698425: step 85, loss 2.44312, acc 0.625, prec 0.0171875, recall 0.586667
2017-12-10T13:51:53.533671: step 86, loss 3.62697, acc 0.5, prec 0.0173544, recall 0.592105
2017-12-10T13:51:54.347705: step 87, loss 4.25568, acc 0.5625, prec 0.0175439, recall 0.597403
2017-12-10T13:51:55.168116: step 88, loss 5.57231, acc 0.5, prec 0.0180723, recall 0.607595
2017-12-10T13:51:55.987044: step 89, loss 3.78569, acc 0.625, prec 0.0186428, recall 0.617284
2017-12-10T13:51:56.809730: step 90, loss 4.15387, acc 0.546875, prec 0.019167, recall 0.626506
2017-12-10T13:51:57.627252: step 91, loss 3.02022, acc 0.6875, prec 0.0190267, recall 0.626506
2017-12-10T13:51:58.422607: step 92, loss 3.74656, acc 0.59375, prec 0.0192029, recall 0.630952
2017-12-10T13:51:59.239296: step 93, loss 27.0644, acc 0.5, prec 0.019341, recall 0.627907
2017-12-10T13:52:00.066084: step 94, loss 4.48414, acc 0.515625, prec 0.019823, recall 0.636364
2017-12-10T13:52:00.885611: step 95, loss 4.84358, acc 0.53125, prec 0.0196147, recall 0.636364
2017-12-10T13:52:01.695728: step 96, loss 3.56613, acc 0.59375, prec 0.0201179, recall 0.644444
2017-12-10T13:52:02.527579: step 97, loss 2.56818, acc 0.71875, prec 0.0199931, recall 0.644444
2017-12-10T13:52:03.351199: step 98, loss 5.26134, acc 0.625, prec 0.0198358, recall 0.637363
2017-12-10T13:52:04.165601: step 99, loss 4.16049, acc 0.59375, prec 0.019661, recall 0.637363
2017-12-10T13:52:04.983766: step 100, loss 3.68691, acc 0.515625, prec 0.0197854, recall 0.641304
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-100

2017-12-10T13:52:07.838163: step 101, loss 2.10808, acc 0.75, prec 0.0196798, recall 0.641304
2017-12-10T13:52:08.664918: step 102, loss 2.47362, acc 0.703125, prec 0.0195558, recall 0.641304
2017-12-10T13:52:09.483283: step 103, loss 11.6153, acc 0.75, prec 0.0194591, recall 0.634409
2017-12-10T13:52:10.304926: step 104, loss 2.1274, acc 0.71875, prec 0.0193443, recall 0.634409
2017-12-10T13:52:11.145250: step 105, loss 1.32231, acc 0.765625, prec 0.0192496, recall 0.634409
2017-12-10T13:52:11.987554: step 106, loss 27.227, acc 0.625, prec 0.0194301, recall 0.625
2017-12-10T13:52:12.800602: step 107, loss 21.4421, acc 0.734375, prec 0.0193299, recall 0.618557
2017-12-10T13:52:13.611253: step 108, loss 2.6726, acc 0.609375, prec 0.0194888, recall 0.622449
2017-12-10T13:52:14.434379: step 109, loss 8.35839, acc 0.5625, prec 0.0196327, recall 0.62
2017-12-10T13:52:15.273399: step 110, loss 2.36729, acc 0.609375, prec 0.0194785, recall 0.62
2017-12-10T13:52:16.098168: step 111, loss 4.67051, acc 0.5, prec 0.0198943, recall 0.627451
2017-12-10T13:52:16.937987: step 112, loss 6.26748, acc 0.40625, prec 0.0199631, recall 0.631068
2017-12-10T13:52:17.749662: step 113, loss 5.87542, acc 0.375, prec 0.0197209, recall 0.631068
2017-12-10T13:52:18.566680: step 114, loss 11.9823, acc 0.46875, prec 0.0201141, recall 0.632075
2017-12-10T13:52:19.389389: step 115, loss 17.2999, acc 0.296875, prec 0.0198519, recall 0.626168
2017-12-10T13:52:20.221014: step 116, loss 9.29416, acc 0.21875, prec 0.019562, recall 0.626168
2017-12-10T13:52:21.044545: step 117, loss 8.89444, acc 0.265625, prec 0.0192972, recall 0.626168
2017-12-10T13:52:21.856058: step 118, loss 8.71634, acc 0.25, prec 0.0193127, recall 0.62963
2017-12-10T13:52:22.684855: step 119, loss 8.00932, acc 0.359375, prec 0.0193657, recall 0.633027
2017-12-10T13:52:23.505745: step 120, loss 13.9931, acc 0.234375, prec 0.0196513, recall 0.633929
2017-12-10T13:52:24.333575: step 121, loss 6.0521, acc 0.5, prec 0.0202851, recall 0.643478
2017-12-10T13:52:25.153178: step 122, loss 10.4124, acc 0.234375, prec 0.0200162, recall 0.643478
2017-12-10T13:52:25.954730: step 123, loss 7.73534, acc 0.4375, prec 0.0200857, recall 0.646552
2017-12-10T13:52:26.768365: step 124, loss 8.29756, acc 0.359375, prec 0.0198676, recall 0.646552
2017-12-10T13:52:27.575651: step 125, loss 8.04484, acc 0.390625, prec 0.0201782, recall 0.652542
2017-12-10T13:52:28.399437: step 126, loss 7.09809, acc 0.359375, prec 0.0204716, recall 0.658333
2017-12-10T13:52:29.214647: step 127, loss 8.00263, acc 0.3125, prec 0.0204918, recall 0.661157
2017-12-10T13:52:30.034392: step 128, loss 5.85047, acc 0.453125, prec 0.0203097, recall 0.661157
2017-12-10T13:52:30.858213: step 129, loss 3.01591, acc 0.578125, prec 0.0201715, recall 0.661157
2017-12-10T13:52:31.662773: step 130, loss 4.88013, acc 0.484375, prec 0.020005, recall 0.661157
2017-12-10T13:52:32.479041: step 131, loss 3.11751, acc 0.703125, prec 0.0201543, recall 0.663934
2017-12-10T13:52:33.320904: step 132, loss 13.9681, acc 0.8125, prec 0.0200993, recall 0.658537
2017-12-10T13:52:34.151750: step 133, loss 2.18736, acc 0.78125, prec 0.0207561, recall 0.666667
2017-12-10T13:52:34.982815: step 134, loss 33.1351, acc 0.71875, prec 0.0209205, recall 0.653846
2017-12-10T13:52:35.803307: step 135, loss 1.91181, acc 0.703125, prec 0.0208231, recall 0.653846
2017-12-10T13:52:36.615756: step 136, loss 2.20481, acc 0.703125, prec 0.0207267, recall 0.653846
2017-12-10T13:52:37.430720: step 137, loss 2.71774, acc 0.703125, prec 0.0206311, recall 0.653846
2017-12-10T13:52:38.257564: step 138, loss 3.4037, acc 0.546875, prec 0.0204869, recall 0.653846
2017-12-10T13:52:39.088066: step 139, loss 3.2167, acc 0.546875, prec 0.0203447, recall 0.653846
2017-12-10T13:52:39.920900: step 140, loss 4.0105, acc 0.609375, prec 0.0202236, recall 0.653846
2017-12-10T13:52:40.735511: step 141, loss 18.5741, acc 0.546875, prec 0.0203262, recall 0.646617
2017-12-10T13:52:41.561980: step 142, loss 2.98807, acc 0.609375, prec 0.0202068, recall 0.646617
2017-12-10T13:52:42.375793: step 143, loss 3.24835, acc 0.609375, prec 0.0200888, recall 0.646617
2017-12-10T13:52:43.188811: step 144, loss 4.77984, acc 0.5, prec 0.0199397, recall 0.646617
2017-12-10T13:52:44.011183: step 145, loss 3.84229, acc 0.546875, prec 0.0204833, recall 0.654412
2017-12-10T13:52:44.810460: step 146, loss 3.64878, acc 0.515625, prec 0.0203382, recall 0.654412
2017-12-10T13:52:45.627827: step 147, loss 4.25467, acc 0.53125, prec 0.0204221, recall 0.656934
2017-12-10T13:52:46.465988: step 148, loss 1.6671, acc 0.8125, prec 0.0205882, recall 0.65942
2017-12-10T13:52:47.281538: step 149, loss 3.74119, acc 0.59375, prec 0.0204678, recall 0.65942
2017-12-10T13:52:48.109546: step 150, loss 3.07853, acc 0.703125, prec 0.0206001, recall 0.66187
2017-12-10T13:52:48.932415: step 151, loss 1.40792, acc 0.734375, prec 0.020522, recall 0.66187
2017-12-10T13:52:49.756365: step 152, loss 4.36327, acc 0.75, prec 0.0204535, recall 0.657143
2017-12-10T13:52:50.562289: step 153, loss 3.03781, acc 0.671875, prec 0.0203585, recall 0.657143
2017-12-10T13:52:51.387800: step 154, loss 2.20216, acc 0.765625, prec 0.0202911, recall 0.657143
2017-12-10T13:52:52.219347: step 155, loss 2.60698, acc 0.6875, prec 0.020202, recall 0.657143
2017-12-10T13:52:53.061733: step 156, loss 7.11086, acc 0.796875, prec 0.0201489, recall 0.652482
2017-12-10T13:52:53.874148: step 157, loss 1.92085, acc 0.6875, prec 0.0200611, recall 0.652482
2017-12-10T13:52:54.699192: step 158, loss 2.26823, acc 0.75, prec 0.0199913, recall 0.652482
2017-12-10T13:52:55.514617: step 159, loss 12.6061, acc 0.78125, prec 0.019935, recall 0.647887
2017-12-10T13:52:56.352318: step 160, loss 3.07598, acc 0.65625, prec 0.0200517, recall 0.65035
2017-12-10T13:52:57.179260: step 161, loss 6.33917, acc 0.671875, prec 0.0199657, recall 0.645833
2017-12-10T13:52:58.011901: step 162, loss 3.16772, acc 0.71875, prec 0.0203078, recall 0.650685
2017-12-10T13:52:58.831416: step 163, loss 12.6643, acc 0.671875, prec 0.02023, recall 0.637584
2017-12-10T13:52:59.656956: step 164, loss 6.93231, acc 0.640625, prec 0.0203433, recall 0.635762
2017-12-10T13:53:00.503170: step 165, loss 3.51731, acc 0.6875, prec 0.0202574, recall 0.635762
2017-12-10T13:53:01.330635: step 166, loss 5.90312, acc 0.515625, prec 0.0201258, recall 0.635762
2017-12-10T13:53:02.156906: step 167, loss 5.1393, acc 0.46875, prec 0.0199833, recall 0.635762
2017-12-10T13:53:02.975564: step 168, loss 4.30996, acc 0.453125, prec 0.0198388, recall 0.635762
2017-12-10T13:53:03.798817: step 169, loss 6.48336, acc 0.484375, prec 0.0197044, recall 0.635762
2017-12-10T13:53:04.613701: step 170, loss 7.42486, acc 0.296875, prec 0.0195241, recall 0.635762
2017-12-10T13:53:05.426877: step 171, loss 6.76347, acc 0.390625, prec 0.0195683, recall 0.638158
2017-12-10T13:53:06.248402: step 172, loss 5.18013, acc 0.453125, prec 0.0194311, recall 0.638158
2017-12-10T13:53:07.070928: step 173, loss 5.92276, acc 0.484375, prec 0.0194986, recall 0.640523
2017-12-10T13:53:07.888688: step 174, loss 7.39637, acc 0.34375, prec 0.019337, recall 0.640523
2017-12-10T13:53:08.748744: step 175, loss 7.10269, acc 0.46875, prec 0.0192119, recall 0.636364
2017-12-10T13:53:09.576414: step 176, loss 5.95225, acc 0.546875, prec 0.0196766, recall 0.643312
2017-12-10T13:53:10.359509: step 177, loss 5.91861, acc 0.46875, prec 0.0199265, recall 0.647799
2017-12-10T13:53:11.168346: step 178, loss 4.74687, acc 0.5625, prec 0.0198191, recall 0.647799
2017-12-10T13:53:11.991948: step 179, loss 4.13981, acc 0.609375, prec 0.0199119, recall 0.65
2017-12-10T13:53:12.817951: step 180, loss 4.66317, acc 0.546875, prec 0.019802, recall 0.65
2017-12-10T13:53:13.637227: step 181, loss 3.41807, acc 0.625, prec 0.0197119, recall 0.65
2017-12-10T13:53:14.467919: step 182, loss 22.8801, acc 0.71875, prec 0.0198375, recall 0.644172
2017-12-10T13:53:15.279710: step 183, loss 20.5636, acc 0.703125, prec 0.0203237, recall 0.646707
2017-12-10T13:53:16.100259: step 184, loss 2.90616, acc 0.625, prec 0.0205993, recall 0.650888
2017-12-10T13:53:16.916977: step 185, loss 3.35424, acc 0.625, prec 0.0208722, recall 0.654971
2017-12-10T13:53:17.741707: step 186, loss 4.19604, acc 0.625, prec 0.0207792, recall 0.654971
2017-12-10T13:53:18.541329: step 187, loss 13.4427, acc 0.609375, prec 0.0206871, recall 0.651163
2017-12-10T13:53:19.355989: step 188, loss 4.23301, acc 0.546875, prec 0.0205769, recall 0.651163
2017-12-10T13:53:20.176497: step 189, loss 1.86083, acc 0.59375, prec 0.0204791, recall 0.651163
2017-12-10T13:53:21.002108: step 190, loss 3.91722, acc 0.65625, prec 0.020397, recall 0.651163
2017-12-10T13:53:21.823463: step 191, loss 3.73221, acc 0.609375, prec 0.0203046, recall 0.651163
2017-12-10T13:53:22.647283: step 192, loss 3.29041, acc 0.546875, prec 0.0201984, recall 0.651163
2017-12-10T13:53:23.461131: step 193, loss 3.97254, acc 0.59375, prec 0.0201041, recall 0.651163
2017-12-10T13:53:24.285050: step 194, loss 2.92312, acc 0.65625, prec 0.020025, recall 0.651163
2017-12-10T13:53:25.097778: step 195, loss 15.6332, acc 0.71875, prec 0.0199643, recall 0.647399
2017-12-10T13:53:25.922348: step 196, loss 3.41149, acc 0.65625, prec 0.0198864, recall 0.647399
2017-12-10T13:53:26.751551: step 197, loss 9.24723, acc 0.640625, prec 0.019809, recall 0.643678
2017-12-10T13:53:27.566660: step 198, loss 14.671, acc 0.671875, prec 0.0197392, recall 0.64
2017-12-10T13:53:28.383744: step 199, loss 4.18675, acc 0.53125, prec 0.0198072, recall 0.642045
2017-12-10T13:53:29.201423: step 200, loss 4.08554, acc 0.609375, prec 0.0197208, recall 0.642045
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-200

2017-12-10T13:53:31.894794: step 201, loss 4.96689, acc 0.578125, prec 0.0201389, recall 0.648045
2017-12-10T13:53:32.707357: step 202, loss 5.74805, acc 0.53125, prec 0.0202073, recall 0.646409
2017-12-10T13:53:33.527968: step 203, loss 6.53094, acc 0.453125, prec 0.0205903, recall 0.652174
2017-12-10T13:53:34.365052: step 204, loss 3.61887, acc 0.53125, prec 0.020652, recall 0.654054
2017-12-10T13:53:35.178049: step 205, loss 5.69719, acc 0.421875, prec 0.0205224, recall 0.654054
2017-12-10T13:53:36.013482: step 206, loss 5.3356, acc 0.484375, prec 0.0204082, recall 0.654054
2017-12-10T13:53:36.862278: step 207, loss 3.44485, acc 0.65625, prec 0.0209908, recall 0.661376
2017-12-10T13:53:37.685703: step 208, loss 3.08384, acc 0.578125, prec 0.0212233, recall 0.664921
2017-12-10T13:53:38.499833: step 209, loss 2.86555, acc 0.65625, prec 0.0211455, recall 0.664921
2017-12-10T13:53:39.330590: step 210, loss 3.59677, acc 0.6875, prec 0.0215624, recall 0.670103
2017-12-10T13:53:40.149953: step 211, loss 4.6965, acc 0.5625, prec 0.0217858, recall 0.673469
2017-12-10T13:53:40.973073: step 212, loss 11.6243, acc 0.65625, prec 0.0217141, recall 0.666667
2017-12-10T13:53:41.814177: step 213, loss 32.985, acc 0.59375, prec 0.0217854, recall 0.665
2017-12-10T13:53:42.623382: step 214, loss 3.13166, acc 0.703125, prec 0.0217178, recall 0.665
2017-12-10T13:53:43.435840: step 215, loss 2.12464, acc 0.75, prec 0.0216612, recall 0.665
2017-12-10T13:53:44.260978: step 216, loss 3.30658, acc 0.640625, prec 0.0217391, recall 0.666667
2017-12-10T13:53:45.082229: step 217, loss 3.17384, acc 0.609375, prec 0.0219674, recall 0.669951
2017-12-10T13:53:45.919350: step 218, loss 4.96621, acc 0.625, prec 0.0220434, recall 0.668293
2017-12-10T13:53:46.725131: step 219, loss 3.57662, acc 0.546875, prec 0.0219411, recall 0.668293
2017-12-10T13:53:47.538404: step 220, loss 3.52732, acc 0.671875, prec 0.0218675, recall 0.668293
2017-12-10T13:53:48.378151: step 221, loss 2.45887, acc 0.6875, prec 0.0219535, recall 0.669903
2017-12-10T13:53:49.195986: step 222, loss 3.77062, acc 0.625, prec 0.02187, recall 0.669903
2017-12-10T13:53:50.028846: step 223, loss 4.76193, acc 0.671875, prec 0.0219555, recall 0.668269
2017-12-10T13:53:50.832950: step 224, loss 3.97272, acc 0.609375, prec 0.0218691, recall 0.668269
2017-12-10T13:53:51.654623: step 225, loss 14.8576, acc 0.6875, prec 0.0219573, recall 0.666667
2017-12-10T13:53:52.451816: step 226, loss 2.47879, acc 0.703125, prec 0.022045, recall 0.668246
2017-12-10T13:53:53.270518: step 227, loss 3.24066, acc 0.625, prec 0.0219626, recall 0.668246
2017-12-10T13:53:54.098943: step 228, loss 7.13056, acc 0.640625, prec 0.0218876, recall 0.665094
2017-12-10T13:53:54.929660: step 229, loss 5.05724, acc 0.640625, prec 0.0219644, recall 0.663551
2017-12-10T13:53:55.754500: step 230, loss 1.95162, acc 0.65625, prec 0.0218899, recall 0.663551
2017-12-10T13:53:56.581199: step 231, loss 3.0868, acc 0.59375, prec 0.0218025, recall 0.663551
2017-12-10T13:53:57.409112: step 232, loss 11.9476, acc 0.609375, prec 0.0217225, recall 0.660465
2017-12-10T13:53:58.248437: step 233, loss 10.4196, acc 0.65625, prec 0.0216529, recall 0.657407
2017-12-10T13:53:59.077673: step 234, loss 3.76931, acc 0.5625, prec 0.0215609, recall 0.657407
2017-12-10T13:53:59.888893: step 235, loss 4.54999, acc 0.578125, prec 0.0219166, recall 0.6621
2017-12-10T13:54:00.705224: step 236, loss 5.39677, acc 0.40625, prec 0.0217914, recall 0.6621
2017-12-10T13:54:01.508268: step 237, loss 4.91479, acc 0.5625, prec 0.0218465, recall 0.663636
2017-12-10T13:54:02.320224: step 238, loss 5.81992, acc 0.46875, prec 0.0218815, recall 0.665158
2017-12-10T13:54:03.151106: step 239, loss 4.10659, acc 0.5625, prec 0.0217907, recall 0.665158
2017-12-10T13:54:03.980652: step 240, loss 3.8252, acc 0.515625, prec 0.021691, recall 0.665158
2017-12-10T13:54:04.795704: step 241, loss 19.4221, acc 0.59375, prec 0.0218989, recall 0.665179
2017-12-10T13:54:05.626295: step 242, loss 25.6913, acc 0.46875, prec 0.0217932, recall 0.662222
2017-12-10T13:54:06.450174: step 243, loss 5.15596, acc 0.484375, prec 0.0216885, recall 0.662222
2017-12-10T13:54:07.253768: step 244, loss 5.8024, acc 0.421875, prec 0.021714, recall 0.663717
2017-12-10T13:54:08.075885: step 245, loss 6.77109, acc 0.4375, prec 0.0216014, recall 0.663717
2017-12-10T13:54:08.915383: step 246, loss 4.25911, acc 0.515625, prec 0.0215054, recall 0.663717
2017-12-10T13:54:09.737364: step 247, loss 4.17583, acc 0.578125, prec 0.0215622, recall 0.665198
2017-12-10T13:54:10.565855: step 248, loss 6.42962, acc 0.546875, prec 0.0214763, recall 0.662281
2017-12-10T13:54:11.382106: step 249, loss 4.55164, acc 0.546875, prec 0.0215267, recall 0.663755
2017-12-10T13:54:12.194441: step 250, loss 5.10579, acc 0.546875, prec 0.0214386, recall 0.663755
2017-12-10T13:54:13.014718: step 251, loss 4.10773, acc 0.59375, prec 0.0213603, recall 0.663755
2017-12-10T13:54:13.839308: step 252, loss 4.4134, acc 0.609375, prec 0.0212855, recall 0.663755
2017-12-10T13:54:14.669001: step 253, loss 4.11061, acc 0.515625, prec 0.02133, recall 0.665217
2017-12-10T13:54:15.500178: step 254, loss 3.69865, acc 0.59375, prec 0.0215248, recall 0.668103
2017-12-10T13:54:16.338997: step 255, loss 3.77865, acc 0.5625, prec 0.0214414, recall 0.668103
2017-12-10T13:54:17.161659: step 256, loss 2.40087, acc 0.75, prec 0.0215291, recall 0.669528
2017-12-10T13:54:17.972667: step 257, loss 2.19073, acc 0.765625, prec 0.0214846, recall 0.669528
2017-12-10T13:54:18.794302: step 258, loss 13.1712, acc 0.765625, prec 0.0215778, recall 0.668085
2017-12-10T13:54:19.613922: step 259, loss 0.894704, acc 0.859375, prec 0.0215511, recall 0.668085
2017-12-10T13:54:20.419632: step 260, loss 12.526, acc 0.75, prec 0.0215068, recall 0.665254
2017-12-10T13:54:21.252989: step 261, loss 2.21587, acc 0.71875, prec 0.0215876, recall 0.666667
2017-12-10T13:54:22.059124: step 262, loss 2.81557, acc 0.734375, prec 0.0215406, recall 0.663866
2017-12-10T13:54:22.877521: step 263, loss 2.2506, acc 0.734375, prec 0.0220228, recall 0.669421
2017-12-10T13:54:23.721535: step 264, loss 1.85447, acc 0.75, prec 0.021975, recall 0.669421
2017-12-10T13:54:24.526399: step 265, loss 1.00651, acc 0.796875, prec 0.0219364, recall 0.669421
2017-12-10T13:54:25.353940: step 266, loss 7.6661, acc 0.796875, prec 0.0219008, recall 0.666667
2017-12-10T13:54:26.186729: step 267, loss 1.82025, acc 0.78125, prec 0.0221233, recall 0.669388
2017-12-10T13:54:27.003847: step 268, loss 1.94542, acc 0.8125, prec 0.0220875, recall 0.669388
2017-12-10T13:54:27.821332: step 269, loss 1.74868, acc 0.703125, prec 0.0220312, recall 0.669388
2017-12-10T13:54:28.615179: step 270, loss 25.3632, acc 0.8125, prec 0.022261, recall 0.669355
2017-12-10T13:54:29.434561: step 271, loss 18.7517, acc 0.765625, prec 0.0226117, recall 0.670635
2017-12-10T13:54:30.253443: step 272, loss 1.10012, acc 0.84375, prec 0.0227121, recall 0.671937
2017-12-10T13:54:31.078439: step 273, loss 3.47011, acc 0.6875, prec 0.0227818, recall 0.673228
2017-12-10T13:54:31.943581: step 274, loss 3.67743, acc 0.703125, prec 0.0228541, recall 0.67451
2017-12-10T13:54:32.775468: step 275, loss 4.33808, acc 0.578125, prec 0.0227724, recall 0.67451
2017-12-10T13:54:33.582366: step 276, loss 4.3833, acc 0.546875, prec 0.0228142, recall 0.675781
2017-12-10T13:54:34.400369: step 277, loss 28.2463, acc 0.578125, prec 0.0228647, recall 0.674419
2017-12-10T13:54:35.209327: step 278, loss 4.11398, acc 0.578125, prec 0.0227838, recall 0.674419
2017-12-10T13:54:36.037378: step 279, loss 4.20122, acc 0.484375, prec 0.0228132, recall 0.675676
2017-12-10T13:54:36.845703: step 280, loss 16.6853, acc 0.421875, prec 0.0229602, recall 0.675573
2017-12-10T13:54:37.667228: step 281, loss 3.86892, acc 0.546875, prec 0.0231266, recall 0.67803
2017-12-10T13:54:38.472069: step 282, loss 3.41318, acc 0.578125, prec 0.0234234, recall 0.681648
2017-12-10T13:54:39.270057: step 283, loss 4.94434, acc 0.453125, prec 0.0234435, recall 0.682836
2017-12-10T13:54:40.104670: step 284, loss 5.69054, acc 0.421875, prec 0.0233329, recall 0.682836
2017-12-10T13:54:40.939990: step 285, loss 6.14767, acc 0.421875, prec 0.0233473, recall 0.684015
2017-12-10T13:54:41.763356: step 286, loss 4.4848, acc 0.484375, prec 0.0232499, recall 0.684015
2017-12-10T13:54:42.569518: step 287, loss 2.76112, acc 0.640625, prec 0.0231826, recall 0.684015
2017-12-10T13:54:43.397547: step 288, loss 4.61378, acc 0.5625, prec 0.0232237, recall 0.685185
2017-12-10T13:54:44.227218: step 289, loss 2.97053, acc 0.671875, prec 0.0231626, recall 0.685185
2017-12-10T13:54:45.043921: step 290, loss 3.61885, acc 0.609375, prec 0.0230904, recall 0.685185
2017-12-10T13:54:45.857277: step 291, loss 1.71368, acc 0.734375, prec 0.0230415, recall 0.685185
2017-12-10T13:54:46.687043: step 292, loss 1.28594, acc 0.75, prec 0.0229956, recall 0.685185
2017-12-10T13:54:47.514921: step 293, loss 2.60139, acc 0.765625, prec 0.0233164, recall 0.688645
2017-12-10T13:54:48.336874: step 294, loss 1.77761, acc 0.734375, prec 0.0233882, recall 0.689781
2017-12-10T13:54:49.160378: step 295, loss 0.575274, acc 0.921875, prec 0.0233737, recall 0.689781
2017-12-10T13:54:49.981101: step 296, loss 0.95775, acc 0.859375, prec 0.0234684, recall 0.690909
2017-12-10T13:54:50.791229: step 297, loss 0.710263, acc 0.890625, prec 0.0234481, recall 0.690909
2017-12-10T13:54:51.611655: step 298, loss 0.847862, acc 0.875, prec 0.023425, recall 0.690909
2017-12-10T13:54:52.443958: step 299, loss 0.932738, acc 0.875, prec 0.0234019, recall 0.690909
2017-12-10T13:54:53.269140: step 300, loss 1.09745, acc 0.84375, prec 0.0234932, recall 0.692029
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-300

2017-12-10T13:54:56.243995: step 301, loss 0.220102, acc 0.921875, prec 0.0235988, recall 0.693141
2017-12-10T13:54:57.047501: step 302, loss 42.1347, acc 0.90625, prec 0.0235843, recall 0.690647
2017-12-10T13:54:57.871644: step 303, loss 0.660337, acc 0.890625, prec 0.0235641, recall 0.690647
2017-12-10T13:54:58.680573: step 304, loss 0.604145, acc 0.890625, prec 0.0235438, recall 0.690647
2017-12-10T13:54:59.499377: step 305, loss 30.4567, acc 0.890625, prec 0.0236491, recall 0.686833
2017-12-10T13:55:00.334536: step 306, loss 0.576119, acc 0.84375, prec 0.0236201, recall 0.686833
2017-12-10T13:55:01.166234: step 307, loss 4.71243, acc 0.921875, prec 0.0236086, recall 0.684397
2017-12-10T13:55:02.023749: step 308, loss 34.9894, acc 0.78125, prec 0.0235711, recall 0.681979
2017-12-10T13:55:02.842052: step 309, loss 4.6904, acc 0.8125, prec 0.0237776, recall 0.681818
2017-12-10T13:55:03.684229: step 310, loss 2.4286, acc 0.75, prec 0.0238501, recall 0.682927
2017-12-10T13:55:04.473650: step 311, loss 2.80624, acc 0.71875, prec 0.023798, recall 0.682927
2017-12-10T13:55:05.295436: step 312, loss 3.55522, acc 0.53125, prec 0.0239478, recall 0.685121
2017-12-10T13:55:06.106430: step 313, loss 3.48203, acc 0.640625, prec 0.0238813, recall 0.685121
2017-12-10T13:55:06.935167: step 314, loss 4.31235, acc 0.5, prec 0.0239068, recall 0.686207
2017-12-10T13:55:07.760733: step 315, loss 3.77491, acc 0.546875, prec 0.0238238, recall 0.686207
2017-12-10T13:55:08.582670: step 316, loss 4.31852, acc 0.5, prec 0.0237329, recall 0.686207
2017-12-10T13:55:09.389987: step 317, loss 4.51208, acc 0.578125, prec 0.0236567, recall 0.686207
2017-12-10T13:55:10.216504: step 318, loss 3.94258, acc 0.546875, prec 0.0235754, recall 0.686207
2017-12-10T13:55:11.055651: step 319, loss 4.19713, acc 0.578125, prec 0.0236155, recall 0.687285
2017-12-10T13:55:11.883898: step 320, loss 3.19075, acc 0.5625, prec 0.0235377, recall 0.687285
2017-12-10T13:55:12.702740: step 321, loss 2.99978, acc 0.609375, prec 0.0234687, recall 0.687285
2017-12-10T13:55:13.522550: step 322, loss 7.1748, acc 0.546875, prec 0.023506, recall 0.686007
2017-12-10T13:55:14.343574: step 323, loss 2.64861, acc 0.5625, prec 0.0236569, recall 0.688136
2017-12-10T13:55:15.157635: step 324, loss 1.31878, acc 0.765625, prec 0.0237292, recall 0.689189
2017-12-10T13:55:15.958165: step 325, loss 2.25161, acc 0.671875, prec 0.0237847, recall 0.690236
2017-12-10T13:55:16.795827: step 326, loss 9.54348, acc 0.625, prec 0.0240602, recall 0.69103
2017-12-10T13:55:17.608281: step 327, loss 1.63165, acc 0.75, prec 0.0240157, recall 0.69103
2017-12-10T13:55:18.427071: step 328, loss 1.26065, acc 0.765625, prec 0.0239742, recall 0.69103
2017-12-10T13:55:19.250272: step 329, loss 1.73914, acc 0.71875, prec 0.024149, recall 0.693069
2017-12-10T13:55:20.073852: step 330, loss 2.069, acc 0.71875, prec 0.0240992, recall 0.693069
2017-12-10T13:55:20.895903: step 331, loss 1.7483, acc 0.734375, prec 0.0240522, recall 0.693069
2017-12-10T13:55:21.713045: step 332, loss 14.8701, acc 0.75, prec 0.0240137, recall 0.688525
2017-12-10T13:55:22.544029: step 333, loss 3.1704, acc 0.5625, prec 0.0240483, recall 0.689542
2017-12-10T13:55:23.399872: step 334, loss 10.2657, acc 0.765625, prec 0.02401, recall 0.687296
2017-12-10T13:55:24.214187: step 335, loss 1.31059, acc 0.78125, prec 0.0239718, recall 0.687296
2017-12-10T13:55:25.027379: step 336, loss 2.28919, acc 0.640625, prec 0.0239093, recall 0.687296
2017-12-10T13:55:25.844602: step 337, loss 3.62291, acc 0.53125, prec 0.0238283, recall 0.687296
2017-12-10T13:55:26.667659: step 338, loss 7.27048, acc 0.5625, prec 0.0238658, recall 0.686084
2017-12-10T13:55:27.489011: step 339, loss 2.73988, acc 0.671875, prec 0.0238095, recall 0.686084
2017-12-10T13:55:28.313039: step 340, loss 2.6234, acc 0.734375, prec 0.0238736, recall 0.687097
2017-12-10T13:55:29.132470: step 341, loss 7.21661, acc 0.671875, prec 0.0240385, recall 0.686901
2017-12-10T13:55:29.960923: step 342, loss 2.22898, acc 0.671875, prec 0.024091, recall 0.687898
2017-12-10T13:55:30.793559: step 343, loss 13.6298, acc 0.546875, prec 0.0241245, recall 0.686709
2017-12-10T13:55:31.637859: step 344, loss 4.3427, acc 0.46875, prec 0.0241417, recall 0.687697
2017-12-10T13:55:32.446115: step 345, loss 5.63665, acc 0.4375, prec 0.0241535, recall 0.688679
2017-12-10T13:55:33.259916: step 346, loss 4.92239, acc 0.546875, prec 0.0240765, recall 0.688679
2017-12-10T13:55:34.075515: step 347, loss 4.44131, acc 0.515625, prec 0.0239947, recall 0.688679
2017-12-10T13:55:34.900398: step 348, loss 6.7287, acc 0.40625, prec 0.0238953, recall 0.688679
2017-12-10T13:55:35.748484: step 349, loss 5.56627, acc 0.4375, prec 0.0239078, recall 0.689655
2017-12-10T13:55:36.581766: step 350, loss 4.88727, acc 0.4375, prec 0.0241316, recall 0.692547
2017-12-10T13:55:37.396674: step 351, loss 6.86621, acc 0.4375, prec 0.0240405, recall 0.690402
2017-12-10T13:55:38.224187: step 352, loss 3.86908, acc 0.5625, prec 0.0239682, recall 0.690402
2017-12-10T13:55:39.036412: step 353, loss 3.31358, acc 0.59375, prec 0.0239014, recall 0.690402
2017-12-10T13:55:39.870363: step 354, loss 23.0257, acc 0.703125, prec 0.0239598, recall 0.689231
2017-12-10T13:55:40.694914: step 355, loss 29.7717, acc 0.65625, prec 0.0239061, recall 0.687117
2017-12-10T13:55:41.514282: step 356, loss 3.86111, acc 0.609375, prec 0.0240502, recall 0.689024
2017-12-10T13:55:42.335152: step 357, loss 3.88165, acc 0.5625, prec 0.0240823, recall 0.68997
2017-12-10T13:55:43.166415: step 358, loss 3.71871, acc 0.5, prec 0.0242072, recall 0.691843
2017-12-10T13:55:44.009370: step 359, loss 4.54174, acc 0.484375, prec 0.024123, recall 0.691843
2017-12-10T13:55:44.849000: step 360, loss 2.81724, acc 0.59375, prec 0.0240571, recall 0.691843
2017-12-10T13:55:45.671723: step 361, loss 4.28394, acc 0.578125, prec 0.0240939, recall 0.690691
2017-12-10T13:55:46.500066: step 362, loss 4.07491, acc 0.515625, prec 0.0240159, recall 0.690691
2017-12-10T13:55:47.347811: step 363, loss 1.95862, acc 0.65625, prec 0.0239608, recall 0.690691
2017-12-10T13:55:48.179687: step 364, loss 1.86366, acc 0.765625, prec 0.0241265, recall 0.692537
2017-12-10T13:55:49.004642: step 365, loss 2.61751, acc 0.671875, prec 0.0241751, recall 0.693452
2017-12-10T13:55:49.817512: step 366, loss 3.396, acc 0.671875, prec 0.0241226, recall 0.693452
2017-12-10T13:55:50.647226: step 367, loss 3.28446, acc 0.65625, prec 0.0241686, recall 0.694362
2017-12-10T13:55:51.475680: step 368, loss 2.04811, acc 0.6875, prec 0.0242193, recall 0.695266
2017-12-10T13:55:52.290691: step 369, loss 1.1805, acc 0.75, prec 0.0241794, recall 0.695266
2017-12-10T13:55:53.105947: step 370, loss 1.4497, acc 0.796875, prec 0.0242474, recall 0.696165
2017-12-10T13:55:53.920179: step 371, loss 8.13143, acc 0.78125, prec 0.0242151, recall 0.694118
2017-12-10T13:55:54.727192: step 372, loss 1.08278, acc 0.828125, prec 0.0242878, recall 0.695015
2017-12-10T13:55:55.566109: step 373, loss 0.510978, acc 0.921875, prec 0.0242753, recall 0.695015
2017-12-10T13:55:56.354376: step 374, loss 12.4995, acc 0.796875, prec 0.0245449, recall 0.695652
2017-12-10T13:55:57.193552: step 375, loss 0.952124, acc 0.828125, prec 0.0245173, recall 0.695652
2017-12-10T13:55:58.015537: step 376, loss 1.15459, acc 0.75, prec 0.0244773, recall 0.695652
2017-12-10T13:55:58.864105: step 377, loss 2.23194, acc 0.71875, prec 0.0245318, recall 0.696532
2017-12-10T13:55:59.673691: step 378, loss 1.61973, acc 0.734375, prec 0.0244894, recall 0.696532
2017-12-10T13:56:00.496354: step 379, loss 2.38329, acc 0.671875, prec 0.024635, recall 0.698276
2017-12-10T13:56:01.307043: step 380, loss 2.66921, acc 0.640625, prec 0.0245777, recall 0.698276
2017-12-10T13:56:02.134297: step 381, loss 1.47724, acc 0.796875, prec 0.024644, recall 0.69914
2017-12-10T13:56:02.958845: step 382, loss 1.45109, acc 0.78125, prec 0.0246092, recall 0.69914
2017-12-10T13:56:03.773211: step 383, loss 12.5513, acc 0.8125, prec 0.0245819, recall 0.697143
2017-12-10T13:56:04.603273: step 384, loss 2.17607, acc 0.78125, prec 0.0245473, recall 0.697143
2017-12-10T13:56:05.439403: step 385, loss 1.60942, acc 0.828125, prec 0.0246182, recall 0.698006
2017-12-10T13:56:06.256184: step 386, loss 0.596941, acc 0.84375, prec 0.0245935, recall 0.698006
2017-12-10T13:56:07.075512: step 387, loss 13.4982, acc 0.71875, prec 0.0245516, recall 0.696023
2017-12-10T13:56:07.897420: step 388, loss 2.37825, acc 0.734375, prec 0.0247049, recall 0.69774
2017-12-10T13:56:08.727008: step 389, loss 1.10043, acc 0.78125, prec 0.0246704, recall 0.69774
2017-12-10T13:56:09.546212: step 390, loss 14.3453, acc 0.703125, prec 0.0246261, recall 0.695775
2017-12-10T13:56:10.381665: step 391, loss 3.9451, acc 0.71875, prec 0.0245845, recall 0.69382
2017-12-10T13:56:11.204985: step 392, loss 2.43945, acc 0.703125, prec 0.0246349, recall 0.694678
2017-12-10T13:56:12.019342: step 393, loss 2.98795, acc 0.578125, prec 0.0245691, recall 0.694678
2017-12-10T13:56:12.849530: step 394, loss 4.28968, acc 0.578125, prec 0.0245035, recall 0.694678
2017-12-10T13:56:13.662500: step 395, loss 2.79493, acc 0.65625, prec 0.0244504, recall 0.694678
2017-12-10T13:56:14.479068: step 396, loss 2.77504, acc 0.609375, prec 0.0243902, recall 0.694678
2017-12-10T13:56:15.291497: step 397, loss 7.82155, acc 0.625, prec 0.0245266, recall 0.694444
2017-12-10T13:56:16.112810: step 398, loss 20.7406, acc 0.5, prec 0.0244523, recall 0.692521
2017-12-10T13:56:16.939963: step 399, loss 5.23141, acc 0.546875, prec 0.0244783, recall 0.69337
2017-12-10T13:56:17.749705: step 400, loss 14.8394, acc 0.546875, prec 0.0245065, recall 0.692308
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-400

2017-12-10T13:56:20.415829: step 401, loss 3.47321, acc 0.59375, prec 0.0246339, recall 0.693989
2017-12-10T13:56:21.240362: step 402, loss 6.45661, acc 0.59375, prec 0.024763, recall 0.693767
2017-12-10T13:56:22.063731: step 403, loss 6.94648, acc 0.5, prec 0.0248747, recall 0.695418
2017-12-10T13:56:22.872762: step 404, loss 5.83027, acc 0.46875, prec 0.0251681, recall 0.698667
2017-12-10T13:56:23.691758: step 405, loss 6.10635, acc 0.484375, prec 0.0251819, recall 0.699468
2017-12-10T13:56:24.509456: step 406, loss 5.04445, acc 0.484375, prec 0.0251956, recall 0.700265
2017-12-10T13:56:25.324165: step 407, loss 5.912, acc 0.46875, prec 0.0252069, recall 0.701058
2017-12-10T13:56:26.145365: step 408, loss 4.75463, acc 0.53125, prec 0.0251352, recall 0.701058
2017-12-10T13:56:26.974400: step 409, loss 6.21089, acc 0.390625, prec 0.0250425, recall 0.701058
2017-12-10T13:56:27.807558: step 410, loss 6.42574, acc 0.46875, prec 0.0249623, recall 0.701058
2017-12-10T13:56:28.629885: step 411, loss 6.55755, acc 0.390625, prec 0.024871, recall 0.701058
2017-12-10T13:56:29.465585: step 412, loss 3.39052, acc 0.546875, prec 0.0248034, recall 0.701058
2017-12-10T13:56:30.271589: step 413, loss 4.49607, acc 0.515625, prec 0.0249137, recall 0.702632
2017-12-10T13:56:31.076592: step 414, loss 4.39215, acc 0.59375, prec 0.0249442, recall 0.703412
2017-12-10T13:56:31.883118: step 415, loss 3.6537, acc 0.640625, prec 0.0249814, recall 0.704188
2017-12-10T13:56:32.721633: step 416, loss 10.1817, acc 0.734375, prec 0.0251252, recall 0.703896
2017-12-10T13:56:33.544669: step 417, loss 2.47631, acc 0.703125, prec 0.025081, recall 0.703896
2017-12-10T13:56:34.358300: step 418, loss 1.40694, acc 0.78125, prec 0.0251386, recall 0.704663
2017-12-10T13:56:35.181299: step 419, loss 2.02491, acc 0.703125, prec 0.0250946, recall 0.704663
2017-12-10T13:56:35.998329: step 420, loss 1.20166, acc 0.8125, prec 0.0250668, recall 0.704663
2017-12-10T13:56:36.830289: step 421, loss 4.25375, acc 0.71875, prec 0.0250276, recall 0.702842
2017-12-10T13:56:37.634157: step 422, loss 1.09894, acc 0.75, prec 0.0249908, recall 0.702842
2017-12-10T13:56:38.477809: step 423, loss 1.68446, acc 0.796875, prec 0.024961, recall 0.702842
2017-12-10T13:56:39.287193: step 424, loss 18.5269, acc 0.71875, prec 0.0249244, recall 0.699229
2017-12-10T13:56:40.116222: step 425, loss 1.95019, acc 0.71875, prec 0.0249726, recall 0.7
2017-12-10T13:56:40.948390: step 426, loss 2.82468, acc 0.640625, prec 0.0250981, recall 0.701531
2017-12-10T13:56:41.778641: step 427, loss 1.4511, acc 0.796875, prec 0.0251572, recall 0.70229
2017-12-10T13:56:42.610904: step 428, loss 2.62916, acc 0.625, prec 0.025191, recall 0.703046
2017-12-10T13:56:43.430004: step 429, loss 1.89024, acc 0.71875, prec 0.0253268, recall 0.704545
2017-12-10T13:56:44.239924: step 430, loss 1.72591, acc 0.734375, prec 0.0252878, recall 0.704545
2017-12-10T13:56:45.065010: step 431, loss 9.7762, acc 0.609375, prec 0.025321, recall 0.703518
2017-12-10T13:56:45.902466: step 432, loss 2.69896, acc 0.6875, prec 0.0252753, recall 0.703518
2017-12-10T13:56:46.715285: step 433, loss 10.0952, acc 0.765625, prec 0.0252434, recall 0.701754
2017-12-10T13:56:47.533174: step 434, loss 1.73654, acc 0.75, prec 0.0253825, recall 0.703242
2017-12-10T13:56:48.362336: step 435, loss 3.31771, acc 0.59375, prec 0.0254108, recall 0.70398
2017-12-10T13:56:49.162108: step 436, loss 4.25644, acc 0.546875, prec 0.0254321, recall 0.704715
2017-12-10T13:56:49.997130: step 437, loss 2.23703, acc 0.703125, prec 0.0255631, recall 0.706173
2017-12-10T13:56:50.826415: step 438, loss 5.17733, acc 0.578125, prec 0.0255016, recall 0.706173
2017-12-10T13:56:51.667407: step 439, loss 3.48999, acc 0.53125, prec 0.0254335, recall 0.706173
2017-12-10T13:56:52.499001: step 440, loss 31.0326, acc 0.5, prec 0.0253636, recall 0.704434
2017-12-10T13:56:53.337879: step 441, loss 5.03484, acc 0.46875, prec 0.0252874, recall 0.704434
2017-12-10T13:56:54.158642: step 442, loss 2.79405, acc 0.6875, prec 0.0255008, recall 0.706601
2017-12-10T13:56:54.977387: step 443, loss 5.4526, acc 0.5, prec 0.0255147, recall 0.707317
2017-12-10T13:56:55.783735: step 444, loss 4.90905, acc 0.578125, prec 0.0254542, recall 0.707317
2017-12-10T13:56:56.600554: step 445, loss 5.31316, acc 0.53125, prec 0.0256433, recall 0.709443
2017-12-10T13:56:57.424405: step 446, loss 3.71266, acc 0.609375, prec 0.0258425, recall 0.711538
2017-12-10T13:56:58.259567: step 447, loss 5.21238, acc 0.515625, prec 0.0257727, recall 0.711538
2017-12-10T13:56:59.069904: step 448, loss 3.05851, acc 0.640625, prec 0.0258059, recall 0.71223
2017-12-10T13:56:59.895452: step 449, loss 4.13795, acc 0.625, prec 0.0258367, recall 0.712919
2017-12-10T13:57:00.708637: step 450, loss 12.2725, acc 0.71875, prec 0.0257986, recall 0.711217
2017-12-10T13:57:01.513129: step 451, loss 2.60329, acc 0.703125, prec 0.0258405, recall 0.711905
2017-12-10T13:57:02.338421: step 452, loss 3.2074, acc 0.625, prec 0.025787, recall 0.711905
2017-12-10T13:57:03.148501: step 453, loss 3.4035, acc 0.625, prec 0.0258176, recall 0.712589
2017-12-10T13:57:03.980561: step 454, loss 2.46439, acc 0.671875, prec 0.0258547, recall 0.71327
2017-12-10T13:57:04.797428: step 455, loss 1.18231, acc 0.8125, prec 0.0259116, recall 0.713948
2017-12-10T13:57:05.628954: step 456, loss 2.25098, acc 0.71875, prec 0.0258717, recall 0.713948
2017-12-10T13:57:06.475674: step 457, loss 3.0603, acc 0.6875, prec 0.0259107, recall 0.714623
2017-12-10T13:57:07.305013: step 458, loss 1.84681, acc 0.75, prec 0.0258753, recall 0.714623
2017-12-10T13:57:08.121080: step 459, loss 10.2817, acc 0.734375, prec 0.02584, recall 0.712941
2017-12-10T13:57:08.946387: step 460, loss 1.29769, acc 0.828125, prec 0.0258988, recall 0.713615
2017-12-10T13:57:09.761975: step 461, loss 0.599069, acc 0.828125, prec 0.0259574, recall 0.714286
2017-12-10T13:57:10.583194: step 462, loss 0.787901, acc 0.859375, prec 0.0259376, recall 0.714286
2017-12-10T13:57:11.408849: step 463, loss 1.28121, acc 0.859375, prec 0.0260005, recall 0.714953
2017-12-10T13:57:12.218580: step 464, loss 0.996746, acc 0.84375, prec 0.0259784, recall 0.714953
2017-12-10T13:57:13.044141: step 465, loss 0.743699, acc 0.890625, prec 0.0260456, recall 0.715618
2017-12-10T13:57:13.869823: step 466, loss 5.08921, acc 0.734375, prec 0.0260103, recall 0.713953
2017-12-10T13:57:14.707031: step 467, loss 1.56004, acc 0.796875, prec 0.0259817, recall 0.713953
2017-12-10T13:57:15.523163: step 468, loss 1.37087, acc 0.84375, prec 0.0259597, recall 0.713953
2017-12-10T13:57:16.344550: step 469, loss 1.55272, acc 0.78125, prec 0.0260113, recall 0.714617
2017-12-10T13:57:17.166636: step 470, loss 1.66961, acc 0.765625, prec 0.0259784, recall 0.714617
2017-12-10T13:57:17.995395: step 471, loss 1.20269, acc 0.78125, prec 0.0259478, recall 0.714617
2017-12-10T13:57:18.825823: step 472, loss 1.29268, acc 0.796875, prec 0.0259194, recall 0.714617
2017-12-10T13:57:19.658178: step 473, loss 1.76055, acc 0.796875, prec 0.0258911, recall 0.714617
2017-12-10T13:57:20.494470: step 474, loss 1.47113, acc 0.8125, prec 0.025865, recall 0.714617
2017-12-10T13:57:21.311045: step 475, loss 13.3866, acc 0.875, prec 0.0259315, recall 0.713626
2017-12-10T13:57:22.144047: step 476, loss 18.0405, acc 0.890625, prec 0.0259206, recall 0.710345
2017-12-10T13:57:22.996880: step 477, loss 2.49684, acc 0.78125, prec 0.0259719, recall 0.711009
2017-12-10T13:57:23.834036: step 478, loss 2.24919, acc 0.65625, prec 0.0259241, recall 0.711009
2017-12-10T13:57:24.664439: step 479, loss 3.25892, acc 0.578125, prec 0.0261094, recall 0.712984
2017-12-10T13:57:25.499323: step 480, loss 2.10513, acc 0.765625, prec 0.0262391, recall 0.714286
2017-12-10T13:57:26.324609: step 481, loss 3.42497, acc 0.671875, prec 0.0263552, recall 0.715576
2017-12-10T13:57:27.143771: step 482, loss 2.78579, acc 0.578125, prec 0.0262961, recall 0.715576
2017-12-10T13:57:27.971312: step 483, loss 4.60035, acc 0.515625, prec 0.0263093, recall 0.716216
2017-12-10T13:57:28.808183: step 484, loss 2.09833, acc 0.65625, prec 0.0262615, recall 0.716216
2017-12-10T13:57:29.621721: step 485, loss 2.72429, acc 0.625, prec 0.0262898, recall 0.716854
2017-12-10T13:57:30.450518: step 486, loss 4.48678, acc 0.59375, prec 0.0263937, recall 0.718121
2017-12-10T13:57:31.269266: step 487, loss 2.66856, acc 0.65625, prec 0.026346, recall 0.718121
2017-12-10T13:57:32.093363: step 488, loss 1.82462, acc 0.71875, prec 0.0265465, recall 0.72
2017-12-10T13:57:32.920068: step 489, loss 3.38858, acc 0.671875, prec 0.0265805, recall 0.720621
2017-12-10T13:57:33.726174: step 490, loss 3.22991, acc 0.65625, prec 0.0265328, recall 0.720621
2017-12-10T13:57:34.535587: step 491, loss 2.93958, acc 0.625, prec 0.0264809, recall 0.720621
2017-12-10T13:57:35.365716: step 492, loss 1.9762, acc 0.65625, prec 0.0264335, recall 0.720621
2017-12-10T13:57:36.188445: step 493, loss 6.93268, acc 0.65625, prec 0.0263884, recall 0.719027
2017-12-10T13:57:37.008118: step 494, loss 1.67871, acc 0.734375, prec 0.026431, recall 0.719647
2017-12-10T13:57:37.837852: step 495, loss 1.73401, acc 0.765625, prec 0.0264777, recall 0.720264
2017-12-10T13:57:38.689925: step 496, loss 2.09878, acc 0.71875, prec 0.0264392, recall 0.720264
2017-12-10T13:57:39.418372: step 497, loss 3.08926, acc 0.607843, prec 0.0263965, recall 0.720264
2017-12-10T13:57:40.244184: step 498, loss 2.01949, acc 0.765625, prec 0.0264431, recall 0.720879
2017-12-10T13:57:41.065313: step 499, loss 1.62626, acc 0.8125, prec 0.0264175, recall 0.720879
2017-12-10T13:57:41.871133: step 500, loss 1.44541, acc 0.765625, prec 0.0263856, recall 0.720879
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-500

2017-12-10T13:57:44.750071: step 501, loss 0.982938, acc 0.828125, prec 0.0264406, recall 0.721491
2017-12-10T13:57:45.574554: step 502, loss 0.828289, acc 0.828125, prec 0.0264172, recall 0.721491
2017-12-10T13:57:46.383409: step 503, loss 0.441277, acc 0.921875, prec 0.0264066, recall 0.721491
2017-12-10T13:57:47.204266: step 504, loss 0.661662, acc 0.875, prec 0.0264678, recall 0.722101
2017-12-10T13:57:48.032713: step 505, loss 1.05939, acc 0.875, prec 0.0265288, recall 0.722707
2017-12-10T13:57:48.844090: step 506, loss 0.801258, acc 0.796875, prec 0.0265012, recall 0.722707
2017-12-10T13:57:49.668977: step 507, loss 30.1884, acc 0.921875, prec 0.0265706, recall 0.721739
2017-12-10T13:57:50.490669: step 508, loss 0.648439, acc 0.84375, prec 0.0265494, recall 0.721739
2017-12-10T13:57:51.308794: step 509, loss 0.597721, acc 0.90625, prec 0.0266144, recall 0.722343
2017-12-10T13:57:52.130507: step 510, loss 0.555795, acc 0.921875, prec 0.0266816, recall 0.722944
2017-12-10T13:57:52.956972: step 511, loss 13.0924, acc 0.90625, prec 0.0267486, recall 0.721983
2017-12-10T13:57:53.765302: step 512, loss 19.3987, acc 0.875, prec 0.0267337, recall 0.72043
2017-12-10T13:57:54.592219: step 513, loss 1.63374, acc 0.828125, prec 0.0267878, recall 0.72103
2017-12-10T13:57:55.405848: step 514, loss 1.4577, acc 0.78125, prec 0.026758, recall 0.72103
2017-12-10T13:57:56.253713: step 515, loss 0.971997, acc 0.796875, prec 0.0268077, recall 0.721627
2017-12-10T13:57:57.075742: step 516, loss 2.50539, acc 0.796875, prec 0.0269347, recall 0.722815
2017-12-10T13:57:57.921208: step 517, loss 1.43478, acc 0.828125, prec 0.0270656, recall 0.723992
2017-12-10T13:57:58.716769: step 518, loss 1.58034, acc 0.859375, prec 0.0271235, recall 0.724576
2017-12-10T13:57:59.536899: step 519, loss 1.11844, acc 0.78125, prec 0.0272475, recall 0.725738
2017-12-10T13:58:00.362491: step 520, loss 1.5112, acc 0.78125, prec 0.0275251, recall 0.728033
2017-12-10T13:58:01.187799: step 521, loss 1.83121, acc 0.703125, prec 0.0276374, recall 0.729167
2017-12-10T13:58:02.009090: step 522, loss 1.43922, acc 0.796875, prec 0.0276858, recall 0.72973
2017-12-10T13:58:02.814626: step 523, loss 1.55772, acc 0.71875, prec 0.0276465, recall 0.72973
2017-12-10T13:58:03.658738: step 524, loss 3.70321, acc 0.6875, prec 0.0276817, recall 0.728778
2017-12-10T13:58:04.485631: step 525, loss 1.29969, acc 0.796875, prec 0.0276534, recall 0.728778
2017-12-10T13:58:05.317617: step 526, loss 2.27923, acc 0.734375, prec 0.0278453, recall 0.730453
2017-12-10T13:58:06.134152: step 527, loss 1.39423, acc 0.8125, prec 0.0278191, recall 0.730453
2017-12-10T13:58:06.973577: step 528, loss 1.92014, acc 0.765625, prec 0.0277865, recall 0.730453
2017-12-10T13:58:07.810799: step 529, loss 1.42416, acc 0.796875, prec 0.0277582, recall 0.730453
2017-12-10T13:58:08.637819: step 530, loss 2.41404, acc 0.734375, prec 0.0277214, recall 0.730453
2017-12-10T13:58:09.469813: step 531, loss 1.71971, acc 0.765625, prec 0.0277648, recall 0.731006
2017-12-10T13:58:10.280330: step 532, loss 1.56664, acc 0.734375, prec 0.027728, recall 0.731006
2017-12-10T13:58:11.095440: step 533, loss 4.26532, acc 0.8125, prec 0.0277043, recall 0.729508
2017-12-10T13:58:11.930105: step 534, loss 2.29095, acc 0.75, prec 0.0276698, recall 0.729508
2017-12-10T13:58:12.761398: step 535, loss 1.90994, acc 0.78125, prec 0.0277152, recall 0.730061
2017-12-10T13:58:13.571106: step 536, loss 2.20336, acc 0.734375, prec 0.0277541, recall 0.730612
2017-12-10T13:58:14.384244: step 537, loss 1.50728, acc 0.75, prec 0.027795, recall 0.731161
2017-12-10T13:58:15.204162: step 538, loss 1.16077, acc 0.828125, prec 0.0277713, recall 0.731161
2017-12-10T13:58:16.031798: step 539, loss 2.54973, acc 0.6875, prec 0.0278035, recall 0.731707
2017-12-10T13:58:16.878158: step 540, loss 1.5073, acc 0.765625, prec 0.0278463, recall 0.732252
2017-12-10T13:58:17.694765: step 541, loss 1.3621, acc 0.8125, prec 0.0278206, recall 0.732252
2017-12-10T13:58:18.502284: step 542, loss 1.37237, acc 0.84375, prec 0.0277992, recall 0.732252
2017-12-10T13:58:19.320362: step 543, loss 0.956845, acc 0.859375, prec 0.0277799, recall 0.732252
2017-12-10T13:58:20.153970: step 544, loss 1.70085, acc 0.796875, prec 0.0277522, recall 0.732252
2017-12-10T13:58:20.977978: step 545, loss 1.32191, acc 0.8125, prec 0.0278759, recall 0.733333
2017-12-10T13:58:21.800536: step 546, loss 1.08104, acc 0.765625, prec 0.0278438, recall 0.733333
2017-12-10T13:58:22.589585: step 547, loss 1.04868, acc 0.84375, prec 0.0278225, recall 0.733333
2017-12-10T13:58:23.406741: step 548, loss 2.54113, acc 0.859375, prec 0.0279544, recall 0.732932
2017-12-10T13:58:24.218772: step 549, loss 0.844107, acc 0.90625, prec 0.0279415, recall 0.732932
2017-12-10T13:58:25.051641: step 550, loss 0.715118, acc 0.90625, prec 0.0280031, recall 0.733467
2017-12-10T13:58:25.876352: step 551, loss 0.624464, acc 0.859375, prec 0.0279838, recall 0.733467
2017-12-10T13:58:26.705903: step 552, loss 22.432, acc 0.78125, prec 0.027956, recall 0.732
2017-12-10T13:58:27.526256: step 553, loss 10.3617, acc 0.8125, prec 0.0279325, recall 0.730539
2017-12-10T13:58:28.361647: step 554, loss 20.7496, acc 0.8125, prec 0.0280573, recall 0.730159
2017-12-10T13:58:29.193452: step 555, loss 1.29096, acc 0.84375, prec 0.028184, recall 0.731225
2017-12-10T13:58:30.012175: step 556, loss 2.30341, acc 0.703125, prec 0.0282172, recall 0.731755
2017-12-10T13:58:30.843352: step 557, loss 1.91782, acc 0.765625, prec 0.0282589, recall 0.732283
2017-12-10T13:58:31.655968: step 558, loss 2.09484, acc 0.6875, prec 0.028216, recall 0.732283
2017-12-10T13:58:32.468358: step 559, loss 2.90185, acc 0.640625, prec 0.0281669, recall 0.732283
2017-12-10T13:58:33.307399: step 560, loss 2.3087, acc 0.609375, prec 0.0281137, recall 0.732283
2017-12-10T13:58:34.131941: step 561, loss 3.3172, acc 0.5625, prec 0.0280543, recall 0.732283
2017-12-10T13:58:34.942780: step 562, loss 3.38632, acc 0.59375, prec 0.0281457, recall 0.733333
2017-12-10T13:58:35.753794: step 563, loss 5.88015, acc 0.609375, prec 0.028168, recall 0.732422
2017-12-10T13:58:36.563856: step 564, loss 3.86755, acc 0.53125, prec 0.0281775, recall 0.732943
2017-12-10T13:58:37.390969: step 565, loss 3.52794, acc 0.5625, prec 0.0281185, recall 0.732943
2017-12-10T13:58:38.203354: step 566, loss 3.65056, acc 0.640625, prec 0.0282153, recall 0.733981
2017-12-10T13:58:39.036382: step 567, loss 3.72623, acc 0.5625, prec 0.0283012, recall 0.73501
2017-12-10T13:58:39.863554: step 568, loss 7.57383, acc 0.578125, prec 0.0283909, recall 0.734615
2017-12-10T13:58:40.685152: step 569, loss 2.85937, acc 0.65625, prec 0.0284888, recall 0.735632
2017-12-10T13:58:41.493702: step 570, loss 4.78741, acc 0.546875, prec 0.0284995, recall 0.736138
2017-12-10T13:58:42.315497: step 571, loss 3.51783, acc 0.59375, prec 0.0285165, recall 0.736641
2017-12-10T13:58:43.134924: step 572, loss 3.20902, acc 0.640625, prec 0.0284682, recall 0.736641
2017-12-10T13:58:43.946573: step 573, loss 3.76753, acc 0.578125, prec 0.0284116, recall 0.736641
2017-12-10T13:58:44.769005: step 574, loss 3.11102, acc 0.640625, prec 0.028435, recall 0.737143
2017-12-10T13:58:45.589718: step 575, loss 3.10938, acc 0.640625, prec 0.028387, recall 0.737143
2017-12-10T13:58:46.414856: step 576, loss 1.85136, acc 0.71875, prec 0.0283496, recall 0.737143
2017-12-10T13:58:47.268995: step 577, loss 1.82305, acc 0.765625, prec 0.0283896, recall 0.737643
2017-12-10T13:58:48.111538: step 578, loss 1.25228, acc 0.828125, prec 0.0284378, recall 0.73814
2017-12-10T13:58:48.936200: step 579, loss 5.98125, acc 0.84375, prec 0.0284191, recall 0.736742
2017-12-10T13:58:49.742624: step 580, loss 1.22342, acc 0.84375, prec 0.0283983, recall 0.736742
2017-12-10T13:58:50.566284: step 581, loss 1.95414, acc 0.703125, prec 0.0284298, recall 0.73724
2017-12-10T13:58:51.379999: step 582, loss 0.634228, acc 0.875, prec 0.028484, recall 0.737736
2017-12-10T13:58:52.208384: step 583, loss 0.900607, acc 0.84375, prec 0.0284633, recall 0.737736
2017-12-10T13:58:53.023784: step 584, loss 0.755018, acc 0.84375, prec 0.0284426, recall 0.737736
2017-12-10T13:58:53.836165: step 585, loss 1.00848, acc 0.875, prec 0.028426, recall 0.737736
2017-12-10T13:58:54.647795: step 586, loss 0.843356, acc 0.875, prec 0.0284095, recall 0.737736
2017-12-10T13:58:55.472102: step 587, loss 0.854697, acc 0.84375, prec 0.0284594, recall 0.73823
2017-12-10T13:58:56.304753: step 588, loss 19.7389, acc 0.875, prec 0.028445, recall 0.736842
2017-12-10T13:58:57.137618: step 589, loss 0.872591, acc 0.890625, prec 0.0284305, recall 0.736842
2017-12-10T13:58:57.961650: step 590, loss 0.629336, acc 0.9375, prec 0.0284927, recall 0.737336
2017-12-10T13:58:58.784694: step 591, loss 0.74088, acc 0.90625, prec 0.0285507, recall 0.737828
2017-12-10T13:58:59.594206: step 592, loss 1.03881, acc 0.796875, prec 0.0285942, recall 0.738318
2017-12-10T13:59:00.407738: step 593, loss 0.53701, acc 0.90625, prec 0.0285818, recall 0.738318
2017-12-10T13:59:01.240690: step 594, loss 1.97725, acc 0.84375, prec 0.0285632, recall 0.73694
2017-12-10T13:59:02.085115: step 595, loss 0.755517, acc 0.859375, prec 0.028685, recall 0.737918
2017-12-10T13:59:02.922518: step 596, loss 0.375714, acc 0.921875, prec 0.0286746, recall 0.737918
2017-12-10T13:59:03.733257: step 597, loss 38.0848, acc 0.828125, prec 0.0286539, recall 0.736549
2017-12-10T13:59:04.555799: step 598, loss 1.16253, acc 0.828125, prec 0.0286312, recall 0.736549
2017-12-10T13:59:05.386206: step 599, loss 0.805361, acc 0.921875, prec 0.0287609, recall 0.737523
2017-12-10T13:59:06.204903: step 600, loss 0.951644, acc 0.84375, prec 0.0287402, recall 0.737523
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-600

2017-12-10T13:59:09.021793: step 601, loss 1.31742, acc 0.796875, prec 0.0287832, recall 0.738007
2017-12-10T13:59:09.848881: step 602, loss 1.2965, acc 0.8125, prec 0.0287584, recall 0.738007
2017-12-10T13:59:10.663817: step 603, loss 1.37992, acc 0.828125, prec 0.0287356, recall 0.738007
2017-12-10T13:59:11.487996: step 604, loss 1.7611, acc 0.75, prec 0.0287026, recall 0.738007
2017-12-10T13:59:12.296389: step 605, loss 0.325449, acc 0.890625, prec 0.0287579, recall 0.73849
2017-12-10T13:59:13.116484: step 606, loss 1.35757, acc 0.8125, prec 0.0287332, recall 0.73849
2017-12-10T13:59:13.945956: step 607, loss 14.1225, acc 0.8125, prec 0.0287105, recall 0.737132
2017-12-10T13:59:14.773364: step 608, loss 1.23889, acc 0.75, prec 0.0286777, recall 0.737132
2017-12-10T13:59:15.599497: step 609, loss 1.8315, acc 0.734375, prec 0.0287122, recall 0.737615
2017-12-10T13:59:16.437613: step 610, loss 1.10899, acc 0.828125, prec 0.028759, recall 0.738095
2017-12-10T13:59:17.256588: step 611, loss 1.75217, acc 0.765625, prec 0.0287283, recall 0.738095
2017-12-10T13:59:18.075159: step 612, loss 0.80304, acc 0.8125, prec 0.0287037, recall 0.738095
2017-12-10T13:59:18.890611: step 613, loss 1.00686, acc 0.859375, prec 0.0288236, recall 0.739051
2017-12-10T13:59:19.718899: step 614, loss 0.791882, acc 0.828125, prec 0.028801, recall 0.739051
2017-12-10T13:59:20.564553: step 615, loss 0.844783, acc 0.859375, prec 0.0287826, recall 0.739051
2017-12-10T13:59:21.388031: step 616, loss 1.1941, acc 0.875, prec 0.0287662, recall 0.739051
2017-12-10T13:59:22.209297: step 617, loss 1.83226, acc 0.859375, prec 0.0288168, recall 0.739526
2017-12-10T13:59:23.020668: step 618, loss 0.592911, acc 0.890625, prec 0.0290092, recall 0.740942
2017-12-10T13:59:23.843608: step 619, loss 1.23224, acc 0.8125, prec 0.0290533, recall 0.74141
2017-12-10T13:59:24.672072: step 620, loss 11.8377, acc 0.84375, prec 0.0291035, recall 0.740541
2017-12-10T13:59:25.485664: step 621, loss 0.792196, acc 0.8125, prec 0.0291475, recall 0.741007
2017-12-10T13:59:26.299418: step 622, loss 6.01328, acc 0.828125, prec 0.0291955, recall 0.740143
2017-12-10T13:59:27.124003: step 623, loss 0.969507, acc 0.8125, prec 0.0291708, recall 0.740143
2017-12-10T13:59:27.953444: step 624, loss 1.61759, acc 0.734375, prec 0.0291358, recall 0.740143
2017-12-10T13:59:28.773121: step 625, loss 1.37625, acc 0.71875, prec 0.0290989, recall 0.740143
2017-12-10T13:59:29.596970: step 626, loss 2.16444, acc 0.6875, prec 0.0291945, recall 0.741071
2017-12-10T13:59:30.426500: step 627, loss 1.97463, acc 0.71875, prec 0.0291576, recall 0.741071
2017-12-10T13:59:31.244308: step 628, loss 2.95836, acc 0.625, prec 0.0291766, recall 0.741533
2017-12-10T13:59:32.057786: step 629, loss 2.17375, acc 0.734375, prec 0.0291419, recall 0.741533
2017-12-10T13:59:32.906744: step 630, loss 2.81455, acc 0.6875, prec 0.029169, recall 0.741993
2017-12-10T13:59:33.760331: step 631, loss 3.0785, acc 0.65625, prec 0.029192, recall 0.742451
2017-12-10T13:59:34.569716: step 632, loss 3.60097, acc 0.609375, prec 0.0291411, recall 0.742451
2017-12-10T13:59:35.393195: step 633, loss 2.51983, acc 0.625, prec 0.0290924, recall 0.742451
2017-12-10T13:59:36.191212: step 634, loss 1.37777, acc 0.765625, prec 0.0291296, recall 0.742908
2017-12-10T13:59:37.009265: step 635, loss 2.26136, acc 0.640625, prec 0.0290831, recall 0.742908
2017-12-10T13:59:37.837515: step 636, loss 1.11565, acc 0.75, prec 0.0291854, recall 0.743816
2017-12-10T13:59:38.674914: step 637, loss 0.938559, acc 0.8125, prec 0.0291612, recall 0.743816
2017-12-10T13:59:39.491157: step 638, loss 1.76736, acc 0.78125, prec 0.0292673, recall 0.744718
2017-12-10T13:59:40.294318: step 639, loss 0.395452, acc 0.921875, prec 0.0292572, recall 0.744718
2017-12-10T13:59:41.105452: step 640, loss 0.96852, acc 0.859375, prec 0.029239, recall 0.744718
2017-12-10T13:59:41.920637: step 641, loss 0.370271, acc 0.890625, prec 0.0292248, recall 0.744718
2017-12-10T13:59:42.742708: step 642, loss 0.729281, acc 0.875, prec 0.0292757, recall 0.745167
2017-12-10T13:59:43.571641: step 643, loss 0.49481, acc 0.90625, prec 0.0293306, recall 0.745614
2017-12-10T13:59:44.400971: step 644, loss 0.268426, acc 0.953125, prec 0.0293245, recall 0.745614
2017-12-10T13:59:45.239495: step 645, loss 5.3613, acc 0.890625, prec 0.0293124, recall 0.744308
2017-12-10T13:59:46.065032: step 646, loss 3.29791, acc 0.921875, prec 0.0293712, recall 0.743455
2017-12-10T13:59:46.890683: step 647, loss 0.524047, acc 0.875, prec 0.0294219, recall 0.743902
2017-12-10T13:59:47.743545: step 648, loss 2.67375, acc 0.890625, prec 0.0294097, recall 0.742609
2017-12-10T13:59:48.587906: step 649, loss 0.514344, acc 0.890625, prec 0.0294624, recall 0.743056
2017-12-10T13:59:49.436680: step 650, loss 0.268426, acc 0.890625, prec 0.0294482, recall 0.743056
2017-12-10T13:59:50.255506: step 651, loss 0.336631, acc 0.9375, prec 0.0294401, recall 0.743056
2017-12-10T13:59:51.090124: step 652, loss 1.23873, acc 0.796875, prec 0.0294138, recall 0.743056
2017-12-10T13:59:51.908140: step 653, loss 0.556598, acc 0.859375, prec 0.0293956, recall 0.743056
2017-12-10T13:59:52.727451: step 654, loss 1.21966, acc 0.8125, prec 0.029438, recall 0.743501
2017-12-10T13:59:53.553486: step 655, loss 3.43363, acc 0.8125, prec 0.0294823, recall 0.74266
2017-12-10T13:59:54.369780: step 656, loss 6.71267, acc 0.828125, prec 0.0295286, recall 0.741824
2017-12-10T13:59:55.186232: step 657, loss 1.14703, acc 0.828125, prec 0.0295728, recall 0.742268
2017-12-10T13:59:56.028553: step 658, loss 1.59895, acc 0.8125, prec 0.0296813, recall 0.743151
2017-12-10T13:59:56.867741: step 659, loss 1.44035, acc 0.8125, prec 0.0299884, recall 0.745331
2017-12-10T13:59:57.686749: step 660, loss 1.46658, acc 0.78125, prec 0.0300259, recall 0.745763
2017-12-10T13:59:58.515630: step 661, loss 2.21627, acc 0.71875, prec 0.0300552, recall 0.746193
2017-12-10T13:59:59.330738: step 662, loss 5.91482, acc 0.78125, prec 0.0302267, recall 0.746219
2017-12-10T14:00:00.165404: step 663, loss 2.57439, acc 0.65625, prec 0.0301815, recall 0.746219
2017-12-10T14:00:00.978032: step 664, loss 13.3079, acc 0.734375, prec 0.0301487, recall 0.744966
2017-12-10T14:00:01.795209: step 665, loss 2.61139, acc 0.703125, prec 0.0303729, recall 0.746667
2017-12-10T14:00:02.629708: step 666, loss 2.26523, acc 0.6875, prec 0.030463, recall 0.747508
2017-12-10T14:00:03.453464: step 667, loss 3.93313, acc 0.625, prec 0.0305447, recall 0.748344
2017-12-10T14:00:04.286217: step 668, loss 3.72144, acc 0.625, prec 0.0305606, recall 0.74876
2017-12-10T14:00:05.110844: step 669, loss 3.57419, acc 0.59375, prec 0.0306377, recall 0.749588
2017-12-10T14:00:05.930967: step 670, loss 6.88768, acc 0.546875, prec 0.0306452, recall 0.748768
2017-12-10T14:00:06.743297: step 671, loss 5.42669, acc 0.53125, prec 0.0306485, recall 0.74918
2017-12-10T14:00:07.579592: step 672, loss 4.73914, acc 0.46875, prec 0.0305788, recall 0.74918
2017-12-10T14:00:08.394624: step 673, loss 3.45633, acc 0.5625, prec 0.0305863, recall 0.749591
2017-12-10T14:00:09.211311: step 674, loss 4.14307, acc 0.53125, prec 0.030719, recall 0.750814
2017-12-10T14:00:10.033824: step 675, loss 2.60541, acc 0.703125, prec 0.0308092, recall 0.751623
2017-12-10T14:00:10.842185: step 676, loss 3.39156, acc 0.640625, prec 0.0307621, recall 0.751623
2017-12-10T14:00:11.660612: step 677, loss 3.0673, acc 0.640625, prec 0.0308437, recall 0.752427
2017-12-10T14:00:12.483205: step 678, loss 2.13797, acc 0.671875, prec 0.030865, recall 0.752827
2017-12-10T14:00:13.287059: step 679, loss 2.26914, acc 0.671875, prec 0.0308862, recall 0.753226
2017-12-10T14:00:14.121343: step 680, loss 1.61019, acc 0.78125, prec 0.0309857, recall 0.754019
2017-12-10T14:00:14.947857: step 681, loss 1.85174, acc 0.765625, prec 0.031019, recall 0.754414
2017-12-10T14:00:15.777578: step 682, loss 0.609027, acc 0.875, prec 0.0310026, recall 0.754414
2017-12-10T14:00:16.609586: step 683, loss 1.15073, acc 0.84375, prec 0.0310461, recall 0.754808
2017-12-10T14:00:17.436235: step 684, loss 0.844096, acc 0.859375, prec 0.0310915, recall 0.7552
2017-12-10T14:00:18.245405: step 685, loss 1.31764, acc 0.8125, prec 0.0310669, recall 0.7552
2017-12-10T14:00:19.055132: step 686, loss 0.402871, acc 0.9375, prec 0.0311225, recall 0.755591
2017-12-10T14:00:19.871865: step 687, loss 0.590276, acc 0.875, prec 0.0311061, recall 0.755591
2017-12-10T14:00:20.700878: step 688, loss 0.173388, acc 0.9375, prec 0.0312253, recall 0.756369
2017-12-10T14:00:21.508841: step 689, loss 0.760965, acc 0.859375, prec 0.0312069, recall 0.756369
2017-12-10T14:00:22.334130: step 690, loss 26.0573, acc 0.953125, prec 0.0312028, recall 0.755167
2017-12-10T14:00:23.156484: step 691, loss 0.463686, acc 0.90625, prec 0.0311905, recall 0.755167
2017-12-10T14:00:24.002456: step 692, loss 0.288161, acc 0.921875, prec 0.0311803, recall 0.755167
2017-12-10T14:00:24.840635: step 693, loss 0.280854, acc 0.953125, prec 0.0311741, recall 0.755167
2017-12-10T14:00:25.679181: step 694, loss 0.415141, acc 0.90625, prec 0.0312254, recall 0.755556
2017-12-10T14:00:26.492679: step 695, loss 0.116804, acc 0.96875, prec 0.0312848, recall 0.755943
2017-12-10T14:00:27.310729: step 696, loss 10.315, acc 0.90625, prec 0.0312746, recall 0.754747
2017-12-10T14:00:28.129854: step 697, loss 10.6752, acc 0.90625, prec 0.0313913, recall 0.754331
2017-12-10T14:00:28.939422: step 698, loss 0.813604, acc 0.859375, prec 0.0313728, recall 0.754331
2017-12-10T14:00:29.765093: step 699, loss 0.422298, acc 0.890625, prec 0.0314218, recall 0.754717
2017-12-10T14:00:30.593952: step 700, loss 0.631782, acc 0.875, prec 0.0314054, recall 0.754717
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-700

2017-12-10T14:00:33.377356: step 701, loss 6.18236, acc 0.90625, prec 0.0315218, recall 0.754304
2017-12-10T14:00:34.193253: step 702, loss 1.63935, acc 0.875, prec 0.0316319, recall 0.75507
2017-12-10T14:00:35.026446: step 703, loss 7.40121, acc 0.765625, prec 0.0317295, recall 0.754658
2017-12-10T14:00:35.822460: step 704, loss 1.51923, acc 0.765625, prec 0.0316984, recall 0.754658
2017-12-10T14:00:36.653623: step 705, loss 1.76856, acc 0.703125, prec 0.0316592, recall 0.754658
2017-12-10T14:00:37.451178: step 706, loss 1.28707, acc 0.796875, prec 0.0316324, recall 0.754658
2017-12-10T14:00:38.265976: step 707, loss 3.22291, acc 0.609375, prec 0.0316439, recall 0.755039
2017-12-10T14:00:39.094916: step 708, loss 4.17925, acc 0.578125, prec 0.0316513, recall 0.755418
2017-12-10T14:00:39.901942: step 709, loss 4.30119, acc 0.609375, prec 0.0316021, recall 0.75425
2017-12-10T14:00:40.739511: step 710, loss 4.52449, acc 0.546875, prec 0.0316055, recall 0.75463
2017-12-10T14:00:41.545225: step 711, loss 5.07925, acc 0.5, prec 0.0315402, recall 0.75463
2017-12-10T14:00:42.365987: step 712, loss 4.03993, acc 0.484375, prec 0.0315356, recall 0.755008
2017-12-10T14:00:43.193528: step 713, loss 4.41702, acc 0.546875, prec 0.0315391, recall 0.755385
2017-12-10T14:00:44.027161: step 714, loss 3.23845, acc 0.59375, prec 0.0314865, recall 0.755385
2017-12-10T14:00:44.861840: step 715, loss 3.31505, acc 0.609375, prec 0.0314361, recall 0.755385
2017-12-10T14:00:45.688707: step 716, loss 3.04366, acc 0.53125, prec 0.0313758, recall 0.755385
2017-12-10T14:00:46.497052: step 717, loss 4.10525, acc 0.546875, prec 0.0313796, recall 0.75576
2017-12-10T14:00:47.316753: step 718, loss 2.44249, acc 0.625, prec 0.0313933, recall 0.756135
2017-12-10T14:00:48.145557: step 719, loss 2.66828, acc 0.703125, prec 0.0313553, recall 0.756135
2017-12-10T14:00:48.965985: step 720, loss 1.65709, acc 0.75, prec 0.0313235, recall 0.756135
2017-12-10T14:00:49.811969: step 721, loss 1.80095, acc 0.75, prec 0.0314146, recall 0.756881
2017-12-10T14:00:50.643142: step 722, loss 2.17518, acc 0.734375, prec 0.0314422, recall 0.757252
2017-12-10T14:00:51.483974: step 723, loss 0.940136, acc 0.8125, prec 0.0315409, recall 0.757991
2017-12-10T14:00:52.307309: step 724, loss 1.62742, acc 0.71875, prec 0.031505, recall 0.757991
2017-12-10T14:00:53.159339: step 725, loss 1.39593, acc 0.8125, prec 0.0314811, recall 0.757991
2017-12-10T14:00:53.976288: step 726, loss 1.24259, acc 0.8125, prec 0.0314573, recall 0.757991
2017-12-10T14:00:54.796311: step 727, loss 17.3429, acc 0.890625, prec 0.0314454, recall 0.756839
2017-12-10T14:00:55.600994: step 728, loss 1.20412, acc 0.84375, prec 0.0314255, recall 0.756839
2017-12-10T14:00:56.419163: step 729, loss 10.0005, acc 0.84375, prec 0.0314077, recall 0.75569
2017-12-10T14:00:57.243983: step 730, loss 0.208425, acc 0.921875, prec 0.0314588, recall 0.756061
2017-12-10T14:00:58.053477: step 731, loss 0.445534, acc 0.890625, prec 0.031506, recall 0.75643
2017-12-10T14:00:58.855897: step 732, loss 0.548915, acc 0.875, prec 0.0315511, recall 0.756798
2017-12-10T14:00:59.684602: step 733, loss 0.695087, acc 0.875, prec 0.0315962, recall 0.757164
2017-12-10T14:01:00.515420: step 734, loss 0.653925, acc 0.90625, prec 0.0316452, recall 0.75753
2017-12-10T14:01:01.352060: step 735, loss 0.457096, acc 0.90625, prec 0.031755, recall 0.758258
2017-12-10T14:01:02.170303: step 736, loss 1.93949, acc 0.875, prec 0.0318607, recall 0.758982
2017-12-10T14:01:02.989512: step 737, loss 0.622675, acc 0.84375, prec 0.0319015, recall 0.759342
2017-12-10T14:01:03.815462: step 738, loss 0.89347, acc 0.875, prec 0.0319463, recall 0.759701
2017-12-10T14:01:04.640298: step 739, loss 1.00542, acc 0.8125, prec 0.0319829, recall 0.76006
2017-12-10T14:01:05.441548: step 740, loss 0.838066, acc 0.828125, prec 0.0319609, recall 0.76006
2017-12-10T14:01:06.264776: step 741, loss 10.8757, acc 0.8125, prec 0.0319389, recall 0.758929
2017-12-10T14:01:07.098010: step 742, loss 7.21184, acc 0.8125, prec 0.0319169, recall 0.757801
2017-12-10T14:01:07.914225: step 743, loss 3.56538, acc 0.828125, prec 0.0319575, recall 0.757037
2017-12-10T14:01:08.733063: step 744, loss 1.22408, acc 0.75, prec 0.0319255, recall 0.757037
2017-12-10T14:01:09.547602: step 745, loss 2.03467, acc 0.75, prec 0.0318936, recall 0.757037
2017-12-10T14:01:10.370105: step 746, loss 1.58997, acc 0.75, prec 0.0318618, recall 0.757037
2017-12-10T14:01:11.201892: step 747, loss 3.58685, acc 0.6875, prec 0.0318241, recall 0.755917
2017-12-10T14:01:12.013556: step 748, loss 1.85328, acc 0.78125, prec 0.0318566, recall 0.756278
2017-12-10T14:01:12.824450: step 749, loss 6.82981, acc 0.65625, prec 0.0318752, recall 0.755523
2017-12-10T14:01:13.673275: step 750, loss 4.31996, acc 0.578125, prec 0.0318819, recall 0.755882
2017-12-10T14:01:14.484961: step 751, loss 2.42767, acc 0.71875, prec 0.0319063, recall 0.756241
2017-12-10T14:01:15.312238: step 752, loss 2.56063, acc 0.734375, prec 0.0319327, recall 0.756598
2017-12-10T14:01:16.136682: step 753, loss 2.28853, acc 0.578125, prec 0.0318794, recall 0.756598
2017-12-10T14:01:16.975812: step 754, loss 2.99817, acc 0.546875, prec 0.0318224, recall 0.756598
2017-12-10T14:01:17.804459: step 755, loss 4.19639, acc 0.59375, prec 0.0318311, recall 0.756955
2017-12-10T14:01:18.625330: step 756, loss 5.47309, acc 0.515625, prec 0.0318894, recall 0.757664
2017-12-10T14:01:19.461245: step 757, loss 2.7287, acc 0.6875, prec 0.0318503, recall 0.757664
2017-12-10T14:01:20.272632: step 758, loss 3.65954, acc 0.578125, prec 0.0317976, recall 0.757664
2017-12-10T14:01:21.071375: step 759, loss 3.19728, acc 0.59375, prec 0.031747, recall 0.757664
2017-12-10T14:01:21.928667: step 760, loss 2.45253, acc 0.65625, prec 0.0317635, recall 0.758017
2017-12-10T14:01:22.731983: step 761, loss 1.72232, acc 0.703125, prec 0.0317267, recall 0.758017
2017-12-10T14:01:23.556755: step 762, loss 14.502, acc 0.671875, prec 0.031747, recall 0.757267
2017-12-10T14:01:24.387212: step 763, loss 2.9296, acc 0.65625, prec 0.0317634, recall 0.75762
2017-12-10T14:01:25.205835: step 764, loss 1.68721, acc 0.75, prec 0.0318502, recall 0.758321
2017-12-10T14:01:26.017257: step 765, loss 2.79641, acc 0.671875, prec 0.0318684, recall 0.758671
2017-12-10T14:01:26.847765: step 766, loss 1.32932, acc 0.828125, prec 0.0318471, recall 0.758671
2017-12-10T14:01:27.673473: step 767, loss 1.06057, acc 0.8125, prec 0.0318827, recall 0.759019
2017-12-10T14:01:28.494305: step 768, loss 1.41388, acc 0.796875, prec 0.0318576, recall 0.759019
2017-12-10T14:01:29.320345: step 769, loss 15.9, acc 0.84375, prec 0.032016, recall 0.758967
2017-12-10T14:01:30.158241: step 770, loss 6.95376, acc 0.765625, prec 0.0320474, recall 0.758226
2017-12-10T14:01:30.972599: step 771, loss 1.12059, acc 0.75, prec 0.0321334, recall 0.758916
2017-12-10T14:01:31.791206: step 772, loss 1.6389, acc 0.8125, prec 0.0321685, recall 0.759259
2017-12-10T14:01:32.638680: step 773, loss 1.55512, acc 0.84375, prec 0.0321491, recall 0.759259
2017-12-10T14:01:33.460161: step 774, loss 2.599, acc 0.65625, prec 0.0321648, recall 0.759602
2017-12-10T14:01:34.272461: step 775, loss 1.17995, acc 0.84375, prec 0.0321454, recall 0.759602
2017-12-10T14:01:35.087930: step 776, loss 0.931621, acc 0.78125, prec 0.0322348, recall 0.760284
2017-12-10T14:01:35.910224: step 777, loss 1.83094, acc 0.75, prec 0.0322619, recall 0.760623
2017-12-10T14:01:36.749037: step 778, loss 1.0253, acc 0.859375, prec 0.0323026, recall 0.760962
2017-12-10T14:01:37.579480: step 779, loss 2.37578, acc 0.6875, prec 0.0323799, recall 0.761636
2017-12-10T14:01:38.399201: step 780, loss 1.45162, acc 0.78125, prec 0.0324687, recall 0.762307
2017-12-10T14:01:39.215302: step 781, loss 1.75709, acc 0.78125, prec 0.0324994, recall 0.76264
2017-12-10T14:01:40.016109: step 782, loss 0.930986, acc 0.84375, prec 0.03248, recall 0.76264
2017-12-10T14:01:40.844693: step 783, loss 10.6566, acc 0.734375, prec 0.0324489, recall 0.761571
2017-12-10T14:01:41.681332: step 784, loss 0.874845, acc 0.828125, prec 0.0324854, recall 0.761905
2017-12-10T14:01:42.503129: step 785, loss 0.764478, acc 0.859375, prec 0.0324679, recall 0.761905
2017-12-10T14:01:43.323497: step 786, loss 1.76196, acc 0.796875, prec 0.0326158, recall 0.762901
2017-12-10T14:01:44.124004: step 787, loss 22.3555, acc 0.828125, prec 0.032656, recall 0.761111
2017-12-10T14:01:44.951242: step 788, loss 2.23756, acc 0.71875, prec 0.032621, recall 0.761111
2017-12-10T14:01:45.769769: step 789, loss 1.87089, acc 0.71875, prec 0.0325861, recall 0.761111
2017-12-10T14:01:46.574035: step 790, loss 1.33323, acc 0.78125, prec 0.0326164, recall 0.761442
2017-12-10T14:01:47.388148: step 791, loss 1.91762, acc 0.703125, prec 0.0326371, recall 0.761773
2017-12-10T14:01:48.204711: step 792, loss 1.55357, acc 0.703125, prec 0.0326003, recall 0.761773
2017-12-10T14:01:48.998683: step 793, loss 3.75676, acc 0.671875, prec 0.0326743, recall 0.762431
2017-12-10T14:01:49.824217: step 794, loss 3.05539, acc 0.609375, prec 0.0327975, recall 0.763411
2017-12-10T14:01:50.648558: step 795, loss 1.94659, acc 0.6875, prec 0.032873, recall 0.76406
2017-12-10T14:01:51.471204: step 796, loss 3.65693, acc 0.609375, prec 0.0328816, recall 0.764384
2017-12-10T14:01:52.281368: step 797, loss 2.34348, acc 0.65625, prec 0.0330097, recall 0.765348
2017-12-10T14:01:53.105212: step 798, loss 1.60835, acc 0.71875, prec 0.0329748, recall 0.765348
2017-12-10T14:01:53.924806: step 799, loss 3.61117, acc 0.65625, prec 0.032989, recall 0.765668
2017-12-10T14:01:54.751329: step 800, loss 1.80803, acc 0.765625, prec 0.0330167, recall 0.765986
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-800

2017-12-10T14:01:57.849084: step 801, loss 2.17057, acc 0.6875, prec 0.032978, recall 0.765986
2017-12-10T14:01:58.686989: step 802, loss 1.35393, acc 0.8125, prec 0.0329548, recall 0.765986
2017-12-10T14:01:59.502696: step 803, loss 2.03085, acc 0.734375, prec 0.0329221, recall 0.765986
2017-12-10T14:02:00.322060: step 804, loss 6.39958, acc 0.75, prec 0.0330062, recall 0.765583
2017-12-10T14:02:01.155826: step 805, loss 1.57149, acc 0.84375, prec 0.0329869, recall 0.765583
2017-12-10T14:02:01.972120: step 806, loss 1.68508, acc 0.8125, prec 0.0330202, recall 0.7659
2017-12-10T14:02:02.790949: step 807, loss 0.903357, acc 0.875, prec 0.0330048, recall 0.7659
2017-12-10T14:02:03.607815: step 808, loss 0.463977, acc 0.921875, prec 0.0329952, recall 0.7659
2017-12-10T14:02:04.420614: step 809, loss 1.37237, acc 0.765625, prec 0.0329664, recall 0.7659
2017-12-10T14:02:05.250662: step 810, loss 0.978864, acc 0.765625, prec 0.0329376, recall 0.7659
2017-12-10T14:02:06.100713: step 811, loss 0.999252, acc 0.828125, prec 0.0329728, recall 0.766216
2017-12-10T14:02:06.924272: step 812, loss 1.17038, acc 0.75, prec 0.0329421, recall 0.766216
2017-12-10T14:02:07.737187: step 813, loss 0.56764, acc 0.859375, prec 0.0329249, recall 0.766216
2017-12-10T14:02:08.552309: step 814, loss 0.765145, acc 0.84375, prec 0.0329619, recall 0.766532
2017-12-10T14:02:09.388960: step 815, loss 7.80599, acc 0.859375, prec 0.0329466, recall 0.765499
2017-12-10T14:02:10.207105: step 816, loss 0.802356, acc 0.875, prec 0.0329874, recall 0.765814
2017-12-10T14:02:11.048171: step 817, loss 2.70587, acc 0.859375, prec 0.0329721, recall 0.764785
2017-12-10T14:02:11.875284: step 818, loss 1.16309, acc 0.78125, prec 0.0330014, recall 0.765101
2017-12-10T14:02:12.712865: step 819, loss 1.51245, acc 0.8125, prec 0.0329785, recall 0.765101
2017-12-10T14:02:13.535044: step 820, loss 0.91174, acc 0.78125, prec 0.0329518, recall 0.765101
2017-12-10T14:02:14.333916: step 821, loss 0.809702, acc 0.875, prec 0.0329924, recall 0.765416
2017-12-10T14:02:15.160682: step 822, loss 1.17487, acc 0.8125, prec 0.0330254, recall 0.76573
2017-12-10T14:02:15.988662: step 823, loss 1.3936, acc 0.828125, prec 0.0330044, recall 0.76573
2017-12-10T14:02:16.803545: step 824, loss 1.16774, acc 0.84375, prec 0.0330412, recall 0.766043
2017-12-10T14:02:17.621581: step 825, loss 1.62534, acc 0.78125, prec 0.0330702, recall 0.766355
2017-12-10T14:02:18.424342: step 826, loss 1.0023, acc 0.8125, prec 0.0330474, recall 0.766355
2017-12-10T14:02:19.253259: step 827, loss 32.7568, acc 0.8125, prec 0.033084, recall 0.764628
2017-12-10T14:02:20.072631: step 828, loss 1.27791, acc 0.765625, prec 0.0331111, recall 0.76494
2017-12-10T14:02:20.901251: step 829, loss 1.2289, acc 0.84375, prec 0.0332031, recall 0.765563
2017-12-10T14:02:21.715696: step 830, loss 2.31028, acc 0.640625, prec 0.0332148, recall 0.765873
2017-12-10T14:02:22.532086: step 831, loss 1.67344, acc 0.78125, prec 0.0332435, recall 0.766182
2017-12-10T14:02:23.367875: step 832, loss 1.84702, acc 0.71875, prec 0.0332646, recall 0.766491
2017-12-10T14:02:24.221979: step 833, loss 2.26282, acc 0.734375, prec 0.0332876, recall 0.766798
2017-12-10T14:02:25.037227: step 834, loss 2.08674, acc 0.6875, prec 0.0332495, recall 0.766798
2017-12-10T14:02:25.840738: step 835, loss 2.17309, acc 0.65625, prec 0.0332078, recall 0.766798
2017-12-10T14:02:26.661212: step 836, loss 1.7611, acc 0.765625, prec 0.0332896, recall 0.767411
2017-12-10T14:02:27.475443: step 837, loss 1.96828, acc 0.734375, prec 0.0333675, recall 0.768021
2017-12-10T14:02:28.299502: step 838, loss 2.35982, acc 0.671875, prec 0.0333276, recall 0.768021
2017-12-10T14:02:29.127716: step 839, loss 2.42464, acc 0.71875, prec 0.0332936, recall 0.768021
2017-12-10T14:02:29.962978: step 840, loss 1.70296, acc 0.78125, prec 0.0332671, recall 0.768021
2017-12-10T14:02:30.762175: step 841, loss 1.91648, acc 0.734375, prec 0.0334543, recall 0.769231
2017-12-10T14:02:31.592620: step 842, loss 2.35417, acc 0.671875, prec 0.0334692, recall 0.769531
2017-12-10T14:02:32.418274: step 843, loss 1.16493, acc 0.828125, prec 0.0334484, recall 0.769531
2017-12-10T14:02:33.236555: step 844, loss 1.45247, acc 0.796875, prec 0.0335331, recall 0.77013
2017-12-10T14:02:34.039480: step 845, loss 0.587613, acc 0.875, prec 0.0335726, recall 0.770428
2017-12-10T14:02:34.866698: step 846, loss 0.746707, acc 0.875, prec 0.0335574, recall 0.770428
2017-12-10T14:02:35.709093: step 847, loss 0.792478, acc 0.8125, prec 0.0335893, recall 0.770725
2017-12-10T14:02:36.548784: step 848, loss 8.2598, acc 0.78125, prec 0.0335646, recall 0.769728
2017-12-10T14:02:37.382371: step 849, loss 0.657982, acc 0.859375, prec 0.033711, recall 0.770619
2017-12-10T14:02:38.222731: step 850, loss 1.47485, acc 0.8125, prec 0.0336882, recall 0.770619
2017-12-10T14:02:38.973311: step 851, loss 5.91745, acc 0.875, prec 0.033675, recall 0.769627
2017-12-10T14:02:39.701850: step 852, loss 0.860459, acc 0.84375, prec 0.0337648, recall 0.770218
2017-12-10T14:02:40.433142: step 853, loss 0.582064, acc 0.875, prec 0.0338039, recall 0.770513
2017-12-10T14:02:41.262228: step 854, loss 0.996482, acc 0.78125, prec 0.0338316, recall 0.770807
2017-12-10T14:02:42.093390: step 855, loss 0.616531, acc 0.859375, prec 0.0338688, recall 0.7711
2017-12-10T14:02:42.909345: step 856, loss 0.888251, acc 0.875, prec 0.0340705, recall 0.772265
2017-12-10T14:02:43.733799: step 857, loss 1.79436, acc 0.765625, prec 0.0340418, recall 0.772265
2017-12-10T14:02:44.542510: step 858, loss 0.838176, acc 0.828125, prec 0.034075, recall 0.772554
2017-12-10T14:02:45.364032: step 859, loss 1.36447, acc 0.8125, prec 0.0341062, recall 0.772843
2017-12-10T14:02:46.221123: step 860, loss 5.86058, acc 0.796875, prec 0.0340833, recall 0.771863
2017-12-10T14:02:47.038315: step 861, loss 1.17519, acc 0.828125, prec 0.0340623, recall 0.771863
2017-12-10T14:02:47.856738: step 862, loss 1.51912, acc 0.765625, prec 0.0341417, recall 0.77244
2017-12-10T14:02:48.671456: step 863, loss 5.14118, acc 0.8125, prec 0.0342286, recall 0.77204
2017-12-10T14:02:49.483641: step 864, loss 11.6947, acc 0.765625, prec 0.0342019, recall 0.771069
2017-12-10T14:02:50.314135: step 865, loss 1.9558, acc 0.734375, prec 0.0341695, recall 0.771069
2017-12-10T14:02:51.150324: step 866, loss 1.77357, acc 0.75, prec 0.0342466, recall 0.771644
2017-12-10T14:02:51.981795: step 867, loss 2.05749, acc 0.734375, prec 0.0342142, recall 0.771644
2017-12-10T14:02:52.816077: step 868, loss 3.22045, acc 0.578125, prec 0.0342165, recall 0.77193
2017-12-10T14:02:53.619116: step 869, loss 3.10232, acc 0.671875, prec 0.0343374, recall 0.772784
2017-12-10T14:02:54.456000: step 870, loss 2.19618, acc 0.65625, prec 0.0342955, recall 0.772784
2017-12-10T14:02:55.283632: step 871, loss 2.73498, acc 0.671875, prec 0.0343091, recall 0.773067
2017-12-10T14:02:56.098414: step 872, loss 1.71813, acc 0.71875, prec 0.0343284, recall 0.77335
2017-12-10T14:02:56.924981: step 873, loss 2.24274, acc 0.59375, prec 0.0342791, recall 0.77335
2017-12-10T14:02:57.731597: step 874, loss 1.86304, acc 0.734375, prec 0.034247, recall 0.77335
2017-12-10T14:02:58.563833: step 875, loss 2.5167, acc 0.65625, prec 0.0342586, recall 0.773632
2017-12-10T14:02:59.387152: step 876, loss 2.23286, acc 0.703125, prec 0.0342228, recall 0.773632
2017-12-10T14:03:00.186595: step 877, loss 1.78682, acc 0.75, prec 0.0341927, recall 0.773632
2017-12-10T14:03:01.024518: step 878, loss 1.2471, acc 0.796875, prec 0.0341683, recall 0.773632
2017-12-10T14:03:01.841739: step 879, loss 0.969604, acc 0.8125, prec 0.0341458, recall 0.773632
2017-12-10T14:03:02.662935: step 880, loss 1.87747, acc 0.765625, prec 0.0341707, recall 0.773913
2017-12-10T14:03:03.480321: step 881, loss 3.59597, acc 0.75, prec 0.0341955, recall 0.773234
2017-12-10T14:03:04.307986: step 882, loss 0.804314, acc 0.84375, prec 0.0342297, recall 0.773515
2017-12-10T14:03:05.133199: step 883, loss 0.874935, acc 0.875, prec 0.0342676, recall 0.773795
2017-12-10T14:03:05.941939: step 884, loss 0.862043, acc 0.859375, prec 0.0342507, recall 0.773795
2017-12-10T14:03:06.761896: step 885, loss 1.32646, acc 0.75, prec 0.0343263, recall 0.774353
2017-12-10T14:03:07.575148: step 886, loss 0.910252, acc 0.859375, prec 0.0344149, recall 0.774908
2017-12-10T14:03:08.400283: step 887, loss 0.679217, acc 0.875, prec 0.0344526, recall 0.775184
2017-12-10T14:03:09.226280: step 888, loss 1.12187, acc 0.796875, prec 0.0344809, recall 0.77546
2017-12-10T14:03:10.071225: step 889, loss 4.56728, acc 0.6875, prec 0.0344452, recall 0.77451
2017-12-10T14:03:10.890953: step 890, loss 1.01874, acc 0.84375, prec 0.034479, recall 0.774786
2017-12-10T14:03:11.702539: step 891, loss 1.23335, acc 0.8125, prec 0.0344565, recall 0.774786
2017-12-10T14:03:12.535601: step 892, loss 9.01597, acc 0.890625, prec 0.0344452, recall 0.773839
2017-12-10T14:03:13.356981: step 893, loss 5.74857, acc 0.75, prec 0.0344696, recall 0.773171
2017-12-10T14:03:14.191623: step 894, loss 1.69738, acc 0.734375, prec 0.0344378, recall 0.773171
2017-12-10T14:03:15.018917: step 895, loss 1.07244, acc 0.828125, prec 0.0344172, recall 0.773171
2017-12-10T14:03:15.841883: step 896, loss 2.92376, acc 0.703125, prec 0.0344341, recall 0.773447
2017-12-10T14:03:16.672588: step 897, loss 2.96753, acc 0.5625, prec 0.0344342, recall 0.773723
2017-12-10T14:03:17.501158: step 898, loss 2.38918, acc 0.6875, prec 0.034397, recall 0.773723
2017-12-10T14:03:18.322118: step 899, loss 2.51575, acc 0.5625, prec 0.0343971, recall 0.773998
2017-12-10T14:03:19.128191: step 900, loss 1.236, acc 0.796875, prec 0.0345293, recall 0.774818
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-900

2017-12-10T14:03:22.053526: step 901, loss 2.32587, acc 0.703125, prec 0.0345459, recall 0.775091
2017-12-10T14:03:22.883211: step 902, loss 2.73684, acc 0.625, prec 0.0345013, recall 0.775091
2017-12-10T14:03:23.731191: step 903, loss 2.71362, acc 0.734375, prec 0.0345236, recall 0.774427
2017-12-10T14:03:24.559190: step 904, loss 1.82504, acc 0.71875, prec 0.034542, recall 0.774699
2017-12-10T14:03:25.393927: step 905, loss 2.74864, acc 0.640625, prec 0.0344994, recall 0.774699
2017-12-10T14:03:26.225888: step 906, loss 5.19767, acc 0.8125, prec 0.0344791, recall 0.773767
2017-12-10T14:03:27.053278: step 907, loss 1.70924, acc 0.703125, prec 0.034444, recall 0.773767
2017-12-10T14:03:27.862530: step 908, loss 1.72713, acc 0.765625, prec 0.0344163, recall 0.773767
2017-12-10T14:03:28.676331: step 909, loss 1.95476, acc 0.75, prec 0.0343869, recall 0.773767
2017-12-10T14:03:29.509159: step 910, loss 8.92195, acc 0.78125, prec 0.0344662, recall 0.773381
2017-12-10T14:03:30.312132: step 911, loss 0.958125, acc 0.859375, prec 0.0344496, recall 0.773381
2017-12-10T14:03:31.128273: step 912, loss 2.40186, acc 0.6875, prec 0.0344128, recall 0.773381
2017-12-10T14:03:31.946822: step 913, loss 2.32572, acc 0.6875, prec 0.0344276, recall 0.773653
2017-12-10T14:03:32.770406: step 914, loss 1.74824, acc 0.796875, prec 0.0344552, recall 0.773923
2017-12-10T14:03:33.608835: step 915, loss 15.9769, acc 0.8125, prec 0.0344864, recall 0.77327
2017-12-10T14:03:34.421031: step 916, loss 1.38836, acc 0.71875, prec 0.0344534, recall 0.77327
2017-12-10T14:03:35.246251: step 917, loss 1.27101, acc 0.796875, prec 0.0344809, recall 0.77354
2017-12-10T14:03:36.077890: step 918, loss 2.06126, acc 0.734375, prec 0.0345523, recall 0.774078
2017-12-10T14:03:36.894138: step 919, loss 4.11164, acc 0.5625, prec 0.034501, recall 0.774078
2017-12-10T14:03:37.712645: step 920, loss 1.68542, acc 0.734375, prec 0.03447, recall 0.774078
2017-12-10T14:03:38.543603: step 921, loss 1.88029, acc 0.71875, prec 0.0344882, recall 0.774347
2017-12-10T14:03:39.371746: step 922, loss 1.95384, acc 0.78125, prec 0.0345137, recall 0.774614
2017-12-10T14:03:40.198359: step 923, loss 1.39214, acc 0.796875, prec 0.034694, recall 0.775679
2017-12-10T14:03:41.016866: step 924, loss 3.31109, acc 0.6875, prec 0.0346592, recall 0.774764
2017-12-10T14:03:41.826922: step 925, loss 1.68755, acc 0.75, prec 0.03463, recall 0.774764
2017-12-10T14:03:42.654254: step 926, loss 1.96561, acc 0.71875, prec 0.0345972, recall 0.774764
2017-12-10T14:03:43.483283: step 927, loss 6.14143, acc 0.734375, prec 0.034568, recall 0.773852
2017-12-10T14:03:44.310877: step 928, loss 1.10863, acc 0.75, prec 0.0345897, recall 0.774118
2017-12-10T14:03:45.121237: step 929, loss 1.10259, acc 0.796875, prec 0.0345661, recall 0.774118
2017-12-10T14:03:45.933739: step 930, loss 3.6261, acc 0.671875, prec 0.034528, recall 0.774118
2017-12-10T14:03:46.761310: step 931, loss 1.36198, acc 0.78125, prec 0.0345533, recall 0.774383
2017-12-10T14:03:47.582689: step 932, loss 3.30821, acc 0.8125, prec 0.0346345, recall 0.774005
2017-12-10T14:03:48.403387: step 933, loss 1.40812, acc 0.78125, prec 0.0346091, recall 0.774005
2017-12-10T14:03:49.219031: step 934, loss 1.91886, acc 0.765625, prec 0.0346325, recall 0.774269
2017-12-10T14:03:50.025754: step 935, loss 2.31074, acc 0.734375, prec 0.0347026, recall 0.774796
2017-12-10T14:03:50.847652: step 936, loss 1.35284, acc 0.796875, prec 0.0346791, recall 0.774796
2017-12-10T14:03:51.676153: step 937, loss 1.40579, acc 0.75, prec 0.0347508, recall 0.77532
2017-12-10T14:03:52.480420: step 938, loss 0.706053, acc 0.84375, prec 0.0347327, recall 0.77532
2017-12-10T14:03:53.306867: step 939, loss 1.0583, acc 0.796875, prec 0.0347595, recall 0.775581
2017-12-10T14:03:54.114672: step 940, loss 5.11221, acc 0.859375, prec 0.034745, recall 0.774681
2017-12-10T14:03:54.938398: step 941, loss 1.29595, acc 0.796875, prec 0.034822, recall 0.775203
2017-12-10T14:03:55.754545: step 942, loss 0.772354, acc 0.828125, prec 0.0348021, recall 0.775203
2017-12-10T14:03:56.567444: step 943, loss 0.894343, acc 0.796875, prec 0.0347785, recall 0.775203
2017-12-10T14:03:57.420669: step 944, loss 1.29674, acc 0.828125, prec 0.034859, recall 0.775723
2017-12-10T14:03:58.246164: step 945, loss 1.07838, acc 0.78125, prec 0.0349338, recall 0.77624
2017-12-10T14:03:59.075260: step 946, loss 0.698892, acc 0.796875, prec 0.0349103, recall 0.77624
2017-12-10T14:03:59.900318: step 947, loss 16.5546, acc 0.796875, prec 0.0349386, recall 0.775604
2017-12-10T14:04:00.715076: step 948, loss 11.364, acc 0.859375, prec 0.0349241, recall 0.774713
2017-12-10T14:04:01.528574: step 949, loss 1.43752, acc 0.8125, prec 0.0350023, recall 0.775229
2017-12-10T14:04:02.342500: step 950, loss 2.41766, acc 0.65625, prec 0.0349625, recall 0.775229
2017-12-10T14:04:03.164685: step 951, loss 2.06072, acc 0.6875, prec 0.0349264, recall 0.775229
2017-12-10T14:04:03.982303: step 952, loss 1.35321, acc 0.75, prec 0.0349972, recall 0.775744
2017-12-10T14:04:04.797263: step 953, loss 2.6956, acc 0.578125, prec 0.0349485, recall 0.775744
2017-12-10T14:04:05.607013: step 954, loss 7.05616, acc 0.78125, prec 0.0350742, recall 0.775626
2017-12-10T14:04:06.417977: step 955, loss 2.65072, acc 0.703125, prec 0.0350399, recall 0.775626
2017-12-10T14:04:07.244744: step 956, loss 2.994, acc 0.71875, prec 0.0350075, recall 0.775626
2017-12-10T14:04:08.076353: step 957, loss 2.48829, acc 0.65625, prec 0.0350175, recall 0.775882
2017-12-10T14:04:08.893997: step 958, loss 13.1035, acc 0.671875, prec 0.0349815, recall 0.775
2017-12-10T14:04:09.722888: step 959, loss 3.96968, acc 0.546875, prec 0.034979, recall 0.775255
2017-12-10T14:04:10.539741: step 960, loss 3.00736, acc 0.625, prec 0.0350348, recall 0.775764
2017-12-10T14:04:11.372070: step 961, loss 3.33133, acc 0.5625, prec 0.0349847, recall 0.775764
2017-12-10T14:04:12.192380: step 962, loss 4.14257, acc 0.578125, prec 0.0350349, recall 0.776271
2017-12-10T14:04:12.992397: step 963, loss 3.90111, acc 0.546875, prec 0.0350323, recall 0.776524
2017-12-10T14:04:13.814174: step 964, loss 3.79346, acc 0.515625, prec 0.0350752, recall 0.777027
2017-12-10T14:04:14.653978: step 965, loss 3.6066, acc 0.625, prec 0.0350815, recall 0.777278
2017-12-10T14:04:15.468065: step 966, loss 2.74951, acc 0.65625, prec 0.0351402, recall 0.777778
2017-12-10T14:04:16.296693: step 967, loss 2.93261, acc 0.5625, prec 0.0351881, recall 0.778275
2017-12-10T14:04:17.137795: step 968, loss 2.22682, acc 0.640625, prec 0.0351472, recall 0.778275
2017-12-10T14:04:17.951037: step 969, loss 2.45219, acc 0.671875, prec 0.0351586, recall 0.778524
2017-12-10T14:04:18.771185: step 970, loss 1.87789, acc 0.78125, prec 0.0351825, recall 0.778771
2017-12-10T14:04:19.586015: step 971, loss 2.00696, acc 0.78125, prec 0.0352063, recall 0.779018
2017-12-10T14:04:20.430694: step 972, loss 1.72032, acc 0.703125, prec 0.0351726, recall 0.779018
2017-12-10T14:04:21.270653: step 973, loss 1.01861, acc 0.90625, prec 0.0352106, recall 0.779264
2017-12-10T14:04:22.086077: step 974, loss 0.615589, acc 0.875, prec 0.035245, recall 0.77951
2017-12-10T14:04:22.899343: step 975, loss 0.823668, acc 0.8125, prec 0.0352237, recall 0.77951
2017-12-10T14:04:23.733601: step 976, loss 0.634912, acc 0.90625, prec 0.035213, recall 0.77951
2017-12-10T14:04:24.566542: step 977, loss 0.899413, acc 0.859375, prec 0.0351971, recall 0.77951
2017-12-10T14:04:25.396408: step 978, loss 0.423101, acc 0.953125, prec 0.0351918, recall 0.77951
2017-12-10T14:04:26.207987: step 979, loss 0.375052, acc 0.921875, prec 0.0352314, recall 0.779755
2017-12-10T14:04:27.032205: step 980, loss 4.15475, acc 0.921875, prec 0.0352244, recall 0.778889
2017-12-10T14:04:27.837011: step 981, loss 0.122995, acc 0.96875, prec 0.0352208, recall 0.778889
2017-12-10T14:04:28.659381: step 982, loss 0.356101, acc 0.953125, prec 0.0353124, recall 0.779379
2017-12-10T14:04:29.463184: step 983, loss 0.25292, acc 0.953125, prec 0.0353071, recall 0.779379
2017-12-10T14:04:30.294327: step 984, loss 5.40952, acc 0.90625, prec 0.0352983, recall 0.778516
2017-12-10T14:04:31.110713: step 985, loss 0.471879, acc 0.90625, prec 0.0352876, recall 0.778516
2017-12-10T14:04:31.934454: step 986, loss 0.253716, acc 0.921875, prec 0.0353272, recall 0.778761
2017-12-10T14:04:32.767725: step 987, loss 0.793259, acc 0.890625, prec 0.0353148, recall 0.778761
2017-12-10T14:04:33.574863: step 988, loss 0.229889, acc 0.9375, prec 0.0353077, recall 0.778761
2017-12-10T14:04:34.420845: step 989, loss 0.799272, acc 0.84375, prec 0.0353383, recall 0.779006
2017-12-10T14:04:35.230740: step 990, loss 0.818201, acc 0.859375, prec 0.0353707, recall 0.779249
2017-12-10T14:04:36.042308: step 991, loss 0.36639, acc 0.890625, prec 0.0354066, recall 0.779493
2017-12-10T14:04:36.864497: step 992, loss 0.354165, acc 0.890625, prec 0.0354425, recall 0.779736
2017-12-10T14:04:37.699629: step 993, loss 0.407986, acc 0.890625, prec 0.0354301, recall 0.779736
2017-12-10T14:04:38.431558: step 994, loss 0.636595, acc 0.901961, prec 0.0354213, recall 0.779736
2017-12-10T14:04:39.257981: step 995, loss 0.566723, acc 0.890625, prec 0.0354571, recall 0.779978
2017-12-10T14:04:40.093783: step 996, loss 1.08521, acc 0.890625, prec 0.0355411, recall 0.780461
2017-12-10T14:04:40.927825: step 997, loss 2.14825, acc 0.921875, prec 0.035534, recall 0.779605
2017-12-10T14:04:41.743844: step 998, loss 0.377307, acc 0.96875, prec 0.0355305, recall 0.779605
2017-12-10T14:04:42.542063: step 999, loss 0.469117, acc 0.90625, prec 0.0355198, recall 0.779605
2017-12-10T14:04:43.364586: step 1000, loss 0.69456, acc 0.859375, prec 0.0355038, recall 0.779605
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-1000

2017-12-10T14:04:46.313220: step 1001, loss 0.592017, acc 0.890625, prec 0.0355877, recall 0.780088
2017-12-10T14:04:47.129292: step 1002, loss 0.352825, acc 0.875, prec 0.0355735, recall 0.780088
2017-12-10T14:04:47.947735: step 1003, loss 0.7673, acc 0.859375, prec 0.0357018, recall 0.780807
2017-12-10T14:04:48.771235: step 1004, loss 0.812024, acc 0.8125, prec 0.0357285, recall 0.781046
2017-12-10T14:04:49.582800: step 1005, loss 0.695599, acc 0.921875, prec 0.0357677, recall 0.781284
2017-12-10T14:04:50.381535: step 1006, loss 1.23599, acc 0.828125, prec 0.0357961, recall 0.781522
2017-12-10T14:04:51.201146: step 1007, loss 1.0126, acc 0.828125, prec 0.0357765, recall 0.781522
2017-12-10T14:04:52.025585: step 1008, loss 1.07284, acc 0.84375, prec 0.0357587, recall 0.781522
2017-12-10T14:04:52.860899: step 1009, loss 3.51995, acc 0.921875, prec 0.0357516, recall 0.780673
2017-12-10T14:04:53.693057: step 1010, loss 0.698785, acc 0.875, prec 0.0357374, recall 0.780673
2017-12-10T14:04:54.503089: step 1011, loss 0.492647, acc 0.875, prec 0.0357711, recall 0.780911
2017-12-10T14:04:55.314460: step 1012, loss 19.139, acc 0.859375, prec 0.0358047, recall 0.780303
2017-12-10T14:04:56.135653: step 1013, loss 0.640142, acc 0.84375, prec 0.035787, recall 0.780303
2017-12-10T14:04:56.961192: step 1014, loss 0.949239, acc 0.8125, prec 0.0358135, recall 0.780541
2017-12-10T14:04:57.764573: step 1015, loss 0.92118, acc 0.8125, prec 0.0357922, recall 0.780541
2017-12-10T14:04:58.586915: step 1016, loss 7.60628, acc 0.828125, prec 0.0358222, recall 0.779935
2017-12-10T14:04:59.421447: step 1017, loss 4.8909, acc 0.734375, prec 0.0358893, recall 0.77957
2017-12-10T14:05:00.248864: step 1018, loss 1.22537, acc 0.828125, prec 0.0359175, recall 0.779807
2017-12-10T14:05:01.061884: step 1019, loss 2.34505, acc 0.71875, prec 0.0359808, recall 0.780279
2017-12-10T14:05:01.894328: step 1020, loss 1.67889, acc 0.734375, prec 0.0359982, recall 0.780514
2017-12-10T14:05:02.718939: step 1021, loss 1.9436, acc 0.703125, prec 0.0360596, recall 0.780983
2017-12-10T14:05:03.534026: step 1022, loss 2.77509, acc 0.734375, prec 0.0360769, recall 0.781217
2017-12-10T14:05:04.351223: step 1023, loss 2.72022, acc 0.609375, prec 0.0361274, recall 0.781683
2017-12-10T14:05:05.167076: step 1024, loss 1.72404, acc 0.640625, prec 0.0360865, recall 0.781683
2017-12-10T14:05:05.976063: step 1025, loss 2.36766, acc 0.65625, prec 0.0360949, recall 0.781915
2017-12-10T14:05:06.793269: step 1026, loss 3.77355, acc 0.59375, prec 0.0361452, recall 0.781548
2017-12-10T14:05:07.622945: step 1027, loss 2.17361, acc 0.640625, prec 0.0361517, recall 0.78178
2017-12-10T14:05:08.446646: step 1028, loss 1.73091, acc 0.671875, prec 0.0362088, recall 0.782241
2017-12-10T14:05:09.262331: step 1029, loss 5.68634, acc 0.6875, prec 0.0362694, recall 0.781876
2017-12-10T14:05:10.076145: step 1030, loss 2.28163, acc 0.671875, prec 0.0362322, recall 0.781876
2017-12-10T14:05:10.898590: step 1031, loss 2.47881, acc 0.640625, prec 0.0361916, recall 0.781876
2017-12-10T14:05:11.731246: step 1032, loss 1.87038, acc 0.734375, prec 0.0361616, recall 0.781876
2017-12-10T14:05:12.558273: step 1033, loss 1.81819, acc 0.71875, prec 0.0362238, recall 0.782334
2017-12-10T14:05:13.386877: step 1034, loss 1.51857, acc 0.71875, prec 0.0362389, recall 0.782563
2017-12-10T14:05:14.221493: step 1035, loss 8.68389, acc 0.75, prec 0.0362125, recall 0.781742
2017-12-10T14:05:15.057516: step 1036, loss 1.6514, acc 0.703125, prec 0.0361791, recall 0.781742
2017-12-10T14:05:15.887867: step 1037, loss 2.21113, acc 0.65625, prec 0.0361872, recall 0.781971
2017-12-10T14:05:16.706797: step 1038, loss 0.832417, acc 0.796875, prec 0.0361644, recall 0.781971
2017-12-10T14:05:17.517354: step 1039, loss 1.19797, acc 0.765625, prec 0.0361848, recall 0.782199
2017-12-10T14:05:18.328982: step 1040, loss 1.45713, acc 0.71875, prec 0.0362, recall 0.782427
2017-12-10T14:05:19.163948: step 1041, loss 1.75548, acc 0.75, prec 0.0362652, recall 0.782881
2017-12-10T14:05:19.997372: step 1042, loss 2.08511, acc 0.703125, prec 0.036325, recall 0.783333
2017-12-10T14:05:20.802077: step 1043, loss 1.05122, acc 0.765625, prec 0.0362987, recall 0.783333
2017-12-10T14:05:21.630556: step 1044, loss 1.73492, acc 0.734375, prec 0.0362689, recall 0.783333
2017-12-10T14:05:22.458636: step 1045, loss 0.996691, acc 0.8125, prec 0.0362944, recall 0.783559
2017-12-10T14:05:23.278106: step 1046, loss 1.00938, acc 0.84375, prec 0.0363698, recall 0.784008
2017-12-10T14:05:24.092902: step 1047, loss 1.46314, acc 0.796875, prec 0.036347, recall 0.784008
2017-12-10T14:05:24.923676: step 1048, loss 1.41288, acc 0.828125, prec 0.0363741, recall 0.784232
2017-12-10T14:05:25.751521: step 1049, loss 0.694178, acc 0.859375, prec 0.0364047, recall 0.784456
2017-12-10T14:05:26.576439: step 1050, loss 0.794604, acc 0.875, prec 0.0363907, recall 0.784456
2017-12-10T14:05:27.388694: step 1051, loss 0.500031, acc 0.875, prec 0.036423, recall 0.784679
2017-12-10T14:05:28.218995: step 1052, loss 0.325069, acc 0.9375, prec 0.036416, recall 0.784679
2017-12-10T14:05:29.041995: step 1053, loss 0.673771, acc 0.890625, prec 0.0364501, recall 0.784902
2017-12-10T14:05:29.864393: step 1054, loss 1.05692, acc 0.84375, prec 0.0364788, recall 0.785124
2017-12-10T14:05:30.693066: step 1055, loss 0.0384065, acc 0.984375, prec 0.0364771, recall 0.785124
2017-12-10T14:05:31.513577: step 1056, loss 0.408666, acc 0.890625, prec 0.0364648, recall 0.785124
2017-12-10T14:05:32.340453: step 1057, loss 7.99966, acc 0.96875, prec 0.0365093, recall 0.784536
2017-12-10T14:05:33.181660: step 1058, loss 0.677858, acc 0.90625, prec 0.036545, recall 0.784758
2017-12-10T14:05:34.007504: step 1059, loss 0.242287, acc 0.953125, prec 0.0365398, recall 0.784758
2017-12-10T14:05:34.851294: step 1060, loss 0.329264, acc 0.921875, prec 0.0365772, recall 0.784979
2017-12-10T14:05:35.677480: step 1061, loss 0.421055, acc 0.921875, prec 0.0365684, recall 0.784979
2017-12-10T14:05:36.496963: step 1062, loss 0.103033, acc 0.953125, prec 0.0366093, recall 0.7852
2017-12-10T14:05:37.329589: step 1063, loss 0.498517, acc 0.890625, prec 0.0365971, recall 0.7852
2017-12-10T14:05:38.156496: step 1064, loss 0.480569, acc 0.9375, prec 0.03659, recall 0.7852
2017-12-10T14:05:38.969481: step 1065, loss 0.289914, acc 0.96875, prec 0.0366327, recall 0.785421
2017-12-10T14:05:39.803774: step 1066, loss 13.7004, acc 0.890625, prec 0.03667, recall 0.784033
2017-12-10T14:05:40.597357: step 1067, loss 0.408283, acc 0.921875, prec 0.0366612, recall 0.784033
2017-12-10T14:05:41.426132: step 1068, loss 0.31965, acc 0.875, prec 0.0366472, recall 0.784033
2017-12-10T14:05:42.253546: step 1069, loss 0.495571, acc 0.90625, prec 0.0366828, recall 0.784254
2017-12-10T14:05:43.062878: step 1070, loss 0.490584, acc 0.9375, prec 0.0366758, recall 0.784254
2017-12-10T14:05:43.896088: step 1071, loss 1.40105, acc 0.71875, prec 0.0366902, recall 0.784474
2017-12-10T14:05:44.716298: step 1072, loss 1.45547, acc 0.796875, prec 0.0367594, recall 0.784913
2017-12-10T14:05:45.531006: step 1073, loss 2.07439, acc 0.71875, prec 0.0367279, recall 0.784913
2017-12-10T14:05:46.380441: step 1074, loss 1.72153, acc 0.859375, prec 0.036804, recall 0.785351
2017-12-10T14:05:47.200617: step 1075, loss 0.394025, acc 0.890625, prec 0.0369294, recall 0.786004
2017-12-10T14:05:48.046564: step 1076, loss 1.29847, acc 0.78125, prec 0.0369965, recall 0.786437
2017-12-10T14:05:48.848327: step 1077, loss 1.9092, acc 0.765625, prec 0.0370159, recall 0.786653
2017-12-10T14:05:49.676129: step 1078, loss 1.14378, acc 0.8125, prec 0.0369948, recall 0.786653
2017-12-10T14:05:50.509139: step 1079, loss 1.43227, acc 0.8125, prec 0.0370194, recall 0.786869
2017-12-10T14:05:51.318203: step 1080, loss 1.85396, acc 0.71875, prec 0.0370335, recall 0.787084
2017-12-10T14:05:52.143833: step 1081, loss 4.17569, acc 0.796875, prec 0.0370124, recall 0.78629
2017-12-10T14:05:52.966701: step 1082, loss 1.00307, acc 0.84375, prec 0.0370862, recall 0.78672
2017-12-10T14:05:53.796861: step 1083, loss 1.20237, acc 0.78125, prec 0.0370616, recall 0.78672
2017-12-10T14:05:54.615406: step 1084, loss 1.02818, acc 0.765625, prec 0.0370809, recall 0.786935
2017-12-10T14:05:55.437418: step 1085, loss 0.733259, acc 0.796875, prec 0.0371036, recall 0.787149
2017-12-10T14:05:56.259252: step 1086, loss 1.16889, acc 0.8125, prec 0.0371281, recall 0.787362
2017-12-10T14:05:57.106401: step 1087, loss 1.22368, acc 0.84375, prec 0.0371106, recall 0.787362
2017-12-10T14:05:57.915758: step 1088, loss 2.17369, acc 0.921875, prec 0.0371491, recall 0.786787
2017-12-10T14:05:58.735390: step 1089, loss 4.37617, acc 0.859375, prec 0.037135, recall 0.786
2017-12-10T14:05:59.556273: step 1090, loss 0.467763, acc 0.90625, prec 0.03717, recall 0.786214
2017-12-10T14:06:00.385354: step 1091, loss 19.3018, acc 0.78125, prec 0.0371472, recall 0.785429
2017-12-10T14:06:01.207186: step 1092, loss 1.94394, acc 0.75, prec 0.03721, recall 0.785857
2017-12-10T14:06:02.029116: step 1093, loss 1.00195, acc 0.796875, prec 0.0371872, recall 0.785857
2017-12-10T14:06:02.846923: step 1094, loss 1.00707, acc 0.78125, prec 0.037208, recall 0.78607
2017-12-10T14:06:03.664919: step 1095, loss 26.9431, acc 0.703125, prec 0.0371765, recall 0.785288
2017-12-10T14:06:04.495698: step 1096, loss 1.78688, acc 0.703125, prec 0.0371885, recall 0.785501
2017-12-10T14:06:05.326156: step 1097, loss 2.05604, acc 0.671875, prec 0.0371971, recall 0.785714
2017-12-10T14:06:06.150335: step 1098, loss 1.75961, acc 0.703125, prec 0.0371639, recall 0.785714
2017-12-10T14:06:06.991231: step 1099, loss 2.09702, acc 0.71875, prec 0.0371777, recall 0.785927
2017-12-10T14:06:07.807705: step 1100, loss 2.92512, acc 0.65625, prec 0.0372296, recall 0.78635
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/embedding_size_512_fold_0/1512931839/checkpoints/model-1100

2017-12-10T14:06:11.136056: step 1101, loss 1.99093, acc 0.671875, prec 0.037193, recall 0.78635
2017-12-10T14:06:11.954968: step 1102, loss 1.95611, acc 0.671875, prec 0.0371565, recall 0.78635
2017-12-10T14:06:12.778267: step 1103, loss 1.92728, acc 0.703125, prec 0.0371235, recall 0.78635
2017-12-10T14:06:13.604240: step 1104, loss 2.68211, acc 0.6875, prec 0.0370889, recall 0.78635
2017-12-10T14:06:14.428620: step 1105, loss 1.85924, acc 0.65625, prec 0.0370508, recall 0.78635
2017-12-10T14:06:15.267055: step 1106, loss 2.49383, acc 0.671875, prec 0.0370146, recall 0.78635
2017-12-10T14:06:16.088045: step 1107, loss 1.36843, acc 0.8125, prec 0.036994, recall 0.78635
2017-12-10T14:06:16.910951: step 1108, loss 0.881753, acc 0.828125, prec 0.0370198, recall 0.786561
2017-12-10T14:06:17.742972: step 1109, loss 0.979235, acc 0.890625, prec 0.0370973, recall 0.786982
2017-12-10T14:06:18.559108: step 1110, loss 0.441191, acc 0.875, prec 0.0370835, recall 0.786982
2017-12-10T14:06:19.380572: step 1111, loss 10.3553, acc 0.875, prec 0.0371162, recall 0.786417
2017-12-10T14:06:20.206313: step 1112, loss 1.29448, acc 0.8125, prec 0.0370955, recall 0.786417
2017-12-10T14:06:21.045310: step 1113, loss 0.864772, acc 0.765625, prec 0.0371144, recall 0.786627
2017-12-10T14:06:21.867317: step 1114, loss 0.841025, acc 0.859375, prec 0.0370989, recall 0.786627
2017-12-10T14:06:22.693544: step 1115, loss 0.778775, acc 0.84375, prec 0.0371709, recall 0.787046
2017-12-10T14:06:23.515545: step 1116, loss 0.646372, acc 0.859375, prec 0.0371554, recall 0.787046