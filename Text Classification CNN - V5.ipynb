{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from BasicTextCNN import BasicTextCNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)\n",
    "\n",
    "def compute_distance_embedding(word, x):\n",
    "    word_distances = np.zeros(x.shape, dtype='int')\n",
    "    for i in range(x.shape[0]):\n",
    "        word_positions = np.where(x[i] == word)[0]\n",
    "        for j in range(x.shape[1]):\n",
    "            if len(word_positions) > 0:\n",
    "                word_position = word_positions[np.argmin(np.abs(word_positions - j))]\n",
    "                word_distances[i][j] = word_position - j\n",
    "                if word_distances[i][j]<0:\n",
    "                    word_distances[i][j] = 600+word_distances[i][j]\n",
    "            else:\n",
    "                word_distances[i][j] = 299\n",
    "    return word_distances\n",
    "\n",
    "def load_position_vector_mapping():\n",
    "    # bit_array generated with the distance between \n",
    "    # two entities where abs_num represents the distance\n",
    "    def int2bit_by_distance(int_num, bit_len=10):\n",
    "\n",
    "        bit_array = np.zeros(bit_len)\n",
    "        if int_num > 0:\n",
    "            bit_array[0] = 1\n",
    "\n",
    "        abs_num = np.abs(int_num)\n",
    "        if abs_num <= 5:\n",
    "            for i in range(abs_num):\n",
    "                bit_array[-i-1] = 1\n",
    "        elif abs_num <= 10:\n",
    "            for i in range(6):\n",
    "                bit_array[-i-1] = 1\n",
    "        elif abs_num <= 20:\n",
    "            for i in range(7):\n",
    "                bit_array[-i-1] = 1\n",
    "        elif abs_num <= 30:\n",
    "            for i in range(8):\n",
    "                bit_array[-i-1] = 1\n",
    "        else:\n",
    "            for i in range(9):\n",
    "                bit_array[-i-1] = 1\n",
    "        return bit_array\n",
    "    \n",
    "    map = {}\n",
    "    for i in range(-300, 300):\n",
    "        map[i] = int2bit_by_distance(i, 10)\n",
    "        \n",
    "    return pd.DataFrame.from_dict(map, orient='index', dtype='int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "position_vector_mapping = load_position_vector_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionTextCNN(object):\n",
    "    \"\"\"\n",
    "    A Basic CNN for text classification with Position features as well.\n",
    "    \n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \n",
    "    Refer tohttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5181565/pdf/btw486.pdf for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, vocab_processor, \n",
    "                 num_classes=2, embedding_size=128, filter_sizes=[3,4,5], \n",
    "                 num_filters=128, batch_size=64, \n",
    "                 l2_reg_lambda=0.0, num_epochs=200,\n",
    "                 num_checkpoints=5, dropout_prob=0.5, \n",
    "                 checkpoint_every=100, evaluate_every=100, \n",
    "                 allow_soft_placement=True,log_device_placement=False,\n",
    "                 results_dir=\"runs\"):\n",
    "        \n",
    "        tf.reset_default_graph() \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = len(vocab_processor.vocabulary_)\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.num_epochs = num_epochs\n",
    "        self.results_dir = results_dir\n",
    "        \n",
    "        self.vocab_processor = vocab_processor\n",
    "        \n",
    "        self.num_checkpoints = num_checkpoints\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.checkpoint_every = checkpoint_every\n",
    "        self.evaluate_every = evaluate_every\n",
    "        \n",
    "        self.position_vector_mapping = PositionTextCNN.load_position_vector_mapping()\n",
    "        \n",
    "        self.allow_soft_placement = allow_soft_placement\n",
    "        self.log_device_placement = log_device_placement\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        self.word_distancesA = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"train_word_distancesA\")\n",
    "        self.word_distancesB = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"train_word_distancesB\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"word_embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),name=\"W\") \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        # Position Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"position_embedding\"):\n",
    "            embedded_positionsA = tf.nn.embedding_lookup(self.position_vector_mapping, self.word_distancesA)\n",
    "            embedded_positionsB = tf.nn.embedding_lookup(self.position_vector_mapping, self.word_distancesB)\n",
    "            embedded_positions = tf.concat([embedded_positionsA, embedded_positionsB], 2)\n",
    "            self.embedded_positions_expanded = tf.cast(tf.expand_dims(embedded_positions, -1), tf.float32)\n",
    "            \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.final_embedded_expanded = tf.concat([self.embedded_chars_expanded, self.embedded_positions_expanded], 2)\n",
    "        \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, self.embedding_size+20, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.final_embedded_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, self.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "    def train_network(self, x_train, y_train, x_dev, y_dev,\n",
    "                     train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB):\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            class_weight = tf.constant([1.0, 100.0])\n",
    "            weights = tf.reduce_sum(class_weight * self.input_y, axis=1)\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            weighted_losses = losses * weights\n",
    "            self.loss = tf.reduce_mean(weighted_losses) + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            _, self.precision = tf.metrics.precision(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='precision')\n",
    "            _, self.recall = tf.metrics.recall(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='recall')\n",
    "            \n",
    "        # Define Training procedure\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, self.results_dir, timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        precision_summary = tf.summary.scalar(\"precision\", self.precision)\n",
    "        recall_summary = tf.summary.scalar(\"recall\", self.recall)\n",
    "\n",
    "        # Train Summaries\n",
    "        self.train_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        self.train_summary_writer = tf.summary.FileWriter(train_summary_dir, self.sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        self.dev_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        self.dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, self.sess.graph)\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=self.num_checkpoints)\n",
    "        \n",
    "        # Write vocabulary\n",
    "        self.vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        # Initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "        print(\"Start training\")\n",
    "        # Generate batches\n",
    "        batches = BasicTextCNN.batch_iter(\n",
    "            list(zip(x_train, y_train, train_word_distancesA, train_word_distancesB)), self.batch_size, self.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, batch_word_distancesA, batch_word_distancesB = zip(*batch)\n",
    "            self.train_step(x_batch, y_batch, batch_word_distancesA, batch_word_distancesB)\n",
    "            current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "            if current_step % self.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                self.dev_step(x_dev, y_dev, test_word_distancesA, test_word_distancesB, writer=self.dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % self.checkpoint_every == 0:\n",
    "                path = saver.save(self.sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))   \n",
    "        print(\"Training finished\")\n",
    "    \n",
    "    def train_step(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.input_x: x_batch,\n",
    "            self.input_y: y_batch,\n",
    "            self.dropout_keep_prob: self.dropout_prob,\n",
    "            self.word_distancesA: batch_word_distancesA,\n",
    "            self.word_distancesB: batch_word_distancesB,\n",
    "        }\n",
    "        _, step, summaries, loss, accuracy, precision, recall = self.sess.run(\n",
    "            [self.train_op, self.global_step, self.train_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "        self.train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "    \n",
    "    def dev_step(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              self.input_x: x_batch,\n",
    "              self.input_y: y_batch,\n",
    "              self.dropout_keep_prob: 1.0,\n",
    "              self.word_distancesA: batch_word_distancesA,\n",
    "              self.word_distancesB: batch_word_distancesB,\n",
    "            }\n",
    "            step, summaries, loss, accuracy,  precision, recall  = self.sess.run(\n",
    "                [self.global_step, self.dev_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "    @staticmethod            \n",
    "    def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates a batch iterator for a dataset.\n",
    "        \"\"\"\n",
    "        data = np.array(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]\n",
    "    \n",
    "    @staticmethod \n",
    "    def load_position_vector_mapping():\n",
    "        # bit_array generated with the distance between \n",
    "        # two entities where abs_num represents the distance\n",
    "        def int2bit_by_distance(int_num, bit_len=10):\n",
    "\n",
    "            bit_array = np.zeros(bit_len)\n",
    "            if int_num > 0:\n",
    "                bit_array[0] = 1\n",
    "\n",
    "            abs_num = np.abs(int_num)\n",
    "            if abs_num <= 5:\n",
    "                for i in range(abs_num):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 10:\n",
    "                for i in range(6):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 20:\n",
    "                for i in range(7):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 30:\n",
    "                for i in range(8):\n",
    "                    bit_array[-i-1] = 1\n",
    "            else:\n",
    "                for i in range(9):\n",
    "                    bit_array[-i-1] = 1\n",
    "            return bit_array\n",
    "\n",
    "        map = {}\n",
    "        for i in range(-300, 300):\n",
    "            map[i] = int2bit_by_distance(i, 10)\n",
    "\n",
    "        return pd.DataFrame.from_dict(map, orient='index', dtype='int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 53)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedPathwayA, encodedPathwayB = list(vocab_processor.transform(['pathwayA pathwayB']))[0][:2]\n",
    "encodedPathwayA, encodedPathwayB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_distancesA = compute_distance_embedding(encodedPathwayA, x)\n",
    "word_distancesB = compute_distance_embedding(encodedPathwayB, x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 => Train/Dev split: 31795/10599\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/hist is illegal; using word_embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/sparsity is illegal; using word_embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512752358\n",
      "\n",
      "Start training\n",
      "2017-12-08T11:59:21.694011: step 1, loss 14.0965, acc 0.953125, prec 0, recall 0\n",
      "2017-12-08T11:59:22.367695: step 2, loss 0.249249, acc 0.953125, prec 0, recall 0\n",
      "2017-12-08T11:59:23.022178: step 3, loss 11.8205, acc 0.90625, prec 0, recall 0\n",
      "2017-12-08T11:59:23.666304: step 4, loss 0.210571, acc 0.9375, prec 0.0666667, recall 0.333333\n",
      "2017-12-08T11:59:24.329714: step 5, loss 0.179762, acc 0.9375, prec 0.0526316, recall 0.333333\n",
      "2017-12-08T11:59:25.033914: step 6, loss 38.9086, acc 0.953125, prec 0.05, recall 0.2\n",
      "2017-12-08T11:59:25.741024: step 7, loss 20.3371, acc 0.796875, prec 0.03125, recall 0.166667\n",
      "2017-12-08T11:59:26.661339: step 8, loss 2.37682, acc 0.84375, prec 0.0243902, recall 0.142857\n",
      "2017-12-08T11:59:27.383852: step 9, loss 11.4612, acc 0.671875, prec 0.0166667, recall 0.111111\n",
      "2017-12-08T11:59:28.093022: step 10, loss 2.08738, acc 0.546875, prec 0.011236, recall 0.111111\n",
      "2017-12-08T11:59:28.919566: step 11, loss 2.6296, acc 0.390625, prec 0.0078125, recall 0.111111\n",
      "2017-12-08T11:59:29.577966: step 12, loss 8.00415, acc 0.265625, prec 0.00574713, recall 0.1\n",
      "2017-12-08T11:59:30.314448: step 13, loss 6.01265, acc 0.234375, prec 0.00448431, recall 0.1\n",
      "2017-12-08T11:59:31.030740: step 14, loss 5.82696, acc 0.171875, prec 0.00362319, recall 0.1\n",
      "2017-12-08T11:59:31.760696: step 15, loss 6.54811, acc 0.15625, prec 0.0060423, recall 0.181818\n",
      "2017-12-08T11:59:32.497748: step 16, loss 20.2087, acc 0.0625, prec 0.00512821, recall 0.166667\n",
      "2017-12-08T11:59:33.190567: step 17, loss 7.62885, acc 0.109375, prec 0.0111111, recall 0.333333\n",
      "2017-12-08T11:59:33.867402: step 18, loss 7.86199, acc 0.046875, prec 0.0136452, recall 0.411765\n",
      "2017-12-08T11:59:34.560531: step 19, loss 8.0092, acc 0.0625, prec 0.0122164, recall 0.411765\n",
      "2017-12-08T11:59:35.256920: step 20, loss 7.75544, acc 0.09375, prec 0.0110935, recall 0.411765\n",
      "2017-12-08T11:59:35.941690: step 21, loss 6.32895, acc 0.140625, prec 0.0102041, recall 0.411765\n",
      "2017-12-08T11:59:37.842367: step 22, loss 16.559, acc 0.109375, prec 0.00944669, recall 0.368421\n",
      "2017-12-08T11:59:40.027992: step 23, loss 5.88679, acc 0.15625, prec 0.0100503, recall 0.4\n",
      "2017-12-08T11:59:40.873473: step 24, loss 5.64021, acc 0.140625, prec 0.0094007, recall 0.4\n",
      "2017-12-08T11:59:41.514143: step 25, loss 5.16254, acc 0.203125, prec 0.00996678, recall 0.428571\n",
      "2017-12-08T11:59:42.174325: step 26, loss 3.41641, acc 0.375, prec 0.0116402, recall 0.478261\n",
      "2017-12-08T11:59:42.851784: step 27, loss 4.9482, acc 0.5, prec 0.0112705, recall 0.458333\n",
      "2017-12-08T11:59:43.531717: step 28, loss 3.04839, acc 0.40625, prec 0.0108481, recall 0.458333\n",
      "2017-12-08T11:59:44.241161: step 29, loss 2.00848, acc 0.5, prec 0.0124046, recall 0.5\n",
      "2017-12-08T11:59:44.926194: step 30, loss 1.94096, acc 0.515625, prec 0.012963, recall 0.518519\n",
      "2017-12-08T11:59:45.596023: step 31, loss 2.21388, acc 0.515625, prec 0.0126013, recall 0.518519\n",
      "2017-12-08T11:59:46.292901: step 32, loss 0.83704, acc 0.734375, prec 0.0124113, recall 0.518519\n",
      "2017-12-08T11:59:46.960716: step 33, loss 0.776896, acc 0.78125, prec 0.0131234, recall 0.535714\n",
      "2017-12-08T11:59:48.299265: step 34, loss 0.745708, acc 0.8125, prec 0.012987, recall 0.535714\n",
      "2017-12-08T11:59:50.477381: step 35, loss 0.53979, acc 0.84375, prec 0.0145673, recall 0.566667\n",
      "2017-12-08T11:59:51.932841: step 36, loss 7.60981, acc 0.921875, prec 0.0145175, recall 0.548387\n",
      "2017-12-08T11:59:52.570173: step 37, loss 12.0038, acc 0.84375, prec 0.014419, recall 0.515152\n",
      "2017-12-08T11:59:53.261458: step 38, loss 15.6216, acc 0.828125, prec 0.0143218, recall 0.472222\n",
      "2017-12-08T11:59:53.942066: step 39, loss 10.7299, acc 0.796875, prec 0.015, recall 0.473684\n",
      "2017-12-08T11:59:54.669249: step 40, loss 7.25374, acc 0.734375, prec 0.0156122, recall 0.475\n",
      "2017-12-08T11:59:55.342978: step 41, loss 7.03668, acc 0.625, prec 0.0153226, recall 0.463415\n",
      "2017-12-08T11:59:56.029922: step 42, loss 2.7574, acc 0.421875, prec 0.0156495, recall 0.47619\n",
      "2017-12-08T11:59:57.121183: step 43, loss 3.6348, acc 0.34375, prec 0.015897, recall 0.488372\n",
      "2017-12-08T11:59:59.307854: step 44, loss 4.43952, acc 0.328125, prec 0.0168375, recall 0.511111\n",
      "2017-12-08T12:00:01.029245: step 45, loss 3.97273, acc 0.296875, prec 0.0176929, recall 0.531915\n",
      "2017-12-08T12:00:01.661966: step 46, loss 5.28718, acc 0.21875, prec 0.0177596, recall 0.541667\n",
      "2017-12-08T12:00:02.341259: step 47, loss 5.30174, acc 0.171875, prec 0.0171391, recall 0.541667\n",
      "2017-12-08T12:00:03.001681: step 48, loss 5.96061, acc 0.203125, prec 0.0172084, recall 0.55102\n",
      "2017-12-08T12:00:03.676822: step 49, loss 5.10357, acc 0.125, prec 0.0166154, recall 0.55102\n",
      "2017-12-08T12:00:04.360813: step 50, loss 4.81938, acc 0.203125, prec 0.0161098, recall 0.55102\n",
      "2017-12-08T12:00:05.029198: step 51, loss 5.86073, acc 0.203125, prec 0.015634, recall 0.55102\n",
      "2017-12-08T12:00:05.708836: step 52, loss 4.56635, acc 0.21875, prec 0.0163013, recall 0.568627\n",
      "2017-12-08T12:00:07.895886: step 53, loss 3.16213, acc 0.328125, prec 0.0159166, recall 0.568627\n",
      "2017-12-08T12:00:10.095790: step 54, loss 3.30017, acc 0.359375, prec 0.0155663, recall 0.568627\n",
      "2017-12-08T12:00:10.755295: step 55, loss 5.2698, acc 0.328125, prec 0.0157398, recall 0.566038\n",
      "2017-12-08T12:00:11.438640: step 56, loss 2.01312, acc 0.578125, prec 0.0155199, recall 0.566038\n",
      "2017-12-08T12:00:12.132251: step 57, loss 1.29045, acc 0.59375, prec 0.0158163, recall 0.574074\n",
      "2017-12-08T12:00:12.833018: step 58, loss 1.62479, acc 0.703125, prec 0.0161616, recall 0.581818\n",
      "2017-12-08T12:00:13.549476: step 59, loss 1.37728, acc 0.71875, prec 0.016016, recall 0.581818\n",
      "2017-12-08T12:00:14.245684: step 60, loss 13.0834, acc 0.671875, prec 0.0163447, recall 0.578947\n",
      "2017-12-08T12:00:14.933399: step 61, loss 0.970621, acc 0.8125, prec 0.0172159, recall 0.59322\n",
      "2017-12-08T12:00:15.627302: step 62, loss 1.20314, acc 0.71875, prec 0.0175439, recall 0.6\n",
      "2017-12-08T12:00:16.698530: step 63, loss 19.3947, acc 0.703125, prec 0.0178744, recall 0.587302\n",
      "2017-12-08T12:00:18.992466: step 64, loss 0.476889, acc 0.875, prec 0.0178056, recall 0.587302\n",
      "2017-12-08T12:00:21.195451: step 65, loss 0.550669, acc 0.84375, prec 0.0177203, recall 0.587302\n",
      "2017-12-08T12:00:21.902175: step 66, loss 14.1899, acc 0.625, prec 0.0175272, recall 0.578125\n",
      "2017-12-08T12:00:22.598992: step 67, loss 1.36378, acc 0.671875, prec 0.0173546, recall 0.578125\n",
      "2017-12-08T12:00:23.289186: step 68, loss 15.8813, acc 0.640625, prec 0.0171773, recall 0.569231\n",
      "2017-12-08T12:00:24.015220: step 69, loss 1.94675, acc 0.625, prec 0.0169881, recall 0.569231\n",
      "2017-12-08T12:00:24.761439: step 70, loss 1.70266, acc 0.59375, prec 0.0167877, recall 0.569231\n",
      "2017-12-08T12:00:25.432625: step 71, loss 4.53662, acc 0.5, prec 0.0165548, recall 0.560606\n",
      "2017-12-08T12:00:26.107956: step 72, loss 4.7281, acc 0.46875, prec 0.0163139, recall 0.552239\n",
      "2017-12-08T12:00:27.851938: step 73, loss 5.01844, acc 0.40625, prec 0.0160521, recall 0.544118\n",
      "2017-12-08T12:00:30.368232: step 74, loss 3.00343, acc 0.40625, prec 0.0157917, recall 0.544118\n",
      "2017-12-08T12:00:31.135473: step 75, loss 2.33388, acc 0.40625, prec 0.015953, recall 0.550725\n",
      "2017-12-08T12:00:31.811996: step 76, loss 2.80006, acc 0.4375, prec 0.0157155, recall 0.550725\n",
      "2017-12-08T12:00:32.508364: step 77, loss 6.19709, acc 0.46875, prec 0.0155039, recall 0.542857\n",
      "2017-12-08T12:00:33.200262: step 78, loss 9.77823, acc 0.453125, prec 0.0156879, recall 0.541667\n",
      "2017-12-08T12:00:33.894651: step 79, loss 6.89149, acc 0.28125, prec 0.0157978, recall 0.540541\n",
      "2017-12-08T12:00:34.588305: step 80, loss 3.40648, acc 0.453125, prec 0.0155824, recall 0.540541\n",
      "2017-12-08T12:00:36.975725: step 81, loss 4.64698, acc 0.265625, prec 0.0153022, recall 0.540541\n",
      "2017-12-08T12:00:38.924035: step 82, loss 4.48592, acc 0.265625, prec 0.0161411, recall 0.558442\n",
      "2017-12-08T12:00:39.574918: step 83, loss 3.5877, acc 0.296875, prec 0.0162362, recall 0.564103\n",
      "2017-12-08T12:00:40.292332: step 84, loss 3.50551, acc 0.328125, prec 0.0163399, recall 0.56962\n",
      "2017-12-08T12:00:40.995301: step 85, loss 3.33497, acc 0.34375, prec 0.0160944, recall 0.56962\n",
      "2017-12-08T12:00:41.717247: step 86, loss 2.97371, acc 0.453125, prec 0.0162429, recall 0.575\n",
      "2017-12-08T12:00:42.435173: step 87, loss 4.0788, acc 0.34375, prec 0.0160056, recall 0.575\n",
      "2017-12-08T12:00:43.921844: step 88, loss 2.34566, acc 0.546875, prec 0.0165232, recall 0.585366\n",
      "2017-12-08T12:00:46.314211: step 89, loss 2.62692, acc 0.640625, prec 0.0163934, recall 0.585366\n",
      "2017-12-08T12:00:48.195037: step 90, loss 2.26573, acc 0.640625, prec 0.0162657, recall 0.585366\n",
      "2017-12-08T12:00:48.874602: step 91, loss 14.5445, acc 0.59375, prec 0.0161345, recall 0.571429\n",
      "2017-12-08T12:00:50.856095: step 92, loss 2.0014, acc 0.5625, prec 0.015984, recall 0.571429\n",
      "2017-12-08T12:00:53.164167: step 93, loss 2.28079, acc 0.546875, prec 0.0158311, recall 0.571429\n",
      "2017-12-08T12:00:54.503349: step 94, loss 1.57374, acc 0.71875, prec 0.0157377, recall 0.571429\n",
      "2017-12-08T12:00:55.158973: step 95, loss 2.09668, acc 0.515625, prec 0.0158988, recall 0.576471\n",
      "2017-12-08T12:00:55.870439: step 96, loss 6.60774, acc 0.640625, prec 0.0161031, recall 0.574713\n",
      "2017-12-08T12:00:56.585605: step 97, loss 1.22691, acc 0.703125, prec 0.01632, recall 0.579545\n",
      "2017-12-08T12:00:57.355372: step 98, loss 1.77551, acc 0.578125, prec 0.0164922, recall 0.58427\n",
      "2017-12-08T12:00:58.079649: step 99, loss 1.22624, acc 0.65625, prec 0.016378, recall 0.58427\n",
      "2017-12-08T12:00:58.842820: step 100, loss 1.5129, acc 0.734375, prec 0.0169067, recall 0.593407\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512752358/checkpoints/model-100\n",
      "\n",
      "2017-12-08T12:01:00.414761: step 101, loss 1.33448, acc 0.6875, prec 0.0168015, recall 0.593407\n",
      "2017-12-08T12:01:01.142455: step 102, loss 13.0338, acc 0.734375, prec 0.0170226, recall 0.591398\n",
      "2017-12-08T12:01:01.855694: step 103, loss 1.37896, acc 0.71875, prec 0.0169283, recall 0.591398\n",
      "2017-12-08T12:01:03.298906: step 104, loss 0.937745, acc 0.84375, prec 0.0168763, recall 0.591398\n",
      "2017-12-08T12:01:05.529582: step 105, loss 13.3246, acc 0.71875, prec 0.0167888, recall 0.585106\n",
      "2017-12-08T12:01:07.998478: step 106, loss 1.60591, acc 0.703125, prec 0.0169903, recall 0.589474\n",
      "2017-12-08T12:01:10.487028: step 107, loss 7.15934, acc 0.625, prec 0.0171687, recall 0.587629\n",
      "2017-12-08T12:01:11.902967: step 108, loss 1.33016, acc 0.75, prec 0.0176753, recall 0.59596\n",
      "2017-12-08T12:01:12.581949: step 109, loss 8.05187, acc 0.609375, prec 0.0175543, recall 0.584158\n",
      "2017-12-08T12:01:13.331891: step 110, loss 1.27023, acc 0.71875, prec 0.0177515, recall 0.588235\n",
      "2017-12-08T12:01:14.100864: step 111, loss 2.35685, acc 0.5625, prec 0.0176056, recall 0.588235\n",
      "2017-12-08T12:01:15.429203: step 112, loss 2.83772, acc 0.4375, prec 0.0174216, recall 0.588235\n",
      "2017-12-08T12:01:17.777602: step 113, loss 2.58411, acc 0.53125, prec 0.017554, recall 0.592233\n",
      "2017-12-08T12:01:18.400623: step 114, loss 3.23179, acc 0.375, prec 0.0173542, recall 0.592233\n",
      "2017-12-08T12:01:18.991221: step 115, loss 2.63801, acc 0.546875, prec 0.0174894, recall 0.596154\n",
      "2017-12-08T12:01:19.598904: step 116, loss 3.07781, acc 0.4375, prec 0.0173136, recall 0.596154\n",
      "2017-12-08T12:01:20.222324: step 117, loss 4.07894, acc 0.390625, prec 0.0173985, recall 0.6\n",
      "2017-12-08T12:01:20.805313: step 118, loss 3.51408, acc 0.46875, prec 0.0175055, recall 0.603774\n",
      "2017-12-08T12:01:21.417783: step 119, loss 2.68432, acc 0.53125, prec 0.017363, recall 0.603774\n",
      "2017-12-08T12:01:22.032312: step 120, loss 2.93571, acc 0.578125, prec 0.0172367, recall 0.603774\n",
      "2017-12-08T12:01:22.668988: step 121, loss 4.1374, acc 0.5, prec 0.0173565, recall 0.601852\n",
      "2017-12-08T12:01:23.284286: step 122, loss 2.35421, acc 0.484375, prec 0.0172049, recall 0.601852\n",
      "2017-12-08T12:01:23.892553: step 123, loss 2.09164, acc 0.546875, prec 0.0170738, recall 0.601852\n",
      "2017-12-08T12:01:24.535298: step 124, loss 2.27514, acc 0.5625, prec 0.0172054, recall 0.605505\n",
      "2017-12-08T12:01:25.165086: step 125, loss 5.55041, acc 0.671875, prec 0.0171206, recall 0.594595\n",
      "2017-12-08T12:01:25.759984: step 126, loss 1.83708, acc 0.625, prec 0.0175213, recall 0.60177\n",
      "2017-12-08T12:01:26.347277: step 127, loss 1.52347, acc 0.640625, prec 0.0176697, recall 0.605263\n",
      "2017-12-08T12:01:26.963481: step 128, loss 14.1461, acc 0.671875, prec 0.0175796, recall 0.6\n",
      "2017-12-08T12:01:27.579411: step 129, loss 12.459, acc 0.71875, prec 0.017753, recall 0.598291\n",
      "2017-12-08T12:01:28.187314: step 130, loss 1.02327, acc 0.75, prec 0.0176812, recall 0.598291\n",
      "2017-12-08T12:01:28.829655: step 131, loss 1.92273, acc 0.546875, prec 0.0177989, recall 0.601695\n",
      "2017-12-08T12:01:29.438154: step 132, loss 1.87919, acc 0.640625, prec 0.0176969, recall 0.601695\n",
      "2017-12-08T12:01:30.034362: step 133, loss 2.92806, acc 0.484375, prec 0.0180381, recall 0.608333\n",
      "2017-12-08T12:01:30.659179: step 134, loss 3.13087, acc 0.5625, prec 0.0181551, recall 0.61157\n",
      "2017-12-08T12:01:31.268106: step 135, loss 2.41639, acc 0.515625, prec 0.018018, recall 0.61157\n",
      "2017-12-08T12:01:31.879751: step 136, loss 2.24984, acc 0.59375, prec 0.0179047, recall 0.61157\n",
      "2017-12-08T12:01:32.479905: step 137, loss 2.0142, acc 0.59375, prec 0.0177927, recall 0.61157\n",
      "2017-12-08T12:01:33.102435: step 138, loss 2.26743, acc 0.515625, prec 0.0178955, recall 0.614754\n",
      "2017-12-08T12:01:33.698500: step 139, loss 1.70254, acc 0.640625, prec 0.0177978, recall 0.614754\n",
      "2017-12-08T12:01:34.287271: step 140, loss 1.79454, acc 0.625, prec 0.017697, recall 0.614754\n",
      "2017-12-08T12:01:34.915264: step 141, loss 0.882044, acc 0.796875, prec 0.0176429, recall 0.614754\n",
      "2017-12-08T12:01:35.513791: step 142, loss 1.94491, acc 0.671875, prec 0.0180159, recall 0.620968\n",
      "2017-12-08T12:01:36.119397: step 143, loss 1.2214, acc 0.765625, prec 0.0179529, recall 0.620968\n",
      "2017-12-08T12:01:36.735139: step 144, loss 15.8304, acc 0.734375, prec 0.0178862, recall 0.616\n",
      "2017-12-08T12:01:37.393937: step 145, loss 26.4666, acc 0.78125, prec 0.0178323, recall 0.611111\n",
      "2017-12-08T12:01:38.019739: step 146, loss 1.48239, acc 0.75, prec 0.0182196, recall 0.617188\n",
      "2017-12-08T12:01:38.620488: step 147, loss 1.25781, acc 0.734375, prec 0.0183739, recall 0.620155\n",
      "2017-12-08T12:01:39.214001: step 148, loss 13.3151, acc 0.703125, prec 0.0182983, recall 0.615385\n",
      "2017-12-08T12:01:39.820201: step 149, loss 2.01031, acc 0.65625, prec 0.0182066, recall 0.615385\n",
      "2017-12-08T12:01:40.446688: step 150, loss 1.82825, acc 0.609375, prec 0.0181036, recall 0.615385\n",
      "2017-12-08T12:01:41.047397: step 151, loss 1.25849, acc 0.640625, prec 0.0182309, recall 0.618321\n",
      "2017-12-08T12:01:41.656916: step 152, loss 3.53917, acc 0.515625, prec 0.0181087, recall 0.613636\n",
      "2017-12-08T12:01:42.267029: step 153, loss 2.60064, acc 0.578125, prec 0.0182182, recall 0.616541\n",
      "2017-12-08T12:01:42.877097: step 154, loss 2.13825, acc 0.5625, prec 0.0181055, recall 0.616541\n",
      "2017-12-08T12:01:43.485528: step 155, loss 2.01315, acc 0.5625, prec 0.0182097, recall 0.619403\n",
      "2017-12-08T12:01:44.090334: step 156, loss 1.96914, acc 0.53125, prec 0.0183046, recall 0.622222\n",
      "2017-12-08T12:01:44.700511: step 157, loss 1.92108, acc 0.671875, prec 0.0184342, recall 0.625\n",
      "2017-12-08T12:01:45.333016: step 158, loss 2.23607, acc 0.5625, prec 0.0183229, recall 0.625\n",
      "2017-12-08T12:01:45.996711: step 159, loss 1.67278, acc 0.65625, prec 0.018447, recall 0.627737\n",
      "2017-12-08T12:01:46.715671: step 160, loss 1.76376, acc 0.640625, prec 0.0183565, recall 0.627737\n",
      "2017-12-08T12:01:47.383540: step 161, loss 12.5677, acc 0.71875, prec 0.0184988, recall 0.625899\n",
      "2017-12-08T12:01:48.011518: step 162, loss 1.61812, acc 0.640625, prec 0.018824, recall 0.631206\n",
      "2017-12-08T12:01:48.625466: step 163, loss 11.1489, acc 0.6875, prec 0.0187487, recall 0.626761\n",
      "2017-12-08T12:01:49.231913: step 164, loss 1.52819, acc 0.6875, prec 0.01867, recall 0.626761\n",
      "2017-12-08T12:01:49.857354: step 165, loss 3.46792, acc 0.609375, prec 0.0185765, recall 0.622378\n",
      "2017-12-08T12:01:50.522713: step 166, loss 2.61222, acc 0.484375, prec 0.0184494, recall 0.622378\n",
      "2017-12-08T12:01:51.152296: step 167, loss 2.56164, acc 0.4375, prec 0.0185147, recall 0.625\n",
      "2017-12-08T12:01:51.799274: step 168, loss 2.78835, acc 0.5, prec 0.0185942, recall 0.627586\n",
      "2017-12-08T12:01:52.455452: step 169, loss 1.9429, acc 0.578125, prec 0.0186916, recall 0.630137\n",
      "2017-12-08T12:01:53.157696: step 170, loss 4.65527, acc 0.453125, prec 0.0191532, recall 0.637584\n",
      "2017-12-08T12:01:53.846570: step 171, loss 2.74671, acc 0.453125, prec 0.019019, recall 0.637584\n",
      "2017-12-08T12:01:54.617110: step 172, loss 2.16282, acc 0.5, prec 0.0190931, recall 0.64\n",
      "2017-12-08T12:01:55.288241: step 173, loss 2.3677, acc 0.46875, prec 0.0189648, recall 0.64\n",
      "2017-12-08T12:01:55.962926: step 174, loss 2.19591, acc 0.546875, prec 0.0192421, recall 0.644737\n",
      "2017-12-08T12:01:56.668763: step 175, loss 2.51193, acc 0.484375, prec 0.0191182, recall 0.644737\n",
      "2017-12-08T12:01:57.399824: step 176, loss 11.0563, acc 0.625, prec 0.019227, recall 0.63871\n",
      "2017-12-08T12:01:58.166433: step 177, loss 1.28174, acc 0.640625, prec 0.0191415, recall 0.63871\n",
      "2017-12-08T12:01:58.860393: step 178, loss 1.84009, acc 0.609375, prec 0.0192382, recall 0.641026\n",
      "2017-12-08T12:01:59.495674: step 179, loss 1.62692, acc 0.671875, prec 0.0193487, recall 0.643312\n",
      "2017-12-08T12:02:00.148172: step 180, loss 30.75, acc 0.546875, prec 0.0194397, recall 0.63354\n",
      "2017-12-08T12:02:00.808610: step 181, loss 2.06606, acc 0.546875, prec 0.0193328, recall 0.63354\n",
      "2017-12-08T12:02:01.561941: step 182, loss 2.4028, acc 0.4375, prec 0.0192018, recall 0.63354\n",
      "2017-12-08T12:02:02.211215: step 183, loss 2.58024, acc 0.53125, prec 0.0194611, recall 0.638037\n",
      "2017-12-08T12:02:02.899203: step 184, loss 11.7068, acc 0.4375, prec 0.0195167, recall 0.636364\n",
      "2017-12-08T12:02:03.545435: step 185, loss 2.77984, acc 0.4375, prec 0.019387, recall 0.636364\n",
      "2017-12-08T12:02:04.176176: step 186, loss 3.18571, acc 0.328125, prec 0.019773, recall 0.642857\n",
      "2017-12-08T12:02:04.808668: step 187, loss 3.42549, acc 0.359375, prec 0.0198038, recall 0.64497\n",
      "2017-12-08T12:02:05.445639: step 188, loss 4.22338, acc 0.296875, prec 0.0198198, recall 0.647059\n",
      "2017-12-08T12:02:06.084638: step 189, loss 3.97936, acc 0.390625, prec 0.0196815, recall 0.647059\n",
      "2017-12-08T12:02:06.753650: step 190, loss 3.6244, acc 0.359375, prec 0.0195382, recall 0.647059\n",
      "2017-12-08T12:02:07.405691: step 191, loss 11.3638, acc 0.390625, prec 0.0194072, recall 0.643275\n",
      "2017-12-08T12:02:08.035987: step 192, loss 3.50614, acc 0.390625, prec 0.0194464, recall 0.645349\n",
      "2017-12-08T12:02:08.679762: step 193, loss 2.97165, acc 0.453125, prec 0.0193279, recall 0.645349\n",
      "2017-12-08T12:02:09.327967: step 194, loss 2.79604, acc 0.40625, prec 0.0192008, recall 0.645349\n",
      "2017-12-08T12:02:09.959354: step 195, loss 2.14105, acc 0.515625, prec 0.0190984, recall 0.645349\n",
      "2017-12-08T12:02:10.596364: step 196, loss 2.34175, acc 0.375, prec 0.0191355, recall 0.647399\n",
      "2017-12-08T12:02:11.223851: step 197, loss 2.46815, acc 0.5, prec 0.0190314, recall 0.647399\n",
      "2017-12-08T12:02:11.893212: step 198, loss 1.02316, acc 0.640625, prec 0.0191234, recall 0.649425\n",
      "2017-12-08T12:02:12.531028: step 199, loss 0.80045, acc 0.8125, prec 0.0190846, recall 0.649425\n",
      "2017-12-08T12:02:13.153749: step 200, loss 3.62236, acc 0.71875, prec 0.0193603, recall 0.649718\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512752358/checkpoints/model-200\n",
      "\n",
      "2017-12-08T12:02:14.542437: step 201, loss 9.8676, acc 0.71875, prec 0.019305, recall 0.646067\n",
      "2017-12-08T12:02:15.299887: step 202, loss 3.33598, acc 0.78125, prec 0.0194272, recall 0.644444\n",
      "2017-12-08T12:02:15.938495: step 203, loss 1.26231, acc 0.6875, prec 0.0193624, recall 0.644444\n",
      "2017-12-08T12:02:16.640153: step 204, loss 14.0353, acc 0.640625, prec 0.0192915, recall 0.640884\n",
      "2017-12-08T12:02:17.364137: step 205, loss 1.24544, acc 0.71875, prec 0.0193966, recall 0.642857\n",
      "2017-12-08T12:02:18.069360: step 206, loss 4.95169, acc 0.5625, prec 0.0194719, recall 0.641304\n",
      "2017-12-08T12:02:18.734462: step 207, loss 12.1693, acc 0.703125, prec 0.0194143, recall 0.637838\n",
      "2017-12-08T12:02:19.416361: step 208, loss 1.98365, acc 0.625, prec 0.0194986, recall 0.639785\n",
      "2017-12-08T12:02:20.171841: step 209, loss 2.20391, acc 0.5625, prec 0.0195695, recall 0.641711\n",
      "2017-12-08T12:02:20.859153: step 210, loss 2.5063, acc 0.53125, prec 0.0197923, recall 0.645503\n",
      "2017-12-08T12:02:21.545660: step 211, loss 3.11596, acc 0.359375, prec 0.0201353, recall 0.651042\n",
      "2017-12-08T12:02:22.221022: step 212, loss 3.23609, acc 0.359375, prec 0.0200032, recall 0.651042\n",
      "2017-12-08T12:02:22.880011: step 213, loss 3.19234, acc 0.4375, prec 0.0202004, recall 0.654639\n",
      "2017-12-08T12:02:23.538415: step 214, loss 2.58212, acc 0.53125, prec 0.0201045, recall 0.654639\n",
      "2017-12-08T12:02:24.196383: step 215, loss 3.64974, acc 0.46875, prec 0.0206137, recall 0.661616\n",
      "2017-12-08T12:02:24.895893: step 216, loss 2.33053, acc 0.5625, prec 0.0206767, recall 0.663317\n",
      "2017-12-08T12:02:25.568076: step 217, loss 8.40112, acc 0.46875, prec 0.0205704, recall 0.66\n",
      "2017-12-08T12:02:26.233710: step 218, loss 1.80657, acc 0.640625, prec 0.0204969, recall 0.66\n",
      "2017-12-08T12:02:26.903225: step 219, loss 2.26055, acc 0.546875, prec 0.0207078, recall 0.663366\n",
      "2017-12-08T12:02:27.636914: step 220, loss 2.96939, acc 0.46875, prec 0.0207501, recall 0.665025\n",
      "2017-12-08T12:02:28.305734: step 221, loss 2.31769, acc 0.5, prec 0.0206485, recall 0.665025\n",
      "2017-12-08T12:02:28.976408: step 222, loss 1.99886, acc 0.578125, prec 0.0205636, recall 0.665025\n",
      "2017-12-08T12:02:29.645107: step 223, loss 1.71362, acc 0.609375, prec 0.0204856, recall 0.665025\n",
      "2017-12-08T12:02:30.311475: step 224, loss 2.64268, acc 0.703125, prec 0.0204298, recall 0.661765\n",
      "2017-12-08T12:02:30.987405: step 225, loss 1.82924, acc 0.59375, prec 0.0203497, recall 0.661765\n",
      "2017-12-08T12:02:31.639277: step 226, loss 5.28252, acc 0.65625, prec 0.0204327, recall 0.660194\n",
      "2017-12-08T12:02:32.285914: step 227, loss 1.015, acc 0.703125, prec 0.0203745, recall 0.660194\n",
      "2017-12-08T12:02:32.999618: step 228, loss 17.2061, acc 0.609375, prec 0.0203046, recall 0.653846\n",
      "2017-12-08T12:02:33.663122: step 229, loss 1.14967, acc 0.65625, prec 0.0202381, recall 0.653846\n",
      "2017-12-08T12:02:34.330621: step 230, loss 1.61872, acc 0.65625, prec 0.0203174, recall 0.655502\n",
      "2017-12-08T12:02:34.984845: step 231, loss 2.09398, acc 0.515625, prec 0.020369, recall 0.657143\n",
      "2017-12-08T12:02:35.643928: step 232, loss 1.89431, acc 0.5625, prec 0.0202852, recall 0.657143\n",
      "2017-12-08T12:02:36.310203: step 233, loss 1.71751, acc 0.546875, prec 0.0201991, recall 0.657143\n",
      "2017-12-08T12:02:36.969538: step 234, loss 2.109, acc 0.53125, prec 0.0203963, recall 0.660377\n",
      "2017-12-08T12:02:37.667668: step 235, loss 2.41326, acc 0.546875, prec 0.0204526, recall 0.661972\n",
      "2017-12-08T12:02:38.370270: step 236, loss 1.47009, acc 0.625, prec 0.0203816, recall 0.661972\n",
      "2017-12-08T12:02:39.043972: step 237, loss 1.40233, acc 0.65625, prec 0.0204581, recall 0.663551\n",
      "2017-12-08T12:02:39.688796: step 238, loss 1.62837, acc 0.609375, prec 0.0203847, recall 0.663551\n",
      "2017-12-08T12:02:40.333932: step 239, loss 1.3842, acc 0.6875, prec 0.0203264, recall 0.663551\n",
      "2017-12-08T12:02:40.989060: step 240, loss 1.62222, acc 0.609375, prec 0.0202539, recall 0.663551\n",
      "2017-12-08T12:02:41.631543: step 241, loss 1.41787, acc 0.71875, prec 0.020202, recall 0.663551\n",
      "2017-12-08T12:02:42.268675: step 242, loss 1.00208, acc 0.71875, prec 0.0202894, recall 0.665116\n",
      "2017-12-08T12:02:42.894917: step 243, loss 0.907793, acc 0.765625, prec 0.0202464, recall 0.665116\n",
      "2017-12-08T12:02:43.557630: step 244, loss 6.97149, acc 0.796875, prec 0.0203505, recall 0.663594\n",
      "2017-12-08T12:02:44.188378: step 245, loss 0.503224, acc 0.765625, prec 0.0203074, recall 0.663594\n",
      "2017-12-08T12:02:44.805058: step 246, loss 0.499816, acc 0.84375, prec 0.0204168, recall 0.665138\n",
      "2017-12-08T12:02:45.433710: step 247, loss 10.2908, acc 0.78125, prec 0.0205171, recall 0.663636\n",
      "2017-12-08T12:02:46.054680: step 248, loss 0.718786, acc 0.78125, prec 0.0206142, recall 0.665158\n",
      "2017-12-08T12:02:46.670757: step 249, loss 5.81346, acc 0.6875, prec 0.0205594, recall 0.662162\n",
      "2017-12-08T12:02:47.296678: step 250, loss 4.82851, acc 0.703125, prec 0.0205078, recall 0.659193\n",
      "2017-12-08T12:02:47.961634: step 251, loss 0.861555, acc 0.6875, prec 0.0204508, recall 0.659193\n",
      "2017-12-08T12:02:48.616229: step 252, loss 1.22383, acc 0.65625, prec 0.0205242, recall 0.660714\n",
      "2017-12-08T12:02:49.234892: step 253, loss 1.61625, acc 0.703125, prec 0.0206057, recall 0.662222\n",
      "2017-12-08T12:02:49.845747: step 254, loss 1.52252, acc 0.625, prec 0.0205376, recall 0.662222\n",
      "2017-12-08T12:02:50.476354: step 255, loss 2.03675, acc 0.53125, prec 0.020453, recall 0.662222\n",
      "2017-12-08T12:02:51.102145: step 256, loss 2.71798, acc 0.453125, prec 0.0206228, recall 0.665198\n",
      "2017-12-08T12:02:51.736733: step 257, loss 2.72645, acc 0.609375, prec 0.0206859, recall 0.666667\n",
      "2017-12-08T12:02:52.364759: step 258, loss 2.47777, acc 0.5625, prec 0.0208729, recall 0.669565\n",
      "2017-12-08T12:02:52.977697: step 259, loss 2.07035, acc 0.625, prec 0.0210697, recall 0.672414\n",
      "2017-12-08T12:02:53.606200: step 260, loss 3.8572, acc 0.53125, prec 0.0209875, recall 0.669528\n",
      "2017-12-08T12:02:54.263888: step 261, loss 1.34166, acc 0.703125, prec 0.0210653, recall 0.67094\n",
      "2017-12-08T12:02:54.879948: step 262, loss 1.32259, acc 0.640625, prec 0.0210005, recall 0.67094\n",
      "2017-12-08T12:02:55.524486: step 263, loss 8.12912, acc 0.53125, prec 0.0210498, recall 0.669492\n",
      "2017-12-08T12:02:56.157854: step 264, loss 1.98354, acc 0.546875, prec 0.0209688, recall 0.669492\n",
      "2017-12-08T12:02:56.784613: step 265, loss 2.32393, acc 0.5, prec 0.0208801, recall 0.669492\n",
      "2017-12-08T12:02:57.414160: step 266, loss 1.84267, acc 0.5625, prec 0.0209321, recall 0.670886\n",
      "2017-12-08T12:02:58.076091: step 267, loss 8.54887, acc 0.46875, prec 0.0208415, recall 0.668067\n",
      "2017-12-08T12:02:58.711472: step 268, loss 1.98178, acc 0.609375, prec 0.0207735, recall 0.668067\n",
      "2017-12-08T12:02:59.380075: step 269, loss 17.6938, acc 0.5, prec 0.0208171, recall 0.666667\n",
      "2017-12-08T12:03:00.015173: step 270, loss 1.46172, acc 0.578125, prec 0.0208711, recall 0.66805\n",
      "2017-12-08T12:03:00.639117: step 271, loss 2.24956, acc 0.578125, prec 0.0209248, recall 0.669421\n",
      "2017-12-08T12:03:01.278250: step 272, loss 1.84539, acc 0.65625, prec 0.0208655, recall 0.669421\n",
      "2017-12-08T12:03:01.927938: step 273, loss 2.18652, acc 0.609375, prec 0.0207986, recall 0.669421\n",
      "2017-12-08T12:03:02.577438: step 274, loss 5.04334, acc 0.5625, prec 0.020852, recall 0.668033\n",
      "2017-12-08T12:03:03.226899: step 275, loss 2.08454, acc 0.578125, prec 0.0207802, recall 0.668033\n",
      "2017-12-08T12:03:03.883953: step 276, loss 2.52523, acc 0.5, prec 0.0206958, recall 0.668033\n",
      "2017-12-08T12:03:04.568813: step 277, loss 1.81286, acc 0.59375, prec 0.0206277, recall 0.668033\n",
      "2017-12-08T12:03:05.225373: step 278, loss 2.45319, acc 0.515625, prec 0.0205471, recall 0.668033\n",
      "2017-12-08T12:03:05.872008: step 279, loss 1.93724, acc 0.5625, prec 0.0204748, recall 0.668033\n",
      "2017-12-08T12:03:06.522147: step 280, loss 0.998355, acc 0.625, prec 0.0204133, recall 0.668033\n",
      "2017-12-08T12:03:07.177521: step 281, loss 9.4589, acc 0.578125, prec 0.0204693, recall 0.666667\n",
      "2017-12-08T12:03:07.872135: step 282, loss 1.76209, acc 0.65625, prec 0.0205352, recall 0.668016\n",
      "2017-12-08T12:03:08.528643: step 283, loss 2.7274, acc 0.75, prec 0.0208618, recall 0.669323\n",
      "2017-12-08T12:03:09.181801: step 284, loss 3.43187, acc 0.578125, prec 0.0207947, recall 0.666667\n",
      "2017-12-08T12:03:09.866235: step 285, loss 1.11318, acc 0.765625, prec 0.0208771, recall 0.667984\n",
      "2017-12-08T12:03:10.527743: step 286, loss 1.80725, acc 0.65625, prec 0.0208205, recall 0.667984\n",
      "2017-12-08T12:03:11.178517: step 287, loss 7.39957, acc 0.53125, prec 0.0209867, recall 0.667969\n",
      "2017-12-08T12:03:11.837355: step 288, loss 2.82557, acc 0.5625, prec 0.0210346, recall 0.669261\n",
      "2017-12-08T12:03:12.485147: step 289, loss 5.15038, acc 0.546875, prec 0.0209628, recall 0.666667\n",
      "2017-12-08T12:03:13.139302: step 290, loss 1.56369, acc 0.59375, prec 0.0211345, recall 0.669231\n",
      "2017-12-08T12:03:13.783693: step 291, loss 2.40397, acc 0.421875, prec 0.0211583, recall 0.670498\n",
      "2017-12-08T12:03:14.441747: step 292, loss 2.8628, acc 0.484375, prec 0.0214277, recall 0.674242\n",
      "2017-12-08T12:03:15.152090: step 293, loss 3.93141, acc 0.46875, prec 0.0214603, recall 0.672932\n",
      "2017-12-08T12:03:15.946186: step 294, loss 2.19013, acc 0.515625, prec 0.0213808, recall 0.672932\n",
      "2017-12-08T12:03:16.821599: step 295, loss 2.90166, acc 0.375, prec 0.0215118, recall 0.675373\n",
      "2017-12-08T12:03:17.587228: step 296, loss 3.07338, acc 0.46875, prec 0.0214252, recall 0.675373\n",
      "2017-12-08T12:03:18.344801: step 297, loss 2.56383, acc 0.46875, prec 0.0213393, recall 0.675373\n",
      "2017-12-08T12:03:19.060106: step 298, loss 2.56477, acc 0.515625, prec 0.0214915, recall 0.677778\n",
      "2017-12-08T12:03:19.770707: step 299, loss 8.84102, acc 0.375, prec 0.021396, recall 0.672794\n",
      "2017-12-08T12:03:20.507909: step 300, loss 2.69725, acc 0.46875, prec 0.0213113, recall 0.672794\n",
      "\n",
      "Evaluation:\n",
      "2017-12-08T12:04:33.295806: step 300, loss 2.05651, acc 0.484574, prec 0.0240777, recall 0.784404\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512752358/checkpoints/model-300\n",
      "\n",
      "2017-12-08T12:04:36.553511: step 301, loss 1.75855, acc 0.5625, prec 0.0240304, recall 0.784404\n",
      "2017-12-08T12:04:37.237491: step 302, loss 2.80783, acc 0.453125, prec 0.0239714, recall 0.784404\n",
      "2017-12-08T12:04:37.823717: step 303, loss 2.04543, acc 0.59375, prec 0.0239278, recall 0.784404\n",
      "2017-12-08T12:04:38.435030: step 304, loss 2.86401, acc 0.5, prec 0.0238743, recall 0.784404\n",
      "2017-12-08T12:04:39.029109: step 305, loss 2.92054, acc 0.640625, prec 0.0240401, recall 0.785877\n",
      "2017-12-08T12:04:39.623181: step 306, loss 1.4796, acc 0.578125, prec 0.023995, recall 0.785877\n",
      "2017-12-08T12:04:40.231417: step 307, loss 1.62146, acc 0.640625, prec 0.0240244, recall 0.786364\n",
      "2017-12-08T12:04:40.840662: step 308, loss 1.5146, acc 0.671875, prec 0.0239895, recall 0.786364\n",
      "2017-12-08T12:04:41.482136: step 309, loss 0.98027, acc 0.765625, prec 0.0239645, recall 0.786364\n",
      "2017-12-08T12:04:42.152253: step 310, loss 0.886468, acc 0.6875, prec 0.0239314, recall 0.786364\n",
      "2017-12-08T12:04:42.838124: step 311, loss 1.02619, acc 0.765625, prec 0.023974, recall 0.786848\n",
      "2017-12-08T12:04:43.574983: step 312, loss 0.910893, acc 0.765625, prec 0.0240166, recall 0.78733\n",
      "2017-12-08T12:04:44.317808: step 313, loss 11.625, acc 0.8125, prec 0.0240656, recall 0.786036\n",
      "2017-12-08T12:04:45.037693: step 314, loss 0.668993, acc 0.84375, prec 0.0241163, recall 0.786517\n",
      "2017-12-08T12:04:45.755299: step 315, loss 3.00258, acc 0.890625, prec 0.0242408, recall 0.785714\n",
      "2017-12-08T12:04:46.598082: step 316, loss 1.27966, acc 0.828125, prec 0.0242895, recall 0.786192\n",
      "2017-12-08T12:04:47.372325: step 317, loss 5.67026, acc 0.8125, prec 0.0244724, recall 0.785872\n",
      "2017-12-08T12:04:48.148745: step 318, loss 2.06707, acc 0.6875, prec 0.0245058, recall 0.786344\n",
      "2017-12-08T12:04:48.962674: step 319, loss 2.32819, acc 0.578125, prec 0.0246609, recall 0.787746\n",
      "2017-12-08T12:04:49.732290: step 320, loss 1.61294, acc 0.578125, prec 0.0246154, recall 0.787746\n",
      "2017-12-08T12:04:50.464079: step 321, loss 1.63564, acc 0.5625, prec 0.0245683, recall 0.787746\n",
      "2017-12-08T12:04:51.192805: step 322, loss 1.54296, acc 0.5625, prec 0.0245215, recall 0.787746\n",
      "2017-12-08T12:04:51.933071: step 323, loss 1.75807, acc 0.5625, prec 0.0244748, recall 0.787746\n",
      "2017-12-08T12:04:52.729160: step 324, loss 2.1464, acc 0.53125, prec 0.024425, recall 0.787746\n",
      "2017-12-08T12:04:53.456683: step 325, loss 2.87604, acc 0.515625, prec 0.0245719, recall 0.78913\n",
      "2017-12-08T12:04:54.267328: step 326, loss 1.65701, acc 0.546875, prec 0.0247214, recall 0.790497\n",
      "2017-12-08T12:04:55.007560: step 327, loss 2.11549, acc 0.515625, prec 0.0247355, recall 0.790948\n",
      "2017-12-08T12:04:55.912045: step 328, loss 2.3071, acc 0.484375, prec 0.0246806, recall 0.790948\n",
      "2017-12-08T12:04:56.649703: step 329, loss 2.12573, acc 0.53125, prec 0.0246309, recall 0.790948\n",
      "2017-12-08T12:04:57.351722: step 330, loss 0.958746, acc 0.6875, prec 0.0247286, recall 0.791846\n",
      "2017-12-08T12:04:58.080565: step 331, loss 8.2109, acc 0.625, prec 0.0247558, recall 0.790598\n",
      "2017-12-08T12:04:58.908415: step 332, loss 1.58112, acc 0.640625, prec 0.0247829, recall 0.791045\n",
      "2017-12-08T12:04:59.617080: step 333, loss 0.908862, acc 0.703125, prec 0.0247515, recall 0.791045\n",
      "2017-12-08T12:05:00.387083: step 334, loss 1.05034, acc 0.6875, prec 0.0247835, recall 0.791489\n",
      "2017-12-08T12:05:01.147635: step 335, loss 2.48209, acc 0.703125, prec 0.0247538, recall 0.789809\n",
      "2017-12-08T12:05:01.891855: step 336, loss 1.40916, acc 0.640625, prec 0.024716, recall 0.789809\n",
      "2017-12-08T12:05:02.564178: step 337, loss 0.829978, acc 0.734375, prec 0.0247528, recall 0.790254\n",
      "2017-12-08T12:05:03.272142: step 338, loss 1.03386, acc 0.6875, prec 0.02472, recall 0.790254\n",
      "2017-12-08T12:05:03.927206: step 339, loss 0.885333, acc 0.765625, prec 0.0246954, recall 0.790254\n",
      "2017-12-08T12:05:04.569942: step 340, loss 0.544559, acc 0.796875, prec 0.0246742, recall 0.790254\n",
      "2017-12-08T12:05:05.220414: step 341, loss 1.72536, acc 0.734375, prec 0.0247754, recall 0.791139\n",
      "2017-12-08T12:05:05.868820: step 342, loss 0.81842, acc 0.703125, prec 0.0247443, recall 0.791139\n",
      "2017-12-08T12:05:06.519047: step 343, loss 2.80861, acc 0.75, prec 0.0247841, recall 0.789916\n",
      "2017-12-08T12:05:07.161527: step 344, loss 0.578153, acc 0.796875, prec 0.0247629, recall 0.789916\n",
      "2017-12-08T12:05:07.797987: step 345, loss 0.960964, acc 0.71875, prec 0.0247977, recall 0.790356\n",
      "2017-12-08T12:05:08.481521: step 346, loss 1.45715, acc 0.78125, prec 0.024839, recall 0.790795\n",
      "2017-12-08T12:05:09.109103: step 347, loss 12.598, acc 0.796875, prec 0.0249475, recall 0.790021\n",
      "2017-12-08T12:05:09.766876: step 348, loss 0.36326, acc 0.84375, prec 0.0249311, recall 0.790021\n",
      "2017-12-08T12:05:10.393588: step 349, loss 1.47944, acc 0.828125, prec 0.0249771, recall 0.790456\n",
      "2017-12-08T12:05:11.033838: step 350, loss 16.4242, acc 0.828125, prec 0.0250246, recall 0.789256\n",
      "2017-12-08T12:05:11.663879: step 351, loss 1.0899, acc 0.703125, prec 0.0249935, recall 0.789256\n",
      "2017-12-08T12:05:12.266442: step 352, loss 0.963157, acc 0.703125, prec 0.0250261, recall 0.789691\n",
      "2017-12-08T12:05:12.895703: step 353, loss 1.17399, acc 0.75, prec 0.025, recall 0.789691\n",
      "2017-12-08T12:05:13.686307: step 354, loss 1.44128, acc 0.578125, prec 0.0250195, recall 0.790123\n",
      "2017-12-08T12:05:14.476562: step 355, loss 1.69952, acc 0.59375, prec 0.0249772, recall 0.790123\n",
      "2017-12-08T12:05:15.227842: step 356, loss 1.67424, acc 0.609375, prec 0.0249367, recall 0.790123\n",
      "2017-12-08T12:05:15.917106: step 357, loss 1.80052, acc 0.640625, prec 0.0249627, recall 0.790554\n",
      "2017-12-08T12:05:16.716961: step 358, loss 2.14791, acc 0.671875, prec 0.025055, recall 0.791411\n",
      "2017-12-08T12:05:17.453979: step 359, loss 1.29956, acc 0.671875, prec 0.025021, recall 0.791411\n",
      "2017-12-08T12:05:18.272758: step 360, loss 6.37408, acc 0.65625, prec 0.025113, recall 0.79065\n",
      "2017-12-08T12:05:19.076356: step 361, loss 3.61811, acc 0.65625, prec 0.0252047, recall 0.789899\n",
      "2017-12-08T12:05:19.825430: step 362, loss 0.930884, acc 0.703125, prec 0.0251738, recall 0.789899\n",
      "2017-12-08T12:05:20.678706: step 363, loss 2.17997, acc 0.46875, prec 0.0251188, recall 0.789899\n",
      "2017-12-08T12:05:21.404164: step 364, loss 2.83306, acc 0.59375, prec 0.0250786, recall 0.788306\n",
      "2017-12-08T12:05:22.104482: step 365, loss 1.88373, acc 0.578125, prec 0.0250976, recall 0.788732\n",
      "2017-12-08T12:05:22.797724: step 366, loss 2.34678, acc 0.609375, prec 0.0251822, recall 0.789579\n",
      "2017-12-08T12:05:23.462223: step 367, loss 1.94997, acc 0.515625, prec 0.0251945, recall 0.79\n",
      "2017-12-08T12:05:24.120733: step 368, loss 2.41156, acc 0.515625, prec 0.0252689, recall 0.790837\n",
      "2017-12-08T12:05:24.731163: step 369, loss 2.39803, acc 0.5, prec 0.0252176, recall 0.790837\n",
      "2017-12-08T12:05:25.372887: step 370, loss 2.20674, acc 0.5, prec 0.0251664, recall 0.790837\n",
      "2017-12-08T12:05:26.033100: step 371, loss 2.90624, acc 0.4375, prec 0.0251708, recall 0.791252\n",
      "2017-12-08T12:05:26.688346: step 372, loss 1.47091, acc 0.71875, prec 0.0253268, recall 0.79249\n",
      "2017-12-08T12:05:27.339504: step 373, loss 3.02358, acc 0.59375, prec 0.0254082, recall 0.793307\n",
      "2017-12-08T12:05:28.035572: step 374, loss 1.09751, acc 0.65625, prec 0.0254344, recall 0.793713\n",
      "2017-12-08T12:05:28.814526: step 375, loss 2.76393, acc 0.671875, prec 0.0255234, recall 0.794521\n",
      "2017-12-08T12:05:29.589677: step 376, loss 1.83255, acc 0.625, prec 0.0256072, recall 0.795322\n",
      "2017-12-08T12:05:30.279343: step 377, loss 0.947476, acc 0.796875, prec 0.0256475, recall 0.79572\n",
      "2017-12-08T12:05:31.010481: step 378, loss 2.66179, acc 0.65625, prec 0.0256137, recall 0.794175\n",
      "2017-12-08T12:05:31.753337: step 379, loss 1.59088, acc 0.671875, prec 0.025641, recall 0.794574\n",
      "2017-12-08T12:05:32.537479: step 380, loss 1.21185, acc 0.671875, prec 0.0257899, recall 0.795761\n",
      "2017-12-08T12:05:33.284637: step 381, loss 1.97367, acc 0.578125, prec 0.0258073, recall 0.796154\n",
      "2017-12-08T12:05:34.044998: step 382, loss 1.1859, acc 0.703125, prec 0.025898, recall 0.796935\n",
      "2017-12-08T12:05:34.809035: step 383, loss 1.57916, acc 0.625, prec 0.0259805, recall 0.79771\n",
      "2017-12-08T12:05:35.584023: step 384, loss 9.2988, acc 0.625, prec 0.0260643, recall 0.796964\n",
      "2017-12-08T12:05:36.274891: step 385, loss 1.48128, acc 0.671875, prec 0.0260907, recall 0.797348\n",
      "2017-12-08T12:05:36.972106: step 386, loss 1.1547, acc 0.703125, prec 0.0261203, recall 0.797732\n",
      "2017-12-08T12:05:37.685930: step 387, loss 4.79467, acc 0.609375, prec 0.0260816, recall 0.796226\n",
      "2017-12-08T12:05:38.437607: step 388, loss 1.40255, acc 0.6875, prec 0.0260494, recall 0.796226\n",
      "2017-12-08T12:05:39.202185: step 389, loss 0.934864, acc 0.734375, prec 0.0260821, recall 0.79661\n",
      "2017-12-08T12:05:40.033188: step 390, loss 2.29799, acc 0.71875, prec 0.0260548, recall 0.795113\n",
      "2017-12-08T12:05:40.762807: step 391, loss 1.12069, acc 0.71875, prec 0.0260859, recall 0.795497\n",
      "2017-12-08T12:05:41.459656: step 392, loss 8.45611, acc 0.609375, prec 0.0260474, recall 0.794007\n",
      "2017-12-08T12:05:42.170909: step 393, loss 1.39231, acc 0.65625, prec 0.0261318, recall 0.794776\n",
      "2017-12-08T12:05:42.871657: step 394, loss 1.40886, acc 0.671875, prec 0.0261578, recall 0.795158\n",
      "2017-12-08T12:05:43.551360: step 395, loss 9.48539, acc 0.59375, prec 0.0261774, recall 0.794063\n",
      "2017-12-08T12:05:44.225342: step 396, loss 1.21157, acc 0.6875, prec 0.0262049, recall 0.794444\n",
      "2017-12-08T12:05:44.934264: step 397, loss 1.98908, acc 0.546875, prec 0.0262179, recall 0.794824\n",
      "2017-12-08T12:05:45.659227: step 398, loss 2.10298, acc 0.515625, prec 0.026287, recall 0.79558\n",
      "2017-12-08T12:05:46.421852: step 399, loss 1.82835, acc 0.515625, prec 0.0262375, recall 0.79558\n",
      "2017-12-08T12:05:47.202120: step 400, loss 3.82961, acc 0.46875, prec 0.026185, recall 0.794118\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512752358/checkpoints/model-400\n",
      "\n",
      "2017-12-08T12:05:49.749099: step 401, loss 2.30916, acc 0.5, prec 0.0261932, recall 0.794495\n",
      "2017-12-08T12:05:50.704336: step 402, loss 1.91589, acc 0.5, prec 0.0262602, recall 0.795247\n",
      "2017-12-08T12:05:51.605806: step 403, loss 1.96063, acc 0.5, prec 0.0262682, recall 0.79562\n",
      "2017-12-08T12:05:52.455537: step 404, loss 2.34377, acc 0.453125, prec 0.0262129, recall 0.79562\n",
      "2017-12-08T12:05:53.260723: step 405, loss 2.51672, acc 0.515625, prec 0.0262226, recall 0.795993\n",
      "2017-12-08T12:05:54.006594: step 406, loss 1.50264, acc 0.640625, prec 0.0261865, recall 0.795993\n",
      "2017-12-08T12:05:54.727779: step 407, loss 1.35893, acc 0.578125, prec 0.0262024, recall 0.796364\n",
      "2017-12-08T12:05:55.496558: step 408, loss 1.673, acc 0.59375, prec 0.0262199, recall 0.796733\n",
      "2017-12-08T12:05:56.291331: step 409, loss 0.853148, acc 0.75, prec 0.0261949, recall 0.796733\n",
      "2017-12-08T12:05:56.995232: step 410, loss 0.650342, acc 0.71875, prec 0.0262248, recall 0.797101\n",
      "2017-12-08T12:05:57.677249: step 411, loss 0.60354, acc 0.734375, prec 0.0262563, recall 0.797468\n",
      "2017-12-08T12:05:58.366180: step 412, loss 0.720927, acc 0.796875, prec 0.0262939, recall 0.797834\n",
      "2017-12-08T12:05:59.097075: step 413, loss 0.681486, acc 0.8125, prec 0.0262751, recall 0.797834\n",
      "2017-12-08T12:05:59.750895: step 414, loss 0.31328, acc 0.875, prec 0.0262626, recall 0.797834\n",
      "2017-12-08T12:06:00.407165: step 415, loss 0.581094, acc 0.875, prec 0.026308, recall 0.798198\n",
      "2017-12-08T12:06:01.077813: step 416, loss 17.9773, acc 0.8125, prec 0.0263517, recall 0.794275\n",
      "2017-12-08T12:06:01.750871: step 417, loss 0.263922, acc 0.921875, prec 0.0263439, recall 0.794275\n",
      "2017-12-08T12:06:02.448020: step 418, loss 0.435492, acc 0.859375, prec 0.0263876, recall 0.794643\n",
      "2017-12-08T12:06:03.105837: step 419, loss 1.01346, acc 0.8125, prec 0.0264842, recall 0.795374\n",
      "2017-12-08T12:06:03.772296: step 420, loss 0.310168, acc 0.890625, prec 0.0264732, recall 0.795374\n",
      "2017-12-08T12:06:04.464876: step 421, loss 0.327559, acc 0.859375, prec 0.0264591, recall 0.795374\n",
      "2017-12-08T12:06:05.112882: step 422, loss 7.56952, acc 0.71875, prec 0.0264901, recall 0.794326\n",
      "2017-12-08T12:06:05.795303: step 423, loss 0.744725, acc 0.703125, prec 0.0265178, recall 0.79469\n",
      "2017-12-08T12:06:06.482656: step 424, loss 7.91128, acc 0.796875, prec 0.0264991, recall 0.793286\n",
      "2017-12-08T12:06:07.135125: step 425, loss 3.91358, acc 0.796875, prec 0.0265377, recall 0.792253\n",
      "2017-12-08T12:06:07.786918: step 426, loss 1.32579, acc 0.796875, prec 0.0265747, recall 0.792619\n",
      "2017-12-08T12:06:08.432183: step 427, loss 1.28221, acc 0.65625, prec 0.0266549, recall 0.793345\n",
      "2017-12-08T12:06:09.089557: step 428, loss 1.8919, acc 0.59375, prec 0.0266714, recall 0.793706\n",
      "2017-12-08T12:06:09.743745: step 429, loss 2.14993, acc 0.46875, prec 0.0266182, recall 0.793706\n",
      "2017-12-08T12:06:10.393170: step 430, loss 2.32417, acc 0.484375, prec 0.0265668, recall 0.793706\n",
      "2017-12-08T12:06:11.040673: step 431, loss 1.44468, acc 0.515625, prec 0.0265187, recall 0.793706\n",
      "2017-12-08T12:06:11.701591: step 432, loss 1.75275, acc 0.53125, prec 0.0265291, recall 0.794066\n",
      "2017-12-08T12:06:12.326411: step 433, loss 1.94134, acc 0.4375, prec 0.0266434, recall 0.795139\n",
      "2017-12-08T12:06:12.956960: step 434, loss 2.56454, acc 0.421875, prec 0.0265862, recall 0.795139\n",
      "2017-12-08T12:06:13.587000: step 435, loss 1.86724, acc 0.5625, prec 0.026543, recall 0.795139\n",
      "2017-12-08T12:06:14.230349: step 436, loss 2.1435, acc 0.53125, prec 0.0266096, recall 0.795848\n",
      "2017-12-08T12:06:14.879142: step 437, loss 1.98958, acc 0.453125, prec 0.0265558, recall 0.795848\n",
      "2017-12-08T12:06:15.524069: step 438, loss 1.43623, acc 0.65625, prec 0.0265221, recall 0.795848\n",
      "2017-12-08T12:06:16.197512: step 439, loss 1.0861, acc 0.640625, prec 0.0265431, recall 0.7962\n",
      "2017-12-08T12:06:16.936786: step 440, loss 0.756562, acc 0.796875, prec 0.0265232, recall 0.7962\n",
      "2017-12-08T12:06:17.588132: step 441, loss 0.856145, acc 0.796875, prec 0.0266153, recall 0.796902\n",
      "2017-12-08T12:06:18.242227: step 442, loss 1.52424, acc 0.75, prec 0.0267027, recall 0.797599\n",
      "2017-12-08T12:06:18.903969: step 443, loss 0.565044, acc 0.765625, prec 0.0266797, recall 0.797599\n",
      "2017-12-08T12:06:19.544252: step 444, loss 1.06151, acc 0.75, prec 0.0266552, recall 0.797599\n",
      "2017-12-08T12:06:20.174480: step 445, loss 7.20192, acc 0.796875, prec 0.0266369, recall 0.796233\n",
      "2017-12-08T12:06:20.810124: step 446, loss 0.763347, acc 0.6875, prec 0.0266064, recall 0.796233\n",
      "2017-12-08T12:06:21.449093: step 447, loss 1.32695, acc 0.875, prec 0.0266499, recall 0.796581\n",
      "2017-12-08T12:06:22.098627: step 448, loss 3.42315, acc 0.875, prec 0.0266949, recall 0.795571\n",
      "2017-12-08T12:06:22.746149: step 449, loss 2.20504, acc 0.875, prec 0.0266842, recall 0.794218\n",
      "2017-12-08T12:06:23.384391: step 450, loss 0.601495, acc 0.78125, prec 0.0266629, recall 0.794218\n",
      "2017-12-08T12:06:24.012527: step 451, loss 1.37913, acc 0.71875, prec 0.026691, recall 0.794567\n",
      "2017-12-08T12:06:24.652076: step 452, loss 2.11719, acc 0.71875, prec 0.0267191, recall 0.794915\n",
      "2017-12-08T12:06:25.285666: step 453, loss 1.35313, acc 0.6875, prec 0.0267441, recall 0.795262\n",
      "2017-12-08T12:06:25.923923: step 454, loss 3.40583, acc 0.59375, prec 0.0267061, recall 0.793919\n",
      "2017-12-08T12:06:26.562391: step 455, loss 3.54225, acc 0.671875, prec 0.0266757, recall 0.79258\n",
      "2017-12-08T12:06:27.201100: step 456, loss 2.92891, acc 0.5625, prec 0.0266886, recall 0.792929\n",
      "2017-12-08T12:06:27.841849: step 457, loss 2.4851, acc 0.421875, prec 0.0266327, recall 0.792929\n",
      "2017-12-08T12:06:28.514285: step 458, loss 2.23197, acc 0.453125, prec 0.0266351, recall 0.793277\n",
      "2017-12-08T12:06:29.224335: step 459, loss 2.6848, acc 0.453125, prec 0.0265826, recall 0.793277\n",
      "2017-12-08T12:06:29.862342: step 460, loss 1.94332, acc 0.515625, prec 0.0265362, recall 0.793277\n",
      "2017-12-08T12:06:30.501862: step 461, loss 2.33342, acc 0.484375, prec 0.0264871, recall 0.793277\n",
      "2017-12-08T12:06:31.149764: step 462, loss 2.56031, acc 0.453125, prec 0.0264352, recall 0.793277\n",
      "2017-12-08T12:06:31.786625: step 463, loss 2.13386, acc 0.5, prec 0.0264967, recall 0.79397\n",
      "2017-12-08T12:06:32.420443: step 464, loss 2.22148, acc 0.5, prec 0.0265037, recall 0.794314\n",
      "2017-12-08T12:06:33.078612: step 465, loss 1.53384, acc 0.734375, prec 0.0265329, recall 0.794658\n",
      "2017-12-08T12:06:33.718175: step 466, loss 1.51954, acc 0.609375, prec 0.026496, recall 0.794658\n",
      "2017-12-08T12:06:34.374664: step 467, loss 1.37481, acc 0.703125, prec 0.026468, recall 0.794658\n",
      "2017-12-08T12:06:35.049569: step 468, loss 1.63945, acc 0.609375, prec 0.0264312, recall 0.794658\n",
      "2017-12-08T12:06:35.725147: step 469, loss 0.970102, acc 0.71875, prec 0.0264048, recall 0.794658\n",
      "2017-12-08T12:06:36.416386: step 470, loss 1.05242, acc 0.703125, prec 0.026377, recall 0.794658\n",
      "2017-12-08T12:06:37.105369: step 471, loss 0.435267, acc 0.828125, prec 0.026361, recall 0.794658\n",
      "2017-12-08T12:06:37.814723: step 472, loss 0.602805, acc 0.828125, prec 0.0263988, recall 0.795\n",
      "2017-12-08T12:06:38.478483: step 473, loss 3.22913, acc 0.8125, prec 0.0264366, recall 0.79402\n",
      "2017-12-08T12:06:39.191175: step 474, loss 19.7555, acc 0.765625, prec 0.0265252, recall 0.792079\n",
      "2017-12-08T12:06:39.876139: step 475, loss 0.620968, acc 0.8125, prec 0.0265076, recall 0.792079\n",
      "2017-12-08T12:06:40.569285: step 476, loss 0.57204, acc 0.875, prec 0.0264959, recall 0.792079\n",
      "2017-12-08T12:06:41.244573: step 477, loss 1.36061, acc 0.703125, prec 0.0264682, recall 0.792079\n",
      "2017-12-08T12:06:41.904652: step 478, loss 0.665164, acc 0.796875, prec 0.0264492, recall 0.792079\n",
      "2017-12-08T12:06:42.562477: step 479, loss 1.15103, acc 0.65625, prec 0.0264172, recall 0.792079\n",
      "2017-12-08T12:06:43.247660: step 480, loss 0.680994, acc 0.8125, prec 0.0263997, recall 0.792079\n",
      "2017-12-08T12:06:43.931624: step 481, loss 1.1054, acc 0.65625, prec 0.0263678, recall 0.792079\n",
      "2017-12-08T12:06:44.591035: step 482, loss 0.841421, acc 0.828125, prec 0.0264054, recall 0.792422\n",
      "2017-12-08T12:06:45.240831: step 483, loss 1.19088, acc 0.703125, prec 0.0264312, recall 0.792763\n",
      "2017-12-08T12:06:45.896819: step 484, loss 1.17003, acc 0.703125, prec 0.0264571, recall 0.793103\n",
      "2017-12-08T12:06:46.557024: step 485, loss 0.989658, acc 0.796875, prec 0.0265448, recall 0.793781\n",
      "2017-12-08T12:06:47.218378: step 486, loss 0.804919, acc 0.703125, prec 0.0265704, recall 0.794118\n",
      "2017-12-08T12:06:47.872165: step 487, loss 0.665574, acc 0.796875, prec 0.0266048, recall 0.794454\n",
      "2017-12-08T12:06:48.567704: step 488, loss 0.611308, acc 0.796875, prec 0.0265859, recall 0.794454\n",
      "2017-12-08T12:06:49.249005: step 489, loss 0.635974, acc 0.796875, prec 0.0266201, recall 0.794788\n",
      "2017-12-08T12:06:49.893140: step 490, loss 0.819422, acc 0.8125, prec 0.0266027, recall 0.794788\n",
      "2017-12-08T12:06:50.535301: step 491, loss 0.50988, acc 0.8125, prec 0.0265853, recall 0.794788\n",
      "2017-12-08T12:06:51.168085: step 492, loss 1.02779, acc 0.828125, prec 0.0266754, recall 0.795455\n",
      "2017-12-08T12:06:51.806201: step 493, loss 0.563787, acc 0.828125, prec 0.0266594, recall 0.795455\n",
      "2017-12-08T12:06:52.437681: step 494, loss 0.245484, acc 0.890625, prec 0.0266493, recall 0.795455\n",
      "2017-12-08T12:06:53.112348: step 495, loss 0.468389, acc 0.828125, prec 0.0266862, recall 0.795786\n",
      "2017-12-08T12:06:53.842635: step 496, loss 7.57147, acc 0.90625, prec 0.026679, recall 0.794498\n",
      "2017-12-08T12:06:54.381391: step 497, loss 0.434772, acc 0.843137, prec 0.0266674, recall 0.794498\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Creating folds\n",
    "kf = KFold(n_splits=4, random_state=5, shuffle=True)\n",
    "for k, (train_index, test_index) in enumerate(kf.split(x, y)):\n",
    "# for train_index, test_index in kf.split(x):\n",
    "#     print(\"Fold: %s =>\" % k,  \"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_dev = x[train_index], x[test_index]\n",
    "    y_train, y_dev = y[train_index], y[test_index]\n",
    "    \n",
    "    train_word_distancesA = word_distancesA[train_index]\n",
    "    train_word_distancesB = word_distancesB[train_index]\n",
    "    \n",
    "    test_word_distancesA = word_distancesA[test_index]\n",
    "    test_word_distancesB = word_distancesB[test_index]\n",
    "    \n",
    "    print(\"Fold: %s =>\" % k, \"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    \n",
    "    model = PositionTextCNN(sequence_length=x_train.shape[1],\n",
    "            vocab_processor=vocab_processor, num_epochs=1, evaluate_every=300, results_dir='fold%s'%k)\n",
    "    model.train_network(x_train, y_train, x_dev, y_dev, \n",
    "                        train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "embedded_positionsA = tf.nn.embedding_lookup(position_vector_mapping, word_distancesA)\n",
    "embedded_positionsB = tf.nn.embedding_lookup(position_vector_mapping, word_distancesB)\n",
    "\n",
    "# embedded_positions = tf.concat([embedded_positionsA, embedded_positionsB], 2)\n",
    "embedded_positions = tf.concat([tf.expand_dims(embedded_positionsA, -1), tf.expand_dims(embedded_positionsB, -1)], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "pos = sess.run(embedded_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(273), Dimension(20), Dimension(1)])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 53)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_distancesA = compute_distance_embedding(encodedPathwayA, x_train[:100])\n",
    "word_distancesB = compute_distance_embedding(encodedPathwayB, x_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7,   6,   5,   4,   3,   2,   1,   0, 599, 598, 597, 596, 595,\n",
       "       594, 593, 592, 591,   8,   7,   6,   5,   4,   3,   2,   1,   0,\n",
       "       599, 598, 597, 596, 595, 594, 593, 592, 591, 590, 589, 588, 587,\n",
       "       586, 585, 584, 583, 582, 581, 580, 579, 578, 577, 576, 575, 574,\n",
       "       573, 572, 571, 570, 569, 568, 567, 566, 565, 564, 563, 562, 561,\n",
       "       560, 559, 558, 557, 556, 555, 554, 553, 552, 551, 550, 549, 548,\n",
       "       547, 546, 545, 544, 543, 542, 541, 540, 539, 538, 537, 536, 535,\n",
       "       534, 533, 532, 531, 530, 529, 528, 527, 526, 525, 524, 523, 522,\n",
       "       521, 520, 519, 518, 517, 516, 515, 514, 513, 512, 511, 510, 509,\n",
       "       508, 507, 506, 505, 504, 503, 502, 501, 500, 499, 498, 497, 496,\n",
       "       495, 494, 493, 492, 491, 490, 489, 488, 487, 486, 485, 484, 483,\n",
       "       482, 481, 480, 479, 478, 477, 476, 475, 474, 473, 472, 471, 470,\n",
       "       469, 468, 467, 466, 465, 464, 463, 462, 461, 460, 459, 458, 457,\n",
       "       456, 455, 454, 453, 452, 451, 450, 449, 448, 447, 446, 445, 444,\n",
       "       443, 442, 441, 440, 439, 438, 437, 436, 435, 434, 433, 432, 431,\n",
       "       430, 429, 428, 427, 426, 425, 424, 423, 422, 421, 420, 419, 418,\n",
       "       417, 416, 415, 414, 413, 412, 411, 410, 409, 408, 407, 406, 405,\n",
       "       404, 403, 402, 401, 400, 399, 398, 397, 396, 395, 394, 393, 392,\n",
       "       391, 390, 389, 388, 387, 386, 385, 384, 383, 382, 381, 380, 379,\n",
       "       378, 377, 376, 375, 374, 373, 372, 371, 370, 369, 368, 367, 366,\n",
       "       365, 364, 363, 362, 361, 360, 359, 358, 357, 356, 355, 354, 353])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 273)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_distancesA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-f1bb0d980bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
