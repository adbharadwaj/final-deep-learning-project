{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "                 embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"W\") \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "#             losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "\n",
    "            class_weight = tf.constant([1.0, 100.0])\n",
    "            weights = tf.reduce_sum(class_weight * self.input_y, axis=1)\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            weighted_losses = losses * weights\n",
    "#             losses = tf.nn.softmax(tf.nn.weighted_cross_entropy_with_logits(logits=self.scores, targets=self.input_y, pos_weight=-1000))\n",
    "            self.loss = tf.reduce_mean(weighted_losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            _, self.precision = tf.metrics.precision(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='precision')\n",
    "            _, self.recall = tf.metrics.recall(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='recall')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.25\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .25, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n",
      "Train/Dev split: 31796/10598\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(out_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(out_dir, 'x_dev.npy'), x_dev)\n",
    "np.save(os.path.join(out_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(out_dir, 'y_dev.npy'), y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623\n",
      "\n",
      "2017-11-26T12:17:06.441083: step 1, loss 7.61647, acc 0.953125, prec 0, recall 0\n",
      "2017-11-26T12:17:06.945972: step 2, loss 8.80949, acc 0.921875, prec 0, recall 0\n",
      "2017-11-26T12:17:07.462414: step 3, loss 15.4421, acc 0.953125, prec 0, recall 0\n",
      "2017-11-26T12:17:08.004809: step 4, loss 5.35551, acc 0.921875, prec 0, recall 0\n",
      "2017-11-26T12:17:08.520473: step 5, loss 0.950449, acc 0.75, prec 0, recall 0\n",
      "2017-11-26T12:17:09.039073: step 6, loss 10.8321, acc 0.75, prec 0, recall 0\n",
      "2017-11-26T12:17:09.565567: step 7, loss 8.72895, acc 0.609375, prec 0, recall 0\n",
      "2017-11-26T12:17:10.094677: step 8, loss 5.1175, acc 0.59375, prec 0, recall 0\n",
      "2017-11-26T12:17:10.616971: step 9, loss 16.8735, acc 0.453125, prec 0.015625, recall 0.2\n",
      "2017-11-26T12:17:11.131943: step 10, loss 2.9226, acc 0.453125, prec 0.0122699, recall 0.2\n",
      "2017-11-26T12:17:11.640939: step 11, loss 7.06824, acc 0.40625, prec 0.0149254, recall 0.25\n",
      "2017-11-26T12:17:12.157340: step 12, loss 6.00661, acc 0.21875, prec 0.0198413, recall 0.333333\n",
      "2017-11-26T12:17:12.714505: step 13, loss 4.49042, acc 0.234375, prec 0.0231023, recall 0.411765\n",
      "2017-11-26T12:17:13.239881: step 14, loss 6.56409, acc 0.09375, prec 0.0193906, recall 0.411765\n",
      "2017-11-26T12:17:13.769189: step 15, loss 5.80304, acc 0.203125, prec 0.0194175, recall 0.421053\n",
      "2017-11-26T12:17:14.431700: step 16, loss 7.95822, acc 0.109375, prec 0.0170576, recall 0.421053\n",
      "2017-11-26T12:17:15.093109: step 17, loss 6.57452, acc 0.125, prec 0.0152381, recall 0.421053\n",
      "2017-11-26T12:17:15.693703: step 18, loss 7.24783, acc 0.09375, prec 0.015411, recall 0.45\n",
      "2017-11-26T12:17:16.286391: step 19, loss 5.55868, acc 0.125, prec 0.0156006, recall 0.47619\n",
      "2017-11-26T12:17:16.880949: step 20, loss 6.73869, acc 0.1875, prec 0.0172662, recall 0.521739\n",
      "2017-11-26T12:17:17.537507: step 21, loss 7.13328, acc 0.09375, prec 0.018543, recall 0.56\n",
      "2017-11-26T12:17:18.145172: step 22, loss 6.09349, acc 0.171875, prec 0.0173267, recall 0.56\n",
      "2017-11-26T12:17:18.760541: step 23, loss 5.4031, acc 0.1875, prec 0.0162791, recall 0.56\n",
      "2017-11-26T12:17:19.358447: step 24, loss 8.17884, acc 0.203125, prec 0.0175439, recall 0.571429\n",
      "2017-11-26T12:17:19.989875: step 25, loss 13.2075, acc 0.28125, prec 0.0187696, recall 0.580645\n",
      "2017-11-26T12:17:20.586719: step 26, loss 10.8285, acc 0.171875, prec 0.0187747, recall 0.575758\n",
      "2017-11-26T12:17:21.182560: step 27, loss 3.66592, acc 0.296875, prec 0.0189036, recall 0.588235\n",
      "2017-11-26T12:17:21.775975: step 28, loss 4.0619, acc 0.28125, prec 0.0190045, recall 0.6\n",
      "2017-11-26T12:17:22.368298: step 29, loss 4.64752, acc 0.25, prec 0.0182134, recall 0.6\n",
      "2017-11-26T12:17:22.964738: step 30, loss 4.82869, acc 0.28125, prec 0.0183333, recall 0.611111\n",
      "2017-11-26T12:17:23.582008: step 31, loss 21.5417, acc 0.34375, prec 0.0177276, recall 0.594595\n",
      "2017-11-26T12:17:24.221849: step 32, loss 5.90638, acc 0.328125, prec 0.0179128, recall 0.589744\n",
      "2017-11-26T12:17:24.908275: step 33, loss 2.83931, acc 0.375, prec 0.0173716, recall 0.589744\n",
      "2017-11-26T12:17:25.607333: step 34, loss 3.5232, acc 0.390625, prec 0.0175953, recall 0.6\n",
      "2017-11-26T12:17:26.347044: step 35, loss 3.08826, acc 0.359375, prec 0.0170819, recall 0.6\n",
      "2017-11-26T12:17:27.072466: step 36, loss 22.9357, acc 0.34375, prec 0.0179682, recall 0.590909\n",
      "2017-11-26T12:17:27.766609: step 37, loss 2.55865, acc 0.421875, prec 0.0175202, recall 0.590909\n",
      "2017-11-26T12:17:28.476685: step 38, loss 26.3016, acc 0.40625, prec 0.0171053, recall 0.565217\n",
      "2017-11-26T12:17:29.197964: step 39, loss 8.13579, acc 0.421875, prec 0.0167095, recall 0.553191\n",
      "2017-11-26T12:17:29.940805: step 40, loss 2.94004, acc 0.375, prec 0.0162907, recall 0.553191\n",
      "2017-11-26T12:17:30.712952: step 41, loss 4.13509, acc 0.296875, prec 0.015844, recall 0.553191\n",
      "2017-11-26T12:17:31.503851: step 42, loss 4.08581, acc 0.375, prec 0.015467, recall 0.553191\n",
      "2017-11-26T12:17:32.247660: step 43, loss 3.68749, acc 0.296875, prec 0.0167727, recall 0.58\n",
      "2017-11-26T12:17:33.003977: step 44, loss 3.86545, acc 0.328125, prec 0.0163657, recall 0.58\n",
      "2017-11-26T12:17:33.807101: step 45, loss 3.40836, acc 0.375, prec 0.0160044, recall 0.58\n",
      "2017-11-26T12:17:34.530098: step 46, loss 2.70534, acc 0.375, prec 0.0167206, recall 0.596154\n",
      "2017-11-26T12:17:35.238394: step 47, loss 5.95941, acc 0.28125, prec 0.016333, recall 0.574074\n",
      "2017-11-26T12:17:35.991700: step 48, loss 3.41026, acc 0.359375, prec 0.0159876, recall 0.574074\n",
      "2017-11-26T12:17:36.799108: step 49, loss 9.12827, acc 0.265625, prec 0.0161128, recall 0.571429\n",
      "2017-11-26T12:17:37.562815: step 50, loss 2.83254, acc 0.453125, prec 0.0158337, recall 0.571429\n",
      "2017-11-26T12:17:38.350720: step 51, loss 6.71887, acc 0.359375, prec 0.0160039, recall 0.568965\n",
      "2017-11-26T12:17:39.078123: step 52, loss 3.09803, acc 0.375, prec 0.016635, recall 0.583333\n",
      "2017-11-26T12:17:39.771138: step 53, loss 4.09814, acc 0.3125, prec 0.0172093, recall 0.596774\n",
      "2017-11-26T12:17:40.469714: step 54, loss 2.75384, acc 0.40625, prec 0.0169104, recall 0.596774\n",
      "2017-11-26T12:17:41.192492: step 55, loss 2.67909, acc 0.40625, prec 0.0166217, recall 0.596774\n",
      "2017-11-26T12:17:41.866587: step 56, loss 3.52355, acc 0.34375, prec 0.0163139, recall 0.596774\n",
      "2017-11-26T12:17:42.635549: step 57, loss 1.9036, acc 0.53125, prec 0.0169565, recall 0.609375\n",
      "2017-11-26T12:17:43.373462: step 58, loss 11.9131, acc 0.453125, prec 0.0171306, recall 0.606061\n",
      "2017-11-26T12:17:44.185413: step 59, loss 2.73295, acc 0.40625, prec 0.0168563, recall 0.606061\n",
      "2017-11-26T12:17:44.974736: step 60, loss 5.59955, acc 0.40625, prec 0.0170054, recall 0.602941\n",
      "2017-11-26T12:17:45.758245: step 61, loss 9.17342, acc 0.375, prec 0.0171359, recall 0.6\n",
      "2017-11-26T12:17:46.533007: step 62, loss 8.16648, acc 0.390625, prec 0.0168742, recall 0.591549\n",
      "2017-11-26T12:17:47.244825: step 63, loss 11.806, acc 0.375, prec 0.0173913, recall 0.594595\n",
      "2017-11-26T12:17:47.948937: step 64, loss 4.15381, acc 0.25, prec 0.0174486, recall 0.6\n",
      "2017-11-26T12:17:48.752167: step 65, loss 4.04777, acc 0.296875, prec 0.0178979, recall 0.61039\n",
      "2017-11-26T12:17:49.564030: step 66, loss 3.67051, acc 0.375, prec 0.0179978, recall 0.615385\n",
      "2017-11-26T12:17:50.301299: step 67, loss 4.28327, acc 0.203125, prec 0.0180213, recall 0.620253\n",
      "2017-11-26T12:17:51.025187: step 68, loss 3.45777, acc 0.421875, prec 0.0184917, recall 0.62963\n",
      "2017-11-26T12:17:51.801090: step 69, loss 4.01665, acc 0.328125, prec 0.0185582, recall 0.634146\n",
      "2017-11-26T12:17:52.566300: step 70, loss 3.52593, acc 0.34375, prec 0.0182841, recall 0.634146\n",
      "2017-11-26T12:17:53.308756: step 71, loss 5.51905, acc 0.28125, prec 0.0179993, recall 0.626506\n",
      "2017-11-26T12:17:54.024582: step 72, loss 4.99551, acc 0.3125, prec 0.0183986, recall 0.635294\n",
      "2017-11-26T12:17:54.785378: step 73, loss 4.00138, acc 0.234375, prec 0.0180965, recall 0.635294\n",
      "2017-11-26T12:17:55.511757: step 74, loss 4.28961, acc 0.3125, prec 0.0181578, recall 0.639535\n",
      "2017-11-26T12:17:56.250404: step 75, loss 4.13133, acc 0.28125, prec 0.0178862, recall 0.639535\n",
      "2017-11-26T12:17:56.995541: step 76, loss 2.6447, acc 0.46875, prec 0.0180064, recall 0.643678\n",
      "2017-11-26T12:17:57.726696: step 77, loss 2.46989, acc 0.5, prec 0.017823, recall 0.643678\n",
      "2017-11-26T12:17:58.450444: step 78, loss 1.7725, acc 0.53125, prec 0.0179641, recall 0.647727\n",
      "2017-11-26T12:17:59.175373: step 79, loss 1.74568, acc 0.5625, prec 0.0181137, recall 0.651685\n",
      "2017-11-26T12:18:00.020767: step 80, loss 3.37178, acc 0.59375, prec 0.0179734, recall 0.644444\n",
      "2017-11-26T12:18:00.775140: step 81, loss 2.14649, acc 0.609375, prec 0.0181371, recall 0.648352\n",
      "2017-11-26T12:18:01.570787: step 82, loss 1.57079, acc 0.609375, prec 0.0182983, recall 0.652174\n",
      "2017-11-26T12:18:02.288642: step 83, loss 1.0503, acc 0.671875, prec 0.0181818, recall 0.652174\n",
      "2017-11-26T12:18:02.983509: step 84, loss 12.0594, acc 0.75, prec 0.0183957, recall 0.648936\n",
      "2017-11-26T12:18:03.728685: step 85, loss 1.17517, acc 0.6875, prec 0.0185796, recall 0.652632\n",
      "2017-11-26T12:18:04.428829: step 86, loss 1.21433, acc 0.609375, prec 0.0184414, recall 0.652632\n",
      "2017-11-26T12:18:05.086902: step 87, loss 12.0866, acc 0.703125, prec 0.0183432, recall 0.645833\n",
      "2017-11-26T12:18:05.797213: step 88, loss 10.1987, acc 0.734375, prec 0.0182568, recall 0.639175\n",
      "2017-11-26T12:18:06.544459: step 89, loss 0.993874, acc 0.78125, prec 0.0181818, recall 0.639175\n",
      "2017-11-26T12:18:07.240170: step 90, loss 5.2487, acc 0.5625, prec 0.018039, recall 0.632653\n",
      "2017-11-26T12:18:07.909313: step 91, loss 0.673642, acc 0.765625, prec 0.0179606, recall 0.632653\n",
      "2017-11-26T12:18:08.563521: step 92, loss 1.22361, acc 0.6875, prec 0.0184226, recall 0.64\n",
      "2017-11-26T12:18:09.248410: step 93, loss 1.81423, acc 0.640625, prec 0.018582, recall 0.643564\n",
      "2017-11-26T12:18:09.916502: step 94, loss 1.33963, acc 0.609375, prec 0.0184502, recall 0.643564\n",
      "2017-11-26T12:18:10.569207: step 95, loss 1.19316, acc 0.734375, prec 0.0183616, recall 0.643564\n",
      "2017-11-26T12:18:11.237162: step 96, loss 3.26025, acc 0.640625, prec 0.0182482, recall 0.637255\n",
      "2017-11-26T12:18:11.905624: step 97, loss 3.311, acc 0.6875, prec 0.0184255, recall 0.634615\n",
      "2017-11-26T12:18:12.595898: step 98, loss 27.7927, acc 0.578125, prec 0.0183028, recall 0.616822\n",
      "2017-11-26T12:18:13.255881: step 99, loss 14.569, acc 0.53125, prec 0.0181568, recall 0.611111\n",
      "2017-11-26T12:18:13.941126: step 100, loss 3.00188, acc 0.5, prec 0.0182661, recall 0.614679\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:19:07.592412: step 100, loss 2.08106, acc 0.303265, prec 0.0185003, recall 0.821429\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-100\n",
      "\n",
      "2017-11-26T12:19:09.899667: step 101, loss 2.0344, acc 0.5625, prec 0.0185416, recall 0.822134\n",
      "2017-11-26T12:19:10.455810: step 102, loss 3.14318, acc 0.359375, prec 0.0184741, recall 0.822134\n",
      "2017-11-26T12:19:11.016174: step 103, loss 4.16319, acc 0.265625, prec 0.0184841, recall 0.822835\n",
      "2017-11-26T12:19:11.566549: step 104, loss 6.64235, acc 0.234375, prec 0.018406, recall 0.819608\n",
      "2017-11-26T12:19:12.105206: step 105, loss 5.17267, acc 0.21875, prec 0.0183253, recall 0.819608\n",
      "2017-11-26T12:19:12.647680: step 106, loss 4.28471, acc 0.296875, prec 0.018339, recall 0.820312\n",
      "2017-11-26T12:19:13.195211: step 107, loss 4.9389, acc 0.234375, prec 0.0184316, recall 0.821705\n",
      "2017-11-26T12:19:13.733063: step 108, loss 5.60187, acc 0.265625, prec 0.0183582, recall 0.818533\n",
      "2017-11-26T12:19:14.277287: step 109, loss 4.79594, acc 0.265625, prec 0.0183684, recall 0.819231\n",
      "2017-11-26T12:19:14.829551: step 110, loss 4.82964, acc 0.296875, prec 0.0182974, recall 0.819231\n",
      "2017-11-26T12:19:15.389860: step 111, loss 3.91397, acc 0.3125, prec 0.0182285, recall 0.819231\n",
      "2017-11-26T12:19:15.944281: step 112, loss 3.87659, acc 0.234375, prec 0.0181524, recall 0.819231\n",
      "2017-11-26T12:19:16.488841: step 113, loss 3.45418, acc 0.40625, prec 0.0181772, recall 0.819923\n",
      "2017-11-26T12:19:17.036584: step 114, loss 7.56422, acc 0.375, prec 0.0182834, recall 0.818182\n",
      "2017-11-26T12:19:17.608540: step 115, loss 5.0448, acc 0.359375, prec 0.0183045, recall 0.815789\n",
      "2017-11-26T12:19:18.170036: step 116, loss 3.0822, acc 0.4375, prec 0.0183317, recall 0.816479\n",
      "2017-11-26T12:19:18.732617: step 117, loss 3.24274, acc 0.359375, prec 0.0182687, recall 0.816479\n",
      "2017-11-26T12:19:19.310906: step 118, loss 11.2538, acc 0.28125, prec 0.018282, recall 0.814126\n",
      "2017-11-26T12:19:19.864155: step 119, loss 3.29203, acc 0.40625, prec 0.0182242, recall 0.814126\n",
      "2017-11-26T12:19:20.472197: step 120, loss 3.06828, acc 0.375, prec 0.0184893, recall 0.81685\n",
      "2017-11-26T12:19:21.037905: step 121, loss 2.1669, acc 0.484375, prec 0.0184389, recall 0.81685\n",
      "2017-11-26T12:19:21.600683: step 122, loss 4.05285, acc 0.578125, prec 0.0183993, recall 0.813869\n",
      "2017-11-26T12:19:22.161356: step 123, loss 2.20346, acc 0.484375, prec 0.0183494, recall 0.813869\n",
      "2017-11-26T12:19:22.724903: step 124, loss 8.90236, acc 0.546875, prec 0.0183072, recall 0.810909\n",
      "2017-11-26T12:19:23.285109: step 125, loss 2.45342, acc 0.5, prec 0.0182592, recall 0.810909\n",
      "2017-11-26T12:19:23.836938: step 126, loss 2.21711, acc 0.53125, prec 0.0182145, recall 0.810909\n",
      "2017-11-26T12:19:24.395977: step 127, loss 2.4345, acc 0.46875, prec 0.018164, recall 0.810909\n",
      "2017-11-26T12:19:24.956279: step 128, loss 1.51969, acc 0.59375, prec 0.0181257, recall 0.810909\n",
      "2017-11-26T12:19:25.513126: step 129, loss 1.30484, acc 0.6875, prec 0.0180962, recall 0.810909\n",
      "2017-11-26T12:19:26.080384: step 130, loss 5.38545, acc 0.546875, prec 0.0180567, recall 0.805054\n",
      "2017-11-26T12:19:26.640153: step 131, loss 2.11594, acc 0.578125, prec 0.0180173, recall 0.805054\n",
      "2017-11-26T12:19:27.195856: step 132, loss 1.49879, acc 0.671875, prec 0.0179868, recall 0.805054\n",
      "2017-11-26T12:19:27.744759: step 133, loss 2.58767, acc 0.75, prec 0.017965, recall 0.802158\n",
      "2017-11-26T12:19:28.297370: step 134, loss 5.8762, acc 0.515625, prec 0.0180006, recall 0.8\n",
      "2017-11-26T12:19:28.858111: step 135, loss 13.031, acc 0.5625, prec 0.0181191, recall 0.798587\n",
      "2017-11-26T12:19:29.433600: step 136, loss 18.4059, acc 0.640625, prec 0.0182444, recall 0.797203\n",
      "2017-11-26T12:19:30.023289: step 137, loss 5.76351, acc 0.5625, prec 0.018205, recall 0.794425\n",
      "2017-11-26T12:19:30.635725: step 138, loss 2.73917, acc 0.4375, prec 0.0181529, recall 0.794425\n",
      "2017-11-26T12:19:31.211297: step 139, loss 3.07861, acc 0.375, prec 0.0180952, recall 0.794425\n",
      "2017-11-26T12:19:31.800998: step 140, loss 3.80347, acc 0.5625, prec 0.0180565, recall 0.791667\n",
      "2017-11-26T12:19:32.428862: step 141, loss 3.69326, acc 0.375, prec 0.018077, recall 0.792388\n",
      "2017-11-26T12:19:33.035072: step 142, loss 9.32502, acc 0.421875, prec 0.0181031, recall 0.790378\n",
      "2017-11-26T12:19:33.640927: step 143, loss 3.75751, acc 0.375, prec 0.0181233, recall 0.791096\n",
      "2017-11-26T12:19:34.252770: step 144, loss 4.09059, acc 0.28125, prec 0.0180582, recall 0.791096\n",
      "2017-11-26T12:19:34.866341: step 145, loss 3.10817, acc 0.34375, prec 0.0179991, recall 0.791096\n",
      "2017-11-26T12:19:35.473340: step 146, loss 4.32225, acc 0.34375, prec 0.0180166, recall 0.791809\n",
      "2017-11-26T12:19:36.091521: step 147, loss 5.16752, acc 0.203125, prec 0.0180215, recall 0.792517\n",
      "2017-11-26T12:19:36.679320: step 148, loss 4.21587, acc 0.296875, prec 0.017959, recall 0.792517\n",
      "2017-11-26T12:19:37.274462: step 149, loss 3.69395, acc 0.3125, prec 0.0178983, recall 0.792517\n",
      "2017-11-26T12:19:37.860652: step 150, loss 4.7522, acc 0.21875, prec 0.0179801, recall 0.793919\n",
      "2017-11-26T12:19:38.474799: step 151, loss 8.85084, acc 0.34375, prec 0.0179239, recall 0.791246\n",
      "2017-11-26T12:19:39.084180: step 152, loss 2.93059, acc 0.375, prec 0.017944, recall 0.791946\n",
      "2017-11-26T12:19:39.668670: step 153, loss 3.11406, acc 0.453125, prec 0.0178964, recall 0.791946\n",
      "2017-11-26T12:19:40.291563: step 154, loss 3.60222, acc 0.375, prec 0.0178423, recall 0.791946\n",
      "2017-11-26T12:19:40.880482: step 155, loss 3.94157, acc 0.4375, prec 0.017942, recall 0.793333\n",
      "2017-11-26T12:19:41.497316: step 156, loss 2.21725, acc 0.515625, prec 0.017974, recall 0.79402\n",
      "2017-11-26T12:19:42.107949: step 157, loss 2.81123, acc 0.390625, prec 0.0179214, recall 0.79402\n",
      "2017-11-26T12:19:42.708218: step 158, loss 2.93462, acc 0.5625, prec 0.0181043, recall 0.796053\n",
      "2017-11-26T12:19:43.309202: step 159, loss 2.23783, acc 0.5625, prec 0.0180664, recall 0.796053\n",
      "2017-11-26T12:19:43.921906: step 160, loss 1.2854, acc 0.75, prec 0.0180449, recall 0.796053\n",
      "2017-11-26T12:19:44.521033: step 161, loss 5.24872, acc 0.453125, prec 0.0181453, recall 0.794788\n",
      "2017-11-26T12:19:45.121427: step 162, loss 1.53374, acc 0.703125, prec 0.0181926, recall 0.795455\n",
      "2017-11-26T12:19:45.717926: step 163, loss 4.98526, acc 0.59375, prec 0.0181589, recall 0.79288\n",
      "2017-11-26T12:19:46.298975: step 164, loss 5.88442, acc 0.6875, prec 0.0181334, recall 0.790323\n",
      "2017-11-26T12:19:46.879010: step 165, loss 2.16141, acc 0.59375, prec 0.0181711, recall 0.790997\n",
      "2017-11-26T12:19:47.459801: step 166, loss 1.41161, acc 0.625, prec 0.0182113, recall 0.791667\n",
      "2017-11-26T12:19:48.035312: step 167, loss 5.79404, acc 0.5, prec 0.0181698, recall 0.789137\n",
      "2017-11-26T12:19:48.618299: step 168, loss 1.88596, acc 0.5625, prec 0.0182045, recall 0.789809\n",
      "2017-11-26T12:19:49.194902: step 169, loss 1.73758, acc 0.578125, prec 0.0181685, recall 0.789809\n",
      "2017-11-26T12:19:49.767160: step 170, loss 4.26201, acc 0.5625, prec 0.0182762, recall 0.788644\n",
      "2017-11-26T12:19:50.364811: step 171, loss 2.10393, acc 0.515625, prec 0.0183065, recall 0.789308\n",
      "2017-11-26T12:19:50.957192: step 172, loss 3.96923, acc 0.515625, prec 0.0182665, recall 0.786834\n",
      "2017-11-26T12:19:51.535125: step 173, loss 1.80434, acc 0.609375, prec 0.0182333, recall 0.786834\n",
      "2017-11-26T12:19:52.110200: step 174, loss 1.5594, acc 0.59375, prec 0.0182701, recall 0.7875\n",
      "2017-11-26T12:19:52.686701: step 175, loss 3.11787, acc 0.515625, prec 0.0183015, recall 0.785714\n",
      "2017-11-26T12:19:53.268943: step 176, loss 1.9492, acc 0.578125, prec 0.0182658, recall 0.785714\n",
      "2017-11-26T12:19:53.850843: step 177, loss 1.60617, acc 0.53125, prec 0.0182264, recall 0.785714\n",
      "2017-11-26T12:19:54.438373: step 178, loss 7.44225, acc 0.546875, prec 0.0182602, recall 0.783951\n",
      "2017-11-26T12:19:55.020847: step 179, loss 1.28953, acc 0.609375, prec 0.0182979, recall 0.784615\n",
      "2017-11-26T12:19:55.628736: step 180, loss 18.7853, acc 0.640625, prec 0.0182691, recall 0.782209\n",
      "2017-11-26T12:19:56.220396: step 181, loss 2.20274, acc 0.5625, prec 0.0182325, recall 0.782209\n",
      "2017-11-26T12:19:56.804354: step 182, loss 1.5333, acc 0.578125, prec 0.0181974, recall 0.782209\n",
      "2017-11-26T12:19:57.381905: step 183, loss 2.03472, acc 0.578125, prec 0.0181624, recall 0.782209\n",
      "2017-11-26T12:19:57.957034: step 184, loss 2.29308, acc 0.5, prec 0.0181211, recall 0.782209\n",
      "2017-11-26T12:19:58.537936: step 185, loss 2.00207, acc 0.609375, prec 0.018089, recall 0.782209\n",
      "2017-11-26T12:19:59.122439: step 186, loss 2.06783, acc 0.59375, prec 0.0181252, recall 0.782875\n",
      "2017-11-26T12:19:59.726345: step 187, loss 13.553, acc 0.609375, prec 0.0180944, recall 0.780488\n",
      "2017-11-26T12:20:00.350250: step 188, loss 1.25253, acc 0.71875, prec 0.0182793, recall 0.782477\n",
      "2017-11-26T12:20:00.973124: step 189, loss 1.53347, acc 0.65625, prec 0.0183202, recall 0.783133\n",
      "2017-11-26T12:20:01.581318: step 190, loss 1.90131, acc 0.578125, prec 0.0184235, recall 0.784431\n",
      "2017-11-26T12:20:02.185407: step 191, loss 1.33588, acc 0.609375, prec 0.0183911, recall 0.784431\n",
      "2017-11-26T12:20:02.785112: step 192, loss 1.81094, acc 0.578125, prec 0.0183563, recall 0.784431\n",
      "2017-11-26T12:20:03.387140: step 193, loss 2.30189, acc 0.640625, prec 0.0184641, recall 0.785714\n",
      "2017-11-26T12:20:03.989732: step 194, loss 1.17849, acc 0.65625, prec 0.0184358, recall 0.785714\n",
      "2017-11-26T12:20:04.598647: step 195, loss 2.24625, acc 0.65625, prec 0.0184759, recall 0.78635\n",
      "2017-11-26T12:20:05.202499: step 196, loss 1.84403, acc 0.640625, prec 0.0185147, recall 0.786982\n",
      "2017-11-26T12:20:05.812536: step 197, loss 1.36517, acc 0.640625, prec 0.0184851, recall 0.786982\n",
      "2017-11-26T12:20:06.423573: step 198, loss 0.622053, acc 0.8125, prec 0.0184697, recall 0.786982\n",
      "2017-11-26T12:20:07.031783: step 199, loss 0.802959, acc 0.71875, prec 0.0184466, recall 0.786982\n",
      "2017-11-26T12:20:07.637818: step 200, loss 14.1978, acc 0.765625, prec 0.018498, recall 0.782991\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:20:48.780326: step 200, loss 1.64523, acc 0.857049, prec 0.0219133, recall 0.72314\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-200\n",
      "\n",
      "2017-11-26T12:20:50.322193: step 201, loss 5.40313, acc 0.828125, prec 0.0218996, recall 0.721649\n",
      "2017-11-26T12:20:50.931631: step 202, loss 1.17353, acc 0.640625, prec 0.0218682, recall 0.721649\n",
      "2017-11-26T12:20:51.500427: step 203, loss 2.53245, acc 0.625, prec 0.0218368, recall 0.720165\n",
      "2017-11-26T12:20:52.108984: step 204, loss 1.11451, acc 0.609375, prec 0.0218028, recall 0.720165\n",
      "2017-11-26T12:20:52.735731: step 205, loss 2.96725, acc 0.625, prec 0.0218324, recall 0.719262\n",
      "2017-11-26T12:20:53.317586: step 206, loss 1.42314, acc 0.609375, prec 0.0217985, recall 0.719262\n",
      "2017-11-26T12:20:53.911636: step 207, loss 1.64274, acc 0.65625, prec 0.0218901, recall 0.720408\n",
      "2017-11-26T12:20:54.515016: step 208, loss 1.67477, acc 0.59375, prec 0.0218549, recall 0.720408\n",
      "2017-11-26T12:20:55.118831: step 209, loss 2.07421, acc 0.515625, prec 0.021813, recall 0.720408\n",
      "2017-11-26T12:20:55.727271: step 210, loss 2.22216, acc 0.5, prec 0.0218906, recall 0.721545\n",
      "2017-11-26T12:20:56.331824: step 211, loss 2.10406, acc 0.515625, prec 0.0218488, recall 0.721545\n",
      "2017-11-26T12:20:56.929283: step 212, loss 3.61021, acc 0.53125, prec 0.0218687, recall 0.72211\n",
      "2017-11-26T12:20:57.533022: step 213, loss 2.79195, acc 0.484375, prec 0.0218244, recall 0.72211\n",
      "2017-11-26T12:20:58.127209: step 214, loss 1.63824, acc 0.671875, prec 0.0217964, recall 0.72211\n",
      "2017-11-26T12:20:58.725545: step 215, loss 16.2695, acc 0.5625, prec 0.0218215, recall 0.719758\n",
      "2017-11-26T12:20:59.332517: step 216, loss 2.34332, acc 0.515625, prec 0.0217802, recall 0.719758\n",
      "2017-11-26T12:20:59.935784: step 217, loss 1.89698, acc 0.5625, prec 0.0218027, recall 0.720322\n",
      "2017-11-26T12:21:00.534114: step 218, loss 3.57685, acc 0.5625, prec 0.021944, recall 0.722\n",
      "2017-11-26T12:21:01.167844: step 219, loss 4.38767, acc 0.375, prec 0.0218921, recall 0.720559\n",
      "2017-11-26T12:21:01.776949: step 220, loss 2.27316, acc 0.4375, prec 0.0219036, recall 0.721116\n",
      "2017-11-26T12:21:02.385725: step 221, loss 2.68083, acc 0.515625, prec 0.0219216, recall 0.72167\n",
      "2017-11-26T12:21:02.998863: step 222, loss 2.37249, acc 0.484375, prec 0.021937, recall 0.722222\n",
      "2017-11-26T12:21:03.612714: step 223, loss 2.93138, acc 0.46875, prec 0.0218921, recall 0.722222\n",
      "2017-11-26T12:21:04.215984: step 224, loss 2.64513, acc 0.46875, prec 0.0219061, recall 0.722772\n",
      "2017-11-26T12:21:04.859077: step 225, loss 2.84007, acc 0.421875, prec 0.0219162, recall 0.72332\n",
      "2017-11-26T12:21:05.463857: step 226, loss 3.09985, acc 0.609375, prec 0.0218847, recall 0.721893\n",
      "2017-11-26T12:21:06.081244: step 227, loss 1.89706, acc 0.5, prec 0.0218429, recall 0.721893\n",
      "2017-11-26T12:21:06.691223: step 228, loss 5.77298, acc 0.625, prec 0.021813, recall 0.720472\n",
      "2017-11-26T12:21:07.293240: step 229, loss 3.84159, acc 0.53125, prec 0.0217753, recall 0.719057\n",
      "2017-11-26T12:21:07.915581: step 230, loss 1.937, acc 0.5625, prec 0.0217391, recall 0.719057\n",
      "2017-11-26T12:21:08.510761: step 231, loss 2.20327, acc 0.578125, prec 0.0217043, recall 0.719057\n",
      "2017-11-26T12:21:09.109294: step 232, loss 8.37547, acc 0.515625, prec 0.021725, recall 0.716797\n",
      "2017-11-26T12:21:09.754463: step 233, loss 1.68741, acc 0.640625, prec 0.0218689, recall 0.718447\n",
      "2017-11-26T12:21:10.349957: step 234, loss 2.42264, acc 0.484375, prec 0.021884, recall 0.718992\n",
      "2017-11-26T12:21:11.094760: step 235, loss 2.17237, acc 0.53125, prec 0.0218454, recall 0.718992\n",
      "2017-11-26T12:21:11.870091: step 236, loss 2.16551, acc 0.5625, prec 0.0218094, recall 0.718992\n",
      "2017-11-26T12:21:12.640329: step 237, loss 2.68015, acc 0.4375, prec 0.0217634, recall 0.718992\n",
      "2017-11-26T12:21:13.396641: step 238, loss 1.53752, acc 0.59375, prec 0.0217302, recall 0.718992\n",
      "2017-11-26T12:21:14.226177: step 239, loss 1.74042, acc 0.53125, prec 0.0217493, recall 0.719536\n",
      "2017-11-26T12:21:15.200099: step 240, loss 10.9413, acc 0.609375, prec 0.0217188, recall 0.718147\n",
      "2017-11-26T12:21:16.190464: step 241, loss 3.89017, acc 0.578125, prec 0.0216859, recall 0.716763\n",
      "2017-11-26T12:21:17.142259: step 242, loss 1.52921, acc 0.640625, prec 0.0216569, recall 0.716763\n",
      "2017-11-26T12:21:18.155416: step 243, loss 1.88002, acc 0.5625, prec 0.0216216, recall 0.716763\n",
      "2017-11-26T12:21:19.328127: step 244, loss 1.73918, acc 0.578125, prec 0.0215877, recall 0.716763\n",
      "2017-11-26T12:21:20.476952: step 245, loss 22.6011, acc 0.671875, prec 0.021564, recall 0.714011\n",
      "2017-11-26T12:21:21.885990: step 246, loss 2.11597, acc 0.484375, prec 0.0215794, recall 0.714559\n",
      "2017-11-26T12:21:23.085977: step 247, loss 1.78544, acc 0.578125, prec 0.0216588, recall 0.715649\n",
      "2017-11-26T12:21:24.184969: step 248, loss 2.50124, acc 0.4375, prec 0.0216702, recall 0.71619\n",
      "2017-11-26T12:21:25.387698: step 249, loss 4.91119, acc 0.546875, prec 0.0216353, recall 0.714829\n",
      "2017-11-26T12:21:26.415900: step 250, loss 5.45029, acc 0.5, prec 0.0215968, recall 0.713472\n",
      "2017-11-26T12:21:27.456146: step 251, loss 2.45655, acc 0.4375, prec 0.0215522, recall 0.713472\n",
      "2017-11-26T12:21:28.490778: step 252, loss 3.03656, acc 0.46875, prec 0.0215103, recall 0.713472\n",
      "2017-11-26T12:21:29.388503: step 253, loss 3.38222, acc 0.40625, prec 0.0214636, recall 0.713472\n",
      "2017-11-26T12:21:30.278378: step 254, loss 3.10749, acc 0.515625, prec 0.0214257, recall 0.713472\n",
      "2017-11-26T12:21:31.022082: step 255, loss 2.31106, acc 0.515625, prec 0.0214436, recall 0.714015\n",
      "2017-11-26T12:21:31.770210: step 256, loss 3.48296, acc 0.5, prec 0.0214614, recall 0.713208\n",
      "2017-11-26T12:21:32.486974: step 257, loss 1.87599, acc 0.625, prec 0.0215432, recall 0.714286\n",
      "2017-11-26T12:21:33.187313: step 258, loss 6.0904, acc 0.546875, prec 0.021509, recall 0.712946\n",
      "2017-11-26T12:21:33.882841: step 259, loss 2.46511, acc 0.53125, prec 0.0214726, recall 0.712946\n",
      "2017-11-26T12:21:34.555777: step 260, loss 1.94764, acc 0.578125, prec 0.0214399, recall 0.712946\n",
      "2017-11-26T12:21:35.226906: step 261, loss 3.26636, acc 0.46875, prec 0.0214539, recall 0.713483\n",
      "2017-11-26T12:21:35.899129: step 262, loss 7.05264, acc 0.453125, prec 0.0214129, recall 0.71215\n",
      "2017-11-26T12:21:36.539099: step 263, loss 2.52762, acc 0.421875, prec 0.0213685, recall 0.71215\n",
      "2017-11-26T12:21:37.197755: step 264, loss 2.57168, acc 0.484375, prec 0.0213838, recall 0.712687\n",
      "2017-11-26T12:21:37.880073: step 265, loss 1.8631, acc 0.53125, prec 0.0214026, recall 0.713222\n",
      "2017-11-26T12:21:38.568244: step 266, loss 1.73146, acc 0.578125, prec 0.021425, recall 0.713755\n",
      "2017-11-26T12:21:39.282409: step 267, loss 9.35486, acc 0.5625, prec 0.0214485, recall 0.711645\n",
      "2017-11-26T12:21:40.039608: step 268, loss 1.72251, acc 0.46875, prec 0.0214079, recall 0.711645\n",
      "2017-11-26T12:21:40.806102: step 269, loss 2.16399, acc 0.5625, prec 0.0213746, recall 0.711645\n",
      "2017-11-26T12:21:41.645923: step 270, loss 1.94154, acc 0.5625, prec 0.0213957, recall 0.712177\n",
      "2017-11-26T12:21:42.519270: step 271, loss 1.22976, acc 0.65625, prec 0.0214238, recall 0.712707\n",
      "2017-11-26T12:21:43.361304: step 272, loss 11.2275, acc 0.609375, prec 0.0214507, recall 0.710623\n",
      "2017-11-26T12:21:44.268263: step 273, loss 7.9006, acc 0.5625, prec 0.0214727, recall 0.709854\n",
      "2017-11-26T12:21:45.065722: step 274, loss 1.84568, acc 0.625, prec 0.0214443, recall 0.709854\n",
      "2017-11-26T12:21:45.823743: step 275, loss 2.41604, acc 0.4375, prec 0.0215633, recall 0.711434\n",
      "2017-11-26T12:21:46.542637: step 276, loss 2.42161, acc 0.5, prec 0.0215792, recall 0.711957\n",
      "2017-11-26T12:21:47.249747: step 277, loss 2.48687, acc 0.484375, prec 0.0215938, recall 0.712477\n",
      "2017-11-26T12:21:47.936654: step 278, loss 2.52122, acc 0.4375, prec 0.0216048, recall 0.712996\n",
      "2017-11-26T12:21:48.599303: step 279, loss 2.71107, acc 0.40625, prec 0.02156, recall 0.712996\n",
      "2017-11-26T12:21:49.271759: step 280, loss 2.2211, acc 0.5625, prec 0.0215271, recall 0.712996\n",
      "2017-11-26T12:21:49.929610: step 281, loss 2.35947, acc 0.609375, prec 0.0216575, recall 0.714542\n",
      "2017-11-26T12:21:50.601389: step 282, loss 1.23027, acc 0.609375, prec 0.0216281, recall 0.714542\n",
      "2017-11-26T12:21:51.299570: step 283, loss 7.43899, acc 0.65625, prec 0.0216565, recall 0.713775\n",
      "2017-11-26T12:21:51.950212: step 284, loss 12.924, acc 0.5, prec 0.0216743, recall 0.711744\n",
      "2017-11-26T12:21:52.590487: step 285, loss 1.7329, acc 0.671875, prec 0.0216497, recall 0.711744\n",
      "2017-11-26T12:21:53.217053: step 286, loss 1.76139, acc 0.53125, prec 0.0216146, recall 0.711744\n",
      "2017-11-26T12:21:53.949676: step 287, loss 1.75365, acc 0.546875, prec 0.0216336, recall 0.712256\n",
      "2017-11-26T12:21:54.729316: step 288, loss 2.02642, acc 0.609375, prec 0.0216044, recall 0.712256\n",
      "2017-11-26T12:21:55.426168: step 289, loss 1.72589, acc 0.640625, prec 0.0215777, recall 0.712256\n",
      "2017-11-26T12:21:56.129501: step 290, loss 1.52781, acc 0.609375, prec 0.0216539, recall 0.713274\n",
      "2017-11-26T12:21:56.835209: step 291, loss 12.8221, acc 0.625, prec 0.0216796, recall 0.712522\n",
      "2017-11-26T12:21:57.525147: step 292, loss 2.28904, acc 0.609375, prec 0.0216506, recall 0.712522\n",
      "2017-11-26T12:21:58.206601: step 293, loss 1.34383, acc 0.6875, prec 0.0216798, recall 0.713028\n",
      "2017-11-26T12:21:58.877608: step 294, loss 2.58778, acc 0.53125, prec 0.0216973, recall 0.713533\n",
      "2017-11-26T12:21:59.550103: step 295, loss 20.1678, acc 0.453125, prec 0.0216603, recall 0.70979\n",
      "2017-11-26T12:22:00.201699: step 296, loss 2.37698, acc 0.484375, prec 0.0216222, recall 0.70979\n",
      "2017-11-26T12:22:00.849251: step 297, loss 3.23691, acc 0.46875, prec 0.0216351, recall 0.710297\n",
      "2017-11-26T12:22:01.525803: step 298, loss 2.75254, acc 0.46875, prec 0.0215961, recall 0.710297\n",
      "2017-11-26T12:22:02.181322: step 299, loss 3.19538, acc 0.421875, prec 0.0215538, recall 0.710297\n",
      "2017-11-26T12:22:02.836719: step 300, loss 2.51415, acc 0.453125, prec 0.0215139, recall 0.710297\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:23:06.761058: step 300, loss 2.00426, acc 0.453199, prec 0.0220121, recall 0.763967\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-300\n",
      "\n",
      "2017-11-26T12:23:10.207279: step 301, loss 3.23515, acc 0.46875, prec 0.0220213, recall 0.764296\n",
      "2017-11-26T12:23:11.123747: step 302, loss 2.705, acc 0.46875, prec 0.0221482, recall 0.765603\n",
      "2017-11-26T12:23:12.550583: step 303, loss 2.58709, acc 0.453125, prec 0.0222347, recall 0.766575\n",
      "2017-11-26T12:23:13.895545: step 304, loss 2.88509, acc 0.453125, prec 0.0222427, recall 0.766897\n",
      "2017-11-26T12:23:15.020204: step 305, loss 2.07485, acc 0.59375, prec 0.0222586, recall 0.767218\n",
      "2017-11-26T12:23:15.987788: step 306, loss 16.078, acc 0.546875, prec 0.0222728, recall 0.766484\n",
      "2017-11-26T12:23:17.396471: step 307, loss 2.0628, acc 0.515625, prec 0.0222842, recall 0.766804\n",
      "2017-11-26T12:23:18.718894: step 308, loss 2.22026, acc 0.640625, prec 0.0223028, recall 0.767123\n",
      "2017-11-26T12:23:19.976587: step 309, loss 1.57911, acc 0.578125, prec 0.0222788, recall 0.767123\n",
      "2017-11-26T12:23:20.902517: step 310, loss 1.53331, acc 0.59375, prec 0.0222558, recall 0.767123\n",
      "2017-11-26T12:23:21.841759: step 311, loss 5.16572, acc 0.640625, prec 0.0222363, recall 0.766074\n",
      "2017-11-26T12:23:22.743353: step 312, loss 1.47605, acc 0.65625, prec 0.0222557, recall 0.766393\n",
      "2017-11-26T12:23:23.755824: step 313, loss 1.40571, acc 0.578125, prec 0.0222319, recall 0.766393\n",
      "2017-11-26T12:23:25.205369: step 314, loss 7.45113, acc 0.671875, prec 0.022253, recall 0.765668\n",
      "2017-11-26T12:23:26.366083: step 315, loss 3.83945, acc 0.625, prec 0.0222328, recall 0.764626\n",
      "2017-11-26T12:23:27.271267: step 316, loss 1.58501, acc 0.65625, prec 0.0222134, recall 0.764626\n",
      "2017-11-26T12:23:28.199648: step 317, loss 1.48287, acc 0.609375, prec 0.0221915, recall 0.764626\n",
      "2017-11-26T12:23:29.188342: step 318, loss 1.71866, acc 0.6875, prec 0.0222512, recall 0.765265\n",
      "2017-11-26T12:23:30.120305: step 319, loss 1.03049, acc 0.65625, prec 0.0222704, recall 0.765583\n",
      "2017-11-26T12:23:31.058036: step 320, loss 1.3265, acc 0.703125, prec 0.0222922, recall 0.7659\n",
      "2017-11-26T12:23:31.969947: step 321, loss 1.44027, acc 0.640625, prec 0.0222721, recall 0.7659\n",
      "2017-11-26T12:23:32.853286: step 322, loss 1.72876, acc 0.6875, prec 0.0222546, recall 0.7659\n",
      "2017-11-26T12:23:33.695545: step 323, loss 0.880676, acc 0.734375, prec 0.0222397, recall 0.7659\n",
      "2017-11-26T12:23:34.490088: step 324, loss 1.41641, acc 0.65625, prec 0.0222205, recall 0.7659\n",
      "2017-11-26T12:23:35.277827: step 325, loss 3.08941, acc 0.671875, prec 0.022203, recall 0.764865\n",
      "2017-11-26T12:23:36.052028: step 326, loss 0.819557, acc 0.671875, prec 0.0221848, recall 0.764865\n",
      "2017-11-26T12:23:36.798014: step 327, loss 8.32513, acc 0.671875, prec 0.0221674, recall 0.763833\n",
      "2017-11-26T12:23:37.586843: step 328, loss 2.94546, acc 0.640625, prec 0.0221483, recall 0.762803\n",
      "2017-11-26T12:23:38.425021: step 329, loss 1.11117, acc 0.65625, prec 0.0221293, recall 0.762803\n",
      "2017-11-26T12:23:39.203236: step 330, loss 1.25698, acc 0.703125, prec 0.0221128, recall 0.762803\n",
      "2017-11-26T12:23:40.015545: step 331, loss 1.64471, acc 0.65625, prec 0.0220938, recall 0.762803\n",
      "2017-11-26T12:23:40.867185: step 332, loss 2.24755, acc 0.5625, prec 0.022146, recall 0.763441\n",
      "2017-11-26T12:23:41.740843: step 333, loss 2.47741, acc 0.625, prec 0.0221261, recall 0.762416\n",
      "2017-11-26T12:23:42.627234: step 334, loss 1.77432, acc 0.5625, prec 0.022102, recall 0.762416\n",
      "2017-11-26T12:23:43.568320: step 335, loss 1.62947, acc 0.609375, prec 0.0221186, recall 0.762735\n",
      "2017-11-26T12:23:44.491757: step 336, loss 1.37858, acc 0.671875, prec 0.0221765, recall 0.763369\n",
      "2017-11-26T12:23:45.417072: step 337, loss 1.61275, acc 0.625, prec 0.0221938, recall 0.763685\n",
      "2017-11-26T12:23:46.340759: step 338, loss 12.5109, acc 0.75, prec 0.0221817, recall 0.761651\n",
      "2017-11-26T12:23:47.280749: step 339, loss 1.56413, acc 0.71875, prec 0.0222041, recall 0.761968\n",
      "2017-11-26T12:23:48.210548: step 340, loss 1.50598, acc 0.59375, prec 0.0221818, recall 0.761968\n",
      "2017-11-26T12:23:49.208440: step 341, loss 8.73681, acc 0.6875, prec 0.0221655, recall 0.760956\n",
      "2017-11-26T12:23:50.135187: step 342, loss 1.76031, acc 0.578125, prec 0.0222179, recall 0.761589\n",
      "2017-11-26T12:23:51.029452: step 343, loss 5.66064, acc 0.609375, prec 0.0223106, recall 0.761528\n",
      "2017-11-26T12:23:51.883321: step 344, loss 2.58074, acc 0.46875, prec 0.0222813, recall 0.761528\n",
      "2017-11-26T12:23:52.697893: step 345, loss 1.87123, acc 0.5625, prec 0.022295, recall 0.761842\n",
      "2017-11-26T12:23:53.521512: step 346, loss 1.99735, acc 0.578125, prec 0.0222718, recall 0.761842\n",
      "2017-11-26T12:23:54.364601: step 347, loss 2.55283, acc 0.546875, prec 0.0223221, recall 0.762467\n",
      "2017-11-26T12:23:55.204389: step 348, loss 2.92693, acc 0.578125, prec 0.0223365, recall 0.762779\n",
      "2017-11-26T12:23:56.069169: step 349, loss 2.63731, acc 0.46875, prec 0.0223074, recall 0.762779\n",
      "2017-11-26T12:23:57.010816: step 350, loss 3.00118, acc 0.421875, prec 0.0223506, recall 0.763399\n",
      "2017-11-26T12:23:57.921353: step 351, loss 2.4316, acc 0.5, prec 0.0223233, recall 0.763399\n",
      "2017-11-26T12:23:58.789630: step 352, loss 2.51268, acc 0.453125, prec 0.0222935, recall 0.763399\n",
      "2017-11-26T12:23:59.646108: step 353, loss 2.10049, acc 0.484375, prec 0.02234, recall 0.764016\n",
      "2017-11-26T12:24:00.467063: step 354, loss 1.63074, acc 0.546875, prec 0.0223153, recall 0.764016\n",
      "2017-11-26T12:24:01.301627: step 355, loss 2.07244, acc 0.5625, prec 0.0223287, recall 0.764323\n",
      "2017-11-26T12:24:02.145594: step 356, loss 2.37231, acc 0.53125, prec 0.0223404, recall 0.764629\n",
      "2017-11-26T12:24:03.034201: step 357, loss 2.00956, acc 0.546875, prec 0.0223529, recall 0.764935\n",
      "2017-11-26T12:24:03.886913: step 358, loss 2.03987, acc 0.5625, prec 0.0223292, recall 0.764935\n",
      "2017-11-26T12:24:04.747266: step 359, loss 6.17901, acc 0.640625, prec 0.0223476, recall 0.764249\n",
      "2017-11-26T12:24:05.702917: step 360, loss 5.17779, acc 0.65625, prec 0.0223669, recall 0.763566\n",
      "2017-11-26T12:24:06.664700: step 361, loss 0.988053, acc 0.75, prec 0.0223533, recall 0.763566\n",
      "2017-11-26T12:24:07.673109: step 362, loss 0.707776, acc 0.78125, prec 0.0223415, recall 0.763566\n",
      "2017-11-26T12:24:08.724741: step 363, loss 1.14441, acc 0.703125, prec 0.0223255, recall 0.763566\n",
      "2017-11-26T12:24:09.667094: step 364, loss 1.26761, acc 0.625, prec 0.0223053, recall 0.763566\n",
      "2017-11-26T12:24:10.569357: step 365, loss 1.1572, acc 0.75, prec 0.0222918, recall 0.763566\n",
      "2017-11-26T12:24:11.445413: step 366, loss 1.17237, acc 0.765625, prec 0.0222792, recall 0.763566\n",
      "2017-11-26T12:24:12.321428: step 367, loss 0.866893, acc 0.75, prec 0.0222658, recall 0.763566\n",
      "2017-11-26T12:24:13.214184: step 368, loss 8.80494, acc 0.78125, prec 0.0222917, recall 0.762887\n",
      "2017-11-26T12:24:14.096666: step 369, loss 1.27764, acc 0.765625, prec 0.0223159, recall 0.763192\n",
      "2017-11-26T12:24:14.977824: step 370, loss 0.625762, acc 0.859375, prec 0.0223451, recall 0.763496\n",
      "2017-11-26T12:24:15.865214: step 371, loss 1.17607, acc 0.703125, prec 0.0223291, recall 0.763496\n",
      "2017-11-26T12:24:16.823558: step 372, loss 2.60154, acc 0.625, prec 0.0223099, recall 0.762516\n",
      "2017-11-26T12:24:17.713222: step 373, loss 1.32524, acc 0.703125, prec 0.0222939, recall 0.762516\n",
      "2017-11-26T12:24:18.601914: step 374, loss 0.970682, acc 0.625, prec 0.0223105, recall 0.762821\n",
      "2017-11-26T12:24:19.564349: step 375, loss 8.87466, acc 0.734375, prec 0.0223338, recall 0.762148\n",
      "2017-11-26T12:24:20.452995: step 376, loss 1.20378, acc 0.734375, prec 0.0223196, recall 0.762148\n",
      "2017-11-26T12:24:21.333055: step 377, loss 0.972284, acc 0.734375, prec 0.022342, recall 0.762452\n",
      "2017-11-26T12:24:22.214391: step 378, loss 1.81989, acc 0.609375, prec 0.0223211, recall 0.762452\n",
      "2017-11-26T12:24:23.084844: step 379, loss 1.30911, acc 0.6875, prec 0.0223044, recall 0.762452\n",
      "2017-11-26T12:24:23.952271: step 380, loss 1.99725, acc 0.546875, prec 0.0223168, recall 0.762755\n",
      "2017-11-26T12:24:24.823399: step 381, loss 1.50083, acc 0.734375, prec 0.0223026, recall 0.762755\n",
      "2017-11-26T12:24:25.720693: step 382, loss 1.20093, acc 0.75, prec 0.0222893, recall 0.762755\n",
      "2017-11-26T12:24:26.602211: step 383, loss 0.967896, acc 0.671875, prec 0.0222719, recall 0.762755\n",
      "2017-11-26T12:24:27.506299: step 384, loss 3.86054, acc 0.625, prec 0.0222528, recall 0.761783\n",
      "2017-11-26T12:24:28.460327: step 385, loss 18.9722, acc 0.703125, prec 0.0222379, recall 0.760814\n",
      "2017-11-26T12:24:29.466075: step 386, loss 1.15284, acc 0.734375, prec 0.0222602, recall 0.761118\n",
      "2017-11-26T12:24:30.412386: step 387, loss 1.41782, acc 0.578125, prec 0.0222379, recall 0.761118\n",
      "2017-11-26T12:24:31.353827: step 388, loss 1.21933, acc 0.6875, prec 0.0222577, recall 0.761421\n",
      "2017-11-26T12:24:32.288072: step 389, loss 1.51527, acc 0.640625, prec 0.0222749, recall 0.761724\n",
      "2017-11-26T12:24:33.218809: step 390, loss 1.58811, acc 0.609375, prec 0.0222543, recall 0.761724\n",
      "2017-11-26T12:24:34.151795: step 391, loss 1.86296, acc 0.703125, prec 0.0223472, recall 0.762626\n",
      "2017-11-26T12:24:35.033623: step 392, loss 1.07927, acc 0.65625, prec 0.022329, recall 0.762626\n",
      "2017-11-26T12:24:35.934535: step 393, loss 11.9415, acc 0.734375, prec 0.0223166, recall 0.760705\n",
      "2017-11-26T12:24:36.804161: step 394, loss 0.976948, acc 0.703125, prec 0.022301, recall 0.760705\n",
      "2017-11-26T12:24:37.648021: step 395, loss 1.7585, acc 0.625, prec 0.0222812, recall 0.760705\n",
      "2017-11-26T12:24:38.584118: step 396, loss 4.27872, acc 0.640625, prec 0.0222992, recall 0.76005\n",
      "2017-11-26T12:24:39.333537: step 397, loss 1.41503, acc 0.53125, prec 0.0222746, recall 0.76005\n",
      "2017-11-26T12:24:40.015799: step 398, loss 2.12035, acc 0.703125, prec 0.022259, recall 0.76005\n",
      "2017-11-26T12:24:40.637755: step 399, loss 2.1871, acc 0.515625, prec 0.0222696, recall 0.760351\n",
      "2017-11-26T12:24:41.230621: step 400, loss 3.80271, acc 0.609375, prec 0.0222859, recall 0.7597\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:25:45.590686: step 400, loss 1.50783, acc 0.594452, prec 0.0234952, recall 0.789809\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-400\n",
      "\n",
      "2017-11-26T12:25:48.217664: step 401, loss 2.95193, acc 0.5625, prec 0.0235984, recall 0.789863\n",
      "2017-11-26T12:25:49.085595: step 402, loss 4.23889, acc 0.5, prec 0.0236062, recall 0.789252\n",
      "2017-11-26T12:25:49.978839: step 403, loss 2.36143, acc 0.515625, prec 0.0235831, recall 0.789252\n",
      "2017-11-26T12:25:50.828666: step 404, loss 2.24241, acc 0.5, prec 0.0235901, recall 0.789474\n",
      "2017-11-26T12:25:51.678457: step 405, loss 2.36897, acc 0.484375, prec 0.0235963, recall 0.789695\n",
      "2017-11-26T12:25:52.529258: step 406, loss 3.35067, acc 0.453125, prec 0.0235704, recall 0.789695\n",
      "2017-11-26T12:25:53.386275: step 407, loss 2.84022, acc 0.4375, prec 0.023605, recall 0.790136\n",
      "2017-11-26T12:25:54.209383: step 408, loss 4.03932, acc 0.4375, prec 0.0235791, recall 0.789308\n",
      "2017-11-26T12:25:55.078998: step 409, loss 2.53541, acc 0.46875, prec 0.0236152, recall 0.789749\n",
      "2017-11-26T12:25:55.912459: step 410, loss 13.4116, acc 0.609375, prec 0.023628, recall 0.789144\n",
      "2017-11-26T12:25:56.759586: step 411, loss 1.55543, acc 0.5625, prec 0.0236073, recall 0.789144\n",
      "2017-11-26T12:25:57.621877: step 412, loss 2.09962, acc 0.546875, prec 0.0235859, recall 0.789144\n",
      "2017-11-26T12:25:58.586941: step 413, loss 1.85992, acc 0.46875, prec 0.0235914, recall 0.789364\n",
      "2017-11-26T12:25:59.583594: step 414, loss 1.91524, acc 0.53125, prec 0.0235997, recall 0.789583\n",
      "2017-11-26T12:26:00.657267: step 415, loss 1.83499, acc 0.5625, prec 0.0236096, recall 0.789802\n",
      "2017-11-26T12:26:01.830033: step 416, loss 3.97896, acc 0.5, prec 0.0235868, recall 0.788981\n",
      "2017-11-26T12:26:02.557562: step 417, loss 1.01034, acc 0.703125, prec 0.0235729, recall 0.788981\n",
      "2017-11-26T12:26:03.232305: step 418, loss 1.59729, acc 0.59375, prec 0.0235539, recall 0.788981\n",
      "2017-11-26T12:26:03.863650: step 419, loss 2.20662, acc 0.53125, prec 0.023532, recall 0.788981\n",
      "2017-11-26T12:26:04.472621: step 420, loss 1.87384, acc 0.609375, prec 0.023544, recall 0.7892\n",
      "2017-11-26T12:26:05.055659: step 421, loss 1.55493, acc 0.609375, prec 0.0235258, recall 0.7892\n",
      "2017-11-26T12:26:05.622261: step 422, loss 1.29495, acc 0.609375, prec 0.0235076, recall 0.7892\n",
      "2017-11-26T12:26:06.195439: step 423, loss 2.14177, acc 0.53125, prec 0.0234858, recall 0.7892\n",
      "2017-11-26T12:26:06.742321: step 424, loss 6.08306, acc 0.640625, prec 0.0234698, recall 0.788382\n",
      "2017-11-26T12:26:07.285990: step 425, loss 1.30532, acc 0.734375, prec 0.0234575, recall 0.788382\n",
      "2017-11-26T12:26:07.815610: step 426, loss 0.884553, acc 0.78125, prec 0.0235076, recall 0.78882\n",
      "2017-11-26T12:26:08.356145: step 427, loss 20.6841, acc 0.796875, prec 0.0234989, recall 0.788004\n",
      "2017-11-26T12:26:08.887743: step 428, loss 8.92779, acc 0.671875, prec 0.0234845, recall 0.78719\n",
      "2017-11-26T12:26:09.449178: step 429, loss 1.04517, acc 0.65625, prec 0.0234685, recall 0.78719\n",
      "2017-11-26T12:26:09.964614: step 430, loss 0.882167, acc 0.765625, prec 0.0234878, recall 0.78741\n",
      "2017-11-26T12:26:10.519803: step 431, loss 1.47702, acc 0.71875, prec 0.0235348, recall 0.787848\n",
      "2017-11-26T12:26:11.036515: step 432, loss 1.27601, acc 0.71875, prec 0.0235819, recall 0.788284\n",
      "2017-11-26T12:26:11.545656: step 433, loss 0.928822, acc 0.734375, prec 0.0235695, recall 0.788284\n",
      "2017-11-26T12:26:12.056804: step 434, loss 1.40172, acc 0.671875, prec 0.0235543, recall 0.788284\n",
      "2017-11-26T12:26:12.569222: step 435, loss 0.973168, acc 0.6875, prec 0.0235998, recall 0.788718\n",
      "2017-11-26T12:26:13.072905: step 436, loss 1.30129, acc 0.71875, prec 0.0236167, recall 0.788934\n",
      "2017-11-26T12:26:13.585056: step 437, loss 7.11138, acc 0.625, prec 0.02363, recall 0.788344\n",
      "2017-11-26T12:26:14.092575: step 438, loss 1.67588, acc 0.671875, prec 0.0236746, recall 0.788776\n",
      "2017-11-26T12:26:14.606828: step 439, loss 0.888127, acc 0.78125, prec 0.0236645, recall 0.788776\n",
      "2017-11-26T12:26:15.119351: step 440, loss 0.811266, acc 0.734375, prec 0.0236522, recall 0.788776\n",
      "2017-11-26T12:26:15.631094: step 441, loss 5.35346, acc 0.6875, prec 0.0236683, recall 0.788187\n",
      "2017-11-26T12:26:16.158974: step 442, loss 4.76099, acc 0.609375, prec 0.0236509, recall 0.787386\n",
      "2017-11-26T12:26:16.685729: step 443, loss 1.58558, acc 0.578125, prec 0.0236612, recall 0.787602\n",
      "2017-11-26T12:26:17.209368: step 444, loss 1.13065, acc 0.71875, prec 0.023678, recall 0.787817\n",
      "2017-11-26T12:26:17.743133: step 445, loss 1.16179, acc 0.640625, prec 0.0236614, recall 0.787817\n",
      "2017-11-26T12:26:18.331815: step 446, loss 1.1666, acc 0.6875, prec 0.0237362, recall 0.788462\n",
      "2017-11-26T12:26:18.893531: step 447, loss 2.4794, acc 0.65625, prec 0.0237508, recall 0.787879\n",
      "2017-11-26T12:26:19.658187: step 448, loss 1.58884, acc 0.671875, prec 0.0237356, recall 0.787879\n",
      "2017-11-26T12:26:20.296130: step 449, loss 1.64022, acc 0.640625, prec 0.0237784, recall 0.788306\n",
      "2017-11-26T12:26:20.894157: step 450, loss 1.41135, acc 0.578125, prec 0.0237589, recall 0.788306\n",
      "2017-11-26T12:26:21.475758: step 451, loss 18.6324, acc 0.578125, prec 0.0237698, recall 0.787726\n",
      "2017-11-26T12:26:22.186629: step 452, loss 1.8721, acc 0.640625, prec 0.0237828, recall 0.78794\n",
      "2017-11-26T12:26:22.850706: step 453, loss 1.88333, acc 0.515625, prec 0.02379, recall 0.788153\n",
      "2017-11-26T12:26:23.493591: step 454, loss 2.44489, acc 0.546875, prec 0.0238283, recall 0.788577\n",
      "2017-11-26T12:26:24.136036: step 455, loss 1.60512, acc 0.609375, prec 0.0238693, recall 0.789\n",
      "2017-11-26T12:26:24.768372: step 456, loss 2.3692, acc 0.671875, prec 0.0238837, recall 0.789211\n",
      "2017-11-26T12:26:25.407434: step 457, loss 2.0288, acc 0.578125, prec 0.0238937, recall 0.789421\n",
      "2017-11-26T12:26:26.053628: step 458, loss 1.27522, acc 0.6875, prec 0.0238792, recall 0.789421\n",
      "2017-11-26T12:26:26.723298: step 459, loss 7.78787, acc 0.59375, prec 0.0238612, recall 0.788634\n",
      "2017-11-26T12:26:27.423656: step 460, loss 8.32405, acc 0.671875, prec 0.0238476, recall 0.787065\n",
      "2017-11-26T12:26:28.085382: step 461, loss 11.9807, acc 0.546875, prec 0.0238275, recall 0.786282\n",
      "2017-11-26T12:26:28.729060: step 462, loss 2.89093, acc 0.5, prec 0.0238633, recall 0.786706\n",
      "2017-11-26T12:26:29.384855: step 463, loss 2.04003, acc 0.546875, prec 0.0238425, recall 0.786706\n",
      "2017-11-26T12:26:30.021521: step 464, loss 3.30315, acc 0.5, prec 0.0238489, recall 0.786918\n",
      "2017-11-26T12:26:30.680797: step 465, loss 3.24973, acc 0.34375, prec 0.0238774, recall 0.787339\n",
      "2017-11-26T12:26:31.423834: step 466, loss 3.53289, acc 0.4375, prec 0.0238809, recall 0.787549\n",
      "2017-11-26T12:26:32.100928: step 467, loss 3.4631, acc 0.46875, prec 0.0238566, recall 0.787549\n",
      "2017-11-26T12:26:32.714765: step 468, loss 3.52145, acc 0.421875, prec 0.0238885, recall 0.787968\n",
      "2017-11-26T12:26:33.324720: step 469, loss 2.70369, acc 0.5625, prec 0.023956, recall 0.788594\n",
      "2017-11-26T12:26:33.898315: step 470, loss 3.83144, acc 0.34375, prec 0.023926, recall 0.788594\n",
      "2017-11-26T12:26:34.490195: step 471, loss 2.54902, acc 0.5, prec 0.0239032, recall 0.788594\n",
      "2017-11-26T12:26:35.080831: step 472, loss 3.09405, acc 0.40625, prec 0.0239052, recall 0.788802\n",
      "2017-11-26T12:26:35.713267: step 473, loss 2.9058, acc 0.4375, prec 0.0239086, recall 0.789009\n",
      "2017-11-26T12:26:36.482667: step 474, loss 2.35758, acc 0.484375, prec 0.0239142, recall 0.789216\n",
      "2017-11-26T12:26:37.247214: step 475, loss 2.2802, acc 0.53125, prec 0.0239219, recall 0.789422\n",
      "2017-11-26T12:26:38.032039: step 476, loss 1.84305, acc 0.609375, prec 0.0239331, recall 0.789628\n",
      "2017-11-26T12:26:38.848557: step 477, loss 1.5925, acc 0.703125, prec 0.0239196, recall 0.789628\n",
      "2017-11-26T12:26:39.738075: step 478, loss 1.39167, acc 0.671875, prec 0.0239336, recall 0.789834\n",
      "2017-11-26T12:26:40.581117: step 479, loss 0.845867, acc 0.78125, prec 0.0239237, recall 0.789834\n",
      "2017-11-26T12:26:41.418093: step 480, loss 14.5792, acc 0.796875, prec 0.0240315, recall 0.789116\n",
      "2017-11-26T12:26:42.239662: step 481, loss 1.78645, acc 0.671875, prec 0.0240454, recall 0.78932\n",
      "2017-11-26T12:26:43.028171: step 482, loss 1.4402, acc 0.703125, prec 0.0240896, recall 0.789729\n",
      "2017-11-26T12:26:43.943127: step 483, loss 0.853116, acc 0.765625, prec 0.0240789, recall 0.789729\n",
      "2017-11-26T12:26:44.860849: step 484, loss 2.63041, acc 0.765625, prec 0.024069, recall 0.788964\n",
      "2017-11-26T12:26:45.734216: step 485, loss 1.79075, acc 0.734375, prec 0.0240857, recall 0.789168\n",
      "2017-11-26T12:26:46.628744: step 486, loss 1.10547, acc 0.734375, prec 0.02416, recall 0.789778\n",
      "2017-11-26T12:26:47.546725: step 487, loss 0.853572, acc 0.734375, prec 0.0241479, recall 0.789778\n",
      "2017-11-26T12:26:48.581450: step 488, loss 0.983981, acc 0.75, prec 0.0241365, recall 0.789778\n",
      "2017-11-26T12:26:49.866540: step 489, loss 1.17845, acc 0.6875, prec 0.0241223, recall 0.789778\n",
      "2017-11-26T12:26:51.288902: step 490, loss 3.69587, acc 0.703125, prec 0.0241382, recall 0.78922\n",
      "2017-11-26T12:26:52.535265: step 491, loss 1.44648, acc 0.703125, prec 0.0241822, recall 0.789625\n",
      "2017-11-26T12:26:53.402505: step 492, loss 7.10039, acc 0.515625, prec 0.0241895, recall 0.78907\n",
      "2017-11-26T12:26:54.292987: step 493, loss 1.70224, acc 0.703125, prec 0.024176, recall 0.78907\n",
      "2017-11-26T12:26:55.221801: step 494, loss 1.20603, acc 0.734375, prec 0.0241926, recall 0.789272\n",
      "2017-11-26T12:26:56.074701: step 495, loss 2.49494, acc 0.515625, prec 0.0241706, recall 0.789272\n",
      "2017-11-26T12:26:57.001834: step 496, loss 5.17176, acc 0.640625, prec 0.0241836, recall 0.788719\n",
      "2017-11-26T12:26:57.815071: step 497, loss 1.944, acc 0.596154, prec 0.0241973, recall 0.788921\n",
      "2017-11-26T12:26:58.729457: step 498, loss 1.20291, acc 0.703125, prec 0.024241, recall 0.789323\n",
      "2017-11-26T12:26:59.559274: step 499, loss 2.13882, acc 0.46875, prec 0.0242169, recall 0.789323\n",
      "2017-11-26T12:27:00.383170: step 500, loss 2.54244, acc 0.65625, prec 0.0242306, recall 0.788773\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:28:05.424838: step 500, loss 1.59533, acc 0.569258, prec 0.0248516, recall 0.809883\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-500\n",
      "\n",
      "2017-11-26T12:28:07.284240: step 501, loss 1.9706, acc 0.578125, prec 0.0248344, recall 0.809883\n",
      "2017-11-26T12:28:07.852932: step 502, loss 1.16767, acc 0.5625, prec 0.0248415, recall 0.810042\n",
      "2017-11-26T12:28:08.407085: step 503, loss 2.51096, acc 0.5, prec 0.0248211, recall 0.810042\n",
      "2017-11-26T12:28:08.960741: step 504, loss 1.17186, acc 0.640625, prec 0.0248065, recall 0.810042\n",
      "2017-11-26T12:28:09.511380: step 505, loss 1.75009, acc 0.640625, prec 0.0247919, recall 0.810042\n",
      "2017-11-26T12:28:10.084268: step 506, loss 7.73868, acc 0.546875, prec 0.0247997, recall 0.808848\n",
      "2017-11-26T12:28:10.649062: step 507, loss 3.73056, acc 0.640625, prec 0.0247858, recall 0.808173\n",
      "2017-11-26T12:28:11.216994: step 508, loss 5.95217, acc 0.515625, prec 0.0247917, recall 0.80766\n",
      "2017-11-26T12:28:11.814992: step 509, loss 2.05885, acc 0.515625, prec 0.024797, recall 0.80782\n",
      "2017-11-26T12:28:12.454573: step 510, loss 2.65664, acc 0.390625, prec 0.0247723, recall 0.80782\n",
      "2017-11-26T12:28:13.029453: step 511, loss 2.64339, acc 0.515625, prec 0.0247527, recall 0.80782\n",
      "2017-11-26T12:28:13.609560: step 512, loss 2.39495, acc 0.46875, prec 0.0247561, recall 0.80798\n",
      "2017-11-26T12:28:14.181746: step 513, loss 3.65623, acc 0.390625, prec 0.0247564, recall 0.80814\n",
      "2017-11-26T12:28:14.860329: step 514, loss 2.82286, acc 0.421875, prec 0.0247827, recall 0.808458\n",
      "2017-11-26T12:28:15.809794: step 515, loss 2.16715, acc 0.640625, prec 0.0248425, recall 0.808933\n",
      "2017-11-26T12:28:16.584983: step 516, loss 4.09763, acc 0.625, prec 0.0248775, recall 0.808581\n",
      "2017-11-26T12:28:17.355524: step 517, loss 3.16366, acc 0.5, prec 0.0249563, recall 0.809211\n",
      "2017-11-26T12:28:18.143670: step 518, loss 5.42266, acc 0.609375, prec 0.0249658, recall 0.808703\n",
      "2017-11-26T12:28:18.958847: step 519, loss 2.92911, acc 0.296875, prec 0.0249373, recall 0.808703\n",
      "2017-11-26T12:28:19.810403: step 520, loss 2.79143, acc 0.46875, prec 0.0249159, recall 0.808703\n",
      "2017-11-26T12:28:20.775147: step 521, loss 2.65274, acc 0.5, prec 0.024945, recall 0.809016\n",
      "2017-11-26T12:28:21.741795: step 522, loss 2.30248, acc 0.53125, prec 0.0249508, recall 0.809173\n",
      "2017-11-26T12:28:22.706784: step 523, loss 2.01151, acc 0.546875, prec 0.0249325, recall 0.809173\n",
      "2017-11-26T12:28:23.605726: step 524, loss 3.52265, acc 0.515625, prec 0.0249376, recall 0.809329\n",
      "2017-11-26T12:28:24.503125: step 525, loss 2.11004, acc 0.515625, prec 0.0249427, recall 0.809485\n",
      "2017-11-26T12:28:25.366354: step 526, loss 1.46335, acc 0.640625, prec 0.0249282, recall 0.809485\n",
      "2017-11-26T12:28:26.242187: step 527, loss 1.96983, acc 0.546875, prec 0.02491, recall 0.809485\n",
      "2017-11-26T12:28:27.125621: step 528, loss 1.63473, acc 0.59375, prec 0.0249428, recall 0.809796\n",
      "2017-11-26T12:28:28.019952: step 529, loss 1.90372, acc 0.578125, prec 0.0249749, recall 0.810106\n",
      "2017-11-26T12:28:29.580888: step 530, loss 1.20553, acc 0.6875, prec 0.0249868, recall 0.810261\n",
      "2017-11-26T12:28:30.945606: step 531, loss 1.34692, acc 0.609375, prec 0.0249956, recall 0.810415\n",
      "2017-11-26T12:28:31.845858: step 532, loss 1.73537, acc 0.6875, prec 0.0250075, recall 0.810569\n",
      "2017-11-26T12:28:32.594737: step 533, loss 9.17459, acc 0.6875, prec 0.0249956, recall 0.809911\n",
      "2017-11-26T12:28:33.449474: step 534, loss 1.0785, acc 0.71875, prec 0.0250088, recall 0.810065\n",
      "2017-11-26T12:28:34.194947: step 535, loss 3.57102, acc 0.734375, prec 0.0250232, recall 0.809562\n",
      "2017-11-26T12:28:34.938690: step 536, loss 1.04458, acc 0.6875, prec 0.0250106, recall 0.809562\n",
      "2017-11-26T12:28:35.671135: step 537, loss 1.12827, acc 0.859375, prec 0.0250294, recall 0.809717\n",
      "2017-11-26T12:28:36.379025: step 538, loss 0.533266, acc 0.859375, prec 0.0250482, recall 0.809871\n",
      "2017-11-26T12:28:37.066087: step 539, loss 4.45021, acc 0.65625, prec 0.025035, recall 0.809216\n",
      "2017-11-26T12:28:37.776446: step 540, loss 1.0929, acc 0.734375, prec 0.0250244, recall 0.809216\n",
      "2017-11-26T12:28:38.578841: step 541, loss 0.773589, acc 0.765625, prec 0.025015, recall 0.809216\n",
      "2017-11-26T12:28:39.415397: step 542, loss 0.893944, acc 0.71875, prec 0.0250037, recall 0.809216\n",
      "2017-11-26T12:28:40.217899: step 543, loss 1.26633, acc 0.71875, prec 0.0250169, recall 0.80937\n",
      "2017-11-26T12:28:41.039504: step 544, loss 1.47424, acc 0.734375, prec 0.0250792, recall 0.809831\n",
      "2017-11-26T12:28:41.903387: step 545, loss 0.897916, acc 0.75, prec 0.0250692, recall 0.809831\n",
      "2017-11-26T12:28:42.821387: step 546, loss 1.22072, acc 0.6875, prec 0.025081, recall 0.809984\n",
      "2017-11-26T12:28:43.697815: step 547, loss 1.08921, acc 0.6875, prec 0.0250685, recall 0.809984\n",
      "2017-11-26T12:28:44.460034: step 548, loss 0.678276, acc 0.84375, prec 0.0250623, recall 0.809984\n",
      "2017-11-26T12:28:45.239932: step 549, loss 0.73797, acc 0.765625, prec 0.0250529, recall 0.809984\n",
      "2017-11-26T12:28:46.091586: step 550, loss 0.666159, acc 0.828125, prec 0.0250703, recall 0.810137\n",
      "2017-11-26T12:28:46.992725: step 551, loss 1.15668, acc 0.75, prec 0.0250603, recall 0.810137\n",
      "2017-11-26T12:28:47.767363: step 552, loss 1.49545, acc 0.703125, prec 0.0250485, recall 0.810137\n",
      "2017-11-26T12:28:48.611922: step 553, loss 2.71402, acc 0.75, prec 0.0251119, recall 0.809944\n",
      "2017-11-26T12:28:49.467304: step 554, loss 1.27146, acc 0.796875, prec 0.0251522, recall 0.810248\n",
      "2017-11-26T12:28:50.289714: step 555, loss 0.688764, acc 0.828125, prec 0.0251454, recall 0.810248\n",
      "2017-11-26T12:28:51.175700: step 556, loss 1.03856, acc 0.828125, prec 0.0252353, recall 0.810854\n",
      "2017-11-26T12:28:52.058815: step 557, loss 10.1636, acc 0.859375, prec 0.0252303, recall 0.810207\n",
      "2017-11-26T12:28:52.908382: step 558, loss 2.00224, acc 0.75, prec 0.0252929, recall 0.81066\n",
      "2017-11-26T12:28:53.759328: step 559, loss 1.856, acc 0.765625, prec 0.0253077, recall 0.810811\n",
      "2017-11-26T12:28:54.617752: step 560, loss 1.09073, acc 0.765625, prec 0.0253224, recall 0.810961\n",
      "2017-11-26T12:28:55.468673: step 561, loss 4.25241, acc 0.625, prec 0.0253321, recall 0.810468\n",
      "2017-11-26T12:28:56.311861: step 562, loss 1.52565, acc 0.640625, prec 0.025366, recall 0.810768\n",
      "2017-11-26T12:28:57.149831: step 563, loss 5.29551, acc 0.5625, prec 0.025349, recall 0.810127\n",
      "2017-11-26T12:28:57.994165: step 564, loss 1.17481, acc 0.625, prec 0.0253822, recall 0.810427\n",
      "2017-11-26T12:28:58.899482: step 565, loss 14.2581, acc 0.53125, prec 0.0253881, recall 0.809937\n",
      "2017-11-26T12:28:59.800995: step 566, loss 1.97359, acc 0.59375, prec 0.0253718, recall 0.809937\n",
      "2017-11-26T12:29:00.751794: step 567, loss 7.33735, acc 0.4375, prec 0.0253499, recall 0.809299\n",
      "2017-11-26T12:29:01.635771: step 568, loss 3.0039, acc 0.40625, prec 0.0253742, recall 0.809599\n",
      "2017-11-26T12:29:02.485634: step 569, loss 2.39657, acc 0.5625, prec 0.0253567, recall 0.809599\n",
      "2017-11-26T12:29:03.331893: step 570, loss 2.55437, acc 0.53125, prec 0.025338, recall 0.809599\n",
      "2017-11-26T12:29:04.178390: step 571, loss 2.7189, acc 0.40625, prec 0.0253143, recall 0.809599\n",
      "2017-11-26T12:29:05.043578: step 572, loss 2.98955, acc 0.484375, prec 0.0253417, recall 0.809898\n",
      "2017-11-26T12:29:06.036250: step 573, loss 2.31611, acc 0.46875, prec 0.0253205, recall 0.809898\n",
      "2017-11-26T12:29:06.901503: step 574, loss 2.56767, acc 0.5, prec 0.0253006, recall 0.809898\n",
      "2017-11-26T12:29:07.800773: step 575, loss 2.75615, acc 0.515625, prec 0.0253053, recall 0.810047\n",
      "2017-11-26T12:29:08.678451: step 576, loss 2.39338, acc 0.46875, prec 0.0252842, recall 0.810047\n",
      "2017-11-26T12:29:09.562631: step 577, loss 2.66891, acc 0.484375, prec 0.0252638, recall 0.810047\n",
      "2017-11-26T12:29:10.477064: step 578, loss 2.13869, acc 0.53125, prec 0.0252691, recall 0.810196\n",
      "2017-11-26T12:29:11.383397: step 579, loss 2.23539, acc 0.640625, prec 0.0253025, recall 0.810493\n",
      "2017-11-26T12:29:12.243019: step 580, loss 1.6332, acc 0.578125, prec 0.0253097, recall 0.810642\n",
      "2017-11-26T12:29:13.102550: step 581, loss 1.67007, acc 0.609375, prec 0.0252942, recall 0.810642\n",
      "2017-11-26T12:29:13.971771: step 582, loss 1.09444, acc 0.6875, prec 0.0253056, recall 0.81079\n",
      "2017-11-26T12:29:15.367030: step 583, loss 1.0016, acc 0.734375, prec 0.0252952, recall 0.81079\n",
      "2017-11-26T12:29:16.653860: step 584, loss 1.09313, acc 0.765625, prec 0.0252859, recall 0.81079\n",
      "2017-11-26T12:29:17.921359: step 585, loss 4.39475, acc 0.78125, prec 0.0252779, recall 0.810156\n",
      "2017-11-26T12:29:19.176813: step 586, loss 0.620258, acc 0.8125, prec 0.0252942, recall 0.810304\n",
      "2017-11-26T12:29:20.447280: step 587, loss 2.46612, acc 0.828125, prec 0.0252881, recall 0.809672\n",
      "2017-11-26T12:29:21.798012: step 588, loss 0.356052, acc 0.84375, prec 0.0252819, recall 0.809672\n",
      "2017-11-26T12:29:23.070186: step 589, loss 0.322527, acc 0.9375, prec 0.0252795, recall 0.809672\n",
      "2017-11-26T12:29:24.320809: step 590, loss 0.6196, acc 0.828125, prec 0.0252727, recall 0.809672\n",
      "2017-11-26T12:29:25.569786: step 591, loss 1.77107, acc 0.796875, prec 0.0253121, recall 0.809969\n",
      "2017-11-26T12:29:26.820132: step 592, loss 2.23939, acc 0.796875, prec 0.0253048, recall 0.809339\n",
      "2017-11-26T12:29:28.080658: step 593, loss 0.612117, acc 0.78125, prec 0.0252961, recall 0.809339\n",
      "2017-11-26T12:29:29.349020: step 594, loss 0.772097, acc 0.765625, prec 0.0252869, recall 0.809339\n",
      "2017-11-26T12:29:30.549692: step 595, loss 0.327942, acc 0.875, prec 0.025282, recall 0.809339\n",
      "2017-11-26T12:29:31.818848: step 596, loss 0.607643, acc 0.8125, prec 0.0252746, recall 0.809339\n",
      "2017-11-26T12:29:33.121499: step 597, loss 0.647497, acc 0.828125, prec 0.0252679, recall 0.809339\n",
      "2017-11-26T12:29:34.381995: step 598, loss 0.59093, acc 0.78125, prec 0.0252829, recall 0.809487\n",
      "2017-11-26T12:29:35.601669: step 599, loss 0.787612, acc 0.765625, prec 0.0252737, recall 0.809487\n",
      "2017-11-26T12:29:36.848488: step 600, loss 0.846659, acc 0.796875, prec 0.0253131, recall 0.809783\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:31:12.732906: step 600, loss 1.86616, acc 0.891489, prec 0.0264593, recall 0.783368\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-600\n",
      "\n",
      "2017-11-26T12:31:14.797241: step 601, loss 0.603288, acc 0.828125, prec 0.0264524, recall 0.783368\n",
      "2017-11-26T12:31:15.432385: step 602, loss 6.70181, acc 0.859375, prec 0.026448, recall 0.782275\n",
      "2017-11-26T12:31:16.070790: step 603, loss 1.2779, acc 0.875, prec 0.0265349, recall 0.782881\n",
      "2017-11-26T12:31:16.695245: step 604, loss 0.75622, acc 0.78125, prec 0.026572, recall 0.783183\n",
      "2017-11-26T12:31:17.313024: step 605, loss 0.660504, acc 0.828125, prec 0.0265881, recall 0.783333\n",
      "2017-11-26T12:31:17.926593: step 606, loss 13.5251, acc 0.8125, prec 0.0265812, recall 0.78279\n",
      "2017-11-26T12:31:18.537026: step 607, loss 0.787816, acc 0.8125, prec 0.0266196, recall 0.783091\n",
      "2017-11-26T12:31:19.152157: step 608, loss 1.26658, acc 0.671875, prec 0.0266064, recall 0.783091\n",
      "2017-11-26T12:31:19.754668: step 609, loss 1.23175, acc 0.671875, prec 0.0265932, recall 0.783091\n",
      "2017-11-26T12:31:20.362463: step 610, loss 0.703709, acc 0.71875, prec 0.026582, recall 0.783091\n",
      "2017-11-26T12:31:20.972015: step 611, loss 1.17018, acc 0.6875, prec 0.0265924, recall 0.783241\n",
      "2017-11-26T12:31:21.579850: step 612, loss 0.955079, acc 0.6875, prec 0.0266485, recall 0.78369\n",
      "2017-11-26T12:31:22.228710: step 613, loss 1.60274, acc 0.640625, prec 0.0266341, recall 0.78369\n",
      "2017-11-26T12:31:22.855852: step 614, loss 2.01063, acc 0.59375, prec 0.0266407, recall 0.78384\n",
      "2017-11-26T12:31:23.479350: step 615, loss 1.78888, acc 0.734375, prec 0.0266757, recall 0.784138\n",
      "2017-11-26T12:31:24.081992: step 616, loss 1.34172, acc 0.671875, prec 0.0266854, recall 0.784287\n",
      "2017-11-26T12:31:24.680850: step 617, loss 9.53419, acc 0.75, prec 0.026676, recall 0.783747\n",
      "2017-11-26T12:31:25.286394: step 618, loss 0.92644, acc 0.6875, prec 0.0267092, recall 0.784044\n",
      "2017-11-26T12:31:25.890748: step 619, loss 0.958798, acc 0.75, prec 0.0267219, recall 0.784192\n",
      "2017-11-26T12:31:26.488590: step 620, loss 1.14104, acc 0.6875, prec 0.0267094, recall 0.784192\n",
      "2017-11-26T12:31:27.093562: step 621, loss 1.64497, acc 0.59375, prec 0.0267159, recall 0.784341\n",
      "2017-11-26T12:31:27.700834: step 622, loss 1.23395, acc 0.6875, prec 0.0267262, recall 0.784489\n",
      "2017-11-26T12:31:28.332044: step 623, loss 1.39404, acc 0.65625, prec 0.0267125, recall 0.784489\n",
      "2017-11-26T12:31:28.941870: step 624, loss 0.630256, acc 0.796875, prec 0.0267271, recall 0.784636\n",
      "2017-11-26T12:31:29.554057: step 625, loss 1.2719, acc 0.671875, prec 0.0267594, recall 0.784931\n",
      "2017-11-26T12:31:30.151044: step 626, loss 1.51136, acc 0.6875, prec 0.0267924, recall 0.785226\n",
      "2017-11-26T12:31:30.759231: step 627, loss 14.0688, acc 0.71875, prec 0.0267818, recall 0.784689\n",
      "2017-11-26T12:31:31.360364: step 628, loss 1.61589, acc 0.734375, prec 0.0267938, recall 0.784836\n",
      "2017-11-26T12:31:31.981750: step 629, loss 0.869442, acc 0.75, prec 0.0267838, recall 0.784836\n",
      "2017-11-26T12:31:32.591430: step 630, loss 1.06324, acc 0.71875, prec 0.0267726, recall 0.784836\n",
      "2017-11-26T12:31:33.198301: step 631, loss 2.81466, acc 0.75, prec 0.0267859, recall 0.784447\n",
      "2017-11-26T12:31:33.808522: step 632, loss 1.44077, acc 0.671875, prec 0.0268181, recall 0.784741\n",
      "2017-11-26T12:31:34.414828: step 633, loss 0.744743, acc 0.78125, prec 0.026832, recall 0.784888\n",
      "2017-11-26T12:31:35.095485: step 634, loss 0.690613, acc 0.75, prec 0.0268221, recall 0.784888\n",
      "2017-11-26T12:31:35.849223: step 635, loss 2.14955, acc 0.734375, prec 0.0268341, recall 0.785034\n",
      "2017-11-26T12:31:36.599223: step 636, loss 0.521217, acc 0.890625, prec 0.0268523, recall 0.78518\n",
      "2017-11-26T12:31:37.400736: step 637, loss 0.994579, acc 0.71875, prec 0.0268637, recall 0.785326\n",
      "2017-11-26T12:31:38.155203: step 638, loss 0.762581, acc 0.765625, prec 0.0268544, recall 0.785326\n",
      "2017-11-26T12:31:38.886022: step 639, loss 0.898074, acc 0.734375, prec 0.0268438, recall 0.785326\n",
      "2017-11-26T12:31:39.689214: step 640, loss 1.01605, acc 0.796875, prec 0.0268583, recall 0.785472\n",
      "2017-11-26T12:31:40.528117: step 641, loss 0.630526, acc 0.828125, prec 0.026874, recall 0.785617\n",
      "2017-11-26T12:31:41.380221: step 642, loss 1.73623, acc 0.703125, prec 0.0268847, recall 0.785763\n",
      "2017-11-26T12:31:42.317612: step 643, loss 0.719416, acc 0.8125, prec 0.0268998, recall 0.785908\n",
      "2017-11-26T12:31:43.197752: step 644, loss 0.465446, acc 0.84375, prec 0.0268936, recall 0.785908\n",
      "2017-11-26T12:31:44.012028: step 645, loss 0.915442, acc 0.796875, prec 0.0268855, recall 0.785908\n",
      "2017-11-26T12:31:44.867552: step 646, loss 12.9914, acc 0.859375, prec 0.0268805, recall 0.785376\n",
      "2017-11-26T12:31:45.840171: step 647, loss 0.413385, acc 0.8125, prec 0.026873, recall 0.785376\n",
      "2017-11-26T12:31:46.683568: step 648, loss 0.9296, acc 0.765625, prec 0.0268862, recall 0.785521\n",
      "2017-11-26T12:31:47.589683: step 649, loss 0.66398, acc 0.8125, prec 0.0268787, recall 0.785521\n",
      "2017-11-26T12:31:48.386771: step 650, loss 0.592308, acc 0.875, prec 0.0268963, recall 0.785666\n",
      "2017-11-26T12:31:49.214913: step 651, loss 1.14152, acc 0.71875, prec 0.0269076, recall 0.785811\n",
      "2017-11-26T12:31:50.070497: step 652, loss 0.597028, acc 0.78125, prec 0.0268989, recall 0.785811\n",
      "2017-11-26T12:31:50.994083: step 653, loss 0.541629, acc 0.875, prec 0.0268939, recall 0.785811\n",
      "2017-11-26T12:31:51.915302: step 654, loss 0.435913, acc 0.828125, prec 0.0268871, recall 0.785811\n",
      "2017-11-26T12:31:52.952149: step 655, loss 0.511674, acc 0.859375, prec 0.0269265, recall 0.7861\n",
      "2017-11-26T12:31:53.866686: step 656, loss 0.586887, acc 0.859375, prec 0.0269209, recall 0.7861\n",
      "2017-11-26T12:31:54.737288: step 657, loss 0.861191, acc 0.796875, prec 0.0269128, recall 0.7861\n",
      "2017-11-26T12:31:55.585157: step 658, loss 0.287349, acc 0.90625, prec 0.026909, recall 0.7861\n",
      "2017-11-26T12:31:56.435324: step 659, loss 0.579183, acc 0.796875, prec 0.0269234, recall 0.786244\n",
      "2017-11-26T12:31:57.244317: step 660, loss 5.33348, acc 0.859375, prec 0.0269409, recall 0.785859\n",
      "2017-11-26T12:31:58.064530: step 661, loss 2.23722, acc 0.875, prec 0.0269366, recall 0.78533\n",
      "2017-11-26T12:31:58.848130: step 662, loss 0.388755, acc 0.890625, prec 0.0269547, recall 0.785474\n",
      "2017-11-26T12:31:59.599059: step 663, loss 0.673354, acc 0.78125, prec 0.026946, recall 0.785474\n",
      "2017-11-26T12:32:00.351380: step 664, loss 0.35906, acc 0.890625, prec 0.0269641, recall 0.785618\n",
      "2017-11-26T12:32:01.098317: step 665, loss 0.656507, acc 0.734375, prec 0.0270208, recall 0.78605\n",
      "2017-11-26T12:32:01.820209: step 666, loss 0.579796, acc 0.859375, prec 0.0270825, recall 0.786479\n",
      "2017-11-26T12:32:02.592689: step 667, loss 0.568806, acc 0.796875, prec 0.0270744, recall 0.786479\n",
      "2017-11-26T12:32:03.306907: step 668, loss 0.895029, acc 0.75, prec 0.0270868, recall 0.786622\n",
      "2017-11-26T12:32:04.023024: step 669, loss 0.505187, acc 0.796875, prec 0.0270787, recall 0.786622\n",
      "2017-11-26T12:32:04.740692: step 670, loss 0.452529, acc 0.828125, prec 0.0270942, recall 0.786765\n",
      "2017-11-26T12:32:05.433552: step 671, loss 1.19928, acc 0.78125, prec 0.0271303, recall 0.787049\n",
      "2017-11-26T12:32:06.147729: step 672, loss 2.21371, acc 0.75, prec 0.0271209, recall 0.786524\n",
      "2017-11-26T12:32:06.852197: step 673, loss 0.23608, acc 0.921875, prec 0.0271402, recall 0.786667\n",
      "2017-11-26T12:32:07.551735: step 674, loss 0.71044, acc 0.765625, prec 0.0271308, recall 0.786667\n",
      "2017-11-26T12:32:08.263492: step 675, loss 0.509031, acc 0.796875, prec 0.0271227, recall 0.786667\n",
      "2017-11-26T12:32:08.965367: step 676, loss 0.664242, acc 0.84375, prec 0.0271165, recall 0.786667\n",
      "2017-11-26T12:32:09.678337: step 677, loss 8.96173, acc 0.78125, prec 0.0271084, recall 0.786143\n",
      "2017-11-26T12:32:10.374300: step 678, loss 0.379709, acc 0.875, prec 0.0271034, recall 0.786143\n",
      "2017-11-26T12:32:11.063489: step 679, loss 0.312282, acc 0.828125, prec 0.0270965, recall 0.786143\n",
      "2017-11-26T12:32:11.745482: step 680, loss 1.76132, acc 0.875, prec 0.0271362, recall 0.786427\n",
      "2017-11-26T12:32:12.475890: step 681, loss 0.716456, acc 0.765625, prec 0.0271269, recall 0.786427\n",
      "2017-11-26T12:32:13.143647: step 682, loss 0.581709, acc 0.8125, prec 0.0271417, recall 0.786569\n",
      "2017-11-26T12:32:13.801672: step 683, loss 2.39647, acc 0.75, prec 0.0271324, recall 0.786047\n",
      "2017-11-26T12:32:14.461953: step 684, loss 1.37065, acc 0.75, prec 0.0271448, recall 0.786189\n",
      "2017-11-26T12:32:15.126007: step 685, loss 1.30885, acc 0.703125, prec 0.0271552, recall 0.78633\n",
      "2017-11-26T12:32:15.780353: step 686, loss 2.56698, acc 0.640625, prec 0.0271415, recall 0.785809\n",
      "2017-11-26T12:32:16.435279: step 687, loss 1.02542, acc 0.65625, prec 0.0271724, recall 0.786093\n",
      "2017-11-26T12:32:17.096311: step 688, loss 1.83573, acc 0.640625, prec 0.0271804, recall 0.786234\n",
      "2017-11-26T12:32:17.753050: step 689, loss 1.63437, acc 0.65625, prec 0.0271667, recall 0.786234\n",
      "2017-11-26T12:32:18.409283: step 690, loss 1.17439, acc 0.78125, prec 0.0272247, recall 0.786658\n",
      "2017-11-26T12:32:19.069634: step 691, loss 0.888196, acc 0.71875, prec 0.0272135, recall 0.786658\n",
      "2017-11-26T12:32:19.726493: step 692, loss 0.775811, acc 0.8125, prec 0.0272061, recall 0.786658\n",
      "2017-11-26T12:32:20.392202: step 693, loss 1.49198, acc 0.625, prec 0.0272134, recall 0.786799\n",
      "2017-11-26T12:32:21.052396: step 694, loss 1.16156, acc 0.734375, prec 0.0272028, recall 0.786799\n",
      "2017-11-26T12:32:21.712003: step 695, loss 0.525976, acc 0.8125, prec 0.0271954, recall 0.786799\n",
      "2017-11-26T12:32:22.409574: step 696, loss 0.962501, acc 0.734375, prec 0.0272292, recall 0.78708\n",
      "2017-11-26T12:32:23.085404: step 697, loss 0.892505, acc 0.6875, prec 0.0272168, recall 0.78708\n",
      "2017-11-26T12:32:23.744234: step 698, loss 0.7319, acc 0.8125, prec 0.0272315, recall 0.78722\n",
      "2017-11-26T12:32:24.403743: step 699, loss 0.73001, acc 0.796875, prec 0.0272234, recall 0.78722\n",
      "2017-11-26T12:32:25.066719: step 700, loss 0.780475, acc 0.796875, prec 0.0272154, recall 0.78722\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:33:07.762646: step 700, loss 1.90052, acc 0.898566, prec 0.0282908, recall 0.766406\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-700\n",
      "\n",
      "2017-11-26T12:33:09.538970: step 701, loss 0.698817, acc 0.8125, prec 0.0282832, recall 0.766406\n",
      "2017-11-26T12:33:10.161349: step 702, loss 0.780487, acc 0.796875, prec 0.0282966, recall 0.766546\n",
      "2017-11-26T12:33:10.801291: step 703, loss 0.542321, acc 0.84375, prec 0.0282904, recall 0.766546\n",
      "2017-11-26T12:33:11.453761: step 704, loss 26.5667, acc 0.796875, prec 0.0282841, recall 0.765165\n",
      "2017-11-26T12:33:12.105568: step 705, loss 2.89324, acc 0.75, prec 0.0282747, recall 0.764706\n",
      "2017-11-26T12:33:12.800169: step 706, loss 0.686264, acc 0.828125, prec 0.0282678, recall 0.764706\n",
      "2017-11-26T12:33:13.456506: step 707, loss 6.79697, acc 0.8125, prec 0.0282824, recall 0.764389\n",
      "2017-11-26T12:33:14.090314: step 708, loss 1.335, acc 0.734375, prec 0.0283149, recall 0.764671\n",
      "2017-11-26T12:33:14.713762: step 709, loss 0.95672, acc 0.765625, prec 0.028327, recall 0.764812\n",
      "2017-11-26T12:33:15.367842: step 710, loss 2.28972, acc 0.609375, prec 0.0283543, recall 0.765093\n",
      "2017-11-26T12:33:16.026715: step 711, loss 1.36577, acc 0.625, prec 0.0283608, recall 0.765233\n",
      "2017-11-26T12:33:16.671793: step 712, loss 1.70131, acc 0.546875, prec 0.0283426, recall 0.765233\n",
      "2017-11-26T12:33:17.323668: step 713, loss 1.31263, acc 0.59375, prec 0.0283263, recall 0.765233\n",
      "2017-11-26T12:33:17.974180: step 714, loss 1.72281, acc 0.515625, prec 0.0283284, recall 0.765373\n",
      "2017-11-26T12:33:18.619098: step 715, loss 1.5104, acc 0.578125, prec 0.0283115, recall 0.765373\n",
      "2017-11-26T12:33:19.267406: step 716, loss 1.44648, acc 0.578125, prec 0.028316, recall 0.765513\n",
      "2017-11-26T12:33:19.906260: step 717, loss 1.47585, acc 0.59375, prec 0.0283212, recall 0.765653\n",
      "2017-11-26T12:33:20.552488: step 718, loss 2.1975, acc 0.578125, prec 0.0283258, recall 0.765793\n",
      "2017-11-26T12:33:21.209189: step 719, loss 1.55016, acc 0.625, prec 0.0283536, recall 0.766071\n",
      "2017-11-26T12:33:21.845784: step 720, loss 1.32706, acc 0.703125, prec 0.0283846, recall 0.76635\n",
      "2017-11-26T12:33:22.483121: step 721, loss 1.49577, acc 0.71875, prec 0.0283947, recall 0.766488\n",
      "2017-11-26T12:33:23.202842: step 722, loss 2.9064, acc 0.75, prec 0.0283853, recall 0.766033\n",
      "2017-11-26T12:33:23.875869: step 723, loss 1.08213, acc 0.765625, prec 0.028376, recall 0.766033\n",
      "2017-11-26T12:33:24.541533: step 724, loss 1.1593, acc 0.703125, prec 0.0283641, recall 0.766033\n",
      "2017-11-26T12:33:25.202650: step 725, loss 0.867751, acc 0.75, prec 0.0283541, recall 0.766033\n",
      "2017-11-26T12:33:25.869643: step 726, loss 0.926626, acc 0.71875, prec 0.0283429, recall 0.766033\n",
      "2017-11-26T12:33:26.547735: step 727, loss 11.0144, acc 0.59375, prec 0.02837, recall 0.765857\n",
      "2017-11-26T12:33:27.208149: step 728, loss 1.63835, acc 0.609375, prec 0.0283971, recall 0.766134\n",
      "2017-11-26T12:33:27.870989: step 729, loss 1.14208, acc 0.625, prec 0.0284035, recall 0.766272\n",
      "2017-11-26T12:33:28.517133: step 730, loss 6.83845, acc 0.5625, prec 0.0283867, recall 0.765819\n",
      "2017-11-26T12:33:29.161531: step 731, loss 1.22792, acc 0.703125, prec 0.0284387, recall 0.766234\n",
      "2017-11-26T12:33:29.797921: step 732, loss 0.893186, acc 0.78125, prec 0.0284513, recall 0.766372\n",
      "2017-11-26T12:33:30.425847: step 733, loss 3.85423, acc 0.703125, prec 0.0284401, recall 0.76592\n",
      "2017-11-26T12:33:31.077793: step 734, loss 1.59625, acc 0.546875, prec 0.028422, recall 0.76592\n",
      "2017-11-26T12:33:31.719485: step 735, loss 2.07795, acc 0.609375, prec 0.0284065, recall 0.76592\n",
      "2017-11-26T12:33:32.364274: step 736, loss 2.02879, acc 0.53125, prec 0.0283879, recall 0.76592\n",
      "2017-11-26T12:33:33.063425: step 737, loss 1.13309, acc 0.734375, prec 0.0284198, recall 0.766196\n",
      "2017-11-26T12:33:33.713467: step 738, loss 1.51192, acc 0.65625, prec 0.0284061, recall 0.766196\n",
      "2017-11-26T12:33:34.703102: step 739, loss 7.55594, acc 0.59375, prec 0.0283912, recall 0.765294\n",
      "2017-11-26T12:33:35.343947: step 740, loss 1.47183, acc 0.703125, prec 0.028443, recall 0.765708\n",
      "2017-11-26T12:33:35.997224: step 741, loss 1.38402, acc 0.671875, prec 0.0285147, recall 0.766257\n",
      "2017-11-26T12:33:36.646560: step 742, loss 1.47199, acc 0.671875, prec 0.028544, recall 0.76653\n",
      "2017-11-26T12:33:37.286317: step 743, loss 7.88086, acc 0.84375, prec 0.0285596, recall 0.766219\n",
      "2017-11-26T12:33:37.937557: step 744, loss 2.40466, acc 0.546875, prec 0.028605, recall 0.766628\n",
      "2017-11-26T12:33:38.579661: step 745, loss 1.72158, acc 0.609375, prec 0.0286106, recall 0.766764\n",
      "2017-11-26T12:33:39.208603: step 746, loss 1.86668, acc 0.578125, prec 0.028636, recall 0.767036\n",
      "2017-11-26T12:33:39.845315: step 747, loss 1.6351, acc 0.53125, prec 0.0286174, recall 0.767036\n",
      "2017-11-26T12:33:40.478395: step 748, loss 2.37209, acc 0.5625, prec 0.0286, recall 0.767036\n",
      "2017-11-26T12:33:41.107957: step 749, loss 2.06589, acc 0.5625, prec 0.0286037, recall 0.767171\n",
      "2017-11-26T12:33:41.745779: step 750, loss 0.817484, acc 0.71875, prec 0.0285925, recall 0.767171\n",
      "2017-11-26T12:33:42.389655: step 751, loss 1.66663, acc 0.609375, prec 0.0285981, recall 0.767307\n",
      "2017-11-26T12:33:43.092542: step 752, loss 1.91518, acc 0.59375, prec 0.028603, recall 0.767442\n",
      "2017-11-26T12:33:43.756863: step 753, loss 1.57957, acc 0.609375, prec 0.0286296, recall 0.767712\n",
      "2017-11-26T12:33:44.424651: step 754, loss 1.06814, acc 0.640625, prec 0.0286153, recall 0.767712\n",
      "2017-11-26T12:33:45.085207: step 755, loss 5.94478, acc 0.640625, prec 0.0286017, recall 0.767266\n",
      "2017-11-26T12:33:45.749928: step 756, loss 1.20684, acc 0.734375, prec 0.0286122, recall 0.767401\n",
      "2017-11-26T12:33:46.412009: step 757, loss 1.17506, acc 0.671875, prec 0.0285992, recall 0.767401\n",
      "2017-11-26T12:33:47.072877: step 758, loss 1.15783, acc 0.65625, prec 0.0286486, recall 0.767805\n",
      "2017-11-26T12:33:47.710095: step 759, loss 0.990032, acc 0.765625, prec 0.0286813, recall 0.768074\n",
      "2017-11-26T12:33:48.338456: step 760, loss 0.658978, acc 0.8125, prec 0.0286948, recall 0.768208\n",
      "2017-11-26T12:33:49.023361: step 761, loss 1.18977, acc 0.765625, prec 0.0286855, recall 0.768208\n",
      "2017-11-26T12:33:49.665705: step 762, loss 8.38999, acc 0.734375, prec 0.0286756, recall 0.767764\n",
      "2017-11-26T12:33:50.298821: step 763, loss 0.787434, acc 0.78125, prec 0.028667, recall 0.767764\n",
      "2017-11-26T12:33:50.931752: step 764, loss 0.668676, acc 0.8125, prec 0.0286805, recall 0.767898\n",
      "2017-11-26T12:33:51.572011: step 765, loss 0.783143, acc 0.765625, prec 0.0286921, recall 0.768032\n",
      "2017-11-26T12:33:52.201546: step 766, loss 0.9745, acc 0.75, prec 0.0286823, recall 0.768032\n",
      "2017-11-26T12:33:52.852027: step 767, loss 0.69682, acc 0.734375, prec 0.0286718, recall 0.768032\n",
      "2017-11-26T12:33:53.486151: step 768, loss 0.908851, acc 0.90625, prec 0.028689, recall 0.768166\n",
      "2017-11-26T12:33:54.120781: step 769, loss 3.96126, acc 0.75, prec 0.0287006, recall 0.767857\n",
      "2017-11-26T12:33:54.750498: step 770, loss 0.518968, acc 0.828125, prec 0.0286938, recall 0.767857\n",
      "2017-11-26T12:33:55.382014: step 771, loss 0.762965, acc 0.765625, prec 0.0287264, recall 0.768124\n",
      "2017-11-26T12:33:56.017084: step 772, loss 0.604191, acc 0.78125, prec 0.0287386, recall 0.768258\n",
      "2017-11-26T12:33:56.651787: step 773, loss 0.703411, acc 0.734375, prec 0.0287281, recall 0.768258\n",
      "2017-11-26T12:33:57.296059: step 774, loss 0.748134, acc 0.8125, prec 0.0287416, recall 0.768391\n",
      "2017-11-26T12:33:57.951945: step 775, loss 6.5341, acc 0.875, prec 0.0287581, recall 0.768083\n",
      "2017-11-26T12:33:58.617614: step 776, loss 3.4043, acc 0.734375, prec 0.02879, recall 0.767908\n",
      "2017-11-26T12:33:59.279529: step 777, loss 10.143, acc 0.640625, prec 0.0288181, recall 0.767735\n",
      "2017-11-26T12:33:59.942158: step 778, loss 3.74929, acc 0.609375, prec 0.0288241, recall 0.767429\n",
      "2017-11-26T12:34:00.601416: step 779, loss 2.75096, acc 0.59375, prec 0.0288086, recall 0.76699\n",
      "2017-11-26T12:34:01.263770: step 780, loss 4.62618, acc 0.453125, prec 0.0287876, recall 0.766553\n",
      "2017-11-26T12:34:01.926560: step 781, loss 2.45432, acc 0.5625, prec 0.028812, recall 0.766819\n",
      "2017-11-26T12:34:02.581561: step 782, loss 2.76313, acc 0.5, prec 0.0287922, recall 0.766819\n",
      "2017-11-26T12:34:03.279787: step 783, loss 3.53486, acc 0.328125, prec 0.0287865, recall 0.766952\n",
      "2017-11-26T12:34:03.950869: step 784, loss 3.09622, acc 0.421875, prec 0.0287845, recall 0.767084\n",
      "2017-11-26T12:34:04.616430: step 785, loss 2.27071, acc 0.53125, prec 0.0287661, recall 0.767084\n",
      "2017-11-26T12:34:05.252809: step 786, loss 4.45973, acc 0.40625, prec 0.0287434, recall 0.766648\n",
      "2017-11-26T12:34:05.900354: step 787, loss 3.62733, acc 0.3125, prec 0.0287164, recall 0.766648\n",
      "2017-11-26T12:34:06.539747: step 788, loss 3.4907, acc 0.3125, prec 0.0287102, recall 0.76678\n",
      "2017-11-26T12:34:07.176599: step 789, loss 3.1413, acc 0.3125, prec 0.028704, recall 0.766913\n",
      "2017-11-26T12:34:07.816138: step 790, loss 2.64777, acc 0.4375, prec 0.0287026, recall 0.767045\n",
      "2017-11-26T12:34:08.449839: step 791, loss 2.84212, acc 0.4375, prec 0.028722, recall 0.76731\n",
      "2017-11-26T12:34:09.100708: step 792, loss 1.85519, acc 0.515625, prec 0.0287649, recall 0.767705\n",
      "2017-11-26T12:34:09.756118: step 793, loss 1.95345, acc 0.59375, prec 0.028749, recall 0.767705\n",
      "2017-11-26T12:34:10.417602: step 794, loss 1.90017, acc 0.578125, prec 0.028815, recall 0.768231\n",
      "2017-11-26T12:34:11.076874: step 795, loss 2.03988, acc 0.578125, prec 0.0288396, recall 0.768492\n",
      "2017-11-26T12:34:11.736618: step 796, loss 1.43455, acc 0.640625, prec 0.0288256, recall 0.768492\n",
      "2017-11-26T12:34:12.398844: step 797, loss 1.31165, acc 0.640625, prec 0.0288115, recall 0.768492\n",
      "2017-11-26T12:34:13.074259: step 798, loss 1.12996, acc 0.671875, prec 0.0288193, recall 0.768623\n",
      "2017-11-26T12:34:13.748180: step 799, loss 0.999059, acc 0.703125, prec 0.0288077, recall 0.768623\n",
      "2017-11-26T12:34:14.405776: step 800, loss 11.303, acc 0.796875, prec 0.0288216, recall 0.767887\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:34:59.454132: step 800, loss 1.35587, acc 0.821004, prec 0.0297277, recall 0.763295\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-800\n",
      "\n",
      "2017-11-26T12:35:01.360501: step 801, loss 0.772835, acc 0.8125, prec 0.0297402, recall 0.763418\n",
      "2017-11-26T12:35:02.006176: step 802, loss 0.809395, acc 0.71875, prec 0.0297293, recall 0.763418\n",
      "2017-11-26T12:35:02.640981: step 803, loss 1.86403, acc 0.75, prec 0.0297202, recall 0.763021\n",
      "2017-11-26T12:35:03.328624: step 804, loss 0.84898, acc 0.75, prec 0.02975, recall 0.763267\n",
      "2017-11-26T12:35:04.039787: step 805, loss 0.73853, acc 0.8125, prec 0.0297427, recall 0.763267\n",
      "2017-11-26T12:35:04.708273: step 806, loss 0.968084, acc 0.765625, prec 0.0297533, recall 0.763391\n",
      "2017-11-26T12:35:05.361002: step 807, loss 0.838175, acc 0.796875, prec 0.0297455, recall 0.763391\n",
      "2017-11-26T12:35:06.053074: step 808, loss 0.816838, acc 0.875, prec 0.0297603, recall 0.763514\n",
      "2017-11-26T12:35:06.725304: step 809, loss 9.55642, acc 0.671875, prec 0.0297679, recall 0.76324\n",
      "2017-11-26T12:35:07.406253: step 810, loss 0.869259, acc 0.734375, prec 0.0297577, recall 0.76324\n",
      "2017-11-26T12:35:08.088898: step 811, loss 6.93794, acc 0.75, prec 0.0297879, recall 0.76309\n",
      "2017-11-26T12:35:08.753571: step 812, loss 1.59242, acc 0.6875, prec 0.0297955, recall 0.763212\n",
      "2017-11-26T12:35:09.428149: step 813, loss 0.648592, acc 0.8125, prec 0.0297883, recall 0.763212\n",
      "2017-11-26T12:35:10.129642: step 814, loss 0.895532, acc 0.71875, prec 0.0297774, recall 0.763212\n",
      "2017-11-26T12:35:10.815844: step 815, loss 1.46066, acc 0.65625, prec 0.0297642, recall 0.763212\n",
      "2017-11-26T12:35:11.510862: step 816, loss 2.19112, acc 0.546875, prec 0.0297468, recall 0.763212\n",
      "2017-11-26T12:35:12.196102: step 817, loss 1.10517, acc 0.6875, prec 0.0297543, recall 0.763335\n",
      "2017-11-26T12:35:12.918781: step 818, loss 1.37828, acc 0.609375, prec 0.0297589, recall 0.763458\n",
      "2017-11-26T12:35:13.636510: step 819, loss 1.99448, acc 0.59375, prec 0.0297629, recall 0.76358\n",
      "2017-11-26T12:35:14.326627: step 820, loss 1.36269, acc 0.703125, prec 0.029771, recall 0.763702\n",
      "2017-11-26T12:35:15.015433: step 821, loss 1.41389, acc 0.625, prec 0.0297566, recall 0.763702\n",
      "2017-11-26T12:35:15.720958: step 822, loss 1.31838, acc 0.6875, prec 0.0297642, recall 0.763824\n",
      "2017-11-26T12:35:16.426970: step 823, loss 1.79458, acc 0.671875, prec 0.0298102, recall 0.76419\n",
      "2017-11-26T12:35:17.113396: step 824, loss 1.7548, acc 0.734375, prec 0.0298195, recall 0.764311\n",
      "2017-11-26T12:35:17.802910: step 825, loss 0.957091, acc 0.734375, prec 0.0298093, recall 0.764311\n",
      "2017-11-26T12:35:18.502328: step 826, loss 0.749785, acc 0.671875, prec 0.0297967, recall 0.764311\n",
      "2017-11-26T12:35:19.192540: step 827, loss 1.36214, acc 0.640625, prec 0.029783, recall 0.764311\n",
      "2017-11-26T12:35:19.877575: step 828, loss 0.95687, acc 0.8125, prec 0.0297758, recall 0.764311\n",
      "2017-11-26T12:35:20.566447: step 829, loss 0.463478, acc 0.84375, prec 0.0297893, recall 0.764433\n",
      "2017-11-26T12:35:21.258169: step 830, loss 0.277837, acc 0.90625, prec 0.0298052, recall 0.764554\n",
      "2017-11-26T12:35:21.944609: step 831, loss 0.663865, acc 0.828125, prec 0.0298181, recall 0.764676\n",
      "2017-11-26T12:35:22.649206: step 832, loss 0.67906, acc 0.796875, prec 0.0298298, recall 0.764797\n",
      "2017-11-26T12:35:23.368684: step 833, loss 2.03865, acc 0.765625, prec 0.0298214, recall 0.764403\n",
      "2017-11-26T12:35:24.065253: step 834, loss 0.742098, acc 0.75, prec 0.0298313, recall 0.764524\n",
      "2017-11-26T12:35:24.761761: step 835, loss 0.581808, acc 0.859375, prec 0.0298259, recall 0.764524\n",
      "2017-11-26T12:35:25.450094: step 836, loss 8.0732, acc 0.796875, prec 0.0298187, recall 0.764132\n",
      "2017-11-26T12:35:26.149605: step 837, loss 0.524758, acc 0.78125, prec 0.0298104, recall 0.764132\n",
      "2017-11-26T12:35:26.842694: step 838, loss 12.7328, acc 0.828125, prec 0.0298238, recall 0.76386\n",
      "2017-11-26T12:35:27.533597: step 839, loss 0.523239, acc 0.8125, prec 0.0298555, recall 0.764103\n",
      "2017-11-26T12:35:28.230495: step 840, loss 0.683912, acc 0.828125, prec 0.0298684, recall 0.764223\n",
      "2017-11-26T12:35:28.927292: step 841, loss 0.783551, acc 0.734375, prec 0.0298582, recall 0.764223\n",
      "2017-11-26T12:35:29.614324: step 842, loss 1.05034, acc 0.703125, prec 0.0298663, recall 0.764344\n",
      "2017-11-26T12:35:30.297326: step 843, loss 0.566251, acc 0.796875, prec 0.0298779, recall 0.764465\n",
      "2017-11-26T12:35:30.962883: step 844, loss 0.398237, acc 0.859375, prec 0.029892, recall 0.764585\n",
      "2017-11-26T12:35:31.635384: step 845, loss 0.88721, acc 0.734375, prec 0.0298818, recall 0.764585\n",
      "2017-11-26T12:35:32.305590: step 846, loss 0.605921, acc 0.78125, prec 0.0298734, recall 0.764585\n",
      "2017-11-26T12:35:32.966462: step 847, loss 0.46511, acc 0.890625, prec 0.0298692, recall 0.764585\n",
      "2017-11-26T12:35:33.685642: step 848, loss 0.433199, acc 0.84375, prec 0.0298633, recall 0.764585\n",
      "2017-11-26T12:35:34.359705: step 849, loss 0.295479, acc 0.90625, prec 0.0298791, recall 0.764706\n",
      "2017-11-26T12:35:35.020135: step 850, loss 7.47107, acc 0.859375, prec 0.0298937, recall 0.764435\n",
      "2017-11-26T12:35:35.689573: step 851, loss 0.363516, acc 0.90625, prec 0.0298901, recall 0.764435\n",
      "2017-11-26T12:35:36.349426: step 852, loss 0.555731, acc 0.796875, prec 0.0299017, recall 0.764556\n",
      "2017-11-26T12:35:37.039457: step 853, loss 1.16282, acc 0.796875, prec 0.0299327, recall 0.764796\n",
      "2017-11-26T12:35:37.724807: step 854, loss 0.743859, acc 0.828125, prec 0.0299842, recall 0.765155\n",
      "2017-11-26T12:35:38.418724: step 855, loss 0.847122, acc 0.765625, prec 0.0299753, recall 0.765155\n",
      "2017-11-26T12:35:39.117800: step 856, loss 0.622599, acc 0.796875, prec 0.0299675, recall 0.765155\n",
      "2017-11-26T12:35:39.810649: step 857, loss 0.624354, acc 0.796875, prec 0.0299597, recall 0.765155\n",
      "2017-11-26T12:35:40.496522: step 858, loss 0.63283, acc 0.796875, prec 0.0299906, recall 0.765394\n",
      "2017-11-26T12:35:41.185794: step 859, loss 0.290317, acc 0.890625, prec 0.0300251, recall 0.765633\n",
      "2017-11-26T12:35:41.875853: step 860, loss 0.454759, acc 0.796875, prec 0.0300367, recall 0.765752\n",
      "2017-11-26T12:35:42.576604: step 861, loss 0.826563, acc 0.8125, prec 0.0300681, recall 0.76599\n",
      "2017-11-26T12:35:43.268945: step 862, loss 7.19238, acc 0.75, prec 0.0300785, recall 0.76572\n",
      "2017-11-26T12:35:43.957687: step 863, loss 0.411001, acc 0.921875, prec 0.0300948, recall 0.765839\n",
      "2017-11-26T12:35:44.697898: step 864, loss 0.752237, acc 0.859375, prec 0.0301087, recall 0.765957\n",
      "2017-11-26T12:35:45.388020: step 865, loss 0.735301, acc 0.796875, prec 0.0301202, recall 0.766076\n",
      "2017-11-26T12:35:46.078183: step 866, loss 0.706151, acc 0.796875, prec 0.0301124, recall 0.766076\n",
      "2017-11-26T12:35:46.761388: step 867, loss 0.834553, acc 0.765625, prec 0.0301228, recall 0.766194\n",
      "2017-11-26T12:35:47.447546: step 868, loss 0.596013, acc 0.796875, prec 0.0301536, recall 0.766431\n",
      "2017-11-26T12:35:48.135120: step 869, loss 0.72917, acc 0.796875, prec 0.030165, recall 0.766549\n",
      "2017-11-26T12:35:48.830044: step 870, loss 0.601252, acc 0.8125, prec 0.0301771, recall 0.766667\n",
      "2017-11-26T12:35:49.524948: step 871, loss 0.358285, acc 0.859375, prec 0.0301717, recall 0.766667\n",
      "2017-11-26T12:35:50.211217: step 872, loss 0.725165, acc 0.796875, prec 0.0301832, recall 0.766784\n",
      "2017-11-26T12:35:50.873746: step 873, loss 0.810645, acc 0.84375, prec 0.0301965, recall 0.766902\n",
      "2017-11-26T12:35:51.528769: step 874, loss 7.98842, acc 0.921875, prec 0.0301941, recall 0.766515\n",
      "2017-11-26T12:35:52.201150: step 875, loss 0.672365, acc 0.859375, prec 0.0302079, recall 0.766633\n",
      "2017-11-26T12:35:52.868351: step 876, loss 11.5586, acc 0.765625, prec 0.0302194, recall 0.765979\n",
      "2017-11-26T12:35:53.578905: step 877, loss 0.894958, acc 0.765625, prec 0.0302104, recall 0.765979\n",
      "2017-11-26T12:35:54.248102: step 878, loss 0.997252, acc 0.828125, prec 0.0302808, recall 0.766449\n",
      "2017-11-26T12:35:54.915574: step 879, loss 1.02735, acc 0.765625, prec 0.030291, recall 0.766566\n",
      "2017-11-26T12:35:55.581350: step 880, loss 0.951948, acc 0.703125, prec 0.0303181, recall 0.7668\n",
      "2017-11-26T12:35:56.238304: step 881, loss 1.08149, acc 0.6875, prec 0.030306, recall 0.7668\n",
      "2017-11-26T12:35:56.899476: step 882, loss 1.34455, acc 0.625, prec 0.0303108, recall 0.766917\n",
      "2017-11-26T12:35:57.572229: step 883, loss 1.46668, acc 0.609375, prec 0.0303342, recall 0.767151\n",
      "2017-11-26T12:35:58.237124: step 884, loss 1.10975, acc 0.671875, prec 0.0303216, recall 0.767151\n",
      "2017-11-26T12:35:58.901079: step 885, loss 1.68171, acc 0.609375, prec 0.0303258, recall 0.767267\n",
      "2017-11-26T12:35:59.564368: step 886, loss 1.59857, acc 0.53125, prec 0.0303078, recall 0.767267\n",
      "2017-11-26T12:36:00.224453: step 887, loss 1.75932, acc 0.5625, prec 0.0302911, recall 0.767267\n",
      "2017-11-26T12:36:00.881727: step 888, loss 8.55903, acc 0.703125, prec 0.0302994, recall 0.767\n",
      "2017-11-26T12:36:01.541037: step 889, loss 1.3514, acc 0.703125, prec 0.0302881, recall 0.767\n",
      "2017-11-26T12:36:02.202806: step 890, loss 0.913228, acc 0.65625, prec 0.0302941, recall 0.767116\n",
      "2017-11-26T12:36:02.882190: step 891, loss 1.5252, acc 0.59375, prec 0.0302977, recall 0.767233\n",
      "2017-11-26T12:36:03.594331: step 892, loss 0.789545, acc 0.78125, prec 0.0302893, recall 0.767233\n",
      "2017-11-26T12:36:04.292628: step 893, loss 1.14159, acc 0.671875, prec 0.0302767, recall 0.767233\n",
      "2017-11-26T12:36:04.981849: step 894, loss 1.67777, acc 0.578125, prec 0.0302989, recall 0.767465\n",
      "2017-11-26T12:36:05.678380: step 895, loss 1.05179, acc 0.671875, prec 0.0302863, recall 0.767465\n",
      "2017-11-26T12:36:06.369338: step 896, loss 1.16415, acc 0.671875, prec 0.030312, recall 0.767697\n",
      "2017-11-26T12:36:07.056717: step 897, loss 1.2268, acc 0.640625, prec 0.0302983, recall 0.767697\n",
      "2017-11-26T12:36:07.751672: step 898, loss 0.8405, acc 0.765625, prec 0.0303084, recall 0.767813\n",
      "2017-11-26T12:36:08.447545: step 899, loss 0.921055, acc 0.703125, prec 0.0302971, recall 0.767813\n",
      "2017-11-26T12:36:09.128965: step 900, loss 0.874067, acc 0.890625, prec 0.030331, recall 0.768044\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:36:49.663517: step 900, loss 1.42062, acc 0.866767, prec 0.0312828, recall 0.760688\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-900\n",
      "\n",
      "2017-11-26T12:36:51.455173: step 901, loss 1.27074, acc 0.796875, prec 0.0312936, recall 0.760799\n",
      "2017-11-26T12:36:52.050296: step 902, loss 0.524793, acc 0.8125, prec 0.0313049, recall 0.76091\n",
      "2017-11-26T12:36:52.664422: step 903, loss 1.50801, acc 0.78125, prec 0.031315, recall 0.761021\n",
      "2017-11-26T12:36:53.288464: step 904, loss 0.32311, acc 0.8125, prec 0.0313079, recall 0.761021\n",
      "2017-11-26T12:36:53.947881: step 905, loss 0.540199, acc 0.84375, prec 0.0313019, recall 0.761021\n",
      "2017-11-26T12:36:54.577403: step 906, loss 0.422468, acc 0.84375, prec 0.0312959, recall 0.761021\n",
      "2017-11-26T12:36:55.204271: step 907, loss 0.423677, acc 0.875, prec 0.0313096, recall 0.761132\n",
      "2017-11-26T12:36:55.853691: step 908, loss 0.474162, acc 0.84375, prec 0.0313037, recall 0.761132\n",
      "2017-11-26T12:36:56.497196: step 909, loss 0.580963, acc 0.875, prec 0.0312989, recall 0.761132\n",
      "2017-11-26T12:36:57.127276: step 910, loss 0.443044, acc 0.875, prec 0.0312941, recall 0.761132\n",
      "2017-11-26T12:36:57.772640: step 911, loss 11.0921, acc 0.875, prec 0.0313275, recall 0.760648\n",
      "2017-11-26T12:36:58.419604: step 912, loss 0.507459, acc 0.90625, prec 0.0313608, recall 0.76087\n",
      "2017-11-26T12:36:59.059786: step 913, loss 0.310334, acc 0.859375, prec 0.0313554, recall 0.76087\n",
      "2017-11-26T12:36:59.698795: step 914, loss 12.4523, acc 0.78125, prec 0.0313483, recall 0.760166\n",
      "2017-11-26T12:37:00.415197: step 915, loss 0.462831, acc 0.84375, prec 0.0313423, recall 0.760166\n",
      "2017-11-26T12:37:01.092159: step 916, loss 2.12984, acc 0.703125, prec 0.0313863, recall 0.760498\n",
      "2017-11-26T12:37:01.797450: step 917, loss 0.81268, acc 0.78125, prec 0.0313964, recall 0.760609\n",
      "2017-11-26T12:37:02.467176: step 918, loss 1.50378, acc 0.609375, prec 0.0313999, recall 0.760719\n",
      "2017-11-26T12:37:03.117037: step 919, loss 2.10879, acc 0.53125, prec 0.0314188, recall 0.76094\n",
      "2017-11-26T12:37:03.774484: step 920, loss 2.12756, acc 0.5625, prec 0.0314389, recall 0.76116\n",
      "2017-11-26T12:37:04.470060: step 921, loss 2.16254, acc 0.578125, prec 0.0314596, recall 0.761379\n",
      "2017-11-26T12:37:05.144161: step 922, loss 1.84827, acc 0.59375, prec 0.0314624, recall 0.761489\n",
      "2017-11-26T12:37:05.808832: step 923, loss 1.66446, acc 0.5625, prec 0.0314641, recall 0.761599\n",
      "2017-11-26T12:37:06.473728: step 924, loss 1.98309, acc 0.609375, prec 0.0315043, recall 0.761927\n",
      "2017-11-26T12:37:07.136952: step 925, loss 1.84442, acc 0.5625, prec 0.0315059, recall 0.762036\n",
      "2017-11-26T12:37:07.801275: step 926, loss 1.42698, acc 0.65625, prec 0.0314928, recall 0.762036\n",
      "2017-11-26T12:37:08.464003: step 927, loss 0.98108, acc 0.671875, prec 0.0315169, recall 0.762254\n",
      "2017-11-26T12:37:09.128597: step 928, loss 1.41508, acc 0.546875, prec 0.031518, recall 0.762363\n",
      "2017-11-26T12:37:09.789955: step 929, loss 1.86986, acc 0.5625, prec 0.0315379, recall 0.76258\n",
      "2017-11-26T12:37:10.426086: step 930, loss 1.65332, acc 0.5625, prec 0.0315395, recall 0.762689\n",
      "2017-11-26T12:37:11.061349: step 931, loss 1.27053, acc 0.65625, prec 0.0315813, recall 0.763014\n",
      "2017-11-26T12:37:11.692145: step 932, loss 1.11395, acc 0.703125, prec 0.03157, recall 0.763014\n",
      "2017-11-26T12:37:12.328090: step 933, loss 3.01611, acc 0.8125, prec 0.0315817, recall 0.762774\n",
      "2017-11-26T12:37:12.963516: step 934, loss 0.684102, acc 0.78125, prec 0.0315734, recall 0.762774\n",
      "2017-11-26T12:37:13.603989: step 935, loss 0.900443, acc 0.765625, prec 0.0315644, recall 0.762774\n",
      "2017-11-26T12:37:14.257453: step 936, loss 0.858065, acc 0.78125, prec 0.0315561, recall 0.762774\n",
      "2017-11-26T12:37:14.918955: step 937, loss 15.3567, acc 0.796875, prec 0.031549, recall 0.762426\n",
      "2017-11-26T12:37:15.593926: step 938, loss 0.648411, acc 0.78125, prec 0.0315589, recall 0.762534\n",
      "2017-11-26T12:37:16.251804: step 939, loss 0.630686, acc 0.828125, prec 0.0315706, recall 0.762642\n",
      "2017-11-26T12:37:16.908670: step 940, loss 13.9614, acc 0.796875, prec 0.0315641, recall 0.761948\n",
      "2017-11-26T12:37:17.571088: step 941, loss 0.774032, acc 0.78125, prec 0.031574, recall 0.762056\n",
      "2017-11-26T12:37:18.221122: step 942, loss 0.965382, acc 0.765625, prec 0.0315833, recall 0.762165\n",
      "2017-11-26T12:37:18.873637: step 943, loss 1.16751, acc 0.71875, prec 0.0315908, recall 0.762273\n",
      "2017-11-26T12:37:19.532446: step 944, loss 1.73106, acc 0.703125, prec 0.0316342, recall 0.762596\n",
      "2017-11-26T12:37:20.189037: step 945, loss 1.04726, acc 0.71875, prec 0.0316418, recall 0.762704\n",
      "2017-11-26T12:37:20.844466: step 946, loss 1.79591, acc 0.578125, prec 0.0316621, recall 0.762919\n",
      "2017-11-26T12:37:21.509701: step 947, loss 0.810231, acc 0.765625, prec 0.0316532, recall 0.762919\n",
      "2017-11-26T12:37:22.168840: step 948, loss 4.09293, acc 0.71875, prec 0.0316431, recall 0.762574\n",
      "2017-11-26T12:37:22.830541: step 949, loss 1.22061, acc 0.640625, prec 0.0316294, recall 0.762574\n",
      "2017-11-26T12:37:23.494188: step 950, loss 1.50253, acc 0.6875, prec 0.0316357, recall 0.762681\n",
      "2017-11-26T12:37:24.185002: step 951, loss 2.64661, acc 0.65625, prec 0.0316232, recall 0.762336\n",
      "2017-11-26T12:37:24.847122: step 952, loss 1.27585, acc 0.6875, prec 0.0316114, recall 0.762336\n",
      "2017-11-26T12:37:25.506530: step 953, loss 0.888509, acc 0.6875, prec 0.0315995, recall 0.762336\n",
      "2017-11-26T12:37:26.162338: step 954, loss 1.20502, acc 0.640625, prec 0.0316222, recall 0.762551\n",
      "2017-11-26T12:37:26.821741: step 955, loss 1.06211, acc 0.65625, prec 0.0316091, recall 0.762551\n",
      "2017-11-26T12:37:27.480428: step 956, loss 0.76376, acc 0.671875, prec 0.0315967, recall 0.762551\n",
      "2017-11-26T12:37:28.135682: step 957, loss 1.2884, acc 0.578125, prec 0.0315807, recall 0.762551\n",
      "2017-11-26T12:37:28.789075: step 958, loss 1.1274, acc 0.765625, prec 0.0315719, recall 0.762551\n",
      "2017-11-26T12:37:29.449847: step 959, loss 0.819054, acc 0.6875, prec 0.0315782, recall 0.762658\n",
      "2017-11-26T12:37:30.107807: step 960, loss 1.13218, acc 0.65625, prec 0.0315652, recall 0.762658\n",
      "2017-11-26T12:37:30.771037: step 961, loss 1.04862, acc 0.65625, prec 0.0315522, recall 0.762658\n",
      "2017-11-26T12:37:31.436617: step 962, loss 3.79784, acc 0.78125, prec 0.0315807, recall 0.762528\n",
      "2017-11-26T12:37:32.098940: step 963, loss 0.716942, acc 0.78125, prec 0.0315906, recall 0.762635\n",
      "2017-11-26T12:37:32.760532: step 964, loss 0.949371, acc 0.734375, prec 0.0315805, recall 0.762635\n",
      "2017-11-26T12:37:33.423739: step 965, loss 0.359818, acc 0.84375, prec 0.0315927, recall 0.762742\n",
      "2017-11-26T12:37:34.077285: step 966, loss 0.737865, acc 0.71875, prec 0.0315821, recall 0.762742\n",
      "2017-11-26T12:37:34.769969: step 967, loss 7.02017, acc 0.765625, prec 0.0315738, recall 0.762399\n",
      "2017-11-26T12:37:35.446668: step 968, loss 0.359826, acc 0.828125, prec 0.0315674, recall 0.762399\n",
      "2017-11-26T12:37:36.104182: step 969, loss 0.944857, acc 0.78125, prec 0.0315591, recall 0.762399\n",
      "2017-11-26T12:37:36.765028: step 970, loss 1.67093, acc 0.796875, prec 0.0316057, recall 0.76272\n",
      "2017-11-26T12:37:37.429609: step 971, loss 0.468453, acc 0.84375, prec 0.0315998, recall 0.76272\n",
      "2017-11-26T12:37:38.094915: step 972, loss 0.703098, acc 0.8125, prec 0.0315927, recall 0.76272\n",
      "2017-11-26T12:37:38.754788: step 973, loss 0.350674, acc 0.84375, prec 0.0315868, recall 0.76272\n",
      "2017-11-26T12:37:39.409478: step 974, loss 0.641171, acc 0.828125, prec 0.0316164, recall 0.762933\n",
      "2017-11-26T12:37:40.064033: step 975, loss 0.499585, acc 0.8125, prec 0.0316274, recall 0.76304\n",
      "2017-11-26T12:37:40.724203: step 976, loss 0.673972, acc 0.78125, prec 0.0316192, recall 0.76304\n",
      "2017-11-26T12:37:41.386907: step 977, loss 1.59143, acc 0.8125, prec 0.0316301, recall 0.763146\n",
      "2017-11-26T12:37:42.044600: step 978, loss 0.603948, acc 0.765625, prec 0.0316574, recall 0.763359\n",
      "2017-11-26T12:37:42.705222: step 979, loss 8.61176, acc 0.75, prec 0.0316665, recall 0.763122\n",
      "2017-11-26T12:37:43.366074: step 980, loss 0.474712, acc 0.828125, prec 0.0316601, recall 0.763122\n",
      "2017-11-26T12:37:44.023754: step 981, loss 0.418609, acc 0.796875, prec 0.0316524, recall 0.763122\n",
      "2017-11-26T12:37:44.726247: step 982, loss 0.740873, acc 0.78125, prec 0.0316442, recall 0.763122\n",
      "2017-11-26T12:37:45.383549: step 983, loss 1.03529, acc 0.703125, prec 0.031651, recall 0.763229\n",
      "2017-11-26T12:37:46.049929: step 984, loss 9.00163, acc 0.78125, prec 0.0316433, recall 0.762887\n",
      "2017-11-26T12:37:46.713853: step 985, loss 0.498086, acc 0.875, prec 0.0316566, recall 0.762993\n",
      "2017-11-26T12:37:47.379102: step 986, loss 1.42846, acc 0.734375, prec 0.0316646, recall 0.763099\n",
      "2017-11-26T12:37:48.041396: step 987, loss 1.20667, acc 0.703125, prec 0.0316534, recall 0.763099\n",
      "2017-11-26T12:37:48.697896: step 988, loss 0.55471, acc 0.8125, prec 0.0316464, recall 0.763099\n",
      "2017-11-26T12:37:49.351915: step 989, loss 0.786215, acc 0.75, prec 0.031637, recall 0.763099\n",
      "2017-11-26T12:37:50.005144: step 990, loss 1.04124, acc 0.734375, prec 0.031645, recall 0.763205\n",
      "2017-11-26T12:37:50.669010: step 991, loss 0.628252, acc 0.75, prec 0.0316536, recall 0.763311\n",
      "2017-11-26T12:37:51.327747: step 992, loss 0.696098, acc 0.765625, prec 0.0316627, recall 0.763417\n",
      "2017-11-26T12:37:51.993424: step 993, loss 5.85558, acc 0.734375, prec 0.0316533, recall 0.763076\n",
      "2017-11-26T12:37:52.563701: step 994, loss 0.310942, acc 0.826923, prec 0.031648, recall 0.763076\n",
      "2017-11-26T12:37:53.220128: step 995, loss 4.25467, acc 0.8125, prec 0.0316595, recall 0.762841\n",
      "2017-11-26T12:37:53.878368: step 996, loss 0.681451, acc 0.78125, prec 0.0316513, recall 0.762841\n",
      "2017-11-26T12:37:54.553639: step 997, loss 0.790326, acc 0.8125, prec 0.0316443, recall 0.762841\n",
      "2017-11-26T12:37:55.225676: step 998, loss 1.28712, acc 0.78125, prec 0.0316899, recall 0.763158\n",
      "2017-11-26T12:37:55.890382: step 999, loss 0.661322, acc 0.71875, prec 0.0316972, recall 0.763263\n",
      "2017-11-26T12:37:56.546551: step 1000, loss 1.04401, acc 0.671875, prec 0.0316849, recall 0.763263\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:38:38.209237: step 1000, loss 1.1182, acc 0.775429, prec 0.03237, recall 0.766555\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1000\n",
      "\n",
      "2017-11-26T12:38:40.101095: step 1001, loss 0.77336, acc 0.734375, prec 0.0323773, recall 0.766653\n",
      "2017-11-26T12:38:40.722912: step 1002, loss 0.637302, acc 0.71875, prec 0.032367, recall 0.766653\n",
      "2017-11-26T12:38:41.350117: step 1003, loss 0.711714, acc 0.75, prec 0.0323579, recall 0.766653\n",
      "2017-11-26T12:38:41.974977: step 1004, loss 0.754625, acc 0.8125, prec 0.032351, recall 0.766653\n",
      "2017-11-26T12:38:42.624513: step 1005, loss 1.41151, acc 0.59375, prec 0.0323533, recall 0.76675\n",
      "2017-11-26T12:38:43.270464: step 1006, loss 0.781149, acc 0.8125, prec 0.0323464, recall 0.76675\n",
      "2017-11-26T12:38:43.910830: step 1007, loss 0.522646, acc 0.796875, prec 0.032339, recall 0.76675\n",
      "2017-11-26T12:38:44.546626: step 1008, loss 0.855727, acc 0.75, prec 0.0323469, recall 0.766848\n",
      "2017-11-26T12:38:45.180598: step 1009, loss 0.475453, acc 0.796875, prec 0.0323395, recall 0.766848\n",
      "2017-11-26T12:38:45.840351: step 1010, loss 0.616248, acc 0.78125, prec 0.0323315, recall 0.766848\n",
      "2017-11-26T12:38:46.500883: step 1011, loss 0.820277, acc 0.828125, prec 0.0323423, recall 0.766946\n",
      "2017-11-26T12:38:47.163718: step 1012, loss 0.341106, acc 0.890625, prec 0.0323554, recall 0.767043\n",
      "2017-11-26T12:38:47.830654: step 1013, loss 0.469642, acc 0.859375, prec 0.0323502, recall 0.767043\n",
      "2017-11-26T12:38:48.486609: step 1014, loss 0.399695, acc 0.84375, prec 0.0323445, recall 0.767043\n",
      "2017-11-26T12:38:49.139388: step 1015, loss 0.244965, acc 0.921875, prec 0.0323417, recall 0.767043\n",
      "2017-11-26T12:38:49.797261: step 1016, loss 0.220322, acc 0.90625, prec 0.0323553, recall 0.76714\n",
      "2017-11-26T12:38:50.457877: step 1017, loss 0.477611, acc 0.828125, prec 0.0323832, recall 0.767335\n",
      "2017-11-26T12:38:51.117810: step 1018, loss 0.24712, acc 0.921875, prec 0.0323974, recall 0.767432\n",
      "2017-11-26T12:38:51.778987: step 1019, loss 0.311174, acc 0.9375, prec 0.0324121, recall 0.767529\n",
      "2017-11-26T12:38:52.442828: step 1020, loss 1.55745, acc 0.921875, prec 0.0324434, recall 0.767723\n",
      "2017-11-26T12:38:53.106107: step 1021, loss 0.961581, acc 0.875, prec 0.0324559, recall 0.76782\n",
      "2017-11-26T12:38:53.764715: step 1022, loss 11.8822, acc 0.875, prec 0.0324524, recall 0.76718\n",
      "2017-11-26T12:38:54.422247: step 1023, loss 0.179523, acc 0.90625, prec 0.032466, recall 0.767277\n",
      "2017-11-26T12:38:55.080417: step 1024, loss 0.54817, acc 0.796875, prec 0.0324757, recall 0.767374\n",
      "2017-11-26T12:38:55.773412: step 1025, loss 0.706917, acc 0.859375, prec 0.0324875, recall 0.767471\n",
      "2017-11-26T12:38:56.437587: step 1026, loss 1.7972, acc 0.875, prec 0.0324835, recall 0.767152\n",
      "2017-11-26T12:38:57.114387: step 1027, loss 0.739042, acc 0.765625, prec 0.032492, recall 0.767249\n",
      "2017-11-26T12:38:57.779744: step 1028, loss 0.465347, acc 0.84375, prec 0.0324863, recall 0.767249\n",
      "2017-11-26T12:38:58.443249: step 1029, loss 0.710155, acc 0.75, prec 0.0324771, recall 0.767249\n",
      "2017-11-26T12:38:59.122016: step 1030, loss 1.09308, acc 0.765625, prec 0.0325026, recall 0.767442\n",
      "2017-11-26T12:38:59.828718: step 1031, loss 0.524772, acc 0.8125, prec 0.0325127, recall 0.767538\n",
      "2017-11-26T12:39:00.556921: step 1032, loss 0.730009, acc 0.84375, prec 0.032524, recall 0.767635\n",
      "2017-11-26T12:39:01.266790: step 1033, loss 0.721899, acc 0.765625, prec 0.0325155, recall 0.767635\n",
      "2017-11-26T12:39:01.965301: step 1034, loss 1.31882, acc 0.734375, prec 0.0325228, recall 0.767731\n",
      "2017-11-26T12:39:02.657401: step 1035, loss 0.90092, acc 0.703125, prec 0.0325119, recall 0.767731\n",
      "2017-11-26T12:39:03.347842: step 1036, loss 0.872057, acc 0.71875, prec 0.0325186, recall 0.767828\n",
      "2017-11-26T12:39:04.046484: step 1037, loss 3.56338, acc 0.609375, prec 0.0325049, recall 0.767509\n",
      "2017-11-26T12:39:04.733427: step 1038, loss 0.980453, acc 0.765625, prec 0.0325133, recall 0.767606\n",
      "2017-11-26T12:39:05.418635: step 1039, loss 0.851127, acc 0.75, prec 0.0325042, recall 0.767606\n",
      "2017-11-26T12:39:06.140326: step 1040, loss 0.803475, acc 0.75, prec 0.0324951, recall 0.767606\n",
      "2017-11-26T12:39:06.847055: step 1041, loss 0.352143, acc 0.921875, prec 0.0325262, recall 0.767798\n",
      "2017-11-26T12:39:07.541191: step 1042, loss 2.58242, acc 0.71875, prec 0.0325165, recall 0.76748\n",
      "2017-11-26T12:39:08.221636: step 1043, loss 1.52297, acc 0.71875, prec 0.0325232, recall 0.767577\n",
      "2017-11-26T12:39:08.884083: step 1044, loss 1.50909, acc 0.75, prec 0.032531, recall 0.767673\n",
      "2017-11-26T12:39:09.562352: step 1045, loss 1.05373, acc 0.71875, prec 0.0325208, recall 0.767673\n",
      "2017-11-26T12:39:10.229566: step 1046, loss 1.13866, acc 0.609375, prec 0.0325235, recall 0.767769\n",
      "2017-11-26T12:39:10.885510: step 1047, loss 0.581627, acc 0.8125, prec 0.0325336, recall 0.767865\n",
      "2017-11-26T12:39:11.538422: step 1048, loss 0.961105, acc 0.734375, prec 0.0325408, recall 0.76796\n",
      "2017-11-26T12:39:12.197639: step 1049, loss 0.896223, acc 0.75, prec 0.0325317, recall 0.76796\n",
      "2017-11-26T12:39:12.859538: step 1050, loss 1.11205, acc 0.71875, prec 0.0325384, recall 0.768056\n",
      "2017-11-26T12:39:13.537284: step 1051, loss 0.961862, acc 0.78125, prec 0.0325811, recall 0.768343\n",
      "2017-11-26T12:39:14.199720: step 1052, loss 0.427992, acc 0.828125, prec 0.0325918, recall 0.768438\n",
      "2017-11-26T12:39:14.855916: step 1053, loss 0.809459, acc 0.84375, prec 0.0326199, recall 0.768629\n",
      "2017-11-26T12:39:15.517956: step 1054, loss 0.774959, acc 0.796875, prec 0.0326294, recall 0.768724\n",
      "2017-11-26T12:39:16.199852: step 1055, loss 0.444006, acc 0.859375, prec 0.0326581, recall 0.768914\n",
      "2017-11-26T12:39:16.870277: step 1056, loss 0.454499, acc 0.90625, prec 0.0326884, recall 0.769104\n",
      "2017-11-26T12:39:17.548358: step 1057, loss 0.484993, acc 0.828125, prec 0.032699, recall 0.769199\n",
      "2017-11-26T12:39:18.237695: step 1058, loss 0.521418, acc 0.765625, prec 0.0326905, recall 0.769199\n",
      "2017-11-26T12:39:18.946540: step 1059, loss 0.229304, acc 0.90625, prec 0.0327039, recall 0.769294\n",
      "2017-11-26T12:39:19.654795: step 1060, loss 0.338582, acc 0.875, prec 0.0327162, recall 0.769389\n",
      "2017-11-26T12:39:20.353792: step 1061, loss 0.291493, acc 0.859375, prec 0.0327111, recall 0.769389\n",
      "2017-11-26T12:39:21.057607: step 1062, loss 8.4107, acc 0.875, prec 0.0327071, recall 0.769073\n",
      "2017-11-26T12:39:21.752072: step 1063, loss 0.238879, acc 0.921875, prec 0.0327211, recall 0.769168\n",
      "2017-11-26T12:39:22.438608: step 1064, loss 17.9397, acc 0.890625, prec 0.0327183, recall 0.768537\n",
      "2017-11-26T12:39:23.165973: step 1065, loss 0.460715, acc 0.859375, prec 0.03273, recall 0.768632\n",
      "2017-11-26T12:39:23.856065: step 1066, loss 0.47505, acc 0.875, prec 0.0327423, recall 0.768727\n",
      "2017-11-26T12:39:24.539803: step 1067, loss 0.712596, acc 0.859375, prec 0.032754, recall 0.768822\n",
      "2017-11-26T12:39:25.230076: step 1068, loss 0.862327, acc 0.734375, prec 0.0327443, recall 0.768822\n",
      "2017-11-26T12:39:25.976638: step 1069, loss 1.84408, acc 0.828125, prec 0.0328055, recall 0.769199\n",
      "2017-11-26T12:39:26.672561: step 1070, loss 1.52312, acc 0.640625, prec 0.0327923, recall 0.769199\n",
      "2017-11-26T12:39:27.423624: step 1071, loss 1.11289, acc 0.6875, prec 0.0328146, recall 0.769388\n",
      "2017-11-26T12:39:28.110561: step 1072, loss 1.1984, acc 0.6875, prec 0.03282, recall 0.769482\n",
      "2017-11-26T12:39:28.796208: step 1073, loss 1.15403, acc 0.703125, prec 0.032826, recall 0.769576\n",
      "2017-11-26T12:39:29.456521: step 1074, loss 1.25269, acc 0.671875, prec 0.0328308, recall 0.76967\n",
      "2017-11-26T12:39:30.114179: step 1075, loss 1.46824, acc 0.671875, prec 0.0328188, recall 0.76967\n",
      "2017-11-26T12:39:30.774401: step 1076, loss 1.38277, acc 0.609375, prec 0.0328045, recall 0.76967\n",
      "2017-11-26T12:39:31.431024: step 1077, loss 1.03265, acc 0.703125, prec 0.0327937, recall 0.76967\n",
      "2017-11-26T12:39:32.090956: step 1078, loss 1.4275, acc 0.640625, prec 0.0327974, recall 0.769764\n",
      "2017-11-26T12:39:32.756663: step 1079, loss 1.06013, acc 0.6875, prec 0.0328364, recall 0.770045\n",
      "2017-11-26T12:39:33.418825: step 1080, loss 0.796449, acc 0.734375, prec 0.0328267, recall 0.770045\n",
      "2017-11-26T12:39:34.072509: step 1081, loss 0.689418, acc 0.796875, prec 0.0328193, recall 0.770045\n",
      "2017-11-26T12:39:34.731165: step 1082, loss 1.4284, acc 0.75, prec 0.032827, recall 0.770138\n",
      "2017-11-26T12:39:35.402629: step 1083, loss 1.06945, acc 0.6875, prec 0.0328156, recall 0.770138\n",
      "2017-11-26T12:39:36.131308: step 1084, loss 1.16421, acc 0.65625, prec 0.0328031, recall 0.770138\n",
      "2017-11-26T12:39:36.807632: step 1085, loss 0.434691, acc 0.8125, prec 0.032813, recall 0.770232\n",
      "2017-11-26T12:39:37.478066: step 1086, loss 0.602831, acc 0.84375, prec 0.0328073, recall 0.770232\n",
      "2017-11-26T12:39:38.168032: step 1087, loss 0.437431, acc 0.828125, prec 0.0328011, recall 0.770232\n",
      "2017-11-26T12:39:38.893798: step 1088, loss 0.772964, acc 0.765625, prec 0.0328428, recall 0.770512\n",
      "2017-11-26T12:39:39.709117: step 1089, loss 0.566616, acc 0.8125, prec 0.032836, recall 0.770512\n",
      "2017-11-26T12:39:40.642110: step 1090, loss 0.684223, acc 0.875, prec 0.0328482, recall 0.770605\n",
      "2017-11-26T12:39:41.457991: step 1091, loss 0.41099, acc 0.890625, prec 0.0328609, recall 0.770698\n",
      "2017-11-26T12:39:42.261290: step 1092, loss 0.36218, acc 0.875, prec 0.0328564, recall 0.770698\n",
      "2017-11-26T12:39:43.076369: step 1093, loss 0.349671, acc 0.875, prec 0.0328686, recall 0.770791\n",
      "2017-11-26T12:39:43.899191: step 1094, loss 1.3921, acc 0.890625, prec 0.0328819, recall 0.770572\n",
      "2017-11-26T12:39:44.723353: step 1095, loss 0.29947, acc 0.9375, prec 0.0328796, recall 0.770572\n",
      "2017-11-26T12:39:45.530847: step 1096, loss 0.13888, acc 0.921875, prec 0.0328768, recall 0.770572\n",
      "2017-11-26T12:39:46.470605: step 1097, loss 1.01352, acc 0.953125, prec 0.0328918, recall 0.770665\n",
      "2017-11-26T12:39:47.377228: step 1098, loss 13.4038, acc 0.953125, prec 0.0329074, recall 0.770445\n",
      "2017-11-26T12:39:48.193647: step 1099, loss 0.104411, acc 0.96875, prec 0.0329062, recall 0.770445\n",
      "2017-11-26T12:39:49.039908: step 1100, loss 0.639421, acc 0.890625, prec 0.0329357, recall 0.770631\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:40:34.336069: step 1100, loss 2.11574, acc 0.932346, prec 0.0336499, recall 0.753346\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1100\n",
      "\n",
      "2017-11-26T12:40:36.344360: step 1101, loss 2.9109, acc 0.859375, prec 0.0337113, recall 0.753435\n",
      "2017-11-26T12:40:36.977875: step 1102, loss 1.20314, acc 0.90625, prec 0.0337244, recall 0.753529\n",
      "2017-11-26T12:40:37.593665: step 1103, loss 0.587389, acc 0.875, prec 0.0337692, recall 0.753811\n",
      "2017-11-26T12:40:38.222495: step 1104, loss 1.10523, acc 0.75, prec 0.033793, recall 0.753998\n",
      "2017-11-26T12:40:38.842630: step 1105, loss 0.515734, acc 0.828125, prec 0.0338032, recall 0.754092\n",
      "2017-11-26T12:40:39.476165: step 1106, loss 0.689151, acc 0.78125, prec 0.033828, recall 0.754279\n",
      "2017-11-26T12:40:40.133527: step 1107, loss 1.22267, acc 0.625, prec 0.0338142, recall 0.754279\n",
      "2017-11-26T12:40:40.798287: step 1108, loss 1.09488, acc 0.703125, prec 0.0338197, recall 0.754373\n",
      "2017-11-26T12:40:41.449370: step 1109, loss 0.890431, acc 0.75, prec 0.0338105, recall 0.754373\n",
      "2017-11-26T12:40:42.127139: step 1110, loss 0.751528, acc 0.8125, prec 0.03382, recall 0.754466\n",
      "2017-11-26T12:40:42.790535: step 1111, loss 0.867217, acc 0.765625, prec 0.0338443, recall 0.754653\n",
      "2017-11-26T12:40:43.436490: step 1112, loss 1.54647, acc 0.59375, prec 0.0338293, recall 0.754653\n",
      "2017-11-26T12:40:44.114240: step 1113, loss 1.36764, acc 0.703125, prec 0.0338677, recall 0.754932\n",
      "2017-11-26T12:40:44.763290: step 1114, loss 1.33175, acc 0.671875, prec 0.0338556, recall 0.754932\n",
      "2017-11-26T12:40:45.423118: step 1115, loss 0.978145, acc 0.75, prec 0.0338464, recall 0.754932\n",
      "2017-11-26T12:40:46.103035: step 1116, loss 0.992461, acc 0.71875, prec 0.0338525, recall 0.755025\n",
      "2017-11-26T12:40:46.789557: step 1117, loss 0.578216, acc 0.765625, prec 0.0338439, recall 0.755025\n",
      "2017-11-26T12:40:47.516718: step 1118, loss 0.721973, acc 0.796875, prec 0.0338364, recall 0.755025\n",
      "2017-11-26T12:40:48.209977: step 1119, loss 0.881325, acc 0.75, prec 0.0338436, recall 0.755118\n",
      "2017-11-26T12:40:48.890045: step 1120, loss 3.81273, acc 0.765625, prec 0.033852, recall 0.754924\n",
      "2017-11-26T12:40:49.565105: step 1121, loss 0.6139, acc 0.8125, prec 0.0338451, recall 0.754924\n",
      "2017-11-26T12:40:50.236537: step 1122, loss 0.596289, acc 0.75, prec 0.0338523, recall 0.755017\n",
      "2017-11-26T12:40:50.888828: step 1123, loss 0.919538, acc 0.828125, prec 0.0338787, recall 0.755202\n",
      "2017-11-26T12:40:51.538016: step 1124, loss 0.576777, acc 0.84375, prec 0.033873, recall 0.755202\n",
      "2017-11-26T12:40:52.195143: step 1125, loss 0.640027, acc 0.8125, prec 0.0338825, recall 0.755295\n",
      "2017-11-26T12:40:52.851098: step 1126, loss 0.473161, acc 0.84375, prec 0.0338931, recall 0.755388\n",
      "2017-11-26T12:40:53.494600: step 1127, loss 0.627519, acc 0.796875, prec 0.033902, recall 0.75548\n",
      "2017-11-26T12:40:54.137343: step 1128, loss 1.90782, acc 0.78125, prec 0.0339109, recall 0.755287\n",
      "2017-11-26T12:40:54.789871: step 1129, loss 1.7249, acc 0.921875, prec 0.0339736, recall 0.755656\n",
      "2017-11-26T12:40:55.439580: step 1130, loss 0.355152, acc 0.875, prec 0.033969, recall 0.755656\n",
      "2017-11-26T12:40:56.093680: step 1131, loss 0.464189, acc 0.890625, prec 0.0339813, recall 0.755748\n",
      "2017-11-26T12:40:56.782112: step 1132, loss 0.559845, acc 0.8125, prec 0.0340072, recall 0.755932\n",
      "2017-11-26T12:40:57.424707: step 1133, loss 0.711993, acc 0.765625, prec 0.0340312, recall 0.756116\n",
      "2017-11-26T12:40:58.075298: step 1134, loss 0.416814, acc 0.890625, prec 0.0340599, recall 0.756299\n",
      "2017-11-26T12:40:58.731157: step 1135, loss 1.38606, acc 0.890625, prec 0.0341213, recall 0.756665\n",
      "2017-11-26T12:40:59.380105: step 1136, loss 0.418801, acc 0.859375, prec 0.0341161, recall 0.756665\n",
      "2017-11-26T12:41:00.029559: step 1137, loss 0.714933, acc 0.765625, prec 0.0341075, recall 0.756665\n",
      "2017-11-26T12:41:00.699710: step 1138, loss 4.82265, acc 0.78125, prec 0.0341163, recall 0.756473\n",
      "2017-11-26T12:41:01.345691: step 1139, loss 2.40797, acc 0.890625, prec 0.0341128, recall 0.756189\n",
      "2017-11-26T12:41:01.992785: step 1140, loss 0.693584, acc 0.71875, prec 0.0341024, recall 0.756189\n",
      "2017-11-26T12:41:02.640249: step 1141, loss 0.778083, acc 0.8125, prec 0.0340955, recall 0.756189\n",
      "2017-11-26T12:41:03.292874: step 1142, loss 0.838264, acc 0.75, prec 0.0341026, recall 0.75628\n",
      "2017-11-26T12:41:03.944867: step 1143, loss 0.979389, acc 0.734375, prec 0.0340928, recall 0.75628\n",
      "2017-11-26T12:41:04.590485: step 1144, loss 6.15932, acc 0.65625, prec 0.0340807, recall 0.755997\n",
      "2017-11-26T12:41:05.243638: step 1145, loss 1.13659, acc 0.71875, prec 0.0340704, recall 0.755997\n",
      "2017-11-26T12:41:05.892981: step 1146, loss 0.86001, acc 0.75, prec 0.0340775, recall 0.756088\n",
      "2017-11-26T12:41:06.604463: step 1147, loss 1.39504, acc 0.546875, prec 0.0340608, recall 0.756088\n",
      "2017-11-26T12:41:07.279373: step 1148, loss 0.95996, acc 0.75, prec 0.0340842, recall 0.756271\n",
      "2017-11-26T12:41:07.932431: step 1149, loss 0.985238, acc 0.765625, prec 0.0340919, recall 0.756362\n",
      "2017-11-26T12:41:08.588183: step 1150, loss 1.14899, acc 0.71875, prec 0.0341467, recall 0.756726\n",
      "2017-11-26T12:41:09.238942: step 1151, loss 5.04784, acc 0.59375, prec 0.0341486, recall 0.756535\n",
      "2017-11-26T12:41:09.893360: step 1152, loss 1.77151, acc 0.609375, prec 0.0341667, recall 0.756716\n",
      "2017-11-26T12:41:10.546291: step 1153, loss 0.819139, acc 0.71875, prec 0.0341726, recall 0.756807\n",
      "2017-11-26T12:41:11.198560: step 1154, loss 1.48281, acc 0.609375, prec 0.0341908, recall 0.756988\n",
      "2017-11-26T12:41:11.841634: step 1155, loss 2.7908, acc 0.609375, prec 0.034177, recall 0.756706\n",
      "2017-11-26T12:41:12.498101: step 1156, loss 1.44561, acc 0.65625, prec 0.0341968, recall 0.756888\n",
      "2017-11-26T12:41:13.141303: step 1157, loss 1.16311, acc 0.625, prec 0.0341992, recall 0.756978\n",
      "2017-11-26T12:41:13.795088: step 1158, loss 0.967022, acc 0.71875, prec 0.0341889, recall 0.756978\n",
      "2017-11-26T12:41:14.444361: step 1159, loss 1.22711, acc 0.6875, prec 0.0341936, recall 0.757068\n",
      "2017-11-26T12:41:15.095887: step 1160, loss 1.20527, acc 0.625, prec 0.0341799, recall 0.757068\n",
      "2017-11-26T12:41:15.743050: step 1161, loss 1.29751, acc 0.65625, prec 0.0341672, recall 0.757068\n",
      "2017-11-26T12:41:16.405442: step 1162, loss 1.26424, acc 0.65625, prec 0.0341708, recall 0.757159\n",
      "2017-11-26T12:41:17.063323: step 1163, loss 1.05142, acc 0.71875, prec 0.0341767, recall 0.757249\n",
      "2017-11-26T12:41:17.714655: step 1164, loss 1.32101, acc 0.71875, prec 0.0341988, recall 0.757429\n",
      "2017-11-26T12:41:18.358294: step 1165, loss 1.07208, acc 0.78125, prec 0.034207, recall 0.757519\n",
      "2017-11-26T12:41:19.009108: step 1166, loss 0.757129, acc 0.765625, prec 0.0341984, recall 0.757519\n",
      "2017-11-26T12:41:19.663883: step 1167, loss 0.695161, acc 0.765625, prec 0.0341898, recall 0.757519\n",
      "2017-11-26T12:41:20.315955: step 1168, loss 1.81588, acc 0.765625, prec 0.0341973, recall 0.757609\n",
      "2017-11-26T12:41:20.961882: step 1169, loss 0.871258, acc 0.6875, prec 0.0341859, recall 0.757609\n",
      "2017-11-26T12:41:21.609946: step 1170, loss 1.9197, acc 0.828125, prec 0.0342119, recall 0.757789\n",
      "2017-11-26T12:41:22.261689: step 1171, loss 0.824048, acc 0.78125, prec 0.0342363, recall 0.757969\n",
      "2017-11-26T12:41:22.914989: step 1172, loss 0.508577, acc 0.8125, prec 0.0342294, recall 0.757969\n",
      "2017-11-26T12:41:23.560787: step 1173, loss 0.538942, acc 0.875, prec 0.034241, recall 0.758059\n",
      "2017-11-26T12:41:24.213616: step 1174, loss 0.601906, acc 0.796875, prec 0.0342335, recall 0.758059\n",
      "2017-11-26T12:41:24.865947: step 1175, loss 0.257147, acc 0.90625, prec 0.0342462, recall 0.758148\n",
      "2017-11-26T12:41:25.511259: step 1176, loss 0.367882, acc 0.828125, prec 0.0342399, recall 0.758148\n",
      "2017-11-26T12:41:26.166086: step 1177, loss 0.368622, acc 0.890625, prec 0.0342521, recall 0.758238\n",
      "2017-11-26T12:41:26.859719: step 1178, loss 0.847807, acc 0.78125, prec 0.0342602, recall 0.758327\n",
      "2017-11-26T12:41:27.512581: step 1179, loss 0.176497, acc 0.953125, prec 0.0342585, recall 0.758327\n",
      "2017-11-26T12:41:28.166156: step 1180, loss 0.304733, acc 0.890625, prec 0.0342545, recall 0.758327\n",
      "2017-11-26T12:41:28.812918: step 1181, loss 0.372811, acc 0.84375, prec 0.0342649, recall 0.758417\n",
      "2017-11-26T12:41:29.468356: step 1182, loss 3.40681, acc 0.828125, prec 0.0342592, recall 0.758136\n",
      "2017-11-26T12:41:30.119532: step 1183, loss 0.280129, acc 0.84375, prec 0.0342534, recall 0.758136\n",
      "2017-11-26T12:41:30.782437: step 1184, loss 0.302509, acc 0.9375, prec 0.0342673, recall 0.758226\n",
      "2017-11-26T12:41:31.431913: step 1185, loss 8.63687, acc 0.890625, prec 0.0342639, recall 0.757945\n",
      "2017-11-26T12:41:32.088851: step 1186, loss 0.317315, acc 0.90625, prec 0.0342604, recall 0.757945\n",
      "2017-11-26T12:41:32.736016: step 1187, loss 0.288242, acc 0.859375, prec 0.0342553, recall 0.757945\n",
      "2017-11-26T12:41:33.382618: step 1188, loss 0.421305, acc 0.84375, prec 0.0342495, recall 0.757945\n",
      "2017-11-26T12:41:34.029121: step 1189, loss 0.540477, acc 0.8125, prec 0.0342588, recall 0.758035\n",
      "2017-11-26T12:41:34.684908: step 1190, loss 0.690725, acc 0.84375, prec 0.0342853, recall 0.758213\n",
      "2017-11-26T12:41:35.325895: step 1191, loss 0.505302, acc 0.8125, prec 0.0343107, recall 0.758392\n",
      "2017-11-26T12:41:35.977526: step 1192, loss 0.531183, acc 0.859375, prec 0.0343055, recall 0.758392\n",
      "2017-11-26T12:41:36.631695: step 1193, loss 0.541839, acc 0.8125, prec 0.0343148, recall 0.758481\n",
      "2017-11-26T12:41:37.266973: step 1194, loss 0.728145, acc 0.84375, prec 0.0343091, recall 0.758481\n",
      "2017-11-26T12:41:37.900910: step 1195, loss 6.64162, acc 0.765625, prec 0.0343172, recall 0.75829\n",
      "2017-11-26T12:41:38.542423: step 1196, loss 0.8941, acc 0.828125, prec 0.0343431, recall 0.758468\n",
      "2017-11-26T12:41:39.199930: step 1197, loss 5.51597, acc 0.859375, prec 0.0343546, recall 0.758278\n",
      "2017-11-26T12:41:39.844887: step 1198, loss 0.524057, acc 0.859375, prec 0.0343655, recall 0.758367\n",
      "2017-11-26T12:41:40.489415: step 1199, loss 0.952012, acc 0.71875, prec 0.0343552, recall 0.758367\n",
      "2017-11-26T12:41:41.137510: step 1200, loss 0.792241, acc 0.75, prec 0.0343461, recall 0.758367\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:42:21.300873: step 1200, loss 1.14995, acc 0.759577, prec 0.0348854, recall 0.764151\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1200\n",
      "\n",
      "2017-11-26T12:42:23.083191: step 1201, loss 1.15133, acc 0.6875, prec 0.0348897, recall 0.764233\n",
      "2017-11-26T12:42:23.663236: step 1202, loss 0.94139, acc 0.75, prec 0.0348808, recall 0.764233\n",
      "2017-11-26T12:42:24.279458: step 1203, loss 1.19181, acc 0.71875, prec 0.0348861, recall 0.764316\n",
      "2017-11-26T12:42:24.884728: step 1204, loss 1.48971, acc 0.609375, prec 0.0348876, recall 0.764398\n",
      "2017-11-26T12:42:25.509650: step 1205, loss 0.834608, acc 0.75, prec 0.0348787, recall 0.764398\n",
      "2017-11-26T12:42:26.120083: step 1206, loss 0.834394, acc 0.796875, prec 0.0348715, recall 0.764398\n",
      "2017-11-26T12:42:26.756300: step 1207, loss 1.58293, acc 0.625, prec 0.0348735, recall 0.76448\n",
      "2017-11-26T12:42:27.388190: step 1208, loss 0.824008, acc 0.75, prec 0.0348647, recall 0.76448\n",
      "2017-11-26T12:42:28.042708: step 1209, loss 0.791512, acc 0.8125, prec 0.034858, recall 0.76448\n",
      "2017-11-26T12:42:28.690473: step 1210, loss 0.898632, acc 0.796875, prec 0.0348662, recall 0.764562\n",
      "2017-11-26T12:42:29.344078: step 1211, loss 0.831383, acc 0.75, prec 0.0348573, recall 0.764562\n",
      "2017-11-26T12:42:30.000154: step 1212, loss 0.724362, acc 0.8125, prec 0.0348506, recall 0.764562\n",
      "2017-11-26T12:42:30.657951: step 1213, loss 0.40514, acc 0.828125, prec 0.0348445, recall 0.764562\n",
      "2017-11-26T12:42:31.316357: step 1214, loss 1.42259, acc 0.8125, prec 0.0348686, recall 0.764726\n",
      "2017-11-26T12:42:31.971489: step 1215, loss 0.393796, acc 0.921875, prec 0.0349118, recall 0.764972\n",
      "2017-11-26T12:42:32.617289: step 1216, loss 0.47767, acc 0.828125, prec 0.0349057, recall 0.764972\n",
      "2017-11-26T12:42:33.294656: step 1217, loss 0.423095, acc 0.875, prec 0.0349166, recall 0.765054\n",
      "2017-11-26T12:42:33.973735: step 1218, loss 0.579599, acc 0.859375, prec 0.0349116, recall 0.765054\n",
      "2017-11-26T12:42:34.649353: step 1219, loss 4.34367, acc 0.828125, prec 0.0349066, recall 0.764522\n",
      "2017-11-26T12:42:35.330504: step 1220, loss 0.573915, acc 0.8125, prec 0.0349153, recall 0.764604\n",
      "2017-11-26T12:42:36.008866: step 1221, loss 0.348798, acc 0.90625, prec 0.0349273, recall 0.764685\n",
      "2017-11-26T12:42:36.688438: step 1222, loss 0.593722, acc 0.875, prec 0.0349535, recall 0.764849\n",
      "2017-11-26T12:42:37.488576: step 1223, loss 0.598395, acc 0.75, prec 0.0349446, recall 0.764849\n",
      "2017-11-26T12:42:38.175944: step 1224, loss 0.559625, acc 0.84375, prec 0.0349391, recall 0.764849\n",
      "2017-11-26T12:42:38.864703: step 1225, loss 0.37969, acc 0.921875, prec 0.0349363, recall 0.764849\n",
      "2017-11-26T12:42:39.546774: step 1226, loss 3.18088, acc 0.734375, prec 0.0349274, recall 0.764583\n",
      "2017-11-26T12:42:40.226655: step 1227, loss 0.183085, acc 0.921875, prec 0.0349247, recall 0.764583\n",
      "2017-11-26T12:42:40.910556: step 1228, loss 0.952497, acc 0.8125, prec 0.034918, recall 0.764583\n",
      "2017-11-26T12:42:41.592709: step 1229, loss 0.486925, acc 0.84375, prec 0.0349278, recall 0.764665\n",
      "2017-11-26T12:42:42.274393: step 1230, loss 0.504938, acc 0.859375, prec 0.0349381, recall 0.764747\n",
      "2017-11-26T12:42:42.952417: step 1231, loss 0.519813, acc 0.828125, prec 0.0349473, recall 0.764828\n",
      "2017-11-26T12:42:43.619121: step 1232, loss 0.762559, acc 0.796875, prec 0.0349554, recall 0.76491\n",
      "2017-11-26T12:42:44.300763: step 1233, loss 0.772429, acc 0.734375, prec 0.0349613, recall 0.764991\n",
      "2017-11-26T12:42:44.943564: step 1234, loss 0.704556, acc 0.796875, prec 0.0349541, recall 0.764991\n",
      "2017-11-26T12:42:45.591840: step 1235, loss 0.615649, acc 0.8125, prec 0.0349627, recall 0.765073\n",
      "2017-11-26T12:42:46.246490: step 1236, loss 0.846262, acc 0.8125, prec 0.0349713, recall 0.765154\n",
      "2017-11-26T12:42:46.904058: step 1237, loss 0.361678, acc 0.84375, prec 0.0349811, recall 0.765235\n",
      "2017-11-26T12:42:47.573671: step 1238, loss 0.483042, acc 0.828125, prec 0.0349903, recall 0.765317\n",
      "2017-11-26T12:42:48.246518: step 1239, loss 1.30922, acc 0.890625, prec 0.0350169, recall 0.765479\n",
      "2017-11-26T12:42:48.923704: step 1240, loss 2.49432, acc 0.890625, prec 0.0350441, recall 0.765377\n",
      "2017-11-26T12:42:49.604187: step 1241, loss 2.25146, acc 0.71875, prec 0.03505, recall 0.765193\n",
      "2017-11-26T12:42:50.279560: step 1242, loss 0.537071, acc 0.828125, prec 0.0350591, recall 0.765274\n",
      "2017-11-26T12:42:50.954936: step 1243, loss 0.883046, acc 0.734375, prec 0.035065, recall 0.765355\n",
      "2017-11-26T12:42:51.632764: step 1244, loss 0.546042, acc 0.796875, prec 0.035073, recall 0.765436\n",
      "2017-11-26T12:42:52.312218: step 1245, loss 0.839004, acc 0.734375, prec 0.0350636, recall 0.765436\n",
      "2017-11-26T12:42:52.987026: step 1246, loss 0.60264, acc 0.828125, prec 0.0350575, recall 0.765436\n",
      "2017-11-26T12:42:53.653196: step 1247, loss 1.05651, acc 0.78125, prec 0.0350498, recall 0.765436\n",
      "2017-11-26T12:42:54.333030: step 1248, loss 0.965366, acc 0.765625, prec 0.0350415, recall 0.765436\n",
      "2017-11-26T12:42:55.001035: step 1249, loss 2.1358, acc 0.75, prec 0.0350478, recall 0.765517\n",
      "2017-11-26T12:42:55.672328: step 1250, loss 1.06791, acc 0.6875, prec 0.0350368, recall 0.765517\n",
      "2017-11-26T12:42:56.344922: step 1251, loss 0.375409, acc 0.84375, prec 0.0350312, recall 0.765517\n",
      "2017-11-26T12:42:57.040536: step 1252, loss 0.427027, acc 0.8125, prec 0.0350551, recall 0.765679\n",
      "2017-11-26T12:42:57.725400: step 1253, loss 0.649482, acc 0.8125, prec 0.0350789, recall 0.76584\n",
      "2017-11-26T12:42:58.404685: step 1254, loss 0.678658, acc 0.78125, prec 0.0350711, recall 0.76584\n",
      "2017-11-26T12:42:59.074548: step 1255, loss 1.00373, acc 0.75, prec 0.0350775, recall 0.765921\n",
      "2017-11-26T12:42:59.728440: step 1256, loss 0.846927, acc 0.8125, prec 0.0351013, recall 0.766082\n",
      "2017-11-26T12:43:00.381991: step 1257, loss 0.842785, acc 0.734375, prec 0.0350919, recall 0.766082\n",
      "2017-11-26T12:43:01.032438: step 1258, loss 0.42988, acc 0.90625, prec 0.0351038, recall 0.766162\n",
      "2017-11-26T12:43:01.680518: step 1259, loss 0.387717, acc 0.90625, prec 0.0351612, recall 0.766484\n",
      "2017-11-26T12:43:02.332509: step 1260, loss 6.62119, acc 0.734375, prec 0.0351529, recall 0.765957\n",
      "2017-11-26T12:43:02.992730: step 1261, loss 2.23014, acc 0.828125, prec 0.0351924, recall 0.766198\n",
      "2017-11-26T12:43:03.665516: step 1262, loss 0.524861, acc 0.84375, prec 0.0351869, recall 0.766198\n",
      "2017-11-26T12:43:04.339034: step 1263, loss 0.679097, acc 0.890625, prec 0.0352134, recall 0.766358\n",
      "2017-11-26T12:43:05.005711: step 1264, loss 0.966414, acc 0.734375, prec 0.035204, recall 0.766358\n",
      "2017-11-26T12:43:05.677105: step 1265, loss 0.620211, acc 0.78125, prec 0.0351962, recall 0.766358\n",
      "2017-11-26T12:43:06.370389: step 1266, loss 0.711455, acc 0.8125, prec 0.0352047, recall 0.766438\n",
      "2017-11-26T12:43:07.061730: step 1267, loss 1.06261, acc 0.765625, prec 0.0352268, recall 0.766598\n",
      "2017-11-26T12:43:07.757682: step 1268, loss 1.2395, acc 0.75, prec 0.0352331, recall 0.766678\n",
      "2017-11-26T12:43:08.439443: step 1269, loss 0.913642, acc 0.78125, prec 0.0352253, recall 0.766678\n",
      "2017-11-26T12:43:09.115947: step 1270, loss 2.67943, acc 0.65625, prec 0.0352289, recall 0.766496\n",
      "2017-11-26T12:43:09.785324: step 1271, loss 0.823618, acc 0.765625, prec 0.0352357, recall 0.766576\n",
      "2017-11-26T12:43:10.459170: step 1272, loss 1.20783, acc 0.734375, prec 0.0352415, recall 0.766655\n",
      "2017-11-26T12:43:11.128413: step 1273, loss 0.744922, acc 0.734375, prec 0.0352472, recall 0.766735\n",
      "2017-11-26T12:43:11.798224: step 1274, loss 0.889078, acc 0.703125, prec 0.0352518, recall 0.766815\n",
      "2017-11-26T12:43:12.477779: step 1275, loss 1.17258, acc 0.6875, prec 0.0352559, recall 0.766894\n",
      "2017-11-26T12:43:13.144599: step 1276, loss 1.03825, acc 0.734375, prec 0.0352465, recall 0.766894\n",
      "2017-11-26T12:43:13.819556: step 1277, loss 1.11454, acc 0.703125, prec 0.035236, recall 0.766894\n",
      "2017-11-26T12:43:14.491287: step 1278, loss 0.870835, acc 0.765625, prec 0.035258, recall 0.767053\n",
      "2017-11-26T12:43:15.161298: step 1279, loss 0.873044, acc 0.671875, prec 0.0352464, recall 0.767053\n",
      "2017-11-26T12:43:15.833681: step 1280, loss 1.32324, acc 0.6875, prec 0.0352504, recall 0.767133\n",
      "2017-11-26T12:43:16.509959: step 1281, loss 0.765612, acc 0.8125, prec 0.0352438, recall 0.767133\n",
      "2017-11-26T12:43:17.216637: step 1282, loss 0.457587, acc 0.875, prec 0.0352545, recall 0.767212\n",
      "2017-11-26T12:43:17.891239: step 1283, loss 0.264806, acc 0.875, prec 0.0352501, recall 0.767212\n",
      "2017-11-26T12:43:18.561084: step 1284, loss 0.129961, acc 0.9375, prec 0.0352479, recall 0.767212\n",
      "2017-11-26T12:43:19.237069: step 1285, loss 0.472586, acc 0.84375, prec 0.0352424, recall 0.767212\n",
      "2017-11-26T12:43:19.912949: step 1286, loss 0.466754, acc 0.875, prec 0.0352379, recall 0.767212\n",
      "2017-11-26T12:43:20.565098: step 1287, loss 0.266328, acc 0.90625, prec 0.0352346, recall 0.767212\n",
      "2017-11-26T12:43:21.212107: step 1288, loss 0.453099, acc 0.90625, prec 0.0352615, recall 0.767371\n",
      "2017-11-26T12:43:21.868193: step 1289, loss 0.283317, acc 0.875, prec 0.0352571, recall 0.767371\n",
      "2017-11-26T12:43:22.523993: step 1290, loss 0.308939, acc 0.875, prec 0.0352527, recall 0.767371\n",
      "2017-11-26T12:43:23.180151: step 1291, loss 0.343852, acc 0.90625, prec 0.0352494, recall 0.767371\n",
      "2017-11-26T12:43:23.845270: step 1292, loss 0.403757, acc 0.90625, prec 0.0352461, recall 0.767371\n",
      "2017-11-26T12:43:24.520808: step 1293, loss 0.296161, acc 0.90625, prec 0.0352579, recall 0.76745\n",
      "2017-11-26T12:43:25.192906: step 1294, loss 0.186672, acc 0.953125, prec 0.0352562, recall 0.76745\n",
      "2017-11-26T12:43:25.863333: step 1295, loss 0.193144, acc 0.953125, prec 0.0352696, recall 0.767529\n",
      "2017-11-26T12:43:26.535680: step 1296, loss 0.348892, acc 0.90625, prec 0.0352663, recall 0.767529\n",
      "2017-11-26T12:43:27.230147: step 1297, loss 0.0558529, acc 0.984375, prec 0.0352658, recall 0.767529\n",
      "2017-11-26T12:43:27.910788: step 1298, loss 0.260428, acc 0.953125, prec 0.0352641, recall 0.767529\n",
      "2017-11-26T12:43:28.589053: step 1299, loss 0.353417, acc 0.96875, prec 0.0352781, recall 0.767608\n",
      "2017-11-26T12:43:29.258269: step 1300, loss 6.9526, acc 0.9375, prec 0.0353072, recall 0.767244\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:44:08.185792: step 1300, loss 3.89321, acc 0.968013, prec 0.0356915, recall 0.742709\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1300\n",
      "\n",
      "2017-11-26T12:44:09.809627: step 1301, loss 0.772268, acc 0.96875, prec 0.0357054, recall 0.742792\n",
      "2017-11-26T12:44:10.424022: step 1302, loss 0.157233, acc 0.9375, prec 0.0357032, recall 0.742792\n",
      "2017-11-26T12:44:11.053555: step 1303, loss 0.360486, acc 0.921875, prec 0.0357154, recall 0.742876\n",
      "2017-11-26T12:44:11.663633: step 1304, loss 0.193748, acc 0.90625, prec 0.0357121, recall 0.742876\n",
      "2017-11-26T12:44:12.277273: step 1305, loss 6.99683, acc 0.84375, prec 0.0357221, recall 0.742718\n",
      "2017-11-26T12:44:12.895716: step 1306, loss 0.327422, acc 0.875, prec 0.0357176, recall 0.742718\n",
      "2017-11-26T12:44:13.522143: step 1307, loss 0.92534, acc 0.703125, prec 0.0357221, recall 0.742802\n",
      "2017-11-26T12:44:14.160209: step 1308, loss 0.740136, acc 0.78125, prec 0.0357293, recall 0.742885\n",
      "2017-11-26T12:44:14.816166: step 1309, loss 0.508994, acc 0.859375, prec 0.0357243, recall 0.742885\n",
      "2017-11-26T12:44:15.468587: step 1310, loss 0.712817, acc 0.796875, prec 0.0357171, recall 0.742885\n",
      "2017-11-26T12:44:16.120537: step 1311, loss 0.55291, acc 0.84375, prec 0.0357115, recall 0.742885\n",
      "2017-11-26T12:44:16.770151: step 1312, loss 0.380383, acc 0.875, prec 0.035737, recall 0.743051\n",
      "2017-11-26T12:44:17.450005: step 1313, loss 0.560019, acc 0.84375, prec 0.0357765, recall 0.7433\n",
      "2017-11-26T12:44:18.114272: step 1314, loss 0.528917, acc 0.859375, prec 0.0358014, recall 0.743466\n",
      "2017-11-26T12:44:18.783279: step 1315, loss 0.570332, acc 0.78125, prec 0.0357936, recall 0.743466\n",
      "2017-11-26T12:44:19.451931: step 1316, loss 0.89248, acc 0.796875, prec 0.0358014, recall 0.743548\n",
      "2017-11-26T12:44:20.122970: step 1317, loss 1.26367, acc 0.671875, prec 0.0357897, recall 0.743548\n",
      "2017-11-26T12:44:20.801490: step 1318, loss 1.08327, acc 0.703125, prec 0.0357941, recall 0.743631\n",
      "2017-11-26T12:44:21.472313: step 1319, loss 0.530291, acc 0.8125, prec 0.0358024, recall 0.743714\n",
      "2017-11-26T12:44:22.148529: step 1320, loss 2.89316, acc 0.796875, prec 0.0358107, recall 0.743557\n",
      "2017-11-26T12:44:22.826380: step 1321, loss 0.382134, acc 0.859375, prec 0.0358057, recall 0.743557\n",
      "2017-11-26T12:44:23.497059: step 1322, loss 0.751306, acc 0.75, prec 0.0357968, recall 0.743557\n",
      "2017-11-26T12:44:24.161965: step 1323, loss 6.34349, acc 0.796875, prec 0.0357902, recall 0.743317\n",
      "2017-11-26T12:44:24.833658: step 1324, loss 1.224, acc 0.671875, prec 0.0357785, recall 0.743317\n",
      "2017-11-26T12:44:25.506340: step 1325, loss 0.979252, acc 0.796875, prec 0.0357862, recall 0.7434\n",
      "2017-11-26T12:44:26.164639: step 1326, loss 0.865921, acc 0.75, prec 0.0357923, recall 0.743482\n",
      "2017-11-26T12:44:26.816472: step 1327, loss 1.08402, acc 0.734375, prec 0.0357978, recall 0.743565\n",
      "2017-11-26T12:44:27.509074: step 1328, loss 0.682598, acc 0.765625, prec 0.0357895, recall 0.743565\n",
      "2017-11-26T12:44:28.159788: step 1329, loss 0.496674, acc 0.8125, prec 0.0358127, recall 0.74373\n",
      "2017-11-26T12:44:28.830746: step 1330, loss 0.533747, acc 0.84375, prec 0.0358072, recall 0.74373\n",
      "2017-11-26T12:44:29.486219: step 1331, loss 0.716772, acc 0.765625, prec 0.0358138, recall 0.743812\n",
      "2017-11-26T12:44:30.132051: step 1332, loss 1.23219, acc 0.71875, prec 0.0358336, recall 0.743977\n",
      "2017-11-26T12:44:30.785803: step 1333, loss 1.08964, acc 0.734375, prec 0.0358242, recall 0.743977\n",
      "2017-11-26T12:44:31.457050: step 1334, loss 0.243154, acc 0.890625, prec 0.0358203, recall 0.743977\n",
      "2017-11-26T12:44:32.121775: step 1335, loss 0.328024, acc 0.890625, prec 0.0358463, recall 0.744141\n",
      "2017-11-26T12:44:32.795335: step 1336, loss 0.743042, acc 0.796875, prec 0.0358391, recall 0.744141\n",
      "2017-11-26T12:44:33.459857: step 1337, loss 0.683416, acc 0.828125, prec 0.0358479, recall 0.744223\n",
      "2017-11-26T12:44:34.133440: step 1338, loss 0.456575, acc 0.78125, prec 0.035855, recall 0.744305\n",
      "2017-11-26T12:44:34.809092: step 1339, loss 0.782367, acc 0.84375, prec 0.0358644, recall 0.744387\n",
      "2017-11-26T12:44:35.481788: step 1340, loss 1.24153, acc 0.8125, prec 0.0358875, recall 0.744551\n",
      "2017-11-26T12:44:36.156787: step 1341, loss 0.202842, acc 0.90625, prec 0.0358842, recall 0.744551\n",
      "2017-11-26T12:44:36.812305: step 1342, loss 1.70948, acc 0.875, prec 0.0358803, recall 0.744313\n",
      "2017-11-26T12:44:37.496548: step 1343, loss 9.72623, acc 0.859375, prec 0.0358759, recall 0.744074\n",
      "2017-11-26T12:44:38.171581: step 1344, loss 0.515877, acc 0.859375, prec 0.0358709, recall 0.744074\n",
      "2017-11-26T12:44:38.834179: step 1345, loss 0.341325, acc 0.859375, prec 0.0358659, recall 0.744074\n",
      "2017-11-26T12:44:39.493642: step 1346, loss 1.78974, acc 0.921875, prec 0.0358637, recall 0.743836\n",
      "2017-11-26T12:44:40.127021: step 1347, loss 0.279049, acc 0.890625, prec 0.0358896, recall 0.744\n",
      "2017-11-26T12:44:40.762733: step 1348, loss 0.709795, acc 0.8125, prec 0.035883, recall 0.744\n",
      "2017-11-26T12:44:41.412618: step 1349, loss 0.45165, acc 0.90625, prec 0.0358945, recall 0.744082\n",
      "2017-11-26T12:44:42.073901: step 1350, loss 0.81578, acc 0.78125, prec 0.0358868, recall 0.744082\n",
      "2017-11-26T12:44:42.742367: step 1351, loss 5.73042, acc 0.796875, prec 0.0358801, recall 0.743844\n",
      "2017-11-26T12:44:43.422302: step 1352, loss 6.14653, acc 0.828125, prec 0.0358746, recall 0.743606\n",
      "2017-11-26T12:44:44.103201: step 1353, loss 0.777694, acc 0.8125, prec 0.0358977, recall 0.74377\n",
      "2017-11-26T12:44:44.779798: step 1354, loss 0.838149, acc 0.671875, prec 0.0359158, recall 0.743934\n",
      "2017-11-26T12:44:45.441844: step 1355, loss 0.985055, acc 0.734375, prec 0.0359064, recall 0.743934\n",
      "2017-11-26T12:44:46.107019: step 1356, loss 1.07778, acc 0.65625, prec 0.035909, recall 0.744015\n",
      "2017-11-26T12:44:46.773688: step 1357, loss 1.28737, acc 0.71875, prec 0.0358991, recall 0.744015\n",
      "2017-11-26T12:44:47.465634: step 1358, loss 1.16437, acc 0.609375, prec 0.0359001, recall 0.744097\n",
      "2017-11-26T12:44:48.152855: step 1359, loss 1.84984, acc 0.59375, prec 0.0359006, recall 0.744179\n",
      "2017-11-26T12:44:48.834391: step 1360, loss 3.74195, acc 0.671875, prec 0.0359044, recall 0.744023\n",
      "2017-11-26T12:44:49.510245: step 1361, loss 1.99961, acc 0.46875, prec 0.0358856, recall 0.744023\n",
      "2017-11-26T12:44:50.160793: step 1362, loss 1.16909, acc 0.671875, prec 0.0359333, recall 0.744349\n",
      "2017-11-26T12:44:50.810457: step 1363, loss 2.26625, acc 0.53125, prec 0.0359167, recall 0.744349\n",
      "2017-11-26T12:44:51.456165: step 1364, loss 1.66865, acc 0.609375, prec 0.0359029, recall 0.744349\n",
      "2017-11-26T12:44:52.104743: step 1365, loss 2.21623, acc 0.53125, prec 0.0359012, recall 0.74443\n",
      "2017-11-26T12:44:52.750301: step 1366, loss 2.06343, acc 0.53125, prec 0.0358995, recall 0.744512\n",
      "2017-11-26T12:44:53.397179: step 1367, loss 3.44823, acc 0.671875, prec 0.035918, recall 0.744437\n",
      "2017-11-26T12:44:54.043339: step 1368, loss 1.2768, acc 0.640625, prec 0.0359202, recall 0.744519\n",
      "2017-11-26T12:44:54.696765: step 1369, loss 1.31737, acc 0.609375, prec 0.0359064, recall 0.744519\n",
      "2017-11-26T12:44:55.345294: step 1370, loss 1.16687, acc 0.734375, prec 0.0359414, recall 0.744762\n",
      "2017-11-26T12:44:56.004749: step 1371, loss 1.06072, acc 0.6875, prec 0.0359303, recall 0.744762\n",
      "2017-11-26T12:44:56.671154: step 1372, loss 1.42431, acc 0.640625, prec 0.0359325, recall 0.744843\n",
      "2017-11-26T12:44:57.348845: step 1373, loss 0.680406, acc 0.796875, prec 0.0359253, recall 0.744843\n",
      "2017-11-26T12:44:58.053554: step 1374, loss 3.43528, acc 0.8125, prec 0.035934, recall 0.744688\n",
      "2017-11-26T12:44:58.865843: step 1375, loss 1.51358, acc 0.625, prec 0.0359356, recall 0.744769\n",
      "2017-11-26T12:44:59.584563: step 1376, loss 9.95942, acc 0.71875, prec 0.0359262, recall 0.744532\n",
      "2017-11-26T12:45:00.258557: step 1377, loss 1.12997, acc 0.703125, prec 0.0359453, recall 0.744694\n",
      "2017-11-26T12:45:00.943431: step 1378, loss 0.7306, acc 0.8125, prec 0.0359534, recall 0.744775\n",
      "2017-11-26T12:45:01.613949: step 1379, loss 1.03629, acc 0.796875, prec 0.0359757, recall 0.744937\n",
      "2017-11-26T12:45:02.267581: step 1380, loss 0.644498, acc 0.859375, prec 0.0359708, recall 0.744937\n",
      "2017-11-26T12:45:02.921193: step 1381, loss 1.12515, acc 0.703125, prec 0.0359603, recall 0.744937\n",
      "2017-11-26T12:45:03.568288: step 1382, loss 0.957249, acc 0.71875, prec 0.0359946, recall 0.745179\n",
      "2017-11-26T12:45:04.211198: step 1383, loss 1.00482, acc 0.75, prec 0.0360153, recall 0.74534\n",
      "2017-11-26T12:45:04.856528: step 1384, loss 0.548301, acc 0.828125, prec 0.0360386, recall 0.7455\n",
      "2017-11-26T12:45:05.508230: step 1385, loss 0.739085, acc 0.734375, prec 0.0360587, recall 0.745661\n",
      "2017-11-26T12:45:06.164189: step 1386, loss 0.570605, acc 0.8125, prec 0.0360668, recall 0.745741\n",
      "2017-11-26T12:45:06.810315: step 1387, loss 0.677829, acc 0.734375, prec 0.0360575, recall 0.745741\n",
      "2017-11-26T12:45:07.470623: step 1388, loss 1.06266, acc 0.78125, prec 0.0360939, recall 0.745982\n",
      "2017-11-26T12:45:08.196728: step 1389, loss 0.285007, acc 0.890625, prec 0.03609, recall 0.745982\n",
      "2017-11-26T12:45:08.868972: step 1390, loss 0.954558, acc 0.765625, prec 0.0360818, recall 0.745982\n",
      "2017-11-26T12:45:09.544823: step 1391, loss 0.719109, acc 0.765625, prec 0.0361029, recall 0.746142\n",
      "2017-11-26T12:45:10.217439: step 1392, loss 3.23082, acc 0.75, prec 0.0361387, recall 0.746147\n",
      "2017-11-26T12:45:10.896895: step 1393, loss 0.727416, acc 0.75, prec 0.0361446, recall 0.746226\n",
      "2017-11-26T12:45:11.568632: step 1394, loss 0.615362, acc 0.875, prec 0.0361842, recall 0.746466\n",
      "2017-11-26T12:45:12.242595: step 1395, loss 0.50878, acc 0.921875, prec 0.0361961, recall 0.746545\n",
      "2017-11-26T12:45:12.916811: step 1396, loss 0.369471, acc 0.875, prec 0.0361917, recall 0.746545\n",
      "2017-11-26T12:45:13.592854: step 1397, loss 0.47523, acc 0.859375, prec 0.0362161, recall 0.746704\n",
      "2017-11-26T12:45:14.263878: step 1398, loss 0.719548, acc 0.828125, prec 0.0362247, recall 0.746784\n",
      "2017-11-26T12:45:14.912136: step 1399, loss 2.09483, acc 0.796875, prec 0.0362474, recall 0.746708\n",
      "2017-11-26T12:45:15.561467: step 1400, loss 0.828828, acc 0.75, prec 0.0362386, recall 0.746708\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:45:53.423395: step 1400, loss 1.48916, acc 0.877241, prec 0.0368576, recall 0.741674\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1400\n",
      "\n",
      "2017-11-26T12:45:55.258755: step 1401, loss 2.09807, acc 0.765625, prec 0.0368499, recall 0.741452\n",
      "2017-11-26T12:45:55.835277: step 1402, loss 0.454508, acc 0.78125, prec 0.0368565, recall 0.741529\n",
      "2017-11-26T12:45:56.422666: step 1403, loss 0.440704, acc 0.890625, prec 0.036867, recall 0.741607\n",
      "2017-11-26T12:45:57.005954: step 1404, loss 0.716346, acc 0.796875, prec 0.0368599, recall 0.741607\n",
      "2017-11-26T12:45:57.609398: step 1405, loss 0.623879, acc 0.8125, prec 0.0368533, recall 0.741607\n",
      "2017-11-26T12:45:58.227591: step 1406, loss 1.92422, acc 0.671875, prec 0.0368561, recall 0.741684\n",
      "2017-11-26T12:45:58.890838: step 1407, loss 1.1757, acc 0.78125, prec 0.0368628, recall 0.741762\n",
      "2017-11-26T12:45:59.529748: step 1408, loss 0.590761, acc 0.765625, prec 0.0368689, recall 0.741839\n",
      "2017-11-26T12:46:00.205979: step 1409, loss 1.05406, acc 0.828125, prec 0.0369059, recall 0.742071\n",
      "2017-11-26T12:46:00.911375: step 1410, loss 0.550236, acc 0.828125, prec 0.0368998, recall 0.742071\n",
      "2017-11-26T12:46:01.590082: step 1411, loss 0.776062, acc 0.75, prec 0.036934, recall 0.742302\n",
      "2017-11-26T12:46:02.250997: step 1412, loss 2.40942, acc 0.8125, prec 0.036928, recall 0.74208\n",
      "2017-11-26T12:46:02.912646: step 1413, loss 0.998644, acc 0.703125, prec 0.0369175, recall 0.74208\n",
      "2017-11-26T12:46:03.564077: step 1414, loss 1.08741, acc 0.71875, prec 0.036922, recall 0.742157\n",
      "2017-11-26T12:46:04.216411: step 1415, loss 0.875413, acc 0.78125, prec 0.0369572, recall 0.742388\n",
      "2017-11-26T12:46:04.894267: step 1416, loss 0.59883, acc 0.8125, prec 0.036965, recall 0.742465\n",
      "2017-11-26T12:46:05.567926: step 1417, loss 0.917031, acc 0.765625, prec 0.0369567, recall 0.742465\n",
      "2017-11-26T12:46:06.260603: step 1418, loss 0.839847, acc 0.765625, prec 0.0369485, recall 0.742465\n",
      "2017-11-26T12:46:06.946065: step 1419, loss 0.627115, acc 0.84375, prec 0.036943, recall 0.742465\n",
      "2017-11-26T12:46:07.633812: step 1420, loss 0.586794, acc 0.78125, prec 0.0369639, recall 0.742619\n",
      "2017-11-26T12:46:08.314168: step 1421, loss 0.500016, acc 0.84375, prec 0.0369584, recall 0.742619\n",
      "2017-11-26T12:46:09.019863: step 1422, loss 1.27391, acc 0.875, prec 0.0369683, recall 0.742695\n",
      "2017-11-26T12:46:09.703901: step 1423, loss 0.273289, acc 0.90625, prec 0.0369793, recall 0.742772\n",
      "2017-11-26T12:46:10.381319: step 1424, loss 0.558493, acc 0.875, prec 0.0369749, recall 0.742772\n",
      "2017-11-26T12:46:11.053034: step 1425, loss 0.549945, acc 0.84375, prec 0.0369695, recall 0.742772\n",
      "2017-11-26T12:46:11.723585: step 1426, loss 0.692051, acc 0.796875, prec 0.0369623, recall 0.742772\n",
      "2017-11-26T12:46:12.381733: step 1427, loss 2.78628, acc 0.859375, prec 0.0369579, recall 0.742551\n",
      "2017-11-26T12:46:13.036090: step 1428, loss 1.12472, acc 0.921875, prec 0.0369838, recall 0.742704\n",
      "2017-11-26T12:46:13.685533: step 1429, loss 0.652751, acc 0.796875, prec 0.0370195, recall 0.742934\n",
      "2017-11-26T12:46:14.337109: step 1430, loss 0.313067, acc 0.921875, prec 0.0370167, recall 0.742934\n",
      "2017-11-26T12:46:14.991797: step 1431, loss 0.81338, acc 0.8125, prec 0.0370244, recall 0.74301\n",
      "2017-11-26T12:46:15.646425: step 1432, loss 0.465746, acc 0.84375, prec 0.0370332, recall 0.743087\n",
      "2017-11-26T12:46:16.297086: step 1433, loss 0.677784, acc 0.8125, prec 0.0370409, recall 0.743163\n",
      "2017-11-26T12:46:16.941957: step 1434, loss 0.268466, acc 0.890625, prec 0.0370656, recall 0.743316\n",
      "2017-11-26T12:46:17.585295: step 1435, loss 0.399953, acc 0.84375, prec 0.0370601, recall 0.743316\n",
      "2017-11-26T12:46:18.230904: step 1436, loss 0.306175, acc 0.90625, prec 0.0370568, recall 0.743316\n",
      "2017-11-26T12:46:18.927247: step 1437, loss 0.565245, acc 0.84375, prec 0.0370513, recall 0.743316\n",
      "2017-11-26T12:46:19.611817: step 1438, loss 0.44806, acc 0.84375, prec 0.0370458, recall 0.743316\n",
      "2017-11-26T12:46:20.289324: step 1439, loss 0.177766, acc 0.90625, prec 0.0370568, recall 0.743392\n",
      "2017-11-26T12:46:20.961263: step 1440, loss 0.321754, acc 0.875, prec 0.0370524, recall 0.743392\n",
      "2017-11-26T12:46:21.641228: step 1441, loss 0.397331, acc 0.90625, prec 0.0370919, recall 0.74362\n",
      "2017-11-26T12:46:22.322368: step 1442, loss 0.942878, acc 0.796875, prec 0.0371275, recall 0.743848\n",
      "2017-11-26T12:46:23.002558: step 1443, loss 0.306902, acc 0.859375, prec 0.037151, recall 0.744\n",
      "2017-11-26T12:46:23.678165: step 1444, loss 4.17249, acc 0.84375, prec 0.0371746, recall 0.743931\n",
      "2017-11-26T12:46:24.356389: step 1445, loss 0.832559, acc 0.90625, prec 0.0371997, recall 0.744083\n",
      "2017-11-26T12:46:25.036394: step 1446, loss 0.750541, acc 0.875, prec 0.0372238, recall 0.744234\n",
      "2017-11-26T12:46:25.713575: step 1447, loss 1.59611, acc 0.859375, prec 0.0372473, recall 0.744385\n",
      "2017-11-26T12:46:26.368562: step 1448, loss 0.51928, acc 0.84375, prec 0.0372418, recall 0.744385\n",
      "2017-11-26T12:46:27.021083: step 1449, loss 2.25247, acc 0.75, prec 0.0372336, recall 0.744165\n",
      "2017-11-26T12:46:27.672707: step 1450, loss 0.67804, acc 0.75, prec 0.0372532, recall 0.744317\n",
      "2017-11-26T12:46:28.325180: step 1451, loss 0.742902, acc 0.8125, prec 0.0372466, recall 0.744317\n",
      "2017-11-26T12:46:29.018181: step 1452, loss 0.949243, acc 0.6875, prec 0.0372498, recall 0.744392\n",
      "2017-11-26T12:46:29.686601: step 1453, loss 1.15762, acc 0.671875, prec 0.0372383, recall 0.744392\n",
      "2017-11-26T12:46:30.403058: step 1454, loss 0.641832, acc 0.78125, prec 0.0372448, recall 0.744467\n",
      "2017-11-26T12:46:31.085827: step 1455, loss 1.09608, acc 0.6875, prec 0.037248, recall 0.744543\n",
      "2017-11-26T12:46:31.775847: step 1456, loss 1.46147, acc 0.625, prec 0.037249, recall 0.744618\n",
      "2017-11-26T12:46:32.452743: step 1457, loss 1.16969, acc 0.703125, prec 0.0372812, recall 0.744844\n",
      "2017-11-26T12:46:33.131640: step 1458, loss 1.71314, acc 0.578125, prec 0.0372663, recall 0.744844\n",
      "2017-11-26T12:46:33.811540: step 1459, loss 1.0242, acc 0.671875, prec 0.0373116, recall 0.745144\n",
      "2017-11-26T12:46:34.488436: step 1460, loss 0.567793, acc 0.828125, prec 0.0373055, recall 0.745144\n",
      "2017-11-26T12:46:35.162871: step 1461, loss 0.613457, acc 0.828125, prec 0.0373137, recall 0.745219\n",
      "2017-11-26T12:46:35.836510: step 1462, loss 0.509718, acc 0.8125, prec 0.0373071, recall 0.745219\n",
      "2017-11-26T12:46:36.515976: step 1463, loss 0.422094, acc 0.828125, prec 0.037301, recall 0.745219\n",
      "2017-11-26T12:46:37.187766: step 1464, loss 5.4881, acc 0.796875, prec 0.0373233, recall 0.744931\n",
      "2017-11-26T12:46:37.871905: step 1465, loss 0.781704, acc 0.796875, prec 0.0373162, recall 0.744931\n",
      "2017-11-26T12:46:38.551445: step 1466, loss 0.595813, acc 0.828125, prec 0.0373101, recall 0.744931\n",
      "2017-11-26T12:46:39.266300: step 1467, loss 0.319219, acc 0.890625, prec 0.0373205, recall 0.745006\n",
      "2017-11-26T12:46:39.956833: step 1468, loss 0.833713, acc 0.6875, prec 0.0373095, recall 0.745006\n",
      "2017-11-26T12:46:40.633306: step 1469, loss 0.393284, acc 0.875, prec 0.0373051, recall 0.745006\n",
      "2017-11-26T12:46:41.286291: step 1470, loss 0.475934, acc 0.84375, prec 0.0372996, recall 0.745006\n",
      "2017-11-26T12:46:41.931832: step 1471, loss 0.865412, acc 0.8125, prec 0.0373355, recall 0.74523\n",
      "2017-11-26T12:46:42.580106: step 1472, loss 0.686823, acc 0.796875, prec 0.0373425, recall 0.745305\n",
      "2017-11-26T12:46:43.232275: step 1473, loss 0.451858, acc 0.8125, prec 0.0373359, recall 0.745305\n",
      "2017-11-26T12:46:43.878655: step 1474, loss 0.830793, acc 0.78125, prec 0.0373282, recall 0.745305\n",
      "2017-11-26T12:46:44.532093: step 1475, loss 2.00605, acc 0.828125, prec 0.0373369, recall 0.745161\n",
      "2017-11-26T12:46:45.192283: step 1476, loss 0.712756, acc 0.765625, prec 0.0373428, recall 0.745236\n",
      "2017-11-26T12:46:45.862807: step 1477, loss 0.191285, acc 0.90625, prec 0.0373395, recall 0.745236\n",
      "2017-11-26T12:46:46.541098: step 1478, loss 0.673687, acc 0.765625, prec 0.0373454, recall 0.745311\n",
      "2017-11-26T12:46:47.217647: step 1479, loss 0.919442, acc 0.75, prec 0.0373367, recall 0.745311\n",
      "2017-11-26T12:46:47.889582: step 1480, loss 0.973361, acc 0.75, prec 0.0373562, recall 0.74546\n",
      "2017-11-26T12:46:48.555641: step 1481, loss 0.710245, acc 0.765625, prec 0.0373621, recall 0.745534\n",
      "2017-11-26T12:46:49.276427: step 1482, loss 0.630852, acc 0.828125, prec 0.0373843, recall 0.745683\n",
      "2017-11-26T12:46:49.950378: step 1483, loss 0.447037, acc 0.890625, prec 0.0373946, recall 0.745758\n",
      "2017-11-26T12:46:50.633707: step 1484, loss 0.386387, acc 0.953125, prec 0.0374353, recall 0.745981\n",
      "2017-11-26T12:46:51.310226: step 1485, loss 0.430831, acc 0.84375, prec 0.037458, recall 0.746129\n",
      "2017-11-26T12:46:51.984718: step 1486, loss 0.379138, acc 0.921875, prec 0.0374835, recall 0.746277\n",
      "2017-11-26T12:46:52.679198: step 1487, loss 0.360915, acc 0.90625, prec 0.0374943, recall 0.746351\n",
      "2017-11-26T12:46:53.350633: step 1488, loss 0.603869, acc 0.8125, prec 0.0374877, recall 0.746351\n",
      "2017-11-26T12:46:54.026896: step 1489, loss 0.29481, acc 0.921875, prec 0.0374991, recall 0.746425\n",
      "2017-11-26T12:46:54.702403: step 1490, loss 0.306347, acc 0.921875, prec 0.0374963, recall 0.746425\n",
      "2017-11-26T12:46:55.290123: step 1491, loss 0.509078, acc 0.865385, prec 0.0374925, recall 0.746425\n",
      "2017-11-26T12:46:55.962968: step 1492, loss 0.304662, acc 0.859375, prec 0.0375016, recall 0.746499\n",
      "2017-11-26T12:46:56.609031: step 1493, loss 0.212701, acc 0.953125, prec 0.0375282, recall 0.746647\n",
      "2017-11-26T12:46:57.250522: step 1494, loss 0.306197, acc 0.90625, prec 0.037539, recall 0.746721\n",
      "2017-11-26T12:46:57.898293: step 1495, loss 4.48846, acc 0.921875, prec 0.0375368, recall 0.746503\n",
      "2017-11-26T12:46:58.544085: step 1496, loss 0.410184, acc 0.921875, prec 0.0375623, recall 0.746651\n",
      "2017-11-26T12:46:59.220489: step 1497, loss 0.192617, acc 0.953125, prec 0.0376029, recall 0.746872\n",
      "2017-11-26T12:46:59.911620: step 1498, loss 0.26601, acc 0.90625, prec 0.0375996, recall 0.746872\n",
      "2017-11-26T12:47:00.585470: step 1499, loss 0.394315, acc 0.84375, prec 0.0375941, recall 0.746872\n",
      "2017-11-26T12:47:01.264517: step 1500, loss 0.191472, acc 0.9375, prec 0.037606, recall 0.746946\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:47:40.400425: step 1500, loss 2.16041, acc 0.928855, prec 0.0382376, recall 0.737224\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1500\n",
      "\n",
      "2017-11-26T12:47:42.072026: step 1501, loss 0.381969, acc 0.890625, prec 0.0382337, recall 0.737224\n",
      "2017-11-26T12:47:42.680777: step 1502, loss 0.190149, acc 0.9375, prec 0.0382454, recall 0.737298\n",
      "2017-11-26T12:47:43.288428: step 1503, loss 0.419641, acc 0.875, prec 0.0382549, recall 0.737371\n",
      "2017-11-26T12:47:43.897108: step 1504, loss 0.23024, acc 0.875, prec 0.0382505, recall 0.737371\n",
      "2017-11-26T12:47:44.519644: step 1505, loss 1.29384, acc 0.9375, prec 0.0382488, recall 0.737165\n",
      "2017-11-26T12:47:45.139115: step 1506, loss 0.783864, acc 0.953125, prec 0.038275, recall 0.737312\n",
      "2017-11-26T12:47:45.793808: step 1507, loss 2.40566, acc 0.890625, prec 0.0382717, recall 0.737106\n",
      "2017-11-26T12:47:46.451832: step 1508, loss 11.0651, acc 0.8125, prec 0.0382662, recall 0.736695\n",
      "2017-11-26T12:47:47.112616: step 1509, loss 0.545497, acc 0.859375, prec 0.0382612, recall 0.736695\n",
      "2017-11-26T12:47:47.762788: step 1510, loss 0.787812, acc 0.765625, prec 0.0382668, recall 0.736769\n",
      "2017-11-26T12:47:48.409002: step 1511, loss 1.09282, acc 0.640625, prec 0.0382541, recall 0.736769\n",
      "2017-11-26T12:47:49.080807: step 1512, loss 1.45384, acc 0.59375, prec 0.0382397, recall 0.736769\n",
      "2017-11-26T12:47:49.785755: step 1513, loss 1.26041, acc 0.640625, prec 0.0382409, recall 0.736842\n",
      "2017-11-26T12:47:50.461819: step 1514, loss 1.31166, acc 0.625, prec 0.0382415, recall 0.736915\n",
      "2017-11-26T12:47:51.144315: step 1515, loss 2.13139, acc 0.578125, prec 0.0382544, recall 0.737062\n",
      "2017-11-26T12:47:51.823554: step 1516, loss 1.78826, acc 0.609375, prec 0.0382544, recall 0.737135\n",
      "2017-11-26T12:47:52.504465: step 1517, loss 2.07821, acc 0.578125, prec 0.0382673, recall 0.737281\n",
      "2017-11-26T12:47:53.180732: step 1518, loss 0.895257, acc 0.6875, prec 0.038284, recall 0.737427\n",
      "2017-11-26T12:47:53.846771: step 1519, loss 1.42107, acc 0.65625, prec 0.0382719, recall 0.737427\n",
      "2017-11-26T12:47:54.519353: step 1520, loss 1.13348, acc 0.671875, prec 0.0382603, recall 0.737427\n",
      "2017-11-26T12:47:55.199777: step 1521, loss 1.5258, acc 0.578125, prec 0.0382454, recall 0.737427\n",
      "2017-11-26T12:47:55.871527: step 1522, loss 1.20226, acc 0.78125, prec 0.0382931, recall 0.737719\n",
      "2017-11-26T12:47:56.552904: step 1523, loss 0.970977, acc 0.703125, prec 0.0382826, recall 0.737719\n",
      "2017-11-26T12:47:57.232382: step 1524, loss 0.887956, acc 0.6875, prec 0.0382993, recall 0.737864\n",
      "2017-11-26T12:47:57.904854: step 1525, loss 1.04308, acc 0.71875, prec 0.038317, recall 0.738009\n",
      "2017-11-26T12:47:58.581526: step 1526, loss 0.42984, acc 0.828125, prec 0.038311, recall 0.738009\n",
      "2017-11-26T12:47:59.260725: step 1527, loss 0.438592, acc 0.828125, prec 0.0383188, recall 0.738082\n",
      "2017-11-26T12:47:59.962747: step 1528, loss 0.425485, acc 0.890625, prec 0.0383149, recall 0.738082\n",
      "2017-11-26T12:48:00.652097: step 1529, loss 3.46266, acc 0.84375, prec 0.0383382, recall 0.737818\n",
      "2017-11-26T12:48:01.377572: step 1530, loss 0.289744, acc 0.875, prec 0.0383337, recall 0.737818\n",
      "2017-11-26T12:48:02.065473: step 1531, loss 0.60238, acc 0.859375, prec 0.0383426, recall 0.737891\n",
      "2017-11-26T12:48:02.738720: step 1532, loss 4.04592, acc 0.8125, prec 0.0383365, recall 0.737687\n",
      "2017-11-26T12:48:03.416024: step 1533, loss 0.468872, acc 0.859375, prec 0.0383316, recall 0.737687\n",
      "2017-11-26T12:48:04.087251: step 1534, loss 0.812693, acc 0.71875, prec 0.0383355, recall 0.737759\n",
      "2017-11-26T12:48:04.755562: step 1535, loss 0.688743, acc 0.8125, prec 0.0383289, recall 0.737759\n",
      "2017-11-26T12:48:05.433329: step 1536, loss 3.41458, acc 0.703125, prec 0.0383466, recall 0.7377\n",
      "2017-11-26T12:48:06.127056: step 1537, loss 0.412333, acc 0.875, prec 0.038356, recall 0.737773\n",
      "2017-11-26T12:48:06.816971: step 1538, loss 0.58216, acc 0.859375, prec 0.0383787, recall 0.737918\n",
      "2017-11-26T12:48:07.508761: step 1539, loss 0.864512, acc 0.75, prec 0.0383837, recall 0.73799\n",
      "2017-11-26T12:48:08.196279: step 1540, loss 1.10963, acc 0.734375, prec 0.0383743, recall 0.73799\n",
      "2017-11-26T12:48:08.877135: step 1541, loss 0.583578, acc 0.828125, prec 0.038382, recall 0.738062\n",
      "2017-11-26T12:48:09.590182: step 1542, loss 0.684139, acc 0.8125, prec 0.0384168, recall 0.738279\n",
      "2017-11-26T12:48:10.345495: step 1543, loss 0.994744, acc 0.734375, prec 0.0384213, recall 0.738351\n",
      "2017-11-26T12:48:11.060414: step 1544, loss 0.941371, acc 0.734375, prec 0.0384119, recall 0.738351\n",
      "2017-11-26T12:48:11.772666: step 1545, loss 0.929219, acc 0.6875, prec 0.0384009, recall 0.738351\n",
      "2017-11-26T12:48:12.478037: step 1546, loss 1.24949, acc 0.78125, prec 0.0384069, recall 0.738423\n",
      "2017-11-26T12:48:13.188114: step 1547, loss 0.633468, acc 0.796875, prec 0.0384274, recall 0.738567\n",
      "2017-11-26T12:48:13.891368: step 1548, loss 4.28754, acc 0.734375, prec 0.0384186, recall 0.738364\n",
      "2017-11-26T12:48:14.568686: step 1549, loss 0.825043, acc 0.734375, prec 0.0384092, recall 0.738364\n",
      "2017-11-26T12:48:15.239734: step 1550, loss 0.861815, acc 0.75, prec 0.0384417, recall 0.73858\n",
      "2017-11-26T12:48:15.918248: step 1551, loss 0.563783, acc 0.78125, prec 0.0384478, recall 0.738652\n",
      "2017-11-26T12:48:16.587228: step 1552, loss 0.380244, acc 0.84375, prec 0.038456, recall 0.738724\n",
      "2017-11-26T12:48:17.264259: step 1553, loss 0.552826, acc 0.765625, prec 0.0384615, recall 0.738796\n",
      "2017-11-26T12:48:17.934798: step 1554, loss 0.515251, acc 0.8125, prec 0.0384687, recall 0.738868\n",
      "2017-11-26T12:48:18.604814: step 1555, loss 0.807477, acc 0.796875, prec 0.0384753, recall 0.738939\n",
      "2017-11-26T12:48:19.279980: step 1556, loss 0.565621, acc 0.859375, prec 0.0385116, recall 0.739154\n",
      "2017-11-26T12:48:19.953706: step 1557, loss 0.625642, acc 0.828125, prec 0.0385056, recall 0.739154\n",
      "2017-11-26T12:48:20.643166: step 1558, loss 0.410519, acc 0.890625, prec 0.0385154, recall 0.739226\n",
      "2017-11-26T12:48:21.298337: step 1559, loss 0.305859, acc 0.90625, prec 0.0385121, recall 0.739226\n",
      "2017-11-26T12:48:21.966882: step 1560, loss 0.584135, acc 0.859375, prec 0.0385072, recall 0.739226\n",
      "2017-11-26T12:48:22.645206: step 1561, loss 0.619966, acc 0.875, prec 0.0385165, recall 0.739297\n",
      "2017-11-26T12:48:23.323106: step 1562, loss 3.79776, acc 0.8125, prec 0.0385242, recall 0.739166\n",
      "2017-11-26T12:48:23.993729: step 1563, loss 0.55676, acc 0.828125, prec 0.0385182, recall 0.739166\n",
      "2017-11-26T12:48:24.672434: step 1564, loss 0.375161, acc 0.859375, prec 0.0385269, recall 0.739238\n",
      "2017-11-26T12:48:25.349249: step 1565, loss 0.667621, acc 0.828125, prec 0.0385209, recall 0.739238\n",
      "2017-11-26T12:48:26.023439: step 1566, loss 0.34093, acc 0.875, prec 0.0385165, recall 0.739238\n",
      "2017-11-26T12:48:26.701797: step 1567, loss 1.86778, acc 0.78125, prec 0.0385093, recall 0.739035\n",
      "2017-11-26T12:48:27.354788: step 1568, loss 0.692881, acc 0.828125, prec 0.038517, recall 0.739107\n",
      "2017-11-26T12:48:28.006916: step 1569, loss 0.340153, acc 0.890625, prec 0.0385132, recall 0.739107\n",
      "2017-11-26T12:48:28.663670: step 1570, loss 0.488646, acc 0.859375, prec 0.0385082, recall 0.739107\n",
      "2017-11-26T12:48:29.307222: step 1571, loss 0.597719, acc 0.84375, prec 0.0385164, recall 0.739178\n",
      "2017-11-26T12:48:29.966357: step 1572, loss 0.373227, acc 0.90625, prec 0.0385131, recall 0.739178\n",
      "2017-11-26T12:48:30.678867: step 1573, loss 7.92489, acc 0.875, prec 0.0385093, recall 0.738976\n",
      "2017-11-26T12:48:31.365406: step 1574, loss 0.537327, acc 0.859375, prec 0.0385318, recall 0.739119\n",
      "2017-11-26T12:48:32.053161: step 1575, loss 0.421741, acc 0.828125, prec 0.0385395, recall 0.73919\n",
      "2017-11-26T12:48:32.724407: step 1576, loss 0.915543, acc 0.78125, prec 0.0385592, recall 0.739333\n",
      "2017-11-26T12:48:33.392895: step 1577, loss 0.886304, acc 0.796875, prec 0.0385795, recall 0.739475\n",
      "2017-11-26T12:48:34.065115: step 1578, loss 0.569958, acc 0.828125, prec 0.0385871, recall 0.739546\n",
      "2017-11-26T12:48:34.737392: step 1579, loss 0.714438, acc 0.796875, prec 0.03858, recall 0.739546\n",
      "2017-11-26T12:48:35.407342: step 1580, loss 0.732566, acc 0.78125, prec 0.0385997, recall 0.739689\n",
      "2017-11-26T12:48:36.081019: step 1581, loss 0.354151, acc 0.84375, prec 0.0386079, recall 0.73976\n",
      "2017-11-26T12:48:36.747612: step 1582, loss 0.487378, acc 0.84375, prec 0.0386024, recall 0.73976\n",
      "2017-11-26T12:48:37.422296: step 1583, loss 0.847332, acc 0.84375, prec 0.0386106, recall 0.739831\n",
      "2017-11-26T12:48:38.116409: step 1584, loss 4.30676, acc 0.875, prec 0.0386067, recall 0.739629\n",
      "2017-11-26T12:48:38.793633: step 1585, loss 0.552184, acc 0.84375, prec 0.0386149, recall 0.7397\n",
      "2017-11-26T12:48:39.475206: step 1586, loss 0.550309, acc 0.84375, prec 0.0386094, recall 0.7397\n",
      "2017-11-26T12:48:40.168285: step 1587, loss 0.714929, acc 0.828125, prec 0.0386034, recall 0.7397\n",
      "2017-11-26T12:48:40.868883: step 1588, loss 0.306811, acc 0.84375, prec 0.0385979, recall 0.7397\n",
      "2017-11-26T12:48:41.555193: step 1589, loss 0.758321, acc 0.8125, prec 0.0385913, recall 0.7397\n",
      "2017-11-26T12:48:42.235796: step 1590, loss 0.756776, acc 0.734375, prec 0.038582, recall 0.7397\n",
      "2017-11-26T12:48:42.904674: step 1591, loss 1.24556, acc 0.734375, prec 0.0385863, recall 0.739771\n",
      "2017-11-26T12:48:43.575700: step 1592, loss 1.0274, acc 0.90625, prec 0.0386104, recall 0.739913\n",
      "2017-11-26T12:48:44.243363: step 1593, loss 1.06869, acc 0.71875, prec 0.0386005, recall 0.739913\n",
      "2017-11-26T12:48:44.921552: step 1594, loss 0.701501, acc 0.78125, prec 0.0385928, recall 0.739913\n",
      "2017-11-26T12:48:45.595225: step 1595, loss 0.717575, acc 0.84375, prec 0.0386146, recall 0.740054\n",
      "2017-11-26T12:48:46.270607: step 1596, loss 0.793415, acc 0.890625, prec 0.0386381, recall 0.740196\n",
      "2017-11-26T12:48:46.942163: step 1597, loss 0.845661, acc 0.78125, prec 0.0386441, recall 0.740267\n",
      "2017-11-26T12:48:47.620696: step 1598, loss 0.722904, acc 0.828125, prec 0.0386517, recall 0.740337\n",
      "2017-11-26T12:48:48.295282: step 1599, loss 0.932812, acc 0.765625, prec 0.0386572, recall 0.740408\n",
      "2017-11-26T12:48:48.966365: step 1600, loss 0.550749, acc 0.828125, prec 0.0386511, recall 0.740408\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:49:32.149662: step 1600, loss 1.38194, acc 0.856577, prec 0.039168, recall 0.738345\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1600\n",
      "\n",
      "2017-11-26T12:49:34.022025: step 1601, loss 0.424174, acc 0.859375, prec 0.0391898, recall 0.738482\n",
      "2017-11-26T12:49:34.626973: step 1602, loss 1.9008, acc 0.765625, prec 0.0391822, recall 0.738288\n",
      "2017-11-26T12:49:35.236005: step 1603, loss 1.78828, acc 0.828125, prec 0.0391901, recall 0.738164\n",
      "2017-11-26T12:49:35.855203: step 1604, loss 1.01706, acc 0.84375, prec 0.0392113, recall 0.738301\n",
      "2017-11-26T12:49:36.478567: step 1605, loss 0.863734, acc 0.75, prec 0.0392293, recall 0.738437\n",
      "2017-11-26T12:49:37.117438: step 1606, loss 0.37621, acc 0.921875, prec 0.0392399, recall 0.738506\n",
      "2017-11-26T12:49:37.756088: step 1607, loss 0.965876, acc 0.78125, prec 0.0392323, recall 0.738506\n",
      "2017-11-26T12:49:38.411254: step 1608, loss 0.550536, acc 0.84375, prec 0.0392268, recall 0.738506\n",
      "2017-11-26T12:49:39.066793: step 1609, loss 0.54879, acc 0.828125, prec 0.0392342, recall 0.738574\n",
      "2017-11-26T12:49:39.722929: step 1610, loss 0.540436, acc 0.828125, prec 0.0392415, recall 0.738642\n",
      "2017-11-26T12:49:40.386881: step 1611, loss 0.85693, acc 0.75, prec 0.0392328, recall 0.738642\n",
      "2017-11-26T12:49:41.099813: step 1612, loss 0.815018, acc 0.78125, prec 0.0392385, recall 0.738711\n",
      "2017-11-26T12:49:41.842039: step 1613, loss 2.47311, acc 0.71875, prec 0.0392426, recall 0.738586\n",
      "2017-11-26T12:49:42.535332: step 1614, loss 0.478568, acc 0.828125, prec 0.0392366, recall 0.738586\n",
      "2017-11-26T12:49:43.218975: step 1615, loss 0.433735, acc 0.828125, prec 0.0392439, recall 0.738654\n",
      "2017-11-26T12:49:43.921013: step 1616, loss 0.552994, acc 0.859375, prec 0.0392657, recall 0.73879\n",
      "2017-11-26T12:49:44.598419: step 1617, loss 0.656277, acc 0.78125, prec 0.039298, recall 0.738995\n",
      "2017-11-26T12:49:45.276792: step 1618, loss 0.499012, acc 0.84375, prec 0.0392925, recall 0.738995\n",
      "2017-11-26T12:49:45.948515: step 1619, loss 6.30584, acc 0.875, prec 0.0392887, recall 0.738802\n",
      "2017-11-26T12:49:46.630679: step 1620, loss 0.421897, acc 0.828125, prec 0.0392827, recall 0.738802\n",
      "2017-11-26T12:49:47.303727: step 1621, loss 0.396888, acc 0.859375, prec 0.0392779, recall 0.738802\n",
      "2017-11-26T12:49:47.982641: step 1622, loss 0.638684, acc 0.796875, prec 0.0392708, recall 0.738802\n",
      "2017-11-26T12:49:48.651110: step 1623, loss 0.415043, acc 0.875, prec 0.039293, recall 0.738938\n",
      "2017-11-26T12:49:49.330406: step 1624, loss 0.827722, acc 0.78125, prec 0.0393253, recall 0.739142\n",
      "2017-11-26T12:49:50.004660: step 1625, loss 1.12156, acc 0.703125, prec 0.0393283, recall 0.73921\n",
      "2017-11-26T12:49:50.682370: step 1626, loss 1.78409, acc 0.765625, prec 0.0393599, recall 0.739413\n",
      "2017-11-26T12:49:51.383350: step 1627, loss 0.651723, acc 0.796875, prec 0.0393662, recall 0.73948\n",
      "2017-11-26T12:49:52.080275: step 1628, loss 1.1154, acc 0.703125, prec 0.0393691, recall 0.739548\n",
      "2017-11-26T12:49:52.755518: step 1629, loss 1.10279, acc 0.75, prec 0.0393604, recall 0.739548\n",
      "2017-11-26T12:49:53.436829: step 1630, loss 1.00849, acc 0.78125, prec 0.0393528, recall 0.739548\n",
      "2017-11-26T12:49:54.120527: step 1631, loss 0.854396, acc 0.765625, prec 0.0393446, recall 0.739548\n",
      "2017-11-26T12:49:54.794062: step 1632, loss 0.387459, acc 0.828125, prec 0.0393917, recall 0.739818\n",
      "2017-11-26T12:49:55.465021: step 1633, loss 0.135627, acc 0.953125, prec 0.0394034, recall 0.739886\n",
      "2017-11-26T12:49:56.154961: step 1634, loss 0.438439, acc 0.84375, prec 0.0393979, recall 0.739886\n",
      "2017-11-26T12:49:56.833167: step 1635, loss 1.26459, acc 0.875, prec 0.0394334, recall 0.740088\n",
      "2017-11-26T12:49:57.519554: step 1636, loss 0.944341, acc 0.75, prec 0.0394379, recall 0.740155\n",
      "2017-11-26T12:49:58.196231: step 1637, loss 0.706503, acc 0.78125, prec 0.0394435, recall 0.740223\n",
      "2017-11-26T12:49:58.868589: step 1638, loss 0.754388, acc 0.8125, prec 0.0394635, recall 0.740357\n",
      "2017-11-26T12:49:59.541471: step 1639, loss 3.74909, acc 0.875, prec 0.039473, recall 0.740233\n",
      "2017-11-26T12:50:00.217768: step 1640, loss 0.572261, acc 0.875, prec 0.0394819, recall 0.7403\n",
      "2017-11-26T12:50:00.891965: step 1641, loss 7.32486, acc 0.84375, prec 0.0394769, recall 0.740109\n",
      "2017-11-26T12:50:01.602891: step 1642, loss 0.355268, acc 0.875, prec 0.0394726, recall 0.740109\n",
      "2017-11-26T12:50:02.300294: step 1643, loss 0.666834, acc 0.8125, prec 0.0394661, recall 0.740109\n",
      "2017-11-26T12:50:02.994595: step 1644, loss 0.802542, acc 0.765625, prec 0.0394579, recall 0.740109\n",
      "2017-11-26T12:50:03.673111: step 1645, loss 0.84115, acc 0.78125, prec 0.0394503, recall 0.740109\n",
      "2017-11-26T12:50:04.357129: step 1646, loss 0.715536, acc 0.75, prec 0.0394416, recall 0.740109\n",
      "2017-11-26T12:50:05.031777: step 1647, loss 0.706885, acc 0.8125, prec 0.0394351, recall 0.740109\n",
      "2017-11-26T12:50:05.718778: step 1648, loss 1.14409, acc 0.703125, prec 0.0394247, recall 0.740109\n",
      "2017-11-26T12:50:06.401287: step 1649, loss 1.25623, acc 0.75, prec 0.0394822, recall 0.740444\n",
      "2017-11-26T12:50:07.080528: step 1650, loss 0.558721, acc 0.84375, prec 0.0395032, recall 0.740578\n",
      "2017-11-26T12:50:07.750739: step 1651, loss 0.674969, acc 0.734375, prec 0.0395072, recall 0.740645\n",
      "2017-11-26T12:50:08.427774: step 1652, loss 0.905891, acc 0.734375, prec 0.0394979, recall 0.740645\n",
      "2017-11-26T12:50:09.116257: step 1653, loss 0.801371, acc 0.765625, prec 0.0395162, recall 0.740779\n",
      "2017-11-26T12:50:09.793827: step 1654, loss 0.640312, acc 0.796875, prec 0.0395092, recall 0.740779\n",
      "2017-11-26T12:50:10.648973: step 1655, loss 0.576628, acc 0.828125, prec 0.0395164, recall 0.740846\n",
      "2017-11-26T12:50:11.604661: step 1656, loss 3.44934, acc 0.921875, prec 0.0395142, recall 0.740655\n",
      "2017-11-26T12:50:12.529046: step 1657, loss 0.378739, acc 0.875, prec 0.0395099, recall 0.740655\n",
      "2017-11-26T12:50:13.364912: step 1658, loss 0.534675, acc 0.765625, prec 0.0395017, recall 0.740655\n",
      "2017-11-26T12:50:14.245190: step 1659, loss 0.357422, acc 0.875, prec 0.0395106, recall 0.740722\n",
      "2017-11-26T12:50:15.062145: step 1660, loss 0.259735, acc 0.90625, prec 0.0395073, recall 0.740722\n",
      "2017-11-26T12:50:15.897467: step 1661, loss 0.412192, acc 0.8125, prec 0.039514, recall 0.740788\n",
      "2017-11-26T12:50:16.724352: step 1662, loss 0.722917, acc 0.859375, prec 0.0395091, recall 0.740788\n",
      "2017-11-26T12:50:17.552231: step 1663, loss 0.186323, acc 0.953125, prec 0.0395075, recall 0.740788\n",
      "2017-11-26T12:50:18.338248: step 1664, loss 0.547666, acc 0.84375, prec 0.0395153, recall 0.740855\n",
      "2017-11-26T12:50:19.239336: step 1665, loss 0.689731, acc 0.828125, prec 0.0395357, recall 0.740989\n",
      "2017-11-26T12:50:20.097635: step 1666, loss 4.33441, acc 0.796875, prec 0.0395424, recall 0.740865\n",
      "2017-11-26T12:50:20.881399: step 1667, loss 1.23297, acc 0.828125, prec 0.0395496, recall 0.740931\n",
      "2017-11-26T12:50:21.708178: step 1668, loss 0.247847, acc 0.90625, prec 0.0395595, recall 0.740998\n",
      "2017-11-26T12:50:22.678580: step 1669, loss 0.67084, acc 0.78125, prec 0.0395651, recall 0.741065\n",
      "2017-11-26T12:50:23.542262: step 1670, loss 0.535137, acc 0.84375, prec 0.0395728, recall 0.741131\n",
      "2017-11-26T12:50:24.364395: step 1671, loss 2.13106, acc 0.859375, prec 0.0395685, recall 0.740941\n",
      "2017-11-26T12:50:25.197535: step 1672, loss 0.821903, acc 0.796875, prec 0.0395878, recall 0.741074\n",
      "2017-11-26T12:50:26.014988: step 1673, loss 0.558529, acc 0.796875, prec 0.0395939, recall 0.74114\n",
      "2017-11-26T12:50:26.842777: step 1674, loss 0.605634, acc 0.78125, prec 0.0395995, recall 0.741207\n",
      "2017-11-26T12:50:27.672487: step 1675, loss 0.416865, acc 0.84375, prec 0.039594, recall 0.741207\n",
      "2017-11-26T12:50:28.504459: step 1676, loss 0.58812, acc 0.796875, prec 0.039587, recall 0.741207\n",
      "2017-11-26T12:50:29.329522: step 1677, loss 0.728632, acc 0.78125, prec 0.0395926, recall 0.741273\n",
      "2017-11-26T12:50:30.213689: step 1678, loss 0.448942, acc 0.8125, prec 0.0396124, recall 0.741406\n",
      "2017-11-26T12:50:31.109108: step 1679, loss 0.680674, acc 0.75, prec 0.0396037, recall 0.741406\n",
      "2017-11-26T12:50:31.989655: step 1680, loss 0.619894, acc 0.765625, prec 0.0395956, recall 0.741406\n",
      "2017-11-26T12:50:32.883110: step 1681, loss 9.03265, acc 0.828125, prec 0.0396164, recall 0.741348\n",
      "2017-11-26T12:50:33.808655: step 1682, loss 0.548226, acc 0.78125, prec 0.0396088, recall 0.741348\n",
      "2017-11-26T12:50:34.632134: step 1683, loss 0.429516, acc 0.8125, prec 0.0396155, recall 0.741415\n",
      "2017-11-26T12:50:35.491455: step 1684, loss 0.841536, acc 0.75, prec 0.03962, recall 0.741481\n",
      "2017-11-26T12:50:36.325753: step 1685, loss 0.758904, acc 0.75, prec 0.0396113, recall 0.741481\n",
      "2017-11-26T12:50:37.155972: step 1686, loss 0.804288, acc 0.765625, prec 0.0396294, recall 0.741613\n",
      "2017-11-26T12:50:38.207902: step 1687, loss 1.1394, acc 0.78125, prec 0.0396613, recall 0.741812\n",
      "2017-11-26T12:50:39.365987: step 1688, loss 1.84841, acc 0.828125, prec 0.0396558, recall 0.741622\n",
      "2017-11-26T12:50:40.466523: step 1689, loss 0.856141, acc 0.6875, prec 0.0396581, recall 0.741688\n",
      "2017-11-26T12:50:41.531550: step 1690, loss 0.501821, acc 0.828125, prec 0.0396784, recall 0.74182\n",
      "2017-11-26T12:50:42.574718: step 1691, loss 0.655441, acc 0.796875, prec 0.0396976, recall 0.741952\n",
      "2017-11-26T12:50:43.513939: step 1692, loss 0.937719, acc 0.84375, prec 0.0397184, recall 0.742084\n",
      "2017-11-26T12:50:44.425934: step 1693, loss 0.634285, acc 0.796875, prec 0.0397245, recall 0.74215\n",
      "2017-11-26T12:50:45.339289: step 1694, loss 0.733675, acc 0.6875, prec 0.0397137, recall 0.74215\n",
      "2017-11-26T12:50:46.197479: step 1695, loss 0.690588, acc 0.75, prec 0.039705, recall 0.74215\n",
      "2017-11-26T12:50:47.100557: step 1696, loss 1.12681, acc 0.71875, prec 0.0396952, recall 0.74215\n",
      "2017-11-26T12:50:47.959681: step 1697, loss 2.13552, acc 0.859375, prec 0.039704, recall 0.742026\n",
      "2017-11-26T12:50:48.851990: step 1698, loss 0.734006, acc 0.8125, prec 0.0397106, recall 0.742092\n",
      "2017-11-26T12:50:49.683936: step 1699, loss 1.29219, acc 0.890625, prec 0.0397199, recall 0.742158\n",
      "2017-11-26T12:50:50.492722: step 1700, loss 0.576069, acc 0.71875, prec 0.0397102, recall 0.742158\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:51:42.070465: step 1700, loss 1.33269, acc 0.856482, prec 0.0402089, recall 0.74065\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1700\n",
      "\n",
      "2017-11-26T12:51:43.860682: step 1701, loss 0.632787, acc 0.828125, prec 0.040203, recall 0.74065\n",
      "2017-11-26T12:51:44.621371: step 1702, loss 0.571828, acc 0.796875, prec 0.040196, recall 0.74065\n",
      "2017-11-26T12:51:45.330220: step 1703, loss 0.701302, acc 0.796875, prec 0.0401891, recall 0.74065\n",
      "2017-11-26T12:51:46.066254: step 1704, loss 0.268467, acc 0.90625, prec 0.0401987, recall 0.740713\n",
      "2017-11-26T12:51:46.924506: step 1705, loss 1.11153, acc 0.75, prec 0.0402029, recall 0.740777\n",
      "2017-11-26T12:51:47.850905: step 1706, loss 0.402915, acc 0.859375, prec 0.0401981, recall 0.740777\n",
      "2017-11-26T12:51:48.667260: step 1707, loss 0.332808, acc 0.875, prec 0.0401938, recall 0.740777\n",
      "2017-11-26T12:51:49.633572: step 1708, loss 0.423462, acc 0.875, prec 0.0402023, recall 0.740841\n",
      "2017-11-26T12:51:50.683603: step 1709, loss 0.273322, acc 0.875, prec 0.040198, recall 0.740841\n",
      "2017-11-26T12:51:51.840772: step 1710, loss 0.56188, acc 0.78125, prec 0.0402033, recall 0.740905\n",
      "2017-11-26T12:51:53.218489: step 1711, loss 0.386348, acc 0.84375, prec 0.0401979, recall 0.740905\n",
      "2017-11-26T12:51:54.439933: step 1712, loss 1.31445, acc 0.90625, prec 0.0402075, recall 0.740968\n",
      "2017-11-26T12:51:55.895249: step 1713, loss 0.401888, acc 0.890625, prec 0.0402038, recall 0.740968\n",
      "2017-11-26T12:51:57.358746: step 1714, loss 0.555861, acc 0.828125, prec 0.0402234, recall 0.741096\n",
      "2017-11-26T12:51:58.596073: step 1715, loss 1.24586, acc 0.859375, prec 0.0402698, recall 0.74135\n",
      "2017-11-26T12:51:59.715047: step 1716, loss 3.79254, acc 0.921875, prec 0.0402804, recall 0.741231\n",
      "2017-11-26T12:52:01.074366: step 1717, loss 0.48328, acc 0.859375, prec 0.0402884, recall 0.741295\n",
      "2017-11-26T12:52:02.202829: step 1718, loss 0.541915, acc 0.859375, prec 0.0402964, recall 0.741358\n",
      "2017-11-26T12:52:03.492812: step 1719, loss 0.4785, acc 0.859375, prec 0.0402915, recall 0.741358\n",
      "2017-11-26T12:52:04.749328: step 1720, loss 0.407631, acc 0.875, prec 0.0403128, recall 0.741485\n",
      "2017-11-26T12:52:05.971938: step 1721, loss 0.40395, acc 0.890625, prec 0.040309, recall 0.741485\n",
      "2017-11-26T12:52:07.055164: step 1722, loss 1.75066, acc 0.8125, prec 0.0403031, recall 0.741303\n",
      "2017-11-26T12:52:07.959264: step 1723, loss 0.791842, acc 0.703125, prec 0.0402929, recall 0.741303\n",
      "2017-11-26T12:52:08.819933: step 1724, loss 0.75904, acc 0.75, prec 0.0402844, recall 0.741303\n",
      "2017-11-26T12:52:09.651849: step 1725, loss 0.54141, acc 0.8125, prec 0.0402907, recall 0.741367\n",
      "2017-11-26T12:52:10.475581: step 1726, loss 0.951419, acc 0.71875, prec 0.040281, recall 0.741367\n",
      "2017-11-26T12:52:11.275354: step 1727, loss 1.09857, acc 0.734375, prec 0.0402847, recall 0.74143\n",
      "2017-11-26T12:52:12.067412: step 1728, loss 0.459471, acc 0.84375, prec 0.0402793, recall 0.74143\n",
      "2017-11-26T12:52:12.916677: step 1729, loss 0.389591, acc 0.890625, prec 0.0403011, recall 0.741557\n",
      "2017-11-26T12:52:13.697261: step 1730, loss 1.21205, acc 0.78125, prec 0.0403064, recall 0.74162\n",
      "2017-11-26T12:52:14.452622: step 1731, loss 0.378231, acc 0.859375, prec 0.0403271, recall 0.741746\n",
      "2017-11-26T12:52:15.209577: step 1732, loss 0.766566, acc 0.859375, prec 0.040335, recall 0.741809\n",
      "2017-11-26T12:52:15.968085: step 1733, loss 0.964972, acc 0.828125, prec 0.0403674, recall 0.741999\n",
      "2017-11-26T12:52:16.723680: step 1734, loss 0.468775, acc 0.859375, prec 0.0403753, recall 0.742062\n",
      "2017-11-26T12:52:17.507926: step 1735, loss 0.598961, acc 0.8125, prec 0.0403944, recall 0.742188\n",
      "2017-11-26T12:52:18.263476: step 1736, loss 0.462031, acc 0.828125, prec 0.040414, recall 0.742313\n",
      "2017-11-26T12:52:19.034375: step 1737, loss 0.651955, acc 0.84375, prec 0.0404086, recall 0.742313\n",
      "2017-11-26T12:52:19.792112: step 1738, loss 0.722128, acc 0.90625, prec 0.0404181, recall 0.742376\n",
      "2017-11-26T12:52:20.551821: step 1739, loss 0.290215, acc 0.9375, prec 0.040416, recall 0.742376\n",
      "2017-11-26T12:52:21.302259: step 1740, loss 0.24454, acc 0.921875, prec 0.0404643, recall 0.742627\n",
      "2017-11-26T12:52:22.064779: step 1741, loss 0.205799, acc 0.921875, prec 0.0404616, recall 0.742627\n",
      "2017-11-26T12:52:22.863026: step 1742, loss 0.311681, acc 0.859375, prec 0.0404567, recall 0.742627\n",
      "2017-11-26T12:52:23.621405: step 1743, loss 0.53964, acc 0.84375, prec 0.0404641, recall 0.74269\n",
      "2017-11-26T12:52:24.397194: step 1744, loss 0.210756, acc 0.9375, prec 0.040462, recall 0.74269\n",
      "2017-11-26T12:52:25.121477: step 1745, loss 0.99458, acc 0.90625, prec 0.0404715, recall 0.742753\n",
      "2017-11-26T12:52:25.853158: step 1746, loss 0.423736, acc 0.859375, prec 0.0404666, recall 0.742753\n",
      "2017-11-26T12:52:26.581771: step 1747, loss 0.181492, acc 0.9375, prec 0.0404772, recall 0.742815\n",
      "2017-11-26T12:52:27.312487: step 1748, loss 0.307146, acc 0.859375, prec 0.0404851, recall 0.742878\n",
      "2017-11-26T12:52:28.039986: step 1749, loss 0.527572, acc 0.890625, prec 0.0404941, recall 0.742941\n",
      "2017-11-26T12:52:28.759925: step 1750, loss 0.13822, acc 0.9375, prec 0.040492, recall 0.742941\n",
      "2017-11-26T12:52:29.494423: step 1751, loss 0.127085, acc 0.953125, prec 0.0405158, recall 0.743066\n",
      "2017-11-26T12:52:30.220301: step 1752, loss 0.265119, acc 0.90625, prec 0.040538, recall 0.743191\n",
      "2017-11-26T12:52:30.951344: step 1753, loss 1.78403, acc 0.921875, prec 0.0405359, recall 0.74301\n",
      "2017-11-26T12:52:31.690372: step 1754, loss 0.192445, acc 0.953125, prec 0.0405343, recall 0.74301\n",
      "2017-11-26T12:52:32.471274: step 1755, loss 0.117097, acc 0.9375, prec 0.0405448, recall 0.743072\n",
      "2017-11-26T12:52:33.201050: step 1756, loss 0.332792, acc 0.890625, prec 0.0405665, recall 0.743197\n",
      "2017-11-26T12:52:33.933418: step 1757, loss 0.101649, acc 0.953125, prec 0.0405904, recall 0.743322\n",
      "2017-11-26T12:52:34.663195: step 1758, loss 0.204382, acc 0.890625, prec 0.0405866, recall 0.743322\n",
      "2017-11-26T12:52:35.391212: step 1759, loss 1.79862, acc 0.890625, prec 0.0405834, recall 0.743142\n",
      "2017-11-26T12:52:36.115189: step 1760, loss 2.36717, acc 0.875, prec 0.0405796, recall 0.742961\n",
      "2017-11-26T12:52:36.853067: step 1761, loss 0.0987138, acc 0.96875, prec 0.0405785, recall 0.742961\n",
      "2017-11-26T12:52:37.578740: step 1762, loss 1.10664, acc 0.9375, prec 0.0405891, recall 0.743024\n",
      "2017-11-26T12:52:38.314988: step 1763, loss 6.71478, acc 0.90625, prec 0.0406118, recall 0.742968\n",
      "2017-11-26T12:52:39.046531: step 1764, loss 0.647395, acc 0.75, prec 0.0406032, recall 0.742968\n",
      "2017-11-26T12:52:39.772238: step 1765, loss 0.811471, acc 0.78125, prec 0.0406084, recall 0.74303\n",
      "2017-11-26T12:52:40.501584: step 1766, loss 0.748906, acc 0.734375, prec 0.0405993, recall 0.74303\n",
      "2017-11-26T12:52:41.231360: step 1767, loss 0.899873, acc 0.6875, prec 0.0406012, recall 0.743093\n",
      "2017-11-26T12:52:41.953937: step 1768, loss 1.2271, acc 0.671875, prec 0.0406153, recall 0.743217\n",
      "2017-11-26T12:52:42.738791: step 1769, loss 1.6809, acc 0.59375, prec 0.0406267, recall 0.743341\n",
      "2017-11-26T12:52:43.462225: step 1770, loss 1.81034, acc 0.578125, prec 0.0406249, recall 0.743404\n",
      "2017-11-26T12:52:44.194704: step 1771, loss 1.18535, acc 0.734375, prec 0.0406285, recall 0.743466\n",
      "2017-11-26T12:52:44.964451: step 1772, loss 0.896565, acc 0.6875, prec 0.0406304, recall 0.743528\n",
      "2017-11-26T12:52:45.711665: step 1773, loss 1.21933, acc 0.5625, prec 0.0406154, recall 0.743528\n",
      "2017-11-26T12:52:46.442709: step 1774, loss 1.01348, acc 0.703125, prec 0.0406052, recall 0.743528\n",
      "2017-11-26T12:52:47.169142: step 1775, loss 1.04627, acc 0.65625, prec 0.0405934, recall 0.743528\n",
      "2017-11-26T12:52:47.897344: step 1776, loss 1.21316, acc 0.75, prec 0.0406101, recall 0.743652\n",
      "2017-11-26T12:52:48.623488: step 1777, loss 0.885814, acc 0.734375, prec 0.040601, recall 0.743652\n",
      "2017-11-26T12:52:49.347727: step 1778, loss 0.907016, acc 0.765625, prec 0.0406057, recall 0.743714\n",
      "2017-11-26T12:52:50.079447: step 1779, loss 0.44433, acc 0.875, prec 0.040614, recall 0.743776\n",
      "2017-11-26T12:52:50.805488: step 1780, loss 0.609712, acc 0.8125, prec 0.0406203, recall 0.743838\n",
      "2017-11-26T12:52:51.552397: step 1781, loss 7.1322, acc 0.890625, prec 0.040617, recall 0.743658\n",
      "2017-11-26T12:52:52.288639: step 1782, loss 0.347343, acc 0.859375, prec 0.0406249, recall 0.74372\n",
      "2017-11-26T12:52:53.125053: step 1783, loss 0.529148, acc 0.828125, prec 0.0406443, recall 0.743844\n",
      "2017-11-26T12:52:53.845667: step 1784, loss 2.33244, acc 0.828125, prec 0.0406389, recall 0.743664\n",
      "2017-11-26T12:52:54.571356: step 1785, loss 0.709145, acc 0.84375, prec 0.0406589, recall 0.743788\n",
      "2017-11-26T12:52:55.305360: step 1786, loss 0.993275, acc 0.890625, prec 0.0407057, recall 0.744035\n",
      "2017-11-26T12:52:56.037756: step 1787, loss 0.892038, acc 0.75, prec 0.0407098, recall 0.744096\n",
      "2017-11-26T12:52:56.769181: step 1788, loss 0.558066, acc 0.875, prec 0.0407055, recall 0.744096\n",
      "2017-11-26T12:52:57.502240: step 1789, loss 2.0953, acc 0.84375, prec 0.0407633, recall 0.744404\n",
      "2017-11-26T12:52:58.233737: step 1790, loss 0.877686, acc 0.765625, prec 0.0407553, recall 0.744404\n",
      "2017-11-26T12:52:58.967169: step 1791, loss 0.556733, acc 0.78125, prec 0.0407478, recall 0.744404\n",
      "2017-11-26T12:52:59.694814: step 1792, loss 0.557972, acc 0.8125, prec 0.0407666, recall 0.744527\n",
      "2017-11-26T12:53:00.427546: step 1793, loss 0.595399, acc 0.796875, prec 0.0407596, recall 0.744527\n",
      "2017-11-26T12:53:01.129947: step 1794, loss 0.64366, acc 0.78125, prec 0.0407647, recall 0.744589\n",
      "2017-11-26T12:53:01.822752: step 1795, loss 0.645905, acc 0.6875, prec 0.040754, recall 0.744589\n",
      "2017-11-26T12:53:02.527602: step 1796, loss 0.603438, acc 0.796875, prec 0.0407597, recall 0.74465\n",
      "2017-11-26T12:53:03.243863: step 1797, loss 1.02474, acc 0.75, prec 0.0407637, recall 0.744712\n",
      "2017-11-26T12:53:04.002811: step 1798, loss 0.995355, acc 0.6875, prec 0.0407782, recall 0.744834\n",
      "2017-11-26T12:53:04.721815: step 1799, loss 0.6199, acc 0.84375, prec 0.0407855, recall 0.744896\n",
      "2017-11-26T12:53:05.422770: step 1800, loss 0.211911, acc 0.9375, prec 0.0407959, recall 0.744957\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:53:51.270538: step 1800, loss 1.4077, acc 0.87007, prec 0.0413198, recall 0.743209\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1800\n",
      "\n",
      "2017-11-26T12:53:53.405708: step 1801, loss 0.471162, acc 0.796875, prec 0.0413252, recall 0.743268\n",
      "2017-11-26T12:53:54.074169: step 1802, loss 0.525382, acc 0.828125, prec 0.0413193, recall 0.743268\n",
      "2017-11-26T12:53:54.750779: step 1803, loss 0.239585, acc 0.890625, prec 0.0413156, recall 0.743268\n",
      "2017-11-26T12:53:55.412752: step 1804, loss 0.425286, acc 0.84375, prec 0.0413226, recall 0.743328\n",
      "2017-11-26T12:53:56.075293: step 1805, loss 1.60369, acc 0.859375, prec 0.0413184, recall 0.743155\n",
      "2017-11-26T12:53:56.759398: step 1806, loss 1.24388, acc 0.859375, prec 0.0413259, recall 0.743215\n",
      "2017-11-26T12:53:57.440011: step 1807, loss 0.426376, acc 0.8125, prec 0.0413195, recall 0.743215\n",
      "2017-11-26T12:53:58.119406: step 1808, loss 0.586246, acc 0.796875, prec 0.041325, recall 0.743275\n",
      "2017-11-26T12:53:58.814168: step 1809, loss 0.334204, acc 0.875, prec 0.0413207, recall 0.743275\n",
      "2017-11-26T12:53:59.518854: step 1810, loss 0.291856, acc 0.890625, prec 0.041317, recall 0.743275\n",
      "2017-11-26T12:54:00.223988: step 1811, loss 0.173252, acc 0.921875, prec 0.0413267, recall 0.743334\n",
      "2017-11-26T12:54:00.922924: step 1812, loss 0.297677, acc 0.828125, prec 0.0413208, recall 0.743334\n",
      "2017-11-26T12:54:01.633595: step 1813, loss 0.427096, acc 0.828125, prec 0.0413397, recall 0.743453\n",
      "2017-11-26T12:54:02.356319: step 1814, loss 10.1615, acc 0.875, prec 0.0413359, recall 0.743281\n",
      "2017-11-26T12:54:03.119970: step 1815, loss 0.34872, acc 0.859375, prec 0.0413312, recall 0.743281\n",
      "2017-11-26T12:54:03.872090: step 1816, loss 0.291988, acc 0.890625, prec 0.0413274, recall 0.743281\n",
      "2017-11-26T12:54:04.655168: step 1817, loss 0.345118, acc 0.90625, prec 0.0413489, recall 0.7434\n",
      "2017-11-26T12:54:05.383179: step 1818, loss 0.249573, acc 0.9375, prec 0.0413468, recall 0.7434\n",
      "2017-11-26T12:54:06.143674: step 1819, loss 0.274388, acc 0.890625, prec 0.0413431, recall 0.7434\n",
      "2017-11-26T12:54:06.895907: step 1820, loss 0.769841, acc 0.78125, prec 0.041348, recall 0.743459\n",
      "2017-11-26T12:54:07.660642: step 1821, loss 2.69008, acc 0.828125, prec 0.0413426, recall 0.743287\n",
      "2017-11-26T12:54:08.440359: step 1822, loss 0.735463, acc 0.734375, prec 0.0413583, recall 0.743406\n",
      "2017-11-26T12:54:09.191431: step 1823, loss 0.206063, acc 0.921875, prec 0.0413556, recall 0.743406\n",
      "2017-11-26T12:54:09.953093: step 1824, loss 0.600378, acc 0.875, prec 0.041376, recall 0.743524\n",
      "2017-11-26T12:54:10.746958: step 1825, loss 0.382056, acc 0.828125, prec 0.0413825, recall 0.743584\n",
      "2017-11-26T12:54:11.530914: step 1826, loss 0.425915, acc 0.859375, prec 0.0413777, recall 0.743584\n",
      "2017-11-26T12:54:12.288905: step 1827, loss 0.923876, acc 0.796875, prec 0.0413955, recall 0.743702\n",
      "2017-11-26T12:54:13.070270: step 1828, loss 0.693616, acc 0.84375, prec 0.0414148, recall 0.743821\n",
      "2017-11-26T12:54:13.888183: step 1829, loss 1.05591, acc 0.75, prec 0.0414186, recall 0.74388\n",
      "2017-11-26T12:54:14.893006: step 1830, loss 0.24019, acc 0.921875, prec 0.0414159, recall 0.74388\n",
      "2017-11-26T12:54:15.896797: step 1831, loss 0.694063, acc 0.765625, prec 0.0414079, recall 0.74388\n",
      "2017-11-26T12:54:16.797894: step 1832, loss 0.518289, acc 0.8125, prec 0.0414016, recall 0.74388\n",
      "2017-11-26T12:54:17.678557: step 1833, loss 0.450351, acc 0.84375, prec 0.0414086, recall 0.743939\n",
      "2017-11-26T12:54:18.518644: step 1834, loss 0.269559, acc 0.9375, prec 0.0414064, recall 0.743939\n",
      "2017-11-26T12:54:19.268213: step 1835, loss 0.406975, acc 0.90625, prec 0.0414279, recall 0.744057\n",
      "2017-11-26T12:54:20.012279: step 1836, loss 0.310583, acc 0.890625, prec 0.0414488, recall 0.744175\n",
      "2017-11-26T12:54:20.769320: step 1837, loss 0.279637, acc 0.921875, prec 0.0414461, recall 0.744175\n",
      "2017-11-26T12:54:21.497227: step 1838, loss 1.23407, acc 0.890625, prec 0.0414547, recall 0.744234\n",
      "2017-11-26T12:54:22.214996: step 1839, loss 0.202059, acc 0.921875, prec 0.041452, recall 0.744234\n",
      "2017-11-26T12:54:22.920976: step 1840, loss 0.948886, acc 0.9375, prec 0.0414622, recall 0.744293\n",
      "2017-11-26T12:54:23.756567: step 1841, loss 0.393593, acc 0.890625, prec 0.0414831, recall 0.744411\n",
      "2017-11-26T12:54:24.468483: step 1842, loss 0.258514, acc 0.90625, prec 0.0414922, recall 0.74447\n",
      "2017-11-26T12:54:25.168167: step 1843, loss 0.306955, acc 0.90625, prec 0.041489, recall 0.74447\n",
      "2017-11-26T12:54:25.850895: step 1844, loss 0.101529, acc 0.953125, prec 0.0414997, recall 0.744529\n",
      "2017-11-26T12:54:26.545489: step 1845, loss 0.605744, acc 0.84375, prec 0.0414944, recall 0.744529\n",
      "2017-11-26T12:54:27.232168: step 1846, loss 0.300016, acc 0.84375, prec 0.0415137, recall 0.744647\n",
      "2017-11-26T12:54:27.918638: step 1847, loss 0.295007, acc 0.921875, prec 0.041511, recall 0.744647\n",
      "2017-11-26T12:54:28.583773: step 1848, loss 0.318669, acc 0.875, prec 0.0415068, recall 0.744647\n",
      "2017-11-26T12:54:29.253972: step 1849, loss 0.205216, acc 0.9375, prec 0.0415046, recall 0.744647\n",
      "2017-11-26T12:54:29.949489: step 1850, loss 0.11547, acc 0.953125, prec 0.041503, recall 0.744647\n",
      "2017-11-26T12:54:30.607371: step 1851, loss 0.164577, acc 0.921875, prec 0.0415004, recall 0.744647\n",
      "2017-11-26T12:54:31.266859: step 1852, loss 0.287435, acc 0.90625, prec 0.0415095, recall 0.744705\n",
      "2017-11-26T12:54:31.926330: step 1853, loss 0.659146, acc 0.96875, prec 0.0415576, recall 0.74494\n",
      "2017-11-26T12:54:32.552884: step 1854, loss 0.290442, acc 0.875, prec 0.0415533, recall 0.74494\n",
      "2017-11-26T12:54:33.202664: step 1855, loss 0.296018, acc 0.9375, prec 0.0415758, recall 0.745057\n",
      "2017-11-26T12:54:33.833328: step 1856, loss 2.22998, acc 0.921875, prec 0.0415737, recall 0.744886\n",
      "2017-11-26T12:54:34.477799: step 1857, loss 3.98152, acc 0.9375, prec 0.0415721, recall 0.744715\n",
      "2017-11-26T12:54:35.113027: step 1858, loss 3.10889, acc 0.859375, prec 0.0415924, recall 0.744661\n",
      "2017-11-26T12:54:35.746951: step 1859, loss 0.527552, acc 0.890625, prec 0.0416009, recall 0.74472\n",
      "2017-11-26T12:54:36.412197: step 1860, loss 0.779756, acc 0.765625, prec 0.0416052, recall 0.744779\n",
      "2017-11-26T12:54:37.062967: step 1861, loss 0.726861, acc 0.765625, prec 0.0415972, recall 0.744779\n",
      "2017-11-26T12:54:37.714040: step 1862, loss 0.805591, acc 0.765625, prec 0.0415892, recall 0.744779\n",
      "2017-11-26T12:54:38.353949: step 1863, loss 0.511931, acc 0.765625, prec 0.0415812, recall 0.744779\n",
      "2017-11-26T12:54:38.984959: step 1864, loss 1.57983, acc 0.609375, prec 0.0415679, recall 0.744779\n",
      "2017-11-26T12:54:39.617842: step 1865, loss 1.836, acc 0.703125, prec 0.0415701, recall 0.744837\n",
      "2017-11-26T12:54:40.230940: step 1866, loss 0.95328, acc 0.734375, prec 0.041561, recall 0.744837\n",
      "2017-11-26T12:54:40.846921: step 1867, loss 0.945615, acc 0.71875, prec 0.0415515, recall 0.744837\n",
      "2017-11-26T12:54:41.558484: step 1868, loss 1.36472, acc 0.5625, prec 0.0415488, recall 0.744896\n",
      "2017-11-26T12:54:42.188117: step 1869, loss 0.781254, acc 0.8125, prec 0.0415425, recall 0.744896\n",
      "2017-11-26T12:54:42.823554: step 1870, loss 0.890737, acc 0.78125, prec 0.041535, recall 0.744896\n",
      "2017-11-26T12:54:43.502979: step 1871, loss 1.41264, acc 0.6875, prec 0.0415367, recall 0.744954\n",
      "2017-11-26T12:54:44.197357: step 1872, loss 0.490137, acc 0.8125, prec 0.0415303, recall 0.744954\n",
      "2017-11-26T12:54:44.857998: step 1873, loss 1.38156, acc 0.703125, prec 0.0415324, recall 0.745013\n",
      "2017-11-26T12:54:45.592539: step 1874, loss 0.581127, acc 0.828125, prec 0.0415511, recall 0.74513\n",
      "2017-11-26T12:54:46.416754: step 1875, loss 0.616222, acc 0.75, prec 0.0415549, recall 0.745188\n",
      "2017-11-26T12:54:47.371504: step 1876, loss 0.756836, acc 0.71875, prec 0.0415575, recall 0.745246\n",
      "2017-11-26T12:54:48.161477: step 1877, loss 0.430625, acc 0.890625, prec 0.0415538, recall 0.745246\n",
      "2017-11-26T12:54:48.933708: step 1878, loss 0.667799, acc 0.796875, prec 0.0415469, recall 0.745246\n",
      "2017-11-26T12:54:49.751755: step 1879, loss 0.598746, acc 0.796875, prec 0.04154, recall 0.745246\n",
      "2017-11-26T12:54:50.656686: step 1880, loss 0.400701, acc 0.859375, prec 0.0415597, recall 0.745363\n",
      "2017-11-26T12:54:51.579923: step 1881, loss 0.331134, acc 0.921875, prec 0.0415693, recall 0.745421\n",
      "2017-11-26T12:54:52.670219: step 1882, loss 0.583306, acc 0.84375, prec 0.0415762, recall 0.74548\n",
      "2017-11-26T12:54:53.940067: step 1883, loss 0.160752, acc 0.953125, prec 0.0415747, recall 0.74548\n",
      "2017-11-26T12:54:55.210457: step 1884, loss 1.58337, acc 0.875, prec 0.0415709, recall 0.745309\n",
      "2017-11-26T12:54:56.555632: step 1885, loss 0.446535, acc 0.90625, prec 0.0415678, recall 0.745309\n",
      "2017-11-26T12:54:58.415906: step 1886, loss 0.160763, acc 0.953125, prec 0.0415662, recall 0.745309\n",
      "2017-11-26T12:55:00.320132: step 1887, loss 0.122539, acc 0.953125, prec 0.0415768, recall 0.745367\n",
      "2017-11-26T12:55:02.432533: step 1888, loss 3.93624, acc 0.875, prec 0.0415731, recall 0.745197\n",
      "2017-11-26T12:55:04.663145: step 1889, loss 4.6085, acc 0.875, prec 0.0415694, recall 0.745026\n",
      "2017-11-26T12:55:06.989239: step 1890, loss 0.389207, acc 0.859375, prec 0.0415768, recall 0.745085\n",
      "2017-11-26T12:55:09.302333: step 1891, loss 0.51828, acc 0.875, prec 0.0415848, recall 0.745143\n",
      "2017-11-26T12:55:10.894578: step 1892, loss 0.248672, acc 0.90625, prec 0.0415816, recall 0.745143\n",
      "2017-11-26T12:55:12.970685: step 1893, loss 0.289384, acc 0.90625, prec 0.0415784, recall 0.745143\n",
      "2017-11-26T12:55:14.741676: step 1894, loss 0.393939, acc 0.828125, prec 0.0415726, recall 0.745143\n",
      "2017-11-26T12:55:16.208727: step 1895, loss 0.450554, acc 0.84375, prec 0.0415673, recall 0.745143\n",
      "2017-11-26T12:55:17.887743: step 1896, loss 0.359607, acc 0.921875, prec 0.0415647, recall 0.745143\n",
      "2017-11-26T12:55:19.702355: step 1897, loss 0.691978, acc 0.8125, prec 0.0415827, recall 0.745259\n",
      "2017-11-26T12:55:21.628868: step 1898, loss 1.6931, acc 0.796875, prec 0.0415881, recall 0.745318\n",
      "2017-11-26T12:55:22.849286: step 1899, loss 0.713757, acc 0.796875, prec 0.0416056, recall 0.745434\n",
      "2017-11-26T12:55:23.942472: step 1900, loss 0.472648, acc 0.890625, prec 0.0416019, recall 0.745434\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:56:24.651571: step 1900, loss 1.39323, acc 0.859407, prec 0.0420473, recall 0.743975\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-1900\n",
      "\n",
      "2017-11-26T12:56:26.366680: step 1901, loss 0.634869, acc 0.859375, prec 0.0420665, recall 0.744088\n",
      "2017-11-26T12:56:26.927843: step 1902, loss 1.03233, acc 0.84375, prec 0.0420851, recall 0.744201\n",
      "2017-11-26T12:56:27.490657: step 1903, loss 0.775928, acc 0.75, prec 0.0420767, recall 0.744201\n",
      "2017-11-26T12:56:28.066883: step 1904, loss 0.60029, acc 0.796875, prec 0.0420819, recall 0.744258\n",
      "2017-11-26T12:56:28.604368: step 1905, loss 0.622582, acc 0.875, prec 0.0420896, recall 0.744314\n",
      "2017-11-26T12:56:29.143073: step 1906, loss 0.63033, acc 0.84375, prec 0.0421322, recall 0.74454\n",
      "2017-11-26T12:56:29.692941: step 1907, loss 1.04846, acc 0.75, prec 0.0421238, recall 0.74454\n",
      "2017-11-26T12:56:30.323800: step 1908, loss 0.423588, acc 0.859375, prec 0.0421191, recall 0.74454\n",
      "2017-11-26T12:56:31.023895: step 1909, loss 1.9955, acc 0.71875, prec 0.0421101, recall 0.744376\n",
      "2017-11-26T12:56:31.609032: step 1910, loss 0.599091, acc 0.859375, prec 0.0421054, recall 0.744376\n",
      "2017-11-26T12:56:32.186875: step 1911, loss 0.498363, acc 0.875, prec 0.0421012, recall 0.744376\n",
      "2017-11-26T12:56:32.815229: step 1912, loss 0.347332, acc 0.9375, prec 0.042111, recall 0.744432\n",
      "2017-11-26T12:56:33.453284: step 1913, loss 0.844405, acc 0.75, prec 0.0421265, recall 0.744545\n",
      "2017-11-26T12:56:34.117977: step 1914, loss 0.57801, acc 0.84375, prec 0.0421452, recall 0.744657\n",
      "2017-11-26T12:56:34.754542: step 1915, loss 0.517173, acc 0.765625, prec 0.0421492, recall 0.744714\n",
      "2017-11-26T12:56:35.395568: step 1916, loss 0.472867, acc 0.84375, prec 0.042144, recall 0.744714\n",
      "2017-11-26T12:56:36.037531: step 1917, loss 1.70345, acc 0.84375, prec 0.0421512, recall 0.744606\n",
      "2017-11-26T12:56:36.889691: step 1918, loss 0.380977, acc 0.90625, prec 0.042148, recall 0.744606\n",
      "2017-11-26T12:56:37.691246: step 1919, loss 0.90423, acc 0.84375, prec 0.0421547, recall 0.744662\n",
      "2017-11-26T12:56:38.434332: step 1920, loss 0.201836, acc 0.90625, prec 0.0421516, recall 0.744662\n",
      "2017-11-26T12:56:39.125343: step 1921, loss 4.88239, acc 0.90625, prec 0.0421609, recall 0.744554\n",
      "2017-11-26T12:56:39.807679: step 1922, loss 0.473047, acc 0.84375, prec 0.0421675, recall 0.744611\n",
      "2017-11-26T12:56:40.567309: step 1923, loss 2.41296, acc 0.796875, prec 0.0421732, recall 0.744503\n",
      "2017-11-26T12:56:41.419000: step 1924, loss 0.653261, acc 0.78125, prec 0.0421778, recall 0.744559\n",
      "2017-11-26T12:56:42.273684: step 1925, loss 0.711338, acc 0.78125, prec 0.0421823, recall 0.744615\n",
      "2017-11-26T12:56:42.948596: step 1926, loss 0.567369, acc 0.8125, prec 0.042176, recall 0.744615\n",
      "2017-11-26T12:56:43.631215: step 1927, loss 0.977074, acc 0.75, prec 0.0421795, recall 0.744672\n",
      "2017-11-26T12:56:44.322275: step 1928, loss 0.748118, acc 0.78125, prec 0.0421841, recall 0.744728\n",
      "2017-11-26T12:56:44.958699: step 1929, loss 0.917538, acc 0.78125, prec 0.0421768, recall 0.744728\n",
      "2017-11-26T12:56:45.574866: step 1930, loss 1.26046, acc 0.734375, prec 0.0421798, recall 0.744784\n",
      "2017-11-26T12:56:46.192928: step 1931, loss 1.29956, acc 0.71875, prec 0.0421822, recall 0.74484\n",
      "2017-11-26T12:56:46.865283: step 1932, loss 0.678774, acc 0.796875, prec 0.0422111, recall 0.745008\n",
      "2017-11-26T12:56:47.478519: step 1933, loss 0.55792, acc 0.8125, prec 0.0422168, recall 0.745064\n",
      "2017-11-26T12:56:48.046322: step 1934, loss 1.15748, acc 0.671875, prec 0.0422295, recall 0.745175\n",
      "2017-11-26T12:56:48.622376: step 1935, loss 0.468713, acc 0.828125, prec 0.0422476, recall 0.745287\n",
      "2017-11-26T12:56:49.214302: step 1936, loss 0.556293, acc 0.796875, prec 0.0422526, recall 0.745343\n",
      "2017-11-26T12:56:49.785599: step 1937, loss 0.624703, acc 0.828125, prec 0.0422469, recall 0.745343\n",
      "2017-11-26T12:56:50.460265: step 1938, loss 0.712584, acc 0.828125, prec 0.0422411, recall 0.745343\n",
      "2017-11-26T12:56:51.069365: step 1939, loss 0.756481, acc 0.8125, prec 0.0422586, recall 0.745455\n",
      "2017-11-26T12:56:51.663989: step 1940, loss 0.491719, acc 0.875, prec 0.0422544, recall 0.745455\n",
      "2017-11-26T12:56:52.241952: step 1941, loss 0.444386, acc 0.84375, prec 0.042261, recall 0.74551\n",
      "2017-11-26T12:56:52.812412: step 1942, loss 0.704117, acc 0.828125, prec 0.0422553, recall 0.74551\n",
      "2017-11-26T12:56:53.383472: step 1943, loss 1.14091, acc 0.859375, prec 0.0422624, recall 0.745566\n",
      "2017-11-26T12:56:53.949419: step 1944, loss 2.61869, acc 0.890625, prec 0.0423063, recall 0.745789\n",
      "2017-11-26T12:56:54.551404: step 1945, loss 0.126916, acc 0.984375, prec 0.0423177, recall 0.745844\n",
      "2017-11-26T12:56:55.113619: step 1946, loss 0.382957, acc 0.84375, prec 0.0423124, recall 0.745844\n",
      "2017-11-26T12:56:55.679695: step 1947, loss 0.655858, acc 0.765625, prec 0.0423045, recall 0.745844\n",
      "2017-11-26T12:56:56.242612: step 1948, loss 11.3518, acc 0.828125, prec 0.0422993, recall 0.745681\n",
      "2017-11-26T12:56:56.787916: step 1949, loss 0.520991, acc 0.859375, prec 0.0423065, recall 0.745737\n",
      "2017-11-26T12:56:57.339232: step 1950, loss 0.619612, acc 0.828125, prec 0.0423244, recall 0.745848\n",
      "2017-11-26T12:56:57.880877: step 1951, loss 0.296308, acc 0.859375, prec 0.0423197, recall 0.745848\n",
      "2017-11-26T12:56:58.445636: step 1952, loss 0.583686, acc 0.828125, prec 0.0423258, recall 0.745903\n",
      "2017-11-26T12:56:58.989657: step 1953, loss 0.529657, acc 0.859375, prec 0.0423448, recall 0.746014\n",
      "2017-11-26T12:56:59.534392: step 1954, loss 1.02384, acc 0.703125, prec 0.0423467, recall 0.74607\n",
      "2017-11-26T12:57:00.083380: step 1955, loss 0.872437, acc 0.78125, prec 0.0423394, recall 0.74607\n",
      "2017-11-26T12:57:00.625398: step 1956, loss 0.70185, acc 0.765625, prec 0.0423315, recall 0.74607\n",
      "2017-11-26T12:57:01.174841: step 1957, loss 1.21112, acc 0.6875, prec 0.042321, recall 0.74607\n",
      "2017-11-26T12:57:01.702438: step 1958, loss 0.405291, acc 0.8125, prec 0.0423266, recall 0.746125\n",
      "2017-11-26T12:57:02.233602: step 1959, loss 2.02972, acc 0.78125, prec 0.0423435, recall 0.746073\n",
      "2017-11-26T12:57:02.754843: step 1960, loss 0.372292, acc 0.890625, prec 0.0423398, recall 0.746073\n",
      "2017-11-26T12:57:03.276836: step 1961, loss 0.54235, acc 0.859375, prec 0.0423351, recall 0.746073\n",
      "2017-11-26T12:57:03.819844: step 1962, loss 0.566518, acc 0.796875, prec 0.0423283, recall 0.746073\n",
      "2017-11-26T12:57:04.383149: step 1963, loss 0.66436, acc 0.796875, prec 0.0423333, recall 0.746129\n",
      "2017-11-26T12:57:04.906665: step 1964, loss 1.55896, acc 0.828125, prec 0.04234, recall 0.746021\n",
      "2017-11-26T12:57:05.447734: step 1965, loss 0.716753, acc 0.8125, prec 0.0423455, recall 0.746077\n",
      "2017-11-26T12:57:06.006638: step 1966, loss 1.99837, acc 0.8125, prec 0.0423634, recall 0.746025\n",
      "2017-11-26T12:57:06.555189: step 1967, loss 0.623574, acc 0.828125, prec 0.0423577, recall 0.746025\n",
      "2017-11-26T12:57:07.090502: step 1968, loss 0.425032, acc 0.828125, prec 0.0423519, recall 0.746025\n",
      "2017-11-26T12:57:07.633020: step 1969, loss 0.878283, acc 0.78125, prec 0.0423801, recall 0.746191\n",
      "2017-11-26T12:57:08.178093: step 1970, loss 3.13189, acc 0.765625, prec 0.0423846, recall 0.746084\n",
      "2017-11-26T12:57:08.722216: step 1971, loss 1.41407, acc 0.8125, prec 0.042402, recall 0.746194\n",
      "2017-11-26T12:57:09.267017: step 1972, loss 0.631906, acc 0.796875, prec 0.042407, recall 0.746249\n",
      "2017-11-26T12:57:09.812614: step 1973, loss 0.683684, acc 0.765625, prec 0.0423992, recall 0.746249\n",
      "2017-11-26T12:57:10.350558: step 1974, loss 0.916187, acc 0.734375, prec 0.0424021, recall 0.746304\n",
      "2017-11-26T12:57:10.899961: step 1975, loss 1.04422, acc 0.71875, prec 0.0424163, recall 0.746415\n",
      "2017-11-26T12:57:11.442892: step 1976, loss 0.793046, acc 0.734375, prec 0.0424192, recall 0.74647\n",
      "2017-11-26T12:57:11.982770: step 1977, loss 1.15141, acc 0.734375, prec 0.0424221, recall 0.746525\n",
      "2017-11-26T12:57:12.532901: step 1978, loss 0.443201, acc 0.875, prec 0.042418, recall 0.746525\n",
      "2017-11-26T12:57:13.076523: step 1979, loss 0.97319, acc 0.75, prec 0.0424214, recall 0.74658\n",
      "2017-11-26T12:57:13.624144: step 1980, loss 1.05169, acc 0.8125, prec 0.0424506, recall 0.746745\n",
      "2017-11-26T12:57:14.176827: step 1981, loss 0.809456, acc 0.75, prec 0.0424422, recall 0.746745\n",
      "2017-11-26T12:57:14.733226: step 1982, loss 0.811798, acc 0.8125, prec 0.0424477, recall 0.7468\n",
      "2017-11-26T12:57:15.284302: step 1983, loss 0.382822, acc 0.859375, prec 0.0424666, recall 0.74691\n",
      "2017-11-26T12:57:15.827214: step 1984, loss 0.60959, acc 0.8125, prec 0.0424721, recall 0.746964\n",
      "2017-11-26T12:57:16.372482: step 1985, loss 0.702332, acc 0.796875, prec 0.0424771, recall 0.747019\n",
      "2017-11-26T12:57:16.914058: step 1986, loss 1.44818, acc 0.8125, prec 0.0424827, recall 0.747074\n",
      "2017-11-26T12:57:17.461401: step 1987, loss 0.40473, acc 0.90625, prec 0.0424913, recall 0.747129\n",
      "2017-11-26T12:57:17.911759: step 1988, loss 0.316016, acc 0.923077, prec 0.0425128, recall 0.747238\n",
      "2017-11-26T12:57:18.444692: step 1989, loss 0.277313, acc 0.890625, prec 0.0425209, recall 0.747293\n",
      "2017-11-26T12:57:18.968537: step 1990, loss 0.342795, acc 0.890625, prec 0.0425291, recall 0.747348\n",
      "2017-11-26T12:57:19.496722: step 1991, loss 0.346104, acc 0.875, prec 0.0425485, recall 0.747457\n",
      "2017-11-26T12:57:20.021329: step 1992, loss 0.915365, acc 0.90625, prec 0.0425571, recall 0.747512\n",
      "2017-11-26T12:57:20.542745: step 1993, loss 0.24537, acc 0.921875, prec 0.0425545, recall 0.747512\n",
      "2017-11-26T12:57:21.068756: step 1994, loss 0.377643, acc 0.859375, prec 0.0425498, recall 0.747512\n",
      "2017-11-26T12:57:21.598136: step 1995, loss 0.246899, acc 0.921875, prec 0.042559, recall 0.747567\n",
      "2017-11-26T12:57:22.140918: step 1996, loss 0.547689, acc 0.90625, prec 0.042603, recall 0.747785\n",
      "2017-11-26T12:57:22.687199: step 1997, loss 0.262649, acc 0.953125, prec 0.0426014, recall 0.747785\n",
      "2017-11-26T12:57:23.233800: step 1998, loss 0.282795, acc 0.9375, prec 0.0425993, recall 0.747785\n",
      "2017-11-26T12:57:23.778786: step 1999, loss 0.210235, acc 0.921875, prec 0.0426085, recall 0.747839\n",
      "2017-11-26T12:57:24.336739: step 2000, loss 0.275046, acc 0.90625, prec 0.0426053, recall 0.747839\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:58:05.331115: step 2000, loss 2.54321, acc 0.941404, prec 0.0430272, recall 0.738\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-2000\n",
      "\n",
      "2017-11-26T12:58:07.138819: step 2001, loss 0.0959173, acc 0.96875, prec 0.0430495, recall 0.73811\n",
      "2017-11-26T12:58:07.721723: step 2002, loss 0.371985, acc 0.890625, prec 0.0430458, recall 0.73811\n",
      "2017-11-26T12:58:08.328246: step 2003, loss 4.1288, acc 0.875, prec 0.0430538, recall 0.73801\n",
      "2017-11-26T12:58:08.887412: step 2004, loss 0.119203, acc 0.953125, prec 0.0430523, recall 0.73801\n",
      "2017-11-26T12:58:09.442520: step 2005, loss 0.262887, acc 0.859375, prec 0.0430709, recall 0.73812\n",
      "2017-11-26T12:58:09.997035: step 2006, loss 0.206518, acc 0.984375, prec 0.0431054, recall 0.738285\n",
      "2017-11-26T12:58:10.555784: step 2007, loss 0.368615, acc 0.9375, prec 0.0431033, recall 0.738285\n",
      "2017-11-26T12:58:11.114127: step 2008, loss 0.257779, acc 0.9375, prec 0.0431012, recall 0.738285\n",
      "2017-11-26T12:58:11.663231: step 2009, loss 0.208099, acc 0.921875, prec 0.0431103, recall 0.738339\n",
      "2017-11-26T12:58:12.214770: step 2010, loss 0.0524182, acc 0.984375, prec 0.0431214, recall 0.738394\n",
      "2017-11-26T12:58:12.765496: step 2011, loss 0.165568, acc 0.90625, prec 0.0431183, recall 0.738394\n",
      "2017-11-26T12:58:13.322439: step 2012, loss 0.337033, acc 0.90625, prec 0.0431151, recall 0.738394\n",
      "2017-11-26T12:58:13.873216: step 2013, loss 0.236893, acc 0.9375, prec 0.043113, recall 0.738394\n",
      "2017-11-26T12:58:14.425053: step 2014, loss 0.197023, acc 0.90625, prec 0.0431216, recall 0.738449\n",
      "2017-11-26T12:58:15.023831: step 2015, loss 0.271316, acc 0.875, prec 0.043129, recall 0.738503\n",
      "2017-11-26T12:58:15.598081: step 2016, loss 0.5043, acc 0.875, prec 0.0431365, recall 0.738558\n",
      "2017-11-26T12:58:16.166970: step 2017, loss 0.477601, acc 0.875, prec 0.0431323, recall 0.738558\n",
      "2017-11-26T12:58:16.731397: step 2018, loss 0.0651957, acc 0.96875, prec 0.0431429, recall 0.738613\n",
      "2017-11-26T12:58:17.296732: step 2019, loss 0.0998794, acc 0.9375, prec 0.0431408, recall 0.738613\n",
      "2017-11-26T12:58:17.863983: step 2020, loss 0.241884, acc 0.921875, prec 0.0431498, recall 0.738667\n",
      "2017-11-26T12:58:18.429611: step 2021, loss 0.348992, acc 0.859375, prec 0.0431568, recall 0.738722\n",
      "2017-11-26T12:58:19.008095: step 2022, loss 0.15282, acc 0.921875, prec 0.0431658, recall 0.738776\n",
      "2017-11-26T12:58:19.571357: step 2023, loss 0.360501, acc 0.921875, prec 0.0431749, recall 0.738831\n",
      "2017-11-26T12:58:20.131759: step 2024, loss 0.179555, acc 0.953125, prec 0.0431733, recall 0.738831\n",
      "2017-11-26T12:58:20.701802: step 2025, loss 0.392119, acc 0.921875, prec 0.043194, recall 0.73894\n",
      "2017-11-26T12:58:21.264780: step 2026, loss 0.187498, acc 0.9375, prec 0.0431919, recall 0.73894\n",
      "2017-11-26T12:58:21.829397: step 2027, loss 0.184939, acc 0.9375, prec 0.0431898, recall 0.73894\n",
      "2017-11-26T12:58:22.395733: step 2028, loss 0.0705184, acc 0.984375, prec 0.0431892, recall 0.73894\n",
      "2017-11-26T12:58:22.955520: step 2029, loss 0.514021, acc 0.96875, prec 0.0432115, recall 0.739049\n",
      "2017-11-26T12:58:23.523302: step 2030, loss 0.0566192, acc 0.984375, prec 0.043211, recall 0.739049\n",
      "2017-11-26T12:58:24.098294: step 2031, loss 2.17064, acc 0.953125, prec 0.0432333, recall 0.739004\n",
      "2017-11-26T12:58:24.672148: step 2032, loss 0.0543353, acc 0.984375, prec 0.0432444, recall 0.739058\n",
      "2017-11-26T12:58:25.290334: step 2033, loss 0.140592, acc 0.921875, prec 0.0432418, recall 0.739058\n",
      "2017-11-26T12:58:25.856265: step 2034, loss 0.162534, acc 0.96875, prec 0.0432524, recall 0.739112\n",
      "2017-11-26T12:58:26.424713: step 2035, loss 0.234392, acc 0.90625, prec 0.0432492, recall 0.739112\n",
      "2017-11-26T12:58:26.991346: step 2036, loss 1.87449, acc 0.9375, prec 0.0432477, recall 0.738958\n",
      "2017-11-26T12:58:27.562269: step 2037, loss 0.45459, acc 0.921875, prec 0.0432567, recall 0.739013\n",
      "2017-11-26T12:58:28.146821: step 2038, loss 0.390873, acc 0.921875, prec 0.0432774, recall 0.739121\n",
      "2017-11-26T12:58:28.727751: step 2039, loss 0.166415, acc 0.9375, prec 0.0432869, recall 0.739176\n",
      "2017-11-26T12:58:29.313967: step 2040, loss 0.362443, acc 0.890625, prec 0.0433066, recall 0.739284\n",
      "2017-11-26T12:58:29.899312: step 2041, loss 0.218731, acc 0.90625, prec 0.0433034, recall 0.739284\n",
      "2017-11-26T12:58:30.480137: step 2042, loss 0.425429, acc 0.828125, prec 0.0432976, recall 0.739284\n",
      "2017-11-26T12:58:31.062731: step 2043, loss 0.317797, acc 0.890625, prec 0.0432939, recall 0.739284\n",
      "2017-11-26T12:58:31.628193: step 2044, loss 0.473248, acc 0.875, prec 0.043313, recall 0.739393\n",
      "2017-11-26T12:58:32.189757: step 2045, loss 0.205509, acc 0.921875, prec 0.0433337, recall 0.739501\n",
      "2017-11-26T12:58:32.752876: step 2046, loss 1.39101, acc 0.921875, prec 0.0433543, recall 0.739609\n",
      "2017-11-26T12:58:33.318532: step 2047, loss 0.741124, acc 0.765625, prec 0.0433581, recall 0.739663\n",
      "2017-11-26T12:58:33.885082: step 2048, loss 0.439778, acc 0.875, prec 0.0433655, recall 0.739717\n",
      "2017-11-26T12:58:34.452924: step 2049, loss 0.415587, acc 0.90625, prec 0.043374, recall 0.739772\n",
      "2017-11-26T12:58:35.057642: step 2050, loss 0.53488, acc 0.90625, prec 0.0433708, recall 0.739772\n",
      "2017-11-26T12:58:35.624312: step 2051, loss 0.451181, acc 0.859375, prec 0.0433777, recall 0.739826\n",
      "2017-11-26T12:58:36.195046: step 2052, loss 3.22914, acc 0.859375, prec 0.0433968, recall 0.73978\n",
      "2017-11-26T12:58:36.763489: step 2053, loss 0.7204, acc 0.78125, prec 0.0433894, recall 0.73978\n",
      "2017-11-26T12:58:37.331872: step 2054, loss 0.442129, acc 0.875, prec 0.0433968, recall 0.739834\n",
      "2017-11-26T12:58:37.899677: step 2055, loss 0.400648, acc 0.875, prec 0.0434042, recall 0.739888\n",
      "2017-11-26T12:58:38.466224: step 2056, loss 0.487403, acc 0.859375, prec 0.0434111, recall 0.739942\n",
      "2017-11-26T12:58:39.026777: step 2057, loss 0.968688, acc 0.71875, prec 0.0434248, recall 0.74005\n",
      "2017-11-26T12:58:39.608649: step 2058, loss 0.615402, acc 0.796875, prec 0.0434296, recall 0.740104\n",
      "2017-11-26T12:58:40.191478: step 2059, loss 0.701037, acc 0.828125, prec 0.0434354, recall 0.740157\n",
      "2017-11-26T12:58:40.761360: step 2060, loss 0.282162, acc 0.890625, prec 0.0434317, recall 0.740157\n",
      "2017-11-26T12:58:41.322116: step 2061, loss 0.389203, acc 0.796875, prec 0.0434249, recall 0.740157\n",
      "2017-11-26T12:58:41.890541: step 2062, loss 0.552486, acc 0.890625, prec 0.0434328, recall 0.740211\n",
      "2017-11-26T12:58:42.464167: step 2063, loss 0.527561, acc 0.796875, prec 0.0434376, recall 0.740265\n",
      "2017-11-26T12:58:43.025633: step 2064, loss 0.151447, acc 0.9375, prec 0.0434355, recall 0.740265\n",
      "2017-11-26T12:58:43.589272: step 2065, loss 0.505709, acc 0.84375, prec 0.0434534, recall 0.740373\n",
      "2017-11-26T12:58:44.155537: step 2066, loss 0.182311, acc 0.96875, prec 0.0434756, recall 0.74048\n",
      "2017-11-26T12:58:44.723035: step 2067, loss 0.214966, acc 0.90625, prec 0.0434725, recall 0.74048\n",
      "2017-11-26T12:58:45.309968: step 2068, loss 0.31158, acc 0.90625, prec 0.0434693, recall 0.74048\n",
      "2017-11-26T12:58:45.879946: step 2069, loss 0.881414, acc 0.90625, prec 0.0434894, recall 0.740588\n",
      "2017-11-26T12:58:46.444397: step 2070, loss 0.320015, acc 0.921875, prec 0.0434867, recall 0.740588\n",
      "2017-11-26T12:58:47.004612: step 2071, loss 0.574996, acc 0.90625, prec 0.0434952, recall 0.740641\n",
      "2017-11-26T12:58:47.571293: step 2072, loss 0.37521, acc 0.921875, prec 0.0434925, recall 0.740641\n",
      "2017-11-26T12:58:48.135514: step 2073, loss 0.202137, acc 1, prec 0.0435041, recall 0.740695\n",
      "2017-11-26T12:58:48.700096: step 2074, loss 0.492271, acc 0.875, prec 0.0435115, recall 0.740748\n",
      "2017-11-26T12:58:49.276648: step 2075, loss 0.377967, acc 0.90625, prec 0.04352, recall 0.740802\n",
      "2017-11-26T12:58:49.845929: step 2076, loss 0.548717, acc 0.859375, prec 0.0435268, recall 0.740856\n",
      "2017-11-26T12:58:50.422072: step 2077, loss 0.527974, acc 0.875, prec 0.0435226, recall 0.740856\n",
      "2017-11-26T12:58:51.009461: step 2078, loss 2.19452, acc 0.921875, prec 0.0435321, recall 0.740756\n",
      "2017-11-26T12:58:51.592132: step 2079, loss 0.41571, acc 0.890625, prec 0.0435516, recall 0.740863\n",
      "2017-11-26T12:58:52.188307: step 2080, loss 0.235526, acc 0.9375, prec 0.0435611, recall 0.740917\n",
      "2017-11-26T12:58:52.844847: step 2081, loss 0.221307, acc 0.921875, prec 0.0435701, recall 0.74097\n",
      "2017-11-26T12:58:53.551320: step 2082, loss 0.219859, acc 0.953125, prec 0.0435685, recall 0.74097\n",
      "2017-11-26T12:58:54.280324: step 2083, loss 0.428261, acc 0.84375, prec 0.0435748, recall 0.741024\n",
      "2017-11-26T12:58:55.170436: step 2084, loss 0.207794, acc 0.921875, prec 0.0435838, recall 0.741077\n",
      "2017-11-26T12:58:55.973278: step 2085, loss 1.75701, acc 0.875, prec 0.0435917, recall 0.740978\n",
      "2017-11-26T12:58:56.876754: step 2086, loss 7.04427, acc 0.921875, prec 0.0436012, recall 0.740878\n",
      "2017-11-26T12:58:57.855857: step 2087, loss 0.573177, acc 0.8125, prec 0.0435948, recall 0.740878\n",
      "2017-11-26T12:58:58.766661: step 2088, loss 2.70422, acc 0.828125, prec 0.0435895, recall 0.740725\n",
      "2017-11-26T12:58:59.763035: step 2089, loss 0.639654, acc 0.8125, prec 0.0435948, recall 0.740779\n",
      "2017-11-26T12:59:00.772171: step 2090, loss 0.748386, acc 0.78125, prec 0.0435874, recall 0.740779\n",
      "2017-11-26T12:59:01.903985: step 2091, loss 0.706131, acc 0.78125, prec 0.0435916, recall 0.740832\n",
      "2017-11-26T12:59:03.157987: step 2092, loss 0.938411, acc 0.734375, prec 0.0435942, recall 0.740886\n",
      "2017-11-26T12:59:04.176714: step 2093, loss 1.52661, acc 0.734375, prec 0.0435852, recall 0.740886\n",
      "2017-11-26T12:59:05.063810: step 2094, loss 1.13705, acc 0.6875, prec 0.0435746, recall 0.740886\n",
      "2017-11-26T12:59:05.949951: step 2095, loss 0.888206, acc 0.703125, prec 0.0435646, recall 0.740886\n",
      "2017-11-26T12:59:06.886536: step 2096, loss 0.864823, acc 0.765625, prec 0.0435683, recall 0.740939\n",
      "2017-11-26T12:59:07.997576: step 2097, loss 3.84341, acc 0.796875, prec 0.0435735, recall 0.74084\n",
      "2017-11-26T12:59:09.012141: step 2098, loss 1.03442, acc 0.734375, prec 0.0435877, recall 0.740947\n",
      "2017-11-26T12:59:10.030399: step 2099, loss 0.984297, acc 0.6875, prec 0.0435772, recall 0.740947\n",
      "2017-11-26T12:59:11.117193: step 2100, loss 1.07451, acc 0.65625, prec 0.0435771, recall 0.741\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T12:59:48.652945: step 2100, loss 1.22146, acc 0.750142, prec 0.0436671, recall 0.745404\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-2100\n",
      "\n",
      "2017-11-26T12:59:50.062679: step 2101, loss 1.13436, acc 0.703125, prec 0.043691, recall 0.745556\n",
      "2017-11-26T12:59:50.609368: step 2102, loss 1.18061, acc 0.71875, prec 0.0437153, recall 0.745709\n",
      "2017-11-26T12:59:51.164472: step 2103, loss 1.14797, acc 0.71875, prec 0.0437173, recall 0.745759\n",
      "2017-11-26T12:59:51.712599: step 2104, loss 0.931278, acc 0.765625, prec 0.0437096, recall 0.745759\n",
      "2017-11-26T12:59:52.262440: step 2105, loss 0.703835, acc 0.828125, prec 0.0437264, recall 0.745861\n",
      "2017-11-26T12:59:52.824957: step 2106, loss 0.653346, acc 0.796875, prec 0.0437309, recall 0.745911\n",
      "2017-11-26T12:59:53.375835: step 2107, loss 0.673589, acc 0.8125, prec 0.0437248, recall 0.745911\n",
      "2017-11-26T12:59:53.919397: step 2108, loss 0.455847, acc 0.828125, prec 0.0437303, recall 0.745962\n",
      "2017-11-26T12:59:54.467988: step 2109, loss 0.564726, acc 0.796875, prec 0.0437237, recall 0.745962\n",
      "2017-11-26T12:59:55.030302: step 2110, loss 0.375748, acc 0.921875, prec 0.0437323, recall 0.746013\n",
      "2017-11-26T12:59:55.613650: step 2111, loss 0.24918, acc 0.921875, prec 0.0437521, recall 0.746114\n",
      "2017-11-26T12:59:56.175968: step 2112, loss 0.275248, acc 0.890625, prec 0.0437485, recall 0.746114\n",
      "2017-11-26T12:59:56.747311: step 2113, loss 0.366004, acc 0.890625, prec 0.0437561, recall 0.746165\n",
      "2017-11-26T12:59:57.314998: step 2114, loss 0.238166, acc 0.9375, prec 0.0437653, recall 0.746215\n",
      "2017-11-26T12:59:57.874804: step 2115, loss 1.86192, acc 0.875, prec 0.0437729, recall 0.746117\n",
      "2017-11-26T12:59:58.441916: step 2116, loss 1.03839, acc 0.90625, prec 0.0437921, recall 0.746218\n",
      "2017-11-26T12:59:59.012374: step 2117, loss 0.507245, acc 0.875, prec 0.0437992, recall 0.746269\n",
      "2017-11-26T12:59:59.605248: step 2118, loss 0.4078, acc 0.875, prec 0.0437951, recall 0.746269\n",
      "2017-11-26T13:00:00.186459: step 2119, loss 0.530394, acc 0.9375, prec 0.0438042, recall 0.746319\n",
      "2017-11-26T13:00:00.766511: step 2120, loss 0.297327, acc 0.921875, prec 0.0438128, recall 0.74637\n",
      "2017-11-26T13:00:01.357388: step 2121, loss 0.0411203, acc 1, prec 0.0438128, recall 0.74637\n",
      "2017-11-26T13:00:01.944344: step 2122, loss 0.216058, acc 0.90625, prec 0.0438209, recall 0.74642\n",
      "2017-11-26T13:00:02.530859: step 2123, loss 0.305229, acc 0.890625, prec 0.0438174, recall 0.74642\n",
      "2017-11-26T13:00:03.124635: step 2124, loss 0.578561, acc 0.875, prec 0.0438691, recall 0.746672\n",
      "2017-11-26T13:00:03.713639: step 2125, loss 0.126781, acc 0.953125, prec 0.0439122, recall 0.746873\n",
      "2017-11-26T13:00:04.308852: step 2126, loss 1.1181, acc 0.984375, prec 0.0439228, recall 0.746923\n",
      "2017-11-26T13:00:04.975649: step 2127, loss 0.295663, acc 0.953125, prec 0.0439213, recall 0.746923\n",
      "2017-11-26T13:00:05.761539: step 2128, loss 0.447593, acc 0.90625, prec 0.0439405, recall 0.747024\n",
      "2017-11-26T13:00:06.495371: step 2129, loss 0.190715, acc 0.9375, prec 0.0439385, recall 0.747024\n",
      "2017-11-26T13:00:07.310628: step 2130, loss 7.80956, acc 0.921875, prec 0.0439364, recall 0.746876\n",
      "2017-11-26T13:00:08.190607: step 2131, loss 0.162697, acc 0.921875, prec 0.043945, recall 0.746926\n",
      "2017-11-26T13:00:08.992169: step 2132, loss 0.395414, acc 0.859375, prec 0.0439516, recall 0.746976\n",
      "2017-11-26T13:00:09.827173: step 2133, loss 0.395248, acc 0.84375, prec 0.0439464, recall 0.746976\n",
      "2017-11-26T13:00:10.580489: step 2134, loss 0.333104, acc 0.875, prec 0.0439423, recall 0.746976\n",
      "2017-11-26T13:00:11.324881: step 2135, loss 0.537728, acc 0.828125, prec 0.0439367, recall 0.746976\n",
      "2017-11-26T13:00:12.035342: step 2136, loss 0.366019, acc 0.859375, prec 0.0439321, recall 0.746976\n",
      "2017-11-26T13:00:12.750778: step 2137, loss 0.505714, acc 0.875, prec 0.0439391, recall 0.747026\n",
      "2017-11-26T13:00:13.426751: step 2138, loss 0.214597, acc 0.9375, prec 0.0439371, recall 0.747026\n",
      "2017-11-26T13:00:14.088271: step 2139, loss 0.546046, acc 0.859375, prec 0.0439548, recall 0.747126\n",
      "2017-11-26T13:00:14.724596: step 2140, loss 0.534567, acc 0.84375, prec 0.0439496, recall 0.747126\n",
      "2017-11-26T13:00:15.371140: step 2141, loss 5.72832, acc 0.859375, prec 0.0439455, recall 0.746978\n",
      "2017-11-26T13:00:16.021569: step 2142, loss 0.591846, acc 0.796875, prec 0.0439389, recall 0.746978\n",
      "2017-11-26T13:00:16.654198: step 2143, loss 0.754015, acc 0.875, prec 0.0439459, recall 0.747029\n",
      "2017-11-26T13:00:17.294908: step 2144, loss 0.468384, acc 0.875, prec 0.043953, recall 0.747079\n",
      "2017-11-26T13:00:17.905454: step 2145, loss 0.588709, acc 0.796875, prec 0.0439575, recall 0.747129\n",
      "2017-11-26T13:00:18.518213: step 2146, loss 1.69475, acc 0.78125, prec 0.0439508, recall 0.746981\n",
      "2017-11-26T13:00:19.140370: step 2147, loss 0.746057, acc 0.8125, prec 0.0439669, recall 0.747081\n",
      "2017-11-26T13:00:19.759809: step 2148, loss 0.574086, acc 0.8125, prec 0.0439608, recall 0.747081\n",
      "2017-11-26T13:00:20.369359: step 2149, loss 0.52716, acc 0.84375, prec 0.0439668, recall 0.747131\n",
      "2017-11-26T13:00:20.984371: step 2150, loss 0.652488, acc 0.796875, prec 0.0439713, recall 0.747181\n",
      "2017-11-26T13:00:21.597890: step 2151, loss 0.956965, acc 0.765625, prec 0.0439636, recall 0.747181\n",
      "2017-11-26T13:00:22.204300: step 2152, loss 0.399294, acc 0.8125, prec 0.0439574, recall 0.747181\n",
      "2017-11-26T13:00:22.803199: step 2153, loss 0.393924, acc 0.890625, prec 0.043965, recall 0.747231\n",
      "2017-11-26T13:00:23.413782: step 2154, loss 0.408687, acc 0.859375, prec 0.0439715, recall 0.747281\n",
      "2017-11-26T13:00:24.024078: step 2155, loss 0.712063, acc 0.71875, prec 0.0439734, recall 0.747331\n",
      "2017-11-26T13:00:24.634198: step 2156, loss 0.649835, acc 0.78125, prec 0.0439663, recall 0.747331\n",
      "2017-11-26T13:00:25.244694: step 2157, loss 0.402517, acc 0.84375, prec 0.0439723, recall 0.747381\n",
      "2017-11-26T13:00:25.874090: step 2158, loss 0.242368, acc 0.890625, prec 0.0439687, recall 0.747381\n",
      "2017-11-26T13:00:26.481324: step 2159, loss 0.446833, acc 0.90625, prec 0.0439767, recall 0.747431\n",
      "2017-11-26T13:00:27.091665: step 2160, loss 0.194695, acc 0.921875, prec 0.0439853, recall 0.747481\n",
      "2017-11-26T13:00:27.698473: step 2161, loss 4.55858, acc 0.9375, prec 0.0439949, recall 0.747383\n",
      "2017-11-26T13:00:28.314483: step 2162, loss 0.358664, acc 0.875, prec 0.0439908, recall 0.747383\n",
      "2017-11-26T13:00:28.925249: step 2163, loss 5.44919, acc 0.875, prec 0.0439983, recall 0.747285\n",
      "2017-11-26T13:00:29.542100: step 2164, loss 0.429324, acc 0.875, prec 0.0439942, recall 0.747285\n",
      "2017-11-26T13:00:30.146856: step 2165, loss 0.950305, acc 0.734375, prec 0.0439967, recall 0.747335\n",
      "2017-11-26T13:00:30.755515: step 2166, loss 0.459628, acc 0.796875, prec 0.04399, recall 0.747335\n",
      "2017-11-26T13:00:31.371296: step 2167, loss 0.537838, acc 0.796875, prec 0.0439834, recall 0.747335\n",
      "2017-11-26T13:00:31.957090: step 2168, loss 0.729044, acc 0.84375, prec 0.0439783, recall 0.747335\n",
      "2017-11-26T13:00:32.544641: step 2169, loss 0.685292, acc 0.84375, prec 0.0439954, recall 0.747435\n",
      "2017-11-26T13:00:33.130867: step 2170, loss 1.13258, acc 0.796875, prec 0.0439998, recall 0.747485\n",
      "2017-11-26T13:00:33.715692: step 2171, loss 0.590007, acc 0.859375, prec 0.0439952, recall 0.747485\n",
      "2017-11-26T13:00:34.299347: step 2172, loss 0.657899, acc 0.8125, prec 0.0440002, recall 0.747535\n",
      "2017-11-26T13:00:34.881432: step 2173, loss 0.753272, acc 0.796875, prec 0.0440046, recall 0.747584\n",
      "2017-11-26T13:00:35.479515: step 2174, loss 0.648068, acc 0.84375, prec 0.0439995, recall 0.747584\n",
      "2017-11-26T13:00:36.090615: step 2175, loss 1.04654, acc 0.734375, prec 0.0439909, recall 0.747584\n",
      "2017-11-26T13:00:36.685578: step 2176, loss 0.414202, acc 0.90625, prec 0.0439989, recall 0.747634\n",
      "2017-11-26T13:00:37.267526: step 2177, loss 0.917994, acc 0.75, prec 0.0440018, recall 0.747684\n",
      "2017-11-26T13:00:37.859000: step 2178, loss 0.506591, acc 0.859375, prec 0.0439972, recall 0.747684\n",
      "2017-11-26T13:00:38.443039: step 2179, loss 0.979073, acc 0.75, prec 0.0440001, recall 0.747734\n",
      "2017-11-26T13:00:39.023171: step 2180, loss 0.599055, acc 0.875, prec 0.0440515, recall 0.747982\n",
      "2017-11-26T13:00:39.618382: step 2181, loss 0.559474, acc 0.875, prec 0.0440696, recall 0.748081\n",
      "2017-11-26T13:00:40.205205: step 2182, loss 0.458972, acc 0.875, prec 0.0440766, recall 0.748131\n",
      "2017-11-26T13:00:40.788475: step 2183, loss 1.76534, acc 0.890625, prec 0.0440951, recall 0.74823\n",
      "2017-11-26T13:00:41.374599: step 2184, loss 0.316081, acc 0.890625, prec 0.0441026, recall 0.748279\n",
      "2017-11-26T13:00:41.957258: step 2185, loss 0.455193, acc 0.875, prec 0.0440986, recall 0.748279\n",
      "2017-11-26T13:00:42.586946: step 2186, loss 0.537409, acc 0.84375, prec 0.0440934, recall 0.748279\n",
      "2017-11-26T13:00:43.221483: step 2187, loss 0.479951, acc 0.828125, prec 0.04411, recall 0.748378\n",
      "2017-11-26T13:00:43.835651: step 2188, loss 0.422708, acc 0.890625, prec 0.0441175, recall 0.748428\n",
      "2017-11-26T13:00:44.573762: step 2189, loss 0.43164, acc 0.84375, prec 0.0441345, recall 0.748527\n",
      "2017-11-26T13:00:45.267614: step 2190, loss 0.598785, acc 0.84375, prec 0.0441405, recall 0.748576\n",
      "2017-11-26T13:00:46.011933: step 2191, loss 0.639628, acc 0.875, prec 0.0441364, recall 0.748576\n",
      "2017-11-26T13:00:46.727590: step 2192, loss 0.321369, acc 0.890625, prec 0.0441328, recall 0.748576\n",
      "2017-11-26T13:00:47.466595: step 2193, loss 0.3857, acc 0.890625, prec 0.0441292, recall 0.748576\n",
      "2017-11-26T13:00:48.115001: step 2194, loss 0.0812365, acc 0.96875, prec 0.0441393, recall 0.748625\n",
      "2017-11-26T13:00:48.753434: step 2195, loss 0.145615, acc 0.953125, prec 0.0441377, recall 0.748625\n",
      "2017-11-26T13:00:49.381844: step 2196, loss 0.212687, acc 0.96875, prec 0.0441588, recall 0.748724\n",
      "2017-11-26T13:00:50.037474: step 2197, loss 0.393967, acc 0.90625, prec 0.0441779, recall 0.748823\n",
      "2017-11-26T13:00:50.714238: step 2198, loss 0.127352, acc 0.953125, prec 0.0441764, recall 0.748823\n",
      "2017-11-26T13:00:51.355319: step 2199, loss 0.0837991, acc 0.984375, prec 0.044198, recall 0.748921\n",
      "2017-11-26T13:00:52.045850: step 2200, loss 0.10833, acc 0.953125, prec 0.0441965, recall 0.748921\n",
      "\n",
      "Evaluation:\n",
      "2017-11-26T13:01:35.316503: step 2200, loss 3.00882, acc 0.953859, prec 0.0445872, recall 0.73879\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1511716623/checkpoints/model-2200\n",
      "\n",
      "2017-11-26T13:01:37.290604: step 2201, loss 0.325772, acc 0.984375, prec 0.0445977, recall 0.73884\n",
      "2017-11-26T13:01:37.932091: step 2202, loss 0.115086, acc 0.953125, prec 0.0445962, recall 0.73884\n",
      "2017-11-26T13:01:38.730101: step 2203, loss 0.845201, acc 0.9375, prec 0.0446161, recall 0.73894\n",
      "2017-11-26T13:01:39.444343: step 2204, loss 0.20991, acc 0.9375, prec 0.0446141, recall 0.73894\n",
      "2017-11-26T13:01:40.144335: step 2205, loss 0.197638, acc 0.953125, prec 0.0446125, recall 0.73894\n",
      "2017-11-26T13:01:40.863683: step 2206, loss 0.267269, acc 0.90625, prec 0.0446094, recall 0.73894\n",
      "2017-11-26T13:01:41.671331: step 2207, loss 0.286307, acc 0.90625, prec 0.0446064, recall 0.73894\n",
      "2017-11-26T13:01:42.355667: step 2208, loss 0.342011, acc 0.921875, prec 0.0446148, recall 0.73899\n",
      "2017-11-26T13:01:43.087242: step 2209, loss 0.911023, acc 0.9375, prec 0.0446347, recall 0.739089\n",
      "2017-11-26T13:01:43.814525: step 2210, loss 3.34698, acc 0.953125, prec 0.0446557, recall 0.739048\n",
      "2017-11-26T13:01:44.513025: step 2211, loss 2.16292, acc 0.890625, prec 0.0446856, recall 0.739056\n",
      "2017-11-26T13:01:45.162155: step 2212, loss 0.306312, acc 0.90625, prec 0.0446935, recall 0.739106\n",
      "2017-11-26T13:01:45.801450: step 2213, loss 0.105698, acc 0.953125, prec 0.044692, recall 0.739106\n",
      "2017-11-26T13:01:46.505843: step 2214, loss 0.400217, acc 0.859375, prec 0.0446983, recall 0.739155\n",
      "2017-11-26T13:01:47.205753: step 2215, loss 0.458567, acc 0.921875, prec 0.0447067, recall 0.739205\n",
      "2017-11-26T13:01:47.920755: step 2216, loss 0.413433, acc 0.8125, prec 0.0447006, recall 0.739205\n",
      "2017-11-26T13:01:48.544048: step 2217, loss 2.28446, acc 0.796875, prec 0.0446944, recall 0.739064\n",
      "2017-11-26T13:01:49.156160: step 2218, loss 0.431341, acc 0.859375, prec 0.0446898, recall 0.739064\n",
      "2017-11-26T13:01:49.770542: step 2219, loss 0.890925, acc 0.796875, prec 0.0446831, recall 0.739064\n",
      "2017-11-26T13:01:50.381684: step 2220, loss 0.665587, acc 0.84375, prec 0.0446889, recall 0.739114\n",
      "2017-11-26T13:01:50.994684: step 2221, loss 0.7578, acc 0.796875, prec 0.0446933, recall 0.739164\n",
      "2017-11-26T13:01:51.585707: step 2222, loss 2.08323, acc 0.703125, prec 0.0447055, recall 0.739263\n",
      "2017-11-26T13:01:52.172165: step 2223, loss 1.37785, acc 0.71875, prec 0.0447072, recall 0.739312\n",
      "2017-11-26T13:01:52.759066: step 2224, loss 1.39782, acc 0.625, prec 0.0446949, recall 0.739312\n",
      "2017-11-26T13:01:53.346824: step 2225, loss 0.920839, acc 0.796875, prec 0.0446882, recall 0.739312\n",
      "2017-11-26T13:01:53.930410: step 2226, loss 1.27754, acc 0.6875, prec 0.0446779, recall 0.739312\n",
      "2017-11-26T13:01:54.684622: step 2227, loss 0.46982, acc 0.796875, prec 0.0446713, recall 0.739312\n",
      "2017-11-26T13:01:55.360790: step 2228, loss 0.640782, acc 0.765625, prec 0.0446636, recall 0.739312\n",
      "2017-11-26T13:01:56.037196: step 2229, loss 0.940195, acc 0.703125, prec 0.0446538, recall 0.739312\n",
      "2017-11-26T13:01:56.794139: step 2230, loss 1.0648, acc 0.6875, prec 0.0446436, recall 0.739312\n",
      "2017-11-26T13:01:57.731231: step 2231, loss 0.771489, acc 0.78125, prec 0.0446364, recall 0.739312\n",
      "2017-11-26T13:01:58.652364: step 2232, loss 0.373224, acc 0.875, prec 0.0446433, recall 0.739362\n",
      "2017-11-26T13:01:59.495106: step 2233, loss 0.447201, acc 0.796875, prec 0.0446476, recall 0.739411\n",
      "2017-11-26T13:02:00.341393: step 2234, loss 0.511198, acc 0.875, prec 0.0446435, recall 0.739411\n",
      "2017-11-26T13:02:01.224427: step 2235, loss 0.551859, acc 0.8125, prec 0.0446373, recall 0.739411\n",
      "2017-11-26T13:02:02.034715: step 2236, loss 0.585223, acc 0.875, prec 0.0446332, recall 0.739411\n",
      "2017-11-26T13:02:02.795059: step 2237, loss 1.49192, acc 0.84375, prec 0.04465, recall 0.73951\n",
      "2017-11-26T13:02:03.560942: step 2238, loss 0.424656, acc 0.90625, prec 0.0446579, recall 0.73956\n",
      "2017-11-26T13:02:04.357989: step 2239, loss 0.326052, acc 0.90625, prec 0.0446658, recall 0.739609\n",
      "2017-11-26T13:02:05.106490: step 2240, loss 0.239306, acc 0.9375, prec 0.0446637, recall 0.739609\n",
      "2017-11-26T13:02:05.816241: step 2241, loss 0.14211, acc 0.953125, prec 0.0446622, recall 0.739609\n",
      "2017-11-26T13:02:06.767587: step 2242, loss 0.215624, acc 0.921875, prec 0.0446596, recall 0.739609\n",
      "2017-11-26T13:02:07.618410: step 2243, loss 0.203794, acc 0.90625, prec 0.0446566, recall 0.739609\n",
      "2017-11-26T13:02:08.350078: step 2244, loss 0.504117, acc 0.890625, prec 0.0446639, recall 0.739658\n",
      "2017-11-26T13:02:09.111111: step 2245, loss 0.239028, acc 0.90625, prec 0.0446718, recall 0.739708\n",
      "2017-11-26T13:02:09.825748: step 2246, loss 0.225424, acc 0.890625, prec 0.0446682, recall 0.739708\n",
      "2017-11-26T13:02:10.512833: step 2247, loss 0.190587, acc 0.921875, prec 0.0446876, recall 0.739807\n",
      "2017-11-26T13:02:11.194397: step 2248, loss 0.100047, acc 0.953125, prec 0.044686, recall 0.739807\n",
      "2017-11-26T13:02:11.904593: step 2249, loss 0.284185, acc 0.921875, prec 0.0446835, recall 0.739807\n",
      "2017-11-26T13:02:12.567397: step 2250, loss 0.101159, acc 0.96875, prec 0.0447043, recall 0.739905\n",
      "2017-11-26T13:02:13.206294: step 2251, loss 0.162305, acc 0.953125, prec 0.0447028, recall 0.739905\n",
      "2017-11-26T13:02:13.873893: step 2252, loss 10.222, acc 0.921875, prec 0.0447122, recall 0.739674\n",
      "2017-11-26T13:02:14.513528: step 2253, loss 0.405012, acc 0.9375, prec 0.0447211, recall 0.739723\n",
      "2017-11-26T13:02:15.146296: step 2254, loss 0.286798, acc 0.90625, prec 0.044718, recall 0.739723\n",
      "2017-11-26T13:02:15.888376: step 2255, loss 0.0936725, acc 0.984375, prec 0.0447175, recall 0.739723\n",
      "2017-11-26T13:02:16.663470: step 2256, loss 0.365537, acc 0.90625, prec 0.0447144, recall 0.739723\n",
      "2017-11-26T13:02:17.363744: step 2257, loss 1.11218, acc 0.96875, prec 0.0447572, recall 0.73992\n",
      "2017-11-26T13:02:18.021576: step 2258, loss 0.365841, acc 0.921875, prec 0.0447874, recall 0.740068\n",
      "2017-11-26T13:02:18.689484: step 2259, loss 0.404138, acc 0.921875, prec 0.0448067, recall 0.740166\n",
      "2017-11-26T13:02:19.477423: step 2260, loss 0.3021, acc 0.875, prec 0.0448026, recall 0.740166\n",
      "2017-11-26T13:02:20.343789: step 2261, loss 0.535902, acc 0.765625, prec 0.0447949, recall 0.740166\n",
      "2017-11-26T13:02:21.155675: step 2262, loss 0.750191, acc 0.796875, prec 0.0447882, recall 0.740166\n",
      "2017-11-26T13:02:22.148209: step 2263, loss 0.717857, acc 0.796875, prec 0.0447925, recall 0.740216\n",
      "2017-11-26T13:02:23.246461: step 2264, loss 0.351765, acc 0.875, prec 0.0447993, recall 0.740265\n",
      "2017-11-26T13:02:24.335800: step 2265, loss 0.494233, acc 0.8125, prec 0.044815, recall 0.740363\n",
      "2017-11-26T13:02:25.748346: step 2266, loss 0.536344, acc 0.828125, prec 0.0448094, recall 0.740363\n",
      "2017-11-26T13:02:27.211476: step 2267, loss 0.713699, acc 0.828125, prec 0.0448256, recall 0.740461\n",
      "2017-11-26T13:02:28.850108: step 2268, loss 1.35772, acc 0.71875, prec 0.0448273, recall 0.74051\n",
      "2017-11-26T13:02:31.349522: step 2269, loss 3.01004, acc 0.8125, prec 0.0448326, recall 0.740419\n",
      "2017-11-26T13:02:33.182979: step 2270, loss 0.587855, acc 0.828125, prec 0.044827, recall 0.740419\n",
      "2017-11-26T13:02:34.314141: step 2271, loss 0.370167, acc 0.890625, prec 0.0448234, recall 0.740419\n",
      "2017-11-26T13:02:35.517060: step 2272, loss 0.677403, acc 0.859375, prec 0.0448297, recall 0.740468\n",
      "2017-11-26T13:02:36.722564: step 2273, loss 0.623288, acc 0.84375, prec 0.0448355, recall 0.740517\n",
      "2017-11-26T13:02:37.755007: step 2274, loss 0.393039, acc 0.859375, prec 0.0448418, recall 0.740566\n",
      "2017-11-26T13:02:38.774124: step 2275, loss 1.12993, acc 0.703125, prec 0.044832, recall 0.740566\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-638e9eb86f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-638e9eb86f69>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m             _, step, summaries, loss, accuracy, precision, recall = sess.run(\n\u001b[1;32m     78\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "        precision_summary = tf.summary.scalar(\"precision\", cnn.precision)\n",
    "        recall_summary = tf.summary.scalar(\"recall\", cnn.recall)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy, precision, recall = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.precision, cnn.recall],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy,  precision, recall  = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.precision, cnn.recall],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "              cnn.input_x: x_dev,\n",
    "              cnn.input_y: y_dev,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "predictions = sess.run([cnn.predictions], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'pred': np.array(predictions[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['x'] = pd.Series(np.array(list(vocab_processor.reverse(x_dev))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['actual'] = pd.Series(np.argmax(y_dev, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['x', 'pred', 'actual']]\n",
    "df.to_csv(os.path.join(out_dir, 'predictions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'input_x' type=Placeholder>,\n",
       " <tf.Operation 'input_y' type=Placeholder>,\n",
       " <tf.Operation 'dropout_keep_prob' type=Placeholder>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'embedding/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'embedding/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'embedding/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'embedding/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'embedding/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'embedding/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'embedding/random_uniform' type=Add>,\n",
       " <tf.Operation 'embedding/W' type=VariableV2>,\n",
       " <tf.Operation 'embedding/W/Assign' type=Assign>,\n",
       " <tf.Operation 'embedding/W/read' type=Identity>,\n",
       " <tf.Operation 'embedding/embedding_lookup' type=Gather>,\n",
       " <tf.Operation 'embedding/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'embedding/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'conv-maxpool-3/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'conv-maxpool-3/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'conv-maxpool-3/truncated_normal' type=Add>,\n",
       " <tf.Operation 'conv-maxpool-3/W' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-3/W/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-3/Const' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/b' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-3/b/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-3/conv' type=Conv2D>,\n",
       " <tf.Operation 'conv-maxpool-3/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'conv-maxpool-3/relu' type=Relu>,\n",
       " <tf.Operation 'conv-maxpool-3/pool' type=MaxPool>,\n",
       " <tf.Operation 'conv-maxpool-4/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'conv-maxpool-4/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'conv-maxpool-4/truncated_normal' type=Add>,\n",
       " <tf.Operation 'conv-maxpool-4/W' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-4/W/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-4/Const' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/b' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-4/b/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-4/conv' type=Conv2D>,\n",
       " <tf.Operation 'conv-maxpool-4/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'conv-maxpool-4/relu' type=Relu>,\n",
       " <tf.Operation 'conv-maxpool-4/pool' type=MaxPool>,\n",
       " <tf.Operation 'conv-maxpool-5/truncated_normal/shape' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/truncated_normal/mean' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/truncated_normal/stddev' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/truncated_normal/TruncatedNormal' type=TruncatedNormal>,\n",
       " <tf.Operation 'conv-maxpool-5/truncated_normal/mul' type=Mul>,\n",
       " <tf.Operation 'conv-maxpool-5/truncated_normal' type=Add>,\n",
       " <tf.Operation 'conv-maxpool-5/W' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-5/W/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-5/Const' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/b' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-5/b/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-5/conv' type=Conv2D>,\n",
       " <tf.Operation 'conv-maxpool-5/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'conv-maxpool-5/relu' type=Relu>,\n",
       " <tf.Operation 'conv-maxpool-5/pool' type=MaxPool>,\n",
       " <tf.Operation 'concat/axis' type=Const>,\n",
       " <tf.Operation 'concat' type=ConcatV2>,\n",
       " <tf.Operation 'Reshape/shape' type=Const>,\n",
       " <tf.Operation 'Reshape' type=Reshape>,\n",
       " <tf.Operation 'dropout/dropout/Shape' type=Shape>,\n",
       " <tf.Operation 'dropout/dropout/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'dropout/dropout/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'dropout/dropout/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'dropout/dropout/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'dropout/dropout/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'dropout/dropout/random_uniform' type=Add>,\n",
       " <tf.Operation 'dropout/dropout/add' type=Add>,\n",
       " <tf.Operation 'dropout/dropout/Floor' type=Floor>,\n",
       " <tf.Operation 'dropout/dropout/div' type=RealDiv>,\n",
       " <tf.Operation 'dropout/dropout/mul' type=Mul>,\n",
       " <tf.Operation 'W/Initializer/random_uniform/shape' type=Const>,\n",
       " <tf.Operation 'W/Initializer/random_uniform/min' type=Const>,\n",
       " <tf.Operation 'W/Initializer/random_uniform/max' type=Const>,\n",
       " <tf.Operation 'W/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\n",
       " <tf.Operation 'W/Initializer/random_uniform/sub' type=Sub>,\n",
       " <tf.Operation 'W/Initializer/random_uniform/mul' type=Mul>,\n",
       " <tf.Operation 'W/Initializer/random_uniform' type=Add>,\n",
       " <tf.Operation 'W' type=VariableV2>,\n",
       " <tf.Operation 'W/Assign' type=Assign>,\n",
       " <tf.Operation 'W/read' type=Identity>,\n",
       " <tf.Operation 'output/Const' type=Const>,\n",
       " <tf.Operation 'output/b' type=VariableV2>,\n",
       " <tf.Operation 'output/b/Assign' type=Assign>,\n",
       " <tf.Operation 'output/b/read' type=Identity>,\n",
       " <tf.Operation 'output/L2Loss' type=L2Loss>,\n",
       " <tf.Operation 'output/add' type=Add>,\n",
       " <tf.Operation 'output/L2Loss_1' type=L2Loss>,\n",
       " <tf.Operation 'output/add_1' type=Add>,\n",
       " <tf.Operation 'output/scores/MatMul' type=MatMul>,\n",
       " <tf.Operation 'output/scores' type=BiasAdd>,\n",
       " <tf.Operation 'output/predictions/dimension' type=Const>,\n",
       " <tf.Operation 'output/predictions' type=ArgMax>,\n",
       " <tf.Operation 'loss/Const' type=Const>,\n",
       " <tf.Operation 'loss/mul' type=Mul>,\n",
       " <tf.Operation 'loss/Sum/reduction_indices' type=Const>,\n",
       " <tf.Operation 'loss/Sum' type=Sum>,\n",
       " <tf.Operation 'loss/Rank' type=Const>,\n",
       " <tf.Operation 'loss/Shape' type=Shape>,\n",
       " <tf.Operation 'loss/Rank_1' type=Const>,\n",
       " <tf.Operation 'loss/Shape_1' type=Shape>,\n",
       " <tf.Operation 'loss/Sub/y' type=Const>,\n",
       " <tf.Operation 'loss/Sub' type=Sub>,\n",
       " <tf.Operation 'loss/Slice/begin' type=Pack>,\n",
       " <tf.Operation 'loss/Slice/size' type=Const>,\n",
       " <tf.Operation 'loss/Slice' type=Slice>,\n",
       " <tf.Operation 'loss/concat/values_0' type=Const>,\n",
       " <tf.Operation 'loss/concat/axis' type=Const>,\n",
       " <tf.Operation 'loss/concat' type=ConcatV2>,\n",
       " <tf.Operation 'loss/Reshape' type=Reshape>,\n",
       " <tf.Operation 'loss/Rank_2' type=Const>,\n",
       " <tf.Operation 'loss/Shape_2' type=Shape>,\n",
       " <tf.Operation 'loss/Sub_1/y' type=Const>,\n",
       " <tf.Operation 'loss/Sub_1' type=Sub>,\n",
       " <tf.Operation 'loss/Slice_1/begin' type=Pack>,\n",
       " <tf.Operation 'loss/Slice_1/size' type=Const>,\n",
       " <tf.Operation 'loss/Slice_1' type=Slice>,\n",
       " <tf.Operation 'loss/concat_1/values_0' type=Const>,\n",
       " <tf.Operation 'loss/concat_1/axis' type=Const>,\n",
       " <tf.Operation 'loss/concat_1' type=ConcatV2>,\n",
       " <tf.Operation 'loss/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'loss/SoftmaxCrossEntropyWithLogits' type=SoftmaxCrossEntropyWithLogits>,\n",
       " <tf.Operation 'loss/Sub_2/y' type=Const>,\n",
       " <tf.Operation 'loss/Sub_2' type=Sub>,\n",
       " <tf.Operation 'loss/Slice_2/begin' type=Const>,\n",
       " <tf.Operation 'loss/Slice_2/size' type=Pack>,\n",
       " <tf.Operation 'loss/Slice_2' type=Slice>,\n",
       " <tf.Operation 'loss/Reshape_2' type=Reshape>,\n",
       " <tf.Operation 'loss/mul_1' type=Mul>,\n",
       " <tf.Operation 'loss/Const_1' type=Const>,\n",
       " <tf.Operation 'loss/Mean' type=Mean>,\n",
       " <tf.Operation 'loss/mul_2/x' type=Const>,\n",
       " <tf.Operation 'loss/mul_2' type=Mul>,\n",
       " <tf.Operation 'loss/add' type=Add>,\n",
       " <tf.Operation 'accuracy/ArgMax/dimension' type=Const>,\n",
       " <tf.Operation 'accuracy/ArgMax' type=ArgMax>,\n",
       " <tf.Operation 'accuracy/Equal' type=Equal>,\n",
       " <tf.Operation 'accuracy/Cast' type=Cast>,\n",
       " <tf.Operation 'accuracy/Const' type=Const>,\n",
       " <tf.Operation 'accuracy/accuracy' type=Mean>,\n",
       " <tf.Operation 'accuracy/ArgMax_1/dimension' type=Const>,\n",
       " <tf.Operation 'accuracy/ArgMax_1' type=ArgMax>,\n",
       " <tf.Operation 'accuracy/precision/Cast' type=Cast>,\n",
       " <tf.Operation 'accuracy/precision/Cast_1' type=Cast>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/Equal/y' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/Equal' type=Equal>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/Equal_1/y' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/Equal_1' type=Equal>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/LogicalAnd' type=LogicalAnd>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/assert_type/statically_determined_correct_type' type=NoOp>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/count/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/count' type=VariableV2>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/count/Assign' type=Assign>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/count/read' type=Identity>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/ToFloat' type=Cast>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/Identity' type=Identity>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/Const' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/Sum' type=Sum>,\n",
       " <tf.Operation 'accuracy/precision/true_positives/AssignAdd' type=AssignAdd>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/Equal/y' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/Equal' type=Equal>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/Equal_1/y' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/Equal_1' type=Equal>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/LogicalAnd' type=LogicalAnd>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/assert_type/statically_determined_correct_type' type=NoOp>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/count/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/count' type=VariableV2>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/count/Assign' type=Assign>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/count/read' type=Identity>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/ToFloat' type=Cast>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/Identity' type=Identity>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/Const' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/Sum' type=Sum>,\n",
       " <tf.Operation 'accuracy/precision/false_positives/AssignAdd' type=AssignAdd>,\n",
       " <tf.Operation 'accuracy/precision/add' type=Add>,\n",
       " <tf.Operation 'accuracy/precision/Greater/y' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/Greater' type=Greater>,\n",
       " <tf.Operation 'accuracy/precision/add_1' type=Add>,\n",
       " <tf.Operation 'accuracy/precision/div' type=RealDiv>,\n",
       " <tf.Operation 'accuracy/precision/value/e' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/value' type=Select>,\n",
       " <tf.Operation 'accuracy/precision/add_2' type=Add>,\n",
       " <tf.Operation 'accuracy/precision/Greater_1/y' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/Greater_1' type=Greater>,\n",
       " <tf.Operation 'accuracy/precision/add_3' type=Add>,\n",
       " <tf.Operation 'accuracy/precision/div_1' type=RealDiv>,\n",
       " <tf.Operation 'accuracy/precision/update_op/e' type=Const>,\n",
       " <tf.Operation 'accuracy/precision/update_op' type=Select>,\n",
       " <tf.Operation 'accuracy/ArgMax_2/dimension' type=Const>,\n",
       " <tf.Operation 'accuracy/ArgMax_2' type=ArgMax>,\n",
       " <tf.Operation 'accuracy/recall/Cast' type=Cast>,\n",
       " <tf.Operation 'accuracy/recall/Cast_1' type=Cast>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/Equal/y' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/Equal' type=Equal>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/Equal_1/y' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/Equal_1' type=Equal>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/LogicalAnd' type=LogicalAnd>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/assert_type/statically_determined_correct_type' type=NoOp>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/count/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/count' type=VariableV2>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/count/Assign' type=Assign>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/count/read' type=Identity>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/ToFloat' type=Cast>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/Identity' type=Identity>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/Const' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/Sum' type=Sum>,\n",
       " <tf.Operation 'accuracy/recall/true_positives/AssignAdd' type=AssignAdd>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/Equal/y' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/Equal' type=Equal>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/Equal_1/y' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/Equal_1' type=Equal>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/LogicalAnd' type=LogicalAnd>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/assert_type/statically_determined_correct_type' type=NoOp>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/count/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/count' type=VariableV2>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/count/Assign' type=Assign>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/count/read' type=Identity>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/ToFloat' type=Cast>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/Identity' type=Identity>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/Const' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/Sum' type=Sum>,\n",
       " <tf.Operation 'accuracy/recall/false_negatives/AssignAdd' type=AssignAdd>,\n",
       " <tf.Operation 'accuracy/recall/add' type=Add>,\n",
       " <tf.Operation 'accuracy/recall/Greater/y' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/Greater' type=Greater>,\n",
       " <tf.Operation 'accuracy/recall/add_1' type=Add>,\n",
       " <tf.Operation 'accuracy/recall/div' type=RealDiv>,\n",
       " <tf.Operation 'accuracy/recall/value/e' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/value' type=Select>,\n",
       " <tf.Operation 'accuracy/recall/add_2' type=Add>,\n",
       " <tf.Operation 'accuracy/recall/Greater_1/y' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/Greater_1' type=Greater>,\n",
       " <tf.Operation 'accuracy/recall/add_3' type=Add>,\n",
       " <tf.Operation 'accuracy/recall/div_1' type=RealDiv>,\n",
       " <tf.Operation 'accuracy/recall/update_op/e' type=Const>,\n",
       " <tf.Operation 'accuracy/recall/update_op' type=Select>,\n",
       " <tf.Operation 'global_step/initial_value' type=Const>,\n",
       " <tf.Operation 'global_step' type=VariableV2>,\n",
       " <tf.Operation 'global_step/Assign' type=Assign>,\n",
       " <tf.Operation 'global_step/read' type=Identity>,\n",
       " <tf.Operation 'gradients/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/Const' type=Const>,\n",
       " <tf.Operation 'gradients/Fill' type=Fill>,\n",
       " <tf.Operation 'gradients/loss/add_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/loss/add_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/loss/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/loss/add_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/loss/add_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss/add_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/loss/add_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss/add_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/loss/add_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/loss/add_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Reshape/shape' type=Const>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Tile' type=Tile>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Shape_2' type=Const>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Const' type=Const>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Prod' type=Prod>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Const_1' type=Const>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Prod_1' type=Prod>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Maximum/y' type=Const>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Maximum' type=Maximum>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/Cast' type=Cast>,\n",
       " <tf.Operation 'gradients/loss/Mean_grad/truediv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/loss/mul_2_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/loss/mul_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/output/add_1_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/loss/Reshape_2_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/loss/Reshape_2_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/output/add_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/output/add_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/output/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/output/add_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/output/add_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/output/add_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/output/add_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/output/add_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/output/add_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/output/add_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/output/L2Loss_1_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/zeros_like' type=ZerosLike>,\n",
       " <tf.Operation 'gradients/loss/SoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/loss/SoftmaxCrossEntropyWithLogits_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/loss/SoftmaxCrossEntropyWithLogits_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/output/L2Loss_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/loss/Reshape_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/loss/Reshape_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/output/scores_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/output/scores_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/output/scores_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/output/scores_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/output/scores/MatMul_grad/MatMul' type=MatMul>,\n",
       " <tf.Operation 'gradients/output/scores/MatMul_grad/MatMul_1' type=MatMul>,\n",
       " <tf.Operation 'gradients/output/scores/MatMul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/output/scores/MatMul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/output/scores/MatMul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/AddN' type=AddN>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/mul_1' type=Mul>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dropout/dropout/mul_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/AddN_1' type=AddN>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/Shape_1' type=Shape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/RealDiv' type=RealDiv>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/Sum' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/Neg' type=Neg>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/RealDiv_1' type=RealDiv>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/RealDiv_2' type=RealDiv>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/mul' type=Mul>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/Sum_1' type=Sum>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/dropout/dropout/div_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/Reshape_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/Reshape_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/concat_grad/Rank' type=Const>,\n",
       " <tf.Operation 'gradients/concat_grad/mod' type=FloorMod>,\n",
       " <tf.Operation 'gradients/concat_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/concat_grad/ShapeN' type=ShapeN>,\n",
       " <tf.Operation 'gradients/concat_grad/ConcatOffset' type=ConcatOffset>,\n",
       " <tf.Operation 'gradients/concat_grad/Slice' type=Slice>,\n",
       " <tf.Operation 'gradients/concat_grad/Slice_1' type=Slice>,\n",
       " <tf.Operation 'gradients/concat_grad/Slice_2' type=Slice>,\n",
       " <tf.Operation 'gradients/concat_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/concat_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/concat_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/concat_grad/tuple/control_dependency_2' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/pool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/pool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/pool_grad/MaxPoolGrad' type=MaxPoolGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/relu_grad/ReluGrad' type=ReluGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/BiasAdd_grad/BiasAddGrad' type=BiasAddGrad>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/BiasAdd_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/BiasAdd_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/BiasAdd_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/conv_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/conv_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/conv_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/conv_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/conv_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/conv_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-3/conv_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/conv_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/conv_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/conv_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/conv_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/conv_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/conv_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-4/conv_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/conv_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/conv_grad/Conv2DBackpropInput' type=Conv2DBackpropInput>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/conv_grad/Shape_1' type=Const>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/conv_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/conv_grad/tuple/group_deps' type=NoOp>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/conv_grad/tuple/control_dependency' type=Identity>,\n",
       " <tf.Operation 'gradients/conv-maxpool-5/conv_grad/tuple/control_dependency_1' type=Identity>,\n",
       " <tf.Operation 'gradients/AddN_2' type=AddN>,\n",
       " <tf.Operation 'gradients/embedding/ExpandDims_grad/Shape' type=Shape>,\n",
       " <tf.Operation 'gradients/embedding/ExpandDims_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/Shape' type=Const>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/ToInt32' type=Cast>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/Size' type=Size>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/ExpandDims/dim' type=Const>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/ExpandDims' type=ExpandDims>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/strided_slice/stack' type=Const>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/strided_slice/stack_1' type=Const>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/strided_slice/stack_2' type=Const>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/strided_slice' type=StridedSlice>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/concat/axis' type=Const>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/concat' type=ConcatV2>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/Reshape' type=Reshape>,\n",
       " <tf.Operation 'gradients/embedding/embedding_lookup_grad/Reshape_1' type=Reshape>,\n",
       " <tf.Operation 'beta1_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta1_power' type=VariableV2>,\n",
       " <tf.Operation 'beta1_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta1_power/read' type=Identity>,\n",
       " <tf.Operation 'beta2_power/initial_value' type=Const>,\n",
       " <tf.Operation 'beta2_power' type=VariableV2>,\n",
       " <tf.Operation 'beta2_power/Assign' type=Assign>,\n",
       " <tf.Operation 'beta2_power/read' type=Identity>,\n",
       " <tf.Operation 'embedding/W/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'embedding/W/Adam' type=VariableV2>,\n",
       " <tf.Operation 'embedding/W/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'embedding/W/Adam/read' type=Identity>,\n",
       " <tf.Operation 'embedding/W/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'embedding/W/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'embedding/W/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'embedding/W/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-3/W/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-3/b/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-4/W/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-4/b/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-5/W/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam/read' type=Identity>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'conv-maxpool-5/b/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'W/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'W/Adam' type=VariableV2>,\n",
       " <tf.Operation 'W/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'W/Adam/read' type=Identity>,\n",
       " <tf.Operation 'W/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'W/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'W/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'W/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'output/b/Adam/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'output/b/Adam' type=VariableV2>,\n",
       " <tf.Operation 'output/b/Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'output/b/Adam/read' type=Identity>,\n",
       " <tf.Operation 'output/b/Adam_1/Initializer/zeros' type=Const>,\n",
       " <tf.Operation 'output/b/Adam_1' type=VariableV2>,\n",
       " <tf.Operation 'output/b/Adam_1/Assign' type=Assign>,\n",
       " <tf.Operation 'output/b/Adam_1/read' type=Identity>,\n",
       " <tf.Operation 'Adam/learning_rate' type=Const>,\n",
       " <tf.Operation 'Adam/beta1' type=Const>,\n",
       " <tf.Operation 'Adam/beta2' type=Const>,\n",
       " <tf.Operation 'Adam/epsilon' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/Unique' type=Unique>,\n",
       " <tf.Operation 'Adam/update_embedding/W/Shape' type=Shape>,\n",
       " <tf.Operation 'Adam/update_embedding/W/strided_slice/stack' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/strided_slice/stack_1' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/strided_slice/stack_2' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/strided_slice' type=StridedSlice>,\n",
       " <tf.Operation 'Adam/update_embedding/W/UnsortedSegmentSum' type=UnsortedSegmentSum>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub/x' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub' type=Sub>,\n",
       " <tf.Operation 'Adam/update_embedding/W/Sqrt' type=Sqrt>,\n",
       " <tf.Operation 'Adam/update_embedding/W/mul' type=Mul>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub_1/x' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub_1' type=Sub>,\n",
       " <tf.Operation 'Adam/update_embedding/W/truediv' type=RealDiv>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub_2/x' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub_2' type=Sub>,\n",
       " <tf.Operation 'Adam/update_embedding/W/mul_1' type=Mul>,\n",
       " <tf.Operation 'Adam/update_embedding/W/mul_2' type=Mul>,\n",
       " <tf.Operation 'Adam/update_embedding/W/Assign' type=Assign>,\n",
       " <tf.Operation 'Adam/update_embedding/W/ScatterAdd' type=ScatterAdd>,\n",
       " <tf.Operation 'Adam/update_embedding/W/mul_3' type=Mul>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub_3/x' type=Const>,\n",
       " <tf.Operation 'Adam/update_embedding/W/sub_3' type=Sub>,\n",
       " <tf.Operation 'Adam/update_embedding/W/mul_4' type=Mul>,\n",
       " <tf.Operation 'Adam/update_embedding/W/mul_5' type=Mul>,\n",
       " <tf.Operation 'Adam/update_embedding/W/Assign_1' type=Assign>,\n",
       " <tf.Operation 'Adam/update_embedding/W/ScatterAdd_1' type=ScatterAdd>,\n",
       " <tf.Operation 'Adam/update_embedding/W/Sqrt_1' type=Sqrt>,\n",
       " <tf.Operation 'Adam/update_embedding/W/mul_6' type=Mul>,\n",
       " <tf.Operation 'Adam/update_embedding/W/add' type=Add>,\n",
       " <tf.Operation 'Adam/update_embedding/W/truediv_1' type=RealDiv>,\n",
       " <tf.Operation 'Adam/update_embedding/W/AssignSub' type=AssignSub>,\n",
       " <tf.Operation 'Adam/update_embedding/W/group_deps' type=NoOp>,\n",
       " <tf.Operation 'Adam/update_conv-maxpool-3/W/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv-maxpool-3/b/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv-maxpool-4/W/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv-maxpool-4/b/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv-maxpool-5/W/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_conv-maxpool-5/b/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_W/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/update_output/b/ApplyAdam' type=ApplyAdam>,\n",
       " <tf.Operation 'Adam/mul' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign' type=Assign>,\n",
       " <tf.Operation 'Adam/mul_1' type=Mul>,\n",
       " <tf.Operation 'Adam/Assign_1' type=Assign>,\n",
       " <tf.Operation 'Adam/update/NoOp' type=NoOp>,\n",
       " <tf.Operation 'Adam/update/NoOp_1' type=NoOp>,\n",
       " <tf.Operation 'Adam/update' type=NoOp>,\n",
       " <tf.Operation 'Adam/value' type=Const>,\n",
       " <tf.Operation 'Adam' type=AssignAdd>,\n",
       " <tf.Operation 'embedding/W_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'embedding/W_0/grad/hist/strided_slice/stack' type=Const>,\n",
       " <tf.Operation 'embedding/W_0/grad/hist/strided_slice/stack_1' type=Const>,\n",
       " <tf.Operation 'embedding/W_0/grad/hist/strided_slice/stack_2' type=Const>,\n",
       " <tf.Operation 'embedding/W_0/grad/hist/strided_slice' type=StridedSlice>,\n",
       " <tf.Operation 'embedding/W_0/grad/hist/values' type=UnsortedSegmentSum>,\n",
       " <tf.Operation 'embedding/W_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction/strided_slice/stack' type=Const>,\n",
       " <tf.Operation 'zero_fraction/strided_slice/stack_1' type=Const>,\n",
       " <tf.Operation 'zero_fraction/strided_slice/stack_2' type=Const>,\n",
       " <tf.Operation 'zero_fraction/strided_slice' type=StridedSlice>,\n",
       " <tf.Operation 'zero_fraction/value' type=UnsortedSegmentSum>,\n",
       " <tf.Operation 'zero_fraction/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction/Mean' type=Mean>,\n",
       " <tf.Operation 'embedding/W_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'embedding/W_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'conv-maxpool-3/W_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/W_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_1/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_1/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_1/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_1/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_1/Mean' type=Mean>,\n",
       " <tf.Operation 'conv-maxpool-3/W_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/W_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'conv-maxpool-3/b_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/b_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_2/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_2/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_2/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_2/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_2/Mean' type=Mean>,\n",
       " <tf.Operation 'conv-maxpool-3/b_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-3/b_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'conv-maxpool-4/W_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/W_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_3/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_3/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_3/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_3/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_3/Mean' type=Mean>,\n",
       " <tf.Operation 'conv-maxpool-4/W_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/W_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'conv-maxpool-4/b_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/b_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_4/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_4/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_4/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_4/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_4/Mean' type=Mean>,\n",
       " <tf.Operation 'conv-maxpool-4/b_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-4/b_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'conv-maxpool-5/W_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/W_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_5/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_5/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_5/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_5/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_5/Mean' type=Mean>,\n",
       " <tf.Operation 'conv-maxpool-5/W_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/W_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'conv-maxpool-5/b_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/b_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_6/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_6/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_6/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_6/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_6/Mean' type=Mean>,\n",
       " <tf.Operation 'conv-maxpool-5/b_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'conv-maxpool-5/b_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'W_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'W_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_7/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_7/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_7/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_7/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_7/Mean' type=Mean>,\n",
       " <tf.Operation 'W_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'W_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'output/b_0/grad/hist/tag' type=Const>,\n",
       " <tf.Operation 'output/b_0/grad/hist' type=HistogramSummary>,\n",
       " <tf.Operation 'zero_fraction_8/zero' type=Const>,\n",
       " <tf.Operation 'zero_fraction_8/Equal' type=Equal>,\n",
       " <tf.Operation 'zero_fraction_8/Cast' type=Cast>,\n",
       " <tf.Operation 'zero_fraction_8/Const' type=Const>,\n",
       " <tf.Operation 'zero_fraction_8/Mean' type=Mean>,\n",
       " <tf.Operation 'output/b_0/grad/sparsity/tags' type=Const>,\n",
       " <tf.Operation 'output/b_0/grad/sparsity' type=ScalarSummary>,\n",
       " <tf.Operation 'Merge/MergeSummary' type=MergeSummary>,\n",
       " <tf.Operation 'loss_1/tags' type=Const>,\n",
       " <tf.Operation 'loss_1' type=ScalarSummary>,\n",
       " <tf.Operation 'accuracy_1/tags' type=Const>,\n",
       " <tf.Operation 'accuracy_1' type=ScalarSummary>,\n",
       " <tf.Operation 'Merge_1/MergeSummary' type=MergeSummary>,\n",
       " <tf.Operation 'Merge_2/MergeSummary' type=MergeSummary>,\n",
       " <tf.Operation 'save/Const' type=Const>,\n",
       " <tf.Operation 'save/SaveV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/SaveV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/SaveV2' type=SaveV2>,\n",
       " <tf.Operation 'save/control_dependency' type=Identity>,\n",
       " <tf.Operation 'save/RestoreV2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_1/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_1/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_1' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_1' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_2/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_2/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_2' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_2' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_3/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_3/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_3' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_3' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_4/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_4/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_4' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_4' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_5/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_5/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_5' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_5' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_6/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_6/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_6' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_6' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_7/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_7/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_7' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_7' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_8/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_8/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_8' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_8' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_9/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_9/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_9' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_9' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_10/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_10/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_10' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_10' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_11/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_11/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_11' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_11' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_12/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_12/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_12' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_12' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_13/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_13/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_13' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_13' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_14/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_14/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_14' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_14' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_15/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_15/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_15' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_15' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_16/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_16/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_16' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_16' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_17/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_17/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_17' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_17' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_18/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_18/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_18' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_18' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_19/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_19/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_19' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_19' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_20/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_20/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_20' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_20' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_21/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_21/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_21' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_21' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_22/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_22/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_22' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_22' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_23/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_23/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_23' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_23' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_24/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_24/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_24' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_24' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_25/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_25/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_25' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_25' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_26/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_26/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_26' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_26' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_27/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_27/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_27' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_27' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_28/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_28/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_28' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_28' type=Assign>,\n",
       " <tf.Operation 'save/RestoreV2_29/tensor_names' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_29/shape_and_slices' type=Const>,\n",
       " <tf.Operation 'save/RestoreV2_29' type=RestoreV2>,\n",
       " <tf.Operation 'save/Assign_29' type=Assign>,\n",
       " <tf.Operation 'save/restore_all/NoOp' type=NoOp>,\n",
       " <tf.Operation 'save/restore_all/NoOp_1' type=NoOp>,\n",
       " <tf.Operation 'save/restore_all' type=NoOp>,\n",
       " <tf.Operation 'init/NoOp' type=NoOp>,\n",
       " <tf.Operation 'init/NoOp_1' type=NoOp>,\n",
       " <tf.Operation 'init' type=NoOp>,\n",
       " <tf.Operation 'init_1' type=NoOp>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 271)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sess.run(sess.graph.get_tensor_by_name('conv-maxpool-3/conv:0'), feed_dict = {\n",
    "              cnn.input_x: x_dev[:2],\n",
    "              cnn.input_y: y_dev[:2],\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            })\n",
    "temp1 = np.sum(temp.reshape([temp.shape[0], temp.shape[1], temp.shape[-1]]), axis=2)/temp.shape[-1]\n",
    "temp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_activation(conv_layer=3):\n",
    "    temp = sess.run(sess.graph.get_tensor_by_name('conv-maxpool-%s/conv:0' % conv_layer), feed_dict = {\n",
    "              cnn.input_x: x_dev[:10],\n",
    "              cnn.input_y: y_dev[:10],\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            })\n",
    "    temp1 = np.sum(temp.reshape([temp.shape[0], temp.shape[1], temp.shape[-1]]), axis=2)/temp.shape[-1]\n",
    "    wmin = np.min(temp1, axis=1)\n",
    "    wmax = np.max(temp1, axis=1)\n",
    "    temp2 = (temp1.transpose() - wmin)/(wmax-wmin)\n",
    "    temp2 = temp2.transpose()\n",
    "    temp3 = np.zeros([temp.shape[0], 273])\n",
    "    for i in range(temp2.shape[1]):\n",
    "        temp3[:,i:i+3] += temp2[:,i].reshape([temp.shape[0],1])\n",
    "#         temp3[:,i] += temp2[:,i]\n",
    "    temp3 = temp3/conv_layer\n",
    "    return temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 273)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3 = cnn_activation(conv_layer=3)\n",
    "temp3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAFJ4AAAMHCAYAAADWK1kkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTwIAAAAAAAAAAAAAAAAAAAAAAAAATAXUYRTXffD9tQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13874c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "my_cmap = mpl.cm.coolwarm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for j in range(len(dev_input[:10])):\n",
    "    words = dev_input[j].split('<UNK>')[0].split()\n",
    "    for i in range(len(words)):\n",
    "        num_words = len(words)\n",
    "        ax.text(i/num_words*14, j/3, words[i], color=my_cmap(temp3[j][i]), zorder=0)\n",
    "# For the moment, hide everything else...\n",
    "ax.axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
