{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd\n",
    "\n",
    "class CustomTextCNN(object):\n",
    "    \"\"\"\n",
    "    A Basic CNN for text classification with Position plus POS features as well.\n",
    "    \n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \n",
    "    Refer tohttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5181565/pdf/btw486.pdf for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, vocab_processor, \n",
    "                 num_classes=2, embedding_size=128, filter_sizes=[3,4,5], \n",
    "                 num_filters=128, batch_size=64, \n",
    "                 l2_reg_lambda=0.0, num_epochs=200,\n",
    "                 num_checkpoints=5, dropout_prob=0.5, \n",
    "                 checkpoint_every=100, evaluate_every=100, \n",
    "                 allow_soft_placement=True,log_device_placement=False,\n",
    "                 results_dir=\"runs\",\n",
    "                 word_embedding=True,\n",
    "                 position_embedding=True,\n",
    "                 pos_embedding=True\n",
    "                ):\n",
    "        \n",
    "        tf.reset_default_graph() \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = len(vocab_processor.vocabulary_)\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.num_epochs = num_epochs\n",
    "        self.results_dir = results_dir\n",
    "        \n",
    "        self.vocab_processor = vocab_processor\n",
    "        \n",
    "        self.word_embedding = word_embedding\n",
    "        self.position_embedding = position_embedding\n",
    "        self.pos_embedding = pos_embedding\n",
    "        \n",
    "        self.num_checkpoints = num_checkpoints\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.checkpoint_every = checkpoint_every\n",
    "        self.evaluate_every = evaluate_every\n",
    "        \n",
    "        self.position_vector_mapping = CustomTextCNN.load_position_vector_mapping()\n",
    "        \n",
    "        self.allow_soft_placement = allow_soft_placement\n",
    "        self.log_device_placement = log_device_placement\n",
    "        \n",
    "        print('\\n'*2, '#'*20, '\\n'*2)\n",
    "        print(\"SEQUENCE LENGTH\", self.sequence_length)\n",
    "        print(\"BATCH SIZE\", self.batch_size)\n",
    "        print(\"EMBEDDING SIZE\", self.embedding_size)\n",
    "        print(\"FILTER SIZES\", self.filter_sizes)\n",
    "        print(\"NUMBER OF FILTERS\", self.num_filters)\n",
    "        print(\"L2 REG LAMBDA\", self.l2_reg_lambda)\n",
    "        print(\"EPOCHS\", self.num_epochs)\n",
    "        print('\\n'*2)\n",
    "        print(\"RESULT DIR\", self.results_dir)\n",
    "        print(\"VOCAB SIZE\", self.vocab_size)\n",
    "        print(\"DROPOUT PROBABILITY\", self.dropout_prob)\n",
    "        print('\\n'*2)\n",
    "        print(\"WORD EMBEDDING\", self.word_embedding)\n",
    "        print(\"POSITION EMBEDDING\", self.position_embedding)\n",
    "        print(\"POS EMBEDDING\", self.pos_embedding)\n",
    "        print('\\n'*2, '#'*20, '\\n'*2)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        self.word_distancesA = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"word_distancesA\")\n",
    "        self.word_distancesB = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"word_distancesB\")\n",
    "        \n",
    "        self.encoded_pos = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"encoded_pos\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "        embeddings = []\n",
    "        total_embedding_size = 0\n",
    "        \n",
    "        # Embedding layer\n",
    "        if self.word_embedding:\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"word_embedding\"):\n",
    "                self.W = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),name=\"W\") \n",
    "                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "                embeddings.append(self.embedded_chars_expanded)\n",
    "                total_embedding_size+=self.embedding_size\n",
    "            \n",
    "        # Position Embedding layer\n",
    "        if self.position_embedding:\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"position_embedding\"):\n",
    "                embedded_positionsA = tf.nn.embedding_lookup(self.position_vector_mapping, self.word_distancesA)\n",
    "                embedded_positionsB = tf.nn.embedding_lookup(self.position_vector_mapping, self.word_distancesB)\n",
    "                embedded_positions = tf.concat([embedded_positionsA, embedded_positionsB], 2)\n",
    "                self.embedded_positions_expanded = tf.cast(tf.expand_dims(embedded_positions, -1), tf.float32)\n",
    "                embeddings.append(self.embedded_positions_expanded)\n",
    "                total_embedding_size+=20\n",
    "            \n",
    "        # POS Embedding layer\n",
    "        if self.pos_embedding:\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"pos_embedding\"):\n",
    "                one_hot_encoding = tf.one_hot(list(range(8)), 8)\n",
    "                embedded_pos = tf.nn.embedding_lookup(one_hot_encoding, self.encoded_pos)\n",
    "                self.embedded_pos_expanded = tf.cast(tf.expand_dims(embedded_pos, -1), tf.float32)\n",
    "                embeddings.append(self.embedded_pos_expanded)\n",
    "                total_embedding_size+=8\n",
    "            \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.final_embedded_expanded = tf.concat(embeddings, 2)\n",
    "        \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, total_embedding_size, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.final_embedded_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, self.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "    def train_network(self, x_train, y_train, x_dev, y_dev,\n",
    "                     train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB,\n",
    "                     train_pos_embedding, test_pos_embedding):\n",
    "        \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            class_weight = tf.constant([1.0, 100.0])\n",
    "            weights = tf.reduce_sum(class_weight * self.input_y, axis=1)\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            weighted_losses = losses * weights\n",
    "            self.loss = tf.reduce_mean(weighted_losses) + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            _, self.precision = tf.metrics.precision(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='precision')\n",
    "            _, self.recall = tf.metrics.recall(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='recall')\n",
    "            \n",
    "        # Define Training procedure\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, self.results_dir, timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        precision_summary = tf.summary.scalar(\"precision\", self.precision)\n",
    "        recall_summary = tf.summary.scalar(\"recall\", self.recall)\n",
    "\n",
    "        # Train Summaries\n",
    "        self.train_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        self.train_summary_writer = tf.summary.FileWriter(train_summary_dir, self.sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        self.dev_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        self.dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, self.sess.graph)\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=self.num_checkpoints)\n",
    "        \n",
    "        # Write vocabulary\n",
    "        self.vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        # Initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "        print(\"Start training\")\n",
    "        # Generate batches\n",
    "        batches = CustomTextCNN.batch_iter(\n",
    "            list(zip(x_train, y_train, train_word_distancesA, train_word_distancesB, train_pos_embedding)), self.batch_size, self.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding = zip(*batch)\n",
    "            self.train_step(x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding)\n",
    "            current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "            if current_step % self.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                self.dev_step(x_dev, y_dev, test_word_distancesA, test_word_distancesB, test_pos_embedding, writer=self.dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % self.checkpoint_every == 0:\n",
    "                path = saver.save(self.sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))   \n",
    "        print(\"Training finished\")\n",
    "    \n",
    "    def train_step(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.input_x: x_batch,\n",
    "            self.input_y: y_batch,\n",
    "            self.dropout_keep_prob: self.dropout_prob,\n",
    "            self.word_distancesA: batch_word_distancesA,\n",
    "            self.word_distancesB: batch_word_distancesB,\n",
    "            self.encoded_pos: batch_pos_embedding\n",
    "        }\n",
    "        _, step, summaries, loss, accuracy, precision, recall = self.sess.run(\n",
    "            [self.train_op, self.global_step, self.train_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "        self.train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "    \n",
    "    def dev_step(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                self.input_x: x_batch,\n",
    "                self.input_y: y_batch,\n",
    "                self.dropout_keep_prob: 1.0,\n",
    "                self.word_distancesA: batch_word_distancesA,\n",
    "                self.word_distancesB: batch_word_distancesB,\n",
    "                self.encoded_pos: batch_pos_embedding\n",
    "            }\n",
    "            step, summaries, loss, accuracy,  precision, recall  = self.sess.run(\n",
    "                [self.global_step, self.dev_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "    @staticmethod            \n",
    "    def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates a batch iterator for a dataset.\n",
    "        \"\"\"\n",
    "        data = np.array(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]\n",
    "    \n",
    "    @staticmethod \n",
    "    def load_position_vector_mapping():\n",
    "        # bit_array generated with the distance between \n",
    "        # two entities where abs_num represents the distance\n",
    "        def int2bit_by_distance(int_num, bit_len=10):\n",
    "\n",
    "            bit_array = np.zeros(bit_len)\n",
    "            if int_num > 0:\n",
    "                bit_array[0] = 1\n",
    "\n",
    "            abs_num = np.abs(int_num)\n",
    "            if abs_num <= 5:\n",
    "                for i in range(abs_num):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 10:\n",
    "                for i in range(6):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 20:\n",
    "                for i in range(7):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 30:\n",
    "                for i in range(8):\n",
    "                    bit_array[-i-1] = 1\n",
    "            else:\n",
    "                for i in range(9):\n",
    "                    bit_array[-i-1] = 1\n",
    "            return bit_array\n",
    "\n",
    "        map = {}\n",
    "        for i in range(-300, 300):\n",
    "            map[i] = int2bit_by_distance(i, 10)\n",
    "\n",
    "        return pd.DataFrame.from_dict(map, orient='index', dtype='int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from BasicTextCNN import BasicTextCNN\n",
    "from PositionTextCNN import PositionTextCNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)\n",
    "\n",
    "def compute_distance_embedding(word, x):\n",
    "    word_distances = np.zeros(x.shape, dtype='int')\n",
    "    for i in range(x.shape[0]):\n",
    "        word_positions = np.where(x[i] == word)[0]\n",
    "        for j in range(x.shape[1]):\n",
    "            if len(word_positions) > 0:\n",
    "                word_position = word_positions[np.argmin(np.abs(word_positions - j))]\n",
    "                word_distances[i][j] = word_position - j\n",
    "                if word_distances[i][j]<0:\n",
    "                    word_distances[i][j] = 600+word_distances[i][j]\n",
    "            else:\n",
    "                word_distances[i][j] = 299\n",
    "    return word_distances\n",
    "\n",
    "def compute_pos_embedding(data, vocab_processor):\n",
    "    pos_emebedding = np.zeros(data.shape, dtype='int')\n",
    "    for i in range(data.shape[0]):\n",
    "        tags = pos_tag(word_tokenize(list(vocab_processor.reverse([data[i]]))[0].replace('<UNK>', 'XXX')))\n",
    "        for j in range(data.shape[1]):\n",
    "            if tags[j][1].lower() in pos_map:\n",
    "                pos_emebedding[i][j] = pos_map[tags[j][1].lower()]\n",
    "            else:\n",
    "                pos_emebedding[i][j] = 6\n",
    "    return pos_emebedding\n",
    "\n",
    "def load_pos_embedding():\n",
    "    return np.load('pos_emebedding.npy')\n",
    "\n",
    "def load_word_distancesA():\n",
    "    return np.load('word_distancesA.npy')\n",
    "\n",
    "def load_word_distancesB():\n",
    "    return np.load('word_distancesB.npy')\n",
    "\n",
    "def load_pos_mapping():\n",
    "    pos_map = {}\n",
    "    with open('pos-mapping.txt', 'r') as f:\n",
    "        for lines in f.readlines():\n",
    "            pos, num = lines.split()\n",
    "            pos_map[pos] = num\n",
    "    return pos_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 53)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedPathwayA, encodedPathwayB = list(vocab_processor.transform(['pathwayA pathwayB']))[0][:2]\n",
    "encodedPathwayA, encodedPathwayB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_distancesA = compute_distance_embedding(encodedPathwayA, x)\n",
    "# word_distancesB = compute_distance_embedding(encodedPathwayB, x) \n",
    "# np.save('word_distancesA.npy', word_distancesA)\n",
    "# np.save('word_distancesB.npy', word_distancesB)\n",
    "word_distancesA = load_word_distancesA()\n",
    "word_distancesB = load_word_distancesB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42394, 273)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding = load_pos_embedding()\n",
    "pos_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 => Train/Dev split: 31795/10599\n",
      "#################### \n",
      "\n",
      "\n",
      "SEQUENCE LENGTH 273\n",
      "BATCH SIZE 64\n",
      "EMBEDDING SIZE 128\n",
      "FILTER SIZES [3, 4, 5]\n",
      "NUMBER OF FILTERS 128\n",
      "L2 REG LAMBDA 0.0\n",
      "EPOCHS 1\n",
      "\n",
      "\n",
      "\n",
      "RESULT DIR fold0\n",
      "VOCAB SIZE 33447\n",
      "DROPOUT PROBABILITY 0.5\n",
      "\n",
      "\n",
      "\n",
      "WORD EMBEDDING True\n",
      "POSITION EMBEDDING False\n",
      "POS EMBEDDING True\n",
      "#################### \n",
      "\n",
      "\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/hist is illegal; using word_embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/sparsity is illegal; using word_embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512804141\n",
      "\n",
      "Start training\n",
      "2017-12-09T02:22:24.835931: step 1, loss 3.13127, acc 0.390625, prec 0.025, recall 1\n",
      "2017-12-09T02:22:25.380744: step 2, loss 1.24694, acc 0.71875, prec 0.0172414, recall 1\n",
      "2017-12-09T02:22:25.924462: step 3, loss 0.568351, acc 0.84375, prec 0.0289855, recall 1\n",
      "2017-12-09T02:22:26.459992: step 4, loss 27.0191, acc 0.875, prec 0.0266667, recall 0.5\n",
      "2017-12-09T02:22:27.007714: step 5, loss 0.412311, acc 0.875, prec 0.0240964, recall 0.5\n",
      "2017-12-09T02:22:27.551639: step 6, loss 22.2902, acc 0.9375, prec 0.0232558, recall 0.4\n",
      "2017-12-09T02:22:28.089855: step 7, loss 3.04193, acc 0.890625, prec 0.0217391, recall 0.333333\n",
      "2017-12-09T02:22:28.631946: step 8, loss 0.345994, acc 0.890625, prec 0.020202, recall 0.333333\n",
      "2017-12-09T02:22:29.188587: step 9, loss 4.48007, acc 0.859375, prec 0.0186916, recall 0.285714\n",
      "2017-12-09T02:22:29.743774: step 10, loss 12.7658, acc 0.75, prec 0.0163934, recall 0.25\n",
      "2017-12-09T02:22:30.306327: step 11, loss 1.60003, acc 0.78125, prec 0.0218978, recall 0.333333\n",
      "2017-12-09T02:22:30.849025: step 12, loss 14.865, acc 0.734375, prec 0.0322581, recall 0.416667\n",
      "2017-12-09T02:22:31.408446: step 13, loss 1.21078, acc 0.640625, prec 0.0280899, recall 0.416667\n",
      "2017-12-09T02:22:31.961071: step 14, loss 1.69669, acc 0.640625, prec 0.029703, recall 0.461538\n",
      "2017-12-09T02:22:32.488936: step 15, loss 11.9743, acc 0.421875, prec 0.0252101, recall 0.428571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5aeedee99c57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     model.train_network(x_train, y_train, x_dev, y_dev, \n\u001b[1;32m     25\u001b[0m                         \u001b[0mtrain_word_distancesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_word_distancesB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_word_distancesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_word_distancesB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                        train_pos_embedding, test_pos_embedding)\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ff5bb774db93>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, x_train, y_train, x_dev, y_dev, train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB, train_pos_embedding, test_pos_embedding)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ff5bb774db93>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding)\u001b[0m\n\u001b[1;32m    265\u001b[0m         _, step, summaries, loss, accuracy, precision, recall = self.sess.run(\n\u001b[1;32m    266\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating folds\n",
    "kf = KFold(n_splits=4, random_state=5, shuffle=True)\n",
    "for k, (train_index, test_index) in enumerate(kf.split(x, y)):\n",
    "# for train_index, test_index in kf.split(x):\n",
    "#     print(\"Fold: %s =>\" % k,  \"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_dev = x[train_index], x[test_index]\n",
    "    y_train, y_dev = y[train_index], y[test_index]\n",
    "    \n",
    "    train_word_distancesA = word_distancesA[train_index]\n",
    "    train_word_distancesB = word_distancesB[train_index]\n",
    "    \n",
    "    test_word_distancesA = word_distancesA[test_index]\n",
    "    test_word_distancesB = word_distancesB[test_index]\n",
    "    \n",
    "    train_pos_embedding = pos_embedding[train_index]\n",
    "    test_pos_embedding = pos_embedding[test_index]\n",
    "    \n",
    "    print(\"Fold: %s =>\" % k, \"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    \n",
    "    model = CustomTextCNN(sequence_length=x_train.shape[1],\n",
    "            vocab_processor=vocab_processor, num_epochs=1, evaluate_every=300, results_dir='fold%s'%k, \n",
    "                          position_embedding=False)\n",
    "    model.train_network(x_train, y_train, x_dev, y_dev, \n",
    "                        train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB,\n",
    "                       train_pos_embedding, test_pos_embedding)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
