{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n",
      "Train/Dev split: 31796/10598\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(0.25 * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BasicTextCNN import BasicTextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512707113\n",
      "\n",
      "Start training\n",
      "2017-12-07T23:25:16.083348: step 1, loss 6.3683, acc 0.234375, prec 0, recall 0\n",
      "2017-12-07T23:25:16.615382: step 2, loss 2.04468, acc 0.5, prec 0.0123457, recall 0.5\n",
      "2017-12-07T23:25:17.140129: step 3, loss 3.6043, acc 0.46875, prec 0.00877193, recall 0.333333\n",
      "2017-12-07T23:25:17.650474: step 4, loss 5.93962, acc 0.5625, prec 0.0070922, recall 0.25\n",
      "2017-12-07T23:25:18.147766: step 5, loss 1.67, acc 0.578125, prec 0.00595238, recall 0.25\n",
      "2017-12-07T23:25:18.654308: step 6, loss 8.52976, acc 0.53125, prec 0.00507614, recall 0.2\n",
      "2017-12-07T23:25:19.153361: step 7, loss 11.4692, acc 0.40625, prec 0.0042735, recall 0.166667\n",
      "2017-12-07T23:25:19.653807: step 8, loss 8.92388, acc 0.390625, prec 0.00367647, recall 0.142857\n",
      "2017-12-07T23:25:20.154095: step 9, loss 10.841, acc 0.390625, prec 0.00645161, recall 0.2\n",
      "2017-12-07T23:25:20.652757: step 10, loss 3.39703, acc 0.25, prec 0.00558659, recall 0.2\n",
      "2017-12-07T23:25:21.150448: step 11, loss 4.39515, acc 0.265625, prec 0.00982801, recall 0.333333\n",
      "2017-12-07T23:25:21.644371: step 12, loss 4.89488, acc 0.234375, prec 0.0152505, recall 0.466667\n",
      "2017-12-07T23:25:22.144413: step 13, loss 4.92169, acc 0.15625, prec 0.0174757, recall 0.529412\n",
      "2017-12-07T23:25:22.634621: step 14, loss 5.42943, acc 0.09375, prec 0.0157068, recall 0.529412\n",
      "2017-12-07T23:25:23.125140: step 15, loss 5.70851, acc 0.15625, prec 0.0174881, recall 0.578947\n",
      "2017-12-07T23:25:23.620597: step 16, loss 6.46989, acc 0.078125, prec 0.0159884, recall 0.578947\n",
      "2017-12-07T23:25:24.107842: step 17, loss 5.56133, acc 0.09375, prec 0.0147453, recall 0.578947\n",
      "2017-12-07T23:25:24.602611: step 18, loss 4.33255, acc 0.21875, prec 0.0150565, recall 0.6\n",
      "2017-12-07T23:25:25.089533: step 19, loss 4.4704, acc 0.265625, prec 0.0153846, recall 0.619048\n",
      "2017-12-07T23:25:25.581323: step 20, loss 4.67146, acc 0.265625, prec 0.0167785, recall 0.652174\n",
      "2017-12-07T23:25:26.076794: step 21, loss 4.00446, acc 0.28125, prec 0.0180467, recall 0.68\n",
      "2017-12-07T23:25:26.575699: step 22, loss 3.084, acc 0.5, prec 0.0174538, recall 0.68\n",
      "2017-12-07T23:25:27.101945: step 23, loss 2.99526, acc 0.421875, prec 0.016815, recall 0.68\n",
      "2017-12-07T23:25:27.602518: step 24, loss 12.6818, acc 0.375, prec 0.0162214, recall 0.607143\n",
      "2017-12-07T23:25:28.104565: step 25, loss 9.56377, acc 0.53125, prec 0.0167131, recall 0.580645\n",
      "2017-12-07T23:25:28.605014: step 26, loss 5.13365, acc 0.40625, prec 0.0170404, recall 0.575758\n",
      "2017-12-07T23:25:29.108680: step 27, loss 3.10068, acc 0.359375, prec 0.0172861, recall 0.588235\n",
      "2017-12-07T23:25:29.592817: step 28, loss 2.88584, acc 0.359375, prec 0.0175146, recall 0.6\n",
      "2017-12-07T23:25:30.098337: step 29, loss 3.22697, acc 0.359375, prec 0.0169355, recall 0.6\n",
      "2017-12-07T23:25:30.599363: step 30, loss 2.73419, acc 0.390625, prec 0.0171875, recall 0.611111\n",
      "2017-12-07T23:25:31.104230: step 31, loss 3.25707, acc 0.421875, prec 0.0174507, recall 0.621622\n",
      "2017-12-07T23:25:31.610473: step 32, loss 3.60875, acc 0.390625, prec 0.0183959, recall 0.641026\n",
      "2017-12-07T23:25:32.109778: step 33, loss 2.71244, acc 0.390625, prec 0.0178827, recall 0.641026\n",
      "2017-12-07T23:25:32.615215: step 34, loss 6.68444, acc 0.4375, prec 0.0174459, recall 0.625\n",
      "2017-12-07T23:25:33.121644: step 35, loss 2.84576, acc 0.453125, prec 0.01703, recall 0.625\n",
      "2017-12-07T23:25:33.623353: step 36, loss 5.61604, acc 0.453125, prec 0.0186047, recall 0.636364\n",
      "2017-12-07T23:25:34.131257: step 37, loss 3.77313, acc 0.265625, prec 0.0180412, recall 0.636364\n",
      "2017-12-07T23:25:34.641676: step 38, loss 5.32403, acc 0.453125, prec 0.0182735, recall 0.630435\n",
      "2017-12-07T23:25:35.172032: step 39, loss 3.27971, acc 0.359375, prec 0.0184162, recall 0.638298\n",
      "2017-12-07T23:25:35.685946: step 40, loss 2.54476, acc 0.5, prec 0.0180614, recall 0.638298\n",
      "2017-12-07T23:25:36.194141: step 41, loss 2.43499, acc 0.421875, prec 0.0176678, recall 0.638298\n",
      "2017-12-07T23:25:36.710620: step 42, loss 2.70268, acc 0.4375, prec 0.017301, recall 0.638298\n",
      "2017-12-07T23:25:37.256818: step 43, loss 3.58556, acc 0.4375, prec 0.0186125, recall 0.66\n",
      "2017-12-07T23:25:37.793194: step 44, loss 1.87343, acc 0.5, prec 0.0182825, recall 0.66\n",
      "2017-12-07T23:25:38.326536: step 45, loss 2.75916, acc 0.515625, prec 0.0179739, recall 0.66\n",
      "2017-12-07T23:25:38.860600: step 46, loss 5.03911, acc 0.5625, prec 0.0182403, recall 0.653846\n",
      "2017-12-07T23:25:39.389997: step 47, loss 1.74453, acc 0.65625, prec 0.0190678, recall 0.666667\n",
      "2017-12-07T23:25:39.931158: step 48, loss 1.67774, acc 0.59375, prec 0.0188088, recall 0.666667\n",
      "2017-12-07T23:25:40.465257: step 49, loss 3.53816, acc 0.5625, prec 0.0195473, recall 0.678571\n",
      "2017-12-07T23:25:41.022971: step 50, loss 1.49156, acc 0.578125, prec 0.0192796, recall 0.678571\n",
      "2017-12-07T23:25:41.560023: step 51, loss 1.94645, acc 0.640625, prec 0.0200401, recall 0.689655\n",
      "2017-12-07T23:25:42.102887: step 52, loss 10.6935, acc 0.765625, prec 0.0203879, recall 0.683333\n",
      "2017-12-07T23:25:42.641814: step 53, loss 2.15331, acc 0.734375, prec 0.0211823, recall 0.693548\n",
      "2017-12-07T23:25:43.188822: step 54, loss 1.1508, acc 0.71875, prec 0.0209961, recall 0.693548\n",
      "2017-12-07T23:25:43.729652: step 55, loss 1.8866, acc 0.53125, prec 0.020693, recall 0.693548\n",
      "2017-12-07T23:25:44.275415: step 56, loss 1.30157, acc 0.515625, prec 0.0203888, recall 0.693548\n",
      "2017-12-07T23:25:44.838101: step 57, loss 4.97238, acc 0.515625, prec 0.0205607, recall 0.6875\n",
      "2017-12-07T23:25:45.383402: step 58, loss 6.673, acc 0.59375, prec 0.0207756, recall 0.681818\n",
      "2017-12-07T23:25:45.940827: step 59, loss 1.68809, acc 0.625, prec 0.0205479, recall 0.681818\n",
      "2017-12-07T23:25:46.500982: step 60, loss 4.62106, acc 0.546875, prec 0.0207301, recall 0.676471\n",
      "2017-12-07T23:25:47.089713: step 61, loss 17.096, acc 0.5, prec 0.0208796, recall 0.671429\n",
      "2017-12-07T23:25:47.651760: step 62, loss 2.16197, acc 0.484375, prec 0.0210066, recall 0.676056\n",
      "2017-12-07T23:25:48.209577: step 63, loss 5.82972, acc 0.421875, prec 0.0215239, recall 0.675676\n",
      "2017-12-07T23:25:48.765905: step 64, loss 3.56756, acc 0.375, prec 0.0215736, recall 0.68\n",
      "2017-12-07T23:25:49.331266: step 65, loss 3.36012, acc 0.375, prec 0.0220283, recall 0.688312\n",
      "2017-12-07T23:25:49.901627: step 66, loss 6.1929, acc 0.390625, prec 0.0216858, recall 0.679487\n",
      "2017-12-07T23:25:50.467824: step 67, loss 3.99476, acc 0.328125, prec 0.0217042, recall 0.683544\n",
      "2017-12-07T23:25:51.033925: step 68, loss 3.86713, acc 0.375, prec 0.0221344, recall 0.691358\n",
      "2017-12-07T23:25:51.601544: step 69, loss 3.27773, acc 0.390625, prec 0.022179, recall 0.695122\n",
      "2017-12-07T23:25:52.162434: step 70, loss 3.26078, acc 0.4375, prec 0.0218726, recall 0.695122\n",
      "2017-12-07T23:25:52.723725: step 71, loss 3.88551, acc 0.234375, prec 0.0218373, recall 0.698795\n",
      "2017-12-07T23:25:53.288853: step 72, loss 4.24256, acc 0.265625, prec 0.0221811, recall 0.705882\n",
      "2017-12-07T23:25:53.835549: step 73, loss 3.83201, acc 0.25, prec 0.0217944, recall 0.705882\n",
      "2017-12-07T23:25:54.383901: step 74, loss 3.82404, acc 0.265625, prec 0.0217779, recall 0.709302\n",
      "2017-12-07T23:25:54.934047: step 75, loss 2.53064, acc 0.421875, prec 0.021494, recall 0.709302\n",
      "2017-12-07T23:25:55.487002: step 76, loss 2.42774, acc 0.484375, prec 0.0215877, recall 0.712644\n",
      "2017-12-07T23:25:56.038283: step 77, loss 2.33846, acc 0.453125, prec 0.0213278, recall 0.712644\n",
      "2017-12-07T23:25:56.606161: step 78, loss 8.15202, acc 0.5625, prec 0.0211316, recall 0.704545\n",
      "2017-12-07T23:25:57.223664: step 79, loss 3.52461, acc 0.453125, prec 0.0212121, recall 0.707865\n",
      "2017-12-07T23:25:57.788721: step 80, loss 2.2043, acc 0.578125, prec 0.0213476, recall 0.711111\n",
      "2017-12-07T23:25:58.347203: step 81, loss 1.76826, acc 0.5, prec 0.0214451, recall 0.714286\n",
      "2017-12-07T23:25:58.914039: step 82, loss 7.90032, acc 0.5625, prec 0.0212557, recall 0.706522\n",
      "2017-12-07T23:25:59.487805: step 83, loss 0.88276, acc 0.71875, prec 0.0211313, recall 0.706522\n",
      "2017-12-07T23:26:00.065613: step 84, loss 16.1039, acc 0.578125, prec 0.020961, recall 0.691489\n",
      "2017-12-07T23:26:00.630622: step 85, loss 4.37833, acc 0.6875, prec 0.0208333, recall 0.684211\n",
      "2017-12-07T23:26:01.190769: step 86, loss 1.94022, acc 0.515625, prec 0.0206284, recall 0.684211\n",
      "2017-12-07T23:26:01.754781: step 87, loss 2.70415, acc 0.53125, prec 0.0207417, recall 0.6875\n",
      "2017-12-07T23:26:02.334481: step 88, loss 1.70816, acc 0.59375, prec 0.0208788, recall 0.690722\n",
      "2017-12-07T23:26:02.904171: step 89, loss 2.06988, acc 0.515625, prec 0.020679, recall 0.690722\n",
      "2017-12-07T23:26:03.469862: step 90, loss 2.11121, acc 0.53125, prec 0.0207887, recall 0.693878\n",
      "2017-12-07T23:26:04.031855: step 91, loss 1.57433, acc 0.578125, prec 0.0206186, recall 0.693878\n",
      "2017-12-07T23:26:04.594459: step 92, loss 2.5557, acc 0.609375, prec 0.0210526, recall 0.7\n",
      "2017-12-07T23:26:05.160114: step 93, loss 2.20124, acc 0.53125, prec 0.0211561, recall 0.70297\n",
      "2017-12-07T23:26:05.725709: step 94, loss 1.85823, acc 0.546875, prec 0.0209749, recall 0.70297\n",
      "2017-12-07T23:26:06.305858: step 95, loss 1.97237, acc 0.5, prec 0.0207785, recall 0.70297\n",
      "2017-12-07T23:26:06.886425: step 96, loss 7.3575, acc 0.546875, prec 0.0206096, recall 0.696078\n",
      "2017-12-07T23:26:07.480306: step 97, loss 1.91631, acc 0.609375, prec 0.0210253, recall 0.701923\n",
      "2017-12-07T23:26:08.111363: step 98, loss 21.1729, acc 0.53125, prec 0.0211368, recall 0.691589\n",
      "2017-12-07T23:26:08.747263: step 99, loss 8.7525, acc 0.4375, prec 0.0209276, recall 0.685185\n",
      "2017-12-07T23:26:09.395730: step 100, loss 2.33877, acc 0.4375, prec 0.0209908, recall 0.688073\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512707113/checkpoints/model-100\n",
      "\n",
      "2017-12-07T23:26:10.733580: step 101, loss 7.91695, acc 0.390625, prec 0.0207699, recall 0.681818\n",
      "2017-12-07T23:26:11.325101: step 102, loss 3.26847, acc 0.359375, prec 0.0205367, recall 0.681818\n",
      "2017-12-07T23:26:11.898627: step 103, loss 5.60591, acc 0.40625, prec 0.0203307, recall 0.675676\n",
      "2017-12-07T23:26:12.473133: step 104, loss 3.88686, acc 0.34375, prec 0.0203644, recall 0.678571\n",
      "2017-12-07T23:26:13.057041: step 105, loss 3.27931, acc 0.328125, prec 0.0201325, recall 0.678571\n",
      "2017-12-07T23:26:13.632830: step 106, loss 3.90076, acc 0.234375, prec 0.0201307, recall 0.681416\n",
      "2017-12-07T23:26:14.192094: step 107, loss 8.98754, acc 0.28125, prec 0.0201498, recall 0.678261\n",
      "2017-12-07T23:26:14.754529: step 108, loss 7.5087, acc 0.25, prec 0.0199081, recall 0.672414\n",
      "2017-12-07T23:26:15.313678: step 109, loss 5.63535, acc 0.234375, prec 0.0199093, recall 0.675214\n",
      "2017-12-07T23:26:15.878081: step 110, loss 5.40832, acc 0.203125, prec 0.0196566, recall 0.675214\n",
      "2017-12-07T23:26:16.456712: step 111, loss 4.02885, acc 0.296875, prec 0.019439, recall 0.675214\n",
      "2017-12-07T23:26:17.028707: step 112, loss 4.39733, acc 0.21875, prec 0.0192027, recall 0.675214\n",
      "2017-12-07T23:26:17.607398: step 113, loss 4.61145, acc 0.25, prec 0.0192169, recall 0.677966\n",
      "2017-12-07T23:26:18.180675: step 114, loss 3.94081, acc 0.359375, prec 0.019729, recall 0.68595\n",
      "2017-12-07T23:26:18.769449: step 115, loss 3.45142, acc 0.234375, prec 0.0199624, recall 0.691057\n",
      "2017-12-07T23:26:19.340333: step 116, loss 2.89235, acc 0.390625, prec 0.0200093, recall 0.693548\n",
      "2017-12-07T23:26:19.902958: step 117, loss 2.31474, acc 0.453125, prec 0.0198477, recall 0.693548\n",
      "2017-12-07T23:26:20.458042: step 118, loss 3.82518, acc 0.421875, prec 0.0199085, recall 0.690476\n",
      "2017-12-07T23:26:21.017763: step 119, loss 2.98643, acc 0.4375, prec 0.0197458, recall 0.690476\n",
      "2017-12-07T23:26:21.587461: step 120, loss 2.33385, acc 0.484375, prec 0.0204817, recall 0.7\n",
      "2017-12-07T23:26:22.157091: step 121, loss 1.80722, acc 0.5625, prec 0.0203534, recall 0.7\n",
      "2017-12-07T23:26:22.720276: step 122, loss 15.0733, acc 0.546875, prec 0.0202267, recall 0.694656\n",
      "2017-12-07T23:26:23.280096: step 123, loss 1.65785, acc 0.609375, prec 0.0201149, recall 0.694656\n",
      "2017-12-07T23:26:23.855356: step 124, loss 1.53615, acc 0.671875, prec 0.0202376, recall 0.69697\n",
      "2017-12-07T23:26:24.407481: step 125, loss 1.86105, acc 0.578125, prec 0.0201181, recall 0.69697\n",
      "2017-12-07T23:26:24.974443: step 126, loss 1.26856, acc 0.65625, prec 0.0200218, recall 0.69697\n",
      "2017-12-07T23:26:25.546060: step 127, loss 1.12559, acc 0.65625, prec 0.0199264, recall 0.69697\n",
      "2017-12-07T23:26:26.109541: step 128, loss 1.05787, acc 0.703125, prec 0.0198447, recall 0.69697\n",
      "2017-12-07T23:26:26.667180: step 129, loss 1.67557, acc 0.625, prec 0.0197425, recall 0.69697\n",
      "2017-12-07T23:26:27.252854: step 130, loss 3.21523, acc 0.71875, prec 0.0196749, recall 0.686567\n",
      "2017-12-07T23:26:27.849413: step 131, loss 0.683284, acc 0.84375, prec 0.0196329, recall 0.686567\n",
      "2017-12-07T23:26:28.428413: step 132, loss 0.798089, acc 0.78125, prec 0.0195745, recall 0.686567\n",
      "2017-12-07T23:26:28.995507: step 133, loss 0.859377, acc 0.75, prec 0.0197159, recall 0.688889\n",
      "2017-12-07T23:26:29.552523: step 134, loss 10.4494, acc 0.75, prec 0.0196576, recall 0.678832\n",
      "2017-12-07T23:26:30.139227: step 135, loss 7.10761, acc 0.765625, prec 0.0200126, recall 0.678571\n",
      "2017-12-07T23:26:30.713558: step 136, loss 19.7815, acc 0.703125, prec 0.0201469, recall 0.671329\n",
      "2017-12-07T23:26:31.278581: step 137, loss 3.27907, acc 0.640625, prec 0.0200543, recall 0.666667\n",
      "2017-12-07T23:26:31.847527: step 138, loss 2.26351, acc 0.546875, prec 0.0199336, recall 0.666667\n",
      "2017-12-07T23:26:32.408859: step 139, loss 1.4163, acc 0.640625, prec 0.0198388, recall 0.666667\n",
      "2017-12-07T23:26:32.968067: step 140, loss 1.92464, acc 0.484375, prec 0.0199056, recall 0.668966\n",
      "2017-12-07T23:26:33.531904: step 141, loss 2.20378, acc 0.515625, prec 0.0199796, recall 0.671233\n",
      "2017-12-07T23:26:34.086697: step 142, loss 6.80307, acc 0.390625, prec 0.0200243, recall 0.668919\n",
      "2017-12-07T23:26:34.644629: step 143, loss 2.62427, acc 0.390625, prec 0.0200642, recall 0.671141\n",
      "2017-12-07T23:26:35.200714: step 144, loss 2.7354, acc 0.359375, prec 0.0199005, recall 0.671141\n",
      "2017-12-07T23:26:35.756079: step 145, loss 3.41544, acc 0.390625, prec 0.0197472, recall 0.671141\n",
      "2017-12-07T23:26:36.308499: step 146, loss 3.31921, acc 0.375, prec 0.0197845, recall 0.673333\n",
      "2017-12-07T23:26:36.851232: step 147, loss 4.21256, acc 0.375, prec 0.0196345, recall 0.668874\n",
      "2017-12-07T23:26:37.435590: step 148, loss 3.29669, acc 0.375, prec 0.019483, recall 0.668874\n",
      "2017-12-07T23:26:38.012945: step 149, loss 2.03809, acc 0.453125, prec 0.0193524, recall 0.668874\n",
      "2017-12-07T23:26:38.572092: step 150, loss 3.07865, acc 0.453125, prec 0.0195967, recall 0.673203\n",
      "2017-12-07T23:26:39.123330: step 151, loss 2.69437, acc 0.4375, prec 0.0196486, recall 0.675325\n",
      "2017-12-07T23:26:39.673939: step 152, loss 5.15293, acc 0.46875, prec 0.0195268, recall 0.670968\n",
      "2017-12-07T23:26:40.217253: step 153, loss 3.15906, acc 0.390625, prec 0.0193849, recall 0.670968\n",
      "2017-12-07T23:26:40.761918: step 154, loss 1.94669, acc 0.515625, prec 0.0192735, recall 0.670968\n",
      "2017-12-07T23:26:41.325652: step 155, loss 3.20616, acc 0.5, prec 0.0195212, recall 0.675159\n",
      "2017-12-07T23:26:41.874542: step 156, loss 2.20471, acc 0.453125, prec 0.0195756, recall 0.677215\n",
      "2017-12-07T23:26:42.421635: step 157, loss 3.00483, acc 0.296875, prec 0.0194157, recall 0.677215\n",
      "2017-12-07T23:26:42.964581: step 158, loss 23.4305, acc 0.59375, prec 0.0195087, recall 0.670807\n",
      "2017-12-07T23:26:43.517522: step 159, loss 1.83422, acc 0.59375, prec 0.0194175, recall 0.670807\n",
      "2017-12-07T23:26:44.074959: step 160, loss 1.99723, acc 0.59375, prec 0.0193271, recall 0.670807\n",
      "2017-12-07T23:26:44.621173: step 161, loss 4.81, acc 0.59375, prec 0.0195904, recall 0.670732\n",
      "2017-12-07T23:26:45.168746: step 162, loss 1.99927, acc 0.609375, prec 0.0196774, recall 0.672727\n",
      "2017-12-07T23:26:45.709225: step 163, loss 6.04741, acc 0.59375, prec 0.0195905, recall 0.668675\n",
      "2017-12-07T23:26:46.258075: step 164, loss 5.40825, acc 0.609375, prec 0.0195079, recall 0.664671\n",
      "2017-12-07T23:26:46.818457: step 165, loss 2.4144, acc 0.4375, prec 0.0195565, recall 0.666667\n",
      "2017-12-07T23:26:47.388181: step 166, loss 2.32331, acc 0.546875, prec 0.0196283, recall 0.668639\n",
      "2017-12-07T23:26:47.957167: step 167, loss 3.67064, acc 0.4375, prec 0.0196755, recall 0.670588\n",
      "2017-12-07T23:26:48.500052: step 168, loss 2.21287, acc 0.453125, prec 0.0197256, recall 0.672515\n",
      "2017-12-07T23:26:49.046966: step 169, loss 1.90466, acc 0.515625, prec 0.0196212, recall 0.672515\n",
      "2017-12-07T23:26:49.594057: step 170, loss 10.4809, acc 0.40625, prec 0.0198305, recall 0.672414\n",
      "2017-12-07T23:26:50.135719: step 171, loss 2.50228, acc 0.4375, prec 0.0198754, recall 0.674286\n",
      "2017-12-07T23:26:50.681042: step 172, loss 8.54428, acc 0.375, prec 0.0197456, recall 0.670455\n",
      "2017-12-07T23:26:51.244050: step 173, loss 2.87394, acc 0.453125, prec 0.0196307, recall 0.670455\n",
      "2017-12-07T23:26:51.797974: step 174, loss 3.04212, acc 0.453125, prec 0.0196792, recall 0.672316\n",
      "2017-12-07T23:26:52.339846: step 175, loss 2.41361, acc 0.4375, prec 0.019885, recall 0.675978\n",
      "2017-12-07T23:26:52.887069: step 176, loss 2.07181, acc 0.515625, prec 0.0197842, recall 0.675978\n",
      "2017-12-07T23:26:53.435479: step 177, loss 1.75381, acc 0.5625, prec 0.019694, recall 0.675978\n",
      "2017-12-07T23:26:53.980181: step 178, loss 1.92838, acc 0.640625, prec 0.0199384, recall 0.679558\n",
      "2017-12-07T23:26:54.512190: step 179, loss 1.56714, acc 0.59375, prec 0.0200129, recall 0.681319\n",
      "2017-12-07T23:26:55.045156: step 180, loss 2.40003, acc 0.515625, prec 0.0200706, recall 0.68306\n",
      "2017-12-07T23:26:55.576753: step 181, loss 1.86172, acc 0.453125, prec 0.0199585, recall 0.68306\n",
      "2017-12-07T23:26:56.118668: step 182, loss 1.40029, acc 0.640625, prec 0.0198855, recall 0.68306\n",
      "2017-12-07T23:26:56.655093: step 183, loss 1.30808, acc 0.71875, prec 0.0198287, recall 0.68306\n",
      "2017-12-07T23:26:57.204471: step 184, loss 0.877228, acc 0.75, prec 0.0197785, recall 0.68306\n",
      "2017-12-07T23:26:57.790297: step 185, loss 0.867792, acc 0.75, prec 0.0197285, recall 0.68306\n",
      "2017-12-07T23:26:58.340333: step 186, loss 4.38218, acc 0.8125, prec 0.0196943, recall 0.679348\n",
      "2017-12-07T23:26:58.925338: step 187, loss 1.31155, acc 0.78125, prec 0.0198051, recall 0.681081\n",
      "2017-12-07T23:26:59.485908: step 188, loss 5.87641, acc 0.84375, prec 0.0199341, recall 0.675532\n",
      "2017-12-07T23:27:00.060557: step 189, loss 7.22493, acc 0.78125, prec 0.0198935, recall 0.671958\n",
      "2017-12-07T23:27:00.622447: step 190, loss 7.20453, acc 0.75, prec 0.02, recall 0.670157\n",
      "2017-12-07T23:27:01.165376: step 191, loss 0.970418, acc 0.71875, prec 0.0199439, recall 0.670157\n",
      "2017-12-07T23:27:01.704737: step 192, loss 1.74117, acc 0.59375, prec 0.0198634, recall 0.670157\n",
      "2017-12-07T23:27:02.250895: step 193, loss 4.39508, acc 0.5625, prec 0.019932, recall 0.668394\n",
      "2017-12-07T23:27:02.787985: step 194, loss 2.31507, acc 0.53125, prec 0.01984, recall 0.668394\n",
      "2017-12-07T23:27:03.325789: step 195, loss 2.4046, acc 0.40625, prec 0.0198746, recall 0.670103\n",
      "2017-12-07T23:27:03.866886: step 196, loss 2.66475, acc 0.4375, prec 0.0199149, recall 0.671795\n",
      "2017-12-07T23:27:04.393727: step 197, loss 2.44662, acc 0.453125, prec 0.0198095, recall 0.671795\n",
      "2017-12-07T23:27:04.921844: step 198, loss 1.6757, acc 0.484375, prec 0.0197111, recall 0.671795\n",
      "2017-12-07T23:27:05.449557: step 199, loss 2.23233, acc 0.484375, prec 0.0196137, recall 0.671795\n",
      "2017-12-07T23:27:05.976057: step 200, loss 6.89518, acc 0.5, prec 0.0198153, recall 0.671717\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512707113/checkpoints/model-200\n",
      "\n",
      "2017-12-07T23:27:07.286230: step 201, loss 4.0229, acc 0.5, prec 0.0197242, recall 0.668342\n",
      "2017-12-07T23:27:07.871686: step 202, loss 2.53449, acc 0.453125, prec 0.0196223, recall 0.668342\n",
      "2017-12-07T23:27:08.431568: step 203, loss 3.35001, acc 0.40625, prec 0.0196567, recall 0.67\n",
      "2017-12-07T23:27:08.965115: step 204, loss 2.62948, acc 0.4375, prec 0.0195535, recall 0.67\n",
      "2017-12-07T23:27:09.552900: step 205, loss 3.40921, acc 0.421875, prec 0.019733, recall 0.673267\n",
      "2017-12-07T23:27:10.094153: step 206, loss 2.77104, acc 0.484375, prec 0.019639, recall 0.673267\n",
      "2017-12-07T23:27:10.628100: step 207, loss 2.93138, acc 0.421875, prec 0.0198162, recall 0.676471\n",
      "2017-12-07T23:27:11.160812: step 208, loss 2.56777, acc 0.421875, prec 0.0197115, recall 0.676471\n",
      "2017-12-07T23:27:11.698009: step 209, loss 2.88791, acc 0.46875, prec 0.0196162, recall 0.676471\n",
      "2017-12-07T23:27:12.229710: step 210, loss 2.08178, acc 0.453125, prec 0.0197964, recall 0.679612\n",
      "2017-12-07T23:27:12.764710: step 211, loss 1.70137, acc 0.59375, prec 0.0197239, recall 0.679612\n",
      "2017-12-07T23:27:13.307253: step 212, loss 11.1541, acc 0.53125, prec 0.0196436, recall 0.676328\n",
      "2017-12-07T23:27:13.856001: step 213, loss 1.99888, acc 0.59375, prec 0.0195722, recall 0.676328\n",
      "2017-12-07T23:27:14.396555: step 214, loss 2.05825, acc 0.5625, prec 0.0194959, recall 0.676328\n",
      "2017-12-07T23:27:14.938933: step 215, loss 21.3284, acc 0.59375, prec 0.019567, recall 0.671429\n",
      "2017-12-07T23:27:15.475444: step 216, loss 1.85033, acc 0.546875, prec 0.0194886, recall 0.671429\n",
      "2017-12-07T23:27:16.013250: step 217, loss 1.33243, acc 0.65625, prec 0.0195646, recall 0.672986\n",
      "2017-12-07T23:27:16.546691: step 218, loss 11.7701, acc 0.515625, prec 0.0196213, recall 0.668224\n",
      "2017-12-07T23:27:17.084832: step 219, loss 1.58488, acc 0.625, prec 0.019691, recall 0.669767\n",
      "2017-12-07T23:27:17.658237: step 220, loss 12.4769, acc 0.625, prec 0.0196292, recall 0.666667\n",
      "2017-12-07T23:27:18.211309: step 221, loss 5.60775, acc 0.46875, prec 0.0195413, recall 0.663594\n",
      "2017-12-07T23:27:18.759507: step 222, loss 2.48149, acc 0.453125, prec 0.0195814, recall 0.665138\n",
      "2017-12-07T23:27:19.304693: step 223, loss 2.60917, acc 0.375, prec 0.0194762, recall 0.665138\n",
      "2017-12-07T23:27:19.876942: step 224, loss 14.0787, acc 0.375, prec 0.0193747, recall 0.6621\n",
      "2017-12-07T23:27:20.488709: step 225, loss 5.92562, acc 0.375, prec 0.0192742, recall 0.659091\n",
      "2017-12-07T23:27:21.107339: step 226, loss 3.64998, acc 0.265625, prec 0.0192841, recall 0.660634\n",
      "2017-12-07T23:27:21.732068: step 227, loss 3.46576, acc 0.328125, prec 0.0191752, recall 0.660634\n",
      "2017-12-07T23:27:22.304110: step 228, loss 4.0048, acc 0.328125, prec 0.0191956, recall 0.662162\n",
      "2017-12-07T23:27:22.907640: step 229, loss 3.5299, acc 0.265625, prec 0.0192058, recall 0.663677\n",
      "2017-12-07T23:27:23.510239: step 230, loss 3.39431, acc 0.328125, prec 0.0190992, recall 0.663677\n",
      "2017-12-07T23:27:24.145019: step 231, loss 4.13389, acc 0.21875, prec 0.0189768, recall 0.663677\n",
      "2017-12-07T23:27:24.781851: step 232, loss 4.33692, acc 0.390625, prec 0.0192577, recall 0.668142\n",
      "2017-12-07T23:27:25.408782: step 233, loss 3.17666, acc 0.484375, prec 0.0195506, recall 0.672489\n",
      "2017-12-07T23:27:26.030447: step 234, loss 3.17749, acc 0.390625, prec 0.0195781, recall 0.673913\n",
      "2017-12-07T23:27:26.685280: step 235, loss 3.10322, acc 0.40625, prec 0.0194846, recall 0.673913\n",
      "2017-12-07T23:27:27.327440: step 236, loss 2.5664, acc 0.4375, prec 0.0193968, recall 0.673913\n",
      "2017-12-07T23:27:28.018152: step 237, loss 3.00731, acc 0.359375, prec 0.0192978, recall 0.673913\n",
      "2017-12-07T23:27:28.680157: step 238, loss 2.39817, acc 0.5, prec 0.0192212, recall 0.673913\n",
      "2017-12-07T23:27:29.331917: step 239, loss 1.81904, acc 0.515625, prec 0.0192688, recall 0.675325\n",
      "2017-12-07T23:27:29.972540: step 240, loss 1.30218, acc 0.640625, prec 0.019335, recall 0.676724\n",
      "2017-12-07T23:27:30.612458: step 241, loss 10.3431, acc 0.53125, prec 0.0192662, recall 0.67382\n",
      "2017-12-07T23:27:31.258463: step 242, loss 1.47993, acc 0.671875, prec 0.0192166, recall 0.67382\n",
      "2017-12-07T23:27:31.913940: step 243, loss 0.982191, acc 0.765625, prec 0.0191814, recall 0.67382\n",
      "2017-12-07T23:27:32.571665: step 244, loss 1.26899, acc 0.640625, prec 0.0191277, recall 0.67382\n",
      "2017-12-07T23:27:33.231757: step 245, loss 2.8918, acc 0.921875, prec 0.0192378, recall 0.67234\n",
      "2017-12-07T23:27:33.877923: step 246, loss 1.10968, acc 0.734375, prec 0.0193172, recall 0.673729\n",
      "2017-12-07T23:27:34.518989: step 247, loss 1.18822, acc 0.78125, prec 0.0195222, recall 0.676471\n",
      "2017-12-07T23:27:35.164087: step 248, loss 0.709038, acc 0.75, prec 0.0196031, recall 0.677824\n",
      "2017-12-07T23:27:35.817924: step 249, loss 7.19457, acc 0.8125, prec 0.019577, recall 0.675\n",
      "2017-12-07T23:27:36.454627: step 250, loss 0.674075, acc 0.75, prec 0.0196575, recall 0.676349\n",
      "2017-12-07T23:27:37.100924: step 251, loss 0.653726, acc 0.796875, prec 0.0196267, recall 0.676349\n",
      "2017-12-07T23:27:37.780511: step 252, loss 0.937667, acc 0.734375, prec 0.0195866, recall 0.676349\n",
      "2017-12-07T23:27:38.438715: step 253, loss 0.674883, acc 0.796875, prec 0.0195561, recall 0.676349\n",
      "2017-12-07T23:27:39.079261: step 254, loss 0.683401, acc 0.75, prec 0.0195186, recall 0.676349\n",
      "2017-12-07T23:27:39.737313: step 255, loss 0.524054, acc 0.765625, prec 0.0196008, recall 0.677686\n",
      "2017-12-07T23:27:40.322505: step 256, loss 1.50646, acc 0.78125, prec 0.019802, recall 0.680328\n",
      "2017-12-07T23:27:40.917654: step 257, loss 6.44705, acc 0.875, prec 0.0199023, recall 0.678862\n",
      "2017-12-07T23:27:41.512908: step 258, loss 4.19278, acc 0.734375, prec 0.0198644, recall 0.676113\n",
      "2017-12-07T23:27:42.109137: step 259, loss 1.06961, acc 0.6875, prec 0.0198173, recall 0.676113\n",
      "2017-12-07T23:27:42.700647: step 260, loss 1.37582, acc 0.65625, prec 0.0197657, recall 0.676113\n",
      "2017-12-07T23:27:43.293241: step 261, loss 5.06552, acc 0.71875, prec 0.019726, recall 0.673387\n",
      "2017-12-07T23:27:43.892931: step 262, loss 0.880457, acc 0.75, prec 0.0198043, recall 0.674699\n",
      "2017-12-07T23:27:44.484831: step 263, loss 0.927596, acc 0.6875, prec 0.0197577, recall 0.674699\n",
      "2017-12-07T23:27:45.069232: step 264, loss 1.98191, acc 0.5625, prec 0.0198078, recall 0.676\n",
      "2017-12-07T23:27:45.657132: step 265, loss 1.73154, acc 0.65625, prec 0.0198714, recall 0.677291\n",
      "2017-12-07T23:27:46.250298: step 266, loss 1.83347, acc 0.625, prec 0.0199301, recall 0.678571\n",
      "2017-12-07T23:27:46.874295: step 267, loss 16.8915, acc 0.625, prec 0.019993, recall 0.67451\n",
      "2017-12-07T23:27:47.456964: step 268, loss 1.9278, acc 0.625, prec 0.0199374, recall 0.67451\n",
      "2017-12-07T23:27:48.048910: step 269, loss 1.96006, acc 0.59375, prec 0.0198775, recall 0.67451\n",
      "2017-12-07T23:27:48.626439: step 270, loss 5.2715, acc 0.4375, prec 0.0197974, recall 0.671875\n",
      "2017-12-07T23:27:49.197338: step 271, loss 2.99215, acc 0.484375, prec 0.0198349, recall 0.673152\n",
      "2017-12-07T23:27:49.769313: step 272, loss 2.53376, acc 0.484375, prec 0.0200959, recall 0.676923\n",
      "2017-12-07T23:27:50.339729: step 273, loss 2.26682, acc 0.453125, prec 0.0202388, recall 0.679389\n",
      "2017-12-07T23:27:50.909538: step 274, loss 1.84788, acc 0.59375, prec 0.0201791, recall 0.679389\n",
      "2017-12-07T23:27:51.506278: step 275, loss 2.91908, acc 0.421875, prec 0.0204266, recall 0.683019\n",
      "2017-12-07T23:27:52.079853: step 276, loss 3.69616, acc 0.328125, prec 0.020438, recall 0.684211\n",
      "2017-12-07T23:27:52.652165: step 277, loss 2.65174, acc 0.4375, prec 0.0204652, recall 0.685393\n",
      "2017-12-07T23:27:53.228733: step 278, loss 2.26712, acc 0.5, prec 0.0205014, recall 0.686567\n",
      "2017-12-07T23:27:53.799427: step 279, loss 3.12328, acc 0.40625, prec 0.020415, recall 0.686567\n",
      "2017-12-07T23:27:54.365846: step 280, loss 2.3305, acc 0.5, prec 0.0203427, recall 0.686567\n",
      "2017-12-07T23:27:54.942458: step 281, loss 2.84308, acc 0.53125, prec 0.0205993, recall 0.690037\n",
      "2017-12-07T23:27:55.516042: step 282, loss 2.18829, acc 0.5625, prec 0.0205359, recall 0.690037\n",
      "2017-12-07T23:27:56.103844: step 283, loss 1.71892, acc 0.703125, prec 0.0207078, recall 0.692308\n",
      "2017-12-07T23:27:56.669022: step 284, loss 2.96878, acc 0.640625, prec 0.0209767, recall 0.695652\n",
      "2017-12-07T23:27:57.247969: step 285, loss 1.63447, acc 0.65625, prec 0.0209264, recall 0.695652\n",
      "2017-12-07T23:27:57.836621: step 286, loss 1.41267, acc 0.640625, prec 0.0208741, recall 0.695652\n",
      "2017-12-07T23:27:58.453990: step 287, loss 1.73373, acc 0.71875, prec 0.0209396, recall 0.696751\n",
      "2017-12-07T23:27:59.042987: step 288, loss 0.836746, acc 0.75, prec 0.0209033, recall 0.696751\n",
      "2017-12-07T23:27:59.636577: step 289, loss 1.38308, acc 0.734375, prec 0.0208649, recall 0.696751\n",
      "2017-12-07T23:28:00.260311: step 290, loss 2.26388, acc 0.734375, prec 0.0209345, recall 0.695341\n",
      "2017-12-07T23:28:00.854666: step 291, loss 3.29878, acc 0.78125, prec 0.0210107, recall 0.69395\n",
      "2017-12-07T23:28:01.455362: step 292, loss 0.965096, acc 0.765625, prec 0.0209768, recall 0.69395\n",
      "2017-12-07T23:28:02.057246: step 293, loss 4.46418, acc 0.828125, prec 0.0209542, recall 0.691489\n",
      "2017-12-07T23:28:02.656373: step 294, loss 3.81746, acc 0.6875, prec 0.0209115, recall 0.689046\n",
      "2017-12-07T23:28:03.241334: step 295, loss 2.9606, acc 0.609375, prec 0.0210673, recall 0.688811\n",
      "2017-12-07T23:28:03.836849: step 296, loss 1.13822, acc 0.671875, prec 0.0210201, recall 0.688811\n",
      "2017-12-07T23:28:04.412665: step 297, loss 2.61289, acc 0.515625, prec 0.0210549, recall 0.689895\n",
      "2017-12-07T23:28:04.988879: step 298, loss 1.59151, acc 0.53125, prec 0.0209879, recall 0.689895\n",
      "2017-12-07T23:28:05.566035: step 299, loss 1.75802, acc 0.578125, prec 0.020928, recall 0.689895\n",
      "2017-12-07T23:28:06.141473: step 300, loss 1.72959, acc 0.609375, prec 0.0208729, recall 0.689895\n",
      "\n",
      "Evaluation:\n",
      "2017-12-07T23:28:48.385357: step 300, loss 1.47463, acc 0.575863, prec 0.0235361, recall 0.772093\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512707113/checkpoints/model-300\n",
      "\n",
      "2017-12-07T23:28:50.693268: step 301, loss 2.27981, acc 0.5625, prec 0.0235585, recall 0.772622\n",
      "2017-12-07T23:28:51.231880: step 302, loss 3.17529, acc 0.640625, prec 0.0237288, recall 0.772414\n",
      "2017-12-07T23:28:51.741173: step 303, loss 7.9403, acc 0.46875, prec 0.0238112, recall 0.771689\n",
      "2017-12-07T23:28:52.321446: step 304, loss 2.23861, acc 0.5625, prec 0.023833, recall 0.77221\n",
      "2017-12-07T23:28:52.865534: step 305, loss 2.1097, acc 0.5, prec 0.0238479, recall 0.772727\n",
      "2017-12-07T23:28:53.427724: step 306, loss 1.73968, acc 0.578125, prec 0.0239395, recall 0.773756\n",
      "2017-12-07T23:28:53.976858: step 307, loss 2.96099, acc 0.453125, prec 0.0239492, recall 0.774266\n",
      "2017-12-07T23:28:54.543318: step 308, loss 2.80682, acc 0.484375, prec 0.0239621, recall 0.774775\n",
      "2017-12-07T23:28:55.109320: step 309, loss 2.26864, acc 0.53125, prec 0.0239121, recall 0.774775\n",
      "2017-12-07T23:28:55.674352: step 310, loss 2.33489, acc 0.5, prec 0.0238591, recall 0.774775\n",
      "2017-12-07T23:28:56.233629: step 311, loss 1.66032, acc 0.65625, prec 0.0238903, recall 0.775281\n",
      "2017-12-07T23:28:56.811064: step 312, loss 1.64951, acc 0.5, prec 0.0239049, recall 0.775785\n",
      "2017-12-07T23:28:57.390864: step 313, loss 1.36081, acc 0.578125, prec 0.0238604, recall 0.775785\n",
      "2017-12-07T23:28:57.987882: step 314, loss 1.51522, acc 0.59375, prec 0.0239521, recall 0.776786\n",
      "2017-12-07T23:28:58.603564: step 315, loss 1.17947, acc 0.625, prec 0.0239797, recall 0.777283\n",
      "2017-12-07T23:28:59.186305: step 316, loss 1.00786, acc 0.65625, prec 0.0239435, recall 0.777283\n",
      "2017-12-07T23:28:59.777912: step 317, loss 1.18024, acc 0.765625, prec 0.0239189, recall 0.777283\n",
      "2017-12-07T23:29:00.379709: step 318, loss 2.43596, acc 0.71875, prec 0.0239578, recall 0.776053\n",
      "2017-12-07T23:29:00.952944: step 319, loss 2.49006, acc 0.734375, prec 0.0239316, recall 0.774336\n",
      "2017-12-07T23:29:01.522597: step 320, loss 4.86357, acc 0.75, prec 0.0239071, recall 0.772627\n",
      "2017-12-07T23:29:02.102220: step 321, loss 0.70069, acc 0.78125, prec 0.0238843, recall 0.772627\n",
      "2017-12-07T23:29:02.686231: step 322, loss 0.874867, acc 0.75, prec 0.0238582, recall 0.772627\n",
      "2017-12-07T23:29:03.260659: step 323, loss 0.714084, acc 0.75, prec 0.0238322, recall 0.772627\n",
      "2017-12-07T23:29:03.839441: step 324, loss 0.722074, acc 0.75, prec 0.0238063, recall 0.772627\n",
      "2017-12-07T23:29:04.422673: step 325, loss 0.906421, acc 0.84375, prec 0.0238565, recall 0.773128\n",
      "2017-12-07T23:29:05.001488: step 326, loss 0.698934, acc 0.8125, prec 0.023837, recall 0.773128\n",
      "2017-12-07T23:29:05.593508: step 327, loss 0.764413, acc 0.734375, prec 0.0238757, recall 0.773626\n",
      "2017-12-07T23:29:06.169102: step 328, loss 1.04667, acc 0.703125, prec 0.0239111, recall 0.774123\n",
      "2017-12-07T23:29:06.750214: step 329, loss 0.534084, acc 0.8125, prec 0.0238917, recall 0.774123\n",
      "2017-12-07T23:29:07.332785: step 330, loss 0.703015, acc 0.8125, prec 0.0238723, recall 0.774123\n",
      "2017-12-07T23:29:07.904173: step 331, loss 0.749515, acc 0.765625, prec 0.0238481, recall 0.774123\n",
      "2017-12-07T23:29:08.516890: step 332, loss 16.2786, acc 0.78125, prec 0.0238288, recall 0.770742\n",
      "2017-12-07T23:29:09.104040: step 333, loss 2.13715, acc 0.828125, prec 0.0238127, recall 0.769063\n",
      "2017-12-07T23:29:09.680327: step 334, loss 1.33567, acc 0.671875, prec 0.023779, recall 0.769063\n",
      "2017-12-07T23:29:10.255269: step 335, loss 3.9955, acc 0.71875, prec 0.0237519, recall 0.767391\n",
      "2017-12-07T23:29:10.824427: step 336, loss 5.20747, acc 0.71875, prec 0.0237263, recall 0.764069\n",
      "2017-12-07T23:29:11.397939: step 337, loss 2.06194, acc 0.5625, prec 0.0237472, recall 0.764579\n",
      "2017-12-07T23:29:11.971926: step 338, loss 1.77425, acc 0.625, prec 0.0238398, recall 0.765591\n",
      "2017-12-07T23:29:12.547004: step 339, loss 3.90823, acc 0.609375, prec 0.0238016, recall 0.763949\n",
      "2017-12-07T23:29:13.122566: step 340, loss 1.90845, acc 0.53125, prec 0.0237539, recall 0.763949\n",
      "2017-12-07T23:29:13.698033: step 341, loss 3.79475, acc 0.515625, prec 0.0237065, recall 0.762313\n",
      "2017-12-07T23:29:14.301743: step 342, loss 5.13323, acc 0.390625, prec 0.0237115, recall 0.761194\n",
      "2017-12-07T23:29:14.854418: step 343, loss 3.41461, acc 0.5, prec 0.02392, recall 0.763214\n",
      "2017-12-07T23:29:15.411627: step 344, loss 3.61887, acc 0.25, prec 0.0238441, recall 0.763214\n",
      "2017-12-07T23:29:15.966509: step 345, loss 4.15278, acc 0.390625, prec 0.0238472, recall 0.763713\n",
      "2017-12-07T23:29:16.511500: step 346, loss 3.09226, acc 0.421875, prec 0.0237892, recall 0.763713\n",
      "2017-12-07T23:29:17.055513: step 347, loss 3.25792, acc 0.390625, prec 0.0238563, recall 0.764706\n",
      "2017-12-07T23:29:17.603127: step 348, loss 3.47541, acc 0.4375, prec 0.023864, recall 0.765199\n",
      "2017-12-07T23:29:18.171991: step 349, loss 3.38359, acc 0.40625, prec 0.0238049, recall 0.765199\n",
      "2017-12-07T23:29:18.735158: step 350, loss 3.25882, acc 0.3125, prec 0.0238637, recall 0.76618\n",
      "2017-12-07T23:29:19.291041: step 351, loss 2.82572, acc 0.40625, prec 0.0238049, recall 0.76618\n",
      "2017-12-07T23:29:19.851298: step 352, loss 3.22358, acc 0.375, prec 0.0237433, recall 0.76618\n",
      "2017-12-07T23:29:20.415231: step 353, loss 7.88808, acc 0.453125, prec 0.0237542, recall 0.765073\n",
      "2017-12-07T23:29:20.986948: step 354, loss 2.01509, acc 0.46875, prec 0.0237022, recall 0.765073\n",
      "2017-12-07T23:29:21.561527: step 355, loss 3.07377, acc 0.421875, prec 0.0237086, recall 0.76556\n",
      "2017-12-07T23:29:22.140481: step 356, loss 3.60998, acc 0.578125, prec 0.0237301, recall 0.766046\n",
      "2017-12-07T23:29:22.714892: step 357, loss 1.55248, acc 0.59375, prec 0.0237531, recall 0.766529\n",
      "2017-12-07T23:29:23.290838: step 358, loss 2.12115, acc 0.546875, prec 0.0237091, recall 0.766529\n",
      "2017-12-07T23:29:23.867824: step 359, loss 1.85418, acc 0.625, prec 0.0237974, recall 0.76749\n",
      "2017-12-07T23:29:24.445587: step 360, loss 2.16671, acc 0.5625, prec 0.0238793, recall 0.768443\n",
      "2017-12-07T23:29:25.015804: step 361, loss 0.859866, acc 0.78125, prec 0.023858, recall 0.768443\n",
      "2017-12-07T23:29:25.583052: step 362, loss 1.69649, acc 0.65625, prec 0.0238247, recall 0.768443\n",
      "2017-12-07T23:29:26.142186: step 363, loss 1.51111, acc 0.625, prec 0.0237884, recall 0.768443\n",
      "2017-12-07T23:29:26.701360: step 364, loss 1.08978, acc 0.734375, prec 0.0237628, recall 0.768443\n",
      "2017-12-07T23:29:27.266682: step 365, loss 0.600841, acc 0.8125, prec 0.0237447, recall 0.768443\n",
      "2017-12-07T23:29:27.819137: step 366, loss 0.791485, acc 0.796875, prec 0.0237252, recall 0.768443\n",
      "2017-12-07T23:29:28.401874: step 367, loss 0.367604, acc 0.875, prec 0.0237132, recall 0.768443\n",
      "2017-12-07T23:29:28.958058: step 368, loss 2.72557, acc 0.859375, prec 0.0237629, recall 0.767347\n",
      "2017-12-07T23:29:29.518886: step 369, loss 0.655221, acc 0.828125, prec 0.023808, recall 0.767821\n",
      "2017-12-07T23:29:30.087510: step 370, loss 2.20044, acc 0.8125, prec 0.0237915, recall 0.76626\n",
      "2017-12-07T23:29:30.646612: step 371, loss 0.840187, acc 0.765625, prec 0.023769, recall 0.76626\n",
      "2017-12-07T23:29:31.207217: step 372, loss 0.36293, acc 0.859375, prec 0.023817, recall 0.766734\n",
      "2017-12-07T23:29:31.769782: step 373, loss 0.429067, acc 0.859375, prec 0.0238035, recall 0.766734\n",
      "2017-12-07T23:29:32.336189: step 374, loss 4.3611, acc 0.859375, prec 0.0237915, recall 0.765182\n",
      "2017-12-07T23:29:32.946911: step 375, loss 9.43893, acc 0.8125, prec 0.0237766, recall 0.762097\n",
      "2017-12-07T23:29:33.518476: step 376, loss 0.596843, acc 0.765625, prec 0.0237542, recall 0.762097\n",
      "2017-12-07T23:29:34.083726: step 377, loss 6.19805, acc 0.859375, prec 0.0237422, recall 0.760563\n",
      "2017-12-07T23:29:34.650334: step 378, loss 1.00108, acc 0.75, prec 0.0237184, recall 0.760563\n",
      "2017-12-07T23:29:35.234774: step 379, loss 0.926989, acc 0.734375, prec 0.0236931, recall 0.760563\n",
      "2017-12-07T23:29:35.810504: step 380, loss 1.45166, acc 0.625, prec 0.0237186, recall 0.761044\n",
      "2017-12-07T23:29:36.372542: step 381, loss 1.62514, acc 0.578125, prec 0.0236786, recall 0.761044\n",
      "2017-12-07T23:29:36.943379: step 382, loss 2.19836, acc 0.609375, prec 0.0236417, recall 0.761044\n",
      "2017-12-07T23:29:37.525029: step 383, loss 1.86961, acc 0.609375, prec 0.0236049, recall 0.761044\n",
      "2017-12-07T23:29:38.118182: step 384, loss 2.32339, acc 0.578125, prec 0.023626, recall 0.761523\n",
      "2017-12-07T23:29:38.693084: step 385, loss 1.80705, acc 0.671875, prec 0.0236558, recall 0.762\n",
      "2017-12-07T23:29:39.268061: step 386, loss 2.23148, acc 0.546875, prec 0.0236738, recall 0.762475\n",
      "2017-12-07T23:29:39.840933: step 387, loss 1.97993, acc 0.515625, prec 0.0236284, recall 0.762475\n",
      "2017-12-07T23:29:40.413277: step 388, loss 1.6625, acc 0.53125, prec 0.0236449, recall 0.762948\n",
      "2017-12-07T23:29:40.964538: step 389, loss 1.21683, acc 0.625, prec 0.0236701, recall 0.76342\n",
      "2017-12-07T23:29:41.520764: step 390, loss 1.62681, acc 0.625, prec 0.0236351, recall 0.76342\n",
      "2017-12-07T23:29:42.063544: step 391, loss 1.38962, acc 0.703125, prec 0.0237876, recall 0.764822\n",
      "2017-12-07T23:29:42.611225: step 392, loss 1.0819, acc 0.75, prec 0.0237642, recall 0.764822\n",
      "2017-12-07T23:29:43.162208: step 393, loss 1.19453, acc 0.78125, prec 0.0238636, recall 0.765748\n",
      "2017-12-07T23:29:43.712605: step 394, loss 1.03293, acc 0.703125, prec 0.0238358, recall 0.765748\n",
      "2017-12-07T23:29:44.260805: step 395, loss 0.876993, acc 0.796875, prec 0.0238168, recall 0.765748\n",
      "2017-12-07T23:29:44.812846: step 396, loss 2.01284, acc 0.75, prec 0.0239129, recall 0.766667\n",
      "2017-12-07T23:29:45.370330: step 397, loss 0.666647, acc 0.8125, prec 0.0238954, recall 0.766667\n",
      "2017-12-07T23:29:45.924163: step 398, loss 0.971438, acc 0.6875, prec 0.0238662, recall 0.766667\n",
      "2017-12-07T23:29:46.479442: step 399, loss 0.705426, acc 0.796875, prec 0.0239068, recall 0.767123\n",
      "2017-12-07T23:29:47.031050: step 400, loss 12.6262, acc 0.703125, prec 0.0239401, recall 0.766082\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512707113/checkpoints/model-400\n",
      "\n",
      "2017-12-07T23:29:48.446079: step 401, loss 3.13569, acc 0.796875, prec 0.0241602, recall 0.766409\n",
      "2017-12-07T23:29:49.021525: step 402, loss 1.7345, acc 0.875, prec 0.0242671, recall 0.767308\n",
      "2017-12-07T23:29:49.552212: step 403, loss 1.3463, acc 0.75, prec 0.0242435, recall 0.767308\n",
      "2017-12-07T23:29:50.080960: step 404, loss 7.19715, acc 0.75, prec 0.0242215, recall 0.765835\n",
      "2017-12-07T23:29:50.616038: step 405, loss 1.56635, acc 0.671875, prec 0.0242498, recall 0.766284\n",
      "2017-12-07T23:29:51.162424: step 406, loss 2.32964, acc 0.53125, prec 0.0242057, recall 0.766284\n",
      "2017-12-07T23:29:51.718192: step 407, loss 5.06751, acc 0.4375, prec 0.0242135, recall 0.765267\n",
      "2017-12-07T23:29:52.277992: step 408, loss 1.9693, acc 0.6875, prec 0.0242432, recall 0.765714\n",
      "2017-12-07T23:29:52.831298: step 409, loss 1.95931, acc 0.578125, prec 0.0243212, recall 0.766603\n",
      "2017-12-07T23:29:53.380967: step 410, loss 1.27383, acc 0.640625, prec 0.0244049, recall 0.767486\n",
      "2017-12-07T23:29:53.932211: step 411, loss 1.92294, acc 0.625, prec 0.0243697, recall 0.767486\n",
      "2017-12-07T23:29:54.486474: step 412, loss 1.84776, acc 0.640625, prec 0.0243361, recall 0.767486\n",
      "2017-12-07T23:29:55.037541: step 413, loss 2.34996, acc 0.515625, prec 0.0243494, recall 0.767925\n",
      "2017-12-07T23:29:55.581766: step 414, loss 2.61844, acc 0.578125, prec 0.0243684, recall 0.768362\n",
      "2017-12-07T23:29:56.144726: step 415, loss 4.2394, acc 0.46875, prec 0.0243205, recall 0.766917\n",
      "2017-12-07T23:29:56.719330: step 416, loss 1.64727, acc 0.640625, prec 0.0243452, recall 0.767355\n",
      "2017-12-07T23:29:57.295467: step 417, loss 1.76431, acc 0.65625, prec 0.0243134, recall 0.767355\n",
      "2017-12-07T23:29:57.871943: step 418, loss 2.13582, acc 0.59375, prec 0.0242759, recall 0.767355\n",
      "2017-12-07T23:29:58.483944: step 419, loss 1.95708, acc 0.578125, prec 0.024237, recall 0.767355\n",
      "2017-12-07T23:29:59.054939: step 420, loss 1.479, acc 0.640625, prec 0.0242618, recall 0.76779\n",
      "2017-12-07T23:29:59.622950: step 421, loss 1.85897, acc 0.609375, prec 0.024226, recall 0.76779\n",
      "2017-12-07T23:30:00.212479: step 422, loss 2.07287, acc 0.5625, prec 0.0241859, recall 0.76779\n",
      "2017-12-07T23:30:00.872802: step 423, loss 1.82046, acc 0.546875, prec 0.0241446, recall 0.76779\n",
      "2017-12-07T23:30:01.535868: step 424, loss 1.98181, acc 0.59375, prec 0.0241651, recall 0.768224\n",
      "2017-12-07T23:30:02.161360: step 425, loss 1.8944, acc 0.6875, prec 0.0241367, recall 0.768224\n",
      "2017-12-07T23:30:02.737740: step 426, loss 2.47728, acc 0.796875, prec 0.024177, recall 0.767225\n",
      "2017-12-07T23:30:03.336633: step 427, loss 12.5227, acc 0.78125, prec 0.0241586, recall 0.765799\n",
      "2017-12-07T23:30:03.967858: step 428, loss 5.7054, acc 0.703125, prec 0.0241331, recall 0.764378\n",
      "2017-12-07T23:30:04.554348: step 429, loss 1.05094, acc 0.78125, prec 0.0241133, recall 0.764378\n",
      "2017-12-07T23:30:05.206578: step 430, loss 1.23284, acc 0.78125, prec 0.0241506, recall 0.764815\n",
      "2017-12-07T23:30:05.850453: step 431, loss 0.597458, acc 0.8125, prec 0.0242477, recall 0.765683\n",
      "2017-12-07T23:30:06.466891: step 432, loss 11.4618, acc 0.71875, prec 0.0242251, recall 0.762868\n",
      "2017-12-07T23:30:07.052869: step 433, loss 0.85356, acc 0.703125, prec 0.0241983, recall 0.762868\n",
      "2017-12-07T23:30:07.685234: step 434, loss 1.1256, acc 0.703125, prec 0.0241715, recall 0.762868\n",
      "2017-12-07T23:30:08.341139: step 435, loss 1.47978, acc 0.640625, prec 0.0242526, recall 0.763736\n",
      "2017-12-07T23:30:09.057142: step 436, loss 4.96365, acc 0.59375, prec 0.0242174, recall 0.76234\n",
      "2017-12-07T23:30:09.706159: step 437, loss 8.77866, acc 0.59375, prec 0.0242389, recall 0.761384\n",
      "2017-12-07T23:30:10.397008: step 438, loss 2.55397, acc 0.6875, prec 0.0243239, recall 0.76225\n",
      "2017-12-07T23:30:11.041286: step 439, loss 1.61071, acc 0.5625, prec 0.0242845, recall 0.76225\n",
      "2017-12-07T23:30:11.696178: step 440, loss 1.70502, acc 0.546875, prec 0.0242438, recall 0.76225\n",
      "2017-12-07T23:30:12.373450: step 441, loss 1.88217, acc 0.640625, prec 0.0243242, recall 0.76311\n",
      "2017-12-07T23:30:13.046130: step 442, loss 4.61153, acc 0.34375, prec 0.0242668, recall 0.761733\n",
      "2017-12-07T23:30:13.716826: step 443, loss 2.23641, acc 0.484375, prec 0.0242769, recall 0.762162\n",
      "2017-12-07T23:30:14.403270: step 444, loss 1.91483, acc 0.59375, prec 0.0242966, recall 0.76259\n",
      "2017-12-07T23:30:15.083832: step 445, loss 1.68244, acc 0.59375, prec 0.0242605, recall 0.76259\n",
      "2017-12-07T23:30:15.751654: step 446, loss 6.43164, acc 0.546875, prec 0.0243331, recall 0.762075\n",
      "2017-12-07T23:30:16.382758: step 447, loss 2.34445, acc 0.546875, prec 0.0244042, recall 0.762923\n",
      "2017-12-07T23:30:17.045744: step 448, loss 2.48854, acc 0.453125, prec 0.0243555, recall 0.762923\n",
      "2017-12-07T23:30:17.691828: step 449, loss 2.3418, acc 0.578125, prec 0.024429, recall 0.763766\n",
      "2017-12-07T23:30:18.397757: step 450, loss 2.07065, acc 0.53125, prec 0.0243875, recall 0.763766\n",
      "2017-12-07T23:30:19.126266: step 451, loss 2.36717, acc 0.640625, prec 0.0244662, recall 0.764602\n",
      "2017-12-07T23:30:19.812410: step 452, loss 2.15034, acc 0.625, prec 0.0244882, recall 0.765018\n",
      "2017-12-07T23:30:20.439247: step 453, loss 2.07678, acc 0.546875, prec 0.0245032, recall 0.765432\n",
      "2017-12-07T23:30:21.093839: step 454, loss 1.95332, acc 0.5625, prec 0.0245745, recall 0.766257\n",
      "2017-12-07T23:30:21.744770: step 455, loss 2.73068, acc 0.625, prec 0.0245975, recall 0.765324\n",
      "2017-12-07T23:30:22.370888: step 456, loss 1.38005, acc 0.625, prec 0.0246192, recall 0.765734\n",
      "2017-12-07T23:30:23.003057: step 457, loss 7.72979, acc 0.5625, prec 0.0245819, recall 0.764398\n",
      "2017-12-07T23:30:23.645571: step 458, loss 1.00888, acc 0.625, prec 0.0245488, recall 0.764398\n",
      "2017-12-07T23:30:24.276796: step 459, loss 1.61552, acc 0.640625, prec 0.0245718, recall 0.764808\n",
      "2017-12-07T23:30:24.942050: step 460, loss 1.21483, acc 0.703125, prec 0.0246548, recall 0.765625\n",
      "2017-12-07T23:30:25.639504: step 461, loss 10.0074, acc 0.59375, prec 0.0246204, recall 0.764298\n",
      "2017-12-07T23:30:26.311389: step 462, loss 4.83983, acc 0.609375, prec 0.0245888, recall 0.761658\n",
      "2017-12-07T23:30:26.953100: step 463, loss 1.75477, acc 0.6875, prec 0.0245614, recall 0.761658\n",
      "2017-12-07T23:30:27.612831: step 464, loss 5.85762, acc 0.59375, prec 0.0245273, recall 0.760345\n",
      "2017-12-07T23:30:28.318177: step 465, loss 15.174, acc 0.515625, prec 0.0245406, recall 0.75945\n",
      "2017-12-07T23:30:29.019802: step 466, loss 3.0119, acc 0.46875, prec 0.0245484, recall 0.759863\n",
      "2017-12-07T23:30:29.708575: step 467, loss 2.81703, acc 0.453125, prec 0.0245009, recall 0.759863\n",
      "2017-12-07T23:30:30.359141: step 468, loss 3.62617, acc 0.4375, prec 0.0245599, recall 0.760684\n",
      "2017-12-07T23:30:30.982354: step 469, loss 7.28129, acc 0.5, prec 0.0246254, recall 0.760204\n",
      "2017-12-07T23:30:31.607830: step 470, loss 3.7513, acc 0.40625, prec 0.0245739, recall 0.760204\n",
      "2017-12-07T23:30:32.259568: step 471, loss 2.76092, acc 0.40625, prec 0.0245227, recall 0.760204\n",
      "2017-12-07T23:30:32.894015: step 472, loss 4.126, acc 0.3125, prec 0.024517, recall 0.760611\n",
      "2017-12-07T23:30:33.562729: step 473, loss 4.39617, acc 0.328125, prec 0.0245127, recall 0.761017\n",
      "2017-12-07T23:30:34.224238: step 474, loss 3.53839, acc 0.234375, prec 0.0245005, recall 0.761421\n",
      "2017-12-07T23:30:34.868856: step 475, loss 4.04368, acc 0.296875, prec 0.0244936, recall 0.761824\n",
      "2017-12-07T23:30:35.494536: step 476, loss 3.62479, acc 0.4375, prec 0.0244986, recall 0.762226\n",
      "2017-12-07T23:30:36.166294: step 477, loss 2.73358, acc 0.453125, prec 0.0244523, recall 0.762226\n",
      "2017-12-07T23:30:36.803128: step 478, loss 2.71297, acc 0.484375, prec 0.0244614, recall 0.762626\n",
      "2017-12-07T23:30:37.415120: step 479, loss 2.31355, acc 0.46875, prec 0.0244165, recall 0.762626\n",
      "2017-12-07T23:30:38.024297: step 480, loss 2.04951, acc 0.671875, prec 0.024704, recall 0.765\n",
      "2017-12-07T23:30:38.685781: step 481, loss 1.95596, acc 0.53125, prec 0.0247166, recall 0.765391\n",
      "2017-12-07T23:30:39.313496: step 482, loss 1.69973, acc 0.625, prec 0.0247894, recall 0.766169\n",
      "2017-12-07T23:30:39.940712: step 483, loss 1.96097, acc 0.59375, prec 0.0247549, recall 0.766169\n",
      "2017-12-07T23:30:40.588157: step 484, loss 5.59048, acc 0.6875, prec 0.0247297, recall 0.764901\n",
      "2017-12-07T23:30:41.220143: step 485, loss 3.78365, acc 0.734375, prec 0.0247085, recall 0.763636\n",
      "2017-12-07T23:30:41.880647: step 486, loss 2.51415, acc 0.703125, prec 0.024789, recall 0.763158\n",
      "2017-12-07T23:30:42.591376: step 487, loss 1.39983, acc 0.671875, prec 0.0247612, recall 0.763158\n",
      "2017-12-07T23:30:43.313312: step 488, loss 1.19162, acc 0.640625, prec 0.0247308, recall 0.763158\n",
      "2017-12-07T23:30:44.015271: step 489, loss 1.46171, acc 0.65625, prec 0.0247019, recall 0.763158\n",
      "2017-12-07T23:30:44.723868: step 490, loss 2.42565, acc 0.734375, prec 0.0247833, recall 0.763934\n",
      "2017-12-07T23:30:45.486407: step 491, loss 1.24343, acc 0.625, prec 0.0248553, recall 0.764706\n",
      "2017-12-07T23:30:46.246202: step 492, loss 1.31158, acc 0.71875, prec 0.024935, recall 0.765472\n",
      "2017-12-07T23:30:46.994801: step 493, loss 1.53555, acc 0.625, prec 0.0249033, recall 0.765472\n",
      "2017-12-07T23:30:47.661663: step 494, loss 3.97476, acc 0.75, prec 0.0248835, recall 0.764228\n",
      "2017-12-07T23:30:48.375292: step 495, loss 1.37832, acc 0.640625, prec 0.0248533, recall 0.764228\n",
      "2017-12-07T23:30:49.216597: step 496, loss 2.85518, acc 0.71875, prec 0.0248825, recall 0.763371\n",
      "2017-12-07T23:30:49.854196: step 497, loss 13.2973, acc 0.519231, prec 0.0248509, recall 0.762136\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "model = BasicTextCNN(sequence_length=x_train.shape[1],\n",
    "            vocab_processor=vocab_processor, num_epochs=1, evaluate_every=300)\n",
    "model.train_network(x_train, y_train, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
