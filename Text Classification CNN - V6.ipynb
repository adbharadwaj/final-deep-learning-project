{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from BasicTextCNN import BasicTextCNN\n",
    "from PositionTextCNN import PositionTextCNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)\n",
    "\n",
    "def compute_distance_embedding(word, x):\n",
    "    word_distances = np.zeros(x.shape, dtype='int')\n",
    "    for i in range(x.shape[0]):\n",
    "        word_positions = np.where(x[i] == word)[0]\n",
    "        for j in range(x.shape[1]):\n",
    "            if len(word_positions) > 0:\n",
    "                word_position = word_positions[np.argmin(np.abs(word_positions - j))]\n",
    "                word_distances[i][j] = word_position - j\n",
    "                if word_distances[i][j]<0:\n",
    "                    word_distances[i][j] = 600+word_distances[i][j]\n",
    "            else:\n",
    "                word_distances[i][j] = 299\n",
    "    return word_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 53)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedPathwayA, encodedPathwayB = list(vocab_processor.transform(['pathwayA pathwayB']))[0][:2]\n",
    "encodedPathwayA, encodedPathwayB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_distancesA = compute_distance_embedding(encodedPathwayA, x)\n",
    "word_distancesB = compute_distance_embedding(encodedPathwayB, x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 => Train/Dev split: 31795/10599\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/hist is illegal; using word_embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/sparsity is illegal; using word_embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512753526\n",
      "\n",
      "Start training\n",
      "2017-12-08T12:18:49.036681: step 1, loss 3.91717, acc 0.3125, prec 0, recall 0\n",
      "2017-12-08T12:18:49.570427: step 2, loss 0.967082, acc 0.71875, prec 0, recall 0\n",
      "2017-12-08T12:18:50.110394: step 3, loss 5.08619, acc 0.84375, prec 0, recall 0\n",
      "2017-12-08T12:18:50.661911: step 4, loss 0.186819, acc 0.921875, prec 0, recall 0\n",
      "2017-12-08T12:18:51.193354: step 5, loss 11.3692, acc 0.921875, prec 0, recall 0\n",
      "2017-12-08T12:18:51.736621: step 6, loss 67.7221, acc 0.9375, prec 0, recall 0\n",
      "2017-12-08T12:18:52.277950: step 7, loss 0.101954, acc 0.9375, prec 0, recall 0\n",
      "2017-12-08T12:18:52.840066: step 8, loss 11.0737, acc 0.765625, prec 0, recall 0\n",
      "2017-12-08T12:18:53.439207: step 9, loss 0.939082, acc 0.734375, prec 0.00854701, recall 0.142857\n",
      "2017-12-08T12:18:54.000739: step 10, loss 1.72932, acc 0.6875, prec 0.0144928, recall 0.25\n",
      "2017-12-08T12:18:54.605052: step 11, loss 2.04936, acc 0.546875, prec 0.011976, recall 0.25\n",
      "2017-12-08T12:18:55.159572: step 12, loss 8.90945, acc 0.390625, prec 0.0097561, recall 0.222222\n",
      "2017-12-08T12:18:55.742651: step 13, loss 2.26756, acc 0.5, prec 0.00843882, recall 0.222222\n",
      "2017-12-08T12:18:56.340804: step 14, loss 4.22677, acc 0.328125, prec 0.0106762, recall 0.3\n",
      "2017-12-08T12:18:56.896128: step 15, loss 7.46018, acc 0.34375, prec 0.0123839, recall 0.333333\n",
      "2017-12-08T12:18:57.491647: step 16, loss 11.5611, acc 0.28125, prec 0.0108696, recall 0.307692\n",
      "2017-12-08T12:18:58.110459: step 17, loss 4.84902, acc 0.1875, prec 0.00952381, recall 0.307692\n",
      "2017-12-08T12:18:58.710923: step 18, loss 10.7028, acc 0.25, prec 0.0106838, recall 0.333333\n",
      "2017-12-08T12:18:59.253557: step 19, loss 4.26856, acc 0.1875, prec 0.00961538, recall 0.333333\n",
      "2017-12-08T12:18:59.807189: step 20, loss 12.1919, acc 0.265625, prec 0.0123239, recall 0.388889\n",
      "2017-12-08T12:19:00.373070: step 21, loss 5.98068, acc 0.125, prec 0.0112179, recall 0.388889\n",
      "2017-12-08T12:19:00.989986: step 22, loss 5.16919, acc 0.25, prec 0.0104167, recall 0.388889\n",
      "2017-12-08T12:19:01.547551: step 23, loss 4.36873, acc 0.296875, prec 0.0111421, recall 0.421053\n",
      "2017-12-08T12:19:02.108866: step 24, loss 4.80194, acc 0.234375, prec 0.0104302, recall 0.421053\n",
      "2017-12-08T12:19:02.659478: step 25, loss 3.89747, acc 0.265625, prec 0.00982801, recall 0.421053\n",
      "2017-12-08T12:19:03.227153: step 26, loss 3.79346, acc 0.28125, prec 0.00930233, recall 0.421053\n",
      "2017-12-08T12:19:03.803163: step 27, loss 3.20229, acc 0.40625, prec 0.00890869, recall 0.421053\n",
      "2017-12-08T12:19:04.355175: step 28, loss 2.54649, acc 0.515625, prec 0.00861141, recall 0.421053\n",
      "2017-12-08T12:19:04.918910: step 29, loss 2.08172, acc 0.5625, prec 0.00835946, recall 0.421053\n",
      "2017-12-08T12:19:05.481506: step 30, loss 6.39097, acc 0.671875, prec 0.00818833, recall 0.4\n",
      "2017-12-08T12:19:06.076110: step 31, loss 2.1298, acc 0.5625, prec 0.00894632, recall 0.428571\n",
      "2017-12-08T12:19:06.643201: step 32, loss 1.63025, acc 0.6875, prec 0.0097371, recall 0.454545\n",
      "2017-12-08T12:19:07.218859: step 33, loss 21.3379, acc 0.90625, prec 0.0106486, recall 0.458333\n",
      "2017-12-08T12:19:07.787228: step 34, loss 15.1236, acc 0.671875, prec 0.0104463, recall 0.44\n",
      "2017-12-08T12:19:08.361277: step 35, loss 1.10077, acc 0.796875, prec 0.0112465, recall 0.461538\n",
      "2017-12-08T12:19:08.932350: step 36, loss 1.70542, acc 0.640625, prec 0.0110092, recall 0.461538\n",
      "2017-12-08T12:19:09.508472: step 37, loss 1.52116, acc 0.6875, prec 0.0125899, recall 0.5\n",
      "2017-12-08T12:19:10.069688: step 38, loss 13.772, acc 0.75, prec 0.0124224, recall 0.482759\n",
      "2017-12-08T12:19:10.640926: step 39, loss 7.20101, acc 0.671875, prec 0.0122058, recall 0.466667\n",
      "2017-12-08T12:19:11.253557: step 40, loss 1.73093, acc 0.71875, prec 0.0145548, recall 0.515152\n",
      "2017-12-08T12:19:11.819153: step 41, loss 10.3261, acc 0.5625, prec 0.0142259, recall 0.5\n",
      "2017-12-08T12:19:12.430914: step 42, loss 1.94637, acc 0.5625, prec 0.0139002, recall 0.5\n",
      "2017-12-08T12:19:13.008507: step 43, loss 2.98312, acc 0.484375, prec 0.013535, recall 0.5\n",
      "2017-12-08T12:19:13.597802: step 44, loss 3.23231, acc 0.40625, prec 0.0138996, recall 0.514286\n",
      "2017-12-08T12:19:14.169387: step 45, loss 3.09776, acc 0.4375, prec 0.0135237, recall 0.514286\n",
      "2017-12-08T12:19:14.739050: step 46, loss 3.42066, acc 0.453125, prec 0.013899, recall 0.527778\n",
      "2017-12-08T12:19:15.320408: step 47, loss 2.29987, acc 0.515625, prec 0.015, recall 0.552632\n",
      "2017-12-08T12:19:15.929934: step 48, loss 2.79037, acc 0.4375, prec 0.0173611, recall 0.595238\n",
      "2017-12-08T12:19:16.549619: step 49, loss 1.89417, acc 0.609375, prec 0.0170648, recall 0.595238\n",
      "2017-12-08T12:19:17.127156: step 50, loss 2.55693, acc 0.53125, prec 0.0173797, recall 0.604651\n",
      "2017-12-08T12:19:17.716051: step 51, loss 2.70525, acc 0.484375, prec 0.0176471, recall 0.613636\n",
      "2017-12-08T12:19:18.295291: step 52, loss 2.14726, acc 0.546875, prec 0.0173188, recall 0.613636\n",
      "2017-12-08T12:19:18.886876: step 53, loss 1.33013, acc 0.703125, prec 0.0171103, recall 0.613636\n",
      "2017-12-08T12:19:19.476408: step 54, loss 3.4094, acc 0.671875, prec 0.0168961, recall 0.6\n",
      "2017-12-08T12:19:20.055874: step 55, loss 2.15414, acc 0.59375, prec 0.0172308, recall 0.608696\n",
      "2017-12-08T12:19:20.648742: step 56, loss 1.24918, acc 0.734375, prec 0.0170524, recall 0.608696\n",
      "2017-12-08T12:19:21.223109: step 57, loss 0.985883, acc 0.828125, prec 0.0169389, recall 0.608696\n",
      "2017-12-08T12:19:21.840369: step 58, loss 0.694989, acc 0.8125, prec 0.017407, recall 0.617021\n",
      "2017-12-08T12:19:22.422787: step 59, loss 8.24921, acc 0.75, prec 0.0172619, recall 0.591837\n",
      "2017-12-08T12:19:23.002163: step 60, loss 1.2543, acc 0.71875, prec 0.0176574, recall 0.6\n",
      "2017-12-08T12:19:23.595043: step 61, loss 21.1688, acc 0.75, prec 0.0175131, recall 0.576923\n",
      "2017-12-08T12:19:24.179954: step 62, loss 0.610523, acc 0.8125, prec 0.0173913, recall 0.576923\n",
      "2017-12-08T12:19:24.762970: step 63, loss 1.49302, acc 0.765625, prec 0.0189329, recall 0.6\n",
      "2017-12-08T12:19:25.344631: step 64, loss 1.52503, acc 0.734375, prec 0.01875, recall 0.6\n",
      "2017-12-08T12:19:25.918211: step 65, loss 1.4586, acc 0.71875, prec 0.0185602, recall 0.6\n",
      "2017-12-08T12:19:26.499128: step 66, loss 1.97623, acc 0.59375, prec 0.0182927, recall 0.6\n",
      "2017-12-08T12:19:27.119952: step 67, loss 2.20293, acc 0.640625, prec 0.0180624, recall 0.6\n",
      "2017-12-08T12:19:27.747856: step 68, loss 1.77691, acc 0.640625, prec 0.0178378, recall 0.6\n",
      "2017-12-08T12:19:28.327380: step 69, loss 4.92655, acc 0.59375, prec 0.0186468, recall 0.603448\n",
      "2017-12-08T12:19:28.908838: step 70, loss 20.8239, acc 0.515625, prec 0.0183727, recall 0.57377\n",
      "2017-12-08T12:19:29.497259: step 71, loss 3.16091, acc 0.453125, prec 0.0180412, recall 0.57377\n",
      "2017-12-08T12:19:30.082640: step 72, loss 9.21149, acc 0.484375, prec 0.0177485, recall 0.564516\n",
      "2017-12-08T12:19:30.662025: step 73, loss 3.37211, acc 0.390625, prec 0.0178926, recall 0.571429\n",
      "2017-12-08T12:19:31.237040: step 74, loss 4.32481, acc 0.34375, prec 0.0180049, recall 0.578125\n",
      "2017-12-08T12:19:31.829115: step 75, loss 4.53488, acc 0.265625, prec 0.0176023, recall 0.578125\n",
      "2017-12-08T12:19:32.417768: step 76, loss 4.62246, acc 0.34375, prec 0.0172575, recall 0.578125\n",
      "2017-12-08T12:19:33.020495: step 77, loss 4.09269, acc 0.359375, prec 0.0169336, recall 0.578125\n",
      "2017-12-08T12:19:33.645305: step 78, loss 4.30809, acc 0.3125, prec 0.017481, recall 0.590909\n",
      "2017-12-08T12:19:34.236041: step 79, loss 5.36421, acc 0.21875, prec 0.0175285, recall 0.597015\n",
      "2017-12-08T12:19:34.821426: step 80, loss 3.59119, acc 0.34375, prec 0.0176344, recall 0.602941\n",
      "2017-12-08T12:19:35.410103: step 81, loss 3.09998, acc 0.484375, prec 0.0178042, recall 0.608696\n",
      "2017-12-08T12:19:35.983524: step 82, loss 2.76243, acc 0.515625, prec 0.0179841, recall 0.614286\n",
      "2017-12-08T12:19:36.574603: step 83, loss 2.98561, acc 0.5625, prec 0.0181818, recall 0.619718\n",
      "2017-12-08T12:19:37.152394: step 84, loss 2.00487, acc 0.625, prec 0.0180033, recall 0.619718\n",
      "2017-12-08T12:19:37.741370: step 85, loss 1.9617, acc 0.546875, prec 0.0177922, recall 0.619718\n",
      "2017-12-08T12:19:38.365023: step 86, loss 0.892592, acc 0.796875, prec 0.0184887, recall 0.630137\n",
      "2017-12-08T12:19:38.951733: step 87, loss 1.03454, acc 0.765625, prec 0.0183779, recall 0.630137\n",
      "2017-12-08T12:19:39.538352: step 88, loss 1.39331, acc 0.78125, prec 0.0190552, recall 0.64\n",
      "2017-12-08T12:19:40.124607: step 89, loss 1.04975, acc 0.734375, prec 0.0189274, recall 0.64\n",
      "2017-12-08T12:19:40.702722: step 90, loss 0.709873, acc 0.828125, prec 0.0188457, recall 0.64\n",
      "2017-12-08T12:19:41.284917: step 91, loss 0.587035, acc 0.796875, prec 0.01875, recall 0.64\n",
      "2017-12-08T12:19:41.863614: step 92, loss 0.785081, acc 0.796875, prec 0.0190365, recall 0.644737\n",
      "2017-12-08T12:19:42.453392: step 93, loss 15.341, acc 0.8125, prec 0.0189628, recall 0.628205\n",
      "2017-12-08T12:19:43.043738: step 94, loss 3.89488, acc 0.84375, prec 0.018897, recall 0.620253\n",
      "2017-12-08T12:19:43.696974: step 95, loss 12.5826, acc 0.75, prec 0.0187955, recall 0.604938\n",
      "2017-12-08T12:19:44.283182: step 96, loss 0.541016, acc 0.8125, prec 0.0187094, recall 0.604938\n",
      "2017-12-08T12:19:44.924864: step 97, loss 3.12659, acc 0.78125, prec 0.018617, recall 0.597561\n",
      "2017-12-08T12:19:45.512561: step 98, loss 1.38788, acc 0.734375, prec 0.0188679, recall 0.60241\n",
      "2017-12-08T12:19:46.084539: step 99, loss 1.0756, acc 0.78125, prec 0.019137, recall 0.607143\n",
      "2017-12-08T12:19:46.665177: step 100, loss 2.29734, acc 0.5625, prec 0.018938, recall 0.607143\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512753526/checkpoints/model-100\n",
      "\n",
      "2017-12-08T12:19:48.031402: step 101, loss 1.79613, acc 0.671875, prec 0.0191529, recall 0.611765\n",
      "2017-12-08T12:19:48.627767: step 102, loss 2.23457, acc 0.640625, prec 0.0193501, recall 0.616279\n",
      "2017-12-08T12:19:49.218335: step 103, loss 2.30914, acc 0.640625, prec 0.019189, recall 0.616279\n",
      "2017-12-08T12:19:49.790441: step 104, loss 2.86167, acc 0.46875, prec 0.0193064, recall 0.62069\n",
      "2017-12-08T12:19:50.385320: step 105, loss 1.88193, acc 0.625, prec 0.0194897, recall 0.625\n",
      "2017-12-08T12:19:50.951346: step 106, loss 6.74442, acc 0.546875, prec 0.0192982, recall 0.617977\n",
      "2017-12-08T12:19:51.526626: step 107, loss 2.89526, acc 0.484375, prec 0.0190773, recall 0.617977\n",
      "2017-12-08T12:19:52.094406: step 108, loss 2.32055, acc 0.609375, prec 0.0199244, recall 0.630435\n",
      "2017-12-08T12:19:52.662345: step 109, loss 3.3798, acc 0.4375, prec 0.019681, recall 0.630435\n",
      "2017-12-08T12:19:53.224603: step 110, loss 3.04676, acc 0.421875, prec 0.019437, recall 0.630435\n",
      "2017-12-08T12:19:53.845832: step 111, loss 2.17799, acc 0.640625, prec 0.0192883, recall 0.630435\n",
      "2017-12-08T12:19:54.453950: step 112, loss 1.82618, acc 0.5625, prec 0.0197563, recall 0.638298\n",
      "2017-12-08T12:19:55.022057: step 113, loss 1.86273, acc 0.640625, prec 0.0196078, recall 0.638298\n",
      "2017-12-08T12:19:55.607457: step 114, loss 1.91368, acc 0.65625, prec 0.0197859, recall 0.642105\n",
      "2017-12-08T12:19:56.197834: step 115, loss 1.60592, acc 0.671875, prec 0.0196521, recall 0.642105\n",
      "2017-12-08T12:19:56.786400: step 116, loss 23.5895, acc 0.671875, prec 0.0198463, recall 0.632653\n",
      "2017-12-08T12:19:57.372025: step 117, loss 2.1573, acc 0.640625, prec 0.0200127, recall 0.636364\n",
      "2017-12-08T12:19:57.961177: step 118, loss 1.40848, acc 0.765625, prec 0.0202276, recall 0.64\n",
      "2017-12-08T12:19:58.554169: step 119, loss 7.08527, acc 0.734375, prec 0.0204338, recall 0.637255\n",
      "2017-12-08T12:19:59.202007: step 120, loss 2.04819, acc 0.703125, prec 0.0206186, recall 0.640777\n",
      "2017-12-08T12:19:59.938350: step 121, loss 1.69597, acc 0.71875, prec 0.0205033, recall 0.640777\n",
      "2017-12-08T12:20:00.618417: step 122, loss 2.5622, acc 0.671875, prec 0.0203704, recall 0.640777\n",
      "2017-12-08T12:20:01.271653: step 123, loss 1.52811, acc 0.6875, prec 0.0208461, recall 0.647619\n",
      "2017-12-08T12:20:01.958085: step 124, loss 1.35402, acc 0.765625, prec 0.021348, recall 0.654206\n",
      "2017-12-08T12:20:02.657703: step 125, loss 9.36456, acc 0.578125, prec 0.02118, recall 0.648148\n",
      "2017-12-08T12:20:03.290489: step 126, loss 1.82643, acc 0.671875, prec 0.0210463, recall 0.648148\n",
      "2017-12-08T12:20:03.996279: step 127, loss 9.51652, acc 0.59375, prec 0.0208893, recall 0.642202\n",
      "2017-12-08T12:20:04.680117: step 128, loss 2.87187, acc 0.546875, prec 0.0209997, recall 0.645455\n",
      "2017-12-08T12:20:05.358479: step 129, loss 2.4994, acc 0.53125, prec 0.020815, recall 0.645455\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9317c04934fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             vocab_processor=vocab_processor, num_epochs=1, evaluate_every=300, results_dir='fold%s'%k)\n\u001b[1;32m     20\u001b[0m     model.train_network(x_train, y_train, x_dev, y_dev, \n\u001b[0;32m---> 21\u001b[0;31m                         train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB)\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/PositionTextCNN.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, x_train, y_train, x_dev, y_dev, train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_distancesB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/PositionTextCNN.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB)\u001b[0m\n\u001b[1;32m    226\u001b[0m         _, step, summaries, loss, accuracy, precision, recall = self.sess.run(\n\u001b[1;32m    227\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adb/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating folds\n",
    "kf = KFold(n_splits=4, random_state=5, shuffle=True)\n",
    "for k, (train_index, test_index) in enumerate(kf.split(x, y)):\n",
    "# for train_index, test_index in kf.split(x):\n",
    "#     print(\"Fold: %s =>\" % k,  \"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_dev = x[train_index], x[test_index]\n",
    "    y_train, y_dev = y[train_index], y[test_index]\n",
    "    \n",
    "    train_word_distancesA = word_distancesA[train_index]\n",
    "    train_word_distancesB = word_distancesB[train_index]\n",
    "    \n",
    "    test_word_distancesA = word_distancesA[test_index]\n",
    "    test_word_distancesB = word_distancesB[test_index]\n",
    "    \n",
    "    print(\"Fold: %s =>\" % k, \"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    \n",
    "    model = PositionTextCNN(sequence_length=x_train.shape[1],\n",
    "            vocab_processor=vocab_processor, num_epochs=1, evaluate_every=300, results_dir='fold%s'%k)\n",
    "    model.train_network(x_train, y_train, x_dev, y_dev, \n",
    "                        train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "one_hot_encoding = tf.one_hot(list(range(8)), 8)\n",
    "sess.run(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
