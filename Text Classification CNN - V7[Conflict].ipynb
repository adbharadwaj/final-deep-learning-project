{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from BasicTextCNN import BasicTextCNN\n",
    "from PositionTextCNN import PositionTextCNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)\n",
    "\n",
    "def compute_distance_embedding(word, x):\n",
    "    word_distances = np.zeros(x.shape, dtype='int')\n",
    "    for i in range(x.shape[0]):\n",
    "        word_positions = np.where(x[i] == word)[0]\n",
    "        for j in range(x.shape[1]):\n",
    "            if len(word_positions) > 0:\n",
    "                word_position = word_positions[np.argmin(np.abs(word_positions - j))]\n",
    "                word_distances[i][j] = word_position - j\n",
    "                if word_distances[i][j]<0:\n",
    "                    word_distances[i][j] = 600+word_distances[i][j]\n",
    "            else:\n",
    "                word_distances[i][j] = 299\n",
    "    return word_distances\n",
    "\n",
    "def compute_pos_embedding(data, vocab_processor):\n",
    "    pos_emebedding = np.zeros(data.shape, dtype='int')\n",
    "    for i in range(data.shape[0]):\n",
    "        tags = pos_tag(word_tokenize(list(vocab_processor.reverse([data[i]]))[0].replace('<UNK>', 'XXX')))\n",
    "        for j in range(data.shape[1]):\n",
    "            if tags[j][1].lower() in pos_map:\n",
    "                pos_emebedding[i][j] = pos_map[tags[j][1].lower()]\n",
    "            else:\n",
    "                pos_emebedding[i][j] = 6\n",
    "    return pos_emebedding\n",
    "\n",
    "def load_pos_embedding():\n",
    "    return np.load('pos_emebedding.npy')\n",
    "\n",
    "def load_pos_mapping():\n",
    "    pos_map = {}\n",
    "    with open('pos-mapping.txt', 'r') as f:\n",
    "        for lines in f.readlines():\n",
    "            pos, num = lines.split()\n",
    "            pos_map[pos] = num\n",
    "    return pos_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos_map = load_pos_mapping()\n",
    "# %time pos_emebedding = compute_pos_embedding(x, vocab_processor)\n",
    "# np.save('pos_emebedding.npy', pos_emebedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 53)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedPathwayA, encodedPathwayB = list(vocab_processor.transform(['pathwayA pathwayB']))[0][:2]\n",
    "encodedPathwayA, encodedPathwayB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_distancesA = compute_distance_embedding(encodedPathwayA, x)\n",
    "word_distancesB = compute_distance_embedding(encodedPathwayB, x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42394, 273)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding = load_pos_embedding()\n",
    "pos_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 => Train/Dev split: 31795/10599\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/hist is illegal; using word_embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/sparsity is illegal; using word_embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512787555\n",
      "\n",
      "Start training\n",
      "2017-12-08T21:45:58.444165: step 1, loss 0.285854, acc 0.921875, prec 0, recall 0\n",
      "2017-12-08T21:45:59.058020: step 2, loss 0.0648554, acc 0.984375, prec 0, recall 0\n",
      "2017-12-08T21:45:59.667540: step 3, loss 12.1334, acc 0.984375, prec 0, recall 0\n",
      "2017-12-08T21:46:00.273421: step 4, loss 0.000144284, acc 1, prec 0, recall 0\n",
      "2017-12-08T21:46:00.873238: step 5, loss 27.8499, acc 0.984375, prec 0, recall 0\n",
      "2017-12-08T21:46:01.469586: step 6, loss 0.0718376, acc 0.984375, prec 0, recall 0\n",
      "2017-12-08T21:46:02.087959: step 7, loss 0.00925774, acc 1, prec 0, recall 0\n",
      "2017-12-08T21:46:02.711378: step 8, loss 27.5791, acc 0.984375, prec 0, recall 0\n",
      "2017-12-08T21:46:03.352804: step 9, loss 9.95655, acc 0.984375, prec 0, recall 0\n",
      "2017-12-08T21:46:04.029814: step 10, loss 9.97675, acc 0.984375, prec 0, recall 0\n",
      "2017-12-08T21:46:04.667735: step 11, loss 23.8744, acc 0.953125, prec 0, recall 0\n",
      "2017-12-08T21:46:05.326375: step 12, loss 0.114287, acc 0.96875, prec 0, recall 0\n",
      "2017-12-08T21:46:06.050326: step 13, loss 0.226028, acc 0.890625, prec 0, recall 0\n",
      "2017-12-08T21:46:06.701168: step 14, loss 15.0072, acc 0.796875, prec 0.0333333, recall 0.111111\n",
      "2017-12-08T21:46:07.313702: step 15, loss 1.00025, acc 0.765625, prec 0.0434783, recall 0.2\n",
      "2017-12-08T21:46:07.969060: step 16, loss 2.58744, acc 0.5625, prec 0.04, recall 0.272727\n",
      "2017-12-08T21:46:08.645141: step 17, loss 1.77275, acc 0.578125, prec 0.0294118, recall 0.272727\n",
      "2017-12-08T21:46:09.254311: step 18, loss 2.42088, acc 0.375, prec 0.0347222, recall 0.384615\n",
      "2017-12-08T21:46:09.876107: step 19, loss 3.66661, acc 0.296875, prec 0.026455, recall 0.384615\n",
      "2017-12-08T21:46:10.486665: step 20, loss 4.75535, acc 0.203125, prec 0.0208333, recall 0.384615\n",
      "2017-12-08T21:46:11.101570: step 21, loss 13.44, acc 0.1875, prec 0.0171821, recall 0.357143\n",
      "2017-12-08T21:46:11.728887: step 22, loss 7.61181, acc 0.265625, prec 0.0148368, recall 0.333333\n",
      "2017-12-08T21:46:12.338751: step 23, loss 17.2196, acc 0.21875, prec 0.0129534, recall 0.3125\n",
      "2017-12-08T21:46:12.954821: step 24, loss 6.22462, acc 0.15625, prec 0.0113636, recall 0.3125\n",
      "2017-12-08T21:46:13.540498: step 25, loss 5.56748, acc 0.125, prec 0.0100806, recall 0.3125\n",
      "2017-12-08T21:46:14.126647: step 26, loss 6.07079, acc 0.203125, prec 0.0109489, recall 0.352941\n",
      "2017-12-08T21:46:14.740791: step 27, loss 6.38041, acc 0.125, prec 0.00993378, recall 0.352941\n",
      "2017-12-08T21:46:15.333779: step 28, loss 6.59132, acc 0.140625, prec 0.0091047, recall 0.352941\n",
      "2017-12-08T21:46:15.927233: step 29, loss 6.22106, acc 0.171875, prec 0.00981767, recall 0.388889\n",
      "2017-12-08T21:46:16.518052: step 30, loss 4.17683, acc 0.328125, prec 0.010568, recall 0.421053\n",
      "2017-12-08T21:46:17.116250: step 31, loss 3.95626, acc 0.359375, prec 0.0112641, recall 0.45\n",
      "2017-12-08T21:46:17.722880: step 32, loss 3.15909, acc 0.40625, prec 0.0107527, recall 0.45\n",
      "2017-12-08T21:46:18.333479: step 33, loss 4.17013, acc 0.46875, prec 0.0126147, recall 0.478261\n",
      "2017-12-08T21:46:18.943169: step 34, loss 1.8003, acc 0.609375, prec 0.0122631, recall 0.478261\n",
      "2017-12-08T21:46:19.549489: step 35, loss 1.39757, acc 0.6875, prec 0.0130719, recall 0.5\n",
      "2017-12-08T21:46:20.148094: step 36, loss 2.28961, acc 0.71875, prec 0.0138741, recall 0.52\n",
      "2017-12-08T21:46:20.753782: step 37, loss 1.34801, acc 0.734375, prec 0.0136268, recall 0.52\n",
      "2017-12-08T21:46:21.367701: step 38, loss 8.02176, acc 0.6875, prec 0.0133607, recall 0.5\n",
      "2017-12-08T21:46:21.983374: step 39, loss 12.2158, acc 0.6875, prec 0.0131048, recall 0.481481\n",
      "2017-12-08T21:46:22.649246: step 40, loss 13.9814, acc 0.84375, prec 0.0139721, recall 0.482759\n",
      "2017-12-08T21:46:23.317141: step 41, loss 9.64466, acc 0.625, prec 0.0136585, recall 0.466667\n",
      "2017-12-08T21:46:24.007911: step 42, loss 3.09308, acc 0.71875, prec 0.0134357, recall 0.451613\n",
      "2017-12-08T21:46:24.688685: step 43, loss 2.57538, acc 0.640625, prec 0.0131579, recall 0.4375\n",
      "2017-12-08T21:46:25.422821: step 44, loss 1.765, acc 0.546875, prec 0.0128088, recall 0.4375\n",
      "2017-12-08T21:46:26.139888: step 45, loss 2.64579, acc 0.484375, prec 0.0124334, recall 0.4375\n",
      "2017-12-08T21:46:26.788553: step 46, loss 3.79029, acc 0.53125, prec 0.0129758, recall 0.441176\n",
      "2017-12-08T21:46:27.427836: step 47, loss 2.00007, acc 0.453125, prec 0.0125945, recall 0.441176\n",
      "2017-12-08T21:46:28.053908: step 48, loss 2.26599, acc 0.484375, prec 0.0138662, recall 0.472222\n",
      "2017-12-08T21:46:28.728244: step 49, loss 3.12259, acc 0.421875, prec 0.01346, recall 0.472222\n",
      "2017-12-08T21:46:29.310182: step 50, loss 3.12504, acc 0.453125, prec 0.0138568, recall 0.486486\n",
      "2017-12-08T21:46:29.922031: step 51, loss 5.40173, acc 0.359375, prec 0.0134429, recall 0.473684\n",
      "2017-12-08T21:46:30.526989: step 52, loss 3.0192, acc 0.375, prec 0.0137681, recall 0.487179\n",
      "2017-12-08T21:46:31.136256: step 53, loss 3.60764, acc 0.46875, prec 0.0134466, recall 0.475\n",
      "2017-12-08T21:46:31.774925: step 54, loss 2.21454, acc 0.515625, prec 0.0138408, recall 0.487805\n",
      "2017-12-08T21:46:32.382760: step 55, loss 2.05502, acc 0.484375, prec 0.0135318, recall 0.487805\n",
      "2017-12-08T21:46:33.051583: step 56, loss 1.83168, acc 0.515625, prec 0.0139073, recall 0.5\n",
      "2017-12-08T21:46:33.660348: step 57, loss 2.37248, acc 0.5625, prec 0.014295, recall 0.511628\n",
      "2017-12-08T21:46:34.273148: step 58, loss 1.22165, acc 0.671875, prec 0.0141026, recall 0.511628\n",
      "2017-12-08T21:46:34.891089: step 59, loss 2.2743, acc 0.75, prec 0.0152091, recall 0.533333\n",
      "2017-12-08T21:46:35.508461: step 60, loss 11.3899, acc 0.71875, prec 0.015047, recall 0.521739\n",
      "2017-12-08T21:46:36.168068: step 61, loss 1.46486, acc 0.65625, prec 0.0148423, recall 0.521739\n",
      "2017-12-08T21:46:36.820545: step 62, loss 3.36544, acc 0.71875, prec 0.0146879, recall 0.510638\n",
      "2017-12-08T21:46:37.478911: step 63, loss 11.7577, acc 0.703125, prec 0.0145278, recall 0.5\n",
      "2017-12-08T21:46:38.113495: step 64, loss 2.48135, acc 0.734375, prec 0.0155689, recall 0.509804\n",
      "2017-12-08T21:46:38.768522: step 65, loss 1.93099, acc 0.609375, prec 0.0159198, recall 0.519231\n",
      "2017-12-08T21:46:39.393262: step 66, loss 2.61145, acc 0.515625, prec 0.0162037, recall 0.528302\n",
      "2017-12-08T21:46:40.068559: step 67, loss 1.98457, acc 0.484375, prec 0.0159001, recall 0.528302\n",
      "2017-12-08T21:46:40.719624: step 68, loss 2.32941, acc 0.5625, prec 0.0156512, recall 0.528302\n",
      "2017-12-08T21:46:41.359004: step 69, loss 3.75289, acc 0.53125, prec 0.0154015, recall 0.518519\n",
      "2017-12-08T21:46:41.970637: step 70, loss 1.97439, acc 0.546875, prec 0.0151597, recall 0.518519\n",
      "2017-12-08T21:46:42.591788: step 71, loss 6.97613, acc 0.53125, prec 0.0149254, recall 0.509091\n",
      "2017-12-08T21:46:43.205393: step 72, loss 2.36909, acc 0.5, prec 0.0146751, recall 0.509091\n",
      "2017-12-08T21:46:43.905737: step 73, loss 2.08492, acc 0.515625, prec 0.0144404, recall 0.509091\n",
      "2017-12-08T21:46:44.621042: step 74, loss 1.64973, acc 0.609375, prec 0.0147583, recall 0.517857\n",
      "2017-12-08T21:46:45.291479: step 75, loss 2.11774, acc 0.609375, prec 0.0150678, recall 0.526316\n",
      "2017-12-08T21:46:45.950771: step 76, loss 2.11399, acc 0.609375, prec 0.014881, recall 0.526316\n",
      "2017-12-08T21:46:46.642022: step 77, loss 8.11033, acc 0.65625, prec 0.015211, recall 0.525424\n",
      "2017-12-08T21:46:47.334826: step 78, loss 2.1316, acc 0.59375, prec 0.0150194, recall 0.525424\n",
      "2017-12-08T21:46:47.957613: step 79, loss 2.54063, acc 0.546875, prec 0.0152818, recall 0.533333\n",
      "2017-12-08T21:46:48.662286: step 80, loss 3.02374, acc 0.625, prec 0.0174282, recall 0.569231\n",
      "2017-12-08T21:46:49.372162: step 81, loss 1.54146, acc 0.65625, prec 0.0177074, recall 0.575758\n",
      "2017-12-08T21:46:50.161727: step 82, loss 1.47506, acc 0.640625, prec 0.0184247, recall 0.588235\n",
      "2017-12-08T21:46:50.980909: step 83, loss 1.94634, acc 0.59375, prec 0.0186533, recall 0.594203\n",
      "2017-12-08T21:46:51.763084: step 84, loss 1.3879, acc 0.6875, prec 0.0198109, recall 0.611111\n",
      "2017-12-08T21:46:52.460583: step 85, loss 1.47458, acc 0.640625, prec 0.0200445, recall 0.616438\n",
      "2017-12-08T21:46:53.141937: step 86, loss 7.15263, acc 0.546875, prec 0.0202376, recall 0.605263\n",
      "2017-12-08T21:46:53.838377: step 87, loss 1.55749, acc 0.59375, prec 0.0208605, recall 0.615385\n",
      "2017-12-08T21:46:54.546400: step 88, loss 1.19057, acc 0.71875, prec 0.0211207, recall 0.620253\n",
      "2017-12-08T21:46:55.240234: step 89, loss 1.84847, acc 0.640625, prec 0.0209134, recall 0.620253\n",
      "2017-12-08T21:46:55.922141: step 90, loss 11.8871, acc 0.734375, prec 0.0211864, recall 0.617284\n",
      "2017-12-08T21:46:56.625551: step 91, loss 1.09577, acc 0.640625, prec 0.020982, recall 0.617284\n",
      "2017-12-08T21:46:57.298967: step 92, loss 2.02414, acc 0.59375, prec 0.0211618, recall 0.621951\n",
      "2017-12-08T21:46:57.980359: step 93, loss 2.27946, acc 0.59375, prec 0.0217391, recall 0.630952\n",
      "2017-12-08T21:46:58.659188: step 94, loss 2.02261, acc 0.59375, prec 0.0219067, recall 0.635294\n",
      "2017-12-08T21:46:59.353068: step 95, loss 1.49425, acc 0.640625, prec 0.02249, recall 0.643678\n",
      "2017-12-08T21:47:00.022393: step 96, loss 2.42214, acc 0.578125, prec 0.022637, recall 0.647727\n",
      "2017-12-08T21:47:00.707259: step 97, loss 1.46056, acc 0.625, prec 0.0224233, recall 0.647727\n",
      "2017-12-08T21:47:01.409120: step 98, loss 2.0615, acc 0.703125, prec 0.0226386, recall 0.651685\n",
      "2017-12-08T21:47:02.091356: step 99, loss 1.70214, acc 0.703125, prec 0.0224719, recall 0.651685\n",
      "2017-12-08T21:47:02.819659: step 100, loss 1.98983, acc 0.640625, prec 0.0226488, recall 0.655556\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512787555/checkpoints/model-100\n",
      "\n",
      "2017-12-08T21:47:04.332859: step 101, loss 0.836284, acc 0.734375, prec 0.0225019, recall 0.655556\n",
      "2017-12-08T21:47:04.998609: step 102, loss 1.3513, acc 0.78125, prec 0.0223824, recall 0.655556\n",
      "2017-12-08T21:47:05.666706: step 103, loss 29.4752, acc 0.6875, prec 0.0226074, recall 0.638298\n",
      "2017-12-08T21:47:06.336104: step 104, loss 2.20842, acc 0.78125, prec 0.0228636, recall 0.635417\n",
      "2017-12-08T21:47:07.005669: step 105, loss 1.22061, acc 0.640625, prec 0.0226682, recall 0.635417\n",
      "2017-12-08T21:47:07.671969: step 106, loss 1.87967, acc 0.546875, prec 0.0224265, recall 0.635417\n",
      "2017-12-08T21:47:08.337309: step 107, loss 2.87505, acc 0.484375, prec 0.0221576, recall 0.635417\n",
      "2017-12-08T21:47:09.059158: step 108, loss 3.21539, acc 0.5, prec 0.022605, recall 0.642857\n",
      "2017-12-08T21:47:09.744345: step 109, loss 3.26852, acc 0.375, prec 0.0226308, recall 0.646465\n",
      "2017-12-08T21:47:10.417484: step 110, loss 3.85342, acc 0.359375, prec 0.0226481, recall 0.65\n",
      "2017-12-08T21:47:11.084599: step 111, loss 2.02026, acc 0.46875, prec 0.0227194, recall 0.653465\n",
      "2017-12-08T21:47:11.782580: step 112, loss 2.46642, acc 0.46875, prec 0.0227891, recall 0.656863\n",
      "2017-12-08T21:47:12.452109: step 113, loss 2.37363, acc 0.5, prec 0.0232011, recall 0.663462\n",
      "2017-12-08T21:47:13.169728: step 114, loss 4.4884, acc 0.46875, prec 0.0229465, recall 0.657143\n",
      "2017-12-08T21:47:13.846305: step 115, loss 2.59567, acc 0.5625, prec 0.0233783, recall 0.663551\n",
      "2017-12-08T21:47:14.527660: step 116, loss 2.06858, acc 0.484375, prec 0.023127, recall 0.663551\n",
      "2017-12-08T21:47:15.207843: step 117, loss 1.59249, acc 0.671875, prec 0.0229699, recall 0.663551\n",
      "2017-12-08T21:47:15.889275: step 118, loss 1.05354, acc 0.703125, prec 0.0234576, recall 0.669725\n",
      "2017-12-08T21:47:16.565935: step 119, loss 0.866682, acc 0.734375, prec 0.0233301, recall 0.669725\n",
      "2017-12-08T21:47:17.248435: step 120, loss 1.31451, acc 0.71875, prec 0.023507, recall 0.672727\n",
      "2017-12-08T21:47:17.933559: step 121, loss 0.999744, acc 0.8125, prec 0.0234177, recall 0.672727\n",
      "2017-12-08T21:47:18.604914: step 122, loss 18.5155, acc 0.84375, prec 0.0233512, recall 0.666667\n",
      "2017-12-08T21:47:19.290675: step 123, loss 9.81822, acc 0.859375, prec 0.0235997, recall 0.663717\n",
      "2017-12-08T21:47:19.999614: step 124, loss 0.654881, acc 0.84375, prec 0.0235257, recall 0.663717\n",
      "2017-12-08T21:47:20.676640: step 125, loss 1.04196, acc 0.65625, prec 0.0233645, recall 0.663717\n",
      "2017-12-08T21:47:21.356514: step 126, loss 0.769717, acc 0.734375, prec 0.023544, recall 0.666667\n",
      "2017-12-08T21:47:22.033249: step 127, loss 0.708219, acc 0.84375, prec 0.0234713, recall 0.666667\n",
      "2017-12-08T21:47:22.714572: step 128, loss 3.97508, acc 0.671875, prec 0.0233272, recall 0.66087\n",
      "2017-12-08T21:47:23.397487: step 129, loss 1.60401, acc 0.78125, prec 0.0235258, recall 0.663793\n",
      "2017-12-08T21:47:24.076552: step 130, loss 1.38685, acc 0.625, prec 0.0233546, recall 0.663793\n",
      "2017-12-08T21:47:24.743866: step 131, loss 17.0396, acc 0.734375, prec 0.0238311, recall 0.663866\n",
      "2017-12-08T21:47:25.423892: step 132, loss 2.55335, acc 0.640625, prec 0.0239664, recall 0.661157\n",
      "2017-12-08T21:47:26.106372: step 133, loss 2.03247, acc 0.640625, prec 0.0240928, recall 0.663934\n",
      "2017-12-08T21:47:26.805392: step 134, loss 2.74578, acc 0.5, prec 0.0241532, recall 0.666667\n",
      "2017-12-08T21:47:27.526458: step 135, loss 3.11629, acc 0.46875, prec 0.0239137, recall 0.666667\n",
      "2017-12-08T21:47:28.251557: step 136, loss 2.9821, acc 0.421875, prec 0.0236584, recall 0.666667\n",
      "2017-12-08T21:47:28.975442: step 137, loss 2.87664, acc 0.453125, prec 0.0237007, recall 0.669355\n",
      "2017-12-08T21:47:29.663676: step 138, loss 2.9941, acc 0.453125, prec 0.0234662, recall 0.669355\n",
      "2017-12-08T21:47:30.342263: step 139, loss 2.37415, acc 0.515625, prec 0.0238095, recall 0.674603\n",
      "2017-12-08T21:47:31.021175: step 140, loss 6.27039, acc 0.53125, prec 0.0236177, recall 0.669291\n",
      "2017-12-08T21:47:31.710741: step 141, loss 3.64215, acc 0.515625, prec 0.0239537, recall 0.674419\n",
      "2017-12-08T21:47:32.384847: step 142, loss 3.39985, acc 0.53125, prec 0.0242904, recall 0.679389\n",
      "2017-12-08T21:47:33.056501: step 143, loss 3.19594, acc 0.453125, prec 0.0240606, recall 0.679389\n",
      "2017-12-08T21:47:33.723459: step 144, loss 2.28257, acc 0.484375, prec 0.0238478, recall 0.679389\n",
      "2017-12-08T21:47:34.405342: step 145, loss 2.22491, acc 0.515625, prec 0.0236513, recall 0.679389\n",
      "2017-12-08T21:47:35.072259: step 146, loss 2.38084, acc 0.5, prec 0.0237092, recall 0.681818\n",
      "2017-12-08T21:47:35.737415: step 147, loss 2.85591, acc 0.546875, prec 0.0235294, recall 0.681818\n",
      "2017-12-08T21:47:36.397336: step 148, loss 5.92613, acc 0.65625, prec 0.0236548, recall 0.679105\n",
      "2017-12-08T21:47:37.067561: step 149, loss 2.64153, acc 0.640625, prec 0.0235203, recall 0.674074\n",
      "2017-12-08T21:47:37.737346: step 150, loss 1.14265, acc 0.6875, prec 0.0233993, recall 0.674074\n",
      "2017-12-08T21:47:38.395060: step 151, loss 2.00914, acc 0.671875, prec 0.0235234, recall 0.676471\n",
      "2017-12-08T21:47:39.124499: step 152, loss 1.8765, acc 0.65625, prec 0.0233918, recall 0.676471\n",
      "2017-12-08T21:47:39.800729: step 153, loss 6.76787, acc 0.5625, prec 0.0232323, recall 0.671533\n",
      "2017-12-08T21:47:40.477967: step 154, loss 1.34072, acc 0.703125, prec 0.0231214, recall 0.671533\n",
      "2017-12-08T21:47:41.164271: step 155, loss 1.39718, acc 0.734375, prec 0.023023, recall 0.671533\n",
      "2017-12-08T21:47:41.843271: step 156, loss 1.83637, acc 0.59375, prec 0.0231171, recall 0.673913\n",
      "2017-12-08T21:47:42.535604: step 157, loss 1.65663, acc 0.671875, prec 0.022997, recall 0.673913\n",
      "2017-12-08T21:47:43.260651: step 158, loss 3.4108, acc 0.6875, prec 0.0231356, recall 0.666667\n",
      "2017-12-08T21:47:43.970345: step 159, loss 1.23981, acc 0.640625, prec 0.0230054, recall 0.666667\n",
      "2017-12-08T21:47:44.701057: step 160, loss 1.39768, acc 0.765625, prec 0.0231594, recall 0.669014\n",
      "2017-12-08T21:47:45.403874: step 161, loss 16.2784, acc 0.6875, prec 0.0230583, recall 0.659722\n",
      "2017-12-08T21:47:46.120907: step 162, loss 2.10655, acc 0.578125, prec 0.0233791, recall 0.664384\n",
      "2017-12-08T21:47:46.822320: step 163, loss 2.22697, acc 0.578125, prec 0.023228, recall 0.664384\n",
      "2017-12-08T21:47:47.532552: step 164, loss 1.62018, acc 0.65625, prec 0.0235714, recall 0.668919\n",
      "2017-12-08T21:47:48.243482: step 165, loss 1.92057, acc 0.609375, prec 0.023432, recall 0.668919\n",
      "2017-12-08T21:47:48.986448: step 166, loss 2.58304, acc 0.515625, prec 0.0232613, recall 0.668919\n",
      "2017-12-08T21:47:49.710041: step 167, loss 3.48169, acc 0.4375, prec 0.0230662, recall 0.668919\n",
      "2017-12-08T21:47:50.526722: step 168, loss 2.43263, acc 0.578125, prec 0.0231481, recall 0.671141\n",
      "2017-12-08T21:47:51.336588: step 169, loss 2.30128, acc 0.578125, prec 0.0232291, recall 0.673333\n",
      "2017-12-08T21:47:52.140012: step 170, loss 6.55205, acc 0.5, prec 0.0232877, recall 0.671053\n",
      "2017-12-08T21:47:52.923204: step 171, loss 1.69944, acc 0.65625, prec 0.0231713, recall 0.671053\n",
      "2017-12-08T21:47:53.678729: step 172, loss 1.87511, acc 0.640625, prec 0.0232716, recall 0.673203\n",
      "2017-12-08T21:47:54.436231: step 173, loss 2.52414, acc 0.484375, prec 0.0230993, recall 0.673203\n",
      "2017-12-08T21:47:55.193575: step 174, loss 12.5181, acc 0.734375, prec 0.0230168, recall 0.668831\n",
      "2017-12-08T21:47:55.964281: step 175, loss 0.988393, acc 0.65625, prec 0.0229042, recall 0.668831\n",
      "2017-12-08T21:47:56.721872: step 176, loss 0.830875, acc 0.75, prec 0.022823, recall 0.668831\n",
      "2017-12-08T21:47:57.483150: step 177, loss 1.32249, acc 0.734375, prec 0.0227373, recall 0.668831\n",
      "2017-12-08T21:47:58.238297: step 178, loss 16.9807, acc 0.609375, prec 0.0226274, recall 0.656051\n",
      "2017-12-08T21:47:59.022959: step 179, loss 1.07254, acc 0.703125, prec 0.0225334, recall 0.656051\n",
      "2017-12-08T21:47:59.783731: step 180, loss 1.57619, acc 0.625, prec 0.0224157, recall 0.656051\n",
      "2017-12-08T21:48:00.544789: step 181, loss 4.24018, acc 0.640625, prec 0.0225206, recall 0.654088\n",
      "2017-12-08T21:48:01.317910: step 182, loss 1.72127, acc 0.640625, prec 0.0226196, recall 0.65625\n",
      "2017-12-08T21:48:02.074783: step 183, loss 1.84485, acc 0.625, prec 0.0225032, recall 0.65625\n",
      "2017-12-08T21:48:02.835764: step 184, loss 2.94305, acc 0.53125, prec 0.0223595, recall 0.65625\n",
      "2017-12-08T21:48:03.591551: step 185, loss 2.81302, acc 0.625, prec 0.0226599, recall 0.660494\n",
      "2017-12-08T21:48:04.355426: step 186, loss 2.26093, acc 0.546875, prec 0.0229329, recall 0.664634\n",
      "2017-12-08T21:48:05.115772: step 187, loss 2.1496, acc 0.53125, prec 0.0231975, recall 0.668675\n",
      "2017-12-08T21:48:05.879555: step 188, loss 15.801, acc 0.609375, prec 0.0232848, recall 0.666667\n",
      "2017-12-08T21:48:06.649452: step 189, loss 2.11307, acc 0.59375, prec 0.0231596, recall 0.666667\n",
      "2017-12-08T21:48:07.405172: step 190, loss 5.18914, acc 0.515625, prec 0.0230169, recall 0.662722\n",
      "2017-12-08T21:48:08.165928: step 191, loss 2.72494, acc 0.484375, prec 0.0230612, recall 0.664706\n",
      "2017-12-08T21:48:08.924201: step 192, loss 2.11589, acc 0.484375, prec 0.022907, recall 0.664706\n",
      "2017-12-08T21:48:09.703280: step 193, loss 2.5116, acc 0.53125, prec 0.0227685, recall 0.664706\n",
      "2017-12-08T21:48:10.546578: step 194, loss 2.47339, acc 0.40625, prec 0.0227909, recall 0.666667\n",
      "2017-12-08T21:48:11.371537: step 195, loss 1.71054, acc 0.640625, prec 0.0226866, recall 0.666667\n",
      "2017-12-08T21:48:12.190624: step 196, loss 1.81863, acc 0.59375, prec 0.0227633, recall 0.668605\n",
      "2017-12-08T21:48:12.995799: step 197, loss 9.50133, acc 0.515625, prec 0.0226289, recall 0.66474\n",
      "2017-12-08T21:48:13.782534: step 198, loss 5.93748, acc 0.59375, prec 0.0229008, recall 0.664773\n",
      "2017-12-08T21:48:14.562511: step 199, loss 2.14108, acc 0.515625, prec 0.0227626, recall 0.664773\n",
      "2017-12-08T21:48:15.352888: step 200, loss 2.10506, acc 0.5, prec 0.0226218, recall 0.664773\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512787555/checkpoints/model-200\n",
      "\n",
      "2017-12-08T21:48:17.132400: step 201, loss 2.06227, acc 0.5625, prec 0.0226879, recall 0.666667\n",
      "2017-12-08T21:48:17.896472: step 202, loss 2.29054, acc 0.546875, prec 0.022749, recall 0.668539\n",
      "2017-12-08T21:48:18.645966: step 203, loss 1.20881, acc 0.6875, prec 0.0226624, recall 0.668539\n",
      "2017-12-08T21:48:19.467817: step 204, loss 0.940877, acc 0.6875, prec 0.0229471, recall 0.672222\n",
      "2017-12-08T21:48:20.225227: step 205, loss 15.0519, acc 0.65625, prec 0.0230406, recall 0.67033\n",
      "2017-12-08T21:48:20.985320: step 206, loss 1.15997, acc 0.703125, prec 0.0229582, recall 0.67033\n",
      "2017-12-08T21:48:21.748760: step 207, loss 3.97011, acc 0.71875, prec 0.022885, recall 0.666667\n",
      "2017-12-08T21:48:22.514580: step 208, loss 1.55277, acc 0.609375, prec 0.0227782, recall 0.666667\n",
      "2017-12-08T21:48:23.327846: step 209, loss 1.25395, acc 0.703125, prec 0.0228795, recall 0.668478\n",
      "2017-12-08T21:48:24.140184: step 210, loss 15.1929, acc 0.5625, prec 0.0227693, recall 0.66129\n",
      "2017-12-08T21:48:24.950322: step 211, loss 1.81385, acc 0.5, prec 0.0226353, recall 0.66129\n",
      "2017-12-08T21:48:25.752826: step 212, loss 2.09576, acc 0.5625, prec 0.0225192, recall 0.66129\n",
      "2017-12-08T21:48:26.564412: step 213, loss 2.76128, acc 0.46875, prec 0.0223799, recall 0.66129\n",
      "2017-12-08T21:48:27.381810: step 214, loss 1.85029, acc 0.546875, prec 0.0224394, recall 0.663102\n",
      "2017-12-08T21:48:28.186496: step 215, loss 10.457, acc 0.640625, prec 0.0225266, recall 0.661376\n",
      "2017-12-08T21:48:28.985180: step 216, loss 2.53358, acc 0.5, prec 0.0225726, recall 0.663158\n",
      "2017-12-08T21:48:29.797347: step 217, loss 9.12587, acc 0.40625, prec 0.0227717, recall 0.663212\n",
      "2017-12-08T21:48:30.660170: step 218, loss 2.60504, acc 0.46875, prec 0.0228076, recall 0.664948\n",
      "2017-12-08T21:48:31.570621: step 219, loss 3.98145, acc 0.375, prec 0.022819, recall 0.666667\n",
      "2017-12-08T21:48:32.434110: step 220, loss 3.18566, acc 0.40625, prec 0.0226678, recall 0.666667\n",
      "2017-12-08T21:48:33.353677: step 221, loss 3.33194, acc 0.359375, prec 0.0228453, recall 0.670051\n",
      "2017-12-08T21:48:34.232600: step 222, loss 3.28321, acc 0.375, prec 0.0228562, recall 0.671717\n",
      "2017-12-08T21:48:35.156123: step 223, loss 3.17865, acc 0.390625, prec 0.022704, recall 0.671717\n",
      "2017-12-08T21:48:36.040400: step 224, loss 14.4947, acc 0.578125, prec 0.0227736, recall 0.666667\n",
      "2017-12-08T21:48:36.966032: step 225, loss 2.89073, acc 0.46875, prec 0.0226428, recall 0.666667\n",
      "2017-12-08T21:48:37.849569: step 226, loss 2.35579, acc 0.453125, prec 0.022838, recall 0.669951\n",
      "2017-12-08T21:48:38.755140: step 227, loss 2.72585, acc 0.4375, prec 0.0227007, recall 0.669951\n",
      "2017-12-08T21:48:39.723769: step 228, loss 2.08216, acc 0.546875, prec 0.0227537, recall 0.671569\n",
      "2017-12-08T21:48:40.669989: step 229, loss 2.49553, acc 0.515625, prec 0.0226371, recall 0.671569\n",
      "2017-12-08T21:48:41.545161: step 230, loss 1.67312, acc 0.65625, prec 0.022716, recall 0.673171\n",
      "2017-12-08T21:48:42.437776: step 231, loss 1.60017, acc 0.625, prec 0.0226267, recall 0.673171\n",
      "2017-12-08T21:48:43.318265: step 232, loss 2.33042, acc 0.625, prec 0.0226976, recall 0.674757\n",
      "2017-12-08T21:48:44.193612: step 233, loss 1.69238, acc 0.609375, prec 0.0226053, recall 0.674757\n",
      "2017-12-08T21:48:45.066888: step 234, loss 1.92819, acc 0.609375, prec 0.0226721, recall 0.676328\n",
      "2017-12-08T21:48:45.945297: step 235, loss 2.54428, acc 0.6875, prec 0.0227603, recall 0.674641\n",
      "2017-12-08T21:48:46.835205: step 236, loss 9.08217, acc 0.609375, prec 0.0226725, recall 0.671429\n",
      "2017-12-08T21:48:47.701350: step 237, loss 1.63461, acc 0.640625, prec 0.0225889, recall 0.671429\n",
      "2017-12-08T21:48:48.615345: step 238, loss 2.26335, acc 0.65625, prec 0.0228216, recall 0.674528\n",
      "2017-12-08T21:48:49.664855: step 239, loss 15.6763, acc 0.6875, prec 0.0227526, recall 0.671362\n",
      "2017-12-08T21:48:50.627945: step 240, loss 10.7078, acc 0.53125, prec 0.0229576, recall 0.671296\n",
      "2017-12-08T21:48:51.599727: step 241, loss 2.49997, acc 0.515625, prec 0.0229994, recall 0.672811\n",
      "2017-12-08T21:48:52.623273: step 242, loss 2.5997, acc 0.453125, prec 0.0230263, recall 0.674312\n",
      "2017-12-08T21:48:53.574938: step 243, loss 4.73309, acc 0.421875, prec 0.0230494, recall 0.672727\n",
      "2017-12-08T21:48:54.549432: step 244, loss 3.14003, acc 0.390625, prec 0.0229102, recall 0.672727\n",
      "2017-12-08T21:48:55.454304: step 245, loss 3.55113, acc 0.40625, prec 0.0227762, recall 0.672727\n",
      "2017-12-08T21:48:56.362999: step 246, loss 4.32708, acc 0.3125, prec 0.0227724, recall 0.674208\n",
      "2017-12-08T21:48:57.261043: step 247, loss 3.36067, acc 0.390625, prec 0.0229344, recall 0.67713\n",
      "2017-12-08T21:48:58.187216: step 248, loss 3.33169, acc 0.4375, prec 0.0231048, recall 0.68\n",
      "2017-12-08T21:48:59.253865: step 249, loss 4.16049, acc 0.328125, prec 0.0232488, recall 0.682819\n",
      "2017-12-08T21:49:00.130695: step 250, loss 3.63942, acc 0.328125, prec 0.0232454, recall 0.684211\n",
      "2017-12-08T21:49:01.085511: step 251, loss 3.63995, acc 0.4375, prec 0.0231214, recall 0.684211\n",
      "2017-12-08T21:49:01.977115: step 252, loss 2.32197, acc 0.5, prec 0.0230122, recall 0.684211\n",
      "2017-12-08T21:49:02.917537: step 253, loss 1.61601, acc 0.640625, prec 0.0229344, recall 0.684211\n",
      "2017-12-08T21:49:03.832148: step 254, loss 2.20034, acc 0.5625, prec 0.0228404, recall 0.684211\n",
      "2017-12-08T21:49:04.674603: step 255, loss 2.50406, acc 0.515625, prec 0.0228796, recall 0.685589\n",
      "2017-12-08T21:49:05.501493: step 256, loss 5.39402, acc 0.71875, prec 0.0228231, recall 0.682609\n",
      "2017-12-08T21:49:06.298007: step 257, loss 1.16334, acc 0.703125, prec 0.0227602, recall 0.682609\n",
      "2017-12-08T21:49:07.113130: step 258, loss 1.24109, acc 0.671875, prec 0.0228324, recall 0.683983\n",
      "2017-12-08T21:49:07.887549: step 259, loss 0.237096, acc 0.875, prec 0.023229, recall 0.688034\n",
      "2017-12-08T21:49:08.687639: step 260, loss 0.674225, acc 0.828125, prec 0.0231922, recall 0.688034\n",
      "2017-12-08T21:49:09.560979: step 261, loss 0.685719, acc 0.796875, prec 0.0231488, recall 0.688034\n",
      "2017-12-08T21:49:10.382744: step 262, loss 9.85765, acc 0.828125, prec 0.0232558, recall 0.686441\n",
      "2017-12-08T21:49:11.168133: step 263, loss 0.616605, acc 0.84375, prec 0.0232225, recall 0.686441\n",
      "2017-12-08T21:49:11.943812: step 264, loss 11.1845, acc 0.875, prec 0.0231992, recall 0.683544\n",
      "2017-12-08T21:49:12.738107: step 265, loss 8.76351, acc 0.859375, prec 0.0233124, recall 0.682008\n",
      "2017-12-08T21:49:13.503821: step 266, loss 0.777764, acc 0.890625, prec 0.0234286, recall 0.683333\n",
      "2017-12-08T21:49:14.273892: step 267, loss 0.900076, acc 0.84375, prec 0.0236737, recall 0.68595\n",
      "2017-12-08T21:49:15.043647: step 268, loss 0.895832, acc 0.765625, prec 0.0236232, recall 0.68595\n",
      "2017-12-08T21:49:15.790728: step 269, loss 0.914064, acc 0.796875, prec 0.0235795, recall 0.68595\n",
      "2017-12-08T21:49:16.539113: step 270, loss 0.644849, acc 0.8125, prec 0.0235394, recall 0.68595\n",
      "2017-12-08T21:49:17.273914: step 271, loss 1.35635, acc 0.71875, prec 0.0234795, recall 0.68595\n",
      "2017-12-08T21:49:18.014412: step 272, loss 0.819027, acc 0.828125, prec 0.0235809, recall 0.687243\n",
      "2017-12-08T21:49:18.771032: step 273, loss 3.84282, acc 0.671875, prec 0.023652, recall 0.685714\n",
      "2017-12-08T21:49:19.553099: step 274, loss 6.532, acc 0.65625, prec 0.0237226, recall 0.681452\n",
      "2017-12-08T21:49:20.344456: step 275, loss 1.67289, acc 0.546875, prec 0.0237629, recall 0.682731\n",
      "2017-12-08T21:49:21.165704: step 276, loss 1.7565, acc 0.578125, prec 0.0238095, recall 0.684\n",
      "2017-12-08T21:49:21.954292: step 277, loss 2.72357, acc 0.46875, prec 0.0236973, recall 0.684\n",
      "2017-12-08T21:49:22.792191: step 278, loss 2.85073, acc 0.359375, prec 0.0235635, recall 0.684\n",
      "2017-12-08T21:49:23.533097: step 279, loss 2.26, acc 0.484375, prec 0.0237246, recall 0.686508\n",
      "2017-12-08T21:49:24.298076: step 280, loss 2.25811, acc 0.53125, prec 0.0240273, recall 0.690196\n",
      "2017-12-08T21:49:25.030986: step 281, loss 2.05485, acc 0.515625, prec 0.0240587, recall 0.691406\n",
      "2017-12-08T21:49:25.760113: step 282, loss 2.82635, acc 0.46875, prec 0.0240801, recall 0.692607\n",
      "2017-12-08T21:49:26.490409: step 283, loss 7.84475, acc 0.640625, prec 0.0241403, recall 0.69112\n",
      "2017-12-08T21:49:27.220727: step 284, loss 1.90922, acc 0.5625, prec 0.0241805, recall 0.692308\n",
      "2017-12-08T21:49:27.932345: step 285, loss 2.62489, acc 0.4375, prec 0.0240642, recall 0.692308\n",
      "2017-12-08T21:49:28.635282: step 286, loss 1.50657, acc 0.578125, prec 0.0241076, recall 0.693487\n",
      "2017-12-08T21:49:29.342887: step 287, loss 8.348, acc 0.453125, prec 0.0240021, recall 0.688213\n",
      "2017-12-08T21:49:30.058110: step 288, loss 3.16783, acc 0.515625, prec 0.0240327, recall 0.689394\n",
      "2017-12-08T21:49:30.749473: step 289, loss 2.49136, acc 0.515625, prec 0.0239348, recall 0.689394\n",
      "2017-12-08T21:49:31.425541: step 290, loss 2.18789, acc 0.484375, prec 0.0240869, recall 0.691729\n",
      "2017-12-08T21:49:32.106021: step 291, loss 1.78626, acc 0.59375, prec 0.0241325, recall 0.692884\n",
      "2017-12-08T21:49:32.791662: step 292, loss 1.69462, acc 0.5625, prec 0.0240447, recall 0.692884\n",
      "2017-12-08T21:49:33.475634: step 293, loss 1.7313, acc 0.546875, prec 0.0239544, recall 0.692884\n",
      "2017-12-08T21:49:34.163666: step 294, loss 1.69614, acc 0.625, prec 0.0240062, recall 0.69403\n",
      "2017-12-08T21:49:34.839117: step 295, loss 0.958935, acc 0.71875, prec 0.0239506, recall 0.69403\n",
      "2017-12-08T21:49:35.521072: step 296, loss 1.34377, acc 0.703125, prec 0.0240175, recall 0.695167\n",
      "2017-12-08T21:49:36.202443: step 297, loss 1.86403, acc 0.6875, prec 0.024081, recall 0.696296\n",
      "2017-12-08T21:49:36.880206: step 298, loss 2.73437, acc 0.765625, prec 0.0242843, recall 0.698529\n",
      "2017-12-08T21:49:37.559939: step 299, loss 1.6892, acc 0.75, prec 0.0244836, recall 0.70073\n",
      "2017-12-08T21:49:38.251674: step 300, loss 1.23661, acc 0.671875, prec 0.0244182, recall 0.70073\n",
      "\n",
      "Evaluation:\n",
      "2017-12-08T21:51:00.314729: step 300, loss 1.62897, acc 0.798377, prec 0.0306852, recall 0.705479\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512787555/checkpoints/model-300\n",
      "\n",
      "2017-12-08T21:51:03.057595: step 301, loss 1.80021, acc 0.6875, prec 0.0307204, recall 0.70615\n",
      "2017-12-08T21:51:03.936051: step 302, loss 3.30417, acc 0.671875, prec 0.0307555, recall 0.705215\n",
      "2017-12-08T21:51:04.875969: step 303, loss 1.53194, acc 0.671875, prec 0.0306918, recall 0.705215\n",
      "2017-12-08T21:51:05.838006: step 304, loss 1.23003, acc 0.671875, prec 0.0306283, recall 0.705215\n",
      "2017-12-08T21:51:06.691005: step 305, loss 5.3444, acc 0.734375, prec 0.0305801, recall 0.70362\n",
      "2017-12-08T21:51:07.495357: step 306, loss 1.25291, acc 0.765625, prec 0.0306303, recall 0.704289\n",
      "2017-12-08T21:51:08.316339: step 307, loss 1.49406, acc 0.640625, prec 0.0305613, recall 0.704289\n",
      "2017-12-08T21:51:09.173729: step 308, loss 1.19842, acc 0.78125, prec 0.030709, recall 0.705618\n",
      "2017-12-08T21:51:09.962501: step 309, loss 1.0346, acc 0.65625, prec 0.0306431, recall 0.705618\n",
      "2017-12-08T21:51:10.725942: step 310, loss 1.02252, acc 0.703125, prec 0.0305864, recall 0.705618\n",
      "2017-12-08T21:51:11.463233: step 311, loss 1.10487, acc 0.71875, prec 0.0307214, recall 0.706935\n",
      "2017-12-08T21:51:12.202447: step 312, loss 1.11235, acc 0.625, prec 0.0306499, recall 0.706935\n",
      "2017-12-08T21:51:12.973941: step 313, loss 0.666073, acc 0.78125, prec 0.0306083, recall 0.706935\n",
      "2017-12-08T21:51:13.725067: step 314, loss 7.88849, acc 0.65625, prec 0.0306428, recall 0.704444\n",
      "2017-12-08T21:51:14.467483: step 315, loss 1.31232, acc 0.78125, prec 0.030695, recall 0.7051\n",
      "2017-12-08T21:51:15.209417: step 316, loss 1.15407, acc 0.671875, prec 0.0306329, recall 0.7051\n",
      "2017-12-08T21:51:15.940828: step 317, loss 12.0842, acc 0.578125, prec 0.0306524, recall 0.702643\n",
      "2017-12-08T21:51:16.669002: step 318, loss 1.22736, acc 0.6875, prec 0.0306866, recall 0.703297\n",
      "2017-12-08T21:51:17.416281: step 319, loss 1.94889, acc 0.515625, prec 0.0306883, recall 0.703947\n",
      "2017-12-08T21:51:18.145317: step 320, loss 3.30295, acc 0.53125, prec 0.0306035, recall 0.702407\n",
      "2017-12-08T21:51:18.903130: step 321, loss 3.37955, acc 0.390625, prec 0.0305822, recall 0.703057\n",
      "2017-12-08T21:51:19.700638: step 322, loss 3.81578, acc 0.484375, prec 0.0304895, recall 0.701525\n",
      "2017-12-08T21:51:20.546094: step 323, loss 12.0889, acc 0.46875, prec 0.0303946, recall 0.7\n",
      "2017-12-08T21:51:21.294440: step 324, loss 2.72745, acc 0.484375, prec 0.0303002, recall 0.7\n",
      "2017-12-08T21:51:22.088838: step 325, loss 3.46534, acc 0.359375, prec 0.0301837, recall 0.7\n",
      "2017-12-08T21:51:22.882912: step 326, loss 2.6703, acc 0.46875, prec 0.0302691, recall 0.701299\n",
      "2017-12-08T21:51:23.648559: step 327, loss 4.7259, acc 0.28125, prec 0.0301395, recall 0.701299\n",
      "2017-12-08T21:51:24.417433: step 328, loss 4.36706, acc 0.34375, prec 0.0300222, recall 0.701299\n",
      "2017-12-08T21:51:25.161620: step 329, loss 3.13661, acc 0.34375, prec 0.0299954, recall 0.701944\n",
      "2017-12-08T21:51:25.934726: step 330, loss 3.92585, acc 0.375, prec 0.0299743, recall 0.702586\n",
      "2017-12-08T21:51:26.692238: step 331, loss 3.12522, acc 0.375, prec 0.0299533, recall 0.703226\n",
      "2017-12-08T21:51:27.460617: step 332, loss 2.21823, acc 0.453125, prec 0.0299461, recall 0.703863\n",
      "2017-12-08T21:51:28.217465: step 333, loss 1.6415, acc 0.5625, prec 0.0300464, recall 0.705128\n",
      "2017-12-08T21:51:28.974556: step 334, loss 2.17425, acc 0.515625, prec 0.0299619, recall 0.705128\n",
      "2017-12-08T21:51:29.745323: step 335, loss 1.82485, acc 0.625, prec 0.0298967, recall 0.705128\n",
      "2017-12-08T21:51:30.567186: step 336, loss 1.23571, acc 0.6875, prec 0.0298426, recall 0.705128\n",
      "2017-12-08T21:51:31.324859: step 337, loss 0.893496, acc 0.765625, prec 0.0298898, recall 0.705757\n",
      "2017-12-08T21:51:32.057672: step 338, loss 0.903693, acc 0.765625, prec 0.0298494, recall 0.705757\n",
      "2017-12-08T21:51:32.793954: step 339, loss 1.2442, acc 0.765625, prec 0.0298964, recall 0.706383\n",
      "2017-12-08T21:51:33.510545: step 340, loss 6.34773, acc 0.8125, prec 0.0300414, recall 0.706131\n",
      "2017-12-08T21:51:34.250941: step 341, loss 1.08407, acc 0.828125, prec 0.0300988, recall 0.706751\n",
      "2017-12-08T21:51:34.983244: step 342, loss 0.16825, acc 0.921875, prec 0.0300853, recall 0.706751\n",
      "2017-12-08T21:51:35.720064: step 343, loss 0.309024, acc 0.921875, prec 0.0301589, recall 0.707368\n",
      "2017-12-08T21:51:36.452964: step 344, loss 0.430832, acc 0.890625, prec 0.0301399, recall 0.707368\n",
      "2017-12-08T21:51:37.224664: step 345, loss 19.5377, acc 0.828125, prec 0.0301156, recall 0.704403\n",
      "2017-12-08T21:51:38.033101: step 346, loss 4.99064, acc 0.890625, prec 0.0302732, recall 0.704167\n",
      "2017-12-08T21:51:38.836667: step 347, loss 6.35774, acc 0.859375, prec 0.0302515, recall 0.702703\n",
      "2017-12-08T21:51:39.580036: step 348, loss 0.87088, acc 0.828125, prec 0.0302217, recall 0.702703\n",
      "2017-12-08T21:51:40.342429: step 349, loss 11.2682, acc 0.84375, prec 0.0302841, recall 0.701863\n",
      "2017-12-08T21:51:41.067362: step 350, loss 1.26288, acc 0.671875, prec 0.0302274, recall 0.701863\n",
      "2017-12-08T21:51:41.798758: step 351, loss 1.38637, acc 0.671875, prec 0.0301709, recall 0.701863\n",
      "2017-12-08T21:51:42.552192: step 352, loss 1.45688, acc 0.65625, prec 0.0301119, recall 0.701863\n",
      "2017-12-08T21:51:43.291062: step 353, loss 1.68025, acc 0.71875, prec 0.0301499, recall 0.702479\n",
      "2017-12-08T21:51:44.028622: step 354, loss 2.34334, acc 0.53125, prec 0.0301556, recall 0.703093\n",
      "2017-12-08T21:51:44.740597: step 355, loss 1.29922, acc 0.6875, prec 0.0301024, recall 0.703093\n",
      "2017-12-08T21:51:45.474996: step 356, loss 3.32919, acc 0.5, prec 0.0301883, recall 0.704312\n",
      "2017-12-08T21:51:46.196484: step 357, loss 2.79609, acc 0.46875, prec 0.0300983, recall 0.704312\n",
      "2017-12-08T21:51:46.944590: step 358, loss 2.51068, acc 0.484375, prec 0.0300962, recall 0.704918\n",
      "2017-12-08T21:51:47.694724: step 359, loss 3.7674, acc 0.5, prec 0.0303506, recall 0.707317\n",
      "2017-12-08T21:51:48.420863: step 360, loss 1.85158, acc 0.53125, prec 0.0302714, recall 0.707317\n",
      "2017-12-08T21:51:49.145687: step 361, loss 1.30057, acc 0.625, prec 0.0302083, recall 0.707317\n",
      "2017-12-08T21:51:49.880685: step 362, loss 1.5336, acc 0.625, prec 0.0301455, recall 0.707317\n",
      "2017-12-08T21:51:50.672506: step 363, loss 1.46992, acc 0.609375, prec 0.0300804, recall 0.707317\n",
      "2017-12-08T21:51:51.402169: step 364, loss 1.07531, acc 0.703125, prec 0.0300311, recall 0.707317\n",
      "2017-12-08T21:51:52.112790: step 365, loss 1.30322, acc 0.640625, prec 0.0299716, recall 0.707317\n",
      "2017-12-08T21:51:52.825160: step 366, loss 6.3263, acc 0.765625, prec 0.0300189, recall 0.706478\n",
      "2017-12-08T21:51:53.532460: step 367, loss 1.32952, acc 0.6875, prec 0.0299674, recall 0.706478\n",
      "2017-12-08T21:51:54.232969: step 368, loss 3.19805, acc 0.78125, prec 0.029934, recall 0.705051\n",
      "2017-12-08T21:51:54.935322: step 369, loss 0.712575, acc 0.84375, prec 0.0300745, recall 0.706237\n",
      "2017-12-08T21:51:55.613740: step 370, loss 1.437, acc 0.59375, prec 0.0300077, recall 0.706237\n",
      "2017-12-08T21:51:56.297941: step 371, loss 1.20852, acc 0.625, prec 0.0299463, recall 0.706237\n",
      "2017-12-08T21:51:56.976710: step 372, loss 4.12974, acc 0.828125, prec 0.0299207, recall 0.704819\n",
      "2017-12-08T21:51:57.659987: step 373, loss 5.06042, acc 0.65625, prec 0.0298673, recall 0.703407\n",
      "2017-12-08T21:51:58.398003: step 374, loss 0.7014, acc 0.8125, prec 0.0298368, recall 0.703407\n",
      "2017-12-08T21:51:59.089642: step 375, loss 1.76247, acc 0.59375, prec 0.029771, recall 0.703407\n",
      "2017-12-08T21:51:59.879883: step 376, loss 1.13839, acc 0.734375, prec 0.0298103, recall 0.704\n",
      "2017-12-08T21:52:00.715774: step 377, loss 8.90748, acc 0.703125, prec 0.0297649, recall 0.702595\n",
      "2017-12-08T21:52:01.530914: step 378, loss 3.83399, acc 0.625, prec 0.029789, recall 0.701789\n",
      "2017-12-08T21:52:02.319885: step 379, loss 1.30623, acc 0.671875, prec 0.0297363, recall 0.701789\n",
      "2017-12-08T21:52:03.121727: step 380, loss 1.36654, acc 0.703125, prec 0.0297704, recall 0.702381\n",
      "2017-12-08T21:52:04.000774: step 381, loss 1.68482, acc 0.609375, prec 0.0297894, recall 0.70297\n",
      "2017-12-08T21:52:04.763691: step 382, loss 1.71646, acc 0.59375, prec 0.0298058, recall 0.703557\n",
      "2017-12-08T21:52:05.568553: step 383, loss 1.91877, acc 0.578125, prec 0.0298196, recall 0.704142\n",
      "2017-12-08T21:52:06.333066: step 384, loss 1.96539, acc 0.609375, prec 0.0298383, recall 0.704724\n",
      "2017-12-08T21:52:07.121223: step 385, loss 1.77885, acc 0.59375, prec 0.0297738, recall 0.704724\n",
      "2017-12-08T21:52:07.884921: step 386, loss 1.88774, acc 0.609375, prec 0.029712, recall 0.704724\n",
      "2017-12-08T21:52:08.609926: step 387, loss 1.49007, acc 0.640625, prec 0.0298161, recall 0.705882\n",
      "2017-12-08T21:52:09.344094: step 388, loss 4.90143, acc 0.546875, prec 0.0299075, recall 0.705653\n",
      "2017-12-08T21:52:10.074126: step 389, loss 5.22485, acc 0.59375, prec 0.0299258, recall 0.704854\n",
      "2017-12-08T21:52:10.813305: step 390, loss 1.74134, acc 0.625, prec 0.0299465, recall 0.705426\n",
      "2017-12-08T21:52:11.518630: step 391, loss 2.12273, acc 0.546875, prec 0.0299549, recall 0.705996\n",
      "2017-12-08T21:52:12.227758: step 392, loss 5.53391, acc 0.65625, prec 0.0299033, recall 0.704633\n",
      "2017-12-08T21:52:12.949693: step 393, loss 12.7409, acc 0.6875, prec 0.0300155, recall 0.704415\n",
      "2017-12-08T21:52:13.662877: step 394, loss 3.22225, acc 0.59375, prec 0.0300335, recall 0.703633\n",
      "2017-12-08T21:52:14.359047: step 395, loss 2.30993, acc 0.546875, prec 0.0299625, recall 0.703633\n",
      "2017-12-08T21:52:15.055692: step 396, loss 1.92554, acc 0.453125, prec 0.0298774, recall 0.703633\n",
      "2017-12-08T21:52:15.731279: step 397, loss 2.43625, acc 0.453125, prec 0.0297927, recall 0.703633\n",
      "2017-12-08T21:52:16.402412: step 398, loss 2.37259, acc 0.453125, prec 0.0297086, recall 0.703633\n",
      "2017-12-08T21:52:17.079282: step 399, loss 3.08586, acc 0.375, prec 0.029691, recall 0.704198\n",
      "2017-12-08T21:52:17.768330: step 400, loss 2.48251, acc 0.515625, prec 0.0296171, recall 0.704198\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512787555/checkpoints/model-400\n",
      "\n",
      "2017-12-08T21:52:19.383635: step 401, loss 2.21737, acc 0.5, prec 0.0295413, recall 0.704198\n",
      "2017-12-08T21:52:20.044363: step 402, loss 1.65693, acc 0.578125, prec 0.0294776, recall 0.704198\n",
      "2017-12-08T21:52:20.746372: step 403, loss 1.67948, acc 0.578125, prec 0.0294141, recall 0.704198\n",
      "2017-12-08T21:52:21.398023: step 404, loss 1.74047, acc 0.578125, prec 0.0293509, recall 0.704198\n",
      "2017-12-08T21:52:22.061267: step 405, loss 1.76356, acc 0.609375, prec 0.0292927, recall 0.704198\n",
      "2017-12-08T21:52:22.741529: step 406, loss 1.8604, acc 0.640625, prec 0.0293162, recall 0.704762\n",
      "2017-12-08T21:52:23.424797: step 407, loss 1.56511, acc 0.609375, prec 0.0292583, recall 0.704762\n",
      "2017-12-08T21:52:24.095871: step 408, loss 1.08114, acc 0.703125, prec 0.029291, recall 0.705323\n",
      "2017-12-08T21:52:24.798338: step 409, loss 2.2376, acc 0.703125, prec 0.0292494, recall 0.703985\n",
      "2017-12-08T21:52:25.485401: step 410, loss 1.03679, acc 0.703125, prec 0.0292057, recall 0.703985\n",
      "2017-12-08T21:52:26.192427: step 411, loss 11.8561, acc 0.8125, prec 0.0291804, recall 0.702652\n",
      "2017-12-08T21:52:26.916893: step 412, loss 0.696405, acc 0.84375, prec 0.0291575, recall 0.702652\n",
      "2017-12-08T21:52:27.640718: step 413, loss 0.764074, acc 0.78125, prec 0.0291255, recall 0.702652\n",
      "2017-12-08T21:52:28.359406: step 414, loss 0.714512, acc 0.796875, prec 0.029248, recall 0.703774\n",
      "2017-12-08T21:52:29.082708: step 415, loss 0.612898, acc 0.78125, prec 0.0292159, recall 0.703774\n",
      "2017-12-08T21:52:29.798193: step 416, loss 0.730296, acc 0.765625, prec 0.0293335, recall 0.704887\n",
      "2017-12-08T21:52:30.560912: step 417, loss 6.95426, acc 0.859375, prec 0.0294669, recall 0.704673\n",
      "2017-12-08T21:52:31.292105: step 418, loss 14.3532, acc 0.734375, prec 0.0294347, recall 0.700743\n",
      "2017-12-08T21:52:32.011961: step 419, loss 0.844416, acc 0.78125, prec 0.0294783, recall 0.701299\n",
      "2017-12-08T21:52:32.739937: step 420, loss 0.837756, acc 0.71875, prec 0.029437, recall 0.701299\n",
      "2017-12-08T21:52:33.483774: step 421, loss 1.88204, acc 0.609375, prec 0.029606, recall 0.702952\n",
      "2017-12-08T21:52:34.207739: step 422, loss 3.18655, acc 0.5625, prec 0.0296193, recall 0.702206\n",
      "2017-12-08T21:52:34.950275: step 423, loss 1.84026, acc 0.59375, prec 0.0296348, recall 0.702752\n",
      "2017-12-08T21:52:35.726750: step 424, loss 2.38876, acc 0.40625, prec 0.0295479, recall 0.702752\n",
      "2017-12-08T21:52:36.453370: step 425, loss 2.57093, acc 0.5, prec 0.0294751, recall 0.702752\n",
      "2017-12-08T21:52:37.179193: step 426, loss 2.19951, acc 0.515625, prec 0.0294795, recall 0.703297\n",
      "2017-12-08T21:52:37.911019: step 427, loss 2.21107, acc 0.515625, prec 0.0295582, recall 0.70438\n",
      "2017-12-08T21:52:38.632702: step 428, loss 2.51013, acc 0.4375, prec 0.0297733, recall 0.706522\n",
      "2017-12-08T21:52:39.351165: step 429, loss 2.31764, acc 0.4375, prec 0.0296917, recall 0.706522\n",
      "2017-12-08T21:52:40.076464: step 430, loss 2.48954, acc 0.46875, prec 0.0296887, recall 0.707052\n",
      "2017-12-08T21:52:40.845308: step 431, loss 2.73342, acc 0.46875, prec 0.0296857, recall 0.707581\n",
      "2017-12-08T21:52:41.569419: step 432, loss 1.95466, acc 0.578125, prec 0.0296252, recall 0.707581\n",
      "2017-12-08T21:52:42.308014: step 433, loss 1.71315, acc 0.5625, prec 0.0296358, recall 0.708108\n",
      "2017-12-08T21:52:43.051862: step 434, loss 1.282, acc 0.625, prec 0.0296553, recall 0.708633\n",
      "2017-12-08T21:52:43.887256: step 435, loss 1.35806, acc 0.625, prec 0.0296018, recall 0.708633\n",
      "2017-12-08T21:52:44.672835: step 436, loss 6.17001, acc 0.734375, prec 0.0295663, recall 0.707361\n",
      "2017-12-08T21:52:45.492654: step 437, loss 1.70408, acc 0.65625, prec 0.0295902, recall 0.707885\n",
      "2017-12-08T21:52:46.280959: step 438, loss 7.63218, acc 0.671875, prec 0.0296185, recall 0.707143\n",
      "2017-12-08T21:52:47.065205: step 439, loss 3.29585, acc 0.734375, prec 0.0295831, recall 0.705882\n",
      "2017-12-08T21:52:47.892986: step 440, loss 1.42862, acc 0.703125, prec 0.0295412, recall 0.705882\n",
      "2017-12-08T21:52:48.702069: step 441, loss 1.412, acc 0.65625, prec 0.0294928, recall 0.705882\n",
      "2017-12-08T21:52:49.474955: step 442, loss 2.48902, acc 0.71875, prec 0.0295999, recall 0.705674\n",
      "2017-12-08T21:52:50.255918: step 443, loss 1.19104, acc 0.75, prec 0.0297089, recall 0.706714\n",
      "2017-12-08T21:52:51.032609: step 444, loss 1.69986, acc 0.75, prec 0.0298895, recall 0.70826\n",
      "2017-12-08T21:52:51.813754: step 445, loss 1.43867, acc 0.609375, prec 0.029906, recall 0.708772\n",
      "2017-12-08T21:52:52.559014: step 446, loss 1.57119, acc 0.734375, prec 0.0300118, recall 0.70979\n",
      "2017-12-08T21:52:53.294422: step 447, loss 1.2751, acc 0.609375, prec 0.0299565, recall 0.70979\n",
      "2017-12-08T21:52:54.031803: step 448, loss 3.45839, acc 0.625, prec 0.0299057, recall 0.708551\n",
      "2017-12-08T21:52:54.764174: step 449, loss 1.8377, acc 0.59375, prec 0.0299199, recall 0.709059\n",
      "2017-12-08T21:52:55.488931: step 450, loss 3.96277, acc 0.5625, prec 0.0298628, recall 0.706597\n",
      "2017-12-08T21:52:56.193355: step 451, loss 1.89081, acc 0.609375, prec 0.0298792, recall 0.707106\n",
      "2017-12-08T21:52:56.899335: step 452, loss 1.66909, acc 0.640625, prec 0.0298998, recall 0.707612\n",
      "2017-12-08T21:52:57.600518: step 453, loss 1.84653, acc 0.65625, prec 0.0298518, recall 0.707612\n",
      "2017-12-08T21:52:58.305131: step 454, loss 1.55874, acc 0.640625, prec 0.0298018, recall 0.707612\n",
      "2017-12-08T21:52:59.012141: step 455, loss 0.973476, acc 0.6875, prec 0.029829, recall 0.708117\n",
      "2017-12-08T21:52:59.714528: step 456, loss 1.69037, acc 0.609375, prec 0.0298453, recall 0.708621\n",
      "2017-12-08T21:53:00.426327: step 457, loss 5.37745, acc 0.625, prec 0.0298659, recall 0.707904\n",
      "2017-12-08T21:53:01.136339: step 458, loss 0.894886, acc 0.625, prec 0.029814, recall 0.707904\n",
      "2017-12-08T21:53:01.848544: step 459, loss 1.56721, acc 0.59375, prec 0.029758, recall 0.707904\n",
      "2017-12-08T21:53:02.550498: step 460, loss 6.50399, acc 0.65625, prec 0.0297829, recall 0.707192\n",
      "2017-12-08T21:53:03.260566: step 461, loss 0.963441, acc 0.6875, prec 0.02974, recall 0.707192\n",
      "2017-12-08T21:53:03.983889: step 462, loss 1.42003, acc 0.5625, prec 0.0297499, recall 0.707692\n",
      "2017-12-08T21:53:04.698762: step 463, loss 0.786527, acc 0.734375, prec 0.0297136, recall 0.707692\n",
      "2017-12-08T21:53:05.445825: step 464, loss 1.367, acc 0.59375, prec 0.0297278, recall 0.708191\n",
      "2017-12-08T21:53:06.146715: step 465, loss 0.939145, acc 0.78125, prec 0.029698, recall 0.708191\n",
      "2017-12-08T21:53:06.854137: step 466, loss 3.10731, acc 0.640625, prec 0.0297207, recall 0.707483\n",
      "2017-12-08T21:53:07.564187: step 467, loss 1.92405, acc 0.765625, prec 0.0298965, recall 0.708968\n",
      "2017-12-08T21:53:08.272277: step 468, loss 1.52025, acc 0.671875, prec 0.0299209, recall 0.709459\n",
      "2017-12-08T21:53:08.980521: step 469, loss 1.38254, acc 0.609375, prec 0.0300057, recall 0.710438\n",
      "2017-12-08T21:53:09.683430: step 470, loss 7.63978, acc 0.78125, prec 0.0300469, recall 0.709732\n",
      "2017-12-08T21:53:10.389800: step 471, loss 1.21948, acc 0.59375, prec 0.0300603, recall 0.710218\n",
      "2017-12-08T21:53:11.140414: step 472, loss 2.09595, acc 0.546875, prec 0.0301358, recall 0.711185\n",
      "2017-12-08T21:53:11.879665: step 473, loss 0.625651, acc 0.84375, prec 0.0301831, recall 0.711667\n",
      "2017-12-08T21:53:12.619677: step 474, loss 2.12521, acc 0.578125, prec 0.030194, recall 0.712146\n",
      "2017-12-08T21:53:13.362530: step 475, loss 1.31201, acc 0.65625, prec 0.0301472, recall 0.712146\n",
      "2017-12-08T21:53:14.191122: step 476, loss 0.730395, acc 0.71875, prec 0.030109, recall 0.712146\n",
      "2017-12-08T21:53:14.989537: step 477, loss 1.01356, acc 0.671875, prec 0.0302009, recall 0.713101\n",
      "2017-12-08T21:53:15.800891: step 478, loss 1.09525, acc 0.734375, prec 0.0301649, recall 0.713101\n",
      "2017-12-08T21:53:16.615660: step 479, loss 1.04181, acc 0.71875, prec 0.0301268, recall 0.713101\n",
      "2017-12-08T21:53:17.383423: step 480, loss 1.29825, acc 0.703125, prec 0.0300868, recall 0.713101\n",
      "2017-12-08T21:53:18.164875: step 481, loss 1.29171, acc 0.625, prec 0.0301041, recall 0.713576\n",
      "2017-12-08T21:53:18.926219: step 482, loss 0.936136, acc 0.765625, prec 0.0300726, recall 0.713576\n",
      "2017-12-08T21:53:19.693608: step 483, loss 1.01608, acc 0.703125, prec 0.0300328, recall 0.713576\n",
      "2017-12-08T21:53:20.423458: step 484, loss 0.680289, acc 0.859375, prec 0.0302165, recall 0.714992\n",
      "2017-12-08T21:53:21.195052: step 485, loss 0.426358, acc 0.84375, prec 0.0301955, recall 0.714992\n",
      "2017-12-08T21:53:21.927892: step 486, loss 0.583355, acc 0.828125, prec 0.0302398, recall 0.715461\n",
      "2017-12-08T21:53:22.637174: step 487, loss 5.59645, acc 0.796875, prec 0.030282, recall 0.714754\n",
      "2017-12-08T21:53:23.339599: step 488, loss 0.388875, acc 0.90625, prec 0.0302694, recall 0.714754\n",
      "2017-12-08T21:53:24.046190: step 489, loss 6.36452, acc 0.8125, prec 0.0302463, recall 0.713584\n",
      "2017-12-08T21:53:24.749756: step 490, loss 0.246056, acc 0.90625, prec 0.0302337, recall 0.713584\n",
      "2017-12-08T21:53:25.446792: step 491, loss 0.527037, acc 0.859375, prec 0.0302148, recall 0.713584\n",
      "2017-12-08T21:53:26.141929: step 492, loss 2.99669, acc 0.765625, prec 0.0301855, recall 0.712418\n",
      "2017-12-08T21:53:26.825505: step 493, loss 0.701315, acc 0.796875, prec 0.0302255, recall 0.712887\n",
      "2017-12-08T21:53:27.505875: step 494, loss 3.08528, acc 0.65625, prec 0.0301816, recall 0.711726\n",
      "2017-12-08T21:53:28.184407: step 495, loss 1.83837, acc 0.765625, prec 0.0303511, recall 0.713128\n",
      "2017-12-08T21:53:28.862209: step 496, loss 0.826486, acc 0.796875, prec 0.0303907, recall 0.713592\n",
      "2017-12-08T21:53:29.432897: step 497, loss 0.986601, acc 0.764706, prec 0.0303656, recall 0.713592\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Creating folds\n",
    "kf = KFold(n_splits=4, random_state=5, shuffle=True)\n",
    "for k, (train_index, test_index) in enumerate(kf.split(x, y)):\n",
    "# for train_index, test_index in kf.split(x):\n",
    "#     print(\"Fold: %s =>\" % k,  \"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_dev = x[train_index], x[test_index]\n",
    "    y_train, y_dev = y[train_index], y[test_index]\n",
    "    \n",
    "    train_word_distancesA = word_distancesA[train_index]\n",
    "    train_word_distancesB = word_distancesB[train_index]\n",
    "    \n",
    "    test_word_distancesA = word_distancesA[test_index]\n",
    "    test_word_distancesB = word_distancesB[test_index]\n",
    "    \n",
    "    train_pos_embedding = pos_embedding[train_index]\n",
    "    test_pos_embedding = pos_embedding[test_index]\n",
    "    \n",
    "    print(\"Fold: %s =>\" % k, \"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    \n",
    "    model = POSTextCNN(sequence_length=x_train.shape[1],\n",
    "            vocab_processor=vocab_processor, num_epochs=1, evaluate_every=300, results_dir='fold%s'%k)\n",
    "    model.train_network(x_train, y_train, x_dev, y_dev, \n",
    "                        train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB,\n",
    "                       train_pos_embedding, test_pos_embedding)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class POSTextCNN(object):\n",
    "    \"\"\"\n",
    "    A Basic CNN for text classification with Position plus POS features as well.\n",
    "    \n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \n",
    "    Refer tohttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5181565/pdf/btw486.pdf for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, vocab_processor, \n",
    "                 num_classes=2, embedding_size=128, filter_sizes=[3,4,5], \n",
    "                 num_filters=128, batch_size=64, \n",
    "                 l2_reg_lambda=0.0, num_epochs=200,\n",
    "                 num_checkpoints=5, dropout_prob=0.5, \n",
    "                 checkpoint_every=100, evaluate_every=100, \n",
    "                 allow_soft_placement=True,log_device_placement=False,\n",
    "                 results_dir=\"runs\"):\n",
    "        \n",
    "        tf.reset_default_graph() \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = len(vocab_processor.vocabulary_)\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.num_epochs = num_epochs\n",
    "        self.results_dir = results_dir\n",
    "        \n",
    "        self.vocab_processor = vocab_processor\n",
    "        \n",
    "        self.num_checkpoints = num_checkpoints\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.checkpoint_every = checkpoint_every\n",
    "        self.evaluate_every = evaluate_every\n",
    "        \n",
    "        self.position_vector_mapping = PositionTextCNN.load_position_vector_mapping()\n",
    "        \n",
    "        self.allow_soft_placement = allow_soft_placement\n",
    "        self.log_device_placement = log_device_placement\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        self.word_distancesA = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"word_distancesA\")\n",
    "        self.word_distancesB = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"word_distancesB\")\n",
    "        \n",
    "        self.encoded_pos = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"encoded_pos\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"word_embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),name=\"W\") \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        # Position Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"position_embedding\"):\n",
    "            embedded_positionsA = tf.nn.embedding_lookup(self.position_vector_mapping, self.word_distancesA)\n",
    "            embedded_positionsB = tf.nn.embedding_lookup(self.position_vector_mapping, self.word_distancesB)\n",
    "            embedded_positions = tf.concat([embedded_positionsA, embedded_positionsB], 2)\n",
    "            self.embedded_positions_expanded = tf.cast(tf.expand_dims(embedded_positions, -1), tf.float32)\n",
    "            \n",
    "        # POS Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"pos_embedding\"):\n",
    "            one_hot_encoding = tf.one_hot(list(range(8)), 8)\n",
    "            embedded_pos = tf.nn.embedding_lookup(one_hot_encoding, self.encoded_pos)\n",
    "            self.embedded_pos_expanded = tf.cast(tf.expand_dims(embedded_pos, -1), tf.float32)\n",
    "            \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.final_embedded_expanded = tf.concat([self.embedded_chars_expanded, self.embedded_positions_expanded, self.embedded_pos_expanded], 2)\n",
    "        \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, self.embedding_size+28, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.final_embedded_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, self.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "    def train_network(self, x_train, y_train, x_dev, y_dev,\n",
    "                     train_word_distancesA, train_word_distancesB, test_word_distancesA, test_word_distancesB,\n",
    "                     train_pos_embedding, test_pos_embedding):\n",
    "        \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            class_weight = tf.constant([1.0, 100.0])\n",
    "            weights = tf.reduce_sum(class_weight * self.input_y, axis=1)\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            weighted_losses = losses * weights\n",
    "            self.loss = tf.reduce_mean(weighted_losses) + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            _, self.precision = tf.metrics.precision(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='precision')\n",
    "            _, self.recall = tf.metrics.recall(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='recall')\n",
    "            \n",
    "        # Define Training procedure\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, self.results_dir, timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        precision_summary = tf.summary.scalar(\"precision\", self.precision)\n",
    "        recall_summary = tf.summary.scalar(\"recall\", self.recall)\n",
    "\n",
    "        # Train Summaries\n",
    "        self.train_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        self.train_summary_writer = tf.summary.FileWriter(train_summary_dir, self.sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        self.dev_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        self.dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, self.sess.graph)\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=self.num_checkpoints)\n",
    "        \n",
    "        # Write vocabulary\n",
    "        self.vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        # Initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "        print(\"Start training\")\n",
    "        # Generate batches\n",
    "        batches = PositionTextCNN.batch_iter(\n",
    "            list(zip(x_train, y_train, train_word_distancesA, train_word_distancesB, train_pos_embedding)), self.batch_size, self.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding = zip(*batch)\n",
    "            self.train_step(x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding)\n",
    "            current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "            if current_step % self.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                self.dev_step(x_dev, y_dev, test_word_distancesA, test_word_distancesB, test_pos_embedding, writer=self.dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % self.checkpoint_every == 0:\n",
    "                path = saver.save(self.sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))   \n",
    "        print(\"Training finished\")\n",
    "    \n",
    "    def train_step(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.input_x: x_batch,\n",
    "            self.input_y: y_batch,\n",
    "            self.dropout_keep_prob: self.dropout_prob,\n",
    "            self.word_distancesA: batch_word_distancesA,\n",
    "            self.word_distancesB: batch_word_distancesB,\n",
    "            self.encoded_pos: batch_pos_embedding\n",
    "        }\n",
    "        _, step, summaries, loss, accuracy, precision, recall = self.sess.run(\n",
    "            [self.train_op, self.global_step, self.train_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "        self.train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "    \n",
    "    def dev_step(self, x_batch, y_batch, batch_word_distancesA, batch_word_distancesB, batch_pos_embedding, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                self.input_x: x_batch,\n",
    "                self.input_y: y_batch,\n",
    "                self.dropout_keep_prob: 1.0,\n",
    "                self.word_distancesA: batch_word_distancesA,\n",
    "                self.word_distancesB: batch_word_distancesB,\n",
    "                self.encoded_pos: batch_pos_embedding\n",
    "            }\n",
    "            step, summaries, loss, accuracy,  precision, recall  = self.sess.run(\n",
    "                [self.global_step, self.dev_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "    @staticmethod            \n",
    "    def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates a batch iterator for a dataset.\n",
    "        \"\"\"\n",
    "        data = np.array(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]\n",
    "    \n",
    "    @staticmethod \n",
    "    def load_position_vector_mapping():\n",
    "        # bit_array generated with the distance between \n",
    "        # two entities where abs_num represents the distance\n",
    "        def int2bit_by_distance(int_num, bit_len=10):\n",
    "\n",
    "            bit_array = np.zeros(bit_len)\n",
    "            if int_num > 0:\n",
    "                bit_array[0] = 1\n",
    "\n",
    "            abs_num = np.abs(int_num)\n",
    "            if abs_num <= 5:\n",
    "                for i in range(abs_num):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 10:\n",
    "                for i in range(6):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 20:\n",
    "                for i in range(7):\n",
    "                    bit_array[-i-1] = 1\n",
    "            elif abs_num <= 30:\n",
    "                for i in range(8):\n",
    "                    bit_array[-i-1] = 1\n",
    "            else:\n",
    "                for i in range(9):\n",
    "                    bit_array[-i-1] = 1\n",
    "            return bit_array\n",
    "\n",
    "        map = {}\n",
    "        for i in range(-300, 300):\n",
    "            map[i] = int2bit_by_distance(i, 10)\n",
    "\n",
    "        return pd.DataFrame.from_dict(map, orient='index', dtype='int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(273), Dimension(156), Dimension(1)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = model.sess.run(model.embedded_chars, feed_dict = {\n",
    "                model.input_x: x_dev,\n",
    "                model.input_y: y_dev,\n",
    "                model.dropout_keep_prob: 1.0,\n",
    "                model.word_distancesA: test_word_distancesA,\n",
    "                model.word_distancesB: test_word_distancesB,\n",
    "                model.encoded_pos: test_pos_embedding\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10599, 273, 128)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10599, 273)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_distancesA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10599,273,128) and (10599,273) not aligned: 128 (dim 2) != 10599 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b8bb6c4d974d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_word_distancesA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shapes (10599,273,128) and (10599,273) not aligned: 128 (dim 2) != 10599 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.matmul(temp,test_word_distancesA).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273, 128)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_distancesA[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10599,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(test_word_distancesA, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp[:,np.argmin(test_word_distancesA, axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
