{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from BasicTextCNN import BasicTextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 => Train/Dev split: 31795/10599\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512708702\n",
      "\n",
      "Start training\n",
      "2017-12-07T23:51:45.599275: step 1, loss 1.46206, acc 0.6875, prec 0, recall 0\n",
      "2017-12-07T23:51:46.140684: step 2, loss 35.2333, acc 0.859375, prec 0.037037, recall 0.25\n",
      "2017-12-07T23:51:46.722344: step 3, loss 3.8336, acc 0.671875, prec 0.0212766, recall 0.2\n",
      "2017-12-07T23:51:47.313200: step 4, loss 1.07154, acc 0.71875, prec 0.0153846, recall 0.2\n",
      "2017-12-07T23:51:47.860944: step 5, loss 1.58163, acc 0.640625, prec 0.0224719, recall 0.333333\n",
      "2017-12-07T23:51:48.395383: step 6, loss 1.56662, acc 0.546875, prec 0.0169492, recall 0.333333\n",
      "2017-12-07T23:51:48.948136: step 7, loss 2.58264, acc 0.4375, prec 0.0193548, recall 0.428571\n",
      "2017-12-07T23:51:49.485356: step 8, loss 3.30018, acc 0.46875, prec 0.021164, recall 0.444444\n",
      "2017-12-07T23:51:50.006796: step 9, loss 3.08313, acc 0.359375, prec 0.0173913, recall 0.444444\n",
      "2017-12-07T23:51:50.502571: step 10, loss 2.58458, acc 0.484375, prec 0.0189394, recall 0.5\n",
      "2017-12-07T23:51:51.001709: step 11, loss 3.54045, acc 0.390625, prec 0.0197368, recall 0.545455\n",
      "2017-12-07T23:51:51.502621: step 12, loss 2.23253, acc 0.421875, prec 0.0204678, recall 0.583333\n",
      "2017-12-07T23:51:52.022661: step 13, loss 2.42179, acc 0.453125, prec 0.0185676, recall 0.583333\n",
      "2017-12-07T23:51:52.532446: step 14, loss 3.76555, acc 0.53125, prec 0.0172414, recall 0.538462\n",
      "2017-12-07T23:51:53.040942: step 15, loss 1.41819, acc 0.640625, prec 0.0186047, recall 0.571429\n",
      "2017-12-07T23:51:53.552766: step 16, loss 1.56895, acc 0.546875, prec 0.0195652, recall 0.6\n",
      "2017-12-07T23:51:54.060523: step 17, loss 1.88683, acc 0.65625, prec 0.0207039, recall 0.625\n",
      "2017-12-07T23:51:54.568913: step 18, loss 1.45366, acc 0.53125, prec 0.0194932, recall 0.625\n",
      "2017-12-07T23:51:55.076657: step 19, loss 1.06408, acc 0.640625, prec 0.0186567, recall 0.625\n",
      "2017-12-07T23:51:55.582657: step 20, loss 2.45637, acc 0.703125, prec 0.0180505, recall 0.588235\n",
      "2017-12-07T23:51:56.086941: step 21, loss 8.74481, acc 0.625, prec 0.0190311, recall 0.578947\n",
      "2017-12-07T23:51:56.634481: step 22, loss 8.48894, acc 0.8125, prec 0.0186757, recall 0.55\n",
      "2017-12-07T23:51:57.146491: step 23, loss 1.47481, acc 0.640625, prec 0.0179739, recall 0.55\n",
      "2017-12-07T23:51:57.649417: step 24, loss 18.0491, acc 0.671875, prec 0.0189573, recall 0.545455\n",
      "2017-12-07T23:51:58.149043: step 25, loss 0.808403, acc 0.65625, prec 0.0198171, recall 0.565217\n",
      "2017-12-07T23:51:58.655536: step 26, loss 1.36273, acc 0.5625, prec 0.020438, recall 0.583333\n",
      "2017-12-07T23:51:59.157409: step 27, loss 2.66305, acc 0.53125, prec 0.0209497, recall 0.6\n",
      "2017-12-07T23:51:59.673857: step 28, loss 4.44909, acc 0.5, prec 0.0226969, recall 0.607143\n",
      "2017-12-07T23:52:00.176537: step 29, loss 8.65084, acc 0.5, prec 0.0218228, recall 0.566667\n",
      "2017-12-07T23:52:00.685204: step 30, loss 3.2378, acc 0.4375, prec 0.0220588, recall 0.580645\n",
      "2017-12-07T23:52:01.196262: step 31, loss 4.0731, acc 0.40625, prec 0.0222482, recall 0.575758\n",
      "2017-12-07T23:52:01.696023: step 32, loss 6.92628, acc 0.375, prec 0.0212766, recall 0.558824\n",
      "2017-12-07T23:52:02.208141: step 33, loss 3.56753, acc 0.25, prec 0.0201913, recall 0.558824\n",
      "2017-12-07T23:52:02.717038: step 34, loss 4.09295, acc 0.234375, prec 0.0201816, recall 0.571429\n",
      "2017-12-07T23:52:03.212764: step 35, loss 3.86216, acc 0.265625, prec 0.0202117, recall 0.583333\n",
      "2017-12-07T23:52:03.714771: step 36, loss 5.43958, acc 0.234375, prec 0.020202, recall 0.594595\n",
      "2017-12-07T23:52:04.216554: step 37, loss 4.77513, acc 0.203125, prec 0.0210158, recall 0.615385\n",
      "2017-12-07T23:52:04.717292: step 38, loss 6.33984, acc 0.15625, prec 0.0208855, recall 0.625\n",
      "2017-12-07T23:52:05.219125: step 39, loss 4.05735, acc 0.3125, prec 0.020934, recall 0.634146\n",
      "2017-12-07T23:52:05.719792: step 40, loss 6.17945, acc 0.1875, prec 0.0208494, recall 0.642857\n",
      "2017-12-07T23:52:06.229684: step 41, loss 4.36664, acc 0.34375, prec 0.021658, recall 0.659091\n",
      "2017-12-07T23:52:06.779404: step 42, loss 4.35811, acc 0.25, prec 0.0209084, recall 0.659091\n",
      "2017-12-07T23:52:07.284146: step 43, loss 4.41875, acc 0.203125, prec 0.0201669, recall 0.659091\n",
      "2017-12-07T23:52:07.782910: step 44, loss 2.94284, acc 0.296875, prec 0.019555, recall 0.659091\n",
      "2017-12-07T23:52:08.283325: step 45, loss 9.69452, acc 0.3125, prec 0.0190039, recall 0.644444\n",
      "2017-12-07T23:52:08.789142: step 46, loss 1.9485, acc 0.53125, prec 0.0186375, recall 0.644444\n",
      "2017-12-07T23:52:09.293607: step 47, loss 2.16566, acc 0.578125, prec 0.0189394, recall 0.652174\n",
      "2017-12-07T23:52:09.800597: step 48, loss 1.76191, acc 0.59375, prec 0.0204588, recall 0.673469\n",
      "2017-12-07T23:52:10.300345: step 49, loss 1.32813, acc 0.65625, prec 0.0201835, recall 0.673469\n",
      "2017-12-07T23:52:10.814990: step 50, loss 12.4668, acc 0.6875, prec 0.0199637, recall 0.647059\n",
      "2017-12-07T23:52:11.358761: step 51, loss 1.05887, acc 0.703125, prec 0.0197368, recall 0.647059\n",
      "2017-12-07T23:52:11.886101: step 52, loss 1.22436, acc 0.71875, prec 0.0195266, recall 0.647059\n",
      "2017-12-07T23:52:12.406779: step 53, loss 11.1318, acc 0.609375, prec 0.0192644, recall 0.622642\n",
      "2017-12-07T23:52:12.972495: step 54, loss 3.02648, acc 0.625, prec 0.0190092, recall 0.611111\n",
      "2017-12-07T23:52:13.529129: step 55, loss 22.3998, acc 0.4375, prec 0.0192199, recall 0.576271\n",
      "2017-12-07T23:52:14.075299: step 56, loss 0.924992, acc 0.671875, prec 0.0189944, recall 0.576271\n",
      "2017-12-07T23:52:14.642845: step 57, loss 2.28459, acc 0.46875, prec 0.0186404, recall 0.576271\n",
      "2017-12-07T23:52:15.213493: step 58, loss 2.25994, acc 0.5, prec 0.018319, recall 0.576271\n",
      "2017-12-07T23:52:15.742822: step 59, loss 20.993, acc 0.359375, prec 0.0184599, recall 0.564516\n",
      "2017-12-07T23:52:16.279455: step 60, loss 3.89391, acc 0.28125, prec 0.0180227, recall 0.564516\n",
      "2017-12-07T23:52:16.854355: step 61, loss 3.16715, acc 0.3125, prec 0.0191051, recall 0.584615\n",
      "2017-12-07T23:52:17.390771: step 62, loss 4.46653, acc 0.234375, prec 0.019127, recall 0.590909\n",
      "2017-12-07T23:52:17.921552: step 63, loss 4.35091, acc 0.3125, prec 0.0191939, recall 0.597015\n",
      "2017-12-07T23:52:18.439459: step 64, loss 4.783, acc 0.21875, prec 0.0192037, recall 0.602941\n",
      "2017-12-07T23:52:18.962693: step 65, loss 3.87982, acc 0.25, prec 0.0196796, recall 0.614286\n",
      "2017-12-07T23:52:19.486707: step 66, loss 4.22096, acc 0.28125, prec 0.0192739, recall 0.614286\n",
      "2017-12-07T23:52:20.001778: step 67, loss 4.17388, acc 0.296875, prec 0.0201843, recall 0.630137\n",
      "2017-12-07T23:52:20.524208: step 68, loss 3.76044, acc 0.390625, prec 0.0202674, recall 0.635135\n",
      "2017-12-07T23:52:21.051046: step 69, loss 3.39342, acc 0.34375, prec 0.0199068, recall 0.635135\n",
      "2017-12-07T23:52:21.581436: step 70, loss 4.08079, acc 0.328125, prec 0.0199584, recall 0.64\n",
      "2017-12-07T23:52:22.113407: step 71, loss 3.47342, acc 0.359375, prec 0.0204248, recall 0.649351\n",
      "2017-12-07T23:52:22.647527: step 72, loss 3.50961, acc 0.421875, prec 0.0205149, recall 0.653846\n",
      "2017-12-07T23:52:23.177952: step 73, loss 2.69241, acc 0.421875, prec 0.020214, recall 0.653846\n",
      "2017-12-07T23:52:23.710180: step 74, loss 2.10597, acc 0.5625, prec 0.0199922, recall 0.653846\n",
      "2017-12-07T23:52:24.247372: step 75, loss 2.31613, acc 0.5, prec 0.0197445, recall 0.653846\n",
      "2017-12-07T23:52:24.780027: step 76, loss 5.81197, acc 0.5625, prec 0.0195402, recall 0.64557\n",
      "2017-12-07T23:52:25.310572: step 77, loss 1.50509, acc 0.625, prec 0.0193622, recall 0.64557\n",
      "2017-12-07T23:52:25.845096: step 78, loss 1.97475, acc 0.609375, prec 0.0195489, recall 0.65\n",
      "2017-12-07T23:52:26.378935: step 79, loss 1.43117, acc 0.65625, prec 0.019754, recall 0.654321\n",
      "2017-12-07T23:52:26.958841: step 80, loss 1.46885, acc 0.71875, prec 0.0196224, recall 0.654321\n",
      "2017-12-07T23:52:27.507143: step 81, loss 4.92761, acc 0.765625, prec 0.0198822, recall 0.650602\n",
      "2017-12-07T23:52:28.058672: step 82, loss 6.32999, acc 0.640625, prec 0.0197224, recall 0.642857\n",
      "2017-12-07T23:52:28.611041: step 83, loss 3.03053, acc 0.65625, prec 0.0202825, recall 0.643678\n",
      "2017-12-07T23:52:29.167173: step 84, loss 23.6568, acc 0.640625, prec 0.0201294, recall 0.629214\n",
      "2017-12-07T23:52:29.717680: step 85, loss 1.92364, acc 0.5625, prec 0.0206259, recall 0.637363\n",
      "2017-12-07T23:52:30.267112: step 86, loss 1.98585, acc 0.609375, prec 0.0211342, recall 0.645161\n",
      "2017-12-07T23:52:30.821638: step 87, loss 2.02633, acc 0.5, prec 0.0208986, recall 0.645161\n",
      "2017-12-07T23:52:31.372794: step 88, loss 1.56528, acc 0.578125, prec 0.0210417, recall 0.648936\n",
      "2017-12-07T23:52:31.913843: step 89, loss 1.83259, acc 0.53125, prec 0.0214944, recall 0.65625\n",
      "2017-12-07T23:52:32.467097: step 90, loss 2.06728, acc 0.46875, prec 0.0212479, recall 0.65625\n",
      "2017-12-07T23:52:33.021043: step 91, loss 21.7602, acc 0.5625, prec 0.0217101, recall 0.656566\n",
      "2017-12-07T23:52:33.568594: step 92, loss 6.38896, acc 0.453125, prec 0.0217894, recall 0.653465\n",
      "2017-12-07T23:52:34.119116: step 93, loss 3.06898, acc 0.3125, prec 0.0214774, recall 0.653465\n",
      "2017-12-07T23:52:34.673235: step 94, loss 3.14987, acc 0.4375, prec 0.0212287, recall 0.653465\n",
      "2017-12-07T23:52:35.224061: step 95, loss 3.35747, acc 0.375, prec 0.0212698, recall 0.656863\n",
      "2017-12-07T23:52:35.787843: step 96, loss 3.24992, acc 0.390625, prec 0.0210097, recall 0.656863\n",
      "2017-12-07T23:52:36.358698: step 97, loss 3.33584, acc 0.34375, prec 0.0207366, recall 0.656863\n",
      "2017-12-07T23:52:36.954998: step 98, loss 2.70216, acc 0.40625, prec 0.0210945, recall 0.663462\n",
      "2017-12-07T23:52:37.516562: step 99, loss 3.01675, acc 0.34375, prec 0.0211225, recall 0.666667\n",
      "2017-12-07T23:52:38.077750: step 100, loss 3.3975, acc 0.453125, prec 0.0214861, recall 0.672897\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512708702/checkpoints/model-100\n",
      "\n",
      "2017-12-07T23:52:39.418218: step 101, loss 3.79481, acc 0.421875, prec 0.0218289, recall 0.678899\n",
      "2017-12-07T23:52:40.012936: step 102, loss 6.97214, acc 0.453125, prec 0.0216121, recall 0.672727\n",
      "2017-12-07T23:52:40.587476: step 103, loss 2.52239, acc 0.484375, prec 0.0216888, recall 0.675676\n",
      "2017-12-07T23:52:41.145257: step 104, loss 2.62256, acc 0.484375, prec 0.0214838, recall 0.675676\n",
      "2017-12-07T23:52:41.697201: step 105, loss 2.12381, acc 0.46875, prec 0.0212766, recall 0.675676\n",
      "2017-12-07T23:52:42.258447: step 106, loss 2.07399, acc 0.609375, prec 0.0211268, recall 0.675676\n",
      "2017-12-07T23:52:42.817024: step 107, loss 16.639, acc 0.46875, prec 0.0212054, recall 0.672566\n",
      "2017-12-07T23:52:43.383678: step 108, loss 2.67712, acc 0.5, prec 0.0212884, recall 0.675439\n",
      "2017-12-07T23:52:43.950786: step 109, loss 2.20271, acc 0.53125, prec 0.0213816, recall 0.678261\n",
      "2017-12-07T23:52:44.506716: step 110, loss 1.7497, acc 0.5625, prec 0.0214849, recall 0.681035\n",
      "2017-12-07T23:52:45.066823: step 111, loss 18.8267, acc 0.6875, prec 0.0213745, recall 0.675214\n",
      "2017-12-07T23:52:45.628625: step 112, loss 3.3242, acc 0.5, prec 0.0211967, recall 0.669492\n",
      "2017-12-07T23:52:46.194107: step 113, loss 2.33642, acc 0.609375, prec 0.0210554, recall 0.669492\n",
      "2017-12-07T23:52:46.770874: step 114, loss 1.9845, acc 0.515625, prec 0.0211416, recall 0.672269\n",
      "2017-12-07T23:52:47.360823: step 115, loss 2.31809, acc 0.484375, prec 0.0209589, recall 0.672269\n",
      "2017-12-07T23:52:47.942676: step 116, loss 2.27866, acc 0.59375, prec 0.0210718, recall 0.675\n",
      "2017-12-07T23:52:48.516524: step 117, loss 3.89056, acc 0.4375, prec 0.021134, recall 0.672131\n",
      "2017-12-07T23:52:49.091524: step 118, loss 2.90154, acc 0.515625, prec 0.0209665, recall 0.672131\n",
      "2017-12-07T23:52:49.651590: step 119, loss 2.11566, acc 0.5625, prec 0.0213144, recall 0.677419\n",
      "2017-12-07T23:52:50.226845: step 120, loss 1.55372, acc 0.53125, prec 0.0211534, recall 0.677419\n",
      "2017-12-07T23:52:50.790898: step 121, loss 1.37047, acc 0.65625, prec 0.0210368, recall 0.677419\n",
      "2017-12-07T23:52:51.378185: step 122, loss 1.48431, acc 0.59375, prec 0.0209007, recall 0.677419\n",
      "2017-12-07T23:52:51.985450: step 123, loss 4.61565, acc 0.765625, prec 0.0210709, recall 0.674603\n",
      "2017-12-07T23:52:52.566908: step 124, loss 8.27065, acc 0.6875, prec 0.0209773, recall 0.664062\n",
      "2017-12-07T23:52:53.142360: step 125, loss 1.46278, acc 0.671875, prec 0.0208691, recall 0.664062\n",
      "2017-12-07T23:52:53.709849: step 126, loss 1.38973, acc 0.59375, prec 0.0207368, recall 0.664062\n",
      "2017-12-07T23:52:54.287758: step 127, loss 2.60602, acc 0.59375, prec 0.0206111, recall 0.658915\n",
      "2017-12-07T23:52:54.855109: step 128, loss 1.09244, acc 0.703125, prec 0.0205165, recall 0.658915\n",
      "2017-12-07T23:52:55.421868: step 129, loss 1.40758, acc 0.578125, prec 0.0206186, recall 0.661538\n",
      "2017-12-07T23:52:55.985734: step 130, loss 1.25101, acc 0.625, prec 0.020734, recall 0.664122\n",
      "2017-12-07T23:52:56.554467: step 131, loss 1.42966, acc 0.65625, prec 0.0206259, recall 0.664122\n",
      "2017-12-07T23:52:57.136451: step 132, loss 1.42839, acc 0.65625, prec 0.0205189, recall 0.664122\n",
      "2017-12-07T23:52:57.710082: step 133, loss 2.4838, acc 0.59375, prec 0.0213115, recall 0.674074\n",
      "2017-12-07T23:52:58.286667: step 134, loss 1.10968, acc 0.75, prec 0.0212319, recall 0.674074\n",
      "2017-12-07T23:52:58.860881: step 135, loss 4.44031, acc 0.515625, prec 0.0210843, recall 0.669118\n",
      "2017-12-07T23:52:59.433205: step 136, loss 1.48262, acc 0.71875, prec 0.0209968, recall 0.669118\n",
      "2017-12-07T23:53:00.019153: step 137, loss 1.09078, acc 0.6875, prec 0.0211251, recall 0.671533\n",
      "2017-12-07T23:53:00.597865: step 138, loss 1.34866, acc 0.625, prec 0.0212329, recall 0.673913\n",
      "2017-12-07T23:53:01.164586: step 139, loss 1.11072, acc 0.71875, prec 0.0215909, recall 0.678571\n",
      "2017-12-07T23:53:01.738374: step 140, loss 2.44328, acc 0.65625, prec 0.0214884, recall 0.673759\n",
      "2017-12-07T23:53:02.315093: step 141, loss 1.86749, acc 0.6875, prec 0.0216119, recall 0.676056\n",
      "2017-12-07T23:53:02.896307: step 142, loss 1.29455, acc 0.703125, prec 0.0215198, recall 0.676056\n",
      "2017-12-07T23:53:03.471544: step 143, loss 1.44449, acc 0.59375, prec 0.0213951, recall 0.676056\n",
      "2017-12-07T23:53:04.049593: step 144, loss 6.5809, acc 0.5625, prec 0.0212672, recall 0.671329\n",
      "2017-12-07T23:53:04.623012: step 145, loss 1.72451, acc 0.671875, prec 0.0211687, recall 0.671329\n",
      "2017-12-07T23:53:05.205858: step 146, loss 12.4524, acc 0.6875, prec 0.0212953, recall 0.668966\n",
      "2017-12-07T23:53:05.786172: step 147, loss 8.64497, acc 0.78125, prec 0.0212347, recall 0.664384\n",
      "2017-12-07T23:53:06.357420: step 148, loss 1.50587, acc 0.65625, prec 0.0211329, recall 0.664384\n",
      "2017-12-07T23:53:06.963302: step 149, loss 1.74838, acc 0.53125, prec 0.0209957, recall 0.664384\n",
      "2017-12-07T23:53:07.539046: step 150, loss 2.95267, acc 0.578125, prec 0.0212949, recall 0.668919\n",
      "2017-12-07T23:53:08.151135: step 151, loss 2.29033, acc 0.46875, prec 0.0215582, recall 0.673333\n",
      "2017-12-07T23:53:08.761143: step 152, loss 2.07434, acc 0.53125, prec 0.0216285, recall 0.675497\n",
      "2017-12-07T23:53:09.343928: step 153, loss 6.14551, acc 0.484375, prec 0.0214827, recall 0.671053\n",
      "2017-12-07T23:53:09.920027: step 154, loss 1.72344, acc 0.546875, prec 0.0213523, recall 0.671053\n",
      "2017-12-07T23:53:10.495753: step 155, loss 1.70313, acc 0.65625, prec 0.0214583, recall 0.673203\n",
      "2017-12-07T23:53:11.068203: step 156, loss 15.1789, acc 0.53125, prec 0.0213295, recall 0.668831\n",
      "2017-12-07T23:53:11.645624: step 157, loss 1.92637, acc 0.5625, prec 0.0212065, recall 0.668831\n",
      "2017-12-07T23:53:12.234069: step 158, loss 2.8407, acc 0.40625, prec 0.0210419, recall 0.668831\n",
      "2017-12-07T23:53:12.815210: step 159, loss 2.3743, acc 0.515625, prec 0.0209095, recall 0.668831\n",
      "2017-12-07T23:53:13.389355: step 160, loss 2.14094, acc 0.5625, prec 0.0211864, recall 0.673077\n",
      "2017-12-07T23:53:13.956225: step 161, loss 1.85809, acc 0.59375, prec 0.0212723, recall 0.675159\n",
      "2017-12-07T23:53:14.549931: step 162, loss 6.51313, acc 0.421875, prec 0.0215097, recall 0.675\n",
      "2017-12-07T23:53:15.138809: step 163, loss 5.96101, acc 0.625, prec 0.0216055, recall 0.67284\n",
      "2017-12-07T23:53:15.720634: step 164, loss 2.89222, acc 0.390625, prec 0.0214398, recall 0.67284\n",
      "2017-12-07T23:53:16.297331: step 165, loss 2.47616, acc 0.484375, prec 0.0213015, recall 0.67284\n",
      "2017-12-07T23:53:16.889337: step 166, loss 3.0545, acc 0.390625, prec 0.0211404, recall 0.67284\n",
      "2017-12-07T23:53:17.477589: step 167, loss 2.01437, acc 0.484375, prec 0.021006, recall 0.67284\n",
      "2017-12-07T23:53:18.059925: step 168, loss 2.91438, acc 0.375, prec 0.0210325, recall 0.674847\n",
      "2017-12-07T23:53:18.643931: step 169, loss 2.69704, acc 0.375, prec 0.0210586, recall 0.676829\n",
      "2017-12-07T23:53:19.207184: step 170, loss 2.09116, acc 0.453125, prec 0.0209197, recall 0.676829\n",
      "2017-12-07T23:53:19.786660: step 171, loss 2.20021, acc 0.46875, prec 0.0207865, recall 0.676829\n",
      "2017-12-07T23:53:20.364165: step 172, loss 2.37494, acc 0.453125, prec 0.0206512, recall 0.676829\n",
      "2017-12-07T23:53:20.930955: step 173, loss 5.43447, acc 0.625, prec 0.0207446, recall 0.674699\n",
      "2017-12-07T23:53:21.502875: step 174, loss 19.6614, acc 0.640625, prec 0.020841, recall 0.672619\n",
      "2017-12-07T23:53:22.078210: step 175, loss 24.5902, acc 0.640625, prec 0.0209405, recall 0.666667\n",
      "2017-12-07T23:53:22.673148: step 176, loss 25.9649, acc 0.609375, prec 0.0210315, recall 0.66092\n",
      "2017-12-07T23:53:23.256951: step 177, loss 11.4521, acc 0.53125, prec 0.0212805, recall 0.657303\n",
      "2017-12-07T23:53:23.828580: step 178, loss 3.03361, acc 0.34375, prec 0.0212958, recall 0.659218\n",
      "2017-12-07T23:53:24.391918: step 179, loss 3.47758, acc 0.390625, prec 0.021147, recall 0.659218\n",
      "2017-12-07T23:53:24.962119: step 180, loss 3.34798, acc 0.359375, prec 0.0213409, recall 0.662983\n",
      "2017-12-07T23:53:25.529232: step 181, loss 4.24897, acc 0.3125, prec 0.0211752, recall 0.662983\n",
      "2017-12-07T23:53:26.098216: step 182, loss 4.36104, acc 0.328125, prec 0.0215298, recall 0.668478\n",
      "2017-12-07T23:53:26.662096: step 183, loss 4.59297, acc 0.25, prec 0.0215203, recall 0.67027\n",
      "2017-12-07T23:53:27.282961: step 184, loss 4.85154, acc 0.25, prec 0.0215109, recall 0.672043\n",
      "2017-12-07T23:53:27.861880: step 185, loss 4.1, acc 0.21875, prec 0.0216613, recall 0.675532\n",
      "2017-12-07T23:53:28.445428: step 186, loss 4.73288, acc 0.1875, prec 0.0216362, recall 0.677249\n",
      "2017-12-07T23:53:29.025402: step 187, loss 3.44186, acc 0.40625, prec 0.0218267, recall 0.680628\n",
      "2017-12-07T23:53:29.594433: step 188, loss 2.7009, acc 0.4375, prec 0.0216956, recall 0.680628\n",
      "2017-12-07T23:53:30.185742: step 189, loss 4.29674, acc 0.375, prec 0.0217139, recall 0.682292\n",
      "2017-12-07T23:53:30.763455: step 190, loss 3.28013, acc 0.34375, prec 0.0215638, recall 0.682292\n",
      "2017-12-07T23:53:31.330675: step 191, loss 3.5891, acc 0.3125, prec 0.0214087, recall 0.682292\n",
      "2017-12-07T23:53:31.904178: step 192, loss 2.46347, acc 0.515625, prec 0.0214599, recall 0.683938\n",
      "2017-12-07T23:53:32.484675: step 193, loss 2.21238, acc 0.5625, prec 0.0213627, recall 0.683938\n",
      "2017-12-07T23:53:33.058796: step 194, loss 1.55872, acc 0.6875, prec 0.0212938, recall 0.683938\n",
      "2017-12-07T23:53:33.625069: step 195, loss 1.8291, acc 0.5625, prec 0.021198, recall 0.683938\n",
      "2017-12-07T23:53:34.194674: step 196, loss 1.33816, acc 0.671875, prec 0.0211268, recall 0.683938\n",
      "2017-12-07T23:53:34.764658: step 197, loss 1.68959, acc 0.625, prec 0.0210459, recall 0.683938\n",
      "2017-12-07T23:53:35.331348: step 198, loss 10.712, acc 0.703125, prec 0.0211413, recall 0.682051\n",
      "2017-12-07T23:53:35.887994: step 199, loss 2.12032, acc 0.6875, prec 0.0212294, recall 0.683673\n",
      "2017-12-07T23:53:36.444709: step 200, loss 1.18351, acc 0.671875, prec 0.021159, recall 0.683673\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512708702/checkpoints/model-200\n",
      "\n",
      "2017-12-07T23:53:37.792163: step 201, loss 0.836761, acc 0.734375, prec 0.0212565, recall 0.685279\n",
      "2017-12-07T23:53:38.389428: step 202, loss 0.839533, acc 0.78125, prec 0.0213635, recall 0.686869\n",
      "2017-12-07T23:53:38.956551: step 203, loss 2.90196, acc 0.71875, prec 0.0213066, recall 0.683417\n",
      "2017-12-07T23:53:39.522518: step 204, loss 0.647412, acc 0.765625, prec 0.0212566, recall 0.683417\n",
      "2017-12-07T23:53:40.078471: step 205, loss 4.7876, acc 0.78125, prec 0.0212135, recall 0.68\n",
      "2017-12-07T23:53:40.634073: step 206, loss 0.884392, acc 0.8125, prec 0.0211739, recall 0.68\n",
      "2017-12-07T23:53:41.199454: step 207, loss 0.571005, acc 0.84375, prec 0.0212931, recall 0.681592\n",
      "2017-12-07T23:53:41.770236: step 208, loss 1.24826, acc 0.796875, prec 0.0215537, recall 0.684729\n",
      "2017-12-07T23:53:42.342051: step 209, loss 1.27976, acc 0.671875, prec 0.0214838, recall 0.684729\n",
      "2017-12-07T23:53:42.902912: step 210, loss 0.565306, acc 0.875, prec 0.0214572, recall 0.684729\n",
      "2017-12-07T23:53:43.461348: step 211, loss 19.9299, acc 0.71875, prec 0.0214077, recall 0.674757\n",
      "2017-12-07T23:53:44.030646: step 212, loss 1.19918, acc 0.703125, prec 0.0214955, recall 0.676328\n",
      "2017-12-07T23:53:44.596330: step 213, loss 0.867849, acc 0.765625, prec 0.0214461, recall 0.676328\n",
      "2017-12-07T23:53:45.153194: step 214, loss 9.69318, acc 0.65625, prec 0.0215267, recall 0.674641\n",
      "2017-12-07T23:53:45.718582: step 215, loss 11.8086, acc 0.640625, prec 0.0216035, recall 0.672986\n",
      "2017-12-07T23:53:46.279326: step 216, loss 1.98292, acc 0.46875, prec 0.0214924, recall 0.672986\n",
      "2017-12-07T23:53:46.837904: step 217, loss 2.30548, acc 0.546875, prec 0.0213984, recall 0.672986\n",
      "2017-12-07T23:53:47.447188: step 218, loss 2.79731, acc 0.578125, prec 0.0213117, recall 0.672986\n",
      "2017-12-07T23:53:48.012924: step 219, loss 2.93467, acc 0.453125, prec 0.0213465, recall 0.674528\n",
      "2017-12-07T23:53:48.579990: step 220, loss 2.4614, acc 0.421875, prec 0.0212292, recall 0.674528\n",
      "2017-12-07T23:53:49.137117: step 221, loss 2.89083, acc 0.359375, prec 0.0213896, recall 0.67757\n",
      "2017-12-07T23:53:49.695544: step 222, loss 2.92257, acc 0.390625, prec 0.0214108, recall 0.67907\n",
      "2017-12-07T23:53:50.248112: step 223, loss 2.55351, acc 0.4375, prec 0.0214411, recall 0.680556\n",
      "2017-12-07T23:53:50.806412: step 224, loss 2.76368, acc 0.46875, prec 0.0213353, recall 0.680556\n",
      "2017-12-07T23:53:51.377372: step 225, loss 2.60393, acc 0.46875, prec 0.0212305, recall 0.680556\n",
      "2017-12-07T23:53:51.944010: step 226, loss 2.79966, acc 0.34375, prec 0.021243, recall 0.682028\n",
      "2017-12-07T23:53:52.500219: step 227, loss 4.90696, acc 0.484375, prec 0.0214255, recall 0.681818\n",
      "2017-12-07T23:53:53.051032: step 228, loss 4.95575, acc 0.5625, prec 0.0214824, recall 0.68018\n",
      "2017-12-07T23:53:53.609726: step 229, loss 2.26876, acc 0.546875, prec 0.0213942, recall 0.68018\n",
      "2017-12-07T23:53:54.168261: step 230, loss 8.02867, acc 0.4375, prec 0.0212886, recall 0.67713\n",
      "2017-12-07T23:53:54.724250: step 231, loss 1.80921, acc 0.5625, prec 0.0212049, recall 0.67713\n",
      "2017-12-07T23:53:55.270541: step 232, loss 2.47364, acc 0.453125, prec 0.0213747, recall 0.68\n",
      "2017-12-07T23:53:55.823988: step 233, loss 2.09411, acc 0.5, prec 0.0212796, recall 0.68\n",
      "2017-12-07T23:53:56.376122: step 234, loss 10.6394, acc 0.53125, prec 0.0213326, recall 0.675439\n",
      "2017-12-07T23:53:56.924290: step 235, loss 2.07454, acc 0.4375, prec 0.0212267, recall 0.675439\n",
      "2017-12-07T23:53:57.523991: step 236, loss 3.08941, acc 0.53125, prec 0.0211393, recall 0.675439\n",
      "2017-12-07T23:53:58.091488: step 237, loss 2.40078, acc 0.453125, prec 0.021172, recall 0.676856\n",
      "2017-12-07T23:53:58.649242: step 238, loss 1.65122, acc 0.578125, prec 0.0212274, recall 0.678261\n",
      "2017-12-07T23:53:59.193263: step 239, loss 4.53805, acc 0.515625, prec 0.0211411, recall 0.675325\n",
      "2017-12-07T23:53:59.763738: step 240, loss 3.99293, acc 0.515625, prec 0.0210555, recall 0.672414\n",
      "2017-12-07T23:54:00.319688: step 241, loss 2.56912, acc 0.515625, prec 0.0209677, recall 0.672414\n",
      "2017-12-07T23:54:00.878274: step 242, loss 1.99202, acc 0.53125, prec 0.0210146, recall 0.67382\n",
      "2017-12-07T23:54:01.440854: step 243, loss 2.11646, acc 0.515625, prec 0.0210582, recall 0.675214\n",
      "2017-12-07T23:54:01.996660: step 244, loss 5.23632, acc 0.625, prec 0.021124, recall 0.673729\n",
      "2017-12-07T23:54:02.554451: step 245, loss 10.4481, acc 0.5, prec 0.0210373, recall 0.670886\n",
      "2017-12-07T23:54:03.099548: step 246, loss 2.20694, acc 0.453125, prec 0.0210693, recall 0.672269\n",
      "2017-12-07T23:54:03.654880: step 247, loss 2.45575, acc 0.4375, prec 0.0209699, recall 0.672269\n",
      "2017-12-07T23:54:04.208359: step 248, loss 2.585, acc 0.453125, prec 0.0208741, recall 0.672269\n",
      "2017-12-07T23:54:04.762076: step 249, loss 2.94591, acc 0.4375, prec 0.0210308, recall 0.675\n",
      "2017-12-07T23:54:05.314091: step 250, loss 2.69128, acc 0.53125, prec 0.0210758, recall 0.676349\n",
      "2017-12-07T23:54:05.867059: step 251, loss 2.76095, acc 0.515625, prec 0.0209916, recall 0.676349\n",
      "2017-12-07T23:54:06.413091: step 252, loss 1.35348, acc 0.609375, prec 0.0209243, recall 0.676349\n",
      "2017-12-07T23:54:06.963726: step 253, loss 2.21241, acc 0.46875, prec 0.0208333, recall 0.676349\n",
      "2017-12-07T23:54:07.551324: step 254, loss 1.84421, acc 0.5625, prec 0.020759, recall 0.676349\n",
      "2017-12-07T23:54:08.107106: step 255, loss 3.00233, acc 0.640625, prec 0.020701, recall 0.673554\n",
      "2017-12-07T23:54:08.661040: step 256, loss 1.32577, acc 0.546875, prec 0.0206251, recall 0.673554\n",
      "2017-12-07T23:54:09.205866: step 257, loss 6.44617, acc 0.640625, prec 0.0205678, recall 0.670782\n",
      "2017-12-07T23:54:09.755241: step 258, loss 2.00066, acc 0.609375, prec 0.0208726, recall 0.674797\n",
      "2017-12-07T23:54:10.311781: step 259, loss 2.56884, acc 0.59375, prec 0.0209273, recall 0.676113\n",
      "2017-12-07T23:54:10.861530: step 260, loss 1.56689, acc 0.609375, prec 0.020862, recall 0.676113\n",
      "2017-12-07T23:54:11.406763: step 261, loss 1.38526, acc 0.671875, prec 0.0208074, recall 0.676113\n",
      "2017-12-07T23:54:11.966815: step 262, loss 2.33904, acc 0.671875, prec 0.021118, recall 0.68\n",
      "2017-12-07T23:54:12.518550: step 263, loss 9.73805, acc 0.578125, prec 0.02105, recall 0.677291\n",
      "2017-12-07T23:54:13.069218: step 264, loss 1.54407, acc 0.59375, prec 0.0211033, recall 0.678571\n",
      "2017-12-07T23:54:13.616397: step 265, loss 5.73301, acc 0.59375, prec 0.0211588, recall 0.677165\n",
      "2017-12-07T23:54:14.167590: step 266, loss 1.09507, acc 0.71875, prec 0.0212322, recall 0.678431\n",
      "2017-12-07T23:54:14.713880: step 267, loss 1.83998, acc 0.625, prec 0.0211698, recall 0.678431\n",
      "2017-12-07T23:54:15.256714: step 268, loss 5.44052, acc 0.5625, prec 0.0211001, recall 0.675781\n",
      "2017-12-07T23:54:15.806452: step 269, loss 1.69691, acc 0.546875, prec 0.0210258, recall 0.675781\n",
      "2017-12-07T23:54:16.362260: step 270, loss 1.82987, acc 0.546875, prec 0.0210705, recall 0.677043\n",
      "2017-12-07T23:54:16.912202: step 271, loss 3.94236, acc 0.40625, prec 0.0209765, recall 0.674419\n",
      "2017-12-07T23:54:17.504892: step 272, loss 3.10636, acc 0.640625, prec 0.0210387, recall 0.673077\n",
      "2017-12-07T23:54:18.079645: step 273, loss 2.89946, acc 0.453125, prec 0.0210678, recall 0.67433\n",
      "2017-12-07T23:54:18.637666: step 274, loss 2.85238, acc 0.40625, prec 0.0209724, recall 0.67433\n",
      "2017-12-07T23:54:19.194253: step 275, loss 2.51446, acc 0.40625, prec 0.0208778, recall 0.67433\n",
      "2017-12-07T23:54:19.744039: step 276, loss 2.28612, acc 0.484375, prec 0.0209121, recall 0.675573\n",
      "2017-12-07T23:54:20.293540: step 277, loss 3.90922, acc 0.5, prec 0.020951, recall 0.674242\n",
      "2017-12-07T23:54:20.848263: step 278, loss 1.83117, acc 0.671875, prec 0.0211292, recall 0.676692\n",
      "2017-12-07T23:54:21.417212: step 279, loss 1.88164, acc 0.546875, prec 0.0210576, recall 0.676692\n",
      "2017-12-07T23:54:21.973311: step 280, loss 3.50592, acc 0.546875, prec 0.0212146, recall 0.679105\n",
      "2017-12-07T23:54:22.542086: step 281, loss 2.38462, acc 0.609375, prec 0.021153, recall 0.679105\n",
      "2017-12-07T23:54:23.110281: step 282, loss 2.6126, acc 0.46875, prec 0.0210697, recall 0.679105\n",
      "2017-12-07T23:54:23.675821: step 283, loss 1.97058, acc 0.546875, prec 0.0209992, recall 0.679105\n",
      "2017-12-07T23:54:24.237988: step 284, loss 8.89944, acc 0.609375, prec 0.0211665, recall 0.678967\n",
      "2017-12-07T23:54:24.807472: step 285, loss 4.72009, acc 0.5625, prec 0.0211009, recall 0.676471\n",
      "2017-12-07T23:54:25.391193: step 286, loss 2.00932, acc 0.65625, prec 0.0211598, recall 0.677656\n",
      "2017-12-07T23:54:25.980852: step 287, loss 1.85869, acc 0.5625, prec 0.0210922, recall 0.677656\n",
      "2017-12-07T23:54:26.556780: step 288, loss 1.45306, acc 0.640625, prec 0.0211484, recall 0.678832\n",
      "2017-12-07T23:54:27.141508: step 289, loss 2.24899, acc 0.53125, prec 0.0211874, recall 0.68\n",
      "2017-12-07T23:54:27.745691: step 290, loss 1.69772, acc 0.578125, prec 0.0212333, recall 0.681159\n",
      "2017-12-07T23:54:28.324053: step 291, loss 12.407, acc 0.515625, prec 0.0212718, recall 0.679856\n",
      "2017-12-07T23:54:28.939260: step 292, loss 6.04119, acc 0.609375, prec 0.0212145, recall 0.677419\n",
      "2017-12-07T23:54:29.562096: step 293, loss 1.97134, acc 0.5625, prec 0.021148, recall 0.677419\n",
      "2017-12-07T23:54:30.171296: step 294, loss 2.33171, acc 0.6875, prec 0.0212101, recall 0.678571\n",
      "2017-12-07T23:54:30.765522: step 295, loss 2.83819, acc 0.5625, prec 0.0213618, recall 0.680851\n",
      "2017-12-07T23:54:31.338154: step 296, loss 1.83988, acc 0.546875, prec 0.0212931, recall 0.680851\n",
      "2017-12-07T23:54:31.915742: step 297, loss 2.102, acc 0.546875, prec 0.021333, recall 0.681979\n",
      "2017-12-07T23:54:32.502492: step 298, loss 2.59915, acc 0.484375, prec 0.021471, recall 0.684211\n",
      "2017-12-07T23:54:33.089441: step 299, loss 1.82199, acc 0.5625, prec 0.021405, recall 0.684211\n",
      "2017-12-07T23:54:33.672505: step 300, loss 2.35699, acc 0.5, prec 0.0216512, recall 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2017-12-07T23:55:50.958125: step 300, loss 1.60667, acc 0.520521, prec 0.0246261, recall 0.783186\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512708702/checkpoints/model-300\n",
      "\n",
      "2017-12-07T23:55:53.319765: step 301, loss 1.63794, acc 0.5, prec 0.0245714, recall 0.783186\n",
      "2017-12-07T23:55:53.906881: step 302, loss 2.37177, acc 0.5, prec 0.0245845, recall 0.783664\n",
      "2017-12-07T23:55:54.464829: step 303, loss 1.96932, acc 0.640625, prec 0.0246128, recall 0.784141\n",
      "2017-12-07T23:55:55.017199: step 304, loss 1.99567, acc 0.578125, prec 0.0247016, recall 0.785088\n",
      "2017-12-07T23:55:55.563931: step 305, loss 1.32869, acc 0.625, prec 0.0246607, recall 0.785088\n",
      "2017-12-07T23:55:56.106603: step 306, loss 1.72607, acc 0.71875, prec 0.0246973, recall 0.785558\n",
      "2017-12-07T23:55:56.635826: step 307, loss 1.47865, acc 0.625, prec 0.0246566, recall 0.785558\n",
      "2017-12-07T23:55:57.171804: step 308, loss 0.977076, acc 0.6875, prec 0.0246228, recall 0.785558\n",
      "2017-12-07T23:55:57.697782: step 309, loss 6.44042, acc 0.765625, prec 0.024666, recall 0.784314\n",
      "2017-12-07T23:55:58.278408: step 310, loss 1.37935, acc 0.625, prec 0.0246255, recall 0.784314\n",
      "2017-12-07T23:55:58.864154: step 311, loss 0.959678, acc 0.734375, prec 0.0245969, recall 0.784314\n",
      "2017-12-07T23:55:59.434228: step 312, loss 4.01776, acc 0.71875, prec 0.0246349, recall 0.78308\n",
      "2017-12-07T23:56:00.012153: step 313, loss 11.544, acc 0.6875, prec 0.0247359, recall 0.782328\n",
      "2017-12-07T23:56:00.629610: step 314, loss 0.780923, acc 0.875, prec 0.0248553, recall 0.783262\n",
      "2017-12-07T23:56:01.288519: step 315, loss 2.03849, acc 0.796875, prec 0.024835, recall 0.781585\n",
      "2017-12-07T23:56:01.900019: step 316, loss 1.63082, acc 0.625, prec 0.0248608, recall 0.782051\n",
      "2017-12-07T23:56:02.515423: step 317, loss 1.19153, acc 0.671875, prec 0.0248253, recall 0.782051\n",
      "2017-12-07T23:56:03.119539: step 318, loss 2.57737, acc 0.703125, prec 0.0247951, recall 0.780384\n",
      "2017-12-07T23:56:03.740893: step 319, loss 12.574, acc 0.515625, prec 0.0248766, recall 0.779661\n",
      "2017-12-07T23:56:04.394915: step 320, loss 1.274, acc 0.703125, prec 0.0248447, recall 0.779661\n",
      "2017-12-07T23:56:05.024243: step 321, loss 1.47261, acc 0.640625, prec 0.0248719, recall 0.780127\n",
      "2017-12-07T23:56:05.634999: step 322, loss 1.99911, acc 0.59375, prec 0.024894, recall 0.780591\n",
      "2017-12-07T23:56:06.247982: step 323, loss 2.17325, acc 0.53125, prec 0.0248439, recall 0.780591\n",
      "2017-12-07T23:56:06.968805: step 324, loss 2.14269, acc 0.546875, prec 0.0247956, recall 0.780591\n",
      "2017-12-07T23:56:07.587346: step 325, loss 1.25005, acc 0.65625, prec 0.0248244, recall 0.781053\n",
      "2017-12-07T23:56:08.217144: step 326, loss 3.05935, acc 0.640625, prec 0.0247879, recall 0.779412\n",
      "2017-12-07T23:56:08.837308: step 327, loss 2.3438, acc 0.5, prec 0.024735, recall 0.779412\n",
      "2017-12-07T23:56:09.525009: step 328, loss 2.21204, acc 0.546875, prec 0.0247521, recall 0.779874\n",
      "2017-12-07T23:56:10.121114: step 329, loss 8.29942, acc 0.53125, prec 0.024834, recall 0.779167\n",
      "2017-12-07T23:56:10.726930: step 330, loss 1.28956, acc 0.703125, prec 0.0248027, recall 0.779167\n",
      "2017-12-07T23:56:11.312104: step 331, loss 4.24808, acc 0.4375, prec 0.0247453, recall 0.777547\n",
      "2017-12-07T23:56:11.897610: step 332, loss 2.35064, acc 0.53125, prec 0.0247606, recall 0.778008\n",
      "2017-12-07T23:56:12.476801: step 333, loss 1.93208, acc 0.546875, prec 0.0247133, recall 0.778008\n",
      "2017-12-07T23:56:13.065636: step 334, loss 2.74557, acc 0.453125, prec 0.0246565, recall 0.778008\n",
      "2017-12-07T23:56:13.636287: step 335, loss 3.13166, acc 0.453125, prec 0.0245998, recall 0.778008\n",
      "2017-12-07T23:56:14.245095: step 336, loss 1.75008, acc 0.53125, prec 0.0246154, recall 0.778468\n",
      "2017-12-07T23:56:14.838803: step 337, loss 1.81256, acc 0.578125, prec 0.024572, recall 0.778468\n",
      "2017-12-07T23:56:15.418678: step 338, loss 2.00314, acc 0.625, prec 0.0245971, recall 0.778926\n",
      "2017-12-07T23:56:15.993201: step 339, loss 1.85863, acc 0.5625, prec 0.0245523, recall 0.778926\n",
      "2017-12-07T23:56:16.558358: step 340, loss 1.22881, acc 0.65625, prec 0.024644, recall 0.779835\n",
      "2017-12-07T23:56:17.168684: step 341, loss 1.50509, acc 0.609375, prec 0.024604, recall 0.779835\n",
      "2017-12-07T23:56:17.777687: step 342, loss 6.95814, acc 0.703125, prec 0.0245753, recall 0.778234\n",
      "2017-12-07T23:56:18.406729: step 343, loss 5.95858, acc 0.59375, prec 0.0245355, recall 0.776639\n",
      "2017-12-07T23:56:18.988595: step 344, loss 1.2574, acc 0.734375, prec 0.0245085, recall 0.776639\n",
      "2017-12-07T23:56:19.582053: step 345, loss 1.14114, acc 0.65625, prec 0.0244737, recall 0.776639\n",
      "2017-12-07T23:56:20.173560: step 346, loss 2.42557, acc 0.734375, prec 0.0245743, recall 0.775967\n",
      "2017-12-07T23:56:20.753289: step 347, loss 1.00872, acc 0.671875, prec 0.0245411, recall 0.775967\n",
      "2017-12-07T23:56:21.317436: step 348, loss 2.3417, acc 0.71875, prec 0.0245142, recall 0.77439\n",
      "2017-12-07T23:56:21.880026: step 349, loss 3.39695, acc 0.671875, prec 0.0245454, recall 0.773279\n",
      "2017-12-07T23:56:22.445162: step 350, loss 0.983727, acc 0.75, prec 0.0245828, recall 0.773737\n",
      "2017-12-07T23:56:23.038218: step 351, loss 1.47426, acc 0.703125, prec 0.0245529, recall 0.773737\n",
      "2017-12-07T23:56:23.593929: step 352, loss 1.69759, acc 0.640625, prec 0.0245167, recall 0.773737\n",
      "2017-12-07T23:56:24.135424: step 353, loss 8.16011, acc 0.609375, prec 0.0244791, recall 0.772177\n",
      "2017-12-07T23:56:24.673827: step 354, loss 1.52338, acc 0.640625, prec 0.0244432, recall 0.772177\n",
      "2017-12-07T23:56:25.210470: step 355, loss 1.83996, acc 0.59375, prec 0.0244027, recall 0.772177\n",
      "2017-12-07T23:56:25.746138: step 356, loss 2.3244, acc 0.515625, prec 0.0244786, recall 0.773092\n",
      "2017-12-07T23:56:26.287803: step 357, loss 9.48653, acc 0.609375, prec 0.0245033, recall 0.772\n",
      "2017-12-07T23:56:26.840181: step 358, loss 2.60518, acc 0.5, prec 0.0245154, recall 0.772455\n",
      "2017-12-07T23:56:27.375619: step 359, loss 2.0173, acc 0.40625, prec 0.0244565, recall 0.772455\n",
      "2017-12-07T23:56:27.912234: step 360, loss 2.16969, acc 0.53125, prec 0.0244718, recall 0.772908\n",
      "2017-12-07T23:56:28.453249: step 361, loss 2.72132, acc 0.46875, prec 0.0244808, recall 0.77336\n",
      "2017-12-07T23:56:29.004766: step 362, loss 3.11501, acc 0.546875, prec 0.024499, recall 0.772277\n",
      "2017-12-07T23:56:29.555271: step 363, loss 1.73296, acc 0.53125, prec 0.0244529, recall 0.772277\n",
      "2017-12-07T23:56:30.119116: step 364, loss 1.27985, acc 0.703125, prec 0.0244238, recall 0.772277\n",
      "2017-12-07T23:56:30.683889: step 365, loss 1.70481, acc 0.578125, prec 0.0243826, recall 0.772277\n",
      "2017-12-07T23:56:31.244945: step 366, loss 1.99599, acc 0.6875, prec 0.0245349, recall 0.773622\n",
      "2017-12-07T23:56:31.811519: step 367, loss 1.26306, acc 0.625, prec 0.024559, recall 0.774067\n",
      "2017-12-07T23:56:32.377416: step 368, loss 1.21207, acc 0.671875, prec 0.0245876, recall 0.77451\n",
      "2017-12-07T23:56:32.944731: step 369, loss 2.99099, acc 0.609375, prec 0.0246706, recall 0.775391\n",
      "2017-12-07T23:56:33.505631: step 370, loss 1.1985, acc 0.75, prec 0.0246461, recall 0.775391\n",
      "2017-12-07T23:56:34.074047: step 371, loss 1.43262, acc 0.578125, prec 0.0246653, recall 0.775828\n",
      "2017-12-07T23:56:34.638875: step 372, loss 3.32502, acc 0.6875, prec 0.0246363, recall 0.774319\n",
      "2017-12-07T23:56:35.202325: step 373, loss 1.69222, acc 0.609375, prec 0.0245983, recall 0.774319\n",
      "2017-12-07T23:56:35.774611: step 374, loss 1.22789, acc 0.640625, prec 0.0245634, recall 0.774319\n",
      "2017-12-07T23:56:36.321082: step 375, loss 1.14122, acc 0.6875, prec 0.0245932, recall 0.774757\n",
      "2017-12-07T23:56:36.905331: step 376, loss 0.809952, acc 0.765625, prec 0.0246305, recall 0.775194\n",
      "2017-12-07T23:56:37.470679: step 377, loss 3.90679, acc 0.671875, prec 0.0247202, recall 0.774566\n",
      "2017-12-07T23:56:38.030782: step 378, loss 1.14058, acc 0.6875, prec 0.0247497, recall 0.775\n",
      "2017-12-07T23:56:38.607074: step 379, loss 1.26467, acc 0.671875, prec 0.0247179, recall 0.775\n",
      "2017-12-07T23:56:39.179679: step 380, loss 1.72146, acc 0.640625, prec 0.0247428, recall 0.775432\n",
      "2017-12-07T23:56:39.737265: step 381, loss 2.59981, acc 0.71875, prec 0.0249541, recall 0.777143\n",
      "2017-12-07T23:56:40.302539: step 382, loss 0.931084, acc 0.71875, prec 0.0249267, recall 0.777143\n",
      "2017-12-07T23:56:40.872354: step 383, loss 6.68606, acc 0.59375, prec 0.0248887, recall 0.775665\n",
      "2017-12-07T23:56:41.455427: step 384, loss 18.4954, acc 0.5625, prec 0.0248477, recall 0.774194\n",
      "2017-12-07T23:56:42.018608: step 385, loss 1.11133, acc 0.640625, prec 0.024813, recall 0.774194\n",
      "2017-12-07T23:56:42.579841: step 386, loss 1.44608, acc 0.578125, prec 0.0247723, recall 0.774194\n",
      "2017-12-07T23:56:43.140150: step 387, loss 0.924617, acc 0.765625, prec 0.0247498, recall 0.774194\n",
      "2017-12-07T23:56:43.706672: step 388, loss 1.77532, acc 0.609375, prec 0.0247714, recall 0.774621\n",
      "2017-12-07T23:56:44.264277: step 389, loss 1.44744, acc 0.59375, prec 0.0247324, recall 0.774621\n",
      "2017-12-07T23:56:44.805583: step 390, loss 1.45802, acc 0.65625, prec 0.0246996, recall 0.774621\n",
      "2017-12-07T23:56:45.338513: step 391, loss 1.49595, acc 0.6875, prec 0.0247286, recall 0.775047\n",
      "2017-12-07T23:56:45.874987: step 392, loss 4.69654, acc 0.609375, prec 0.0246928, recall 0.773585\n",
      "2017-12-07T23:56:46.419337: step 393, loss 10.9586, acc 0.734375, prec 0.0246691, recall 0.772128\n",
      "2017-12-07T23:56:46.959930: step 394, loss 2.49244, acc 0.609375, prec 0.0246335, recall 0.770677\n",
      "2017-12-07T23:56:47.499103: step 395, loss 1.60085, acc 0.609375, prec 0.0245966, recall 0.770677\n",
      "2017-12-07T23:56:48.036003: step 396, loss 1.82629, acc 0.515625, prec 0.0246093, recall 0.771107\n",
      "2017-12-07T23:56:48.615549: step 397, loss 2.65342, acc 0.4375, prec 0.0246146, recall 0.771536\n",
      "2017-12-07T23:56:49.159699: step 398, loss 1.90609, acc 0.5, prec 0.0246258, recall 0.771963\n",
      "2017-12-07T23:56:49.713004: step 399, loss 2.1733, acc 0.515625, prec 0.0246965, recall 0.772812\n",
      "2017-12-07T23:56:50.264383: step 400, loss 1.40017, acc 0.6875, prec 0.0246671, recall 0.772812\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold0/1512708702/checkpoints/model-400\n",
      "\n",
      "2017-12-07T23:56:51.668632: step 401, loss 1.57288, acc 0.546875, prec 0.0246247, recall 0.772812\n",
      "2017-12-07T23:56:52.256810: step 402, loss 1.62146, acc 0.625, prec 0.0245897, recall 0.772812\n",
      "2017-12-07T23:56:52.797551: step 403, loss 1.24616, acc 0.609375, prec 0.0246687, recall 0.773655\n",
      "2017-12-07T23:56:53.338096: step 404, loss 2.59144, acc 0.640625, prec 0.0246943, recall 0.772643\n",
      "2017-12-07T23:56:53.873793: step 405, loss 3.73075, acc 0.640625, prec 0.0247198, recall 0.771639\n",
      "2017-12-07T23:56:54.415839: step 406, loss 1.09935, acc 0.671875, prec 0.0247466, recall 0.772059\n",
      "2017-12-07T23:56:54.953872: step 407, loss 1.16255, acc 0.6875, prec 0.0248323, recall 0.772894\n",
      "2017-12-07T23:56:55.492203: step 408, loss 1.31916, acc 0.765625, prec 0.024925, recall 0.773723\n",
      "2017-12-07T23:56:56.028509: step 409, loss 1.18646, acc 0.703125, prec 0.0248972, recall 0.773723\n",
      "2017-12-07T23:56:56.574353: step 410, loss 1.84175, acc 0.625, prec 0.0248622, recall 0.773723\n",
      "2017-12-07T23:56:57.127099: step 411, loss 1.15223, acc 0.6875, prec 0.0248331, recall 0.773723\n",
      "2017-12-07T23:56:57.675198: step 412, loss 7.15358, acc 0.78125, prec 0.0248142, recall 0.772313\n",
      "2017-12-07T23:56:58.256825: step 413, loss 1.13088, acc 0.703125, prec 0.0248436, recall 0.772727\n",
      "2017-12-07T23:56:58.832833: step 414, loss 0.603128, acc 0.828125, prec 0.0248277, recall 0.772727\n",
      "2017-12-07T23:56:59.409866: step 415, loss 1.45497, acc 0.71875, prec 0.0248585, recall 0.77314\n",
      "2017-12-07T23:56:59.989968: step 416, loss 1.10571, acc 0.703125, prec 0.0248878, recall 0.773551\n",
      "2017-12-07T23:57:00.556892: step 417, loss 1.36557, acc 0.640625, prec 0.0248545, recall 0.773551\n",
      "2017-12-07T23:57:01.125976: step 418, loss 6.15192, acc 0.6875, prec 0.024827, recall 0.772152\n",
      "2017-12-07T23:57:01.707679: step 419, loss 0.989062, acc 0.65625, prec 0.0247953, recall 0.772152\n",
      "2017-12-07T23:57:02.296587: step 420, loss 1.32052, acc 0.703125, prec 0.0248245, recall 0.772563\n",
      "2017-12-07T23:57:02.886745: step 421, loss 1.55462, acc 0.625, prec 0.02479, recall 0.772563\n",
      "2017-12-07T23:57:03.467250: step 422, loss 1.15111, acc 0.6875, prec 0.0247614, recall 0.772563\n",
      "2017-12-07T23:57:04.045008: step 423, loss 1.5968, acc 0.65625, prec 0.0247299, recall 0.772563\n",
      "2017-12-07T23:57:04.629717: step 424, loss 0.737237, acc 0.765625, prec 0.0247648, recall 0.772973\n",
      "2017-12-07T23:57:05.209242: step 425, loss 1.04907, acc 0.6875, prec 0.0247362, recall 0.772973\n",
      "2017-12-07T23:57:05.800380: step 426, loss 0.893703, acc 0.8125, prec 0.0247191, recall 0.772973\n",
      "2017-12-07T23:57:06.381459: step 427, loss 14.3262, acc 0.75, prec 0.0247539, recall 0.771993\n",
      "2017-12-07T23:57:06.971821: step 428, loss 4.96471, acc 0.765625, prec 0.0247901, recall 0.77102\n",
      "2017-12-07T23:57:07.557301: step 429, loss 1.08412, acc 0.734375, prec 0.0248779, recall 0.771836\n",
      "2017-12-07T23:57:08.141629: step 430, loss 0.970512, acc 0.65625, prec 0.0248465, recall 0.771836\n",
      "2017-12-07T23:57:08.760893: step 431, loss 0.796389, acc 0.765625, prec 0.0248251, recall 0.771836\n",
      "2017-12-07T23:57:09.356553: step 432, loss 1.29663, acc 0.65625, prec 0.0248497, recall 0.772242\n",
      "2017-12-07T23:57:09.943639: step 433, loss 3.44817, acc 0.59375, prec 0.0249257, recall 0.771681\n",
      "2017-12-07T23:57:10.528311: step 434, loss 1.58101, acc 0.734375, prec 0.0249572, recall 0.772085\n",
      "2017-12-07T23:57:11.118254: step 435, loss 1.52366, acc 0.75, prec 0.02499, recall 0.772487\n",
      "2017-12-07T23:57:11.697563: step 436, loss 1.25307, acc 0.71875, prec 0.0249644, recall 0.772487\n",
      "2017-12-07T23:57:12.281122: step 437, loss 1.26727, acc 0.703125, prec 0.0249374, recall 0.772487\n",
      "2017-12-07T23:57:12.866434: step 438, loss 2.052, acc 0.5625, prec 0.0250085, recall 0.773286\n",
      "2017-12-07T23:57:13.454321: step 439, loss 1.35331, acc 0.625, prec 0.0249745, recall 0.773286\n",
      "2017-12-07T23:57:14.037948: step 440, loss 1.29007, acc 0.640625, prec 0.0249419, recall 0.773286\n",
      "2017-12-07T23:57:14.619061: step 441, loss 1.25647, acc 0.578125, prec 0.0249038, recall 0.773286\n",
      "2017-12-07T23:57:15.203761: step 442, loss 1.40025, acc 0.625, prec 0.0249251, recall 0.773684\n",
      "2017-12-07T23:57:15.787558: step 443, loss 1.63031, acc 0.609375, prec 0.0248899, recall 0.773684\n",
      "2017-12-07T23:57:16.374159: step 444, loss 2.97129, acc 0.671875, prec 0.0248619, recall 0.772329\n",
      "2017-12-07T23:57:16.961632: step 445, loss 17.8376, acc 0.65625, prec 0.0248888, recall 0.770035\n",
      "2017-12-07T23:57:17.607387: step 446, loss 1.61483, acc 0.71875, prec 0.0249184, recall 0.770435\n",
      "2017-12-07T23:57:18.245564: step 447, loss 1.11869, acc 0.65625, prec 0.0249424, recall 0.770833\n",
      "2017-12-07T23:57:18.877023: step 448, loss 1.59921, acc 0.640625, prec 0.0249649, recall 0.771231\n",
      "2017-12-07T23:57:19.466061: step 449, loss 1.35879, acc 0.625, prec 0.024986, recall 0.771626\n",
      "2017-12-07T23:57:20.053947: step 450, loss 1.33144, acc 0.65625, prec 0.0250643, recall 0.772414\n",
      "2017-12-07T23:57:20.638582: step 451, loss 1.63379, acc 0.625, prec 0.0250307, recall 0.772414\n",
      "2017-12-07T23:57:21.222217: step 452, loss 9.75029, acc 0.6875, prec 0.025113, recall 0.77187\n",
      "2017-12-07T23:57:21.795298: step 453, loss 2.09101, acc 0.5625, prec 0.0250738, recall 0.77187\n",
      "2017-12-07T23:57:22.370671: step 454, loss 1.90961, acc 0.59375, prec 0.0250918, recall 0.77226\n",
      "2017-12-07T23:57:22.947188: step 455, loss 1.7049, acc 0.578125, prec 0.0250542, recall 0.77226\n",
      "2017-12-07T23:57:23.536456: step 456, loss 1.96837, acc 0.640625, prec 0.0250763, recall 0.77265\n",
      "2017-12-07T23:57:24.128448: step 457, loss 1.62736, acc 0.578125, prec 0.0250388, recall 0.77265\n",
      "2017-12-07T23:57:24.713080: step 458, loss 1.61206, acc 0.625, prec 0.0250595, recall 0.773038\n",
      "2017-12-07T23:57:25.293095: step 459, loss 1.04433, acc 0.671875, prec 0.0250304, recall 0.773038\n",
      "2017-12-07T23:57:25.989516: step 460, loss 2.35751, acc 0.734375, prec 0.0251145, recall 0.77381\n",
      "2017-12-07T23:57:26.621516: step 461, loss 1.10759, acc 0.703125, prec 0.0250882, recall 0.77381\n",
      "2017-12-07T23:57:27.289217: step 462, loss 2.41249, acc 0.5625, prec 0.0251569, recall 0.774576\n",
      "2017-12-07T23:57:27.869247: step 463, loss 1.26052, acc 0.75, prec 0.0251347, recall 0.774576\n",
      "2017-12-07T23:57:28.444843: step 464, loss 1.29967, acc 0.71875, prec 0.0251635, recall 0.774958\n",
      "2017-12-07T23:57:29.091655: step 465, loss 1.27062, acc 0.6875, prec 0.0251893, recall 0.775338\n",
      "2017-12-07T23:57:29.706849: step 466, loss 1.28401, acc 0.640625, prec 0.0251576, recall 0.775338\n",
      "2017-12-07T23:57:30.308840: step 467, loss 5.65133, acc 0.703125, prec 0.0251862, recall 0.774411\n",
      "2017-12-07T23:57:30.911921: step 468, loss 1.10555, acc 0.75, prec 0.0252174, recall 0.77479\n",
      "2017-12-07T23:57:31.539137: step 469, loss 0.541686, acc 0.828125, prec 0.0252023, recall 0.77479\n",
      "2017-12-07T23:57:32.151663: step 470, loss 0.876418, acc 0.78125, prec 0.0252895, recall 0.775544\n",
      "2017-12-07T23:57:32.758693: step 471, loss 1.24299, acc 0.640625, prec 0.0252578, recall 0.775544\n",
      "2017-12-07T23:57:33.366091: step 472, loss 1.02374, acc 0.765625, prec 0.0252371, recall 0.775544\n",
      "2017-12-07T23:57:33.991695: step 473, loss 1.03511, acc 0.6875, prec 0.0252096, recall 0.775544\n",
      "2017-12-07T23:57:34.601518: step 474, loss 0.441689, acc 0.8125, prec 0.0251932, recall 0.775544\n",
      "2017-12-07T23:57:35.201413: step 475, loss 0.807412, acc 0.765625, prec 0.0251726, recall 0.775544\n",
      "2017-12-07T23:57:35.807450: step 476, loss 0.900685, acc 0.765625, prec 0.0252051, recall 0.77592\n",
      "2017-12-07T23:57:36.414613: step 477, loss 0.76984, acc 0.734375, prec 0.0251818, recall 0.77592\n",
      "2017-12-07T23:57:37.017526: step 478, loss 6.24784, acc 0.78125, prec 0.0252183, recall 0.77371\n",
      "2017-12-07T23:57:37.623349: step 479, loss 3.6144, acc 0.84375, prec 0.0253117, recall 0.773179\n",
      "2017-12-07T23:57:38.213228: step 480, loss 0.94735, acc 0.671875, prec 0.0252829, recall 0.773179\n",
      "2017-12-07T23:57:38.825338: step 481, loss 12.4379, acc 0.78125, prec 0.0252651, recall 0.771901\n",
      "2017-12-07T23:57:39.407707: step 482, loss 0.776358, acc 0.765625, prec 0.0252446, recall 0.771901\n",
      "2017-12-07T23:57:40.045717: step 483, loss 1.56287, acc 0.65625, prec 0.0252672, recall 0.772277\n",
      "2017-12-07T23:57:40.647672: step 484, loss 0.724357, acc 0.796875, prec 0.0253021, recall 0.772652\n",
      "2017-12-07T23:57:41.252462: step 485, loss 5.19338, acc 0.640625, prec 0.0253246, recall 0.771757\n",
      "2017-12-07T23:57:41.850409: step 486, loss 1.68853, acc 0.609375, prec 0.0252906, recall 0.771757\n",
      "2017-12-07T23:57:42.452794: step 487, loss 2.51406, acc 0.53125, prec 0.0253022, recall 0.772131\n",
      "2017-12-07T23:57:43.054617: step 488, loss 2.33662, acc 0.5625, prec 0.0253687, recall 0.772876\n",
      "2017-12-07T23:57:43.662782: step 489, loss 2.39281, acc 0.46875, prec 0.0253747, recall 0.773246\n",
      "2017-12-07T23:57:44.271285: step 490, loss 1.84497, acc 0.5625, prec 0.0253888, recall 0.773616\n",
      "2017-12-07T23:57:44.872771: step 491, loss 2.53457, acc 0.46875, prec 0.0253428, recall 0.773616\n",
      "2017-12-07T23:57:45.481622: step 492, loss 3.09534, acc 0.484375, prec 0.0252996, recall 0.772358\n",
      "2017-12-07T23:57:46.087969: step 493, loss 2.1888, acc 0.4375, prec 0.025303, recall 0.772727\n",
      "2017-12-07T23:57:46.689708: step 494, loss 2.15134, acc 0.515625, prec 0.0253131, recall 0.773096\n",
      "2017-12-07T23:57:47.295205: step 495, loss 2.21136, acc 0.515625, prec 0.0253232, recall 0.773463\n",
      "2017-12-07T23:57:47.899602: step 496, loss 1.80788, acc 0.6875, prec 0.0252964, recall 0.773463\n",
      "2017-12-07T23:57:48.410979: step 497, loss 1.50621, acc 0.568627, prec 0.0252669, recall 0.773463\n",
      "Training finished\n",
      "Fold: 1 => Train/Dev split: 31795/10599\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold1/1512709069\n",
      "\n",
      "Start training\n",
      "2017-12-07T23:57:52.483115: step 1, loss 12.3164, acc 0.578125, prec 0, recall 0\n",
      "2017-12-07T23:57:52.999708: step 2, loss 7.137, acc 0.59375, prec 0, recall 0\n",
      "2017-12-07T23:57:53.517421: step 3, loss 1.67992, acc 0.578125, prec 0, recall 0\n",
      "2017-12-07T23:57:54.037148: step 4, loss 8.30886, acc 0.40625, prec 0, recall 0\n",
      "2017-12-07T23:57:54.559450: step 5, loss 11.8301, acc 0.296875, prec 0, recall 0\n",
      "2017-12-07T23:57:55.089390: step 6, loss 3.51986, acc 0.203125, prec 0, recall 0\n",
      "2017-12-07T23:57:55.631526: step 7, loss 6.2478, acc 0.25, prec 0, recall 0\n",
      "2017-12-07T23:57:56.172797: step 8, loss 7.18916, acc 0.21875, prec 0, recall 0\n",
      "2017-12-07T23:57:56.731863: step 9, loss 4.84174, acc 0.1875, prec 0.00558659, recall 0.2\n",
      "2017-12-07T23:57:57.295315: step 10, loss 4.58961, acc 0.1875, prec 0.00970874, recall 0.333333\n",
      "2017-12-07T23:57:57.864913: step 11, loss 4.96035, acc 0.203125, prec 0.0129032, recall 0.428571\n",
      "2017-12-07T23:57:58.448608: step 12, loss 5.67571, acc 0.140625, prec 0.0134357, recall 0.466667\n",
      "2017-12-07T23:57:59.048026: step 13, loss 5.46706, acc 0.125, prec 0.0121317, recall 0.466667\n",
      "2017-12-07T23:57:59.653930: step 14, loss 4.97358, acc 0.21875, prec 0.0127389, recall 0.5\n",
      "2017-12-07T23:58:00.270447: step 15, loss 4.63887, acc 0.265625, prec 0.0133136, recall 0.529412\n",
      "2017-12-07T23:58:00.874529: step 16, loss 5.26618, acc 0.296875, prec 0.0138504, recall 0.555556\n",
      "2017-12-07T23:58:01.482960: step 17, loss 10.6695, acc 0.25, prec 0.0130039, recall 0.526316\n",
      "2017-12-07T23:58:02.089289: step 18, loss 3.44143, acc 0.328125, prec 0.0123153, recall 0.526316\n",
      "2017-12-07T23:58:02.686843: step 19, loss 3.45167, acc 0.25, prec 0.0116279, recall 0.526316\n",
      "2017-12-07T23:58:03.287107: step 20, loss 2.71661, acc 0.375, prec 0.0122087, recall 0.55\n",
      "2017-12-07T23:58:03.890879: step 21, loss 2.2481, acc 0.46875, prec 0.0117647, recall 0.55\n",
      "2017-12-07T23:58:04.492157: step 22, loss 10.1719, acc 0.453125, prec 0.0113519, recall 0.52381\n",
      "2017-12-07T23:58:05.104744: step 23, loss 4.55597, acc 0.59375, prec 0.0110664, recall 0.5\n",
      "2017-12-07T23:58:05.714001: step 24, loss 2.13845, acc 0.453125, prec 0.01069, recall 0.5\n",
      "2017-12-07T23:58:06.316223: step 25, loss 1.81248, acc 0.609375, prec 0.0113744, recall 0.521739\n",
      "2017-12-07T23:58:06.913335: step 26, loss 1.56326, acc 0.640625, prec 0.0111317, recall 0.521739\n",
      "2017-12-07T23:58:07.514330: step 27, loss 1.52198, acc 0.625, prec 0.0108893, recall 0.521739\n",
      "2017-12-07T23:58:08.120098: step 28, loss 1.08519, acc 0.6875, prec 0.0115761, recall 0.541667\n",
      "2017-12-07T23:58:08.722534: step 29, loss 1.58686, acc 0.578125, prec 0.0121633, recall 0.56\n",
      "2017-12-07T23:58:09.375965: step 30, loss 7.83249, acc 0.65625, prec 0.0127986, recall 0.535714\n",
      "2017-12-07T23:58:10.004412: step 31, loss 1.06784, acc 0.703125, prec 0.0125945, recall 0.535714\n",
      "2017-12-07T23:58:10.638738: step 32, loss 1.38235, acc 0.625, prec 0.0131579, recall 0.551724\n",
      "2017-12-07T23:58:11.264870: step 33, loss 12.1578, acc 0.59375, prec 0.0128928, recall 0.533333\n",
      "2017-12-07T23:58:11.895793: step 34, loss 7.8975, acc 0.671875, prec 0.0126883, recall 0.516129\n",
      "2017-12-07T23:58:12.516365: step 35, loss 1.28327, acc 0.625, prec 0.0124514, recall 0.516129\n",
      "2017-12-07T23:58:13.147614: step 36, loss 2.13745, acc 0.609375, prec 0.0137195, recall 0.545455\n",
      "2017-12-07T23:58:13.779405: step 37, loss 1.74978, acc 0.53125, prec 0.0141474, recall 0.558824\n",
      "2017-12-07T23:58:14.410414: step 38, loss 2.13786, acc 0.53125, prec 0.0138383, recall 0.558824\n",
      "2017-12-07T23:58:15.036233: step 39, loss 2.24674, acc 0.4375, prec 0.0134847, recall 0.558824\n",
      "2017-12-07T23:58:15.640404: step 40, loss 8.25042, acc 0.5, prec 0.0138793, recall 0.555556\n",
      "2017-12-07T23:58:16.251679: step 41, loss 2.09157, acc 0.515625, prec 0.0149254, recall 0.578947\n",
      "2017-12-07T23:58:16.850740: step 42, loss 4.45747, acc 0.46875, prec 0.0145985, recall 0.564103\n",
      "2017-12-07T23:58:17.448690: step 43, loss 2.35886, acc 0.421875, prec 0.0142487, recall 0.564103\n",
      "2017-12-07T23:58:18.047325: step 44, loss 8.63866, acc 0.390625, prec 0.0145294, recall 0.560976\n",
      "2017-12-07T23:58:18.651007: step 45, loss 15.0792, acc 0.421875, prec 0.0154226, recall 0.568182\n",
      "2017-12-07T23:58:19.268196: step 46, loss 3.29123, acc 0.3125, prec 0.0156062, recall 0.577778\n",
      "2017-12-07T23:58:19.894327: step 47, loss 2.95356, acc 0.3125, prec 0.0152047, recall 0.577778\n",
      "2017-12-07T23:58:20.500399: step 48, loss 3.28924, acc 0.296875, prec 0.0148148, recall 0.577778\n",
      "2017-12-07T23:58:21.112041: step 49, loss 10.5219, acc 0.375, prec 0.0150418, recall 0.574468\n",
      "2017-12-07T23:58:21.715029: step 50, loss 8.00349, acc 0.390625, prec 0.0158038, recall 0.58\n",
      "2017-12-07T23:58:22.324839: step 51, loss 3.19198, acc 0.328125, prec 0.0159659, recall 0.588235\n",
      "2017-12-07T23:58:22.928040: step 52, loss 3.70931, acc 0.25, prec 0.0155682, recall 0.588235\n",
      "2017-12-07T23:58:23.526666: step 53, loss 4.46961, acc 0.21875, prec 0.0151745, recall 0.588235\n",
      "2017-12-07T23:58:24.129027: step 54, loss 3.35624, acc 0.21875, prec 0.0148002, recall 0.588235\n",
      "2017-12-07T23:58:24.746165: step 55, loss 9.8488, acc 0.234375, prec 0.0149326, recall 0.584906\n",
      "2017-12-07T23:58:25.347336: step 56, loss 5.77244, acc 0.296875, prec 0.0146226, recall 0.574074\n",
      "2017-12-07T23:58:25.951192: step 57, loss 4.24289, acc 0.203125, prec 0.0142791, recall 0.574074\n",
      "2017-12-07T23:58:26.558703: step 58, loss 4.8521, acc 0.265625, prec 0.0148649, recall 0.589286\n",
      "2017-12-07T23:58:27.203939: step 59, loss 3.28116, acc 0.21875, prec 0.0145374, recall 0.589286\n",
      "2017-12-07T23:58:27.874988: step 60, loss 4.12422, acc 0.1875, prec 0.0142119, recall 0.589286\n",
      "2017-12-07T23:58:28.542566: step 61, loss 3.27962, acc 0.28125, prec 0.0139358, recall 0.589286\n",
      "2017-12-07T23:58:29.162501: step 62, loss 3.91634, acc 0.265625, prec 0.0136646, recall 0.589286\n",
      "2017-12-07T23:58:29.816652: step 63, loss 3.34288, acc 0.390625, prec 0.0134474, recall 0.589286\n",
      "2017-12-07T23:58:30.456672: step 64, loss 2.99495, acc 0.421875, prec 0.0136437, recall 0.596491\n",
      "2017-12-07T23:58:31.084317: step 65, loss 2.52709, acc 0.515625, prec 0.013476, recall 0.596491\n",
      "2017-12-07T23:58:31.716616: step 66, loss 1.74513, acc 0.546875, prec 0.0133229, recall 0.596491\n",
      "2017-12-07T23:58:32.341479: step 67, loss 2.08849, acc 0.453125, prec 0.0131426, recall 0.596491\n",
      "2017-12-07T23:58:32.971207: step 68, loss 1.3926, acc 0.65625, prec 0.0130318, recall 0.596491\n",
      "2017-12-07T23:58:33.589464: step 69, loss 12.3526, acc 0.625, prec 0.0132928, recall 0.59322\n",
      "2017-12-07T23:58:34.217488: step 70, loss 1.46048, acc 0.640625, prec 0.0131777, recall 0.59322\n",
      "2017-12-07T23:58:34.848232: step 71, loss 0.763592, acc 0.75, prec 0.0130988, recall 0.59322\n",
      "2017-12-07T23:58:35.478238: step 72, loss 1.12028, acc 0.6875, prec 0.013368, recall 0.6\n",
      "2017-12-07T23:58:36.086652: step 73, loss 7.80218, acc 0.75, prec 0.0132939, recall 0.590164\n",
      "2017-12-07T23:58:36.696912: step 74, loss 8.1286, acc 0.796875, prec 0.0135979, recall 0.587302\n",
      "2017-12-07T23:58:37.308396: step 75, loss 1.07502, acc 0.84375, prec 0.0139092, recall 0.59375\n",
      "2017-12-07T23:58:37.916213: step 76, loss 1.8314, acc 0.78125, prec 0.0141973, recall 0.6\n",
      "2017-12-07T23:58:38.521953: step 77, loss 4.94694, acc 0.625, prec 0.0144352, recall 0.597015\n",
      "2017-12-07T23:58:39.122677: step 78, loss 1.99377, acc 0.609375, prec 0.0150107, recall 0.608696\n",
      "2017-12-07T23:58:39.751097: step 79, loss 1.41177, acc 0.5625, prec 0.0152105, recall 0.614286\n",
      "2017-12-07T23:58:40.355729: step 80, loss 1.16367, acc 0.671875, prec 0.015444, recall 0.619718\n",
      "2017-12-07T23:58:40.963784: step 81, loss 1.81507, acc 0.53125, prec 0.0152831, recall 0.619718\n",
      "2017-12-07T23:58:41.563269: step 82, loss 3.01909, acc 0.625, prec 0.0155012, recall 0.616438\n",
      "2017-12-07T23:58:42.168079: step 83, loss 1.95396, acc 0.46875, prec 0.0153218, recall 0.616438\n",
      "2017-12-07T23:58:42.769148: step 84, loss 1.88561, acc 0.53125, prec 0.0154987, recall 0.621622\n",
      "2017-12-07T23:58:43.371023: step 85, loss 3.28774, acc 0.484375, prec 0.0156614, recall 0.618421\n",
      "2017-12-07T23:58:43.971847: step 86, loss 1.82285, acc 0.609375, prec 0.0155321, recall 0.618421\n",
      "2017-12-07T23:58:44.585141: step 87, loss 2.02808, acc 0.609375, prec 0.0157274, recall 0.623377\n",
      "2017-12-07T23:58:45.196175: step 88, loss 1.75689, acc 0.484375, prec 0.016197, recall 0.632911\n",
      "2017-12-07T23:58:45.800132: step 89, loss 3.58055, acc 0.484375, prec 0.0160308, recall 0.625\n",
      "2017-12-07T23:58:46.408253: step 90, loss 6.24753, acc 0.484375, prec 0.015868, recall 0.617284\n",
      "2017-12-07T23:58:47.018510: step 91, loss 3.04595, acc 0.359375, prec 0.0156642, recall 0.617284\n",
      "2017-12-07T23:58:47.622543: step 92, loss 2.62654, acc 0.375, prec 0.0154703, recall 0.617284\n",
      "2017-12-07T23:58:48.230283: step 93, loss 1.97602, acc 0.53125, prec 0.0156298, recall 0.621951\n",
      "2017-12-07T23:58:48.839861: step 94, loss 1.51913, acc 0.546875, prec 0.0154921, recall 0.621951\n",
      "2017-12-07T23:58:49.445591: step 95, loss 1.73253, acc 0.546875, prec 0.0156532, recall 0.626506\n",
      "2017-12-07T23:58:50.094480: step 96, loss 1.19517, acc 0.71875, prec 0.0155689, recall 0.626506\n",
      "2017-12-07T23:58:50.703324: step 97, loss 1.32583, acc 0.671875, prec 0.0157644, recall 0.630952\n",
      "2017-12-07T23:58:51.306806: step 98, loss 1.55414, acc 0.515625, prec 0.0156204, recall 0.630952\n",
      "2017-12-07T23:58:51.920596: step 99, loss 0.730571, acc 0.796875, prec 0.0155608, recall 0.630952\n",
      "2017-12-07T23:58:52.524221: step 100, loss 1.32657, acc 0.703125, prec 0.016049, recall 0.639535\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold1/1512709069/checkpoints/model-100\n",
      "\n",
      "2017-12-07T23:58:54.018633: step 101, loss 0.90604, acc 0.75, prec 0.0159744, recall 0.639535\n",
      "2017-12-07T23:58:54.618682: step 102, loss 0.919707, acc 0.796875, prec 0.0159144, recall 0.639535\n",
      "2017-12-07T23:58:55.217148: step 103, loss 0.711207, acc 0.765625, prec 0.0158456, recall 0.639535\n",
      "2017-12-07T23:58:55.804645: step 104, loss 20.1881, acc 0.8125, prec 0.0158046, recall 0.617977\n",
      "2017-12-07T23:58:56.397989: step 105, loss 20.5979, acc 0.703125, prec 0.0157278, recall 0.604396\n",
      "2017-12-07T23:58:56.985830: step 106, loss 0.492366, acc 0.859375, prec 0.0156874, recall 0.604396\n",
      "2017-12-07T23:58:57.573358: step 107, loss 1.10335, acc 0.734375, prec 0.0156117, recall 0.604396\n",
      "2017-12-07T23:58:58.175159: step 108, loss 9.81031, acc 0.734375, prec 0.0158192, recall 0.602151\n",
      "2017-12-07T23:58:58.784763: step 109, loss 6.23443, acc 0.578125, prec 0.0157039, recall 0.595745\n",
      "2017-12-07T23:58:59.386723: step 110, loss 2.53185, acc 0.421875, prec 0.0158158, recall 0.6\n",
      "2017-12-07T23:59:00.036022: step 111, loss 2.19464, acc 0.5, prec 0.0156766, recall 0.6\n",
      "2017-12-07T23:59:00.647371: step 112, loss 3.16429, acc 0.453125, prec 0.0155271, recall 0.6\n",
      "2017-12-07T23:59:01.317681: step 113, loss 3.29282, acc 0.3125, prec 0.0156082, recall 0.604167\n",
      "2017-12-07T23:59:01.931873: step 114, loss 2.99101, acc 0.40625, prec 0.0154502, recall 0.604167\n",
      "2017-12-07T23:59:02.528878: step 115, loss 3.40832, acc 0.375, prec 0.0152873, recall 0.604167\n",
      "2017-12-07T23:59:03.135572: step 116, loss 5.01046, acc 0.34375, prec 0.0153806, recall 0.602041\n",
      "2017-12-07T23:59:03.740227: step 117, loss 2.83725, acc 0.4375, prec 0.0152376, recall 0.602041\n",
      "2017-12-07T23:59:04.340843: step 118, loss 2.35402, acc 0.40625, prec 0.0150895, recall 0.602041\n",
      "2017-12-07T23:59:04.946986: step 119, loss 3.49133, acc 0.34375, prec 0.0149292, recall 0.602041\n",
      "2017-12-07T23:59:05.562679: step 120, loss 3.27927, acc 0.265625, prec 0.015, recall 0.606061\n",
      "2017-12-07T23:59:06.167997: step 121, loss 3.14187, acc 0.328125, prec 0.0150841, recall 0.61\n",
      "2017-12-07T23:59:06.772605: step 122, loss 13.1772, acc 0.390625, prec 0.0154261, recall 0.61165\n",
      "2017-12-07T23:59:07.376228: step 123, loss 2.28189, acc 0.46875, prec 0.0155378, recall 0.615385\n",
      "2017-12-07T23:59:07.973960: step 124, loss 1.61374, acc 0.59375, prec 0.0156778, recall 0.619048\n",
      "2017-12-07T23:59:08.575652: step 125, loss 1.6448, acc 0.578125, prec 0.0155763, recall 0.619048\n",
      "2017-12-07T23:59:09.175664: step 126, loss 4.9812, acc 0.5625, prec 0.0159448, recall 0.62037\n",
      "2017-12-07T23:59:09.774674: step 127, loss 2.07829, acc 0.453125, prec 0.0158131, recall 0.62037\n",
      "2017-12-07T23:59:10.416830: step 128, loss 1.27849, acc 0.59375, prec 0.0157166, recall 0.62037\n",
      "2017-12-07T23:59:11.020622: step 129, loss 1.24626, acc 0.609375, prec 0.015625, recall 0.62037\n",
      "2017-12-07T23:59:11.633138: step 130, loss 1.60619, acc 0.640625, prec 0.0159981, recall 0.627273\n",
      "2017-12-07T23:59:12.238202: step 131, loss 7.45717, acc 0.59375, prec 0.0159059, recall 0.621622\n",
      "2017-12-07T23:59:12.850841: step 132, loss 5.87695, acc 0.671875, prec 0.0160624, recall 0.614035\n",
      "2017-12-07T23:59:13.454754: step 133, loss 1.27836, acc 0.6875, prec 0.015989, recall 0.614035\n",
      "2017-12-07T23:59:14.057425: step 134, loss 1.01493, acc 0.671875, prec 0.0159127, recall 0.614035\n",
      "2017-12-07T23:59:14.660739: step 135, loss 1.61145, acc 0.609375, prec 0.0158228, recall 0.614035\n",
      "2017-12-07T23:59:15.263153: step 136, loss 1.87153, acc 0.53125, prec 0.0157162, recall 0.614035\n",
      "2017-12-07T23:59:15.871190: step 137, loss 1.42281, acc 0.5625, prec 0.0158376, recall 0.617391\n",
      "2017-12-07T23:59:16.482790: step 138, loss 1.55604, acc 0.59375, prec 0.0161827, recall 0.623932\n",
      "2017-12-07T23:59:17.080449: step 139, loss 2.35711, acc 0.640625, prec 0.0163175, recall 0.627119\n",
      "2017-12-07T23:59:17.688876: step 140, loss 6.42368, acc 0.578125, prec 0.0164402, recall 0.625\n",
      "2017-12-07T23:59:18.300614: step 141, loss 1.3988, acc 0.5625, prec 0.0163399, recall 0.625\n",
      "2017-12-07T23:59:18.898973: step 142, loss 3.11827, acc 0.515625, prec 0.0162338, recall 0.619835\n",
      "2017-12-07T23:59:19.506198: step 143, loss 1.18774, acc 0.703125, prec 0.0165913, recall 0.626016\n",
      "2017-12-07T23:59:20.122148: step 144, loss 1.90918, acc 0.578125, prec 0.0167059, recall 0.629032\n",
      "2017-12-07T23:59:20.728741: step 145, loss 1.74852, acc 0.546875, prec 0.0166028, recall 0.629032\n",
      "2017-12-07T23:59:21.333289: step 146, loss 6.98924, acc 0.625, prec 0.0167302, recall 0.626984\n",
      "2017-12-07T23:59:21.940467: step 147, loss 3.79031, acc 0.578125, prec 0.0168457, recall 0.625\n",
      "2017-12-07T23:59:22.548624: step 148, loss 1.45211, acc 0.53125, prec 0.0167399, recall 0.625\n",
      "2017-12-07T23:59:23.148194: step 149, loss 5.99245, acc 0.578125, prec 0.0166493, recall 0.620155\n",
      "2017-12-07T23:59:23.756185: step 150, loss 2.24892, acc 0.53125, prec 0.0167494, recall 0.623077\n",
      "2017-12-07T23:59:24.348057: step 151, loss 3.86298, acc 0.484375, prec 0.0166393, recall 0.618321\n",
      "2017-12-07T23:59:24.955518: step 152, loss 2.65933, acc 0.390625, prec 0.016507, recall 0.618321\n",
      "2017-12-07T23:59:25.554791: step 153, loss 2.50695, acc 0.453125, prec 0.0163901, recall 0.618321\n",
      "2017-12-07T23:59:26.159869: step 154, loss 3.28403, acc 0.390625, prec 0.0166566, recall 0.62406\n",
      "2017-12-07T23:59:26.760750: step 155, loss 3.0568, acc 0.390625, prec 0.0167231, recall 0.626866\n",
      "2017-12-07T23:59:27.361135: step 156, loss 3.56711, acc 0.34375, prec 0.0167785, recall 0.62963\n",
      "2017-12-07T23:59:27.973453: step 157, loss 2.78108, acc 0.40625, prec 0.0168462, recall 0.632353\n",
      "2017-12-07T23:59:28.579825: step 158, loss 2.67401, acc 0.4375, prec 0.0169195, recall 0.635036\n",
      "2017-12-07T23:59:29.169159: step 159, loss 2.92239, acc 0.421875, prec 0.0169884, recall 0.637681\n",
      "2017-12-07T23:59:29.753618: step 160, loss 2.21326, acc 0.5, prec 0.0172612, recall 0.642857\n",
      "2017-12-07T23:59:30.355920: step 161, loss 1.3969, acc 0.65625, prec 0.0173764, recall 0.64539\n",
      "2017-12-07T23:59:30.945866: step 162, loss 2.22808, acc 0.5625, prec 0.0174706, recall 0.647887\n",
      "2017-12-07T23:59:31.529860: step 163, loss 1.69993, acc 0.578125, prec 0.0173814, recall 0.647887\n",
      "2017-12-07T23:59:32.122306: step 164, loss 11.8422, acc 0.5625, prec 0.0174812, recall 0.641379\n",
      "2017-12-07T23:59:32.707060: step 165, loss 1.21087, acc 0.671875, prec 0.0174125, recall 0.641379\n",
      "2017-12-07T23:59:33.302798: step 166, loss 1.32406, acc 0.609375, prec 0.0173313, recall 0.641379\n",
      "2017-12-07T23:59:33.913032: step 167, loss 1.67596, acc 0.59375, prec 0.01743, recall 0.643836\n",
      "2017-12-07T23:59:34.514005: step 168, loss 7.58803, acc 0.5625, prec 0.0173464, recall 0.635135\n",
      "2017-12-07T23:59:35.118110: step 169, loss 1.62643, acc 0.65625, prec 0.0174568, recall 0.637584\n",
      "2017-12-07T23:59:35.724670: step 170, loss 1.69031, acc 0.609375, prec 0.0177363, recall 0.642384\n",
      "2017-12-07T23:59:36.326247: step 171, loss 7.78911, acc 0.609375, prec 0.0178376, recall 0.640523\n",
      "2017-12-07T23:59:36.925704: step 172, loss 3.97222, acc 0.578125, prec 0.0182872, recall 0.643312\n",
      "2017-12-07T23:59:37.531375: step 173, loss 2.32387, acc 0.53125, prec 0.0183651, recall 0.64557\n",
      "2017-12-07T23:59:38.135569: step 174, loss 2.63976, acc 0.515625, prec 0.018439, recall 0.647799\n",
      "2017-12-07T23:59:38.755481: step 175, loss 2.89221, acc 0.421875, prec 0.0184922, recall 0.65\n",
      "2017-12-07T23:59:39.364055: step 176, loss 3.10293, acc 0.390625, prec 0.0185381, recall 0.652174\n",
      "2017-12-07T23:59:39.966272: step 177, loss 5.61848, acc 0.390625, prec 0.0184146, recall 0.648148\n",
      "2017-12-07T23:59:40.600442: step 178, loss 2.35002, acc 0.4375, prec 0.0184701, recall 0.650307\n",
      "2017-12-07T23:59:41.207549: step 179, loss 3.31666, acc 0.34375, prec 0.0183359, recall 0.650307\n",
      "2017-12-07T23:59:41.818253: step 180, loss 8.02871, acc 0.3125, prec 0.0183691, recall 0.648485\n",
      "2017-12-07T23:59:42.427723: step 181, loss 3.36865, acc 0.359375, prec 0.018408, recall 0.650602\n",
      "2017-12-07T23:59:43.007349: step 182, loss 3.69385, acc 0.359375, prec 0.0182803, recall 0.650602\n",
      "2017-12-07T23:59:43.588049: step 183, loss 3.39637, acc 0.3125, prec 0.0181452, recall 0.650602\n",
      "2017-12-07T23:59:44.172703: step 184, loss 3.21913, acc 0.3125, prec 0.018012, recall 0.650602\n",
      "2017-12-07T23:59:44.746481: step 185, loss 4.14687, acc 0.296875, prec 0.0180404, recall 0.652695\n",
      "2017-12-07T23:59:45.319413: step 186, loss 3.08407, acc 0.3125, prec 0.01791, recall 0.652695\n",
      "2017-12-07T23:59:45.899710: step 187, loss 4.43057, acc 0.3125, prec 0.0181018, recall 0.656805\n",
      "2017-12-07T23:59:46.490251: step 188, loss 1.80114, acc 0.515625, prec 0.01817, recall 0.658824\n",
      "2017-12-07T23:59:47.080201: step 189, loss 3.77937, acc 0.515625, prec 0.018082, recall 0.654971\n",
      "2017-12-07T23:59:47.670645: step 190, loss 3.55675, acc 0.578125, prec 0.0183221, recall 0.655172\n",
      "2017-12-07T23:59:48.254119: step 191, loss 3.766, acc 0.578125, prec 0.0182458, recall 0.651429\n",
      "2017-12-07T23:59:48.836343: step 192, loss 2.05434, acc 0.59375, prec 0.0183267, recall 0.653409\n",
      "2017-12-07T23:59:49.423807: step 193, loss 2.08429, acc 0.515625, prec 0.0182366, recall 0.653409\n",
      "2017-12-07T23:59:50.012448: step 194, loss 1.7011, acc 0.5625, prec 0.018156, recall 0.653409\n",
      "2017-12-07T23:59:50.615087: step 195, loss 1.63746, acc 0.53125, prec 0.0180704, recall 0.653409\n",
      "2017-12-07T23:59:51.243619: step 196, loss 1.52343, acc 0.578125, prec 0.0179941, recall 0.653409\n",
      "2017-12-07T23:59:51.860874: step 197, loss 1.85697, acc 0.515625, prec 0.0185185, recall 0.661111\n",
      "2017-12-07T23:59:52.492026: step 198, loss 2.4696, acc 0.640625, prec 0.0186047, recall 0.662983\n",
      "2017-12-07T23:59:53.104583: step 199, loss 1.50503, acc 0.640625, prec 0.0186901, recall 0.664835\n",
      "2017-12-07T23:59:53.713339: step 200, loss 8.61124, acc 0.640625, prec 0.0189289, recall 0.664865\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold1/1512709069/checkpoints/model-200\n",
      "\n",
      "2017-12-07T23:59:55.199001: step 201, loss 0.68438, acc 0.765625, prec 0.0188853, recall 0.664865\n",
      "2017-12-07T23:59:55.779594: step 202, loss 11.7133, acc 0.671875, prec 0.0188275, recall 0.66129\n",
      "2017-12-07T23:59:56.353616: step 203, loss 1.51402, acc 0.625, prec 0.0190578, recall 0.664894\n",
      "2017-12-07T23:59:56.911597: step 204, loss 1.84771, acc 0.640625, prec 0.0192892, recall 0.668421\n",
      "2017-12-07T23:59:57.464353: step 205, loss 1.64262, acc 0.625, prec 0.0193675, recall 0.670157\n",
      "2017-12-07T23:59:58.027532: step 206, loss 0.956719, acc 0.671875, prec 0.0193062, recall 0.670157\n",
      "2017-12-07T23:59:58.600619: step 207, loss 6.11539, acc 0.65625, prec 0.0193927, recall 0.668394\n",
      "2017-12-07T23:59:59.186064: step 208, loss 1.32792, acc 0.65625, prec 0.0193287, recall 0.668394\n",
      "2017-12-07T23:59:59.776775: step 209, loss 13.0819, acc 0.625, prec 0.0192624, recall 0.664948\n",
      "2017-12-08T00:00:00.400789: step 210, loss 2.03421, acc 0.578125, prec 0.0194767, recall 0.668367\n",
      "2017-12-08T00:00:01.006856: step 211, loss 1.44851, acc 0.609375, prec 0.0194045, recall 0.668367\n",
      "2017-12-08T00:00:01.587721: step 212, loss 4.30645, acc 0.53125, prec 0.0196107, recall 0.668342\n",
      "2017-12-08T00:00:02.185443: step 213, loss 2.23816, acc 0.546875, prec 0.0198151, recall 0.671642\n",
      "2017-12-08T00:00:02.793287: step 214, loss 2.71649, acc 0.5625, prec 0.019734, recall 0.671642\n",
      "2017-12-08T00:00:03.421811: step 215, loss 4.16986, acc 0.453125, prec 0.0197789, recall 0.669951\n",
      "2017-12-08T00:00:04.051007: step 216, loss 2.49102, acc 0.5, prec 0.0196873, recall 0.669951\n",
      "2017-12-08T00:00:04.654295: step 217, loss 2.89497, acc 0.40625, prec 0.0197207, recall 0.671569\n",
      "2017-12-08T00:00:05.260481: step 218, loss 4.0018, acc 0.53125, prec 0.0199197, recall 0.671498\n",
      "2017-12-08T00:00:05.867797: step 219, loss 3.02243, acc 0.46875, prec 0.0198232, recall 0.671498\n",
      "2017-12-08T00:00:06.471866: step 220, loss 6.44721, acc 0.421875, prec 0.019861, recall 0.669856\n",
      "2017-12-08T00:00:07.076756: step 221, loss 3.38949, acc 0.46875, prec 0.0199068, recall 0.668246\n",
      "2017-12-08T00:00:07.688459: step 222, loss 3.12959, acc 0.40625, prec 0.0199382, recall 0.669811\n",
      "2017-12-08T00:00:08.292903: step 223, loss 1.9173, acc 0.5625, prec 0.0198601, recall 0.669811\n",
      "2017-12-08T00:00:08.878631: step 224, loss 2.54958, acc 0.46875, prec 0.0199026, recall 0.671362\n",
      "2017-12-08T00:00:09.459078: step 225, loss 2.98318, acc 0.4375, prec 0.0199391, recall 0.672897\n",
      "2017-12-08T00:00:10.039411: step 226, loss 3.67244, acc 0.3125, prec 0.0199532, recall 0.674419\n",
      "2017-12-08T00:00:10.625703: step 227, loss 2.43171, acc 0.453125, prec 0.0198576, recall 0.674419\n",
      "2017-12-08T00:00:11.239796: step 228, loss 3.07752, acc 0.34375, prec 0.0198775, recall 0.675926\n",
      "2017-12-08T00:00:11.831184: step 229, loss 2.09852, acc 0.5, prec 0.0197912, recall 0.675926\n",
      "2017-12-08T00:00:12.415945: step 230, loss 1.763, acc 0.6875, prec 0.0201351, recall 0.680365\n",
      "2017-12-08T00:00:12.997022: step 231, loss 1.30467, acc 0.609375, prec 0.0200673, recall 0.680365\n",
      "2017-12-08T00:00:13.576579: step 232, loss 1.27552, acc 0.6875, prec 0.0200134, recall 0.680365\n",
      "2017-12-08T00:00:14.153512: step 233, loss 1.70162, acc 0.6875, prec 0.0200911, recall 0.681818\n",
      "2017-12-08T00:00:14.740460: step 234, loss 1.90546, acc 0.65625, prec 0.0201629, recall 0.683258\n",
      "2017-12-08T00:00:15.315126: step 235, loss 1.08033, acc 0.734375, prec 0.0202478, recall 0.684685\n",
      "2017-12-08T00:00:15.909642: step 236, loss 8.15152, acc 0.71875, prec 0.0203322, recall 0.683036\n",
      "2017-12-08T00:00:16.546225: step 237, loss 2.24353, acc 0.765625, prec 0.0205516, recall 0.685841\n",
      "2017-12-08T00:00:17.154343: step 238, loss 0.957294, acc 0.75, prec 0.0205081, recall 0.685841\n",
      "2017-12-08T00:00:17.759130: step 239, loss 2.23303, acc 0.734375, prec 0.0204647, recall 0.682819\n",
      "2017-12-08T00:00:18.372995: step 240, loss 3.50641, acc 0.671875, prec 0.0206688, recall 0.682609\n",
      "2017-12-08T00:00:18.985704: step 241, loss 1.01492, acc 0.75, prec 0.0206253, recall 0.682609\n",
      "2017-12-08T00:00:19.592522: step 242, loss 18.8503, acc 0.734375, prec 0.0207104, recall 0.681035\n",
      "2017-12-08T00:00:20.199961: step 243, loss 17.0207, acc 0.609375, prec 0.0206455, recall 0.678112\n",
      "2017-12-08T00:00:20.832453: step 244, loss 0.9872, acc 0.71875, prec 0.0205971, recall 0.678112\n",
      "2017-12-08T00:00:21.448106: step 245, loss 4.3751, acc 0.625, prec 0.0206628, recall 0.676596\n",
      "2017-12-08T00:00:22.056736: step 246, loss 1.6511, acc 0.609375, prec 0.0205959, recall 0.676596\n",
      "2017-12-08T00:00:22.636491: step 247, loss 2.25083, acc 0.515625, prec 0.0205135, recall 0.676596\n",
      "2017-12-08T00:00:23.216324: step 248, loss 5.79136, acc 0.5625, prec 0.0204423, recall 0.673729\n",
      "2017-12-08T00:00:23.793229: step 249, loss 2.41608, acc 0.453125, prec 0.0204761, recall 0.675106\n",
      "2017-12-08T00:00:24.378031: step 250, loss 2.60056, acc 0.453125, prec 0.0208837, recall 0.680498\n",
      "2017-12-08T00:00:24.968164: step 251, loss 2.95844, acc 0.4375, prec 0.0207884, recall 0.680498\n",
      "2017-12-08T00:00:25.551347: step 252, loss 2.52174, acc 0.515625, prec 0.0209543, recall 0.683128\n",
      "2017-12-08T00:00:26.135556: step 253, loss 2.66002, acc 0.4375, prec 0.0208595, recall 0.683128\n",
      "2017-12-08T00:00:26.731985: step 254, loss 3.45494, acc 0.4375, prec 0.0208881, recall 0.684426\n",
      "2017-12-08T00:00:27.321326: step 255, loss 8.73234, acc 0.390625, prec 0.0207892, recall 0.681633\n",
      "2017-12-08T00:00:27.906796: step 256, loss 2.17522, acc 0.53125, prec 0.0207119, recall 0.681633\n",
      "2017-12-08T00:00:28.483797: step 257, loss 3.2703, acc 0.34375, prec 0.0206046, recall 0.681633\n",
      "2017-12-08T00:00:29.082781: step 258, loss 3.12401, acc 0.390625, prec 0.0205059, recall 0.681633\n",
      "2017-12-08T00:00:29.689080: step 259, loss 2.72241, acc 0.4375, prec 0.0205354, recall 0.682927\n",
      "2017-12-08T00:00:30.317427: step 260, loss 2.56712, acc 0.515625, prec 0.0205771, recall 0.684211\n",
      "2017-12-08T00:00:30.939504: step 261, loss 1.80942, acc 0.53125, prec 0.0205022, recall 0.684211\n",
      "2017-12-08T00:00:31.610815: step 262, loss 1.48633, acc 0.578125, prec 0.0205537, recall 0.685484\n",
      "2017-12-08T00:00:32.237265: step 263, loss 1.49462, acc 0.625, prec 0.0204943, recall 0.685484\n",
      "2017-12-08T00:00:32.842353: step 264, loss 0.754002, acc 0.8125, prec 0.0204647, recall 0.685484\n",
      "2017-12-08T00:00:33.446693: step 265, loss 0.911715, acc 0.71875, prec 0.0205381, recall 0.686747\n",
      "2017-12-08T00:00:34.066655: step 266, loss 7.41343, acc 0.6875, prec 0.0204913, recall 0.684\n",
      "2017-12-08T00:00:34.674565: step 267, loss 1.25072, acc 0.71875, prec 0.0205643, recall 0.685259\n",
      "2017-12-08T00:00:35.291804: step 268, loss 2.33543, acc 0.765625, prec 0.0206469, recall 0.683794\n",
      "2017-12-08T00:00:35.897946: step 269, loss 1.85606, acc 0.78125, prec 0.0206149, recall 0.681102\n",
      "2017-12-08T00:00:36.509772: step 270, loss 2.68333, acc 0.78125, prec 0.0206995, recall 0.679688\n",
      "2017-12-08T00:00:37.126607: step 271, loss 3.43923, acc 0.75, prec 0.0207789, recall 0.678295\n",
      "2017-12-08T00:00:37.749871: step 272, loss 1.25047, acc 0.6875, prec 0.0207297, recall 0.678295\n",
      "2017-12-08T00:00:38.355229: step 273, loss 1.27977, acc 0.6875, prec 0.0207964, recall 0.679537\n",
      "2017-12-08T00:00:38.956154: step 274, loss 0.806914, acc 0.8125, prec 0.0208825, recall 0.680769\n",
      "2017-12-08T00:00:39.568805: step 275, loss 10.6264, acc 0.640625, prec 0.0209436, recall 0.679389\n",
      "2017-12-08T00:00:40.186076: step 276, loss 1.72032, acc 0.59375, prec 0.0209946, recall 0.680608\n",
      "2017-12-08T00:00:40.792607: step 277, loss 1.85139, acc 0.609375, prec 0.0209332, recall 0.680608\n",
      "2017-12-08T00:00:41.452744: step 278, loss 7.61149, acc 0.546875, prec 0.0208673, recall 0.675472\n",
      "2017-12-08T00:00:42.065666: step 279, loss 2.09519, acc 0.515625, prec 0.0207922, recall 0.675472\n",
      "2017-12-08T00:00:42.652413: step 280, loss 2.5442, acc 0.4375, prec 0.0208189, recall 0.676692\n",
      "2017-12-08T00:00:43.236943: step 281, loss 2.33173, acc 0.5, prec 0.0208549, recall 0.677903\n",
      "2017-12-08T00:00:43.822851: step 282, loss 14.1727, acc 0.546875, prec 0.0209003, recall 0.67658\n",
      "2017-12-08T00:00:44.407916: step 283, loss 2.92735, acc 0.421875, prec 0.0210358, recall 0.678967\n",
      "2017-12-08T00:00:44.989689: step 284, loss 2.82911, acc 0.421875, prec 0.0210586, recall 0.680147\n",
      "2017-12-08T00:00:45.582266: step 285, loss 3.08699, acc 0.421875, prec 0.0209703, recall 0.680147\n",
      "2017-12-08T00:00:46.169317: step 286, loss 3.28374, acc 0.328125, prec 0.020979, recall 0.681319\n",
      "2017-12-08T00:00:46.752320: step 287, loss 3.44446, acc 0.421875, prec 0.0211117, recall 0.683636\n",
      "2017-12-08T00:00:47.339074: step 288, loss 3.67321, acc 0.375, prec 0.0211268, recall 0.684783\n",
      "2017-12-08T00:00:47.926511: step 289, loss 3.23641, acc 0.375, prec 0.0211416, recall 0.685921\n",
      "2017-12-08T00:00:48.519618: step 290, loss 3.10318, acc 0.34375, prec 0.0210433, recall 0.685921\n",
      "2017-12-08T00:00:49.110792: step 291, loss 2.16816, acc 0.546875, prec 0.0209759, recall 0.685921\n",
      "2017-12-08T00:00:49.711445: step 292, loss 2.5135, acc 0.484375, prec 0.0210075, recall 0.68705\n",
      "2017-12-08T00:00:50.317734: step 293, loss 6.4536, acc 0.515625, prec 0.0209384, recall 0.684588\n",
      "2017-12-08T00:00:50.923061: step 294, loss 1.88298, acc 0.53125, prec 0.0209767, recall 0.685714\n",
      "2017-12-08T00:00:51.582446: step 295, loss 2.69559, acc 0.546875, prec 0.0210194, recall 0.684397\n",
      "2017-12-08T00:00:52.192863: step 296, loss 6.65504, acc 0.609375, prec 0.0209646, recall 0.681979\n",
      "2017-12-08T00:00:52.808996: step 297, loss 1.80857, acc 0.515625, prec 0.0210002, recall 0.683099\n",
      "2017-12-08T00:00:53.415835: step 298, loss 2.39009, acc 0.46875, prec 0.0211344, recall 0.685315\n",
      "2017-12-08T00:00:54.020049: step 299, loss 2.6696, acc 0.578125, prec 0.0210753, recall 0.682927\n",
      "2017-12-08T00:00:54.628663: step 300, loss 12.5683, acc 0.625, prec 0.0210233, recall 0.680556\n",
      "\n",
      "Evaluation:\n",
      "2017-12-08T00:01:42.316106: step 300, loss 1.25972, acc 0.672516, prec 0.0258534, recall 0.76082\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold1/1512709069/checkpoints/model-300\n",
      "\n",
      "2017-12-08T00:01:44.114328: step 301, loss 1.91848, acc 0.53125, prec 0.0257935, recall 0.76082\n",
      "2017-12-08T00:01:44.724311: step 302, loss 1.46241, acc 0.578125, prec 0.0257398, recall 0.76082\n",
      "2017-12-08T00:01:45.333773: step 303, loss 1.75683, acc 0.59375, prec 0.0256884, recall 0.76082\n",
      "2017-12-08T00:01:45.940973: step 304, loss 1.88144, acc 0.625, prec 0.0257158, recall 0.761364\n",
      "2017-12-08T00:01:46.597870: step 305, loss 1.48715, acc 0.65625, prec 0.0256725, recall 0.761364\n",
      "2017-12-08T00:01:47.199925: step 306, loss 1.60383, acc 0.59375, prec 0.0256214, recall 0.761364\n",
      "2017-12-08T00:01:47.810607: step 307, loss 1.92736, acc 0.5, prec 0.0255589, recall 0.761364\n",
      "2017-12-08T00:01:48.438679: step 308, loss 0.83782, acc 0.734375, prec 0.0255258, recall 0.761364\n",
      "2017-12-08T00:01:49.065410: step 309, loss 1.13332, acc 0.796875, prec 0.0255747, recall 0.761905\n",
      "2017-12-08T00:01:49.705140: step 310, loss 0.891191, acc 0.734375, prec 0.0256157, recall 0.762443\n",
      "2017-12-08T00:01:50.332512: step 311, loss 0.770928, acc 0.78125, prec 0.0255885, recall 0.762443\n",
      "2017-12-08T00:01:50.957937: step 312, loss 1.00742, acc 0.78125, prec 0.0257091, recall 0.763514\n",
      "2017-12-08T00:01:51.602970: step 313, loss 1.37918, acc 0.796875, prec 0.0257576, recall 0.764045\n",
      "2017-12-08T00:01:52.279578: step 314, loss 4.76185, acc 0.78125, prec 0.0257322, recall 0.762332\n",
      "2017-12-08T00:01:52.916970: step 315, loss 4.4549, acc 0.734375, prec 0.0257011, recall 0.760626\n",
      "2017-12-08T00:01:53.547541: step 316, loss 0.721242, acc 0.796875, prec 0.0256759, recall 0.760626\n",
      "2017-12-08T00:01:54.195414: step 317, loss 1.06721, acc 0.734375, prec 0.0257164, recall 0.761161\n",
      "2017-12-08T00:01:54.829328: step 318, loss 1.11627, acc 0.8125, prec 0.0257666, recall 0.761693\n",
      "2017-12-08T00:01:55.471441: step 319, loss 3.43421, acc 0.78125, prec 0.0257414, recall 0.76\n",
      "2017-12-08T00:01:56.123019: step 320, loss 0.611739, acc 0.734375, prec 0.0257085, recall 0.76\n",
      "2017-12-08T00:01:56.783910: step 321, loss 1.21178, acc 0.671875, prec 0.0257411, recall 0.760532\n",
      "2017-12-08T00:01:57.431047: step 322, loss 6.85803, acc 0.734375, prec 0.0257102, recall 0.75885\n",
      "2017-12-08T00:01:58.095137: step 323, loss 1.72319, acc 0.671875, prec 0.0257427, recall 0.759382\n",
      "2017-12-08T00:01:58.728916: step 324, loss 2.94503, acc 0.703125, prec 0.0257081, recall 0.757709\n",
      "2017-12-08T00:01:59.378048: step 325, loss 1.89778, acc 0.546875, prec 0.0257252, recall 0.758242\n",
      "2017-12-08T00:02:00.050958: step 326, loss 1.84469, acc 0.53125, prec 0.0257402, recall 0.758772\n",
      "2017-12-08T00:02:00.684806: step 327, loss 3.87846, acc 0.625, prec 0.0257686, recall 0.757642\n",
      "2017-12-08T00:02:01.323434: step 328, loss 1.59374, acc 0.578125, prec 0.0258614, recall 0.758696\n",
      "2017-12-08T00:02:01.944094: step 329, loss 1.79208, acc 0.65625, prec 0.0258193, recall 0.758696\n",
      "2017-12-08T00:02:02.601068: step 330, loss 1.85258, acc 0.546875, prec 0.0257641, recall 0.758696\n",
      "2017-12-08T00:02:03.213627: step 331, loss 1.72754, acc 0.578125, prec 0.0257128, recall 0.758696\n",
      "2017-12-08T00:02:03.838788: step 332, loss 2.20087, acc 0.578125, prec 0.0259483, recall 0.760776\n",
      "2017-12-08T00:02:04.450971: step 333, loss 2.23247, acc 0.578125, prec 0.0259683, recall 0.76129\n",
      "2017-12-08T00:02:05.066194: step 334, loss 2.04099, acc 0.546875, prec 0.0259132, recall 0.76129\n",
      "2017-12-08T00:02:05.667359: step 335, loss 1.18274, acc 0.640625, prec 0.0258696, recall 0.76129\n",
      "2017-12-08T00:02:06.271457: step 336, loss 2.33187, acc 0.515625, prec 0.0258112, recall 0.76129\n",
      "2017-12-08T00:02:06.879898: step 337, loss 1.37203, acc 0.640625, prec 0.0257679, recall 0.76129\n",
      "2017-12-08T00:02:07.483496: step 338, loss 2.17939, acc 0.71875, prec 0.0258069, recall 0.760171\n",
      "2017-12-08T00:02:08.086772: step 339, loss 1.04465, acc 0.671875, prec 0.0257676, recall 0.760171\n",
      "2017-12-08T00:02:08.743841: step 340, loss 4.16824, acc 0.703125, prec 0.025734, recall 0.758547\n",
      "2017-12-08T00:02:09.348632: step 341, loss 1.41801, acc 0.640625, prec 0.0257616, recall 0.759062\n",
      "2017-12-08T00:02:10.033972: step 342, loss 1.79516, acc 0.53125, prec 0.0257058, recall 0.759062\n",
      "2017-12-08T00:02:10.693563: step 343, loss 6.27383, acc 0.71875, prec 0.0256743, recall 0.757447\n",
      "2017-12-08T00:02:11.338546: step 344, loss 4.18193, acc 0.6875, prec 0.0256392, recall 0.755839\n",
      "2017-12-08T00:02:12.001689: step 345, loss 1.24697, acc 0.625, prec 0.025735, recall 0.756871\n",
      "2017-12-08T00:02:12.707165: step 346, loss 1.17502, acc 0.75, prec 0.0257754, recall 0.757384\n",
      "2017-12-08T00:02:13.395628: step 347, loss 1.17608, acc 0.640625, prec 0.0257329, recall 0.757384\n",
      "2017-12-08T00:02:14.061929: step 348, loss 4.91287, acc 0.578125, prec 0.0256851, recall 0.755789\n",
      "2017-12-08T00:02:14.734192: step 349, loss 3.46214, acc 0.578125, prec 0.0257069, recall 0.754717\n",
      "2017-12-08T00:02:15.437532: step 350, loss 2.1419, acc 0.59375, prec 0.0257982, recall 0.755741\n",
      "2017-12-08T00:02:16.117481: step 351, loss 1.99778, acc 0.46875, prec 0.0257358, recall 0.755741\n",
      "2017-12-08T00:02:16.782963: step 352, loss 1.67123, acc 0.578125, prec 0.0256865, recall 0.755741\n",
      "2017-12-08T00:02:17.451604: step 353, loss 1.77294, acc 0.625, prec 0.0257809, recall 0.756757\n",
      "2017-12-08T00:02:18.123299: step 354, loss 2.00919, acc 0.59375, prec 0.0257335, recall 0.756757\n",
      "2017-12-08T00:02:18.770296: step 355, loss 1.75424, acc 0.5625, prec 0.0256826, recall 0.756757\n",
      "2017-12-08T00:02:19.429920: step 356, loss 2.01015, acc 0.546875, prec 0.0256988, recall 0.757261\n",
      "2017-12-08T00:02:20.076173: step 357, loss 4.86157, acc 0.6875, prec 0.0258015, recall 0.756701\n",
      "2017-12-08T00:02:20.732310: step 358, loss 1.47694, acc 0.640625, prec 0.0257598, recall 0.756701\n",
      "2017-12-08T00:02:21.369910: step 359, loss 1.46867, acc 0.6875, prec 0.0257237, recall 0.756701\n",
      "2017-12-08T00:02:21.996285: step 360, loss 2.93559, acc 0.5625, prec 0.0257415, recall 0.757202\n",
      "2017-12-08T00:02:22.667210: step 361, loss 1.56171, acc 0.65625, prec 0.0257019, recall 0.757202\n",
      "2017-12-08T00:02:23.304297: step 362, loss 1.51127, acc 0.59375, prec 0.0257232, recall 0.7577\n",
      "2017-12-08T00:02:23.913591: step 363, loss 8.84437, acc 0.65625, prec 0.0256856, recall 0.756148\n",
      "2017-12-08T00:02:24.525698: step 364, loss 2.66965, acc 0.671875, prec 0.0257177, recall 0.755102\n",
      "2017-12-08T00:02:25.128219: step 365, loss 2.03623, acc 0.625, prec 0.02581, recall 0.756098\n",
      "2017-12-08T00:02:25.737092: step 366, loss 1.30072, acc 0.609375, prec 0.0257653, recall 0.756098\n",
      "2017-12-08T00:02:26.343837: step 367, loss 2.90639, acc 0.65625, prec 0.0257279, recall 0.754564\n",
      "2017-12-08T00:02:26.952939: step 368, loss 1.34363, acc 0.609375, prec 0.025818, recall 0.755556\n",
      "2017-12-08T00:02:27.557694: step 369, loss 3.09657, acc 0.6875, prec 0.0257842, recall 0.754032\n",
      "2017-12-08T00:02:28.167779: step 370, loss 0.892243, acc 0.65625, prec 0.0257452, recall 0.754032\n",
      "2017-12-08T00:02:28.787953: step 371, loss 2.51178, acc 0.53125, prec 0.025759, recall 0.754527\n",
      "2017-12-08T00:02:29.396246: step 372, loss 1.74348, acc 0.5, prec 0.0257693, recall 0.75502\n",
      "2017-12-08T00:02:30.038611: step 373, loss 13.2811, acc 0.6875, prec 0.0258025, recall 0.754\n",
      "2017-12-08T00:02:30.641777: step 374, loss 1.68543, acc 0.5625, prec 0.0257531, recall 0.754\n",
      "2017-12-08T00:02:31.247077: step 375, loss 1.58007, acc 0.65625, prec 0.0257809, recall 0.754491\n",
      "2017-12-08T00:02:31.851480: step 376, loss 1.73174, acc 0.703125, prec 0.0258139, recall 0.75498\n",
      "2017-12-08T00:02:32.463959: step 377, loss 1.98405, acc 0.515625, prec 0.0258257, recall 0.755467\n",
      "2017-12-08T00:02:33.075284: step 378, loss 2.16549, acc 0.546875, prec 0.0257749, recall 0.755467\n",
      "2017-12-08T00:02:33.697542: step 379, loss 2.0068, acc 0.59375, prec 0.0257955, recall 0.755952\n",
      "2017-12-08T00:02:34.301060: step 380, loss 1.31334, acc 0.6875, prec 0.0257606, recall 0.755952\n",
      "2017-12-08T00:02:34.906919: step 381, loss 1.98487, acc 0.671875, prec 0.0259214, recall 0.757396\n",
      "2017-12-08T00:02:35.515836: step 382, loss 0.829855, acc 0.734375, prec 0.0259574, recall 0.757874\n",
      "2017-12-08T00:02:36.115301: step 383, loss 1.83391, acc 0.578125, prec 0.0259102, recall 0.757874\n",
      "2017-12-08T00:02:36.715085: step 384, loss 0.93165, acc 0.640625, prec 0.0258702, recall 0.757874\n",
      "2017-12-08T00:02:37.315675: step 385, loss 1.05001, acc 0.75, prec 0.0258424, recall 0.757874\n",
      "2017-12-08T00:02:37.918159: step 386, loss 1.03513, acc 0.71875, prec 0.0258112, recall 0.757874\n",
      "2017-12-08T00:02:38.517899: step 387, loss 2.53765, acc 0.703125, prec 0.0257801, recall 0.756385\n",
      "2017-12-08T00:02:39.124058: step 388, loss 1.48074, acc 0.765625, prec 0.0258194, recall 0.756863\n",
      "2017-12-08T00:02:39.732818: step 389, loss 0.86732, acc 0.78125, prec 0.0257952, recall 0.756863\n",
      "2017-12-08T00:02:40.335484: step 390, loss 0.797563, acc 0.765625, prec 0.0258344, recall 0.757339\n",
      "2017-12-08T00:02:40.936877: step 391, loss 0.987629, acc 0.71875, prec 0.0258684, recall 0.757812\n",
      "2017-12-08T00:02:41.535476: step 392, loss 2.19392, acc 0.796875, prec 0.0258477, recall 0.756335\n",
      "2017-12-08T00:02:42.124392: step 393, loss 5.00836, acc 0.65625, prec 0.0258116, recall 0.754864\n",
      "2017-12-08T00:02:42.743886: step 394, loss 14.2763, acc 0.71875, prec 0.0257824, recall 0.753398\n",
      "2017-12-08T00:02:43.337196: step 395, loss 0.732315, acc 0.75, prec 0.0257551, recall 0.753398\n",
      "2017-12-08T00:02:43.929139: step 396, loss 1.0174, acc 0.703125, prec 0.0257226, recall 0.753398\n",
      "2017-12-08T00:02:44.513177: step 397, loss 3.16854, acc 0.78125, prec 0.025765, recall 0.752418\n",
      "2017-12-08T00:02:45.096360: step 398, loss 2.06555, acc 0.640625, prec 0.0257902, recall 0.752896\n",
      "2017-12-08T00:02:45.674216: step 399, loss 1.52752, acc 0.65625, prec 0.0257528, recall 0.752896\n",
      "2017-12-08T00:02:46.270403: step 400, loss 1.41159, acc 0.65625, prec 0.0257797, recall 0.753372\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold1/1512709069/checkpoints/model-400\n",
      "\n",
      "2017-12-08T00:02:47.737639: step 401, loss 2.01638, acc 0.578125, prec 0.0258621, recall 0.754319\n",
      "2017-12-08T00:02:48.342682: step 402, loss 1.54473, acc 0.53125, prec 0.0259391, recall 0.755258\n",
      "2017-12-08T00:02:48.928495: step 403, loss 1.70696, acc 0.703125, prec 0.0259706, recall 0.755725\n",
      "2017-12-08T00:02:49.510060: step 404, loss 2.22792, acc 0.625, prec 0.0260574, recall 0.756654\n",
      "2017-12-08T00:02:50.099882: step 405, loss 1.56115, acc 0.59375, prec 0.0263313, recall 0.758945\n",
      "2017-12-08T00:02:50.681325: step 406, loss 2.20033, acc 0.546875, prec 0.026345, recall 0.759399\n",
      "2017-12-08T00:02:51.266806: step 407, loss 13.1497, acc 0.53125, prec 0.0262952, recall 0.757974\n",
      "2017-12-08T00:02:51.862584: step 408, loss 2.3153, acc 0.46875, prec 0.0262372, recall 0.757974\n",
      "2017-12-08T00:02:52.447592: step 409, loss 14.5273, acc 0.515625, prec 0.0262493, recall 0.757009\n",
      "2017-12-08T00:02:53.083587: step 410, loss 2.40386, acc 0.578125, prec 0.0263294, recall 0.757914\n",
      "2017-12-08T00:02:53.693076: step 411, loss 1.93125, acc 0.546875, prec 0.0262801, recall 0.757914\n",
      "2017-12-08T00:02:54.284129: step 412, loss 2.00506, acc 0.46875, prec 0.0262225, recall 0.757914\n",
      "2017-12-08T00:02:54.878066: step 413, loss 6.33696, acc 0.515625, prec 0.0262346, recall 0.756957\n",
      "2017-12-08T00:02:55.493069: step 414, loss 2.41911, acc 0.4375, prec 0.0262364, recall 0.757407\n",
      "2017-12-08T00:02:56.101511: step 415, loss 1.43893, acc 0.640625, prec 0.0261978, recall 0.757407\n",
      "2017-12-08T00:02:56.701019: step 416, loss 2.35894, acc 0.515625, prec 0.0262081, recall 0.757856\n",
      "2017-12-08T00:02:57.300465: step 417, loss 2.2164, acc 0.53125, prec 0.026158, recall 0.757856\n",
      "2017-12-08T00:02:57.913182: step 418, loss 2.2673, acc 0.625, prec 0.026118, recall 0.757856\n",
      "2017-12-08T00:02:58.513647: step 419, loss 1.87212, acc 0.546875, prec 0.0261317, recall 0.758303\n",
      "2017-12-08T00:02:59.115231: step 420, loss 1.86243, acc 0.5625, prec 0.0260853, recall 0.758303\n",
      "2017-12-08T00:02:59.720734: step 421, loss 1.62893, acc 0.5625, prec 0.0261007, recall 0.758748\n",
      "2017-12-08T00:03:00.345926: step 422, loss 1.04954, acc 0.640625, prec 0.0260628, recall 0.758748\n",
      "2017-12-08T00:03:00.954598: step 423, loss 21.6972, acc 0.703125, prec 0.0260963, recall 0.75641\n",
      "2017-12-08T00:03:01.558389: step 424, loss 1.41067, acc 0.671875, prec 0.0261232, recall 0.756856\n",
      "2017-12-08T00:03:02.164226: step 425, loss 1.37213, acc 0.578125, prec 0.0261401, recall 0.757299\n",
      "2017-12-08T00:03:02.792427: step 426, loss 10.2747, acc 0.703125, prec 0.0261105, recall 0.75592\n",
      "2017-12-08T00:03:03.407405: step 427, loss 1.2378, acc 0.640625, prec 0.0260728, recall 0.75592\n",
      "2017-12-08T00:03:04.021440: step 428, loss 8.38507, acc 0.59375, prec 0.0262151, recall 0.755877\n",
      "2017-12-08T00:03:04.631514: step 429, loss 1.72584, acc 0.59375, prec 0.0262334, recall 0.756318\n",
      "2017-12-08T00:03:05.239672: step 430, loss 1.27805, acc 0.671875, prec 0.0262598, recall 0.756757\n",
      "2017-12-08T00:03:05.851764: step 431, loss 1.58393, acc 0.59375, prec 0.026278, recall 0.757194\n",
      "2017-12-08T00:03:06.455282: step 432, loss 1.64532, acc 0.578125, prec 0.0262338, recall 0.757194\n",
      "2017-12-08T00:03:07.058128: step 433, loss 1.69112, acc 0.609375, prec 0.026193, recall 0.757194\n",
      "2017-12-08T00:03:07.663010: step 434, loss 2.15609, acc 0.546875, prec 0.0262668, recall 0.758065\n",
      "2017-12-08T00:03:08.268898: step 435, loss 1.56453, acc 0.546875, prec 0.0262196, recall 0.758065\n",
      "2017-12-08T00:03:08.900850: step 436, loss 1.70794, acc 0.671875, prec 0.0261855, recall 0.758065\n",
      "2017-12-08T00:03:09.561547: step 437, loss 11.2419, acc 0.5625, prec 0.026202, recall 0.757143\n",
      "2017-12-08T00:03:10.197496: step 438, loss 0.981225, acc 0.65625, prec 0.0261664, recall 0.757143\n",
      "2017-12-08T00:03:10.833930: step 439, loss 1.57524, acc 0.640625, prec 0.0262493, recall 0.758007\n",
      "2017-12-08T00:03:11.485719: step 440, loss 1.84099, acc 0.515625, prec 0.0262591, recall 0.758437\n",
      "2017-12-08T00:03:12.109727: step 441, loss 1.75729, acc 0.484375, prec 0.026206, recall 0.758437\n",
      "2017-12-08T00:03:12.758974: step 442, loss 0.747182, acc 0.734375, prec 0.0261787, recall 0.758437\n",
      "2017-12-08T00:03:13.391557: step 443, loss 8.22843, acc 0.640625, prec 0.0261434, recall 0.757092\n",
      "2017-12-08T00:03:14.008075: step 444, loss 1.64257, acc 0.703125, prec 0.0261726, recall 0.757522\n",
      "2017-12-08T00:03:14.671953: step 445, loss 1.15019, acc 0.703125, prec 0.0261422, recall 0.757522\n",
      "2017-12-08T00:03:15.293645: step 446, loss 2.76883, acc 0.609375, prec 0.0262227, recall 0.757042\n",
      "2017-12-08T00:03:15.906058: step 447, loss 7.14279, acc 0.640625, prec 0.0261892, recall 0.754386\n",
      "2017-12-08T00:03:16.511257: step 448, loss 0.901651, acc 0.765625, prec 0.0261653, recall 0.754386\n",
      "2017-12-08T00:03:17.116102: step 449, loss 2.38768, acc 0.5625, prec 0.0261799, recall 0.754816\n",
      "2017-12-08T00:03:17.720317: step 450, loss 1.99615, acc 0.53125, prec 0.0261323, recall 0.754816\n",
      "2017-12-08T00:03:18.323762: step 451, loss 2.30465, acc 0.546875, prec 0.0260864, recall 0.754816\n",
      "2017-12-08T00:03:18.932209: step 452, loss 4.23928, acc 0.5625, prec 0.0260439, recall 0.753497\n",
      "2017-12-08T00:03:19.541944: step 453, loss 4.18721, acc 0.5625, prec 0.0260602, recall 0.752613\n",
      "2017-12-08T00:03:20.148720: step 454, loss 2.30548, acc 0.5625, prec 0.0260163, recall 0.752613\n",
      "2017-12-08T00:03:20.755014: step 455, loss 2.44635, acc 0.46875, prec 0.0260216, recall 0.753043\n",
      "2017-12-08T00:03:21.357404: step 456, loss 2.60662, acc 0.421875, prec 0.0260807, recall 0.753899\n",
      "2017-12-08T00:03:21.965429: step 457, loss 1.66026, acc 0.59375, prec 0.0260401, recall 0.753899\n",
      "2017-12-08T00:03:22.569284: step 458, loss 3.04194, acc 0.421875, prec 0.0259826, recall 0.753899\n",
      "2017-12-08T00:03:23.201103: step 459, loss 2.67472, acc 0.53125, prec 0.0261103, recall 0.755172\n",
      "2017-12-08T00:03:23.814069: step 460, loss 2.01909, acc 0.515625, prec 0.0260621, recall 0.755172\n",
      "2017-12-08T00:03:24.421454: step 461, loss 2.60718, acc 0.359375, prec 0.0259987, recall 0.755172\n",
      "2017-12-08T00:03:25.031570: step 462, loss 2.31964, acc 0.375, prec 0.0259371, recall 0.755172\n",
      "2017-12-08T00:03:25.635651: step 463, loss 1.45365, acc 0.6875, prec 0.0259064, recall 0.755172\n",
      "2017-12-08T00:03:26.241839: step 464, loss 3.07741, acc 0.625, prec 0.0259288, recall 0.754296\n",
      "2017-12-08T00:03:26.854879: step 465, loss 1.2734, acc 0.640625, prec 0.0258936, recall 0.754296\n",
      "2017-12-08T00:03:27.461169: step 466, loss 1.84422, acc 0.578125, prec 0.0259098, recall 0.754717\n",
      "2017-12-08T00:03:28.061895: step 467, loss 3.23746, acc 0.59375, prec 0.0260435, recall 0.754685\n",
      "2017-12-08T00:03:28.671531: step 468, loss 10.5306, acc 0.625, prec 0.0260655, recall 0.75382\n",
      "2017-12-08T00:03:29.291196: step 469, loss 1.46002, acc 0.640625, prec 0.0260875, recall 0.754237\n",
      "2017-12-08T00:03:29.917487: step 470, loss 1.66709, acc 0.609375, prec 0.0260493, recall 0.754237\n",
      "2017-12-08T00:03:30.532094: step 471, loss 1.17565, acc 0.640625, prec 0.0260712, recall 0.754653\n",
      "2017-12-08T00:03:31.135462: step 472, loss 1.77016, acc 0.53125, prec 0.0260824, recall 0.755068\n",
      "2017-12-08T00:03:31.742576: step 473, loss 1.36224, acc 0.640625, prec 0.0261042, recall 0.755481\n",
      "2017-12-08T00:03:32.343466: step 474, loss 0.906273, acc 0.75, prec 0.0260799, recall 0.755481\n",
      "2017-12-08T00:03:32.952718: step 475, loss 1.50166, acc 0.703125, prec 0.0261077, recall 0.755892\n",
      "2017-12-08T00:03:33.602216: step 476, loss 1.58456, acc 0.671875, prec 0.0261324, recall 0.756303\n",
      "2017-12-08T00:03:34.209384: step 477, loss 1.25833, acc 0.6875, prec 0.0261021, recall 0.756303\n",
      "2017-12-08T00:03:34.816318: step 478, loss 1.09448, acc 0.75, prec 0.0261343, recall 0.756711\n",
      "2017-12-08T00:03:35.424655: step 479, loss 1.43469, acc 0.65625, prec 0.0261574, recall 0.757119\n",
      "2017-12-08T00:03:36.027641: step 480, loss 0.714961, acc 0.828125, prec 0.0261971, recall 0.757525\n",
      "2017-12-08T00:03:36.631164: step 481, loss 0.737678, acc 0.734375, prec 0.0261714, recall 0.757525\n",
      "2017-12-08T00:03:37.236529: step 482, loss 1.67383, acc 0.828125, prec 0.0264358, recall 0.759536\n",
      "2017-12-08T00:03:37.845780: step 483, loss 0.713729, acc 0.78125, prec 0.0264144, recall 0.759536\n",
      "2017-12-08T00:03:38.451965: step 484, loss 0.287553, acc 0.8125, prec 0.0263962, recall 0.759536\n",
      "2017-12-08T00:03:39.050010: step 485, loss 0.484574, acc 0.8125, prec 0.026434, recall 0.759934\n",
      "2017-12-08T00:03:39.655146: step 486, loss 10.5081, acc 0.828125, prec 0.0264763, recall 0.757825\n",
      "2017-12-08T00:03:40.259881: step 487, loss 0.515561, acc 0.765625, prec 0.0264535, recall 0.757825\n",
      "2017-12-08T00:03:40.846376: step 488, loss 0.661266, acc 0.859375, prec 0.0265517, recall 0.758621\n",
      "2017-12-08T00:03:41.428316: step 489, loss 0.672614, acc 0.765625, prec 0.0265289, recall 0.758621\n",
      "2017-12-08T00:03:42.008359: step 490, loss 0.652859, acc 0.734375, prec 0.026503, recall 0.758621\n",
      "2017-12-08T00:03:42.586186: step 491, loss 0.664343, acc 0.828125, prec 0.0264863, recall 0.758621\n",
      "2017-12-08T00:03:43.172775: step 492, loss 2.25755, acc 0.8125, prec 0.0264696, recall 0.757377\n",
      "2017-12-08T00:03:43.793522: step 493, loss 4.94393, acc 0.75, prec 0.0265583, recall 0.756933\n",
      "2017-12-08T00:03:44.390372: step 494, loss 0.839199, acc 0.796875, prec 0.0265942, recall 0.757329\n",
      "2017-12-08T00:03:44.996114: step 495, loss 1.03146, acc 0.734375, prec 0.0265684, recall 0.757329\n",
      "2017-12-08T00:03:45.603885: step 496, loss 0.99991, acc 0.71875, prec 0.0266522, recall 0.758117\n",
      "2017-12-08T00:03:46.113509: step 497, loss 2.13588, acc 0.568627, prec 0.0267297, recall 0.7589\n",
      "Training finished\n",
      "Fold: 2 => Train/Dev split: 31796/10598\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold2/1512709427\n",
      "\n",
      "Start training\n",
      "2017-12-08T00:03:50.158392: step 1, loss 5.37141, acc 0.1875, prec 0.0188679, recall 1\n",
      "2017-12-08T00:03:50.654454: step 2, loss 16.3119, acc 0.375, prec 0.0215054, recall 0.666667\n",
      "2017-12-08T00:03:51.159252: step 3, loss 2.42447, acc 0.4375, prec 0.0155039, recall 0.666667\n",
      "2017-12-08T00:03:51.680267: step 4, loss 7.47784, acc 0.453125, prec 0.0122699, recall 0.5\n",
      "2017-12-08T00:03:52.200297: step 5, loss 1.74929, acc 0.515625, prec 0.0153846, recall 0.6\n",
      "2017-12-08T00:03:52.735497: step 6, loss 2.0433, acc 0.546875, prec 0.0177778, recall 0.666667\n",
      "2017-12-08T00:03:53.270109: step 7, loss 9.73973, acc 0.5625, prec 0.015873, recall 0.571429\n",
      "2017-12-08T00:03:53.868007: step 8, loss 1.3646, acc 0.625, prec 0.0144928, recall 0.571429\n",
      "2017-12-08T00:03:54.428807: step 9, loss 3.42335, acc 0.53125, prec 0.0163399, recall 0.555556\n",
      "2017-12-08T00:03:54.989720: step 10, loss 1.07064, acc 0.59375, prec 0.0150602, recall 0.555556\n",
      "2017-12-08T00:03:55.565255: step 11, loss 1.41832, acc 0.640625, prec 0.0140845, recall 0.555556\n",
      "2017-12-08T00:03:56.142687: step 12, loss 1.38764, acc 0.625, prec 0.0131926, recall 0.555556\n",
      "2017-12-08T00:03:56.721301: step 13, loss 1.69908, acc 0.578125, prec 0.014742, recall 0.6\n",
      "2017-12-08T00:03:57.293937: step 14, loss 6.81291, acc 0.5625, prec 0.016092, recall 0.583333\n",
      "2017-12-08T00:03:57.874677: step 15, loss 1.72909, acc 0.640625, prec 0.0174292, recall 0.615385\n",
      "2017-12-08T00:03:58.465295: step 16, loss 12.0207, acc 0.53125, prec 0.0163934, recall 0.571429\n",
      "2017-12-08T00:03:59.080996: step 17, loss 3.04954, acc 0.65625, prec 0.0157171, recall 0.533333\n",
      "2017-12-08T00:03:59.687615: step 18, loss 19.0463, acc 0.484375, prec 0.0148148, recall 0.470588\n",
      "2017-12-08T00:04:00.293064: step 19, loss 9.30995, acc 0.453125, prec 0.0139373, recall 0.444444\n",
      "2017-12-08T00:04:00.895110: step 20, loss 2.24473, acc 0.53125, prec 0.013245, recall 0.444444\n",
      "2017-12-08T00:04:01.488131: step 21, loss 4.72693, acc 0.328125, prec 0.0139104, recall 0.45\n",
      "2017-12-08T00:04:02.081188: step 22, loss 2.70991, acc 0.34375, prec 0.0130624, recall 0.45\n",
      "2017-12-08T00:04:02.680911: step 23, loss 3.73735, acc 0.21875, prec 0.0121786, recall 0.45\n",
      "2017-12-08T00:04:03.285330: step 24, loss 4.56595, acc 0.234375, prec 0.0126743, recall 0.47619\n",
      "2017-12-08T00:04:03.932269: step 25, loss 4.74297, acc 0.21875, prec 0.0130952, recall 0.5\n",
      "2017-12-08T00:04:04.549883: step 26, loss 5.15381, acc 0.171875, prec 0.012318, recall 0.5\n",
      "2017-12-08T00:04:05.157755: step 27, loss 4.23837, acc 0.28125, prec 0.0138151, recall 0.541667\n",
      "2017-12-08T00:04:05.765614: step 28, loss 4.77087, acc 0.1875, prec 0.0140845, recall 0.56\n",
      "2017-12-08T00:04:06.373035: step 29, loss 3.87867, acc 0.296875, prec 0.0144231, recall 0.576923\n",
      "2017-12-08T00:04:06.983426: step 30, loss 3.86024, acc 0.25, prec 0.0137868, recall 0.576923\n",
      "2017-12-08T00:04:07.587166: step 31, loss 3.16978, acc 0.3125, prec 0.0132509, recall 0.576923\n",
      "2017-12-08T00:04:08.184675: step 32, loss 2.91625, acc 0.34375, prec 0.0127768, recall 0.576923\n",
      "2017-12-08T00:04:08.795339: step 33, loss 2.7858, acc 0.421875, prec 0.0140148, recall 0.607143\n",
      "2017-12-08T00:04:09.400855: step 34, loss 1.92683, acc 0.484375, prec 0.0144346, recall 0.62069\n",
      "2017-12-08T00:04:10.016312: step 35, loss 12.8923, acc 0.546875, prec 0.0141176, recall 0.6\n",
      "2017-12-08T00:04:10.674618: step 36, loss 1.87384, acc 0.515625, prec 0.0137825, recall 0.6\n",
      "2017-12-08T00:04:11.326954: step 37, loss 1.27031, acc 0.703125, prec 0.0135849, recall 0.6\n",
      "2017-12-08T00:04:11.937993: step 38, loss 3.15948, acc 0.59375, prec 0.0133333, recall 0.580645\n",
      "2017-12-08T00:04:12.554795: step 39, loss 0.999961, acc 0.703125, prec 0.0131483, recall 0.580645\n",
      "2017-12-08T00:04:13.156366: step 40, loss 1.13785, acc 0.6875, prec 0.012959, recall 0.580645\n",
      "2017-12-08T00:04:13.768726: step 41, loss 0.864124, acc 0.71875, prec 0.0127932, recall 0.580645\n",
      "2017-12-08T00:04:14.375403: step 42, loss 0.860325, acc 0.78125, prec 0.0126671, recall 0.580645\n",
      "2017-12-08T00:04:14.989459: step 43, loss 12.7832, acc 0.765625, prec 0.0125436, recall 0.5625\n",
      "2017-12-08T00:04:15.598310: step 44, loss 0.470712, acc 0.84375, prec 0.0124567, recall 0.5625\n",
      "2017-12-08T00:04:16.201162: step 45, loss 0.386612, acc 0.828125, prec 0.0130405, recall 0.575758\n",
      "2017-12-08T00:04:16.800858: step 46, loss 0.255463, acc 0.921875, prec 0.0129959, recall 0.575758\n",
      "2017-12-08T00:04:17.402022: step 47, loss 17.9992, acc 0.8125, prec 0.0128988, recall 0.558824\n",
      "2017-12-08T00:04:18.006880: step 48, loss 5.02578, acc 0.75, prec 0.0127688, recall 0.542857\n",
      "2017-12-08T00:04:18.613895: step 49, loss 6.86945, acc 0.90625, prec 0.0127261, recall 0.527778\n",
      "2017-12-08T00:04:19.237774: step 50, loss 16.6364, acc 0.78125, prec 0.0126162, recall 0.513514\n",
      "2017-12-08T00:04:19.888907: step 51, loss 10.5034, acc 0.78125, prec 0.0125082, recall 0.5\n",
      "2017-12-08T00:04:20.544385: step 52, loss 4.00397, acc 0.640625, prec 0.0123297, recall 0.487179\n",
      "2017-12-08T00:04:21.188310: step 53, loss 0.799563, acc 0.71875, prec 0.0121873, recall 0.487179\n",
      "2017-12-08T00:04:21.851333: step 54, loss 1.26903, acc 0.703125, prec 0.0120406, recall 0.487179\n",
      "2017-12-08T00:04:22.500105: step 55, loss 7.54001, acc 0.484375, prec 0.0118012, recall 0.475\n",
      "2017-12-08T00:04:23.112073: step 56, loss 2.80001, acc 0.390625, prec 0.0121212, recall 0.487805\n",
      "2017-12-08T00:04:23.714658: step 57, loss 5.94332, acc 0.453125, prec 0.0124629, recall 0.488372\n",
      "2017-12-08T00:04:24.364365: step 58, loss 2.53628, acc 0.40625, prec 0.012188, recall 0.488372\n",
      "2017-12-08T00:04:24.974593: step 59, loss 3.03688, acc 0.390625, prec 0.0135977, recall 0.521739\n",
      "2017-12-08T00:04:25.600157: step 60, loss 3.6929, acc 0.296875, prec 0.0143488, recall 0.541667\n",
      "2017-12-08T00:04:26.262602: step 61, loss 2.78886, acc 0.34375, prec 0.0140237, recall 0.541667\n",
      "2017-12-08T00:04:26.878648: step 62, loss 5.78204, acc 0.328125, prec 0.014233, recall 0.54\n",
      "2017-12-08T00:04:27.482061: step 63, loss 3.60144, acc 0.25, prec 0.0138817, recall 0.54\n",
      "2017-12-08T00:04:28.088803: step 64, loss 3.86356, acc 0.265625, prec 0.0135542, recall 0.54\n",
      "2017-12-08T00:04:28.695104: step 65, loss 9.86866, acc 0.328125, prec 0.0137592, recall 0.538462\n",
      "2017-12-08T00:04:29.294198: step 66, loss 3.96052, acc 0.28125, prec 0.0139289, recall 0.54717\n",
      "2017-12-08T00:04:29.908347: step 67, loss 3.57417, acc 0.265625, prec 0.0140845, recall 0.555556\n",
      "2017-12-08T00:04:30.511430: step 68, loss 6.80229, acc 0.34375, prec 0.0147262, recall 0.561404\n",
      "2017-12-08T00:04:31.115266: step 69, loss 5.75683, acc 0.171875, prec 0.0148181, recall 0.568965\n",
      "2017-12-08T00:04:31.713584: step 70, loss 4.25298, acc 0.25, prec 0.0145055, recall 0.568965\n",
      "2017-12-08T00:04:32.316735: step 71, loss 4.75384, acc 0.21875, prec 0.0141935, recall 0.568965\n",
      "2017-12-08T00:04:32.919582: step 72, loss 7.07866, acc 0.203125, prec 0.0143098, recall 0.566667\n",
      "2017-12-08T00:04:33.526528: step 73, loss 4.01347, acc 0.328125, prec 0.0144628, recall 0.57377\n",
      "2017-12-08T00:04:34.159095: step 74, loss 3.25298, acc 0.3125, prec 0.0146045, recall 0.580645\n",
      "2017-12-08T00:04:34.769256: step 75, loss 3.79841, acc 0.21875, prec 0.0143141, recall 0.580645\n",
      "2017-12-08T00:04:35.371972: step 76, loss 3.05943, acc 0.359375, prec 0.0140845, recall 0.580645\n",
      "2017-12-08T00:04:35.975695: step 77, loss 2.88934, acc 0.328125, prec 0.0138515, recall 0.580645\n",
      "2017-12-08T00:04:36.602773: step 78, loss 2.87219, acc 0.375, prec 0.0136415, recall 0.580645\n",
      "2017-12-08T00:04:37.202288: step 79, loss 2.92785, acc 0.375, prec 0.0134379, recall 0.580645\n",
      "2017-12-08T00:04:37.806041: step 80, loss 4.92567, acc 0.453125, prec 0.013633, recall 0.578125\n",
      "2017-12-08T00:04:38.401273: step 81, loss 3.28388, acc 0.46875, prec 0.0134692, recall 0.569231\n",
      "2017-12-08T00:04:39.001910: step 82, loss 20.7612, acc 0.59375, prec 0.0140693, recall 0.557143\n",
      "2017-12-08T00:04:39.612513: step 83, loss 2.99533, acc 0.53125, prec 0.0142704, recall 0.56338\n",
      "2017-12-08T00:04:40.211389: step 84, loss 1.65947, acc 0.59375, prec 0.0144876, recall 0.569444\n",
      "2017-12-08T00:04:40.820943: step 85, loss 2.28793, acc 0.40625, prec 0.0142957, recall 0.569444\n",
      "2017-12-08T00:04:41.426582: step 86, loss 1.87023, acc 0.578125, prec 0.0148429, recall 0.581081\n",
      "2017-12-08T00:04:42.027024: step 87, loss 2.94093, acc 0.453125, prec 0.0150017, recall 0.586667\n",
      "2017-12-08T00:04:42.632972: step 88, loss 2.59789, acc 0.46875, prec 0.0151617, recall 0.592105\n",
      "2017-12-08T00:04:43.236048: step 89, loss 2.58493, acc 0.390625, prec 0.0149651, recall 0.592105\n",
      "2017-12-08T00:04:43.839886: step 90, loss 5.35641, acc 0.375, prec 0.0147735, recall 0.584416\n",
      "2017-12-08T00:04:44.478062: step 91, loss 2.85482, acc 0.421875, prec 0.0145962, recall 0.584416\n",
      "2017-12-08T00:04:45.088896: step 92, loss 2.20091, acc 0.4375, prec 0.0147436, recall 0.589744\n",
      "2017-12-08T00:04:45.688156: step 93, loss 5.63672, acc 0.40625, prec 0.0145708, recall 0.582278\n",
      "2017-12-08T00:04:46.291904: step 94, loss 5.01567, acc 0.34375, prec 0.014384, recall 0.575\n",
      "2017-12-08T00:04:46.898319: step 95, loss 5.47212, acc 0.484375, prec 0.0145466, recall 0.573171\n",
      "2017-12-08T00:04:47.496099: step 96, loss 2.02395, acc 0.515625, prec 0.0144083, recall 0.573171\n",
      "2017-12-08T00:04:48.097708: step 97, loss 3.37045, acc 0.421875, prec 0.0145455, recall 0.578313\n",
      "2017-12-08T00:04:48.703843: step 98, loss 2.98769, acc 0.390625, prec 0.0143756, recall 0.578313\n",
      "2017-12-08T00:04:49.309403: step 99, loss 2.01553, acc 0.484375, prec 0.0142349, recall 0.578313\n",
      "2017-12-08T00:04:49.907739: step 100, loss 2.53793, acc 0.40625, prec 0.0143653, recall 0.583333\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold2/1512709427/checkpoints/model-100\n",
      "\n",
      "2017-12-08T00:04:51.372372: step 101, loss 1.70936, acc 0.578125, prec 0.0142525, recall 0.583333\n",
      "2017-12-08T00:04:51.962164: step 102, loss 2.34147, acc 0.4375, prec 0.0141048, recall 0.583333\n",
      "2017-12-08T00:04:52.533709: step 103, loss 3.01502, acc 0.546875, prec 0.0142694, recall 0.588235\n",
      "2017-12-08T00:04:53.103019: step 104, loss 2.05136, acc 0.609375, prec 0.0141683, recall 0.588235\n",
      "2017-12-08T00:04:53.663120: step 105, loss 1.91726, acc 0.609375, prec 0.0140687, recall 0.588235\n",
      "2017-12-08T00:04:54.243491: step 106, loss 2.22328, acc 0.59375, prec 0.0147921, recall 0.602273\n",
      "2017-12-08T00:04:54.825933: step 107, loss 1.94563, acc 0.625, prec 0.0149667, recall 0.606742\n",
      "2017-12-08T00:04:55.412306: step 108, loss 21.8832, acc 0.625, prec 0.014876, recall 0.593407\n",
      "2017-12-08T00:04:56.022129: step 109, loss 1.44036, acc 0.6875, prec 0.0150644, recall 0.597826\n",
      "2017-12-08T00:04:56.609465: step 110, loss 1.94606, acc 0.484375, prec 0.0149294, recall 0.597826\n",
      "2017-12-08T00:04:57.203279: step 111, loss 25.9763, acc 0.546875, prec 0.0150862, recall 0.589474\n",
      "2017-12-08T00:04:57.832920: step 112, loss 1.41067, acc 0.53125, prec 0.0149653, recall 0.589474\n",
      "2017-12-08T00:04:58.434889: step 113, loss 1.76848, acc 0.53125, prec 0.0148462, recall 0.589474\n",
      "2017-12-08T00:04:59.042897: step 114, loss 14.5652, acc 0.5, prec 0.0149842, recall 0.587629\n",
      "2017-12-08T00:04:59.651801: step 115, loss 11.441, acc 0.5, prec 0.0148631, recall 0.581633\n",
      "2017-12-08T00:05:00.288076: step 116, loss 3.13718, acc 0.46875, prec 0.0149871, recall 0.585859\n",
      "2017-12-08T00:05:00.902134: step 117, loss 2.93396, acc 0.375, prec 0.0153374, recall 0.594059\n",
      "2017-12-08T00:05:01.505557: step 118, loss 3.28823, acc 0.34375, prec 0.0154235, recall 0.598039\n",
      "2017-12-08T00:05:02.106590: step 119, loss 3.38716, acc 0.375, prec 0.0152691, recall 0.598039\n",
      "2017-12-08T00:05:02.711333: step 120, loss 3.60708, acc 0.375, prec 0.0151177, recall 0.598039\n",
      "2017-12-08T00:05:03.317408: step 121, loss 3.64495, acc 0.265625, prec 0.0149437, recall 0.598039\n",
      "2017-12-08T00:05:03.924975: step 122, loss 4.78518, acc 0.28125, prec 0.0147807, recall 0.592233\n",
      "2017-12-08T00:05:04.550449: step 123, loss 3.3677, acc 0.390625, prec 0.0146423, recall 0.592233\n",
      "2017-12-08T00:05:05.161649: step 124, loss 3.11204, acc 0.390625, prec 0.0145065, recall 0.592233\n",
      "2017-12-08T00:05:05.767695: step 125, loss 4.89777, acc 0.421875, prec 0.0150801, recall 0.598131\n",
      "2017-12-08T00:05:06.371479: step 126, loss 3.18466, acc 0.359375, prec 0.0153954, recall 0.605505\n",
      "2017-12-08T00:05:06.979804: step 127, loss 2.96216, acc 0.328125, prec 0.0152425, recall 0.605505\n",
      "2017-12-08T00:05:07.579837: step 128, loss 2.90746, acc 0.328125, prec 0.0150926, recall 0.605505\n",
      "2017-12-08T00:05:08.182294: step 129, loss 2.69272, acc 0.484375, prec 0.0154265, recall 0.612613\n",
      "2017-12-08T00:05:08.785970: step 130, loss 15.4833, acc 0.453125, prec 0.0153084, recall 0.607143\n",
      "2017-12-08T00:05:09.391175: step 131, loss 1.92552, acc 0.484375, prec 0.0154155, recall 0.610619\n",
      "2017-12-08T00:05:10.009048: step 132, loss 2.57049, acc 0.375, prec 0.015279, recall 0.610619\n",
      "2017-12-08T00:05:10.622277: step 133, loss 11.176, acc 0.578125, prec 0.0154083, recall 0.608696\n",
      "2017-12-08T00:05:11.227219: step 134, loss 7.95371, acc 0.515625, prec 0.0155225, recall 0.606838\n",
      "2017-12-08T00:05:11.833294: step 135, loss 2.2628, acc 0.671875, prec 0.0156658, recall 0.610169\n",
      "2017-12-08T00:05:12.432170: step 136, loss 6.17171, acc 0.546875, prec 0.0155709, recall 0.605042\n",
      "2017-12-08T00:05:13.038382: step 137, loss 18.1131, acc 0.34375, prec 0.0160703, recall 0.604839\n",
      "2017-12-08T00:05:13.639592: step 138, loss 5.34813, acc 0.40625, prec 0.016153, recall 0.603175\n",
      "2017-12-08T00:05:14.241865: step 139, loss 3.87092, acc 0.265625, prec 0.0159933, recall 0.603175\n",
      "2017-12-08T00:05:14.879424: step 140, loss 4.82505, acc 0.296875, prec 0.0158432, recall 0.603175\n",
      "2017-12-08T00:05:15.486568: step 141, loss 4.32339, acc 0.25, prec 0.0156863, recall 0.603175\n",
      "2017-12-08T00:05:16.092910: step 142, loss 3.36218, acc 0.265625, prec 0.0155356, recall 0.603175\n",
      "2017-12-08T00:05:16.690009: step 143, loss 5.78222, acc 0.171875, prec 0.0155681, recall 0.606299\n",
      "2017-12-08T00:05:17.293019: step 144, loss 5.40142, acc 0.140625, prec 0.0155938, recall 0.609375\n",
      "2017-12-08T00:05:17.896296: step 145, loss 5.44329, acc 0.234375, prec 0.0156374, recall 0.612403\n",
      "2017-12-08T00:05:18.483852: step 146, loss 4.86291, acc 0.234375, prec 0.0154872, recall 0.612403\n",
      "2017-12-08T00:05:19.062094: step 147, loss 4.2184, acc 0.25, prec 0.0153428, recall 0.612403\n",
      "2017-12-08T00:05:19.640185: step 148, loss 4.82091, acc 0.15625, prec 0.0151835, recall 0.612403\n",
      "2017-12-08T00:05:20.221943: step 149, loss 4.27371, acc 0.25, prec 0.0150448, recall 0.612403\n",
      "2017-12-08T00:05:20.812208: step 150, loss 4.73617, acc 0.21875, prec 0.0152744, recall 0.618321\n",
      "2017-12-08T00:05:21.388227: step 151, loss 11.5177, acc 0.28125, prec 0.0151458, recall 0.613636\n",
      "2017-12-08T00:05:21.966982: step 152, loss 3.48583, acc 0.375, prec 0.0150334, recall 0.613636\n",
      "2017-12-08T00:05:22.564745: step 153, loss 3.05999, acc 0.359375, prec 0.0151013, recall 0.616541\n",
      "2017-12-08T00:05:23.167536: step 154, loss 2.11819, acc 0.5, prec 0.0153734, recall 0.622222\n",
      "2017-12-08T00:05:23.769644: step 155, loss 2.01659, acc 0.578125, prec 0.0154771, recall 0.625\n",
      "2017-12-08T00:05:24.371831: step 156, loss 2.34994, acc 0.46875, prec 0.0153818, recall 0.625\n",
      "2017-12-08T00:05:25.013335: step 157, loss 1.62002, acc 0.5625, prec 0.0153043, recall 0.625\n",
      "2017-12-08T00:05:25.609297: step 158, loss 1.5419, acc 0.609375, prec 0.0155886, recall 0.630435\n",
      "2017-12-08T00:05:26.223686: step 159, loss 1.57884, acc 0.578125, prec 0.0155136, recall 0.630435\n",
      "2017-12-08T00:05:26.831848: step 160, loss 0.931823, acc 0.671875, prec 0.0156306, recall 0.633094\n",
      "2017-12-08T00:05:27.433898: step 161, loss 0.738978, acc 0.75, prec 0.0155863, recall 0.633094\n",
      "2017-12-08T00:05:28.037273: step 162, loss 1.65887, acc 0.671875, prec 0.0155285, recall 0.633094\n",
      "2017-12-08T00:05:28.644743: step 163, loss 0.848968, acc 0.6875, prec 0.0154739, recall 0.633094\n",
      "2017-12-08T00:05:29.253963: step 164, loss 26.1344, acc 0.84375, prec 0.0154522, recall 0.624114\n",
      "2017-12-08T00:05:29.864661: step 165, loss 2.01114, acc 0.75, prec 0.0154116, recall 0.619718\n",
      "2017-12-08T00:05:30.473969: step 166, loss 6.94384, acc 0.71875, prec 0.0153685, recall 0.611111\n",
      "2017-12-08T00:05:31.084124: step 167, loss 1.09521, acc 0.703125, prec 0.0153177, recall 0.611111\n",
      "2017-12-08T00:05:31.693450: step 168, loss 0.993287, acc 0.71875, prec 0.0152698, recall 0.611111\n",
      "2017-12-08T00:05:32.294043: step 169, loss 1.44011, acc 0.640625, prec 0.0152091, recall 0.611111\n",
      "2017-12-08T00:05:32.896485: step 170, loss 0.681063, acc 0.734375, prec 0.0151646, recall 0.611111\n",
      "2017-12-08T00:05:33.498783: step 171, loss 0.847504, acc 0.734375, prec 0.0151203, recall 0.611111\n",
      "2017-12-08T00:05:34.116543: step 172, loss 1.81504, acc 0.6875, prec 0.0152371, recall 0.613793\n",
      "2017-12-08T00:05:34.723993: step 173, loss 1.66752, acc 0.640625, prec 0.0155131, recall 0.619048\n",
      "2017-12-08T00:05:35.356183: step 174, loss 1.34328, acc 0.59375, prec 0.0154447, recall 0.619048\n",
      "2017-12-08T00:05:35.979055: step 175, loss 1.02178, acc 0.671875, prec 0.0153898, recall 0.619048\n",
      "2017-12-08T00:05:36.586545: step 176, loss 14.9916, acc 0.609375, prec 0.0153276, recall 0.614865\n",
      "2017-12-08T00:05:37.190962: step 177, loss 1.10972, acc 0.609375, prec 0.0152633, recall 0.614865\n",
      "2017-12-08T00:05:37.801033: step 178, loss 2.07511, acc 0.59375, prec 0.0153615, recall 0.61745\n",
      "2017-12-08T00:05:38.410912: step 179, loss 7.04385, acc 0.625, prec 0.0153027, recall 0.613333\n",
      "2017-12-08T00:05:39.026908: step 180, loss 1.69818, acc 0.59375, prec 0.0152368, recall 0.613333\n",
      "2017-12-08T00:05:39.636504: step 181, loss 9.22051, acc 0.609375, prec 0.0153389, recall 0.611842\n",
      "2017-12-08T00:05:40.251828: step 182, loss 2.16629, acc 0.546875, prec 0.0154275, recall 0.614379\n",
      "2017-12-08T00:05:40.875046: step 183, loss 9.35419, acc 0.53125, prec 0.0155178, recall 0.608974\n",
      "2017-12-08T00:05:41.486893: step 184, loss 2.99294, acc 0.40625, prec 0.0154221, recall 0.608974\n",
      "2017-12-08T00:05:42.105317: step 185, loss 2.23169, acc 0.5, prec 0.0155014, recall 0.611465\n",
      "2017-12-08T00:05:42.709787: step 186, loss 2.80777, acc 0.375, prec 0.0155598, recall 0.613924\n",
      "2017-12-08T00:05:43.312717: step 187, loss 2.3451, acc 0.46875, prec 0.0154754, recall 0.613924\n",
      "2017-12-08T00:05:43.910648: step 188, loss 3.2471, acc 0.421875, prec 0.0155408, recall 0.616352\n",
      "2017-12-08T00:05:44.514401: step 189, loss 3.46236, acc 0.359375, prec 0.0155955, recall 0.61875\n",
      "2017-12-08T00:05:45.146523: step 190, loss 3.0708, acc 0.390625, prec 0.0156544, recall 0.621118\n",
      "2017-12-08T00:05:45.782772: step 191, loss 3.10512, acc 0.359375, prec 0.0155545, recall 0.621118\n",
      "2017-12-08T00:05:46.449095: step 192, loss 10.0282, acc 0.328125, prec 0.0154536, recall 0.617284\n",
      "2017-12-08T00:05:47.059687: step 193, loss 3.21799, acc 0.40625, prec 0.0155146, recall 0.619632\n",
      "2017-12-08T00:05:47.663508: step 194, loss 2.58569, acc 0.359375, prec 0.0154175, recall 0.619632\n",
      "2017-12-08T00:05:48.272300: step 195, loss 2.83951, acc 0.40625, prec 0.0153286, recall 0.619632\n",
      "2017-12-08T00:05:48.871183: step 196, loss 3.7673, acc 0.34375, prec 0.0156768, recall 0.626506\n",
      "2017-12-08T00:05:49.475424: step 197, loss 2.91111, acc 0.421875, prec 0.0157374, recall 0.628743\n",
      "2017-12-08T00:05:50.084153: step 198, loss 2.63496, acc 0.4375, prec 0.015653, recall 0.628743\n",
      "2017-12-08T00:05:50.687280: step 199, loss 2.70234, acc 0.46875, prec 0.015866, recall 0.633136\n",
      "2017-12-08T00:05:51.297865: step 200, loss 1.92974, acc 0.578125, prec 0.0158027, recall 0.633136\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold2/1512709427/checkpoints/model-200\n",
      "\n",
      "2017-12-08T00:05:52.788135: step 201, loss 9.11943, acc 0.484375, prec 0.0157307, recall 0.625731\n",
      "2017-12-08T00:05:53.376731: step 202, loss 2.29657, acc 0.515625, prec 0.0159473, recall 0.630058\n",
      "2017-12-08T00:05:53.959522: step 203, loss 1.2274, acc 0.671875, prec 0.0158985, recall 0.630058\n",
      "2017-12-08T00:05:54.545114: step 204, loss 1.63668, acc 0.625, prec 0.015986, recall 0.632184\n",
      "2017-12-08T00:05:55.137822: step 205, loss 1.30564, acc 0.578125, prec 0.0159236, recall 0.632184\n",
      "2017-12-08T00:05:55.750673: step 206, loss 10.6145, acc 0.609375, prec 0.0158707, recall 0.625\n",
      "2017-12-08T00:05:56.336922: step 207, loss 2.35697, acc 0.53125, prec 0.016085, recall 0.629214\n",
      "2017-12-08T00:05:56.919064: step 208, loss 2.01599, acc 0.625, prec 0.0161706, recall 0.631285\n",
      "2017-12-08T00:05:57.511844: step 209, loss 1.94685, acc 0.484375, prec 0.0162347, recall 0.633333\n",
      "2017-12-08T00:05:58.124554: step 210, loss 5.19013, acc 0.5625, prec 0.0163121, recall 0.631868\n",
      "2017-12-08T00:05:58.739781: step 211, loss 1.2949, acc 0.671875, prec 0.0164027, recall 0.63388\n",
      "2017-12-08T00:05:59.331801: step 212, loss 2.72353, acc 0.578125, prec 0.0166174, recall 0.637838\n",
      "2017-12-08T00:05:59.945118: step 213, loss 4.42452, acc 0.46875, prec 0.0165405, recall 0.634409\n",
      "2017-12-08T00:06:00.547019: step 214, loss 1.69235, acc 0.5, prec 0.0167411, recall 0.638298\n",
      "2017-12-08T00:06:01.157915: step 215, loss 3.38414, acc 0.5, prec 0.016669, recall 0.634921\n",
      "2017-12-08T00:06:01.758483: step 216, loss 2.62825, acc 0.53125, prec 0.0167358, recall 0.636842\n",
      "2017-12-08T00:06:02.368056: step 217, loss 2.28079, acc 0.453125, prec 0.0167905, recall 0.638743\n",
      "2017-12-08T00:06:02.974221: step 218, loss 1.76617, acc 0.5625, prec 0.0167261, recall 0.638743\n",
      "2017-12-08T00:06:03.579586: step 219, loss 2.94516, acc 0.390625, prec 0.0166371, recall 0.638743\n",
      "2017-12-08T00:06:04.187002: step 220, loss 2.26783, acc 0.515625, prec 0.0167006, recall 0.640625\n",
      "2017-12-08T00:06:04.790417: step 221, loss 2.22945, acc 0.5625, prec 0.0167704, recall 0.642487\n",
      "2017-12-08T00:06:05.422149: step 222, loss 2.52986, acc 0.40625, prec 0.0166846, recall 0.642487\n",
      "2017-12-08T00:06:06.028550: step 223, loss 2.27003, acc 0.484375, prec 0.0167426, recall 0.64433\n",
      "2017-12-08T00:06:06.631456: step 224, loss 1.54857, acc 0.546875, prec 0.016809, recall 0.646154\n",
      "2017-12-08T00:06:07.235898: step 225, loss 1.49824, acc 0.609375, prec 0.0167531, recall 0.646154\n",
      "2017-12-08T00:06:07.835056: step 226, loss 1.54937, acc 0.578125, prec 0.0166932, recall 0.646154\n",
      "2017-12-08T00:06:08.441884: step 227, loss 1.76313, acc 0.609375, prec 0.0166381, recall 0.646154\n",
      "2017-12-08T00:06:09.053097: step 228, loss 0.96444, acc 0.734375, prec 0.0166008, recall 0.646154\n",
      "2017-12-08T00:06:09.658459: step 229, loss 1.80643, acc 0.8125, prec 0.0167061, recall 0.64467\n",
      "2017-12-08T00:06:10.269880: step 230, loss 2.92862, acc 0.6875, prec 0.0166645, recall 0.641414\n",
      "2017-12-08T00:06:10.881257: step 231, loss 2.42676, acc 0.734375, prec 0.0166296, recall 0.638191\n",
      "2017-12-08T00:06:11.491004: step 232, loss 5.59902, acc 0.734375, prec 0.0165948, recall 0.635\n",
      "2017-12-08T00:06:12.098693: step 233, loss 5.57366, acc 0.765625, prec 0.0165645, recall 0.631841\n",
      "2017-12-08T00:06:12.702310: step 234, loss 2.23182, acc 0.625, prec 0.016515, recall 0.628713\n",
      "2017-12-08T00:06:13.306082: step 235, loss 8.94409, acc 0.640625, prec 0.0165975, recall 0.62439\n",
      "2017-12-08T00:06:13.914159: step 236, loss 2.92244, acc 0.640625, prec 0.0165503, recall 0.621359\n",
      "2017-12-08T00:06:14.523077: step 237, loss 2.00801, acc 0.578125, prec 0.0164927, recall 0.621359\n",
      "2017-12-08T00:06:15.129038: step 238, loss 8.20235, acc 0.453125, prec 0.0166731, recall 0.62201\n",
      "2017-12-08T00:06:15.779595: step 239, loss 2.61618, acc 0.453125, prec 0.0165986, recall 0.62201\n",
      "2017-12-08T00:06:16.387943: step 240, loss 2.53821, acc 0.375, prec 0.0166391, recall 0.62381\n",
      "2017-12-08T00:06:16.990597: step 241, loss 4.46268, acc 0.265625, prec 0.0165404, recall 0.62381\n",
      "2017-12-08T00:06:17.594429: step 242, loss 3.52179, acc 0.328125, prec 0.0166981, recall 0.627358\n",
      "2017-12-08T00:06:18.196534: step 243, loss 3.5838, acc 0.25, prec 0.0167207, recall 0.629108\n",
      "2017-12-08T00:06:18.797578: step 244, loss 3.97075, acc 0.203125, prec 0.0169807, recall 0.634259\n",
      "2017-12-08T00:06:19.400637: step 245, loss 4.04274, acc 0.1875, prec 0.0168719, recall 0.634259\n",
      "2017-12-08T00:06:20.000038: step 246, loss 2.58171, acc 0.421875, prec 0.0167954, recall 0.634259\n",
      "2017-12-08T00:06:20.603706: step 247, loss 3.93131, acc 0.34375, prec 0.0168293, recall 0.635945\n",
      "2017-12-08T00:06:21.211196: step 248, loss 3.45573, acc 0.40625, prec 0.0171096, recall 0.640909\n",
      "2017-12-08T00:06:21.816714: step 249, loss 3.05147, acc 0.28125, prec 0.0171332, recall 0.642534\n",
      "2017-12-08T00:06:22.428370: step 250, loss 2.76698, acc 0.484375, prec 0.0173015, recall 0.64574\n",
      "2017-12-08T00:06:23.026568: step 251, loss 2.99898, acc 0.546875, prec 0.017359, recall 0.647321\n",
      "2017-12-08T00:06:23.618124: step 252, loss 2.7517, acc 0.375, prec 0.0172763, recall 0.647321\n",
      "2017-12-08T00:06:24.214012: step 253, loss 1.92587, acc 0.625, prec 0.0173438, recall 0.648889\n",
      "2017-12-08T00:06:24.822246: step 254, loss 2.05178, acc 0.546875, prec 0.0172842, recall 0.648889\n",
      "2017-12-08T00:06:25.430953: step 255, loss 1.47717, acc 0.546875, prec 0.017341, recall 0.650442\n",
      "2017-12-08T00:06:26.041969: step 256, loss 2.23807, acc 0.4375, prec 0.0172677, recall 0.650442\n",
      "2017-12-08T00:06:26.672654: step 257, loss 2.39364, acc 0.625, prec 0.0172212, recall 0.647577\n",
      "2017-12-08T00:06:27.275470: step 258, loss 1.17627, acc 0.671875, prec 0.0171789, recall 0.647577\n",
      "2017-12-08T00:06:27.875561: step 259, loss 1.35088, acc 0.625, prec 0.0172454, recall 0.649123\n",
      "2017-12-08T00:06:28.476167: step 260, loss 0.650171, acc 0.75, prec 0.0173276, recall 0.650655\n",
      "2017-12-08T00:06:29.080110: step 261, loss 7.86845, acc 0.75, prec 0.0174135, recall 0.646552\n",
      "2017-12-08T00:06:29.674982: step 262, loss 0.953894, acc 0.703125, prec 0.0173752, recall 0.646552\n",
      "2017-12-08T00:06:30.302626: step 263, loss 0.979205, acc 0.625, prec 0.017327, recall 0.646552\n",
      "2017-12-08T00:06:30.910887: step 264, loss 0.971647, acc 0.65625, prec 0.0172831, recall 0.646552\n",
      "2017-12-08T00:06:31.527319: step 265, loss 10.9288, acc 0.75, prec 0.0172533, recall 0.643777\n",
      "2017-12-08T00:06:32.139904: step 266, loss 0.778361, acc 0.765625, prec 0.0174492, recall 0.646809\n",
      "2017-12-08T00:06:32.748081: step 267, loss 2.33298, acc 0.75, prec 0.0175318, recall 0.64557\n",
      "2017-12-08T00:06:33.356854: step 268, loss 29.3154, acc 0.609375, prec 0.017598, recall 0.641667\n",
      "2017-12-08T00:06:33.968678: step 269, loss 1.70966, acc 0.625, prec 0.0176618, recall 0.643154\n",
      "2017-12-08T00:06:34.576328: step 270, loss 3.14422, acc 0.578125, prec 0.0177212, recall 0.641975\n",
      "2017-12-08T00:06:35.189346: step 271, loss 3.30705, acc 0.546875, prec 0.017665, recall 0.639344\n",
      "2017-12-08T00:06:35.819466: step 272, loss 1.95505, acc 0.46875, prec 0.0175973, recall 0.639344\n",
      "2017-12-08T00:06:36.421743: step 273, loss 3.42955, acc 0.546875, prec 0.0175419, recall 0.636735\n",
      "2017-12-08T00:06:37.029585: step 274, loss 2.12655, acc 0.546875, prec 0.0177051, recall 0.639676\n",
      "2017-12-08T00:06:37.606392: step 275, loss 2.56201, acc 0.390625, prec 0.017628, recall 0.639676\n",
      "2017-12-08T00:06:38.190421: step 276, loss 3.18007, acc 0.421875, prec 0.0177738, recall 0.64257\n",
      "2017-12-08T00:06:38.769751: step 277, loss 2.24992, acc 0.46875, prec 0.0178156, recall 0.644\n",
      "2017-12-08T00:06:39.344417: step 278, loss 2.59021, acc 0.453125, prec 0.0177469, recall 0.644\n",
      "2017-12-08T00:06:39.921903: step 279, loss 2.62714, acc 0.46875, prec 0.0177885, recall 0.645418\n",
      "2017-12-08T00:06:40.492136: step 280, loss 2.41509, acc 0.453125, prec 0.0177204, recall 0.645418\n",
      "2017-12-08T00:06:41.069400: step 281, loss 2.19226, acc 0.515625, prec 0.0177676, recall 0.646825\n",
      "2017-12-08T00:06:41.662345: step 282, loss 2.54463, acc 0.46875, prec 0.0178087, recall 0.648221\n",
      "2017-12-08T00:06:42.272009: step 283, loss 2.52665, acc 0.421875, prec 0.0177374, recall 0.648221\n",
      "2017-12-08T00:06:42.868550: step 284, loss 6.45148, acc 0.46875, prec 0.0179918, recall 0.649805\n",
      "2017-12-08T00:06:43.480233: step 285, loss 1.57724, acc 0.59375, prec 0.0180471, recall 0.651163\n",
      "2017-12-08T00:06:44.083533: step 286, loss 3.12165, acc 0.546875, prec 0.0184134, recall 0.653992\n",
      "2017-12-08T00:06:44.685339: step 287, loss 1.9536, acc 0.5625, prec 0.0183584, recall 0.653992\n",
      "2017-12-08T00:06:45.286222: step 288, loss 2.34915, acc 0.53125, prec 0.0182998, recall 0.653992\n",
      "2017-12-08T00:06:45.907785: step 289, loss 2.12147, acc 0.4375, prec 0.01823, recall 0.653992\n",
      "2017-12-08T00:06:46.530300: step 290, loss 1.67688, acc 0.5, prec 0.0181684, recall 0.653992\n",
      "2017-12-08T00:06:47.141156: step 291, loss 1.49052, acc 0.6875, prec 0.0184405, recall 0.657895\n",
      "2017-12-08T00:06:47.747282: step 292, loss 1.38048, acc 0.71875, prec 0.0184056, recall 0.657895\n",
      "2017-12-08T00:06:48.348957: step 293, loss 8.36439, acc 0.6875, prec 0.0184719, recall 0.656716\n",
      "2017-12-08T00:06:48.952248: step 294, loss 3.51389, acc 0.71875, prec 0.018439, recall 0.654275\n",
      "2017-12-08T00:06:49.558037: step 295, loss 4.18647, acc 0.65625, prec 0.0185011, recall 0.653137\n",
      "2017-12-08T00:06:50.167601: step 296, loss 1.49745, acc 0.640625, prec 0.0184567, recall 0.653137\n",
      "2017-12-08T00:06:50.774358: step 297, loss 1.42263, acc 0.625, prec 0.0186148, recall 0.655678\n",
      "2017-12-08T00:06:51.384131: step 298, loss 1.06179, acc 0.71875, prec 0.01858, recall 0.655678\n",
      "2017-12-08T00:06:52.007769: step 299, loss 1.40801, acc 0.6875, prec 0.0186432, recall 0.656934\n",
      "2017-12-08T00:06:52.609666: step 300, loss 1.42243, acc 0.65625, prec 0.0186008, recall 0.656934\n",
      "\n",
      "Evaluation:\n",
      "2017-12-08T00:07:36.773061: step 300, loss 1.38653, acc 0.680034, prec 0.0229414, recall 0.71564\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold2/1512709427/checkpoints/model-300\n",
      "\n",
      "2017-12-08T00:07:38.236960: step 301, loss 1.62024, acc 0.515625, prec 0.0228875, recall 0.71564\n",
      "2017-12-08T00:07:38.829065: step 302, loss 1.38588, acc 0.671875, prec 0.0228511, recall 0.71564\n",
      "2017-12-08T00:07:39.469608: step 303, loss 3.8133, acc 0.5625, prec 0.0229521, recall 0.715294\n",
      "2017-12-08T00:07:40.059961: step 304, loss 1.77655, acc 0.65625, prec 0.0229876, recall 0.715962\n",
      "2017-12-08T00:07:40.662091: step 305, loss 1.65936, acc 0.671875, prec 0.0229513, recall 0.715962\n",
      "2017-12-08T00:07:41.253538: step 306, loss 1.42654, acc 0.6875, prec 0.0229902, recall 0.716628\n",
      "2017-12-08T00:07:41.838982: step 307, loss 2.07703, acc 0.6875, prec 0.0233221, recall 0.719907\n",
      "2017-12-08T00:07:42.433917: step 308, loss 1.24568, acc 0.71875, prec 0.0232906, recall 0.719907\n",
      "2017-12-08T00:07:43.033876: step 309, loss 12.504, acc 0.703125, prec 0.0233323, recall 0.718894\n",
      "2017-12-08T00:07:43.642339: step 310, loss 1.03364, acc 0.703125, prec 0.0232992, recall 0.718894\n",
      "2017-12-08T00:07:44.248938: step 311, loss 1.55839, acc 0.609375, prec 0.0232558, recall 0.718894\n",
      "2017-12-08T00:07:44.856152: step 312, loss 1.11212, acc 0.671875, prec 0.0232195, recall 0.718894\n",
      "2017-12-08T00:07:45.464688: step 313, loss 15.2416, acc 0.65625, prec 0.023185, recall 0.715596\n",
      "2017-12-08T00:07:46.081863: step 314, loss 2.15397, acc 0.65625, prec 0.0233645, recall 0.71754\n",
      "2017-12-08T00:07:46.730066: step 315, loss 1.4204, acc 0.578125, prec 0.0233901, recall 0.718182\n",
      "2017-12-08T00:07:47.344961: step 316, loss 1.00917, acc 0.71875, prec 0.023359, recall 0.718182\n",
      "2017-12-08T00:07:47.947984: step 317, loss 1.30805, acc 0.703125, prec 0.0233262, recall 0.718182\n",
      "2017-12-08T00:07:48.565900: step 318, loss 1.69202, acc 0.609375, prec 0.0232832, recall 0.718182\n",
      "2017-12-08T00:07:49.178543: step 319, loss 2.09636, acc 0.5, prec 0.0232285, recall 0.718182\n",
      "2017-12-08T00:07:49.784988: step 320, loss 1.56217, acc 0.609375, prec 0.0232575, recall 0.718821\n",
      "2017-12-08T00:07:50.390094: step 321, loss 4.34946, acc 0.765625, prec 0.0232337, recall 0.717195\n",
      "2017-12-08T00:07:51.003227: step 322, loss 1.988, acc 0.546875, prec 0.0232558, recall 0.717833\n",
      "2017-12-08T00:07:51.636042: step 323, loss 2.02635, acc 0.578125, prec 0.0232813, recall 0.718468\n",
      "2017-12-08T00:07:52.268653: step 324, loss 16.8621, acc 0.46875, prec 0.0232964, recall 0.717489\n",
      "2017-12-08T00:07:52.879924: step 325, loss 1.68485, acc 0.609375, prec 0.0233251, recall 0.718121\n",
      "2017-12-08T00:07:53.490158: step 326, loss 3.78118, acc 0.46875, prec 0.0233401, recall 0.717149\n",
      "2017-12-08T00:07:54.096160: step 327, loss 2.31591, acc 0.421875, prec 0.0233483, recall 0.717778\n",
      "2017-12-08T00:07:54.704781: step 328, loss 8.33097, acc 0.453125, prec 0.0234319, recall 0.717439\n",
      "2017-12-08T00:07:55.314215: step 329, loss 2.14481, acc 0.421875, prec 0.0233695, recall 0.717439\n",
      "2017-12-08T00:07:55.924463: step 330, loss 2.97189, acc 0.390625, prec 0.0233042, recall 0.717439\n",
      "2017-12-08T00:07:56.545895: step 331, loss 2.82154, acc 0.453125, prec 0.0233855, recall 0.718681\n",
      "2017-12-08T00:07:57.192177: step 332, loss 2.74943, acc 0.453125, prec 0.0233968, recall 0.719298\n",
      "2017-12-08T00:07:57.804504: step 333, loss 2.61167, acc 0.421875, prec 0.0234742, recall 0.720524\n",
      "2017-12-08T00:07:58.411807: step 334, loss 2.54544, acc 0.4375, prec 0.0234835, recall 0.721133\n",
      "2017-12-08T00:07:59.018162: step 335, loss 3.48383, acc 0.46875, prec 0.0234978, recall 0.720174\n",
      "2017-12-08T00:07:59.628016: step 336, loss 15.0144, acc 0.421875, prec 0.0234381, recall 0.718615\n",
      "2017-12-08T00:08:00.265966: step 337, loss 2.23474, acc 0.4375, prec 0.0234474, recall 0.719222\n",
      "2017-12-08T00:08:00.876626: step 338, loss 2.47594, acc 0.4375, prec 0.0233881, recall 0.719222\n",
      "2017-12-08T00:08:01.471210: step 339, loss 1.83231, acc 0.46875, prec 0.0234008, recall 0.719828\n",
      "2017-12-08T00:08:02.070791: step 340, loss 2.20162, acc 0.53125, prec 0.0233517, recall 0.719828\n",
      "2017-12-08T00:08:02.695091: step 341, loss 2.17748, acc 0.5, prec 0.0232996, recall 0.719828\n",
      "2017-12-08T00:08:03.297542: step 342, loss 4.01437, acc 0.453125, prec 0.0232445, recall 0.71828\n",
      "2017-12-08T00:08:03.898645: step 343, loss 1.71236, acc 0.5625, prec 0.0231993, recall 0.71828\n",
      "2017-12-08T00:08:04.508693: step 344, loss 2.82243, acc 0.484375, prec 0.0232139, recall 0.718884\n",
      "2017-12-08T00:08:05.115575: step 345, loss 9.35306, acc 0.515625, prec 0.0231674, recall 0.715812\n",
      "2017-12-08T00:08:05.752880: step 346, loss 2.61897, acc 0.40625, prec 0.023174, recall 0.716418\n",
      "2017-12-08T00:08:06.360137: step 347, loss 2.29256, acc 0.40625, prec 0.0231134, recall 0.716418\n",
      "2017-12-08T00:08:07.018048: step 348, loss 1.80748, acc 0.5625, prec 0.0232031, recall 0.717622\n",
      "2017-12-08T00:08:07.633012: step 349, loss 1.32208, acc 0.640625, prec 0.0231666, recall 0.717622\n",
      "2017-12-08T00:08:08.241138: step 350, loss 1.75441, acc 0.5625, prec 0.0231222, recall 0.717622\n",
      "2017-12-08T00:08:08.837881: step 351, loss 1.74948, acc 0.5625, prec 0.0231447, recall 0.71822\n",
      "2017-12-08T00:08:09.447932: step 352, loss 1.8796, acc 0.53125, prec 0.0232305, recall 0.719409\n",
      "2017-12-08T00:08:10.057747: step 353, loss 2.16189, acc 0.546875, prec 0.0233175, recall 0.720588\n",
      "2017-12-08T00:08:10.662698: step 354, loss 1.16094, acc 0.671875, prec 0.0233505, recall 0.721174\n",
      "2017-12-08T00:08:11.262943: step 355, loss 1.37984, acc 0.578125, prec 0.0233078, recall 0.721174\n",
      "2017-12-08T00:08:11.872195: step 356, loss 1.19383, acc 0.625, prec 0.023336, recall 0.721757\n",
      "2017-12-08T00:08:12.487392: step 357, loss 1.52815, acc 0.65625, prec 0.0233014, recall 0.721757\n",
      "2017-12-08T00:08:13.096359: step 358, loss 1.83046, acc 0.59375, prec 0.0233264, recall 0.722338\n",
      "2017-12-08T00:08:13.693714: step 359, loss 2.94364, acc 0.703125, prec 0.0234296, recall 0.721992\n",
      "2017-12-08T00:08:14.304262: step 360, loss 1.11072, acc 0.625, prec 0.0233918, recall 0.721992\n",
      "2017-12-08T00:08:14.911513: step 361, loss 0.582312, acc 0.78125, prec 0.0233698, recall 0.721992\n",
      "2017-12-08T00:08:15.529898: step 362, loss 4.38725, acc 0.703125, prec 0.0234071, recall 0.721074\n",
      "2017-12-08T00:08:16.114832: step 363, loss 1.74496, acc 0.5625, prec 0.0233632, recall 0.721074\n",
      "2017-12-08T00:08:16.701098: step 364, loss 1.94154, acc 0.75, prec 0.0234688, recall 0.722222\n",
      "2017-12-08T00:08:17.322336: step 365, loss 1.56703, acc 0.71875, prec 0.023571, recall 0.723361\n",
      "2017-12-08T00:08:17.914305: step 366, loss 9.93406, acc 0.65625, prec 0.0236031, recall 0.722449\n",
      "2017-12-08T00:08:18.534676: step 367, loss 1.43212, acc 0.703125, prec 0.0236383, recall 0.723014\n",
      "2017-12-08T00:08:19.146418: step 368, loss 10.9207, acc 0.734375, prec 0.0236131, recall 0.721545\n",
      "2017-12-08T00:08:19.752211: step 369, loss 1.91476, acc 0.59375, prec 0.0236372, recall 0.72211\n",
      "2017-12-08T00:08:20.364995: step 370, loss 2.0083, acc 0.609375, prec 0.0236628, recall 0.722672\n",
      "2017-12-08T00:08:20.975911: step 371, loss 1.26744, acc 0.609375, prec 0.0236236, recall 0.722672\n",
      "2017-12-08T00:08:21.573824: step 372, loss 11.0476, acc 0.703125, prec 0.0235955, recall 0.721212\n",
      "2017-12-08T00:08:22.171338: step 373, loss 1.72952, acc 0.578125, prec 0.0236179, recall 0.721774\n",
      "2017-12-08T00:08:22.774458: step 374, loss 1.97248, acc 0.609375, prec 0.023579, recall 0.721774\n",
      "2017-12-08T00:08:23.377303: step 375, loss 1.82832, acc 0.5625, prec 0.0235998, recall 0.722334\n",
      "2017-12-08T00:08:23.974050: step 376, loss 1.33146, acc 0.640625, prec 0.0235642, recall 0.722334\n",
      "2017-12-08T00:08:24.574619: step 377, loss 2.02631, acc 0.453125, prec 0.0235741, recall 0.722892\n",
      "2017-12-08T00:08:25.179139: step 378, loss 2.11425, acc 0.46875, prec 0.0235855, recall 0.723447\n",
      "2017-12-08T00:08:25.781289: step 379, loss 1.99854, acc 0.546875, prec 0.0235409, recall 0.723447\n",
      "2017-12-08T00:08:26.397476: step 380, loss 1.78015, acc 0.65625, prec 0.0235072, recall 0.723447\n",
      "2017-12-08T00:08:27.028571: step 381, loss 2.72306, acc 0.578125, prec 0.0237198, recall 0.725646\n",
      "2017-12-08T00:08:27.631862: step 382, loss 1.98351, acc 0.515625, prec 0.0237354, recall 0.72619\n",
      "2017-12-08T00:08:28.265209: step 383, loss 4.64299, acc 0.515625, prec 0.0236893, recall 0.724752\n",
      "2017-12-08T00:08:28.929592: step 384, loss 1.97602, acc 0.546875, prec 0.0236449, recall 0.724752\n",
      "2017-12-08T00:08:29.540233: step 385, loss 3.45182, acc 0.625, prec 0.0236099, recall 0.72332\n",
      "2017-12-08T00:08:30.175948: step 386, loss 5.13811, acc 0.578125, prec 0.0236961, recall 0.722986\n",
      "2017-12-08T00:08:30.784399: step 387, loss 1.10489, acc 0.75, prec 0.0237345, recall 0.723529\n",
      "2017-12-08T00:08:31.382555: step 388, loss 1.51348, acc 0.625, prec 0.0237606, recall 0.72407\n",
      "2017-12-08T00:08:31.989741: step 389, loss 1.09177, acc 0.703125, prec 0.0237316, recall 0.72407\n",
      "2017-12-08T00:08:32.593966: step 390, loss 1.71875, acc 0.546875, prec 0.0236876, recall 0.72407\n",
      "2017-12-08T00:08:33.211180: step 391, loss 1.55544, acc 0.671875, prec 0.0237182, recall 0.724609\n",
      "2017-12-08T00:08:33.823361: step 392, loss 1.47179, acc 0.609375, prec 0.0236803, recall 0.724609\n",
      "2017-12-08T00:08:34.442014: step 393, loss 1.55829, acc 0.625, prec 0.0237063, recall 0.725146\n",
      "2017-12-08T00:08:35.049999: step 394, loss 1.09419, acc 0.703125, prec 0.0236777, recall 0.725146\n",
      "2017-12-08T00:08:35.655296: step 395, loss 3.22968, acc 0.6875, prec 0.0236491, recall 0.723735\n",
      "2017-12-08T00:08:36.267933: step 396, loss 2.78109, acc 0.71875, prec 0.0237475, recall 0.723404\n",
      "2017-12-08T00:08:36.884244: step 397, loss 1.49315, acc 0.625, prec 0.0237114, recall 0.723404\n",
      "2017-12-08T00:08:37.537827: step 398, loss 12.4278, acc 0.640625, prec 0.0237417, recall 0.721154\n",
      "2017-12-08T00:08:38.144367: step 399, loss 4.76952, acc 0.578125, prec 0.0237644, recall 0.720307\n",
      "2017-12-08T00:08:38.748624: step 400, loss 5.86853, acc 0.546875, prec 0.0237224, recall 0.718929\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold2/1512709427/checkpoints/model-400\n",
      "\n",
      "2017-12-08T00:08:40.515502: step 401, loss 2.70254, acc 0.53125, prec 0.0238005, recall 0.72\n",
      "2017-12-08T00:08:41.053287: step 402, loss 2.15468, acc 0.5, prec 0.023814, recall 0.720532\n",
      "2017-12-08T00:08:41.598206: step 403, loss 2.84483, acc 0.421875, prec 0.02382, recall 0.721063\n",
      "2017-12-08T00:08:42.156159: step 404, loss 2.82437, acc 0.40625, prec 0.0237634, recall 0.721063\n",
      "2017-12-08T00:08:42.713858: step 405, loss 2.91168, acc 0.46875, prec 0.0238348, recall 0.722117\n",
      "2017-12-08T00:08:43.284048: step 406, loss 3.26016, acc 0.390625, prec 0.0238377, recall 0.722642\n",
      "2017-12-08T00:08:43.869813: step 407, loss 3.07491, acc 0.375, prec 0.0237785, recall 0.722642\n",
      "2017-12-08T00:08:44.447332: step 408, loss 3.30737, acc 0.375, prec 0.0237196, recall 0.722642\n",
      "2017-12-08T00:08:45.029960: step 409, loss 3.03365, acc 0.421875, prec 0.0236653, recall 0.722642\n",
      "2017-12-08T00:08:45.606961: step 410, loss 2.59993, acc 0.515625, prec 0.0236201, recall 0.722642\n",
      "2017-12-08T00:08:46.192552: step 411, loss 2.48983, acc 0.4375, prec 0.0236279, recall 0.723164\n",
      "2017-12-08T00:08:46.778548: step 412, loss 2.13578, acc 0.578125, prec 0.0235887, recall 0.723164\n",
      "2017-12-08T00:08:47.405099: step 413, loss 1.82001, acc 0.546875, prec 0.0235467, recall 0.723164\n",
      "2017-12-08T00:08:48.024896: step 414, loss 1.65832, acc 0.546875, prec 0.0235049, recall 0.723164\n",
      "2017-12-08T00:08:48.633011: step 415, loss 2.78662, acc 0.625, prec 0.0235898, recall 0.724203\n",
      "2017-12-08T00:08:49.240033: step 416, loss 7.01717, acc 0.71875, prec 0.0235653, recall 0.722846\n",
      "2017-12-08T00:08:49.852254: step 417, loss 1.35925, acc 0.71875, prec 0.023599, recall 0.723364\n",
      "2017-12-08T00:08:50.469596: step 418, loss 1.34267, acc 0.625, prec 0.023624, recall 0.723881\n",
      "2017-12-08T00:08:51.079429: step 419, loss 1.38556, acc 0.65625, prec 0.0236517, recall 0.724395\n",
      "2017-12-08T00:08:51.697203: step 420, loss 1.27316, acc 0.6875, prec 0.023623, recall 0.724395\n",
      "2017-12-08T00:08:52.306863: step 421, loss 1.38033, acc 0.625, prec 0.0235886, recall 0.724395\n",
      "2017-12-08T00:08:52.917327: step 422, loss 1.27591, acc 0.6875, prec 0.0235601, recall 0.724395\n",
      "2017-12-08T00:08:53.522247: step 423, loss 1.09209, acc 0.671875, prec 0.0235301, recall 0.724395\n",
      "2017-12-08T00:08:54.122538: step 424, loss 1.069, acc 0.765625, prec 0.0236268, recall 0.725417\n",
      "2017-12-08T00:08:54.724722: step 425, loss 1.29089, acc 0.6875, prec 0.0236572, recall 0.725926\n",
      "2017-12-08T00:08:55.325148: step 426, loss 0.881914, acc 0.828125, prec 0.0237004, recall 0.726433\n",
      "2017-12-08T00:08:55.937861: step 427, loss 5.14214, acc 0.734375, prec 0.0237364, recall 0.725599\n",
      "2017-12-08T00:08:56.536644: step 428, loss 1.514, acc 0.796875, prec 0.0237766, recall 0.726103\n",
      "2017-12-08T00:08:57.150960: step 429, loss 0.994514, acc 0.765625, prec 0.0237551, recall 0.726103\n",
      "2017-12-08T00:08:57.786706: step 430, loss 16.4581, acc 0.796875, prec 0.0237966, recall 0.725275\n",
      "2017-12-08T00:08:58.426208: step 431, loss 0.742746, acc 0.84375, prec 0.0237824, recall 0.725275\n",
      "2017-12-08T00:08:59.034225: step 432, loss 14.3678, acc 0.71875, prec 0.0239938, recall 0.724638\n",
      "2017-12-08T00:08:59.637366: step 433, loss 1.25591, acc 0.671875, prec 0.0239636, recall 0.724638\n",
      "2017-12-08T00:09:00.270468: step 434, loss 1.26183, acc 0.640625, prec 0.023989, recall 0.725136\n",
      "2017-12-08T00:09:00.881875: step 435, loss 1.32445, acc 0.65625, prec 0.0240741, recall 0.726126\n",
      "2017-12-08T00:09:01.482425: step 436, loss 1.58346, acc 0.640625, prec 0.0241575, recall 0.727109\n",
      "2017-12-08T00:09:02.082784: step 437, loss 1.15704, acc 0.65625, prec 0.0241839, recall 0.727599\n",
      "2017-12-08T00:09:02.691797: step 438, loss 1.9846, acc 0.46875, prec 0.0241931, recall 0.728086\n",
      "2017-12-08T00:09:03.291999: step 439, loss 3.59571, acc 0.4375, prec 0.0242007, recall 0.727273\n",
      "2017-12-08T00:09:03.912207: step 440, loss 2.42961, acc 0.484375, prec 0.0242112, recall 0.727758\n",
      "2017-12-08T00:09:04.522383: step 441, loss 2.62044, acc 0.453125, prec 0.0241612, recall 0.727758\n",
      "2017-12-08T00:09:05.131554: step 442, loss 2.22855, acc 0.515625, prec 0.024117, recall 0.727758\n",
      "2017-12-08T00:09:05.739263: step 443, loss 2.36449, acc 0.46875, prec 0.0241262, recall 0.728242\n",
      "2017-12-08T00:09:06.351016: step 444, loss 2.04662, acc 0.53125, prec 0.024141, recall 0.728723\n",
      "2017-12-08T00:09:06.955665: step 445, loss 1.871, acc 0.609375, prec 0.0241628, recall 0.729204\n",
      "2017-12-08T00:09:07.561552: step 446, loss 2.46583, acc 0.4375, prec 0.0241119, recall 0.729204\n",
      "2017-12-08T00:09:08.166729: step 447, loss 1.58856, acc 0.53125, prec 0.0241837, recall 0.730159\n",
      "2017-12-08T00:09:08.779094: step 448, loss 2.2294, acc 0.59375, prec 0.024147, recall 0.730159\n",
      "2017-12-08T00:09:09.387379: step 449, loss 2.95082, acc 0.53125, prec 0.024163, recall 0.72935\n",
      "2017-12-08T00:09:09.993023: step 450, loss 1.39868, acc 0.640625, prec 0.0241307, recall 0.72935\n",
      "2017-12-08T00:09:10.599426: step 451, loss 14.5676, acc 0.578125, prec 0.0241523, recall 0.727273\n",
      "2017-12-08T00:09:11.207992: step 452, loss 1.08969, acc 0.734375, prec 0.0241285, recall 0.727273\n",
      "2017-12-08T00:09:11.819251: step 453, loss 1.37521, acc 0.59375, prec 0.0240922, recall 0.727273\n",
      "2017-12-08T00:09:12.426961: step 454, loss 1.45694, acc 0.640625, prec 0.0240602, recall 0.727273\n",
      "2017-12-08T00:09:13.040633: step 455, loss 1.74038, acc 0.5625, prec 0.0240776, recall 0.727749\n",
      "2017-12-08T00:09:13.644577: step 456, loss 1.63216, acc 0.609375, prec 0.0240429, recall 0.727749\n",
      "2017-12-08T00:09:14.254413: step 457, loss 1.44422, acc 0.640625, prec 0.0240673, recall 0.728223\n",
      "2017-12-08T00:09:14.855865: step 458, loss 1.30277, acc 0.65625, prec 0.0240368, recall 0.728223\n",
      "2017-12-08T00:09:15.453816: step 459, loss 1.56551, acc 0.65625, prec 0.0240064, recall 0.728223\n",
      "2017-12-08T00:09:16.063376: step 460, loss 1.43312, acc 0.578125, prec 0.0239693, recall 0.728223\n",
      "2017-12-08T00:09:16.672196: step 461, loss 1.23449, acc 0.671875, prec 0.0239404, recall 0.728223\n",
      "2017-12-08T00:09:17.277347: step 462, loss 8.98358, acc 0.734375, prec 0.0239744, recall 0.727431\n",
      "2017-12-08T00:09:17.909976: step 463, loss 5.12969, acc 0.734375, prec 0.0239524, recall 0.72617\n",
      "2017-12-08T00:09:18.517888: step 464, loss 1.64385, acc 0.796875, prec 0.0239904, recall 0.726644\n",
      "2017-12-08T00:09:19.122554: step 465, loss 4.52849, acc 0.640625, prec 0.0239603, recall 0.725389\n",
      "2017-12-08T00:09:19.735473: step 466, loss 1.81979, acc 0.515625, prec 0.023918, recall 0.725389\n",
      "2017-12-08T00:09:20.342056: step 467, loss 2.66478, acc 0.625, prec 0.0240519, recall 0.726804\n",
      "2017-12-08T00:09:20.955139: step 468, loss 3.16334, acc 0.625, prec 0.0240759, recall 0.726027\n",
      "2017-12-08T00:09:21.561381: step 469, loss 1.73984, acc 0.578125, prec 0.0240943, recall 0.726496\n",
      "2017-12-08T00:09:22.168828: step 470, loss 2.25894, acc 0.59375, prec 0.0241141, recall 0.726962\n",
      "2017-12-08T00:09:22.767598: step 471, loss 3.44391, acc 0.53125, prec 0.0241297, recall 0.72619\n",
      "2017-12-08T00:09:23.372580: step 472, loss 1.37776, acc 0.578125, prec 0.024093, recall 0.72619\n",
      "2017-12-08T00:09:23.980659: step 473, loss 1.61047, acc 0.59375, prec 0.0241127, recall 0.726655\n",
      "2017-12-08T00:09:24.589238: step 474, loss 1.55312, acc 0.625, prec 0.024135, recall 0.727119\n",
      "2017-12-08T00:09:25.193593: step 475, loss 4.28211, acc 0.65625, prec 0.0242162, recall 0.726813\n",
      "2017-12-08T00:09:25.798571: step 476, loss 1.79135, acc 0.640625, prec 0.0242945, recall 0.727731\n",
      "2017-12-08T00:09:26.419356: step 477, loss 2.3638, acc 0.484375, prec 0.0242496, recall 0.727731\n",
      "2017-12-08T00:09:27.030838: step 478, loss 2.08725, acc 0.484375, prec 0.0243139, recall 0.728643\n",
      "2017-12-08T00:09:27.633440: step 479, loss 2.73472, acc 0.453125, prec 0.0243753, recall 0.729549\n",
      "2017-12-08T00:09:28.258619: step 480, loss 1.51263, acc 0.546875, prec 0.0243359, recall 0.729549\n",
      "2017-12-08T00:09:28.865239: step 481, loss 1.91731, acc 0.59375, prec 0.0244092, recall 0.730449\n",
      "2017-12-08T00:09:29.460363: step 482, loss 2.31417, acc 0.546875, prec 0.0244782, recall 0.731343\n",
      "2017-12-08T00:09:30.072313: step 483, loss 2.02937, acc 0.5625, prec 0.0244403, recall 0.731343\n",
      "2017-12-08T00:09:30.677792: step 484, loss 1.45799, acc 0.578125, prec 0.0244037, recall 0.731343\n",
      "2017-12-08T00:09:31.287866: step 485, loss 1.6522, acc 0.5625, prec 0.024366, recall 0.731343\n",
      "2017-12-08T00:09:31.890969: step 486, loss 1.03064, acc 0.734375, prec 0.024397, recall 0.731788\n",
      "2017-12-08T00:09:32.492142: step 487, loss 5.29744, acc 0.671875, prec 0.0243701, recall 0.730579\n",
      "2017-12-08T00:09:33.105249: step 488, loss 1.52686, acc 0.640625, prec 0.0243929, recall 0.731023\n",
      "2017-12-08T00:09:33.722677: step 489, loss 1.17228, acc 0.625, prec 0.0244144, recall 0.731466\n",
      "2017-12-08T00:09:34.331921: step 490, loss 12.7435, acc 0.71875, prec 0.0244452, recall 0.730706\n",
      "2017-12-08T00:09:34.946553: step 491, loss 5.39713, acc 0.671875, prec 0.0244184, recall 0.729508\n",
      "2017-12-08T00:09:35.556651: step 492, loss 7.74082, acc 0.734375, prec 0.0243969, recall 0.728314\n",
      "2017-12-08T00:09:36.163821: step 493, loss 7.55112, acc 0.53125, prec 0.0244129, recall 0.726384\n",
      "2017-12-08T00:09:36.768536: step 494, loss 1.10603, acc 0.671875, prec 0.0244382, recall 0.726829\n",
      "2017-12-08T00:09:37.374639: step 495, loss 2.2077, acc 0.515625, prec 0.0243969, recall 0.726829\n",
      "2017-12-08T00:09:38.004321: step 496, loss 3.55296, acc 0.421875, prec 0.0244022, recall 0.726094\n",
      "2017-12-08T00:09:38.530352: step 497, loss 1.95513, acc 0.5, prec 0.0244208, recall 0.726537\n",
      "Training finished\n",
      "Fold: 3 => Train/Dev split: 31796/10598\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold3/1512709779\n",
      "\n",
      "Start training\n",
      "2017-12-08T00:09:42.406761: step 1, loss 1.60671, acc 0.59375, prec 0, recall 0\n",
      "2017-12-08T00:09:42.949266: step 2, loss 14.1722, acc 0.78125, prec 0, recall 0\n",
      "2017-12-08T00:09:43.464111: step 3, loss 1.2383, acc 0.703125, prec 0, recall 0\n",
      "2017-12-08T00:09:43.975017: step 4, loss 0.727058, acc 0.8125, prec 0, recall 0\n",
      "2017-12-08T00:09:44.490738: step 5, loss 9.4976, acc 0.765625, prec 0, recall 0\n",
      "2017-12-08T00:09:45.016821: step 6, loss 0.648647, acc 0.796875, prec 0, recall 0\n",
      "2017-12-08T00:09:45.550031: step 7, loss 1.0391, acc 0.671875, prec 0, recall 0\n",
      "2017-12-08T00:09:46.107136: step 8, loss 0.450412, acc 0.84375, prec 0, recall 0\n",
      "2017-12-08T00:09:46.664526: step 9, loss 10.8084, acc 0.703125, prec 0, recall 0\n",
      "2017-12-08T00:09:47.234421: step 10, loss 0.832373, acc 0.796875, prec 0, recall 0\n",
      "2017-12-08T00:09:47.820681: step 11, loss 3.11954, acc 0.734375, prec 0, recall 0\n",
      "2017-12-08T00:09:48.458110: step 12, loss 2.1486, acc 0.71875, prec 0, recall 0\n",
      "2017-12-08T00:09:49.047355: step 13, loss 7.51349, acc 0.734375, prec 0.00956938, recall 0.222222\n",
      "2017-12-08T00:09:49.658256: step 14, loss 2.15848, acc 0.625, prec 0.00858369, recall 0.222222\n",
      "2017-12-08T00:09:50.326168: step 15, loss 12.1164, acc 0.6875, prec 0.015748, recall 0.333333\n",
      "2017-12-08T00:09:50.945073: step 16, loss 26.0589, acc 0.515625, prec 0.0141343, recall 0.285714\n",
      "2017-12-08T00:09:51.559238: step 17, loss 1.89446, acc 0.53125, prec 0.0127796, recall 0.285714\n",
      "2017-12-08T00:09:52.183146: step 18, loss 1.93254, acc 0.5, prec 0.0144509, recall 0.333333\n",
      "2017-12-08T00:09:52.788989: step 19, loss 8.23709, acc 0.390625, prec 0.0130208, recall 0.3125\n",
      "2017-12-08T00:09:53.393478: step 20, loss 5.79567, acc 0.234375, prec 0.0138568, recall 0.333333\n",
      "2017-12-08T00:09:54.006648: step 21, loss 5.97205, acc 0.15625, prec 0.0123203, recall 0.333333\n",
      "2017-12-08T00:09:54.617426: step 22, loss 5.738, acc 0.140625, prec 0.0110906, recall 0.315789\n",
      "2017-12-08T00:09:55.233929: step 23, loss 4.20526, acc 0.203125, prec 0.0101351, recall 0.315789\n",
      "2017-12-08T00:09:55.832157: step 24, loss 5.41176, acc 0.21875, prec 0.0108865, recall 0.35\n",
      "2017-12-08T00:09:56.437260: step 25, loss 6.3728, acc 0.171875, prec 0.012894, recall 0.409091\n",
      "2017-12-08T00:09:57.048300: step 26, loss 5.37126, acc 0.234375, prec 0.013369, recall 0.434783\n",
      "2017-12-08T00:09:57.658459: step 27, loss 6.1384, acc 0.125, prec 0.0124378, recall 0.434783\n",
      "2017-12-08T00:09:58.283026: step 28, loss 5.52069, acc 0.21875, prec 0.0128655, recall 0.458333\n",
      "2017-12-08T00:09:58.916409: step 29, loss 5.66182, acc 0.125, prec 0.0120746, recall 0.458333\n",
      "2017-12-08T00:09:59.547682: step 30, loss 6.63373, acc 0.28125, prec 0.0125392, recall 0.461538\n",
      "2017-12-08T00:10:00.186897: step 31, loss 6.00649, acc 0.234375, prec 0.0129096, recall 0.481481\n",
      "2017-12-08T00:10:00.820790: step 32, loss 5.39802, acc 0.25, prec 0.0132576, recall 0.5\n",
      "2017-12-08T00:10:01.454130: step 33, loss 3.90138, acc 0.359375, prec 0.0127621, recall 0.5\n",
      "2017-12-08T00:10:02.088615: step 34, loss 4.26068, acc 0.34375, prec 0.0122915, recall 0.5\n",
      "2017-12-08T00:10:02.718903: step 35, loss 3.52186, acc 0.390625, prec 0.0118845, recall 0.5\n",
      "2017-12-08T00:10:03.351918: step 36, loss 3.41598, acc 0.421875, prec 0.0123355, recall 0.517241\n",
      "2017-12-08T00:10:03.973702: step 37, loss 3.07911, acc 0.28125, prec 0.0118859, recall 0.517241\n",
      "2017-12-08T00:10:04.583768: step 38, loss 2.73127, acc 0.421875, prec 0.0115473, recall 0.517241\n",
      "2017-12-08T00:10:05.188985: step 39, loss 2.86771, acc 0.546875, prec 0.0113037, recall 0.5\n",
      "2017-12-08T00:10:05.793426: step 40, loss 6.37131, acc 0.5, prec 0.0117734, recall 0.5\n",
      "2017-12-08T00:10:06.394491: step 41, loss 8.43729, acc 0.578125, prec 0.0115523, recall 0.484848\n",
      "2017-12-08T00:10:06.995218: step 42, loss 2.55083, acc 0.578125, prec 0.0127298, recall 0.514286\n",
      "2017-12-08T00:10:07.609047: step 43, loss 4.84638, acc 0.5625, prec 0.0124913, recall 0.5\n",
      "2017-12-08T00:10:08.222947: step 44, loss 1.17442, acc 0.671875, prec 0.0123119, recall 0.5\n",
      "2017-12-08T00:10:08.846013: step 45, loss 0.908309, acc 0.734375, prec 0.0121704, recall 0.5\n",
      "2017-12-08T00:10:09.456690: step 46, loss 9.83848, acc 0.65625, prec 0.012, recall 0.486486\n",
      "2017-12-08T00:10:10.074981: step 47, loss 2.65568, acc 0.5, prec 0.0130378, recall 0.512821\n",
      "2017-12-08T00:10:10.682436: step 48, loss 7.81353, acc 0.59375, prec 0.0141026, recall 0.511628\n",
      "2017-12-08T00:10:11.283313: step 49, loss 7.5186, acc 0.640625, prec 0.0145294, recall 0.511111\n",
      "2017-12-08T00:10:11.893223: step 50, loss 1.9541, acc 0.5625, prec 0.0142768, recall 0.511111\n",
      "2017-12-08T00:10:12.505244: step 51, loss 2.42763, acc 0.515625, prec 0.0146074, recall 0.521739\n",
      "2017-12-08T00:10:13.108240: step 52, loss 2.60125, acc 0.46875, prec 0.0154854, recall 0.541667\n",
      "2017-12-08T00:10:13.709435: step 53, loss 2.58018, acc 0.40625, prec 0.0151427, recall 0.541667\n",
      "2017-12-08T00:10:14.311143: step 54, loss 13.7719, acc 0.390625, prec 0.0153759, recall 0.54\n",
      "2017-12-08T00:10:14.920517: step 55, loss 6.97888, acc 0.375, prec 0.0150418, recall 0.529412\n",
      "2017-12-08T00:10:15.528603: step 56, loss 3.57437, acc 0.265625, prec 0.014658, recall 0.529412\n",
      "2017-12-08T00:10:16.135691: step 57, loss 3.83034, acc 0.234375, prec 0.0147992, recall 0.538462\n",
      "2017-12-08T00:10:16.742897: step 58, loss 6.84158, acc 0.3125, prec 0.0144703, recall 0.528302\n",
      "2017-12-08T00:10:17.361486: step 59, loss 3.92851, acc 0.28125, prec 0.0151286, recall 0.545455\n",
      "2017-12-08T00:10:17.969842: step 60, loss 3.85141, acc 0.296875, prec 0.0157635, recall 0.561404\n",
      "2017-12-08T00:10:18.608192: step 61, loss 5.09595, acc 0.265625, prec 0.016354, recall 0.576271\n",
      "2017-12-08T00:10:19.219325: step 62, loss 6.84088, acc 0.3125, prec 0.0160226, recall 0.566667\n",
      "2017-12-08T00:10:19.826972: step 63, loss 6.11112, acc 0.265625, prec 0.0156827, recall 0.557377\n",
      "2017-12-08T00:10:20.433216: step 64, loss 4.12293, acc 0.3125, prec 0.0153707, recall 0.557377\n",
      "2017-12-08T00:10:21.039414: step 65, loss 4.54704, acc 0.234375, prec 0.015473, recall 0.564516\n",
      "2017-12-08T00:10:21.641893: step 66, loss 4.41821, acc 0.328125, prec 0.0151844, recall 0.564516\n",
      "2017-12-08T00:10:22.247006: step 67, loss 3.56494, acc 0.3125, prec 0.0153191, recall 0.571429\n",
      "2017-12-08T00:10:22.857297: step 68, loss 5.6462, acc 0.296875, prec 0.0150376, recall 0.5625\n",
      "2017-12-08T00:10:23.467542: step 69, loss 3.12318, acc 0.375, prec 0.0147905, recall 0.5625\n",
      "2017-12-08T00:10:24.077849: step 70, loss 2.39595, acc 0.421875, prec 0.014569, recall 0.5625\n",
      "2017-12-08T00:10:24.684486: step 71, loss 3.40916, acc 0.390625, prec 0.0143426, recall 0.5625\n",
      "2017-12-08T00:10:25.283135: step 72, loss 2.44542, acc 0.453125, prec 0.0145326, recall 0.569231\n",
      "2017-12-08T00:10:25.891221: step 73, loss 3.08194, acc 0.5, prec 0.0151163, recall 0.58209\n",
      "2017-12-08T00:10:26.496094: step 74, loss 2.38252, acc 0.46875, prec 0.0156728, recall 0.594203\n",
      "2017-12-08T00:10:27.104531: step 75, loss 5.84647, acc 0.515625, prec 0.0162387, recall 0.597222\n",
      "2017-12-08T00:10:27.706863: step 76, loss 1.46736, acc 0.640625, prec 0.0164671, recall 0.60274\n",
      "2017-12-08T00:10:28.310620: step 77, loss 1.56843, acc 0.546875, prec 0.0166543, recall 0.608108\n",
      "2017-12-08T00:10:28.966575: step 78, loss 8.00717, acc 0.5, prec 0.0164654, recall 0.6\n",
      "2017-12-08T00:10:29.601968: step 79, loss 24.5349, acc 0.625, prec 0.0163339, recall 0.584416\n",
      "2017-12-08T00:10:30.246835: step 80, loss 2.79707, acc 0.609375, prec 0.0165408, recall 0.589744\n",
      "2017-12-08T00:10:30.872474: step 81, loss 1.60807, acc 0.59375, prec 0.0163876, recall 0.589744\n",
      "2017-12-08T00:10:31.499639: step 82, loss 9.642, acc 0.390625, prec 0.0161687, recall 0.582278\n",
      "2017-12-08T00:10:32.125358: step 83, loss 2.02276, acc 0.5625, prec 0.0166957, recall 0.592593\n",
      "2017-12-08T00:10:32.747042: step 84, loss 2.75476, acc 0.46875, prec 0.0165005, recall 0.592593\n",
      "2017-12-08T00:10:33.380296: step 85, loss 6.90777, acc 0.421875, prec 0.0162988, recall 0.585366\n",
      "2017-12-08T00:10:33.990986: step 86, loss 7.55989, acc 0.375, prec 0.0160858, recall 0.578313\n",
      "2017-12-08T00:10:34.598177: step 87, loss 3.86841, acc 0.484375, prec 0.0159151, recall 0.571429\n",
      "2017-12-08T00:10:35.206378: step 88, loss 3.19503, acc 0.375, prec 0.0157068, recall 0.571429\n",
      "2017-12-08T00:10:35.810615: step 89, loss 3.68236, acc 0.3125, prec 0.0154839, recall 0.571429\n",
      "2017-12-08T00:10:36.419600: step 90, loss 4.1232, acc 0.34375, prec 0.0155902, recall 0.576471\n",
      "2017-12-08T00:10:37.020757: step 91, loss 4.74299, acc 0.1875, prec 0.0153365, recall 0.576471\n",
      "2017-12-08T00:10:37.641248: step 92, loss 4.14997, acc 0.359375, prec 0.0154464, recall 0.581395\n",
      "2017-12-08T00:10:38.268091: step 93, loss 3.37962, acc 0.46875, prec 0.0158876, recall 0.590909\n",
      "2017-12-08T00:10:38.921412: step 94, loss 2.82258, acc 0.40625, prec 0.0160024, recall 0.595506\n",
      "2017-12-08T00:10:39.534448: step 95, loss 7.35912, acc 0.25, prec 0.0160714, recall 0.593407\n",
      "2017-12-08T00:10:40.147305: step 96, loss 3.49661, acc 0.359375, prec 0.016167, recall 0.597826\n",
      "2017-12-08T00:10:40.750254: step 97, loss 4.36853, acc 0.328125, prec 0.0159698, recall 0.591398\n",
      "2017-12-08T00:10:41.358118: step 98, loss 3.63538, acc 0.34375, prec 0.0160597, recall 0.595745\n",
      "2017-12-08T00:10:41.960593: step 99, loss 2.97165, acc 0.34375, prec 0.0158685, recall 0.595745\n",
      "2017-12-08T00:10:42.568416: step 100, loss 4.59719, acc 0.46875, prec 0.0157215, recall 0.589474\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold3/1512709779/checkpoints/model-100\n",
      "\n",
      "2017-12-08T00:10:44.046971: step 101, loss 2.35103, acc 0.53125, prec 0.0155902, recall 0.589474\n",
      "2017-12-08T00:10:44.651977: step 102, loss 2.32498, acc 0.484375, prec 0.0154483, recall 0.589474\n",
      "2017-12-08T00:10:45.241249: step 103, loss 8.44789, acc 0.375, prec 0.0152838, recall 0.583333\n",
      "2017-12-08T00:10:45.821454: step 104, loss 8.99201, acc 0.421875, prec 0.0151351, recall 0.57732\n",
      "2017-12-08T00:10:46.415283: step 105, loss 14.1906, acc 0.484375, prec 0.0152692, recall 0.575758\n",
      "2017-12-08T00:10:47.007080: step 106, loss 3.41442, acc 0.5625, prec 0.0151596, recall 0.57\n",
      "2017-12-08T00:10:47.589702: step 107, loss 2.52793, acc 0.421875, prec 0.0152712, recall 0.574257\n",
      "2017-12-08T00:10:48.170874: step 108, loss 2.26563, acc 0.390625, prec 0.015116, recall 0.574257\n",
      "2017-12-08T00:10:48.769659: step 109, loss 2.04168, acc 0.515625, prec 0.0149948, recall 0.574257\n",
      "2017-12-08T00:10:49.398980: step 110, loss 2.98331, acc 0.421875, prec 0.0153571, recall 0.582524\n",
      "2017-12-08T00:10:50.006684: step 111, loss 1.94047, acc 0.546875, prec 0.0152439, recall 0.582524\n",
      "2017-12-08T00:10:50.611033: step 112, loss 2.05368, acc 0.4375, prec 0.0153536, recall 0.586538\n",
      "2017-12-08T00:10:51.216581: step 113, loss 2.46124, acc 0.46875, prec 0.0154691, recall 0.590476\n",
      "2017-12-08T00:10:51.829544: step 114, loss 7.24572, acc 0.40625, prec 0.0153314, recall 0.579439\n",
      "2017-12-08T00:10:52.447755: step 115, loss 2.50198, acc 0.5, prec 0.015211, recall 0.579439\n",
      "2017-12-08T00:10:53.063539: step 116, loss 2.4257, acc 0.484375, prec 0.0150888, recall 0.579439\n",
      "2017-12-08T00:10:53.670925: step 117, loss 2.44458, acc 0.40625, prec 0.0149506, recall 0.579439\n",
      "2017-12-08T00:10:54.278475: step 118, loss 2.99268, acc 0.5625, prec 0.015322, recall 0.587156\n",
      "2017-12-08T00:10:54.888087: step 119, loss 2.17876, acc 0.4375, prec 0.0151911, recall 0.587156\n",
      "2017-12-08T00:10:55.505509: step 120, loss 3.25, acc 0.53125, prec 0.0155514, recall 0.589286\n",
      "2017-12-08T00:10:56.130580: step 121, loss 2.02592, acc 0.546875, prec 0.0159064, recall 0.596491\n",
      "2017-12-08T00:10:56.744186: step 122, loss 1.80099, acc 0.5625, prec 0.0158029, recall 0.596491\n",
      "2017-12-08T00:10:57.362196: step 123, loss 2.71625, acc 0.625, prec 0.0157189, recall 0.591304\n",
      "2017-12-08T00:10:57.967193: step 124, loss 1.65369, acc 0.546875, prec 0.0156142, recall 0.591304\n",
      "2017-12-08T00:10:58.623782: step 125, loss 1.54729, acc 0.53125, prec 0.0157319, recall 0.594828\n",
      "2017-12-08T00:10:59.279915: step 126, loss 1.52146, acc 0.578125, prec 0.0156356, recall 0.594828\n",
      "2017-12-08T00:10:59.895316: step 127, loss 1.28516, acc 0.640625, prec 0.0155546, recall 0.594828\n",
      "2017-12-08T00:11:00.514376: step 128, loss 6.33587, acc 0.625, prec 0.0156951, recall 0.59322\n",
      "2017-12-08T00:11:01.135046: step 129, loss 3.89207, acc 0.75, prec 0.0156425, recall 0.588235\n",
      "2017-12-08T00:11:01.739557: step 130, loss 5.95815, acc 0.625, prec 0.0157813, recall 0.586777\n",
      "2017-12-08T00:11:02.337490: step 131, loss 1.58838, acc 0.5625, prec 0.0156837, recall 0.586777\n",
      "2017-12-08T00:11:02.939391: step 132, loss 1.16941, acc 0.671875, prec 0.0156113, recall 0.586777\n",
      "2017-12-08T00:11:03.545754: step 133, loss 1.53978, acc 0.609375, prec 0.0157411, recall 0.590164\n",
      "2017-12-08T00:11:04.154143: step 134, loss 1.83495, acc 0.671875, prec 0.0156692, recall 0.590164\n",
      "2017-12-08T00:11:04.757504: step 135, loss 1.98338, acc 0.578125, prec 0.0157906, recall 0.593496\n",
      "2017-12-08T00:11:05.368565: step 136, loss 1.41407, acc 0.609375, prec 0.0157057, recall 0.593496\n",
      "2017-12-08T00:11:05.992303: step 137, loss 1.8527, acc 0.59375, prec 0.0156183, recall 0.593496\n",
      "2017-12-08T00:11:06.596400: step 138, loss 24.12, acc 0.65625, prec 0.0155518, recall 0.584\n",
      "2017-12-08T00:11:07.206479: step 139, loss 1.4084, acc 0.734375, prec 0.0157046, recall 0.587302\n",
      "2017-12-08T00:11:07.821669: step 140, loss 1.97017, acc 0.546875, prec 0.0158161, recall 0.590551\n",
      "2017-12-08T00:11:08.425596: step 141, loss 1.63014, acc 0.625, prec 0.0159429, recall 0.59375\n",
      "2017-12-08T00:11:09.053285: step 142, loss 2.53959, acc 0.453125, prec 0.0160316, recall 0.596899\n",
      "2017-12-08T00:11:09.657734: step 143, loss 1.48904, acc 0.609375, prec 0.0161524, recall 0.6\n",
      "2017-12-08T00:11:10.271447: step 144, loss 2.04568, acc 0.546875, prec 0.0164609, recall 0.606061\n",
      "2017-12-08T00:11:10.882140: step 145, loss 6.61894, acc 0.625, prec 0.0165848, recall 0.604478\n",
      "2017-12-08T00:11:11.490908: step 146, loss 3.09146, acc 0.578125, prec 0.0166972, recall 0.602941\n",
      "2017-12-08T00:11:12.096200: step 147, loss 2.61534, acc 0.40625, prec 0.016569, recall 0.602941\n",
      "2017-12-08T00:11:12.709516: step 148, loss 2.31045, acc 0.484375, prec 0.0166566, recall 0.605839\n",
      "2017-12-08T00:11:13.321024: step 149, loss 2.1867, acc 0.5, prec 0.0167464, recall 0.608696\n",
      "2017-12-08T00:11:13.932531: step 150, loss 4.08783, acc 0.453125, prec 0.017023, recall 0.609929\n",
      "2017-12-08T00:11:14.527496: step 151, loss 10.3389, acc 0.5, prec 0.0171125, recall 0.608392\n",
      "2017-12-08T00:11:15.138537: step 152, loss 2.24397, acc 0.40625, prec 0.0169856, recall 0.608392\n",
      "2017-12-08T00:11:15.749144: step 153, loss 16.2344, acc 0.4375, prec 0.0168703, recall 0.604167\n",
      "2017-12-08T00:11:16.363825: step 154, loss 6.56327, acc 0.5, prec 0.0173377, recall 0.608108\n",
      "2017-12-08T00:11:16.979003: step 155, loss 2.34432, acc 0.5, prec 0.0172315, recall 0.608108\n",
      "2017-12-08T00:11:17.589105: step 156, loss 3.54606, acc 0.265625, prec 0.0172643, recall 0.610738\n",
      "2017-12-08T00:11:18.195608: step 157, loss 6.87084, acc 0.359375, prec 0.0173193, recall 0.609272\n",
      "2017-12-08T00:11:18.805503: step 158, loss 2.43623, acc 0.5, prec 0.0173994, recall 0.611842\n",
      "2017-12-08T00:11:19.471753: step 159, loss 3.65171, acc 0.40625, prec 0.0176416, recall 0.616883\n",
      "2017-12-08T00:11:20.093268: step 160, loss 3.3552, acc 0.3125, prec 0.0176796, recall 0.619355\n",
      "2017-12-08T00:11:20.713351: step 161, loss 3.32214, acc 0.359375, prec 0.0179061, recall 0.624204\n",
      "2017-12-08T00:11:21.369644: step 162, loss 6.37862, acc 0.328125, prec 0.0179478, recall 0.622642\n",
      "2017-12-08T00:11:21.997368: step 163, loss 3.14257, acc 0.359375, prec 0.0181687, recall 0.627329\n",
      "2017-12-08T00:11:22.601954: step 164, loss 3.09269, acc 0.46875, prec 0.0182338, recall 0.62963\n",
      "2017-12-08T00:11:23.206333: step 165, loss 3.02478, acc 0.28125, prec 0.0180851, recall 0.62963\n",
      "2017-12-08T00:11:23.819098: step 166, loss 3.08218, acc 0.421875, prec 0.0181402, recall 0.631902\n",
      "2017-12-08T00:11:24.430099: step 167, loss 2.60608, acc 0.390625, prec 0.0180164, recall 0.631902\n",
      "2017-12-08T00:11:25.034466: step 168, loss 9.76742, acc 0.46875, prec 0.017913, recall 0.628049\n",
      "2017-12-08T00:11:25.644251: step 169, loss 2.36581, acc 0.515625, prec 0.0179869, recall 0.630303\n",
      "2017-12-08T00:11:26.248158: step 170, loss 2.72196, acc 0.375, prec 0.0178633, recall 0.630303\n",
      "2017-12-08T00:11:26.854934: step 171, loss 19.0426, acc 0.46875, prec 0.0177656, recall 0.622755\n",
      "2017-12-08T00:11:27.464361: step 172, loss 4.03045, acc 0.5625, prec 0.0176841, recall 0.619048\n",
      "2017-12-08T00:11:28.079560: step 173, loss 2.4302, acc 0.453125, prec 0.0180774, recall 0.625731\n",
      "2017-12-08T00:11:28.686038: step 174, loss 1.94928, acc 0.5, prec 0.0181452, recall 0.627907\n",
      "2017-12-08T00:11:29.308570: step 175, loss 2.07466, acc 0.515625, prec 0.0182152, recall 0.630058\n",
      "2017-12-08T00:11:29.926614: step 176, loss 4.46862, acc 0.34375, prec 0.0180913, recall 0.626437\n",
      "2017-12-08T00:11:30.537089: step 177, loss 2.60918, acc 0.4375, prec 0.0181458, recall 0.628571\n",
      "2017-12-08T00:11:31.136830: step 178, loss 3.17762, acc 0.5, prec 0.0180505, recall 0.628571\n",
      "2017-12-08T00:11:31.756386: step 179, loss 2.17704, acc 0.40625, prec 0.0179387, recall 0.628571\n",
      "2017-12-08T00:11:32.367383: step 180, loss 2.08378, acc 0.484375, prec 0.0178427, recall 0.628571\n",
      "2017-12-08T00:11:32.969137: step 181, loss 2.25658, acc 0.515625, prec 0.0177534, recall 0.628571\n",
      "2017-12-08T00:11:33.572225: step 182, loss 1.69285, acc 0.640625, prec 0.0176877, recall 0.628571\n",
      "2017-12-08T00:11:34.178676: step 183, loss 10.2241, acc 0.4375, prec 0.0175887, recall 0.625\n",
      "2017-12-08T00:11:34.787648: step 184, loss 1.79972, acc 0.53125, prec 0.0175048, recall 0.625\n",
      "2017-12-08T00:11:35.385358: step 185, loss 1.73577, acc 0.640625, prec 0.0175967, recall 0.627119\n",
      "2017-12-08T00:11:35.991307: step 186, loss 5.90857, acc 0.625, prec 0.0175328, recall 0.623595\n",
      "2017-12-08T00:11:36.594931: step 187, loss 1.37791, acc 0.6875, prec 0.0174776, recall 0.623595\n",
      "2017-12-08T00:11:37.197887: step 188, loss 2.00862, acc 0.5625, prec 0.0174008, recall 0.623595\n",
      "2017-12-08T00:11:37.798970: step 189, loss 14.6959, acc 0.65625, prec 0.0174973, recall 0.622222\n",
      "2017-12-08T00:11:38.396146: step 190, loss 1.87329, acc 0.546875, prec 0.0174184, recall 0.622222\n",
      "2017-12-08T00:11:39.001039: step 191, loss 2.16173, acc 0.515625, prec 0.0173348, recall 0.622222\n",
      "2017-12-08T00:11:39.651554: step 192, loss 0.955061, acc 0.65625, prec 0.017276, recall 0.622222\n",
      "2017-12-08T00:11:40.266239: step 193, loss 1.41437, acc 0.640625, prec 0.0173659, recall 0.624309\n",
      "2017-12-08T00:11:40.875595: step 194, loss 6.51638, acc 0.578125, prec 0.0175976, recall 0.625\n",
      "2017-12-08T00:11:41.487083: step 195, loss 1.42114, acc 0.59375, prec 0.0175278, recall 0.625\n",
      "2017-12-08T00:11:42.093786: step 196, loss 1.11716, acc 0.6875, prec 0.0174745, recall 0.625\n",
      "2017-12-08T00:11:42.698839: step 197, loss 1.21771, acc 0.609375, prec 0.0174084, recall 0.625\n",
      "2017-12-08T00:11:43.319222: step 198, loss 1.76683, acc 0.53125, prec 0.0173297, recall 0.625\n",
      "2017-12-08T00:11:43.921788: step 199, loss 1.71232, acc 0.578125, prec 0.017407, recall 0.627027\n",
      "2017-12-08T00:11:44.532464: step 200, loss 0.998689, acc 0.734375, prec 0.0173627, recall 0.627027\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold3/1512709779/checkpoints/model-200\n",
      "\n",
      "2017-12-08T00:11:46.975825: step 201, loss 14.0639, acc 0.59375, prec 0.0175909, recall 0.62766\n",
      "2017-12-08T00:11:47.542635: step 202, loss 1.04969, acc 0.71875, prec 0.0175439, recall 0.62766\n",
      "2017-12-08T00:11:48.085351: step 203, loss 1.17669, acc 0.734375, prec 0.0174996, recall 0.62766\n",
      "2017-12-08T00:11:48.617247: step 204, loss 7.45449, acc 0.609375, prec 0.0175879, recall 0.619792\n",
      "2017-12-08T00:11:49.168556: step 205, loss 2.5974, acc 0.671875, prec 0.0178229, recall 0.623711\n",
      "2017-12-08T00:11:49.762753: step 206, loss 1.28102, acc 0.59375, prec 0.017755, recall 0.623711\n",
      "2017-12-08T00:11:50.331121: step 207, loss 1.22567, acc 0.671875, prec 0.0177004, recall 0.623711\n",
      "2017-12-08T00:11:50.901621: step 208, loss 1.98973, acc 0.53125, prec 0.0176231, recall 0.623711\n",
      "2017-12-08T00:11:51.479149: step 209, loss 6.83144, acc 0.546875, prec 0.017694, recall 0.622449\n",
      "2017-12-08T00:11:52.088370: step 210, loss 1.64204, acc 0.484375, prec 0.0177515, recall 0.624366\n",
      "2017-12-08T00:11:52.681535: step 211, loss 4.64629, acc 0.46875, prec 0.0179495, recall 0.625\n",
      "2017-12-08T00:11:53.265625: step 212, loss 2.19649, acc 0.5, prec 0.0178674, recall 0.625\n",
      "2017-12-08T00:11:53.848530: step 213, loss 2.7071, acc 0.484375, prec 0.0179232, recall 0.626866\n",
      "2017-12-08T00:11:54.456533: step 214, loss 2.98513, acc 0.375, prec 0.0178218, recall 0.626866\n",
      "2017-12-08T00:11:55.066867: step 215, loss 6.31518, acc 0.40625, prec 0.0178672, recall 0.625616\n",
      "2017-12-08T00:11:55.675679: step 216, loss 3.67101, acc 0.359375, prec 0.0177647, recall 0.625616\n",
      "2017-12-08T00:11:56.280933: step 217, loss 3.1075, acc 0.421875, prec 0.0178099, recall 0.627451\n",
      "2017-12-08T00:11:56.892988: step 218, loss 3.2874, acc 0.375, prec 0.0177114, recall 0.627451\n",
      "2017-12-08T00:11:57.506302: step 219, loss 3.32681, acc 0.40625, prec 0.0178891, recall 0.631068\n",
      "2017-12-08T00:11:58.110640: step 220, loss 2.32718, acc 0.515625, prec 0.0178131, recall 0.631068\n",
      "2017-12-08T00:11:58.711995: step 221, loss 3.53769, acc 0.546875, prec 0.0178791, recall 0.629808\n",
      "2017-12-08T00:11:59.319573: step 222, loss 2.37836, acc 0.4375, prec 0.0177917, recall 0.629808\n",
      "2017-12-08T00:11:59.987560: step 223, loss 2.23959, acc 0.546875, prec 0.0178547, recall 0.631579\n",
      "2017-12-08T00:12:00.617288: step 224, loss 1.36476, acc 0.6875, prec 0.0180715, recall 0.635071\n",
      "2017-12-08T00:12:01.228201: step 225, loss 1.70276, acc 0.59375, prec 0.0180083, recall 0.635071\n",
      "2017-12-08T00:12:01.833579: step 226, loss 4.73545, acc 0.59375, prec 0.0180796, recall 0.633803\n",
      "2017-12-08T00:12:02.441950: step 227, loss 6.6703, acc 0.578125, prec 0.0181479, recall 0.632558\n",
      "2017-12-08T00:12:03.055000: step 228, loss 1.67811, acc 0.609375, prec 0.0182181, recall 0.634259\n",
      "2017-12-08T00:12:03.679651: step 229, loss 7.8239, acc 0.4375, prec 0.0181337, recall 0.631336\n",
      "2017-12-08T00:12:04.287758: step 230, loss 1.86197, acc 0.578125, prec 0.0180691, recall 0.631336\n",
      "2017-12-08T00:12:04.897513: step 231, loss 2.20743, acc 0.609375, prec 0.0182678, recall 0.634703\n",
      "2017-12-08T00:12:05.499305: step 232, loss 1.67019, acc 0.5625, prec 0.0182009, recall 0.634703\n",
      "2017-12-08T00:12:06.099617: step 233, loss 1.90356, acc 0.640625, prec 0.0184025, recall 0.638009\n",
      "2017-12-08T00:12:06.710953: step 234, loss 2.11568, acc 0.515625, prec 0.0183284, recall 0.638009\n",
      "2017-12-08T00:12:07.318930: step 235, loss 0.992799, acc 0.6875, prec 0.0182808, recall 0.638009\n",
      "2017-12-08T00:12:07.921661: step 236, loss 6.9697, acc 0.640625, prec 0.0183557, recall 0.636771\n",
      "2017-12-08T00:12:08.530965: step 237, loss 2.02435, acc 0.59375, prec 0.0184207, recall 0.638393\n",
      "2017-12-08T00:12:09.135657: step 238, loss 2.01585, acc 0.65625, prec 0.0184947, recall 0.64\n",
      "2017-12-08T00:12:09.766558: step 239, loss 1.28036, acc 0.703125, prec 0.0187012, recall 0.643172\n",
      "2017-12-08T00:12:10.381903: step 240, loss 2.4603, acc 0.671875, prec 0.0187763, recall 0.644737\n",
      "2017-12-08T00:12:10.994170: step 241, loss 1.69657, acc 0.5625, prec 0.0189592, recall 0.647826\n",
      "2017-12-08T00:12:11.606204: step 242, loss 2.00631, acc 0.59375, prec 0.0191454, recall 0.650862\n",
      "2017-12-08T00:12:12.216135: step 243, loss 1.65668, acc 0.609375, prec 0.0190849, recall 0.650862\n",
      "2017-12-08T00:12:12.832586: step 244, loss 1.17844, acc 0.671875, prec 0.0191581, recall 0.65236\n",
      "2017-12-08T00:12:13.440494: step 245, loss 1.09969, acc 0.640625, prec 0.0191027, recall 0.65236\n",
      "2017-12-08T00:12:14.052309: step 246, loss 0.832895, acc 0.75, prec 0.0190643, recall 0.65236\n",
      "2017-12-08T00:12:14.655622: step 247, loss 14.668, acc 0.671875, prec 0.0191394, recall 0.651064\n",
      "2017-12-08T00:12:15.262938: step 248, loss 1.88723, acc 0.546875, prec 0.0190702, recall 0.651064\n",
      "2017-12-08T00:12:15.866156: step 249, loss 1.16474, acc 0.734375, prec 0.0190299, recall 0.651064\n",
      "2017-12-08T00:12:16.475194: step 250, loss 5.44803, acc 0.703125, prec 0.0192308, recall 0.65126\n",
      "2017-12-08T00:12:17.089773: step 251, loss 5.06577, acc 0.578125, prec 0.0191689, recall 0.648536\n",
      "2017-12-08T00:12:17.890803: step 252, loss 11.1055, acc 0.65625, prec 0.0193612, recall 0.64876\n",
      "2017-12-08T00:12:18.492761: step 253, loss 2.6015, acc 0.46875, prec 0.0192804, recall 0.64876\n",
      "2017-12-08T00:12:19.085261: step 254, loss 4.64267, acc 0.5, prec 0.0193272, recall 0.647541\n",
      "2017-12-08T00:12:19.673104: step 255, loss 2.31972, acc 0.46875, prec 0.0193666, recall 0.64898\n",
      "2017-12-08T00:12:20.275826: step 256, loss 3.1854, acc 0.5, prec 0.0194104, recall 0.650406\n",
      "2017-12-08T00:12:20.851514: step 257, loss 7.74561, acc 0.34375, prec 0.0193143, recall 0.647773\n",
      "2017-12-08T00:12:21.433342: step 258, loss 3.0232, acc 0.421875, prec 0.0193463, recall 0.649194\n",
      "2017-12-08T00:12:21.992041: step 259, loss 3.32513, acc 0.453125, prec 0.0194999, recall 0.652\n",
      "2017-12-08T00:12:22.572955: step 260, loss 3.37451, acc 0.421875, prec 0.0195308, recall 0.653386\n",
      "2017-12-08T00:12:23.128189: step 261, loss 3.16612, acc 0.484375, prec 0.0194543, recall 0.653386\n",
      "2017-12-08T00:12:23.689760: step 262, loss 3.21576, acc 0.375, prec 0.019594, recall 0.656126\n",
      "2017-12-08T00:12:24.257265: step 263, loss 3.0827, acc 0.40625, prec 0.0195065, recall 0.656126\n",
      "2017-12-08T00:12:24.815974: step 264, loss 2.86382, acc 0.375, prec 0.0194152, recall 0.656126\n",
      "2017-12-08T00:12:25.368988: step 265, loss 2.55739, acc 0.5, prec 0.0193428, recall 0.656126\n",
      "2017-12-08T00:12:25.953505: step 266, loss 3.45619, acc 0.375, prec 0.0192531, recall 0.656126\n",
      "2017-12-08T00:12:26.507184: step 267, loss 5.23774, acc 0.375, prec 0.0191664, recall 0.653543\n",
      "2017-12-08T00:12:27.080879: step 268, loss 2.54921, acc 0.5, prec 0.0190958, recall 0.653543\n",
      "2017-12-08T00:12:27.642379: step 269, loss 2.31862, acc 0.515625, prec 0.019028, recall 0.653543\n",
      "2017-12-08T00:12:28.196454: step 270, loss 1.96303, acc 0.578125, prec 0.0190814, recall 0.654902\n",
      "2017-12-08T00:12:28.751706: step 271, loss 1.89204, acc 0.59375, prec 0.0192483, recall 0.657588\n",
      "2017-12-08T00:12:29.313867: step 272, loss 2.89413, acc 0.59375, prec 0.0194142, recall 0.660232\n",
      "2017-12-08T00:12:29.895112: step 273, loss 2.25426, acc 0.578125, prec 0.0194658, recall 0.661538\n",
      "2017-12-08T00:12:30.490512: step 274, loss 1.57014, acc 0.625, prec 0.0195238, recall 0.662835\n",
      "2017-12-08T00:12:31.070726: step 275, loss 4.96455, acc 0.703125, prec 0.0195946, recall 0.661597\n",
      "2017-12-08T00:12:31.645055: step 276, loss 0.908646, acc 0.71875, prec 0.019555, recall 0.661597\n",
      "2017-12-08T00:12:32.222898: step 277, loss 1.52861, acc 0.671875, prec 0.0196188, recall 0.662879\n",
      "2017-12-08T00:12:32.821976: step 278, loss 1.90208, acc 0.71875, prec 0.0197987, recall 0.665414\n",
      "2017-12-08T00:12:33.403450: step 279, loss 1.14554, acc 0.71875, prec 0.0197589, recall 0.665414\n",
      "2017-12-08T00:12:33.983787: step 280, loss 0.902156, acc 0.75, prec 0.0197236, recall 0.665414\n",
      "2017-12-08T00:12:34.564359: step 281, loss 1.13625, acc 0.640625, prec 0.0197822, recall 0.666667\n",
      "2017-12-08T00:12:35.142239: step 282, loss 9.48323, acc 0.703125, prec 0.0197427, recall 0.664179\n",
      "2017-12-08T00:12:35.758512: step 283, loss 0.877094, acc 0.8125, prec 0.0197164, recall 0.664179\n",
      "2017-12-08T00:12:36.327708: step 284, loss 14.1098, acc 0.796875, prec 0.0196924, recall 0.659259\n",
      "2017-12-08T00:12:36.894241: step 285, loss 1.58194, acc 0.640625, prec 0.0196425, recall 0.659259\n",
      "2017-12-08T00:12:37.444836: step 286, loss 1.59471, acc 0.59375, prec 0.0196941, recall 0.660517\n",
      "2017-12-08T00:12:38.013107: step 287, loss 1.51408, acc 0.703125, prec 0.0196531, recall 0.660517\n",
      "2017-12-08T00:12:38.576100: step 288, loss 1.23272, acc 0.6875, prec 0.01961, recall 0.660517\n",
      "2017-12-08T00:12:39.138852: step 289, loss 0.861071, acc 0.765625, prec 0.019685, recall 0.661765\n",
      "2017-12-08T00:12:39.711414: step 290, loss 6.47353, acc 0.671875, prec 0.019856, recall 0.661818\n",
      "2017-12-08T00:12:40.298942: step 291, loss 2.52999, acc 0.5625, prec 0.0197977, recall 0.65942\n",
      "2017-12-08T00:12:40.852246: step 292, loss 1.52177, acc 0.65625, prec 0.0198568, recall 0.66065\n",
      "2017-12-08T00:12:41.417074: step 293, loss 2.30727, acc 0.515625, prec 0.0198962, recall 0.66187\n",
      "2017-12-08T00:12:41.975756: step 294, loss 9.68607, acc 0.609375, prec 0.0198447, recall 0.659498\n",
      "2017-12-08T00:12:42.538010: step 295, loss 2.41333, acc 0.5, prec 0.0199871, recall 0.661922\n",
      "2017-12-08T00:12:43.097268: step 296, loss 1.98062, acc 0.53125, prec 0.0201328, recall 0.664311\n",
      "2017-12-08T00:12:43.649221: step 297, loss 3.09292, acc 0.453125, prec 0.0201622, recall 0.665493\n",
      "2017-12-08T00:12:44.209708: step 298, loss 1.93939, acc 0.5625, prec 0.0202063, recall 0.666667\n",
      "2017-12-08T00:12:44.772529: step 299, loss 2.72374, acc 0.46875, prec 0.0201335, recall 0.666667\n",
      "2017-12-08T00:12:45.322394: step 300, loss 2.36114, acc 0.421875, prec 0.0200549, recall 0.666667\n",
      "\n",
      "Evaluation:\n",
      "2017-12-08T00:13:26.879697: step 300, loss 1.61546, acc 0.517928, prec 0.0230205, recall 0.770455\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold3/1512709779/checkpoints/model-300\n",
      "\n",
      "2017-12-08T00:13:28.745736: step 301, loss 6.38417, acc 0.515625, prec 0.0231061, recall 0.769752\n",
      "2017-12-08T00:13:29.280267: step 302, loss 7.31394, acc 0.59375, prec 0.0231331, recall 0.768539\n",
      "2017-12-08T00:13:29.800234: step 303, loss 2.60171, acc 0.40625, prec 0.0230738, recall 0.768539\n",
      "2017-12-08T00:13:30.348732: step 304, loss 2.13874, acc 0.53125, prec 0.023093, recall 0.769058\n",
      "2017-12-08T00:13:30.902826: step 305, loss 2.40096, acc 0.53125, prec 0.0231777, recall 0.770089\n",
      "2017-12-08T00:13:31.456605: step 306, loss 3.2449, acc 0.515625, prec 0.023326, recall 0.771619\n",
      "2017-12-08T00:13:32.019992: step 307, loss 2.33937, acc 0.53125, prec 0.0232791, recall 0.771619\n",
      "2017-12-08T00:13:32.580318: step 308, loss 5.84431, acc 0.515625, prec 0.0232977, recall 0.770419\n",
      "2017-12-08T00:13:33.144791: step 309, loss 1.87735, acc 0.546875, prec 0.0232527, recall 0.770419\n",
      "2017-12-08T00:13:33.692583: step 310, loss 3.70991, acc 0.5625, prec 0.0232759, recall 0.769231\n",
      "2017-12-08T00:13:34.248074: step 311, loss 1.61511, acc 0.609375, prec 0.0233021, recall 0.769737\n",
      "2017-12-08T00:13:34.806645: step 312, loss 1.66871, acc 0.671875, prec 0.0233992, recall 0.770742\n",
      "2017-12-08T00:13:35.364196: step 313, loss 10.466, acc 0.453125, prec 0.0233481, recall 0.767391\n",
      "2017-12-08T00:13:35.924396: step 314, loss 3.48149, acc 0.578125, prec 0.023308, recall 0.765727\n",
      "2017-12-08T00:13:36.483220: step 315, loss 3.61954, acc 0.484375, prec 0.0233232, recall 0.764579\n",
      "2017-12-08T00:13:37.041401: step 316, loss 2.41872, acc 0.46875, prec 0.0232711, recall 0.764579\n",
      "2017-12-08T00:13:37.607729: step 317, loss 3.0323, acc 0.484375, prec 0.0233489, recall 0.765591\n",
      "2017-12-08T00:13:38.168509: step 318, loss 2.2679, acc 0.484375, prec 0.0234263, recall 0.766595\n",
      "2017-12-08T00:13:38.720545: step 319, loss 3.17325, acc 0.46875, prec 0.0233742, recall 0.766595\n",
      "2017-12-08T00:13:39.281143: step 320, loss 3.97619, acc 0.40625, prec 0.02338, recall 0.767094\n",
      "2017-12-08T00:13:39.832290: step 321, loss 2.47276, acc 0.484375, prec 0.0233299, recall 0.767094\n",
      "2017-12-08T00:13:40.438262: step 322, loss 2.70678, acc 0.40625, prec 0.0232724, recall 0.767094\n",
      "2017-12-08T00:13:41.046727: step 323, loss 2.27171, acc 0.46875, prec 0.0232844, recall 0.767591\n",
      "2017-12-08T00:13:41.617354: step 324, loss 3.51241, acc 0.4375, prec 0.0232303, recall 0.767591\n",
      "2017-12-08T00:13:42.200565: step 325, loss 1.84838, acc 0.546875, prec 0.0232498, recall 0.768085\n",
      "2017-12-08T00:13:42.781500: step 326, loss 1.44414, acc 0.5625, prec 0.023208, recall 0.768085\n",
      "2017-12-08T00:13:43.366737: step 327, loss 4.37906, acc 0.5, prec 0.0232245, recall 0.766949\n",
      "2017-12-08T00:13:43.950135: step 328, loss 14.5815, acc 0.546875, prec 0.0232454, recall 0.765823\n",
      "2017-12-08T00:13:44.527288: step 329, loss 2.3731, acc 0.609375, prec 0.0232707, recall 0.766316\n",
      "2017-12-08T00:13:45.106189: step 330, loss 2.10994, acc 0.578125, prec 0.0232306, recall 0.766316\n",
      "2017-12-08T00:13:45.682302: step 331, loss 7.95845, acc 0.546875, prec 0.0232529, recall 0.763598\n",
      "2017-12-08T00:13:46.248809: step 332, loss 2.10823, acc 0.53125, prec 0.0232706, recall 0.764092\n",
      "2017-12-08T00:13:46.806793: step 333, loss 2.14576, acc 0.515625, prec 0.0232248, recall 0.764092\n",
      "2017-12-08T00:13:47.406687: step 334, loss 1.96057, acc 0.546875, prec 0.0231822, recall 0.764092\n",
      "2017-12-08T00:13:47.974068: step 335, loss 2.3432, acc 0.59375, prec 0.023144, recall 0.764092\n",
      "2017-12-08T00:13:48.555403: step 336, loss 2.23427, acc 0.59375, prec 0.0231677, recall 0.764583\n",
      "2017-12-08T00:13:49.109947: step 337, loss 2.55795, acc 0.578125, prec 0.0232514, recall 0.76556\n",
      "2017-12-08T00:13:49.692380: step 338, loss 2.59401, acc 0.5625, prec 0.0232105, recall 0.76556\n",
      "2017-12-08T00:13:50.471382: step 339, loss 1.40865, acc 0.6875, prec 0.023304, recall 0.766529\n",
      "2017-12-08T00:13:51.214602: step 340, loss 1.78759, acc 0.671875, prec 0.0233346, recall 0.76701\n",
      "2017-12-08T00:13:52.032769: step 341, loss 1.48009, acc 0.6875, prec 0.0233665, recall 0.76749\n",
      "2017-12-08T00:13:52.892959: step 342, loss 2.90365, acc 0.5625, prec 0.0233271, recall 0.765914\n",
      "2017-12-08T00:13:53.719465: step 343, loss 1.85051, acc 0.5625, prec 0.0233473, recall 0.766393\n",
      "2017-12-08T00:13:54.437432: step 344, loss 2.59532, acc 0.53125, prec 0.0233645, recall 0.766871\n",
      "2017-12-08T00:13:55.156596: step 345, loss 1.62867, acc 0.609375, prec 0.0234496, recall 0.767821\n",
      "2017-12-08T00:13:55.908934: step 346, loss 26.2127, acc 0.703125, prec 0.0235448, recall 0.767206\n",
      "2017-12-08T00:13:56.618389: step 347, loss 1.49752, acc 0.640625, prec 0.0235717, recall 0.767677\n",
      "2017-12-08T00:13:57.310980: step 348, loss 1.52353, acc 0.578125, prec 0.0235323, recall 0.767677\n",
      "2017-12-08T00:13:58.043488: step 349, loss 1.2365, acc 0.6875, prec 0.0235032, recall 0.767677\n",
      "2017-12-08T00:13:58.768588: step 350, loss 1.02011, acc 0.703125, prec 0.0234756, recall 0.767677\n",
      "2017-12-08T00:13:59.490975: step 351, loss 2.0844, acc 0.578125, prec 0.0234968, recall 0.768145\n",
      "2017-12-08T00:14:00.271737: step 352, loss 1.78932, acc 0.640625, prec 0.0234635, recall 0.768145\n",
      "2017-12-08T00:14:01.022415: step 353, loss 1.00414, acc 0.671875, prec 0.0234932, recall 0.768612\n",
      "2017-12-08T00:14:01.731652: step 354, loss 1.21624, acc 0.703125, prec 0.0234658, recall 0.768612\n",
      "2017-12-08T00:14:02.425308: step 355, loss 0.761438, acc 0.78125, prec 0.0234457, recall 0.768612\n",
      "2017-12-08T00:14:03.107373: step 356, loss 0.639474, acc 0.75, prec 0.0234227, recall 0.768612\n",
      "2017-12-08T00:14:03.834156: step 357, loss 0.620956, acc 0.859375, prec 0.0234097, recall 0.768612\n",
      "2017-12-08T00:14:04.547961: step 358, loss 0.817059, acc 0.828125, prec 0.0234538, recall 0.769076\n",
      "2017-12-08T00:14:05.235467: step 359, loss 0.427175, acc 0.859375, prec 0.0234408, recall 0.769076\n",
      "2017-12-08T00:14:05.928191: step 360, loss 0.457962, acc 0.859375, prec 0.0234279, recall 0.769076\n",
      "2017-12-08T00:14:06.631648: step 361, loss 0.635793, acc 0.765625, prec 0.0234065, recall 0.769076\n",
      "2017-12-08T00:14:07.357392: step 362, loss 0.432028, acc 0.828125, prec 0.0233907, recall 0.769076\n",
      "2017-12-08T00:14:08.082343: step 363, loss 0.206472, acc 0.953125, prec 0.0233865, recall 0.769076\n",
      "2017-12-08T00:14:08.801088: step 364, loss 4.51486, acc 0.890625, prec 0.0233779, recall 0.767535\n",
      "2017-12-08T00:14:09.550370: step 365, loss 0.950518, acc 0.828125, prec 0.0234218, recall 0.768\n",
      "2017-12-08T00:14:10.267275: step 366, loss 0.297676, acc 0.96875, prec 0.0234785, recall 0.768463\n",
      "2017-12-08T00:14:11.045577: step 367, loss 11.1571, acc 0.90625, prec 0.0234713, recall 0.766932\n",
      "2017-12-08T00:14:11.759887: step 368, loss 0.479707, acc 0.84375, prec 0.023457, recall 0.766932\n",
      "2017-12-08T00:14:12.493149: step 369, loss 5.25693, acc 0.890625, prec 0.0235079, recall 0.765873\n",
      "2017-12-08T00:14:13.178824: step 370, loss 0.571139, acc 0.8125, prec 0.0235502, recall 0.766337\n",
      "2017-12-08T00:14:13.893514: step 371, loss 0.315949, acc 0.890625, prec 0.0235995, recall 0.766798\n",
      "2017-12-08T00:14:14.592775: step 372, loss 3.1238, acc 0.859375, prec 0.0236474, recall 0.765748\n",
      "2017-12-08T00:14:15.269510: step 373, loss 8.09571, acc 0.78125, prec 0.0236302, recall 0.762745\n",
      "2017-12-08T00:14:16.035080: step 374, loss 10.2412, acc 0.75, prec 0.0236087, recall 0.761252\n",
      "2017-12-08T00:14:16.710580: step 375, loss 0.524537, acc 0.8125, prec 0.0235915, recall 0.761252\n",
      "2017-12-08T00:14:17.392123: step 376, loss 1.46154, acc 0.71875, prec 0.0235658, recall 0.761252\n",
      "2017-12-08T00:14:18.105246: step 377, loss 1.57401, acc 0.625, prec 0.0236497, recall 0.762183\n",
      "2017-12-08T00:14:18.838494: step 378, loss 2.20714, acc 0.5, prec 0.0236629, recall 0.762646\n",
      "2017-12-08T00:14:19.584703: step 379, loss 15.1591, acc 0.5, prec 0.0237364, recall 0.762089\n",
      "2017-12-08T00:14:20.281577: step 380, loss 2.5451, acc 0.40625, prec 0.0236822, recall 0.762089\n",
      "2017-12-08T00:14:21.015558: step 381, loss 2.79179, acc 0.40625, prec 0.0236282, recall 0.762089\n",
      "2017-12-08T00:14:21.759106: step 382, loss 2.03852, acc 0.484375, prec 0.02364, recall 0.762548\n",
      "2017-12-08T00:14:22.526636: step 383, loss 2.98693, acc 0.4375, prec 0.0235891, recall 0.762548\n",
      "2017-12-08T00:14:23.232551: step 384, loss 3.75305, acc 0.40625, prec 0.0236521, recall 0.763462\n",
      "2017-12-08T00:14:23.954496: step 385, loss 2.32293, acc 0.515625, prec 0.0236665, recall 0.763916\n",
      "2017-12-08T00:14:24.661114: step 386, loss 2.1317, acc 0.421875, prec 0.0236725, recall 0.764368\n",
      "2017-12-08T00:14:25.350966: step 387, loss 2.63609, acc 0.453125, prec 0.0236234, recall 0.764368\n",
      "2017-12-08T00:14:26.068686: step 388, loss 2.90568, acc 0.390625, prec 0.023569, recall 0.764368\n",
      "2017-12-08T00:14:26.733555: step 389, loss 2.07474, acc 0.5, prec 0.0235246, recall 0.764368\n",
      "2017-12-08T00:14:27.475796: step 390, loss 10.6898, acc 0.59375, prec 0.0236624, recall 0.764259\n",
      "2017-12-08T00:14:28.196554: step 391, loss 1.63526, acc 0.609375, prec 0.0236276, recall 0.764259\n",
      "2017-12-08T00:14:28.858674: step 392, loss 7.99721, acc 0.515625, prec 0.023586, recall 0.762808\n",
      "2017-12-08T00:14:29.521385: step 393, loss 2.60588, acc 0.4375, prec 0.0235363, recall 0.762808\n",
      "2017-12-08T00:14:30.196085: step 394, loss 1.85859, acc 0.609375, prec 0.0235019, recall 0.762808\n",
      "2017-12-08T00:14:30.850588: step 395, loss 6.63338, acc 0.59375, prec 0.0236386, recall 0.762712\n",
      "2017-12-08T00:14:31.555087: step 396, loss 3.78609, acc 0.515625, prec 0.0236542, recall 0.761726\n",
      "2017-12-08T00:14:32.222821: step 397, loss 2.5059, acc 0.515625, prec 0.0237251, recall 0.762617\n",
      "2017-12-08T00:14:32.879846: step 398, loss 2.47375, acc 0.53125, prec 0.0236838, recall 0.762617\n",
      "2017-12-08T00:14:33.510448: step 399, loss 2.4559, acc 0.5, prec 0.0236398, recall 0.762617\n",
      "2017-12-08T00:14:34.146258: step 400, loss 2.72381, acc 0.453125, prec 0.0236485, recall 0.76306\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/fold3/1512709779/checkpoints/model-400\n",
      "\n",
      "2017-12-08T00:14:35.662854: step 401, loss 1.65324, acc 0.578125, prec 0.0236116, recall 0.76306\n",
      "2017-12-08T00:14:36.261533: step 402, loss 2.50388, acc 0.625, prec 0.0236352, recall 0.763501\n",
      "2017-12-08T00:14:36.844952: step 403, loss 1.95803, acc 0.609375, prec 0.0236012, recall 0.763501\n",
      "2017-12-08T00:14:37.433618: step 404, loss 2.19376, acc 0.46875, prec 0.0235551, recall 0.763501\n",
      "2017-12-08T00:14:38.015008: step 405, loss 4.37295, acc 0.578125, prec 0.0235213, recall 0.760668\n",
      "2017-12-08T00:14:38.596853: step 406, loss 2.07719, acc 0.453125, prec 0.0234742, recall 0.760668\n",
      "2017-12-08T00:14:39.181486: step 407, loss 7.81599, acc 0.484375, prec 0.0234312, recall 0.759259\n",
      "2017-12-08T00:14:39.762475: step 408, loss 1.07553, acc 0.65625, prec 0.0234576, recall 0.759704\n",
      "2017-12-08T00:14:40.344449: step 409, loss 10.0251, acc 0.515625, prec 0.0234731, recall 0.758748\n",
      "2017-12-08T00:14:40.940155: step 410, loss 2.60683, acc 0.515625, prec 0.0234873, recall 0.759191\n",
      "2017-12-08T00:14:41.538907: step 411, loss 1.48121, acc 0.640625, prec 0.0235675, recall 0.760073\n",
      "2017-12-08T00:14:42.157659: step 412, loss 2.93961, acc 0.53125, prec 0.0235828, recall 0.760512\n",
      "2017-12-08T00:14:42.747444: step 413, loss 2.042, acc 0.625, prec 0.0235507, recall 0.760512\n",
      "2017-12-08T00:14:43.350446: step 414, loss 2.33089, acc 0.515625, prec 0.0235095, recall 0.760512\n",
      "2017-12-08T00:14:43.953382: step 415, loss 2.66962, acc 0.453125, prec 0.0235732, recall 0.761384\n",
      "2017-12-08T00:14:44.556414: step 416, loss 1.61305, acc 0.578125, prec 0.0236473, recall 0.76225\n",
      "2017-12-08T00:14:45.167823: step 417, loss 1.48792, acc 0.5625, prec 0.0236101, recall 0.76225\n",
      "2017-12-08T00:14:45.783694: step 418, loss 7.48407, acc 0.578125, prec 0.0236304, recall 0.761302\n",
      "2017-12-08T00:14:46.407091: step 419, loss 3.10005, acc 0.640625, prec 0.0236013, recall 0.759928\n",
      "2017-12-08T00:14:47.025463: step 420, loss 1.43638, acc 0.546875, prec 0.023563, recall 0.759928\n",
      "2017-12-08T00:14:47.641431: step 421, loss 1.68631, acc 0.59375, prec 0.0235288, recall 0.759928\n",
      "2017-12-08T00:14:48.250048: step 422, loss 2.19916, acc 0.53125, prec 0.0234894, recall 0.759928\n",
      "2017-12-08T00:14:48.865867: step 423, loss 2.07203, acc 0.53125, prec 0.0234501, recall 0.759928\n",
      "2017-12-08T00:14:49.473555: step 424, loss 2.26263, acc 0.453125, prec 0.0234588, recall 0.76036\n",
      "2017-12-08T00:14:50.067392: step 425, loss 1.38391, acc 0.703125, prec 0.023434, recall 0.76036\n",
      "2017-12-08T00:14:50.678086: step 426, loss 1.48151, acc 0.65625, prec 0.0234054, recall 0.76036\n",
      "2017-12-08T00:14:51.329834: step 427, loss 1.78342, acc 0.6875, prec 0.0234336, recall 0.760791\n",
      "2017-12-08T00:14:51.954983: step 428, loss 2.05961, acc 0.65625, prec 0.0234604, recall 0.759857\n",
      "2017-12-08T00:14:52.571796: step 429, loss 0.821741, acc 0.75, prec 0.0234936, recall 0.760286\n",
      "2017-12-08T00:14:53.183516: step 430, loss 0.981474, acc 0.75, prec 0.0234729, recall 0.760286\n",
      "2017-12-08T00:14:53.792980: step 431, loss 0.949418, acc 0.78125, prec 0.0234547, recall 0.760286\n",
      "2017-12-08T00:14:54.392571: step 432, loss 1.48895, acc 0.890625, prec 0.0234996, recall 0.760714\n",
      "2017-12-08T00:14:55.004774: step 433, loss 8.23677, acc 0.796875, prec 0.0234853, recall 0.758007\n",
      "2017-12-08T00:14:55.616654: step 434, loss 0.736529, acc 0.734375, prec 0.0234633, recall 0.758007\n",
      "2017-12-08T00:14:56.228970: step 435, loss 1.19585, acc 0.828125, prec 0.0235029, recall 0.758437\n",
      "2017-12-08T00:14:56.835804: step 436, loss 11.5897, acc 0.75, prec 0.0235372, recall 0.757522\n",
      "2017-12-08T00:14:57.448399: step 437, loss 1.65135, acc 0.625, prec 0.0235598, recall 0.757951\n",
      "2017-12-08T00:14:58.060490: step 438, loss 1.71665, acc 0.59375, prec 0.0235262, recall 0.757951\n",
      "2017-12-08T00:14:58.671562: step 439, loss 1.15525, acc 0.703125, prec 0.0235552, recall 0.758377\n",
      "2017-12-08T00:14:59.282332: step 440, loss 4.05893, acc 0.640625, prec 0.0235803, recall 0.757469\n",
      "2017-12-08T00:14:59.907645: step 441, loss 5.97354, acc 0.515625, prec 0.023595, recall 0.756567\n",
      "2017-12-08T00:15:00.518329: step 442, loss 2.09547, acc 0.578125, prec 0.0235602, recall 0.756567\n",
      "2017-12-08T00:15:01.136750: step 443, loss 1.58603, acc 0.609375, prec 0.0235281, recall 0.756567\n",
      "2017-12-08T00:15:01.755464: step 444, loss 5.77426, acc 0.546875, prec 0.0235454, recall 0.755672\n",
      "2017-12-08T00:15:02.364834: step 445, loss 2.22055, acc 0.53125, prec 0.0235601, recall 0.756098\n",
      "2017-12-08T00:15:02.976720: step 446, loss 2.36693, acc 0.453125, prec 0.0235154, recall 0.756098\n",
      "2017-12-08T00:15:03.584400: step 447, loss 8.00812, acc 0.4375, prec 0.0234709, recall 0.754783\n",
      "2017-12-08T00:15:04.197133: step 448, loss 7.86843, acc 0.5, prec 0.0234316, recall 0.753472\n",
      "2017-12-08T00:15:04.803371: step 449, loss 3.23842, acc 0.390625, prec 0.0233824, recall 0.753472\n",
      "2017-12-08T00:15:05.402404: step 450, loss 2.65458, acc 0.453125, prec 0.0233384, recall 0.753472\n",
      "2017-12-08T00:15:06.013659: step 451, loss 2.98086, acc 0.359375, prec 0.0233394, recall 0.753899\n",
      "2017-12-08T00:15:06.620632: step 452, loss 3.59873, acc 0.328125, prec 0.023338, recall 0.754325\n",
      "2017-12-08T00:15:07.255461: step 453, loss 3.6721, acc 0.296875, prec 0.0232819, recall 0.754325\n",
      "2017-12-08T00:15:07.863686: step 454, loss 3.31497, acc 0.359375, prec 0.0232831, recall 0.75475\n",
      "2017-12-08T00:15:08.474004: step 455, loss 6.21139, acc 0.421875, prec 0.0233424, recall 0.754296\n",
      "2017-12-08T00:15:09.083117: step 456, loss 3.24691, acc 0.375, prec 0.0232928, recall 0.754296\n",
      "2017-12-08T00:15:09.686947: step 457, loss 3.50653, acc 0.359375, prec 0.0232423, recall 0.754296\n",
      "2017-12-08T00:15:10.290006: step 458, loss 2.96332, acc 0.53125, prec 0.0233087, recall 0.755137\n",
      "2017-12-08T00:15:10.891935: step 459, loss 3.15185, acc 0.421875, prec 0.0233147, recall 0.755556\n",
      "2017-12-08T00:15:11.526137: step 460, loss 3.23934, acc 0.421875, prec 0.0233207, recall 0.755973\n",
      "2017-12-08T00:15:12.138635: step 461, loss 2.07024, acc 0.5625, prec 0.0233377, recall 0.756388\n",
      "2017-12-08T00:15:12.754556: step 462, loss 2.60026, acc 0.4375, prec 0.0234473, recall 0.757627\n",
      "2017-12-08T00:15:13.359986: step 463, loss 2.19802, acc 0.59375, prec 0.0234154, recall 0.757627\n",
      "2017-12-08T00:15:13.968462: step 464, loss 1.79161, acc 0.515625, prec 0.0233774, recall 0.757627\n",
      "2017-12-08T00:15:14.566693: step 465, loss 1.27429, acc 0.578125, prec 0.0233445, recall 0.757627\n",
      "2017-12-08T00:15:15.176560: step 466, loss 6.70788, acc 0.640625, prec 0.0233686, recall 0.756757\n",
      "2017-12-08T00:15:15.786090: step 467, loss 1.2949, acc 0.65625, prec 0.0233418, recall 0.756757\n",
      "2017-12-08T00:15:16.392500: step 468, loss 0.94228, acc 0.78125, prec 0.0233248, recall 0.756757\n",
      "2017-12-08T00:15:17.005679: step 469, loss 1.70335, acc 0.5625, prec 0.0233417, recall 0.757167\n",
      "2017-12-08T00:15:17.612250: step 470, loss 4.98907, acc 0.703125, prec 0.0233706, recall 0.756303\n",
      "2017-12-08T00:15:18.225436: step 471, loss 0.851757, acc 0.78125, prec 0.0234549, recall 0.757119\n",
      "2017-12-08T00:15:18.828334: step 472, loss 1.06193, acc 0.703125, prec 0.0234318, recall 0.757119\n",
      "2017-12-08T00:15:19.441492: step 473, loss 0.655816, acc 0.734375, prec 0.0234112, recall 0.757119\n",
      "2017-12-08T00:15:20.047400: step 474, loss 2.54406, acc 0.8125, prec 0.0233979, recall 0.755853\n",
      "2017-12-08T00:15:20.659641: step 475, loss 4.76107, acc 0.765625, prec 0.0233809, recall 0.754591\n",
      "2017-12-08T00:15:21.263763: step 476, loss 0.939961, acc 0.734375, prec 0.0233604, recall 0.754591\n",
      "2017-12-08T00:15:21.918134: step 477, loss 3.61603, acc 0.734375, prec 0.0233411, recall 0.753333\n",
      "2017-12-08T00:15:22.563497: step 478, loss 1.94908, acc 0.78125, prec 0.0234766, recall 0.753311\n",
      "2017-12-08T00:15:23.175176: step 479, loss 8.17781, acc 0.71875, prec 0.023456, recall 0.752066\n",
      "2017-12-08T00:15:23.795859: step 480, loss 1.41119, acc 0.640625, prec 0.0234282, recall 0.752066\n",
      "2017-12-08T00:15:24.408702: step 481, loss 1.65992, acc 0.65625, prec 0.0234017, recall 0.752066\n",
      "2017-12-08T00:15:25.016725: step 482, loss 1.50057, acc 0.65625, prec 0.0234756, recall 0.752883\n",
      "2017-12-08T00:15:25.626255: step 483, loss 1.57719, acc 0.65625, prec 0.0234992, recall 0.753289\n",
      "2017-12-08T00:15:26.238570: step 484, loss 4.04553, acc 0.59375, prec 0.0235192, recall 0.752459\n",
      "2017-12-08T00:15:26.855316: step 485, loss 1.77862, acc 0.546875, prec 0.0235342, recall 0.752864\n",
      "2017-12-08T00:15:27.472748: step 486, loss 2.11689, acc 0.546875, prec 0.0234994, recall 0.752864\n",
      "2017-12-08T00:15:28.080336: step 487, loss 1.81826, acc 0.640625, prec 0.0234718, recall 0.752864\n",
      "2017-12-08T00:15:28.684753: step 488, loss 2.18924, acc 0.421875, prec 0.0234276, recall 0.752864\n",
      "2017-12-08T00:15:29.289042: step 489, loss 2.44864, acc 0.484375, prec 0.0233882, recall 0.752864\n",
      "2017-12-08T00:15:29.917160: step 490, loss 1.3863, acc 0.53125, prec 0.0233526, recall 0.752864\n",
      "2017-12-08T00:15:30.539852: step 491, loss 1.91134, acc 0.609375, prec 0.0233725, recall 0.753268\n",
      "2017-12-08T00:15:31.150582: step 492, loss 2.42215, acc 0.703125, prec 0.0233995, recall 0.75367\n",
      "2017-12-08T00:15:31.775272: step 493, loss 1.58216, acc 0.59375, prec 0.0234181, recall 0.754072\n",
      "2017-12-08T00:15:32.390717: step 494, loss 1.58673, acc 0.625, prec 0.0233897, recall 0.754072\n",
      "2017-12-08T00:15:32.999224: step 495, loss 1.15184, acc 0.75, prec 0.0234201, recall 0.754472\n",
      "2017-12-08T00:15:33.610586: step 496, loss 0.881071, acc 0.78125, prec 0.0235021, recall 0.755267\n",
      "2017-12-08T00:15:34.141946: step 497, loss 5.19843, acc 0.711538, prec 0.0234855, recall 0.754045\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Creating folds\n",
    "kf = KFold(n_splits=4, random_state=5, shuffle=True)\n",
    "for k, (train_index, test_index) in enumerate(kf.split(x, y)):\n",
    "# for train_index, test_index in kf.split(x):\n",
    "#     print(\"Fold: %s =>\" % k,  \"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_dev = x[train_index], x[test_index]\n",
    "    y_train, y_dev = y[train_index], y[test_index]\n",
    "    print(\"Fold: %s =>\" % k, \"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "    \n",
    "    model = BasicTextCNN(sequence_length=x_train.shape[1],\n",
    "            vocab_processor=vocab_processor, num_epochs=1, evaluate_every=300, results_dir='fold%s'%k)\n",
    "    model.train_network(x_train, y_train, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, embedded_chars, embedded_chars_expanded = model.sess.run([model.W, model.embedded_chars, model.embedded_chars_expanded], {\n",
    "              model.input_x: x_train[:20],\n",
    "              model.input_y: y_train[:20],\n",
    "              model.dropout_keep_prob: 1.0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33447, 128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 273, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 273, 128, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65069515,  0.56143415, -0.39088789, ..., -0.54359448,\n",
       "        -0.94726586, -0.68787569],\n",
       "       [-0.3445662 ,  0.93415439,  0.62170506, ...,  0.30054376,\n",
       "         0.25937274,  0.21429159],\n",
       "       [ 0.25671276,  0.46195653,  0.14779501, ...,  0.43584326,\n",
       "        -0.18676297,  0.68269938],\n",
       "       ..., \n",
       "       [-0.61115575,  0.52459705, -0.90316278, ...,  0.57382399,\n",
       "         0.42873541, -0.32618925],\n",
       "       [-0.61115575,  0.52459705, -0.90316278, ...,  0.57382399,\n",
       "         0.42873541, -0.32618925],\n",
       "       [-0.61115575,  0.52459705, -0.90316278, ...,  0.57382399,\n",
       "         0.42873541, -0.32618925]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
