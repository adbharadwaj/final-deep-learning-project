Vocabulary Size: 33447
encodedPathwayA = 8 encodedPathwayB = 53
Varying batch size
Starting Experiment - batch_size_512 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 512
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_512_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_0/1512882237

Start training
2017-12-10T00:04:01.306771: step 1, loss 4.45811, acc 0.714844, prec 0.0138889, recall 0.333333
2017-12-10T00:04:02.213983: step 2, loss 5.2897, acc 0.507812, prec 0.0175879, recall 0.5
2017-12-10T00:04:03.121497: step 3, loss 7.45007, acc 0.324219, prec 0.0160858, recall 0.545455
2017-12-10T00:04:04.029532: step 4, loss 4.56903, acc 0.296875, prec 0.0170863, recall 0.633333
2017-12-10T00:04:04.936100: step 5, loss 3.91787, acc 0.355469, prec 0.0152355, recall 0.647059
2017-12-10T00:04:05.871021: step 6, loss 4.05778, acc 0.462891, prec 0.0156794, recall 0.658537
2017-12-10T00:04:06.779657: step 7, loss 3.44273, acc 0.589844, prec 0.0155119, recall 0.666667
2017-12-10T00:04:07.692978: step 8, loss 1.66621, acc 0.673828, prec 0.0152236, recall 0.666667
2017-12-10T00:04:08.609562: step 9, loss 11.8855, acc 0.779297, prec 0.0153846, recall 0.596491
2017-12-10T00:04:09.522202: step 10, loss 7.18121, acc 0.787109, prec 0.0155575, recall 0.545455
2017-12-10T00:04:10.435780: step 11, loss 2.3703, acc 0.712891, prec 0.016247, recall 0.547945
2017-12-10T00:04:11.350219: step 12, loss 2.2563, acc 0.652344, prec 0.0159091, recall 0.545455
2017-12-10T00:04:12.267614: step 13, loss 8.95566, acc 0.548828, prec 0.0156849, recall 0.529412
2017-12-10T00:04:13.196215: step 14, loss 5.14888, acc 0.474609, prec 0.0178117, recall 0.554455
2017-12-10T00:04:14.109109: step 15, loss 3.39496, acc 0.369141, prec 0.0181347, recall 0.583333
2017-12-10T00:04:15.011463: step 16, loss 5.80803, acc 0.296875, prec 0.0179781, recall 0.594828
2017-12-10T00:04:15.923462: step 17, loss 5.38101, acc 0.240234, prec 0.01749, recall 0.606557
2017-12-10T00:04:16.836219: step 18, loss 6.75535, acc 0.279297, prec 0.0178106, recall 0.61194
2017-12-10T00:04:17.744874: step 19, loss 5.12572, acc 0.287109, prec 0.017893, recall 0.622378
2017-12-10T00:04:18.659969: step 20, loss 4.31828, acc 0.285156, prec 0.0168539, recall 0.62069
2017-12-10T00:04:19.569106: step 21, loss 5.38612, acc 0.357422, prec 0.0169223, recall 0.627451
2017-12-10T00:04:20.473002: step 22, loss 3.71389, acc 0.433594, prec 0.0172558, recall 0.639752
2017-12-10T00:04:21.399048: step 23, loss 2.76854, acc 0.482422, prec 0.01668, recall 0.641975
2017-12-10T00:04:22.308874: step 24, loss 4.85703, acc 0.537109, prec 0.0165302, recall 0.640719
2017-12-10T00:04:23.224932: step 25, loss 8.54016, acc 0.667969, prec 0.0161193, recall 0.622093
2017-12-10T00:04:24.136312: step 26, loss 1.93925, acc 0.699219, prec 0.0161908, recall 0.625
2017-12-10T00:04:25.053863: step 27, loss 2.19491, acc 0.75, prec 0.0163248, recall 0.620879
2017-12-10T00:04:25.954296: step 28, loss 1.7646, acc 0.738281, prec 0.0167139, recall 0.62766
2017-12-10T00:04:26.867952: step 29, loss 4.75952, acc 0.802734, prec 0.017174, recall 0.624366
2017-12-10T00:04:27.775466: step 30, loss 4.25175, acc 0.773438, prec 0.0173124, recall 0.62069
2017-12-10T00:04:28.685041: step 31, loss 6.64993, acc 0.769531, prec 0.0177123, recall 0.615023
2017-12-10T00:04:29.594624: step 32, loss 4.44546, acc 0.697266, prec 0.017746, recall 0.611872
2017-12-10T00:04:30.519220: step 33, loss 4.53393, acc 0.646484, prec 0.0178456, recall 0.610619
2017-12-10T00:04:31.422562: step 34, loss 3.21448, acc 0.613281, prec 0.0186398, recall 0.624473
2017-12-10T00:04:32.341963: step 35, loss 3.17033, acc 0.509766, prec 0.0186699, recall 0.62963
2017-12-10T00:04:33.253859: step 36, loss 3.8904, acc 0.460938, prec 0.0184114, recall 0.631579
2017-12-10T00:04:34.169023: step 37, loss 5.81585, acc 0.443359, prec 0.0182648, recall 0.632411
2017-12-10T00:04:35.077443: step 38, loss 3.76919, acc 0.429688, prec 0.0187617, recall 0.643939
2017-12-10T00:04:36.007692: step 39, loss 3.02599, acc 0.455078, prec 0.0187266, recall 0.650558
2017-12-10T00:04:36.922536: step 40, loss 5.41047, acc 0.447266, prec 0.0192008, recall 0.656028
2017-12-10T00:04:37.833042: step 41, loss 2.99109, acc 0.455078, prec 0.0193529, recall 0.66436
2017-12-10T00:04:38.730842: step 42, loss 5.22931, acc 0.458984, prec 0.019408, recall 0.666667
2017-12-10T00:04:39.644912: step 43, loss 3.87907, acc 0.574219, prec 0.0195702, recall 0.668852
2017-12-10T00:04:40.566408: step 44, loss 5.56773, acc 0.615234, prec 0.019582, recall 0.666667
2017-12-10T00:04:41.467556: step 45, loss 6.29763, acc 0.582031, prec 0.0196567, recall 0.661491
2017-12-10T00:04:42.375521: step 46, loss 3.91761, acc 0.570312, prec 0.0198029, recall 0.661631
2017-12-10T00:04:43.286716: step 47, loss 6.09537, acc 0.59375, prec 0.0197053, recall 0.656805
2017-12-10T00:04:44.198554: step 48, loss 4.74938, acc 0.574219, prec 0.0197632, recall 0.656069
2017-12-10T00:04:45.103247: step 49, loss 4.60707, acc 0.564453, prec 0.0198941, recall 0.656338
2017-12-10T00:04:46.021851: step 50, loss 3.47499, acc 0.519531, prec 0.0196505, recall 0.656425
2017-12-10T00:04:46.949349: step 51, loss 3.57646, acc 0.509766, prec 0.0196512, recall 0.657534
2017-12-10T00:04:47.861971: step 52, loss 5.81193, acc 0.484375, prec 0.0195528, recall 0.657682
2017-12-10T00:04:48.772885: step 53, loss 3.61335, acc 0.462891, prec 0.0198276, recall 0.660574
2017-12-10T00:04:49.683630: step 54, loss 2.97507, acc 0.458984, prec 0.0199325, recall 0.666667
2017-12-10T00:04:50.598393: step 55, loss 2.60412, acc 0.484375, prec 0.0201262, recall 0.673367
2017-12-10T00:04:51.515409: step 56, loss 3.38154, acc 0.5, prec 0.0201827, recall 0.674877
2017-12-10T00:04:52.424823: step 57, loss 3.02614, acc 0.552734, prec 0.0205603, recall 0.679426
2017-12-10T00:04:53.325745: step 58, loss 3.31022, acc 0.59375, prec 0.0206774, recall 0.680751
2017-12-10T00:04:54.239663: step 59, loss 3.90514, acc 0.646484, prec 0.0209022, recall 0.679634
2017-12-10T00:04:55.150477: step 60, loss 1.57008, acc 0.658203, prec 0.0208536, recall 0.680272
2017-12-10T00:04:56.066078: step 61, loss 2.65591, acc 0.628906, prec 0.0207876, recall 0.677852
2017-12-10T00:04:56.975608: step 62, loss 2.43316, acc 0.689453, prec 0.0208319, recall 0.677704
2017-12-10T00:04:57.159334: step 63, loss 1.0935, acc 0.745098, prec 0.0208799, recall 0.678414
2017-12-10T00:04:58.086666: step 64, loss 1.80989, acc 0.757812, prec 0.0210378, recall 0.678959
2017-12-10T00:04:59.000862: step 65, loss 1.7506, acc 0.759766, prec 0.0211943, recall 0.679487
2017-12-10T00:04:59.905321: step 66, loss 2.20618, acc 0.753906, prec 0.0210853, recall 0.677282
2017-12-10T00:05:00.835309: step 67, loss 1.84609, acc 0.751953, prec 0.0212334, recall 0.677824
2017-12-10T00:05:01.756954: step 68, loss 2.32211, acc 0.736328, prec 0.0211771, recall 0.676349
2017-12-10T00:05:02.664689: step 69, loss 1.18612, acc 0.742188, prec 0.0211875, recall 0.676955
2017-12-10T00:05:03.569165: step 70, loss 2.33129, acc 0.726562, prec 0.0213757, recall 0.678138
2017-12-10T00:05:04.478957: step 71, loss 4.17604, acc 0.693359, prec 0.0216636, recall 0.677866
2017-12-10T00:05:05.391267: step 72, loss 1.74205, acc 0.701172, prec 0.0218873, recall 0.679612
2017-12-10T00:05:06.313268: step 73, loss 2.42393, acc 0.648438, prec 0.021887, recall 0.680769
2017-12-10T00:05:07.228822: step 74, loss 3.65183, acc 0.548828, prec 0.0219392, recall 0.681818
2017-12-10T00:05:08.145904: step 75, loss 2.07711, acc 0.580078, prec 0.0223037, recall 0.687037
2017-12-10T00:05:09.049934: step 76, loss 2.00287, acc 0.630859, prec 0.0227516, recall 0.692586
2017-12-10T00:05:09.967103: step 77, loss 1.91662, acc 0.585938, prec 0.0229272, recall 0.69697
2017-12-10T00:05:10.877284: step 78, loss 2.85275, acc 0.59375, prec 0.0229366, recall 0.697183
2017-12-10T00:05:11.777708: step 79, loss 2.46439, acc 0.585938, prec 0.0232744, recall 0.701724
2017-12-10T00:05:12.688821: step 80, loss 2.33118, acc 0.599609, prec 0.0233386, recall 0.702381
2017-12-10T00:05:13.623814: step 81, loss 2.36066, acc 0.642578, prec 0.0235393, recall 0.704013
2017-12-10T00:05:14.549507: step 82, loss 1.5982, acc 0.642578, prec 0.023463, recall 0.705491
2017-12-10T00:05:15.458790: step 83, loss 2.20303, acc 0.640625, prec 0.0235487, recall 0.707237
2017-12-10T00:05:16.363937: step 84, loss 1.68494, acc 0.708984, prec 0.0235185, recall 0.707516
2017-12-10T00:05:17.264809: step 85, loss 1.94321, acc 0.734375, prec 0.0235066, recall 0.706645
2017-12-10T00:05:18.174096: step 86, loss 1.28556, acc 0.705078, prec 0.0234747, recall 0.706924
2017-12-10T00:05:19.083264: step 87, loss 1.73475, acc 0.705078, prec 0.0234433, recall 0.7072
2017-12-10T00:05:20.005346: step 88, loss 1.30416, acc 0.736328, prec 0.0234322, recall 0.707472
2017-12-10T00:05:20.937337: step 89, loss 1.60172, acc 0.808594, prec 0.0235177, recall 0.708202
2017-12-10T00:05:21.849777: step 90, loss 1.35232, acc 0.783203, prec 0.0236384, recall 0.708268
2017-12-10T00:05:22.760893: step 91, loss 3.72336, acc 0.814453, prec 0.0237281, recall 0.70679
2017-12-10T00:05:23.664503: step 92, loss 2.19134, acc 0.8125, prec 0.0239654, recall 0.707763
2017-12-10T00:05:24.564920: step 93, loss 3.76416, acc 0.806641, prec 0.024349, recall 0.707899
2017-12-10T00:05:25.488066: step 94, loss 3.78207, acc 0.734375, prec 0.0244821, recall 0.707353
2017-12-10T00:05:26.400764: step 95, loss 1.44547, acc 0.689453, prec 0.0244838, recall 0.708029
2017-12-10T00:05:27.314019: step 96, loss 1.37762, acc 0.652344, prec 0.0246074, recall 0.710983
2017-12-10T00:05:28.232235: step 97, loss 1.48366, acc 0.621094, prec 0.0246125, recall 0.713056
2017-12-10T00:05:29.143148: step 98, loss 2.04691, acc 0.642578, prec 0.0246798, recall 0.714489
2017-12-10T00:05:30.052341: step 99, loss 2.8511, acc 0.572266, prec 0.0247513, recall 0.715288
2017-12-10T00:05:30.981837: step 100, loss 2.26435, acc 0.632812, prec 0.0247644, recall 0.715278
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_0/1512882237/checkpoints/model-100

2017-12-10T00:05:32.879403: step 101, loss 2.17041, acc 0.621094, prec 0.0248166, recall 0.715659
2017-12-10T00:05:33.791098: step 102, loss 2.19186, acc 0.625, prec 0.024869, recall 0.717007
2017-12-10T00:05:34.691329: step 103, loss 1.36168, acc 0.660156, prec 0.0248947, recall 0.718919
2017-12-10T00:05:35.597428: step 104, loss 1.59084, acc 0.640625, prec 0.0249548, recall 0.720214
2017-12-10T00:05:36.503713: step 105, loss 1.20246, acc 0.660156, prec 0.0249793, recall 0.722074
2017-12-10T00:05:37.418748: step 106, loss 2.49639, acc 0.734375, prec 0.0253153, recall 0.725131
2017-12-10T00:05:38.324412: step 107, loss 1.76819, acc 0.753906, prec 0.0254826, recall 0.725744
2017-12-10T00:05:39.230381: step 108, loss 1.09259, acc 0.773438, prec 0.0256144, recall 0.726923
2017-12-10T00:05:40.137469: step 109, loss 4.5733, acc 0.787109, prec 0.0257989, recall 0.726582
2017-12-10T00:05:41.042382: step 110, loss 1.75038, acc 0.773438, prec 0.0258864, recall 0.725564
2017-12-10T00:05:41.953945: step 111, loss 1.38483, acc 0.724609, prec 0.025985, recall 0.726708
2017-12-10T00:05:42.854684: step 112, loss 1.27186, acc 0.722656, prec 0.0260382, recall 0.727497
2017-12-10T00:05:43.774093: step 113, loss 2.77148, acc 0.751953, prec 0.0261946, recall 0.728049
2017-12-10T00:05:44.685598: step 114, loss 1.60084, acc 0.732422, prec 0.0262091, recall 0.728485
2017-12-10T00:05:45.591669: step 115, loss 1.14301, acc 0.710938, prec 0.0262941, recall 0.730445
2017-12-10T00:05:46.497719: step 116, loss 1.15486, acc 0.744141, prec 0.0262727, recall 0.730539
2017-12-10T00:05:47.402093: step 117, loss 1.91468, acc 0.736328, prec 0.0263744, recall 0.729858
2017-12-10T00:05:48.306772: step 118, loss 1.74269, acc 0.712891, prec 0.0264177, recall 0.730588
2017-12-10T00:05:49.213414: step 119, loss 2.54022, acc 0.714844, prec 0.0265038, recall 0.730769
2017-12-10T00:05:50.131805: step 120, loss 1.24283, acc 0.701172, prec 0.0266196, recall 0.732948
2017-12-10T00:05:51.055691: step 121, loss 1.15675, acc 0.689453, prec 0.0267678, recall 0.735395
2017-12-10T00:05:51.961071: step 122, loss 1.07814, acc 0.724609, prec 0.0268534, recall 0.737201
2017-12-10T00:05:52.881949: step 123, loss 1.91927, acc 0.705078, prec 0.0270081, recall 0.738739
2017-12-10T00:05:53.799062: step 124, loss 1.40287, acc 0.783203, prec 0.0271256, recall 0.739665
2017-12-10T00:05:54.724043: step 125, loss 1.9944, acc 0.769531, prec 0.0273565, recall 0.738987
2017-12-10T00:05:54.908623: step 126, loss 1.19881, acc 0.666667, prec 0.0273375, recall 0.738987
2017-12-10T00:05:55.824838: step 127, loss 1.29407, acc 0.748047, prec 0.0273928, recall 0.739606
2017-12-10T00:05:56.733127: step 128, loss 1.09597, acc 0.775391, prec 0.0275022, recall 0.740499
2017-12-10T00:05:57.642314: step 129, loss 2.66996, acc 0.783203, prec 0.02746, recall 0.739459
2017-12-10T00:05:58.550940: step 130, loss 1.26909, acc 0.820312, prec 0.0277889, recall 0.740938
2017-12-10T00:05:59.468277: step 131, loss 0.990095, acc 0.794922, prec 0.0280608, recall 0.742887
2017-12-10T00:06:00.391215: step 132, loss 1.40403, acc 0.777344, prec 0.0280903, recall 0.742408
2017-12-10T00:06:01.317147: step 133, loss 1.66353, acc 0.744141, prec 0.0282156, recall 0.742739
2017-12-10T00:06:02.224451: step 134, loss 1.67146, acc 0.738281, prec 0.0283363, recall 0.743063
2017-12-10T00:06:03.132827: step 135, loss 1.35271, acc 0.703125, prec 0.0283588, recall 0.743616
2017-12-10T00:06:04.039749: step 136, loss 2.66085, acc 0.666016, prec 0.0285482, recall 0.745455
2017-12-10T00:06:04.945475: step 137, loss 2.06748, acc 0.669922, prec 0.0284759, recall 0.745473
2017-12-10T00:06:05.881411: step 138, loss 2.04266, acc 0.677734, prec 0.0285583, recall 0.745763
2017-12-10T00:06:06.795694: step 139, loss 1.42043, acc 0.660156, prec 0.0286646, recall 0.747774
2017-12-10T00:06:07.697069: step 140, loss 1.9431, acc 0.652344, prec 0.028693, recall 0.748527
2017-12-10T00:06:08.594424: step 141, loss 2.00592, acc 0.626953, prec 0.0286708, recall 0.749023
2017-12-10T00:06:09.505925: step 142, loss 1.42236, acc 0.722656, prec 0.028701, recall 0.749515
2017-12-10T00:06:10.409480: step 143, loss 1.88287, acc 0.712891, prec 0.0290128, recall 0.751916
2017-12-10T00:06:11.322604: step 144, loss 1.0096, acc 0.742188, prec 0.0289791, recall 0.752627
2017-12-10T00:06:12.240144: step 145, loss 1.30427, acc 0.744141, prec 0.0290179, recall 0.753802
2017-12-10T00:06:13.147645: step 146, loss 1.79716, acc 0.792969, prec 0.0292971, recall 0.75493
2017-12-10T00:06:14.052036: step 147, loss 1.12355, acc 0.769531, prec 0.0293136, recall 0.75514
2017-12-10T00:06:14.966120: step 148, loss 2.35631, acc 0.808594, prec 0.0293873, recall 0.754875
2017-12-10T00:06:15.869759: step 149, loss 2.1192, acc 0.796875, prec 0.0296998, recall 0.755495
2017-12-10T00:06:16.774937: step 150, loss 1.19985, acc 0.806641, prec 0.02984, recall 0.755677
2017-12-10T00:06:17.689458: step 151, loss 1.12524, acc 0.767578, prec 0.0297489, recall 0.755213
2017-12-10T00:06:18.592521: step 152, loss 0.861037, acc 0.773438, prec 0.0297987, recall 0.756318
2017-12-10T00:06:19.513921: step 153, loss 1.12252, acc 0.738281, prec 0.0297271, recall 0.756076
2017-12-10T00:06:20.420765: step 154, loss 1.14007, acc 0.742188, prec 0.0298292, recall 0.756926
2017-12-10T00:06:21.342686: step 155, loss 1.11257, acc 0.742188, prec 0.0299632, recall 0.758651
2017-12-10T00:06:22.252760: step 156, loss 1.25786, acc 0.773438, prec 0.0300799, recall 0.759471
2017-12-10T00:06:23.158747: step 157, loss 0.797233, acc 0.779297, prec 0.0300629, recall 0.760105
2017-12-10T00:06:24.066551: step 158, loss 1.27596, acc 0.763672, prec 0.0302737, recall 0.761532
2017-12-10T00:06:24.984374: step 159, loss 0.843427, acc 0.8125, prec 0.030442, recall 0.762522
2017-12-10T00:06:25.889175: step 160, loss 1.02911, acc 0.800781, prec 0.0306361, recall 0.763699
2017-12-10T00:06:26.797700: step 161, loss 1.74848, acc 0.777344, prec 0.0307177, recall 0.763605
2017-12-10T00:06:27.710431: step 162, loss 1.10353, acc 0.794922, prec 0.030742, recall 0.763113
2017-12-10T00:06:28.614723: step 163, loss 2.43461, acc 0.761719, prec 0.030881, recall 0.762783
2017-12-10T00:06:29.527030: step 164, loss 0.83784, acc 0.757812, prec 0.0308826, recall 0.763576
2017-12-10T00:06:30.451405: step 165, loss 1.64188, acc 0.779297, prec 0.0309955, recall 0.763682
2017-12-10T00:06:31.361636: step 166, loss 1.48884, acc 0.712891, prec 0.0312008, recall 0.765189
2017-12-10T00:06:32.267583: step 167, loss 2.92296, acc 0.740234, prec 0.0312271, recall 0.764274
2017-12-10T00:06:33.172801: step 168, loss 1.65838, acc 0.699219, prec 0.0312935, recall 0.764992
2017-12-10T00:06:34.071937: step 169, loss 2.9622, acc 0.683594, prec 0.0312253, recall 0.76371
2017-12-10T00:06:34.981441: step 170, loss 1.46494, acc 0.613281, prec 0.0312132, recall 0.764848
2017-12-10T00:06:35.911438: step 171, loss 1.52319, acc 0.613281, prec 0.0312957, recall 0.766534
2017-12-10T00:06:36.816803: step 172, loss 2.1286, acc 0.632812, prec 0.0312631, recall 0.766852
2017-12-10T00:06:37.729411: step 173, loss 1.52815, acc 0.59375, prec 0.031241, recall 0.767956
2017-12-10T00:06:38.632654: step 174, loss 1.75558, acc 0.642578, prec 0.0312141, recall 0.768264
2017-12-10T00:06:39.543369: step 175, loss 2.1629, acc 0.691406, prec 0.0312758, recall 0.767732
2017-12-10T00:06:40.461368: step 176, loss 1.19931, acc 0.705078, prec 0.0313724, recall 0.768576
2017-12-10T00:06:41.367705: step 177, loss 1.05583, acc 0.712891, prec 0.0313492, recall 0.76929
2017-12-10T00:06:42.273886: step 178, loss 1.25079, acc 0.736328, prec 0.0313694, recall 0.769585
2017-12-10T00:06:43.184982: step 179, loss 1.08305, acc 0.785156, prec 0.0314138, recall 0.769878
2017-12-10T00:06:44.097326: step 180, loss 1.63879, acc 0.800781, prec 0.0314657, recall 0.770167
2017-12-10T00:06:45.015763: step 181, loss 0.571162, acc 0.845703, prec 0.0315088, recall 0.770865
2017-12-10T00:06:45.920297: step 182, loss 3.33558, acc 0.849609, prec 0.0316184, recall 0.768999
2017-12-10T00:06:46.836749: step 183, loss 1.6695, acc 0.810547, prec 0.0317049, recall 0.768886
2017-12-10T00:06:47.741293: step 184, loss 0.912398, acc 0.804688, prec 0.0316679, recall 0.768657
2017-12-10T00:06:48.637456: step 185, loss 0.626845, acc 0.802734, prec 0.0316293, recall 0.769001
2017-12-10T00:06:49.552031: step 186, loss 1.34168, acc 0.765625, prec 0.0317509, recall 0.7698
2017-12-10T00:06:50.465621: step 187, loss 1.34131, acc 0.757812, prec 0.0318098, recall 0.769684
2017-12-10T00:06:51.383217: step 188, loss 0.845892, acc 0.757812, prec 0.0317782, recall 0.770191
2017-12-10T00:06:51.561250: step 189, loss 0.742592, acc 0.764706, prec 0.0317667, recall 0.770191
2017-12-10T00:06:52.474735: step 190, loss 0.809019, acc 0.757812, prec 0.0317355, recall 0.770696
2017-12-10T00:06:53.379404: step 191, loss 0.860464, acc 0.78125, prec 0.0318033, recall 0.771699
2017-12-10T00:06:54.288259: step 192, loss 0.970388, acc 0.804688, prec 0.031884, recall 0.771574
2017-12-10T00:06:55.191065: step 193, loss 1.33198, acc 0.8125, prec 0.0320268, recall 0.771223
2017-12-10T00:06:56.105737: step 194, loss 3.19152, acc 0.794922, prec 0.0320727, recall 0.770938
2017-12-10T00:06:57.008644: step 195, loss 0.423168, acc 0.861328, prec 0.0321776, recall 0.771917
2017-12-10T00:06:57.926087: step 196, loss 1.04613, acc 0.818359, prec 0.0322342, recall 0.771631
2017-12-10T00:06:58.834760: step 197, loss 0.680865, acc 0.824219, prec 0.032321, recall 0.772054
2017-12-10T00:06:59.736474: step 198, loss 0.910226, acc 0.804688, prec 0.0323977, recall 0.772472
2017-12-10T00:07:00.663594: step 199, loss 1.41937, acc 0.796875, prec 0.032528, recall 0.772664
2017-12-10T00:07:01.572235: step 200, loss 0.618658, acc 0.822266, prec 0.0326679, recall 0.773925
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_0/1512882237/checkpoints/model-200

2017-12-10T00:07:03.583670: step 201, loss 0.921482, acc 0.814453, prec 0.0327476, recall 0.774327
2017-12-10T00:07:04.493076: step 202, loss 1.42372, acc 0.808594, prec 0.0328532, recall 0.774348
2017-12-10T00:07:05.394948: step 203, loss 1.38499, acc 0.748047, prec 0.0329565, recall 0.774523
2017-12-10T00:07:06.323535: step 204, loss 0.676263, acc 0.734375, prec 0.0329109, recall 0.774983
2017-12-10T00:07:07.235587: step 205, loss 1.27512, acc 0.740234, prec 0.0330372, recall 0.775304
2017-12-10T00:07:08.137253: step 206, loss 0.901954, acc 0.740234, prec 0.0330775, recall 0.77621
2017-12-10T00:07:09.043283: step 207, loss 1.21402, acc 0.708984, prec 0.0330206, recall 0.776139
2017-12-10T00:07:09.950902: step 208, loss 0.748723, acc 0.755859, prec 0.0329583, recall 0.776439
2017-12-10T00:07:10.872498: step 209, loss 1.3859, acc 0.734375, prec 0.0328871, recall 0.776219
2017-12-10T00:07:11.776051: step 210, loss 0.809614, acc 0.832031, prec 0.0330257, recall 0.777409
2017-12-10T00:07:12.677169: step 211, loss 2.65486, acc 0.804688, prec 0.0331532, recall 0.777045
2017-12-10T00:07:13.578113: step 212, loss 1.20977, acc 0.783203, prec 0.0333502, recall 0.777632
2017-12-10T00:07:14.491449: step 213, loss 0.710176, acc 0.775391, prec 0.0333781, recall 0.778357
2017-12-10T00:07:15.395614: step 214, loss 0.794413, acc 0.777344, prec 0.0334875, recall 0.779507
2017-12-10T00:07:16.311817: step 215, loss 1.37928, acc 0.832031, prec 0.0336501, recall 0.780284
2017-12-10T00:07:17.210408: step 216, loss 0.665703, acc 0.832031, prec 0.0336244, recall 0.780064
2017-12-10T00:07:18.126799: step 217, loss 0.570604, acc 0.816406, prec 0.0336172, recall 0.780488
2017-12-10T00:07:19.042083: step 218, loss 0.799698, acc 0.802734, prec 0.033711, recall 0.780971
2017-12-10T00:07:19.953978: step 219, loss 1.07486, acc 0.785156, prec 0.0338224, recall 0.781587
2017-12-10T00:07:20.883866: step 220, loss 0.710782, acc 0.808594, prec 0.0338639, recall 0.782278
2017-12-10T00:07:21.786640: step 221, loss 0.559528, acc 0.855469, prec 0.0339803, recall 0.783239
2017-12-10T00:07:22.702077: step 222, loss 0.613564, acc 0.824219, prec 0.0341076, recall 0.784326
2017-12-10T00:07:23.616428: step 223, loss 0.441451, acc 0.871094, prec 0.0342566, recall 0.785402
2017-12-10T00:07:24.521165: step 224, loss 1.48514, acc 0.851562, prec 0.034214, recall 0.784558
2017-12-10T00:07:25.435089: step 225, loss 0.56857, acc 0.908203, prec 0.0343285, recall 0.784873
2017-12-10T00:07:26.344636: step 226, loss 0.673993, acc 0.892578, prec 0.0343306, recall 0.784653
2017-12-10T00:07:27.259203: step 227, loss 0.420426, acc 0.884766, prec 0.0344063, recall 0.785318
2017-12-10T00:07:28.162878: step 228, loss 0.449545, acc 0.888672, prec 0.0344846, recall 0.785495
2017-12-10T00:07:29.067720: step 229, loss 1.27015, acc 0.892578, prec 0.0344874, recall 0.784795
2017-12-10T00:07:29.977349: step 230, loss 0.626027, acc 0.888672, prec 0.0345393, recall 0.784841
2017-12-10T00:07:30.897512: step 231, loss 0.844396, acc 0.853516, prec 0.0346272, recall 0.784672
2017-12-10T00:07:31.815493: step 232, loss 2.36623, acc 0.880859, prec 0.0348061, recall 0.78442
2017-12-10T00:07:32.725156: step 233, loss 1.57135, acc 0.798828, prec 0.0348927, recall 0.784384
2017-12-10T00:07:33.652578: step 234, loss 0.681428, acc 0.796875, prec 0.0350531, recall 0.785672
2017-12-10T00:07:34.563151: step 235, loss 0.808304, acc 0.761719, prec 0.0351445, recall 0.78669
2017-12-10T00:07:35.469486: step 236, loss 0.980588, acc 0.746094, prec 0.0352023, recall 0.787574
2017-12-10T00:07:36.388225: step 237, loss 1.19351, acc 0.681641, prec 0.0352808, recall 0.788235
2017-12-10T00:07:37.288622: step 238, loss 1.12008, acc 0.689453, prec 0.0353361, recall 0.789227
2017-12-10T00:07:38.191608: step 239, loss 1.30567, acc 0.679688, prec 0.0353116, recall 0.789382
2017-12-10T00:07:39.096315: step 240, loss 0.930833, acc 0.726562, prec 0.0353837, recall 0.79036
2017-12-10T00:07:40.013634: step 241, loss 0.794793, acc 0.767578, prec 0.0354246, recall 0.791088
2017-12-10T00:07:40.931410: step 242, loss 1.48351, acc 0.748047, prec 0.0355317, recall 0.791715
2017-12-10T00:07:41.854748: step 243, loss 0.992936, acc 0.771484, prec 0.0355745, recall 0.791977
2017-12-10T00:07:42.771507: step 244, loss 0.903375, acc 0.796875, prec 0.0356538, recall 0.792356
2017-12-10T00:07:43.681763: step 245, loss 1.24043, acc 0.808594, prec 0.0358121, recall 0.793084
2017-12-10T00:07:44.589825: step 246, loss 0.74995, acc 0.845703, prec 0.03606, recall 0.794598
2017-12-10T00:07:45.490172: step 247, loss 0.920028, acc 0.845703, prec 0.0360865, recall 0.794613
2017-12-10T00:07:46.412593: step 248, loss 0.784389, acc 0.857422, prec 0.0362411, recall 0.795201
2017-12-10T00:07:47.332539: step 249, loss 1.652, acc 0.841797, prec 0.0363152, recall 0.795
2017-12-10T00:07:48.239005: step 250, loss 0.608648, acc 0.837891, prec 0.0364585, recall 0.79602
2017-12-10T00:07:49.138460: step 251, loss 0.60151, acc 0.791016, prec 0.0365303, recall 0.796806
2017-12-10T00:07:49.316238: step 252, loss 0.601506, acc 0.803922, prec 0.036521, recall 0.796806
2017-12-10T00:07:50.230018: step 253, loss 0.63628, acc 0.826172, prec 0.0365605, recall 0.797364
2017-12-10T00:07:51.157211: step 254, loss 0.814541, acc 0.824219, prec 0.0367208, recall 0.798035
2017-12-10T00:07:52.059190: step 255, loss 0.676036, acc 0.851562, prec 0.0369646, recall 0.799458
2017-12-10T00:07:52.968333: step 256, loss 0.557166, acc 0.847656, prec 0.0370852, recall 0.800324
2017-12-10T00:07:53.876669: step 257, loss 0.375132, acc 0.875, prec 0.0372182, recall 0.801182
2017-12-10T00:07:54.800192: step 258, loss 0.653721, acc 0.886719, prec 0.0372614, recall 0.801179
2017-12-10T00:07:55.714022: step 259, loss 0.986109, acc 0.869141, prec 0.0373439, recall 0.801388
2017-12-10T00:07:56.617822: step 260, loss 0.992153, acc 0.882812, prec 0.0373848, recall 0.801384
2017-12-10T00:07:57.528556: step 261, loss 0.430783, acc 0.873047, prec 0.03742, recall 0.801807
2017-12-10T00:07:58.432680: step 262, loss 0.578414, acc 0.865234, prec 0.0375944, recall 0.802854
2017-12-10T00:07:59.340476: step 263, loss 1.10163, acc 0.835938, prec 0.0376133, recall 0.802424
2017-12-10T00:08:00.257528: step 264, loss 0.54953, acc 0.837891, prec 0.0376787, recall 0.803046
2017-12-10T00:08:01.188817: step 265, loss 1.12908, acc 0.871094, prec 0.0377368, recall 0.803141
2017-12-10T00:08:02.104610: step 266, loss 0.59903, acc 0.855469, prec 0.0379281, recall 0.804269
2017-12-10T00:08:03.006753: step 267, loss 0.497589, acc 0.871094, prec 0.0380318, recall 0.804979
2017-12-10T00:08:03.911794: step 268, loss 0.739884, acc 0.849609, prec 0.0381257, recall 0.805269
2017-12-10T00:08:04.814081: step 269, loss 0.77554, acc 0.835938, prec 0.0381659, recall 0.805355
2017-12-10T00:08:05.735768: step 270, loss 0.595584, acc 0.839844, prec 0.0383473, recall 0.806452
2017-12-10T00:08:06.630705: step 271, loss 0.829955, acc 0.833984, prec 0.0383158, recall 0.806237
2017-12-10T00:08:07.528915: step 272, loss 0.460643, acc 0.855469, prec 0.0383869, recall 0.80683
2017-12-10T00:08:08.426327: step 273, loss 0.519986, acc 0.847656, prec 0.0384308, recall 0.807321
2017-12-10T00:08:09.331861: step 274, loss 0.439733, acc 0.869141, prec 0.0385777, recall 0.808198
2017-12-10T00:08:10.256460: step 275, loss 0.66826, acc 0.861328, prec 0.0386517, recall 0.808371
2017-12-10T00:08:11.169859: step 276, loss 0.437511, acc 0.875, prec 0.0386847, recall 0.808757
2017-12-10T00:08:12.074616: step 277, loss 0.686792, acc 0.865234, prec 0.038737, recall 0.808831
2017-12-10T00:08:12.985374: step 278, loss 0.322937, acc 0.894531, prec 0.0388713, recall 0.809595
2017-12-10T00:08:13.900916: step 279, loss 0.541362, acc 0.908203, prec 0.0389206, recall 0.809571
2017-12-10T00:08:14.810621: step 280, loss 0.526256, acc 0.884766, prec 0.0390045, recall 0.809737
2017-12-10T00:08:15.717630: step 281, loss 1.90337, acc 0.898438, prec 0.0390737, recall 0.809005
2017-12-10T00:08:16.631778: step 282, loss 0.425194, acc 0.908203, prec 0.0392821, recall 0.810039
2017-12-10T00:08:17.533496: step 283, loss 1.45668, acc 0.865234, prec 0.0393568, recall 0.809804
2017-12-10T00:08:18.440021: step 284, loss 0.999023, acc 0.837891, prec 0.0393942, recall 0.809873
2017-12-10T00:08:19.350951: step 285, loss 0.65376, acc 0.830078, prec 0.0394269, recall 0.810336
2017-12-10T00:08:20.259433: step 286, loss 1.12804, acc 0.814453, prec 0.039521, recall 0.81068
2017-12-10T00:08:21.180112: step 287, loss 0.712333, acc 0.759766, prec 0.039497, recall 0.811047
2017-12-10T00:08:22.085165: step 288, loss 0.891595, acc 0.753906, prec 0.0395164, recall 0.811202
2017-12-10T00:08:22.991985: step 289, loss 1.15709, acc 0.777344, prec 0.0395243, recall 0.811266
2017-12-10T00:08:23.895724: step 290, loss 0.792439, acc 0.746094, prec 0.0395389, recall 0.81181
2017-12-10T00:08:24.813373: step 291, loss 0.610078, acc 0.783203, prec 0.0395262, recall 0.812171
2017-12-10T00:08:25.715137: step 292, loss 0.730436, acc 0.798828, prec 0.0397442, recall 0.813422
2017-12-10T00:08:26.627642: step 293, loss 1.01963, acc 0.804688, prec 0.0397866, recall 0.813567
2017-12-10T00:08:27.545862: step 294, loss 0.661588, acc 0.804688, prec 0.0398723, recall 0.814272
2017-12-10T00:08:28.454969: step 295, loss 0.651613, acc 0.816406, prec 0.0398531, recall 0.814151
2017-12-10T00:08:29.356745: step 296, loss 0.818473, acc 0.808594, prec 0.0398968, recall 0.814292
2017-12-10T00:08:30.280429: step 297, loss 0.936053, acc 0.876953, prec 0.0399512, recall 0.813964
2017-12-10T00:08:31.198025: step 298, loss 0.778007, acc 0.890625, prec 0.0401433, recall 0.814539
2017-12-10T00:08:32.107713: step 299, loss 0.429232, acc 0.908203, prec 0.0403202, recall 0.815399
2017-12-10T00:08:33.006205: step 300, loss 0.627452, acc 0.869141, prec 0.0403471, recall 0.815363

Evaluation:
2017-12-10T00:08:37.738399: step 300, loss 1.77689, acc 0.903104, prec 0.0415418, recall 0.79914

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_0/1512882237/checkpoints/model-300

2017-12-10T00:08:39.749691: step 301, loss 0.595641, acc 0.853516, prec 0.0415374, recall 0.799055
2017-12-10T00:08:40.663372: step 302, loss 0.993224, acc 0.871094, prec 0.0416704, recall 0.799145
2017-12-10T00:08:41.591762: step 303, loss 0.425398, acc 0.84375, prec 0.0416602, recall 0.799402
2017-12-10T00:08:42.500719: step 304, loss 0.482759, acc 0.851562, prec 0.0417175, recall 0.799915
2017-12-10T00:08:43.408451: step 305, loss 0.479905, acc 0.859375, prec 0.0418634, recall 0.800763
2017-12-10T00:08:44.310245: step 306, loss 0.63145, acc 0.857422, prec 0.0419027, recall 0.800846
2017-12-10T00:08:45.215604: step 307, loss 0.469837, acc 0.867188, prec 0.0420936, recall 0.801851
2017-12-10T00:08:46.124439: step 308, loss 0.550727, acc 0.869141, prec 0.0422427, recall 0.802681
2017-12-10T00:08:47.029792: step 309, loss 0.593125, acc 0.880859, prec 0.0423757, recall 0.803422
2017-12-10T00:08:47.941730: step 310, loss 0.742096, acc 0.857422, prec 0.0424139, recall 0.803497
2017-12-10T00:08:48.852262: step 311, loss 0.539231, acc 0.867188, prec 0.0425186, recall 0.804149
2017-12-10T00:08:49.763572: step 312, loss 1.06832, acc 0.878906, prec 0.0425667, recall 0.804222
2017-12-10T00:08:50.687750: step 313, loss 1.02309, acc 0.869141, prec 0.0426109, recall 0.803962
2017-12-10T00:08:51.591838: step 314, loss 1.79744, acc 0.828125, prec 0.042739, recall 0.804437
2017-12-10T00:08:51.769956: step 315, loss 0.585356, acc 0.823529, prec 0.0427306, recall 0.804437
2017-12-10T00:08:52.704477: step 316, loss 0.520638, acc 0.835938, prec 0.0427983, recall 0.804998
2017-12-10T00:08:53.619766: step 317, loss 0.492425, acc 0.851562, prec 0.0428109, recall 0.805317
2017-12-10T00:08:54.533695: step 318, loss 0.61133, acc 0.791016, prec 0.0429814, recall 0.806347
2017-12-10T00:08:55.455161: step 319, loss 0.464921, acc 0.851562, prec 0.043035, recall 0.806818
2017-12-10T00:08:56.368887: step 320, loss 0.453652, acc 0.841797, prec 0.0431044, recall 0.807365
2017-12-10T00:08:57.277441: step 321, loss 1.05441, acc 0.84375, prec 0.0432786, recall 0.807971
2017-12-10T00:08:58.180227: step 322, loss 0.630679, acc 0.867188, prec 0.0433191, recall 0.808032
2017-12-10T00:08:59.080827: step 323, loss 0.606463, acc 0.847656, prec 0.043453, recall 0.808477
2017-12-10T00:08:59.985756: step 324, loss 0.437214, acc 0.876953, prec 0.0434969, recall 0.808859
2017-12-10T00:09:00.907446: step 325, loss 0.677965, acc 0.851562, prec 0.0436924, recall 0.809845
2017-12-10T00:09:01.818844: step 326, loss 0.866201, acc 0.90625, prec 0.0438326, recall 0.810202
2017-12-10T00:09:02.717769: step 327, loss 0.305726, acc 0.898438, prec 0.0438861, recall 0.810576
2017-12-10T00:09:03.618722: step 328, loss 0.564445, acc 0.869141, prec 0.0439457, recall 0.811024
2017-12-10T00:09:04.527944: step 329, loss 0.372885, acc 0.890625, prec 0.0439951, recall 0.811395
2017-12-10T00:09:05.431343: step 330, loss 0.385437, acc 0.892578, prec 0.0441267, recall 0.81206
2017-12-10T00:09:06.348678: step 331, loss 0.548974, acc 0.900391, prec 0.0441408, recall 0.811962
2017-12-10T00:09:07.256014: step 332, loss 0.891047, acc 0.894531, prec 0.0442341, recall 0.811843
2017-12-10T00:09:08.160370: step 333, loss 0.348206, acc 0.882812, prec 0.0444209, recall 0.812718
2017-12-10T00:09:09.084170: step 334, loss 0.44832, acc 0.880859, prec 0.0444849, recall 0.813153
2017-12-10T00:09:09.989051: step 335, loss 1.01535, acc 0.890625, prec 0.0445342, recall 0.8132
2017-12-10T00:09:10.897289: step 336, loss 0.674146, acc 0.871094, prec 0.0446133, recall 0.813703
2017-12-10T00:09:11.804860: step 337, loss 1.23687, acc 0.910156, prec 0.0447925, recall 0.814176
2017-12-10T00:09:12.720798: step 338, loss 0.546983, acc 0.869141, prec 0.0450107, recall 0.815168
2017-12-10T00:09:13.627723: step 339, loss 0.481682, acc 0.835938, prec 0.0450517, recall 0.815589
2017-12-10T00:09:14.534852: step 340, loss 0.887273, acc 0.810547, prec 0.0451411, recall 0.815909
2017-12-10T00:09:15.443552: step 341, loss 1.17629, acc 0.792969, prec 0.0452217, recall 0.816226
2017-12-10T00:09:16.358462: step 342, loss 0.589965, acc 0.798828, prec 0.0453635, recall 0.817055
2017-12-10T00:09:17.263387: step 343, loss 0.548234, acc 0.818359, prec 0.0453949, recall 0.817466
2017-12-10T00:09:18.169819: step 344, loss 0.790938, acc 0.796875, prec 0.0454753, recall 0.81808
2017-12-10T00:09:19.090919: step 345, loss 0.532235, acc 0.839844, prec 0.0455365, recall 0.818554
2017-12-10T00:09:19.997067: step 346, loss 0.648136, acc 0.822266, prec 0.0456087, recall 0.819094
2017-12-10T00:09:20.916483: step 347, loss 0.497755, acc 0.828125, prec 0.0456244, recall 0.819429
2017-12-10T00:09:21.827569: step 348, loss 0.703819, acc 0.837891, prec 0.0456456, recall 0.81946
2017-12-10T00:09:22.750489: step 349, loss 0.361275, acc 0.871094, prec 0.0456426, recall 0.81966
2017-12-10T00:09:23.675711: step 350, loss 0.321281, acc 0.884766, prec 0.0457441, recall 0.820192
2017-12-10T00:09:24.586095: step 351, loss 0.339172, acc 0.90625, prec 0.0458949, recall 0.820852
2017-12-10T00:09:25.490554: step 352, loss 0.320768, acc 0.925781, prec 0.0459374, recall 0.821114
2017-12-10T00:09:26.389068: step 353, loss 0.447143, acc 0.929688, prec 0.0459827, recall 0.821076
2017-12-10T00:09:27.287627: step 354, loss 0.804754, acc 0.9375, prec 0.046013, recall 0.820672
2017-12-10T00:09:28.197884: step 355, loss 0.610737, acc 0.921875, prec 0.0460358, recall 0.82027
2017-12-10T00:09:29.107827: step 356, loss 1.02611, acc 0.945312, prec 0.0461089, recall 0.82
2017-12-10T00:09:30.010914: step 357, loss 0.287143, acc 0.917969, prec 0.0461667, recall 0.820327
2017-12-10T00:09:30.928900: step 358, loss 0.500746, acc 0.910156, prec 0.0461827, recall 0.820225
2017-12-10T00:09:31.836658: step 359, loss 0.923367, acc 0.898438, prec 0.0462523, recall 0.820022
2017-12-10T00:09:32.758937: step 360, loss 0.414621, acc 0.892578, prec 0.0464917, recall 0.820992
2017-12-10T00:09:33.676373: step 361, loss 0.426043, acc 0.865234, prec 0.0465622, recall 0.821441
2017-12-10T00:09:34.578709: step 362, loss 0.62045, acc 0.826172, prec 0.0465748, recall 0.821761
2017-12-10T00:09:35.478346: step 363, loss 0.843718, acc 0.822266, prec 0.0466058, recall 0.821849
2017-12-10T00:09:36.398619: step 364, loss 0.690403, acc 0.791016, prec 0.0466014, recall 0.822167
2017-12-10T00:09:37.298078: step 365, loss 0.77792, acc 0.818359, prec 0.0466495, recall 0.822317
2017-12-10T00:09:38.208364: step 366, loss 0.628078, acc 0.800781, prec 0.0466688, recall 0.822695
2017-12-10T00:09:39.114203: step 367, loss 0.528882, acc 0.830078, prec 0.0467021, recall 0.823071
2017-12-10T00:09:40.015814: step 368, loss 0.572738, acc 0.820312, prec 0.0467307, recall 0.823446
2017-12-10T00:09:40.930215: step 369, loss 0.522208, acc 0.865234, prec 0.0467424, recall 0.823695
2017-12-10T00:09:41.841502: step 370, loss 0.457125, acc 0.871094, prec 0.046776, recall 0.824006
2017-12-10T00:09:42.762002: step 371, loss 0.360648, acc 0.880859, prec 0.0468332, recall 0.824377
2017-12-10T00:09:43.668611: step 372, loss 0.675075, acc 0.923828, prec 0.0469688, recall 0.824641
2017-12-10T00:09:44.578615: step 373, loss 0.710481, acc 0.910156, prec 0.0470026, recall 0.824598
2017-12-10T00:09:45.482779: step 374, loss 0.242885, acc 0.935547, prec 0.0470286, recall 0.824782
2017-12-10T00:09:46.395827: step 375, loss 0.433657, acc 0.910156, prec 0.0470434, recall 0.824678
2017-12-10T00:09:47.307965: step 376, loss 0.484372, acc 0.908203, prec 0.0471698, recall 0.825226
2017-12-10T00:09:48.233819: step 377, loss 0.354382, acc 0.910156, prec 0.047278, recall 0.82571
2017-12-10T00:09:48.414851: step 378, loss 1.4789, acc 0.882353, prec 0.0473102, recall 0.825831
2017-12-10T00:09:49.336502: step 379, loss 0.347265, acc 0.904297, prec 0.0473964, recall 0.826252
2017-12-10T00:09:50.253005: step 380, loss 0.376609, acc 0.886719, prec 0.0474739, recall 0.826671
2017-12-10T00:09:51.194476: step 381, loss 0.472762, acc 0.888672, prec 0.0475334, recall 0.827029
2017-12-10T00:09:52.106636: step 382, loss 0.464974, acc 0.857422, prec 0.0475777, recall 0.827385
2017-12-10T00:09:53.019860: step 383, loss 0.384514, acc 0.873047, prec 0.0476857, recall 0.827917
2017-12-10T00:09:53.924249: step 384, loss 0.529153, acc 0.841797, prec 0.0477783, recall 0.828445
2017-12-10T00:09:54.842404: step 385, loss 0.585197, acc 0.849609, prec 0.0478556, recall 0.828912
2017-12-10T00:09:55.749990: step 386, loss 0.432425, acc 0.824219, prec 0.0478645, recall 0.829202
2017-12-10T00:09:56.649733: step 387, loss 0.440559, acc 0.841797, prec 0.0479004, recall 0.829549
2017-12-10T00:09:57.546950: step 388, loss 0.479889, acc 0.873047, prec 0.048007, recall 0.830068
2017-12-10T00:09:58.452607: step 389, loss 0.485304, acc 0.861328, prec 0.0480148, recall 0.830297
2017-12-10T00:09:59.356010: step 390, loss 0.362408, acc 0.880859, prec 0.0480691, recall 0.83064
2017-12-10T00:10:00.280605: step 391, loss 0.56892, acc 0.871094, prec 0.0481925, recall 0.831208
2017-12-10T00:10:01.193949: step 392, loss 0.407511, acc 0.921875, prec 0.0482661, recall 0.831547
2017-12-10T00:10:02.108344: step 393, loss 0.253572, acc 0.919922, prec 0.0483016, recall 0.831773
2017-12-10T00:10:03.011913: step 394, loss 0.54756, acc 0.914062, prec 0.0483361, recall 0.831442
2017-12-10T00:10:03.927122: step 395, loss 1.2659, acc 0.923828, prec 0.0484296, recall 0.831558
2017-12-10T00:10:04.843002: step 396, loss 1.19656, acc 0.927734, prec 0.0484337, recall 0.831117
2017-12-10T00:10:05.777696: step 397, loss 0.315803, acc 0.921875, prec 0.0485435, recall 0.831565
2017-12-10T00:10:06.692674: step 398, loss 0.912927, acc 0.890625, prec 0.0487135, recall 0.831683
2017-12-10T00:10:07.599853: step 399, loss 0.395266, acc 0.896484, prec 0.0487923, recall 0.832071
2017-12-10T00:10:08.506780: step 400, loss 0.453616, acc 0.865234, prec 0.0488741, recall 0.832512
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_0/1512882237/checkpoints/model-400

2017-12-10T00:10:10.410286: step 401, loss 0.532017, acc 0.896484, prec 0.0490256, recall 0.833115
2017-12-10T00:10:11.322023: step 402, loss 0.626289, acc 0.853516, prec 0.0491204, recall 0.833333
2017-12-10T00:10:12.225345: step 403, loss 0.466572, acc 0.867188, prec 0.0492024, recall 0.833767
2017-12-10T00:10:13.126538: step 404, loss 0.387849, acc 0.867188, prec 0.0491747, recall 0.833875
2017-12-10T00:10:14.034308: step 405, loss 0.461418, acc 0.84375, prec 0.049154, recall 0.834037
2017-12-10T00:10:14.938983: step 406, loss 0.310238, acc 0.878906, prec 0.0491866, recall 0.834306
2017-12-10T00:10:15.849679: step 407, loss 0.338265, acc 0.884766, prec 0.049222, recall 0.834574
2017-12-10T00:10:16.759976: step 408, loss 0.329712, acc 0.919922, prec 0.0493286, recall 0.835002
2017-12-10T00:10:17.666615: step 409, loss 0.659046, acc 0.896484, prec 0.0494789, recall 0.835317
2017-12-10T00:10:18.577857: step 410, loss 0.240882, acc 0.925781, prec 0.0495336, recall 0.835581
2017-12-10T00:10:19.501308: step 411, loss 0.249865, acc 0.912109, prec 0.0495816, recall 0.835845
2017-12-10T00:10:20.410213: step 412, loss 0.237034, acc 0.939453, prec 0.0496427, recall 0.836108
2017-12-10T00:10:21.337534: step 413, loss 0.428552, acc 0.935547, prec 0.0497379, recall 0.836474
2017-12-10T00:10:22.248815: step 414, loss 0.859936, acc 0.935547, prec 0.0498529, recall 0.836358
2017-12-10T00:10:23.156260: step 415, loss 0.772885, acc 0.923828, prec 0.0499251, recall 0.836404
2017-12-10T00:10:24.061410: step 416, loss 0.316106, acc 0.900391, prec 0.0500028, recall 0.836767
2017-12-10T00:10:24.969629: step 417, loss 0.396911, acc 0.884766, prec 0.0501447, recall 0.837334
2017-12-10T00:10:25.888000: step 418, loss 0.291355, acc 0.904297, prec 0.050188, recall 0.837591
2017-12-10T00:10:26.791925: step 419, loss 0.350874, acc 0.902344, prec 0.0502841, recall 0.837999
2017-12-10T00:10:27.698964: step 420, loss 0.474154, acc 0.865234, prec 0.0504156, recall 0.838558
2017-12-10T00:10:28.607237: step 421, loss 0.334273, acc 0.888672, prec 0.0505045, recall 0.838962
2017-12-10T00:10:29.502224: step 422, loss 0.380027, acc 0.855469, prec 0.0505413, recall 0.839263
2017-12-10T00:10:30.428673: step 423, loss 0.633548, acc 0.904297, prec 0.0505671, recall 0.839202
2017-12-10T00:10:31.348570: step 424, loss 0.295117, acc 0.892578, prec 0.0506039, recall 0.839452
2017-12-10T00:10:32.262043: step 425, loss 0.301266, acc 0.896484, prec 0.0506604, recall 0.839752
2017-12-10T00:10:33.165510: step 426, loss 0.33674, acc 0.929688, prec 0.0507862, recall 0.840198
2017-12-10T00:10:34.072465: step 427, loss 0.376248, acc 0.931641, prec 0.0508773, recall 0.840544
2017-12-10T00:10:34.975007: step 428, loss 0.495905, acc 0.910156, prec 0.0509586, recall 0.840629
2017-12-10T00:10:35.910957: step 429, loss 0.329292, acc 0.912109, prec 0.0510221, recall 0.840923
2017-12-10T00:10:36.817625: step 430, loss 0.240172, acc 0.925781, prec 0.0510922, recall 0.841216
2017-12-10T00:10:37.723602: step 431, loss 0.418066, acc 0.917969, prec 0.0511936, recall 0.841605
2017-12-10T00:10:38.629474: step 432, loss 0.78088, acc 0.923828, prec 0.0513694, recall 0.841929
2017-12-10T00:10:39.573239: step 433, loss 0.37956, acc 0.910156, prec 0.0514666, recall 0.842314
2017-12-10T00:10:40.473056: step 434, loss 0.684134, acc 0.916016, prec 0.0515322, recall 0.842345
2017-12-10T00:10:41.377490: step 435, loss 0.533118, acc 0.902344, prec 0.0515558, recall 0.842281
2017-12-10T00:10:42.292961: step 436, loss 0.387107, acc 0.873047, prec 0.0516343, recall 0.842663
2017-12-10T00:10:43.201525: step 437, loss 0.292146, acc 0.914062, prec 0.0516801, recall 0.8429
2017-12-10T00:10:44.117531: step 438, loss 0.801254, acc 0.871094, prec 0.0517933, recall 0.84312
2017-12-10T00:10:45.023675: step 439, loss 0.570553, acc 0.888672, prec 0.0518274, recall 0.843102
2017-12-10T00:10:45.931960: step 440, loss 0.396876, acc 0.869141, prec 0.0520082, recall 0.843759
2017-12-10T00:10:46.108987: step 441, loss 0.188109, acc 0.941176, prec 0.0520228, recall 0.843806
2017-12-10T00:10:47.016171: step 442, loss 0.33747, acc 0.904297, prec 0.0520632, recall 0.844039
2017-12-10T00:10:47.920270: step 443, loss 0.396787, acc 0.892578, prec 0.052185, recall 0.844504
2017-12-10T00:10:48.829015: step 444, loss 0.338387, acc 0.892578, prec 0.052289, recall 0.84492
2017-12-10T00:10:49.743960: step 445, loss 0.445476, acc 0.882812, prec 0.0523532, recall 0.845242
2017-12-10T00:10:50.653239: step 446, loss 0.357355, acc 0.912109, prec 0.0524665, recall 0.845653
2017-12-10T00:10:51.568700: step 447, loss 0.467943, acc 0.912109, prec 0.0526142, recall 0.846154
2017-12-10T00:10:52.469698: step 448, loss 0.228851, acc 0.921875, prec 0.0526104, recall 0.846244
2017-12-10T00:10:53.374527: step 449, loss 0.503701, acc 0.914062, prec 0.0526903, recall 0.846312
2017-12-10T00:10:54.283760: step 450, loss 0.201736, acc 0.939453, prec 0.0527817, recall 0.846628
2017-12-10T00:10:55.184605: step 451, loss 0.566576, acc 0.904297, prec 0.0528556, recall 0.846942
2017-12-10T00:10:56.093187: step 452, loss 0.251863, acc 0.935547, prec 0.0529793, recall 0.847344
2017-12-10T00:10:56.996445: step 453, loss 0.234525, acc 0.929688, prec 0.0530136, recall 0.847522
2017-12-10T00:10:57.904412: step 454, loss 0.316851, acc 0.921875, prec 0.0530957, recall 0.847832
2017-12-10T00:10:58.817843: step 455, loss 0.295313, acc 0.941406, prec 0.0532564, recall 0.848318
2017-12-10T00:10:59.747402: step 456, loss 0.44687, acc 0.921875, prec 0.0533564, recall 0.848423
2017-12-10T00:11:00.673058: step 457, loss 0.269052, acc 0.921875, prec 0.053438, recall 0.84873
2017-12-10T00:11:01.581794: step 458, loss 0.389365, acc 0.923828, prec 0.053658, recall 0.849382
2017-12-10T00:11:02.510869: step 459, loss 0.337024, acc 0.919922, prec 0.0538069, recall 0.849857
2017-12-10T00:11:03.423742: step 460, loss 0.33304, acc 0.925781, prec 0.0538728, recall 0.850114
2017-12-10T00:11:04.329832: step 461, loss 0.242883, acc 0.90625, prec 0.0538945, recall 0.850286
2017-12-10T00:11:05.244121: step 462, loss 0.348065, acc 0.925781, prec 0.0539944, recall 0.850627
2017-12-10T00:11:06.176512: step 463, loss 0.30553, acc 0.867188, prec 0.0540135, recall 0.85084
2017-12-10T00:11:07.085533: step 464, loss 0.637825, acc 0.917969, prec 0.0541263, recall 0.851221
2017-12-10T00:11:08.009395: step 465, loss 0.321771, acc 0.916016, prec 0.0542378, recall 0.8516
2017-12-10T00:11:08.916533: step 466, loss 0.303758, acc 0.912109, prec 0.0543302, recall 0.851936
2017-12-10T00:11:09.826164: step 467, loss 0.354786, acc 0.894531, prec 0.0543965, recall 0.852228
2017-12-10T00:11:10.734520: step 468, loss 0.256903, acc 0.929688, prec 0.0544464, recall 0.852436
2017-12-10T00:11:11.644317: step 469, loss 0.382564, acc 0.927734, prec 0.0545471, recall 0.852528
2017-12-10T00:11:12.543951: step 470, loss 0.369963, acc 0.914062, prec 0.0546059, recall 0.852776
2017-12-10T00:11:13.454066: step 471, loss 0.475892, acc 0.908203, prec 0.0546965, recall 0.852867
2017-12-10T00:11:14.369125: step 472, loss 0.235157, acc 0.939453, prec 0.0547339, recall 0.853032
2017-12-10T00:11:15.280639: step 473, loss 0.20129, acc 0.9375, prec 0.0547702, recall 0.853196
2017-12-10T00:11:16.177562: step 474, loss 0.516229, acc 0.921875, prec 0.054934, recall 0.853686
2017-12-10T00:11:17.085777: step 475, loss 0.219835, acc 0.916016, prec 0.0549763, recall 0.853889
2017-12-10T00:11:17.989671: step 476, loss 0.327609, acc 0.908203, prec 0.0550483, recall 0.854172
2017-12-10T00:11:18.888751: step 477, loss 0.220278, acc 0.933594, prec 0.0551499, recall 0.854495
2017-12-10T00:11:19.797945: step 478, loss 0.408197, acc 0.925781, prec 0.0552979, recall 0.854937
2017-12-10T00:11:20.711133: step 479, loss 0.302514, acc 0.902344, prec 0.0553665, recall 0.855216
2017-12-10T00:11:21.621019: step 480, loss 0.220216, acc 0.925781, prec 0.0554299, recall 0.855455
2017-12-10T00:11:22.522978: step 481, loss 0.308265, acc 0.9375, prec 0.0554656, recall 0.855614
2017-12-10T00:11:23.425020: step 482, loss 0.31493, acc 0.916016, prec 0.0555575, recall 0.85593
2017-12-10T00:11:24.332370: step 483, loss 0.170954, acc 0.949219, prec 0.0556326, recall 0.856166
2017-12-10T00:11:25.239972: step 484, loss 0.515355, acc 0.923828, prec 0.0556621, recall 0.85609
2017-12-10T00:11:26.144515: step 485, loss 0.26127, acc 0.931641, prec 0.055728, recall 0.856325
2017-12-10T00:11:27.065637: step 486, loss 0.815138, acc 0.919922, prec 0.0557732, recall 0.856054
2017-12-10T00:11:27.971155: step 487, loss 0.430344, acc 0.925781, prec 0.0558538, recall 0.856096
2017-12-10T00:11:28.890778: step 488, loss 0.233742, acc 0.914062, prec 0.0558938, recall 0.856291
2017-12-10T00:11:29.794044: step 489, loss 0.575533, acc 0.929688, prec 0.0560095, recall 0.856409
2017-12-10T00:11:30.719280: step 490, loss 0.262359, acc 0.910156, prec 0.0559973, recall 0.856486
2017-12-10T00:11:31.634874: step 491, loss 0.498021, acc 0.882812, prec 0.0560056, recall 0.85641
2017-12-10T00:11:32.548473: step 492, loss 0.404612, acc 0.880859, prec 0.0560453, recall 0.856642
2017-12-10T00:11:33.457359: step 493, loss 0.298298, acc 0.919922, prec 0.0561212, recall 0.856912
2017-12-10T00:11:34.375013: step 494, loss 0.696031, acc 0.914062, prec 0.0562448, recall 0.857066
2017-12-10T00:11:35.283606: step 495, loss 0.842689, acc 0.896484, prec 0.0563759, recall 0.857257
2017-12-10T00:11:36.196314: step 496, loss 0.295426, acc 0.914062, prec 0.0564152, recall 0.857448
2017-12-10T00:11:37.105479: step 497, loss 0.385067, acc 0.882812, prec 0.0564552, recall 0.857676
2017-12-10T00:11:38.007263: step 498, loss 0.463567, acc 0.849609, prec 0.0565112, recall 0.857979
2017-12-10T00:11:38.918275: step 499, loss 0.378803, acc 0.849609, prec 0.0565341, recall 0.858205
2017-12-10T00:11:39.828042: step 500, loss 0.534896, acc 0.855469, prec 0.0565764, recall 0.858468
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_0/1512882237/checkpoints/model-500

2017-12-10T00:11:41.683352: step 501, loss 0.384776, acc 0.857422, prec 0.0566854, recall 0.85888
2017-12-10T00:11:42.597064: step 502, loss 0.362281, acc 0.898438, prec 0.0567491, recall 0.85914
2017-12-10T00:11:43.502589: step 503, loss 0.321164, acc 0.892578, prec 0.0567768, recall 0.859326
2017-12-10T00:11:43.678045: step 504, loss 0.450374, acc 0.862745, prec 0.0567699, recall 0.859326
2017-12-10T00:11:44.591276: step 505, loss 0.303297, acc 0.9375, prec 0.056804, recall 0.859474
2017-12-10T00:11:45.498469: step 506, loss 0.412541, acc 0.925781, prec 0.0568494, recall 0.859432
2017-12-10T00:11:46.405802: step 507, loss 1.0098, acc 0.933594, prec 0.0568669, recall 0.859092
2017-12-10T00:11:47.316494: step 508, loss 0.523749, acc 0.945312, prec 0.0569549, recall 0.859125
2017-12-10T00:11:48.244125: step 509, loss 0.207164, acc 0.939453, prec 0.0570224, recall 0.859346
2017-12-10T00:11:49.147293: step 510, loss 0.546603, acc 0.947266, prec 0.0570948, recall 0.859342
2017-12-10T00:11:50.047589: step 511, loss 0.245022, acc 0.949219, prec 0.0572161, recall 0.859672
2017-12-10T00:11:50.993095: step 512, loss 0.287539, acc 0.958984, prec 0.0573096, recall 0.859927
2017-12-10T00:11:51.902260: step 513, loss 0.391044, acc 0.927734, prec 0.0574687, recall 0.860363
2017-12-10T00:11:52.813066: step 514, loss 0.306975, acc 0.933594, prec 0.0575001, recall 0.860507
2017-12-10T00:11:53.738031: step 515, loss 0.270594, acc 0.917969, prec 0.0575561, recall 0.860723
2017-12-10T00:11:54.650311: step 516, loss 0.199154, acc 0.927734, prec 0.0576332, recall 0.860975
2017-12-10T00:11:55.573238: step 517, loss 0.31176, acc 0.927734, prec 0.057759, recall 0.861333
2017-12-10T00:11:56.488755: step 518, loss 0.265154, acc 0.90625, prec 0.0578086, recall 0.861546
2017-12-10T00:11:57.390742: step 519, loss 0.237998, acc 0.919922, prec 0.0578327, recall 0.861688
2017-12-10T00:11:58.298954: step 520, loss 0.305117, acc 0.933594, prec 0.0579124, recall 0.861936
2017-12-10T00:11:59.206648: step 521, loss 0.468394, acc 0.935547, prec 0.0580587, recall 0.862104
2017-12-10T00:12:00.115351: step 522, loss 0.373569, acc 0.927734, prec 0.0581999, recall 0.86249
2017-12-10T00:12:01.034895: step 523, loss 0.215608, acc 0.925781, prec 0.0582913, recall 0.86277
2017-12-10T00:12:01.947125: step 524, loss 0.266886, acc 0.927734, prec 0.0583027, recall 0.862875
2017-12-10T00:12:02.854687: step 525, loss 0.16606, acc 0.955078, prec 0.0584251, recall 0.863187
2017-12-10T00:12:03.772556: step 526, loss 0.205658, acc 0.925781, prec 0.0584516, recall 0.863326
2017-12-10T00:12:04.683800: step 527, loss 0.348948, acc 0.955078, prec 0.0586059, recall 0.863705
2017-12-10T00:12:05.591994: step 528, loss 0.250045, acc 0.949219, prec 0.0587249, recall 0.864014
2017-12-10T00:12:06.505226: step 529, loss 0.269489, acc 0.931641, prec 0.0587863, recall 0.864219
2017-12-10T00:12:07.409265: step 530, loss 0.805867, acc 0.929688, prec 0.0589602, recall 0.864445
2017-12-10T00:12:08.320900: step 531, loss 0.292742, acc 0.933594, prec 0.059167, recall 0.864953
2017-12-10T00:12:09.227852: step 532, loss 0.261681, acc 0.935547, prec 0.059214, recall 0.865121
2017-12-10T00:12:10.148619: step 533, loss 0.255573, acc 0.896484, prec 0.0592407, recall 0.865289
2017-12-10T00:12:11.052819: step 534, loss 0.224513, acc 0.931641, prec 0.0592855, recall 0.865456
2017-12-10T00:12:11.960140: step 535, loss 0.406457, acc 0.921875, prec 0.0593092, recall 0.86559
2017-12-10T00:12:12.862465: step 536, loss 0.343972, acc 0.900391, prec 0.0593857, recall 0.865857
2017-12-10T00:12:13.759929: step 537, loss 0.286427, acc 0.921875, prec 0.0594413, recall 0.866056
2017-12-10T00:12:14.668780: step 538, loss 0.204964, acc 0.931641, prec 0.0595337, recall 0.866321
2017-12-10T00:12:15.576962: step 539, loss 0.536127, acc 0.902344, prec 0.0596119, recall 0.866371
2017-12-10T00:12:16.482641: step 540, loss 0.271476, acc 0.923828, prec 0.0597319, recall 0.866699
2017-12-10T00:12:17.394102: step 541, loss 0.197517, acc 0.929688, prec 0.059791, recall 0.866896
2017-12-10T00:12:18.298278: step 542, loss 0.219897, acc 0.929688, prec 0.0598819, recall 0.867157
2017-12-10T00:12:19.204913: step 543, loss 0.341575, acc 0.919922, prec 0.060047, recall 0.867579
2017-12-10T00:12:20.110446: step 544, loss 0.217651, acc 0.931641, prec 0.0601068, recall 0.867773
2017-12-10T00:12:21.035898: step 545, loss 0.239719, acc 0.9375, prec 0.0601219, recall 0.867869
2017-12-10T00:12:21.956413: step 546, loss 0.392554, acc 0.923828, prec 0.0601775, recall 0.868062
2017-12-10T00:12:22.872288: step 547, loss 0.253383, acc 0.951172, prec 0.0602631, recall 0.868287
2017-12-10T00:12:23.781396: step 548, loss 0.257433, acc 0.933594, prec 0.0603078, recall 0.868447
2017-12-10T00:12:24.682867: step 549, loss 0.312714, acc 0.943359, prec 0.0603733, recall 0.868638
2017-12-10T00:12:25.607007: step 550, loss 0.277039, acc 0.931641, prec 0.0604168, recall 0.868797
2017-12-10T00:12:26.520206: step 551, loss 0.179513, acc 0.939453, prec 0.0604327, recall 0.868892
2017-12-10T00:12:27.419967: step 552, loss 0.185887, acc 0.966797, prec 0.0605261, recall 0.869114
2017-12-10T00:12:28.328229: step 553, loss 0.400997, acc 0.947266, prec 0.0606417, recall 0.869188
2017-12-10T00:12:29.235787: step 554, loss 1.02751, acc 0.927734, prec 0.0607481, recall 0.869053
2017-12-10T00:12:30.144387: step 555, loss 0.872669, acc 0.929688, prec 0.0607912, recall 0.869002
2017-12-10T00:12:31.074617: step 556, loss 0.897392, acc 0.902344, prec 0.0608515, recall 0.869013
2017-12-10T00:12:31.981233: step 557, loss 0.517193, acc 0.888672, prec 0.0609045, recall 0.869025
2017-12-10T00:12:32.883015: step 558, loss 0.889759, acc 0.869141, prec 0.0609001, recall 0.868942
2017-12-10T00:12:33.794019: step 559, loss 0.519224, acc 0.84375, prec 0.0609285, recall 0.869161
2017-12-10T00:12:34.709313: step 560, loss 0.619747, acc 0.800781, prec 0.0609345, recall 0.869379
2017-12-10T00:12:35.624343: step 561, loss 0.488171, acc 0.814453, prec 0.0609632, recall 0.869627
2017-12-10T00:12:36.540177: step 562, loss 0.454241, acc 0.832031, prec 0.0610009, recall 0.869874
2017-12-10T00:12:37.443142: step 563, loss 0.427816, acc 0.841797, prec 0.0609969, recall 0.870028
2017-12-10T00:12:38.347313: step 564, loss 0.465098, acc 0.847656, prec 0.0610425, recall 0.870274
2017-12-10T00:12:39.257259: step 565, loss 0.610031, acc 0.835938, prec 0.0610985, recall 0.870344
2017-12-10T00:12:40.184143: step 566, loss 0.316313, acc 0.875, prec 0.061127, recall 0.870527
2017-12-10T00:12:40.363821: step 567, loss 0.489567, acc 0.862745, prec 0.061151, recall 0.870588
2017-12-10T00:12:41.277107: step 568, loss 0.236916, acc 0.939453, prec 0.0612437, recall 0.870831
2017-12-10T00:12:42.177833: step 569, loss 0.25664, acc 0.923828, prec 0.0612972, recall 0.871013
2017-12-10T00:12:43.077235: step 570, loss 0.292781, acc 0.927734, prec 0.0613837, recall 0.871255
2017-12-10T00:12:43.998336: step 571, loss 0.176005, acc 0.957031, prec 0.0615007, recall 0.871525
2017-12-10T00:12:44.902195: step 572, loss 0.161135, acc 0.953125, prec 0.0616001, recall 0.871765
2017-12-10T00:12:45.805519: step 573, loss 0.775136, acc 0.962891, prec 0.0616446, recall 0.871478
2017-12-10T00:12:46.710779: step 574, loss 0.103032, acc 0.96875, prec 0.061752, recall 0.871717
2017-12-10T00:12:47.638727: step 575, loss 0.245565, acc 0.982422, prec 0.0618509, recall 0.871926
2017-12-10T00:12:48.553157: step 576, loss 0.148407, acc 0.962891, prec 0.0618779, recall 0.872015
2017-12-10T00:12:49.454930: step 577, loss 0.333096, acc 0.972656, prec 0.0620025, recall 0.872281
2017-12-10T00:12:50.360659: step 578, loss 0.133902, acc 0.955078, prec 0.0620716, recall 0.872458
2017-12-10T00:12:51.281745: step 579, loss 0.281727, acc 0.958984, prec 0.0621128, recall 0.872375
2017-12-10T00:12:52.187661: step 580, loss 0.454171, acc 0.972656, prec 0.0622536, recall 0.872468
2017-12-10T00:12:53.100052: step 581, loss 0.432261, acc 0.939453, prec 0.0622999, recall 0.872414
2017-12-10T00:12:54.012900: step 582, loss 0.216603, acc 0.925781, prec 0.0623688, recall 0.872619
2017-12-10T00:12:54.921103: step 583, loss 0.453838, acc 0.923828, prec 0.0624672, recall 0.872881
2017-12-10T00:12:55.832161: step 584, loss 0.215527, acc 0.935547, prec 0.062587, recall 0.873172
2017-12-10T00:12:56.739770: step 585, loss 0.317173, acc 0.914062, prec 0.0626493, recall 0.873374
2017-12-10T00:12:57.647777: step 586, loss 0.427683, acc 0.902344, prec 0.0626605, recall 0.873291
2017-12-10T00:12:58.552113: step 587, loss 0.341996, acc 0.917969, prec 0.0627553, recall 0.87355
2017-12-10T00:12:59.466744: step 588, loss 0.289156, acc 0.902344, prec 0.0628418, recall 0.873808
2017-12-10T00:13:00.386037: step 589, loss 0.259162, acc 0.900391, prec 0.0628353, recall 0.873894
2017-12-10T00:13:01.298063: step 590, loss 0.210731, acc 0.933594, prec 0.0628463, recall 0.87398
2017-12-10T00:13:02.202659: step 591, loss 0.203972, acc 0.927734, prec 0.0629458, recall 0.874237
2017-12-10T00:13:03.107343: step 592, loss 0.233047, acc 0.925781, prec 0.0630442, recall 0.874492
2017-12-10T00:13:04.020012: step 593, loss 0.275562, acc 0.935547, prec 0.0631322, recall 0.874718
2017-12-10T00:13:04.931755: step 594, loss 0.173458, acc 0.947266, prec 0.063135, recall 0.874775
2017-12-10T00:13:05.865631: step 595, loss 0.259863, acc 0.923828, prec 0.0632319, recall 0.875028
2017-12-10T00:13:06.771783: step 596, loss 0.218545, acc 0.9375, prec 0.0633359, recall 0.87528
2017-12-10T00:13:07.683731: step 597, loss 0.199358, acc 0.970703, prec 0.0634421, recall 0.875504
2017-12-10T00:13:08.590172: step 598, loss 0.209429, acc 0.951172, prec 0.0635227, recall 0.875699
2017-12-10T00:13:09.491586: step 599, loss 0.360538, acc 0.949219, prec 0.0635577, recall 0.875614
2017-12-10T00:13:10.404923: step 600, loss 0.30919, acc 0.957031, prec 0.0636564, recall 0.875836

Evaluation:
2017-12-10T00:13:15.096635: step 600, loss 3.14123, acc 0.959053, prec 0.0642367, recall 0.858065

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_0/1512882237/checkpoints/model-600

2017-12-10T00:13:17.160269: step 601, loss 0.211299, acc 0.970703, prec 0.0642664, recall 0.858156
2017-12-10T00:13:18.072786: step 602, loss 0.236287, acc 0.953125, prec 0.0642717, recall 0.858217
2017-12-10T00:13:18.997825: step 603, loss 0.153125, acc 0.951172, prec 0.0643662, recall 0.85846
2017-12-10T00:13:19.902616: step 604, loss 1.05236, acc 0.945312, prec 0.0645187, recall 0.85864
2017-12-10T00:13:20.834118: step 605, loss 0.253603, acc 0.966797, prec 0.0646213, recall 0.858881
2017-12-10T00:13:21.741058: step 606, loss 0.155354, acc 0.960938, prec 0.0647357, recall 0.859152
2017-12-10T00:13:22.658242: step 607, loss 0.218521, acc 0.943359, prec 0.0648557, recall 0.859451
2017-12-10T00:13:23.569210: step 608, loss 0.263609, acc 0.90625, prec 0.0648807, recall 0.859601
2017-12-10T00:13:24.473994: step 609, loss 0.405429, acc 0.921875, prec 0.064974, recall 0.859869
2017-12-10T00:13:25.379914: step 610, loss 0.248642, acc 0.923828, prec 0.0650382, recall 0.860076
2017-12-10T00:13:26.281700: step 611, loss 0.26513, acc 0.927734, prec 0.0651492, recall 0.860372
2017-12-10T00:13:27.198048: step 612, loss 0.290039, acc 0.916016, prec 0.065224, recall 0.860607
2017-12-10T00:13:28.104218: step 613, loss 0.543338, acc 0.914062, prec 0.0652389, recall 0.860543
2017-12-10T00:13:29.019856: step 614, loss 0.304068, acc 0.912109, prec 0.0652816, recall 0.86072
2017-12-10T00:13:29.937520: step 615, loss 0.384296, acc 0.90625, prec 0.065321, recall 0.860895
2017-12-10T00:13:30.864188: step 616, loss 0.259288, acc 0.925781, prec 0.0653857, recall 0.861099
2017-12-10T00:13:31.768035: step 617, loss 0.279048, acc 0.919922, prec 0.0654769, recall 0.861361
2017-12-10T00:13:32.684840: step 618, loss 0.270811, acc 0.898438, prec 0.0655417, recall 0.861593
2017-12-10T00:13:33.592555: step 619, loss 0.245178, acc 0.919922, prec 0.0655881, recall 0.861767
2017-12-10T00:13:34.504983: step 620, loss 0.214203, acc 0.9375, prec 0.065629, recall 0.861911
2017-12-10T00:13:35.403158: step 621, loss 0.187463, acc 0.941406, prec 0.065657, recall 0.862026
2017-12-10T00:13:36.321463: step 622, loss 0.287639, acc 0.951172, prec 0.06572, recall 0.862198
2017-12-10T00:13:37.237645: step 623, loss 0.551364, acc 0.958984, prec 0.0657298, recall 0.861897
2017-12-10T00:13:38.145367: step 624, loss 0.215087, acc 0.96875, prec 0.0658761, recall 0.862212
2017-12-10T00:13:39.050886: step 625, loss 0.288077, acc 0.935547, prec 0.0659748, recall 0.862469
2017-12-10T00:13:39.961181: step 626, loss 0.350197, acc 0.935547, prec 0.0660734, recall 0.862725
2017-12-10T00:13:40.874527: step 627, loss 0.683595, acc 0.951172, prec 0.0661518, recall 0.862745
2017-12-10T00:13:41.794974: step 628, loss 0.378887, acc 0.929688, prec 0.0662914, recall 0.863084
2017-12-10T00:13:42.705095: step 629, loss 0.215312, acc 0.9375, prec 0.0663906, recall 0.863337
2017-12-10T00:13:42.892797: step 630, loss 0.345254, acc 0.882353, prec 0.0664138, recall 0.863394
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 512
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_512_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_1/1512882823

Start training
2017-12-10T00:13:47.418144: step 1, loss 3.3856, acc 0.558594, prec 0.0218341, recall 0.714286
2017-12-10T00:13:48.320065: step 2, loss 1.78081, acc 0.583984, prec 0.0201794, recall 0.818182
2017-12-10T00:13:49.245042: step 3, loss 5.99654, acc 0.75, prec 0.0157343, recall 0.692308
2017-12-10T00:13:50.151595: step 4, loss 6.46061, acc 0.851562, prec 0.0170807, recall 0.52381
2017-12-10T00:13:51.077207: step 5, loss 10.1657, acc 0.785156, prec 0.0147059, recall 0.407407
2017-12-10T00:13:51.986984: step 6, loss 6.73909, acc 0.65625, prec 0.0119695, recall 0.34375
2017-12-10T00:13:52.912228: step 7, loss 4.26564, acc 0.474609, prec 0.0142737, recall 0.414634
2017-12-10T00:13:53.816981: step 8, loss 4.52411, acc 0.296875, prec 0.014791, recall 0.469388
2017-12-10T00:13:54.726658: step 9, loss 4.45619, acc 0.222656, prec 0.0127877, recall 0.490196
2017-12-10T00:13:55.641593: step 10, loss 4.98974, acc 0.224609, prec 0.0131467, recall 0.54386
2017-12-10T00:13:56.553431: step 11, loss 5.00094, acc 0.230469, prec 0.0144928, recall 0.597015
2017-12-10T00:13:57.457019: step 12, loss 3.81491, acc 0.34375, prec 0.0141981, recall 0.611111
2017-12-10T00:13:58.361619: step 13, loss 5.36542, acc 0.431641, prec 0.0141551, recall 0.607595
2017-12-10T00:13:59.262568: step 14, loss 7.18176, acc 0.521484, prec 0.014038, recall 0.579545
2017-12-10T00:14:00.178976: step 15, loss 3.01569, acc 0.572266, prec 0.0152969, recall 0.59596
2017-12-10T00:14:01.096712: step 16, loss 5.05901, acc 0.605469, prec 0.0155249, recall 0.583333
2017-12-10T00:14:02.013409: step 17, loss 4.8589, acc 0.570312, prec 0.0149708, recall 0.566372
2017-12-10T00:14:02.922394: step 18, loss 3.96453, acc 0.542969, prec 0.0155176, recall 0.569106
2017-12-10T00:14:03.824685: step 19, loss 2.84859, acc 0.509766, prec 0.0161493, recall 0.587786
2017-12-10T00:14:04.737491: step 20, loss 2.39337, acc 0.494141, prec 0.0168818, recall 0.611511
2017-12-10T00:14:05.656343: step 21, loss 3.76046, acc 0.505859, prec 0.0179381, recall 0.629139
2017-12-10T00:14:06.584088: step 22, loss 3.04515, acc 0.515625, prec 0.018031, recall 0.628931
2017-12-10T00:14:07.504718: step 23, loss 2.47229, acc 0.509766, prec 0.0184387, recall 0.640719
2017-12-10T00:14:08.411906: step 24, loss 2.50426, acc 0.5, prec 0.0184727, recall 0.647399
2017-12-10T00:14:09.313828: step 25, loss 2.53487, acc 0.591797, prec 0.0181731, recall 0.647727
2017-12-10T00:14:10.222688: step 26, loss 5.36238, acc 0.673828, prec 0.0186306, recall 0.641711
2017-12-10T00:14:11.146344: step 27, loss 2.14157, acc 0.638672, prec 0.0188537, recall 0.647668
2017-12-10T00:14:12.065144: step 28, loss 4.83306, acc 0.650391, prec 0.0189427, recall 0.645
2017-12-10T00:14:12.980335: step 29, loss 3.60324, acc 0.660156, prec 0.0193216, recall 0.645933
2017-12-10T00:14:13.888088: step 30, loss 3.03348, acc 0.648438, prec 0.0195258, recall 0.648148
2017-12-10T00:14:14.799038: step 31, loss 7.92073, acc 0.660156, prec 0.0194823, recall 0.632743
2017-12-10T00:14:15.724875: step 32, loss 3.55636, acc 0.583984, prec 0.0194573, recall 0.633621
2017-12-10T00:14:16.630292: step 33, loss 6.10033, acc 0.556641, prec 0.0195297, recall 0.630705
2017-12-10T00:14:17.533220: step 34, loss 4.00704, acc 0.503906, prec 0.0200124, recall 0.641434
2017-12-10T00:14:18.435820: step 35, loss 3.03458, acc 0.429688, prec 0.0201342, recall 0.651163
2017-12-10T00:14:19.339679: step 36, loss 5.18388, acc 0.392578, prec 0.0198683, recall 0.651515
2017-12-10T00:14:20.242274: step 37, loss 3.64902, acc 0.390625, prec 0.0196166, recall 0.654275
2017-12-10T00:14:21.168902: step 38, loss 3.32667, acc 0.369141, prec 0.0195678, recall 0.661818
2017-12-10T00:14:22.069139: step 39, loss 2.8258, acc 0.451172, prec 0.0198123, recall 0.671378
2017-12-10T00:14:22.973572: step 40, loss 2.64376, acc 0.478516, prec 0.0198743, recall 0.675862
2017-12-10T00:14:23.886090: step 41, loss 4.56348, acc 0.564453, prec 0.020218, recall 0.677741
2017-12-10T00:14:24.815958: step 42, loss 2.89645, acc 0.625, prec 0.0202276, recall 0.675325
2017-12-10T00:14:25.726654: step 43, loss 4.5856, acc 0.671875, prec 0.0203789, recall 0.671924
2017-12-10T00:14:26.629694: step 44, loss 1.79522, acc 0.667969, prec 0.0203313, recall 0.672897
2017-12-10T00:14:27.547719: step 45, loss 4.07409, acc 0.712891, prec 0.0205142, recall 0.671733
2017-12-10T00:14:28.473006: step 46, loss 1.22998, acc 0.685547, prec 0.0205705, recall 0.675676
2017-12-10T00:14:29.386192: step 47, loss 4.3722, acc 0.714844, prec 0.0205758, recall 0.666667
2017-12-10T00:14:30.318027: step 48, loss 2.05233, acc 0.666016, prec 0.0209629, recall 0.670455
2017-12-10T00:14:31.233841: step 49, loss 1.79548, acc 0.603516, prec 0.0210187, recall 0.67507
2017-12-10T00:14:32.134561: step 50, loss 3.27264, acc 0.630859, prec 0.0211872, recall 0.674863
2017-12-10T00:14:33.039483: step 51, loss 2.39144, acc 0.580078, prec 0.0213823, recall 0.679144
2017-12-10T00:14:33.945929: step 52, loss 2.58731, acc 0.568359, prec 0.0215613, recall 0.681462
2017-12-10T00:14:34.853775: step 53, loss 3.06646, acc 0.548828, prec 0.0215559, recall 0.683805
2017-12-10T00:14:35.777979: step 54, loss 2.2661, acc 0.582031, prec 0.0218914, recall 0.689223
2017-12-10T00:14:36.687029: step 55, loss 4.07856, acc 0.554688, prec 0.0220399, recall 0.691176
2017-12-10T00:14:37.596777: step 56, loss 2.63043, acc 0.574219, prec 0.0222, recall 0.693046
2017-12-10T00:14:38.506512: step 57, loss 4.0797, acc 0.583984, prec 0.0223649, recall 0.693208
2017-12-10T00:14:39.411671: step 58, loss 3.28976, acc 0.601562, prec 0.0228304, recall 0.696145
2017-12-10T00:14:40.316033: step 59, loss 4.48395, acc 0.597656, prec 0.0227755, recall 0.695749
2017-12-10T00:14:41.223697: step 60, loss 2.2822, acc 0.550781, prec 0.0231018, recall 0.702407
2017-12-10T00:14:42.126286: step 61, loss 4.38185, acc 0.546875, prec 0.0230715, recall 0.702586
2017-12-10T00:14:43.034888: step 62, loss 1.88201, acc 0.580078, prec 0.02293, recall 0.704497
2017-12-10T00:14:43.217325: step 63, loss 1.16309, acc 0.666667, prec 0.0229029, recall 0.704497
2017-12-10T00:14:44.122312: step 64, loss 6.29844, acc 0.632812, prec 0.0230161, recall 0.702306
2017-12-10T00:14:45.032219: step 65, loss 2.784, acc 0.669922, prec 0.0232179, recall 0.705155
2017-12-10T00:14:45.954754: step 66, loss 1.40046, acc 0.685547, prec 0.0232293, recall 0.707566
2017-12-10T00:14:46.854791: step 67, loss 2.28499, acc 0.707031, prec 0.0234552, recall 0.708835
2017-12-10T00:14:47.766352: step 68, loss 1.60187, acc 0.703125, prec 0.0234149, recall 0.709163
2017-12-10T00:14:48.681466: step 69, loss 1.25035, acc 0.705078, prec 0.0231846, recall 0.709163
2017-12-10T00:14:49.588657: step 70, loss 1.40462, acc 0.740234, prec 0.0234284, recall 0.711765
2017-12-10T00:14:50.508328: step 71, loss 1.9484, acc 0.746094, prec 0.023549, recall 0.711799
2017-12-10T00:14:51.424900: step 72, loss 1.84886, acc 0.78125, prec 0.023569, recall 0.712092
2017-12-10T00:14:52.334000: step 73, loss 3.2564, acc 0.794922, prec 0.0237855, recall 0.712665
2017-12-10T00:14:53.241913: step 74, loss 1.37172, acc 0.808594, prec 0.0238245, recall 0.712946
2017-12-10T00:14:54.146055: step 75, loss 2.49701, acc 0.814453, prec 0.0240528, recall 0.712177
2017-12-10T00:14:55.061734: step 76, loss 2.41611, acc 0.798828, prec 0.0244476, recall 0.712996
2017-12-10T00:14:55.964849: step 77, loss 2.48536, acc 0.71875, prec 0.0245937, recall 0.713523
2017-12-10T00:14:56.874338: step 78, loss 1.26725, acc 0.697266, prec 0.0245991, recall 0.715548
2017-12-10T00:14:57.789543: step 79, loss 3.81467, acc 0.654297, prec 0.0252207, recall 0.719178
2017-12-10T00:14:58.692226: step 80, loss 2.47892, acc 0.648438, prec 0.0254202, recall 0.718121
2017-12-10T00:14:59.602099: step 81, loss 2.44548, acc 0.564453, prec 0.0253751, recall 0.719269
2017-12-10T00:15:00.521371: step 82, loss 2.08318, acc 0.535156, prec 0.0253077, recall 0.721582
2017-12-10T00:15:01.426136: step 83, loss 2.57635, acc 0.507812, prec 0.0254995, recall 0.726094
2017-12-10T00:15:02.324976: step 84, loss 2.3212, acc 0.490234, prec 0.0255088, recall 0.729167
2017-12-10T00:15:03.229757: step 85, loss 2.06173, acc 0.546875, prec 0.0256666, recall 0.733017
2017-12-10T00:15:04.139045: step 86, loss 1.75824, acc 0.605469, prec 0.025864, recall 0.735614
2017-12-10T00:15:05.040094: step 87, loss 3.55194, acc 0.65625, prec 0.0258341, recall 0.732719
2017-12-10T00:15:05.968787: step 88, loss 1.7439, acc 0.681641, prec 0.0262367, recall 0.736446
2017-12-10T00:15:06.873235: step 89, loss 1.76355, acc 0.689453, prec 0.0263284, recall 0.736607
2017-12-10T00:15:07.794364: step 90, loss 3.16352, acc 0.716797, prec 0.0264908, recall 0.73607
2017-12-10T00:15:08.708385: step 91, loss 1.97891, acc 0.736328, prec 0.0265088, recall 0.736536
2017-12-10T00:15:09.621761: step 92, loss 3.15718, acc 0.746094, prec 0.026487, recall 0.733429
2017-12-10T00:15:10.531307: step 93, loss 2.72245, acc 0.701172, prec 0.0266852, recall 0.732295
2017-12-10T00:15:11.432300: step 94, loss 1.21785, acc 0.689453, prec 0.0267172, recall 0.734177
2017-12-10T00:15:12.333056: step 95, loss 1.76784, acc 0.691406, prec 0.0270476, recall 0.737206
2017-12-10T00:15:13.240693: step 96, loss 2.69243, acc 0.626953, prec 0.0270365, recall 0.735978
2017-12-10T00:15:14.143758: step 97, loss 2.295, acc 0.646484, prec 0.0270862, recall 0.73613
2017-12-10T00:15:15.051706: step 98, loss 1.90519, acc 0.583984, prec 0.0271375, recall 0.738606
2017-12-10T00:15:15.955750: step 99, loss 1.95018, acc 0.585938, prec 0.0271429, recall 0.739708
2017-12-10T00:15:16.866682: step 100, loss 2.16793, acc 0.578125, prec 0.0271899, recall 0.74113
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_1/1512882823/checkpoints/model-100

2017-12-10T00:15:18.791632: step 101, loss 1.95494, acc 0.576172, prec 0.0271882, recall 0.742188
2017-12-10T00:15:19.695498: step 102, loss 1.46135, acc 0.646484, prec 0.0272775, recall 0.744516
2017-12-10T00:15:20.610268: step 103, loss 1.59213, acc 0.677734, prec 0.0272058, recall 0.74359
2017-12-10T00:15:21.521972: step 104, loss 3.22274, acc 0.648438, prec 0.0272545, recall 0.741772
2017-12-10T00:15:22.430038: step 105, loss 3.04777, acc 0.726562, prec 0.0272643, recall 0.738423
2017-12-10T00:15:23.353469: step 106, loss 2.17871, acc 0.734375, prec 0.0272306, recall 0.737562
2017-12-10T00:15:24.260319: step 107, loss 1.06265, acc 0.710938, prec 0.0270467, recall 0.737562
2017-12-10T00:15:25.176361: step 108, loss 1.58791, acc 0.699219, prec 0.0272559, recall 0.739558
2017-12-10T00:15:26.095703: step 109, loss 1.45714, acc 0.664062, prec 0.0273948, recall 0.742092
2017-12-10T00:15:26.996792: step 110, loss 2.0453, acc 0.683594, prec 0.0275041, recall 0.741587
2017-12-10T00:15:27.904750: step 111, loss 3.13022, acc 0.724609, prec 0.0275514, recall 0.740476
2017-12-10T00:15:28.812493: step 112, loss 2.55011, acc 0.677734, prec 0.0276105, recall 0.740566
2017-12-10T00:15:29.719945: step 113, loss 1.47146, acc 0.662109, prec 0.0275718, recall 0.741784
2017-12-10T00:15:30.648906: step 114, loss 2.21579, acc 0.695312, prec 0.0275142, recall 0.740957
2017-12-10T00:15:31.547472: step 115, loss 1.36802, acc 0.667969, prec 0.0274803, recall 0.74216
2017-12-10T00:15:32.466037: step 116, loss 1.49847, acc 0.671875, prec 0.0278228, recall 0.745995
2017-12-10T00:15:33.377145: step 117, loss 1.41278, acc 0.691406, prec 0.0276789, recall 0.745434
2017-12-10T00:15:34.288313: step 118, loss 3.59937, acc 0.703125, prec 0.0276667, recall 0.745743
2017-12-10T00:15:35.208510: step 119, loss 4.78466, acc 0.767578, prec 0.0276986, recall 0.741863
2017-12-10T00:15:36.140016: step 120, loss 1.34836, acc 0.724609, prec 0.0277396, recall 0.742475
2017-12-10T00:15:37.044833: step 121, loss 2.41429, acc 0.707031, prec 0.0277697, recall 0.743079
2017-12-10T00:15:37.953565: step 122, loss 1.72432, acc 0.662109, prec 0.0278143, recall 0.743139
2017-12-10T00:15:38.867698: step 123, loss 2.58459, acc 0.675781, prec 0.0277494, recall 0.740741
2017-12-10T00:15:39.791920: step 124, loss 1.36288, acc 0.71875, prec 0.0277857, recall 0.741342
2017-12-10T00:15:40.698336: step 125, loss 2.09612, acc 0.642578, prec 0.027818, recall 0.741416
2017-12-10T00:15:40.879635: step 126, loss 1.29252, acc 0.666667, prec 0.0278772, recall 0.74197
2017-12-10T00:15:41.792121: step 127, loss 1.29247, acc 0.667969, prec 0.0278821, recall 0.743344
2017-12-10T00:15:42.695112: step 128, loss 1.31477, acc 0.683594, prec 0.0278957, recall 0.744703
2017-12-10T00:15:43.598072: step 129, loss 1.43952, acc 0.685547, prec 0.0278347, recall 0.744726
2017-12-10T00:15:44.509313: step 130, loss 1.14214, acc 0.738281, prec 0.0281077, recall 0.747654
2017-12-10T00:15:45.421980: step 131, loss 1.24712, acc 0.724609, prec 0.0281816, recall 0.748447
2017-12-10T00:15:46.346940: step 132, loss 2.30524, acc 0.8125, prec 0.0282308, recall 0.747174
2017-12-10T00:15:47.260892: step 133, loss 3.17143, acc 0.816406, prec 0.0283935, recall 0.747454
2017-12-10T00:15:48.170933: step 134, loss 0.883627, acc 0.818359, prec 0.0286661, recall 0.75
2017-12-10T00:15:49.079444: step 135, loss 0.849294, acc 0.773438, prec 0.0286876, recall 0.751004
2017-12-10T00:15:49.991438: step 136, loss 0.828504, acc 0.751953, prec 0.0285856, recall 0.751254
2017-12-10T00:15:50.927431: step 137, loss 0.997168, acc 0.808594, prec 0.0287376, recall 0.752988
2017-12-10T00:15:51.829774: step 138, loss 2.08469, acc 0.792969, prec 0.0288829, recall 0.752465
2017-12-10T00:15:52.753356: step 139, loss 1.05922, acc 0.761719, prec 0.0289342, recall 0.752941
2017-12-10T00:15:53.661944: step 140, loss 1.19356, acc 0.796875, prec 0.029041, recall 0.753651
2017-12-10T00:15:54.582243: step 141, loss 1.76332, acc 0.775391, prec 0.0291008, recall 0.752657
2017-12-10T00:15:55.489536: step 142, loss 2.984, acc 0.771484, prec 0.0290847, recall 0.751923
2017-12-10T00:15:56.394949: step 143, loss 3.6438, acc 0.714844, prec 0.0291474, recall 0.750476
2017-12-10T00:15:57.308520: step 144, loss 3.45047, acc 0.724609, prec 0.0293198, recall 0.751178
2017-12-10T00:15:58.219915: step 145, loss 1.67438, acc 0.630859, prec 0.0292957, recall 0.75164
2017-12-10T00:15:59.118301: step 146, loss 1.994, acc 0.630859, prec 0.0293072, recall 0.752328
2017-12-10T00:16:00.031586: step 147, loss 1.71969, acc 0.595703, prec 0.0294732, recall 0.754839
2017-12-10T00:16:00.952908: step 148, loss 1.66055, acc 0.589844, prec 0.0295987, recall 0.757078
2017-12-10T00:16:01.853678: step 149, loss 2.11728, acc 0.615234, prec 0.0297725, recall 0.758123
2017-12-10T00:16:02.769517: step 150, loss 1.46618, acc 0.623047, prec 0.0300823, recall 0.761353
2017-12-10T00:16:03.670599: step 151, loss 1.39371, acc 0.638672, prec 0.030159, recall 0.763042
2017-12-10T00:16:04.580439: step 152, loss 1.2411, acc 0.662109, prec 0.0302473, recall 0.764706
2017-12-10T00:16:05.494642: step 153, loss 1.24073, acc 0.716797, prec 0.0302643, recall 0.765065
2017-12-10T00:16:06.426303: step 154, loss 1.53336, acc 0.757812, prec 0.0303364, recall 0.765625
2017-12-10T00:16:07.340599: step 155, loss 1.5771, acc 0.814453, prec 0.0304713, recall 0.766379
2017-12-10T00:16:08.268757: step 156, loss 0.619971, acc 0.835938, prec 0.0305826, recall 0.767581
2017-12-10T00:16:09.177026: step 157, loss 1.12542, acc 0.855469, prec 0.0306728, recall 0.767263
2017-12-10T00:16:10.086561: step 158, loss 1.56192, acc 0.857422, prec 0.0307975, recall 0.766497
2017-12-10T00:16:10.999620: step 159, loss 1.28049, acc 0.871094, prec 0.030895, recall 0.76619
2017-12-10T00:16:11.905272: step 160, loss 1.7763, acc 0.861328, prec 0.0309223, recall 0.764854
2017-12-10T00:16:12.808335: step 161, loss 2.88198, acc 0.837891, prec 0.0309706, recall 0.763092
2017-12-10T00:16:13.713152: step 162, loss 2.05123, acc 0.832031, prec 0.0310787, recall 0.763006
2017-12-10T00:16:14.627663: step 163, loss 1.59929, acc 0.769531, prec 0.0311212, recall 0.7621
2017-12-10T00:16:15.543716: step 164, loss 1.29245, acc 0.671875, prec 0.0310762, recall 0.762878
2017-12-10T00:16:16.449020: step 165, loss 1.57039, acc 0.662109, prec 0.0310275, recall 0.763029
2017-12-10T00:16:17.356232: step 166, loss 1.4637, acc 0.632812, prec 0.0310906, recall 0.764563
2017-12-10T00:16:18.257913: step 167, loss 1.5228, acc 0.628906, prec 0.0309619, recall 0.764326
2017-12-10T00:16:19.166247: step 168, loss 1.33152, acc 0.650391, prec 0.0308762, recall 0.764895
2017-12-10T00:16:20.069328: step 169, loss 1.41849, acc 0.677734, prec 0.0309318, recall 0.7656
2017-12-10T00:16:21.000872: step 170, loss 1.17927, acc 0.710938, prec 0.0311274, recall 0.767645
2017-12-10T00:16:21.904593: step 171, loss 1.37542, acc 0.724609, prec 0.031236, recall 0.768504
2017-12-10T00:16:22.806911: step 172, loss 1.08635, acc 0.736328, prec 0.0312878, recall 0.76899
2017-12-10T00:16:23.720827: step 173, loss 3.1222, acc 0.808594, prec 0.0315328, recall 0.768576
2017-12-10T00:16:24.632515: step 174, loss 2.5312, acc 0.791016, prec 0.0316129, recall 0.767871
2017-12-10T00:16:25.537460: step 175, loss 1.06742, acc 0.802734, prec 0.0319102, recall 0.769582
2017-12-10T00:16:26.446784: step 176, loss 1.06395, acc 0.751953, prec 0.0319653, recall 0.770628
2017-12-10T00:16:27.352992: step 177, loss 1.72385, acc 0.75, prec 0.0320815, recall 0.770849
2017-12-10T00:16:28.265686: step 178, loss 1.56588, acc 0.703125, prec 0.0321416, recall 0.771471
2017-12-10T00:16:29.176050: step 179, loss 1.54293, acc 0.675781, prec 0.0321872, recall 0.772086
2017-12-10T00:16:30.081312: step 180, loss 1.26949, acc 0.658203, prec 0.0321032, recall 0.772593
2017-12-10T00:16:30.993279: step 181, loss 1.70205, acc 0.667969, prec 0.0321741, recall 0.773363
2017-12-10T00:16:31.898103: step 182, loss 1.16748, acc 0.703125, prec 0.0321726, recall 0.774194
2017-12-10T00:16:32.801814: step 183, loss 1.18986, acc 0.685547, prec 0.0321045, recall 0.774123
2017-12-10T00:16:33.713879: step 184, loss 1.315, acc 0.701172, prec 0.0321909, recall 0.774873
2017-12-10T00:16:34.625521: step 185, loss 1.05803, acc 0.712891, prec 0.0321651, recall 0.775525
2017-12-10T00:16:35.534561: step 186, loss 1.09285, acc 0.748047, prec 0.0322745, recall 0.775701
2017-12-10T00:16:36.452823: step 187, loss 0.720621, acc 0.804688, prec 0.0323225, recall 0.776504
2017-12-10T00:16:37.363005: step 188, loss 0.607838, acc 0.806641, prec 0.0323424, recall 0.777143
2017-12-10T00:16:37.545263: step 189, loss 0.254704, acc 0.901961, prec 0.0323664, recall 0.777302
2017-12-10T00:16:38.462712: step 190, loss 2.3402, acc 0.863281, prec 0.0323891, recall 0.775568
2017-12-10T00:16:39.367245: step 191, loss 0.653785, acc 0.873047, prec 0.0325559, recall 0.776836
2017-12-10T00:16:40.290350: step 192, loss 0.670913, acc 0.888672, prec 0.032844, recall 0.778711
2017-12-10T00:16:41.198047: step 193, loss 0.853691, acc 0.886719, prec 0.0328744, recall 0.778631
2017-12-10T00:16:42.108952: step 194, loss 0.863409, acc 0.892578, prec 0.0329929, recall 0.779013
2017-12-10T00:16:43.025867: step 195, loss 0.906226, acc 0.837891, prec 0.033027, recall 0.779086
2017-12-10T00:16:43.934644: step 196, loss 0.629255, acc 0.861328, prec 0.0331849, recall 0.780303
2017-12-10T00:16:44.842648: step 197, loss 0.788597, acc 0.828125, prec 0.0333265, recall 0.780972
2017-12-10T00:16:45.752212: step 198, loss 0.94699, acc 0.824219, prec 0.0333819, recall 0.780654
2017-12-10T00:16:46.659804: step 199, loss 0.746938, acc 0.820312, prec 0.033434, recall 0.780868
2017-12-10T00:16:47.570442: step 200, loss 0.904507, acc 0.806641, prec 0.033674, recall 0.782638
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_1/1512882823/checkpoints/model-200

2017-12-10T00:16:49.629273: step 201, loss 0.883091, acc 0.765625, prec 0.0339198, recall 0.784523
2017-12-10T00:16:50.550008: step 202, loss 1.65466, acc 0.759766, prec 0.034024, recall 0.784626
2017-12-10T00:16:51.481203: step 203, loss 0.978947, acc 0.767578, prec 0.034075, recall 0.78496
2017-12-10T00:16:52.399766: step 204, loss 1.16283, acc 0.759766, prec 0.0341493, recall 0.785433
2017-12-10T00:16:53.304519: step 205, loss 1.08746, acc 0.771484, prec 0.0342014, recall 0.785761
2017-12-10T00:16:54.212442: step 206, loss 0.958098, acc 0.746094, prec 0.0341859, recall 0.785807
2017-12-10T00:16:55.114674: step 207, loss 2.02188, acc 0.777344, prec 0.0342959, recall 0.785899
2017-12-10T00:16:56.026720: step 208, loss 0.851353, acc 0.769531, prec 0.0343722, recall 0.786864
2017-12-10T00:16:56.935505: step 209, loss 1.0897, acc 0.748047, prec 0.0344383, recall 0.787316
2017-12-10T00:16:57.848447: step 210, loss 1.30345, acc 0.730469, prec 0.0345222, recall 0.787898
2017-12-10T00:16:58.759990: step 211, loss 1.38115, acc 0.689453, prec 0.0345048, recall 0.788071
2017-12-10T00:16:59.670071: step 212, loss 0.850766, acc 0.714844, prec 0.0345257, recall 0.788875
2017-12-10T00:17:00.595594: step 213, loss 1.0732, acc 0.75, prec 0.0345902, recall 0.789805
2017-12-10T00:17:01.509090: step 214, loss 1.03389, acc 0.75, prec 0.0345756, recall 0.789837
2017-12-10T00:17:02.417667: step 215, loss 1.31272, acc 0.759766, prec 0.0346186, recall 0.790131
2017-12-10T00:17:03.341413: step 216, loss 0.63896, acc 0.789062, prec 0.0346483, recall 0.790785
2017-12-10T00:17:04.248244: step 217, loss 0.655773, acc 0.798828, prec 0.0347351, recall 0.791692
2017-12-10T00:17:05.160745: step 218, loss 0.574934, acc 0.835938, prec 0.0348392, recall 0.792593
2017-12-10T00:17:06.093522: step 219, loss 0.922839, acc 0.863281, prec 0.0349048, recall 0.792743
2017-12-10T00:17:07.000770: step 220, loss 0.658061, acc 0.884766, prec 0.0348762, recall 0.792383
2017-12-10T00:17:07.911890: step 221, loss 2.7207, acc 0.884766, prec 0.0349807, recall 0.791209
2017-12-10T00:17:08.827052: step 222, loss 1.44686, acc 0.863281, prec 0.0350726, recall 0.791009
2017-12-10T00:17:09.740794: step 223, loss 0.811182, acc 0.888672, prec 0.0351764, recall 0.79081
2017-12-10T00:17:10.654118: step 224, loss 1.79439, acc 0.847656, prec 0.0352859, recall 0.79074
2017-12-10T00:17:11.563307: step 225, loss 0.590707, acc 0.814453, prec 0.0353511, recall 0.791492
2017-12-10T00:17:12.476158: step 226, loss 0.674841, acc 0.791016, prec 0.0353789, recall 0.792115
2017-12-10T00:17:13.390714: step 227, loss 1.38582, acc 0.734375, prec 0.035457, recall 0.792632
2017-12-10T00:17:14.294793: step 228, loss 1.63429, acc 0.699219, prec 0.0354922, recall 0.793022
2017-12-10T00:17:15.195215: step 229, loss 1.48854, acc 0.705078, prec 0.0354789, recall 0.793164
2017-12-10T00:17:16.104174: step 230, loss 1.04647, acc 0.695312, prec 0.0355109, recall 0.794014
2017-12-10T00:17:17.009659: step 231, loss 0.869476, acc 0.705078, prec 0.0355724, recall 0.794977
2017-12-10T00:17:17.921994: step 232, loss 0.907547, acc 0.712891, prec 0.0355367, recall 0.795455
2017-12-10T00:17:18.816898: step 233, loss 1.15665, acc 0.728516, prec 0.0356596, recall 0.796178
2017-12-10T00:17:19.721740: step 234, loss 1.74788, acc 0.734375, prec 0.0357115, recall 0.795625
2017-12-10T00:17:20.641580: step 235, loss 1.17807, acc 0.818359, prec 0.0358763, recall 0.795883
2017-12-10T00:17:21.572005: step 236, loss 1.66993, acc 0.792969, prec 0.0359309, recall 0.79477
2017-12-10T00:17:22.480880: step 237, loss 1.05, acc 0.775391, prec 0.0359248, recall 0.794785
2017-12-10T00:17:23.381480: step 238, loss 0.711059, acc 0.794922, prec 0.0360501, recall 0.795826
2017-12-10T00:17:24.290998: step 239, loss 0.954719, acc 0.751953, prec 0.0361308, recall 0.796296
2017-12-10T00:17:25.210483: step 240, loss 1.26307, acc 0.744141, prec 0.0362326, recall 0.796431
2017-12-10T00:17:26.108742: step 241, loss 0.775934, acc 0.771484, prec 0.0363448, recall 0.797447
2017-12-10T00:17:27.011523: step 242, loss 0.835061, acc 0.765625, prec 0.0363806, recall 0.798119
2017-12-10T00:17:27.932536: step 243, loss 0.691406, acc 0.808594, prec 0.0364848, recall 0.799009
2017-12-10T00:17:28.838446: step 244, loss 1.12502, acc 0.814453, prec 0.0365196, recall 0.799122
2017-12-10T00:17:29.750745: step 245, loss 1.06891, acc 0.794922, prec 0.0366415, recall 0.799672
2017-12-10T00:17:30.685822: step 246, loss 0.627106, acc 0.8125, prec 0.0366499, recall 0.800109
2017-12-10T00:17:31.598599: step 247, loss 1.00212, acc 0.826172, prec 0.0366895, recall 0.800217
2017-12-10T00:17:32.518196: step 248, loss 0.951432, acc 0.853516, prec 0.0366938, recall 0.800108
2017-12-10T00:17:33.424203: step 249, loss 1.5537, acc 0.833984, prec 0.03669, recall 0.799568
2017-12-10T00:17:34.343006: step 250, loss 1.62141, acc 0.808594, prec 0.036722, recall 0.799246
2017-12-10T00:17:35.252164: step 251, loss 1.10983, acc 0.845703, prec 0.0368652, recall 0.799786
2017-12-10T00:17:35.437325: step 252, loss 0.288528, acc 0.882353, prec 0.0368598, recall 0.799786
2017-12-10T00:17:36.365477: step 253, loss 0.890503, acc 0.808594, prec 0.0368903, recall 0.799893
2017-12-10T00:17:37.280774: step 254, loss 0.700094, acc 0.798828, prec 0.0368916, recall 0.800319
2017-12-10T00:17:38.206641: step 255, loss 0.640442, acc 0.806641, prec 0.0370144, recall 0.801272
2017-12-10T00:17:39.130091: step 256, loss 1.36377, acc 0.798828, prec 0.0370642, recall 0.801055
2017-12-10T00:17:40.049626: step 257, loss 0.836178, acc 0.792969, prec 0.0371804, recall 0.801575
2017-12-10T00:17:40.949819: step 258, loss 0.83122, acc 0.802734, prec 0.0373473, recall 0.802295
2017-12-10T00:17:41.852877: step 259, loss 0.685244, acc 0.791016, prec 0.0373438, recall 0.802707
2017-12-10T00:17:42.767918: step 260, loss 0.954404, acc 0.789062, prec 0.0373868, recall 0.802905
2017-12-10T00:17:43.678165: step 261, loss 0.632561, acc 0.814453, prec 0.0375795, recall 0.804124
2017-12-10T00:17:44.594965: step 262, loss 0.723485, acc 0.783203, prec 0.0376411, recall 0.804828
2017-12-10T00:17:45.512498: step 263, loss 0.716808, acc 0.814453, prec 0.0375794, recall 0.804515
2017-12-10T00:17:46.434198: step 264, loss 0.573503, acc 0.857422, prec 0.0376519, recall 0.805115
2017-12-10T00:17:47.335483: step 265, loss 1.29247, acc 0.871094, prec 0.0377782, recall 0.805089
2017-12-10T00:17:48.248441: step 266, loss 1.22571, acc 0.873047, prec 0.0378353, recall 0.805175
2017-12-10T00:17:49.162073: step 267, loss 0.900795, acc 0.869141, prec 0.0380048, recall 0.805752
2017-12-10T00:17:50.065130: step 268, loss 0.471269, acc 0.835938, prec 0.0380432, recall 0.806241
2017-12-10T00:17:50.989132: step 269, loss 0.758778, acc 0.841797, prec 0.0380623, recall 0.806225
2017-12-10T00:17:51.900804: step 270, loss 0.53409, acc 0.861328, prec 0.0381804, recall 0.807
2017-12-10T00:17:52.816653: step 271, loss 0.473505, acc 0.84375, prec 0.0382445, recall 0.807577
2017-12-10T00:17:53.727248: step 272, loss 2.88178, acc 0.804688, prec 0.0383411, recall 0.805941
2017-12-10T00:17:54.632342: step 273, loss 0.728214, acc 0.824219, prec 0.0384859, recall 0.806897
2017-12-10T00:17:55.535281: step 274, loss 0.717279, acc 0.830078, prec 0.0386779, recall 0.808031
2017-12-10T00:17:56.442974: step 275, loss 1.12936, acc 0.824219, prec 0.0387773, recall 0.808386
2017-12-10T00:17:57.355030: step 276, loss 0.734447, acc 0.773438, prec 0.0386948, recall 0.80848
2017-12-10T00:17:58.258972: step 277, loss 0.769336, acc 0.736328, prec 0.038618, recall 0.808666
2017-12-10T00:17:59.169190: step 278, loss 0.943243, acc 0.759766, prec 0.0387094, recall 0.809109
2017-12-10T00:18:00.078995: step 279, loss 1.24907, acc 0.755859, prec 0.0389085, recall 0.810395
2017-12-10T00:18:01.005328: step 280, loss 0.766371, acc 0.765625, prec 0.0389338, recall 0.810941
2017-12-10T00:18:01.914069: step 281, loss 0.787371, acc 0.759766, prec 0.0390666, recall 0.811933
2017-12-10T00:18:02.831304: step 282, loss 0.734857, acc 0.769531, prec 0.0392031, recall 0.812916
2017-12-10T00:18:03.739701: step 283, loss 0.996299, acc 0.802734, prec 0.0391794, recall 0.812796
2017-12-10T00:18:04.647799: step 284, loss 0.571331, acc 0.802734, prec 0.0392425, recall 0.813415
2017-12-10T00:18:05.552970: step 285, loss 0.614438, acc 0.800781, prec 0.0393699, recall 0.814292
2017-12-10T00:18:06.492773: step 286, loss 0.722472, acc 0.867188, prec 0.039528, recall 0.81478
2017-12-10T00:18:07.405656: step 287, loss 0.859686, acc 0.880859, prec 0.0396049, recall 0.814918
2017-12-10T00:18:08.312818: step 288, loss 0.667599, acc 0.882812, prec 0.0397258, recall 0.815228
2017-12-10T00:18:09.229763: step 289, loss 0.48704, acc 0.890625, prec 0.039784, recall 0.815655
2017-12-10T00:18:10.145501: step 290, loss 0.347301, acc 0.917969, prec 0.0398113, recall 0.815911
2017-12-10T00:18:11.050741: step 291, loss 0.827398, acc 0.886719, prec 0.0398467, recall 0.815874
2017-12-10T00:18:11.962335: step 292, loss 1.12873, acc 0.908203, prec 0.0399793, recall 0.815802
2017-12-10T00:18:12.879798: step 293, loss 0.9396, acc 0.882812, prec 0.0400989, recall 0.816102
2017-12-10T00:18:13.796244: step 294, loss 0.725192, acc 0.871094, prec 0.0401266, recall 0.816066
2017-12-10T00:18:14.739449: step 295, loss 0.38774, acc 0.882812, prec 0.0401587, recall 0.816401
2017-12-10T00:18:15.675401: step 296, loss 0.630095, acc 0.832031, prec 0.0402746, recall 0.817151
2017-12-10T00:18:16.573113: step 297, loss 0.644518, acc 0.84375, prec 0.0403312, recall 0.817647
2017-12-10T00:18:17.469919: step 298, loss 0.748637, acc 0.847656, prec 0.0404117, recall 0.817854
2017-12-10T00:18:18.369025: step 299, loss 0.834031, acc 0.828125, prec 0.0404402, recall 0.817896
2017-12-10T00:18:19.276496: step 300, loss 0.688686, acc 0.8125, prec 0.0404605, recall 0.818304

Evaluation:
2017-12-10T00:18:23.984809: step 300, loss 1.13175, acc 0.834607, prec 0.0413123, recall 0.814286

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_1/1512882823/checkpoints/model-300

2017-12-10T00:18:25.871268: step 301, loss 0.521801, acc 0.824219, prec 0.0414167, recall 0.814985
2017-12-10T00:18:26.781246: step 302, loss 0.713259, acc 0.806641, prec 0.0414721, recall 0.815526
2017-12-10T00:18:27.690832: step 303, loss 0.7179, acc 0.808594, prec 0.0415078, recall 0.815987
2017-12-10T00:18:28.607121: step 304, loss 0.721146, acc 0.824219, prec 0.041672, recall 0.816901
2017-12-10T00:18:29.519427: step 305, loss 1.0071, acc 0.830078, prec 0.0417387, recall 0.816756
2017-12-10T00:18:30.441716: step 306, loss 0.851092, acc 0.84375, prec 0.0418307, recall 0.817023
2017-12-10T00:18:31.374108: step 307, loss 1.50574, acc 0.833984, prec 0.0418383, recall 0.816653
2017-12-10T00:18:32.283108: step 308, loss 0.728275, acc 0.816406, prec 0.0418772, recall 0.816769
2017-12-10T00:18:33.190795: step 309, loss 1.20731, acc 0.84375, prec 0.0420085, recall 0.817182
2017-12-10T00:18:34.118731: step 310, loss 0.541529, acc 0.826172, prec 0.0419905, recall 0.817405
2017-12-10T00:18:35.030072: step 311, loss 1.30635, acc 0.791016, prec 0.0419984, recall 0.817113
2017-12-10T00:18:35.966359: step 312, loss 0.692482, acc 0.8125, prec 0.0420142, recall 0.817483
2017-12-10T00:18:36.880738: step 313, loss 0.615223, acc 0.800781, prec 0.0421439, recall 0.818292
2017-12-10T00:18:37.789111: step 314, loss 0.665562, acc 0.791016, prec 0.0421299, recall 0.818584
2017-12-10T00:18:37.964333: step 315, loss 0.610267, acc 0.803922, prec 0.0421212, recall 0.818584
2017-12-10T00:18:38.895259: step 316, loss 0.624089, acc 0.820312, prec 0.0422192, recall 0.819238
2017-12-10T00:18:39.801707: step 317, loss 0.692859, acc 0.833984, prec 0.0422646, recall 0.819345
2017-12-10T00:18:40.704155: step 318, loss 0.389271, acc 0.847656, prec 0.0422953, recall 0.819705
2017-12-10T00:18:41.603135: step 319, loss 0.339977, acc 0.884766, prec 0.0424408, recall 0.820421
2017-12-10T00:18:42.510771: step 320, loss 0.479722, acc 0.910156, prec 0.0425777, recall 0.821061
2017-12-10T00:18:43.424454: step 321, loss 0.827055, acc 0.916016, prec 0.0427382, recall 0.821119
2017-12-10T00:18:44.326279: step 322, loss 0.467554, acc 0.921875, prec 0.0428208, recall 0.821541
2017-12-10T00:18:45.240550: step 323, loss 1.0928, acc 0.917969, prec 0.0428829, recall 0.821569
2017-12-10T00:18:46.152525: step 324, loss 0.295124, acc 0.90625, prec 0.0429973, recall 0.822127
2017-12-10T00:18:47.068552: step 325, loss 0.310526, acc 0.9375, prec 0.0430865, recall 0.822543
2017-12-10T00:18:48.008220: step 326, loss 0.518815, acc 0.898438, prec 0.0431775, recall 0.823026
2017-12-10T00:18:48.927471: step 327, loss 0.574633, acc 0.894531, prec 0.043286, recall 0.823575
2017-12-10T00:18:49.854946: step 328, loss 0.371873, acc 0.896484, prec 0.0433561, recall 0.823985
2017-12-10T00:18:50.767260: step 329, loss 0.565674, acc 0.898438, prec 0.0434464, recall 0.82446
2017-12-10T00:18:51.681288: step 330, loss 0.409964, acc 0.849609, prec 0.0434562, recall 0.82473
2017-12-10T00:18:52.595892: step 331, loss 0.527874, acc 0.892578, prec 0.0436016, recall 0.825403
2017-12-10T00:18:53.505748: step 332, loss 0.479285, acc 0.851562, prec 0.0436894, recall 0.825937
2017-12-10T00:18:54.423877: step 333, loss 0.861981, acc 0.851562, prec 0.043798, recall 0.825905
2017-12-10T00:18:55.324211: step 334, loss 1.03271, acc 0.837891, prec 0.0439184, recall 0.826252
2017-12-10T00:18:56.236503: step 335, loss 0.540826, acc 0.820312, prec 0.043914, recall 0.826515
2017-12-10T00:18:57.147721: step 336, loss 0.552321, acc 0.855469, prec 0.04406, recall 0.827235
2017-12-10T00:18:58.071227: step 337, loss 0.511139, acc 0.845703, prec 0.0441628, recall 0.82782
2017-12-10T00:18:58.982892: step 338, loss 0.644886, acc 0.832031, prec 0.0442216, recall 0.827961
2017-12-10T00:18:59.889519: step 339, loss 0.521612, acc 0.855469, prec 0.0443281, recall 0.828539
2017-12-10T00:19:00.820793: step 340, loss 0.541832, acc 0.847656, prec 0.0443735, recall 0.828923
2017-12-10T00:19:01.740079: step 341, loss 0.40364, acc 0.875, prec 0.0445073, recall 0.829558
2017-12-10T00:19:02.640246: step 342, loss 1.08161, acc 0.851562, prec 0.0445748, recall 0.829386
2017-12-10T00:19:03.546542: step 343, loss 0.375685, acc 0.863281, prec 0.0445698, recall 0.829575
2017-12-10T00:19:04.454335: step 344, loss 0.423588, acc 0.869141, prec 0.0446243, recall 0.829952
2017-12-10T00:19:05.362135: step 345, loss 0.456928, acc 0.871094, prec 0.0446984, recall 0.83039
2017-12-10T00:19:06.304988: step 346, loss 1.42979, acc 0.882812, prec 0.0448172, recall 0.830341
2017-12-10T00:19:07.224206: step 347, loss 0.378673, acc 0.863281, prec 0.0448684, recall 0.830713
2017-12-10T00:19:08.130710: step 348, loss 0.426923, acc 0.865234, prec 0.0449203, recall 0.831084
2017-12-10T00:19:09.045496: step 349, loss 0.546751, acc 0.855469, prec 0.0449865, recall 0.831514
2017-12-10T00:19:09.954763: step 350, loss 0.587936, acc 0.861328, prec 0.0451302, recall 0.832186
2017-12-10T00:19:10.870819: step 351, loss 0.371122, acc 0.896484, prec 0.0452145, recall 0.83261
2017-12-10T00:19:11.806613: step 352, loss 0.323655, acc 0.876953, prec 0.0452522, recall 0.832912
2017-12-10T00:19:12.705783: step 353, loss 0.658828, acc 0.884766, prec 0.0454065, recall 0.833273
2017-12-10T00:19:13.613572: step 354, loss 0.567809, acc 0.878906, prec 0.0454634, recall 0.833632
2017-12-10T00:19:14.517193: step 355, loss 0.470227, acc 0.884766, prec 0.0455051, recall 0.833631
2017-12-10T00:19:15.423298: step 356, loss 0.682412, acc 0.876953, prec 0.045599, recall 0.833809
2017-12-10T00:19:16.323779: step 357, loss 0.704357, acc 0.845703, prec 0.0457342, recall 0.834162
2017-12-10T00:19:17.227152: step 358, loss 0.693926, acc 0.873047, prec 0.0457153, recall 0.833688
2017-12-10T00:19:18.142939: step 359, loss 0.500233, acc 0.861328, prec 0.0457273, recall 0.833628
2017-12-10T00:19:19.054392: step 360, loss 0.433328, acc 0.878906, prec 0.0457093, recall 0.833746
2017-12-10T00:19:19.967886: step 361, loss 0.45195, acc 0.867188, prec 0.0457045, recall 0.833922
2017-12-10T00:19:20.913036: step 362, loss 0.591354, acc 0.845703, prec 0.0457648, recall 0.834038
2017-12-10T00:19:21.810119: step 363, loss 0.581863, acc 0.865234, prec 0.0458152, recall 0.834095
2017-12-10T00:19:22.710108: step 364, loss 0.307103, acc 0.908203, prec 0.0458473, recall 0.834328
2017-12-10T00:19:23.621829: step 365, loss 0.662537, acc 0.886719, prec 0.0459992, recall 0.834673
2017-12-10T00:19:24.533309: step 366, loss 0.56037, acc 0.857422, prec 0.0460814, recall 0.835134
2017-12-10T00:19:25.439214: step 367, loss 0.412122, acc 0.892578, prec 0.0461426, recall 0.835478
2017-12-10T00:19:26.346275: step 368, loss 0.886209, acc 0.902344, prec 0.0461916, recall 0.835184
2017-12-10T00:19:27.263662: step 369, loss 0.859536, acc 0.912109, prec 0.0463364, recall 0.835176
2017-12-10T00:19:28.169595: step 370, loss 0.482796, acc 0.875, prec 0.0464074, recall 0.835574
2017-12-10T00:19:29.089863: step 371, loss 1.02423, acc 0.841797, prec 0.0465743, recall 0.835734
2017-12-10T00:19:30.003691: step 372, loss 0.480704, acc 0.837891, prec 0.0466461, recall 0.836183
2017-12-10T00:19:30.926251: step 373, loss 0.6941, acc 0.847656, prec 0.0467402, recall 0.836686
2017-12-10T00:19:31.841884: step 374, loss 0.603791, acc 0.810547, prec 0.0467265, recall 0.836908
2017-12-10T00:19:32.756634: step 375, loss 0.562523, acc 0.822266, prec 0.0467362, recall 0.837186
2017-12-10T00:19:33.659649: step 376, loss 0.436627, acc 0.837891, prec 0.0468253, recall 0.837682
2017-12-10T00:19:34.557849: step 377, loss 0.546697, acc 0.833984, prec 0.0467861, recall 0.837792
2017-12-10T00:19:34.734941: step 378, loss 0.423805, acc 0.862745, prec 0.0467799, recall 0.837792
2017-12-10T00:19:35.662475: step 379, loss 0.554918, acc 0.841797, prec 0.0468163, recall 0.838121
2017-12-10T00:19:36.574899: step 380, loss 1.00214, acc 0.894531, prec 0.0469313, recall 0.838329
2017-12-10T00:19:37.491637: step 381, loss 0.494639, acc 0.892578, prec 0.047098, recall 0.83898
2017-12-10T00:19:38.398728: step 382, loss 0.492242, acc 0.873047, prec 0.047148, recall 0.839304
2017-12-10T00:19:39.305250: step 383, loss 0.414116, acc 0.865234, prec 0.0471764, recall 0.839572
2017-12-10T00:19:40.212654: step 384, loss 0.340249, acc 0.908203, prec 0.0472063, recall 0.839786
2017-12-10T00:19:41.124980: step 385, loss 0.500762, acc 0.916016, prec 0.0472754, recall 0.840107
2017-12-10T00:19:42.033503: step 386, loss 0.390992, acc 0.916016, prec 0.0473622, recall 0.840479
2017-12-10T00:19:42.945007: step 387, loss 0.57945, acc 0.919922, prec 0.0474694, recall 0.840623
2017-12-10T00:19:43.858375: step 388, loss 0.304892, acc 0.910156, prec 0.0475888, recall 0.841097
2017-12-10T00:19:44.765921: step 389, loss 0.337816, acc 0.923828, prec 0.0477142, recall 0.841568
2017-12-10T00:19:45.678838: step 390, loss 0.484126, acc 0.900391, prec 0.0478464, recall 0.842088
2017-12-10T00:19:46.597181: step 391, loss 0.634878, acc 0.921875, prec 0.0479714, recall 0.842277
2017-12-10T00:19:47.510962: step 392, loss 0.577697, acc 0.904297, prec 0.0480349, recall 0.842311
2017-12-10T00:19:48.414295: step 393, loss 0.849903, acc 0.912109, prec 0.0481204, recall 0.842122
2017-12-10T00:19:49.335602: step 394, loss 0.434258, acc 0.900391, prec 0.0481641, recall 0.842105
2017-12-10T00:19:50.251633: step 395, loss 0.537219, acc 0.892578, prec 0.0482739, recall 0.842566
2017-12-10T00:19:51.186990: step 396, loss 1.14034, acc 0.853516, prec 0.0482607, recall 0.842446
2017-12-10T00:19:52.089460: step 397, loss 0.400293, acc 0.849609, prec 0.0482095, recall 0.842497
2017-12-10T00:19:52.996695: step 398, loss 0.550734, acc 0.851562, prec 0.0482825, recall 0.842903
2017-12-10T00:19:53.916390: step 399, loss 0.798444, acc 0.826172, prec 0.0483446, recall 0.843036
2017-12-10T00:19:54.823929: step 400, loss 0.478812, acc 0.855469, prec 0.0483663, recall 0.843288
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_1/1512882823/checkpoints/model-400

2017-12-10T00:19:56.686111: step 401, loss 0.856174, acc 0.84375, prec 0.048401, recall 0.843319
2017-12-10T00:19:57.603639: step 402, loss 0.451766, acc 0.857422, prec 0.0484235, recall 0.84357
2017-12-10T00:19:58.506231: step 403, loss 0.463172, acc 0.857422, prec 0.0484634, recall 0.84387
2017-12-10T00:19:59.413473: step 404, loss 0.648901, acc 0.84375, prec 0.0485666, recall 0.844367
2017-12-10T00:20:00.339322: step 405, loss 0.574912, acc 0.851562, prec 0.0486383, recall 0.844762
2017-12-10T00:20:01.256585: step 406, loss 0.489937, acc 0.847656, prec 0.0487253, recall 0.845204
2017-12-10T00:20:02.163658: step 407, loss 1.03482, acc 0.859375, prec 0.0488009, recall 0.845328
2017-12-10T00:20:03.066291: step 408, loss 0.72845, acc 0.900391, prec 0.048792, recall 0.844893
2017-12-10T00:20:03.985936: step 409, loss 0.519336, acc 0.886719, prec 0.0487933, recall 0.844773
2017-12-10T00:20:04.892629: step 410, loss 0.355088, acc 0.890625, prec 0.0488474, recall 0.845066
2017-12-10T00:20:05.815484: step 411, loss 0.517944, acc 0.875, prec 0.0489287, recall 0.845455
2017-12-10T00:20:06.720169: step 412, loss 0.391279, acc 0.890625, prec 0.048948, recall 0.845648
2017-12-10T00:20:07.620567: step 413, loss 0.309113, acc 0.896484, prec 0.0490043, recall 0.845937
2017-12-10T00:20:08.544312: step 414, loss 0.325372, acc 0.884766, prec 0.0490896, recall 0.846322
2017-12-10T00:20:09.453775: step 415, loss 0.462724, acc 0.900391, prec 0.0491818, recall 0.846704
2017-12-10T00:20:10.364923: step 416, loss 0.282329, acc 0.912109, prec 0.0492105, recall 0.846894
2017-12-10T00:20:11.273207: step 417, loss 0.479573, acc 0.929688, prec 0.0493337, recall 0.847059
2017-12-10T00:20:12.181431: step 418, loss 0.611078, acc 0.908203, prec 0.0493956, recall 0.847081
2017-12-10T00:20:13.086431: step 419, loss 0.379862, acc 0.914062, prec 0.0494592, recall 0.847364
2017-12-10T00:20:13.986813: step 420, loss 1.00093, acc 0.910156, prec 0.04959, recall 0.847572
2017-12-10T00:20:14.888250: step 421, loss 0.384339, acc 0.902344, prec 0.0496821, recall 0.847946
2017-12-10T00:20:15.794552: step 422, loss 0.56761, acc 0.914062, prec 0.049712, recall 0.847873
2017-12-10T00:20:16.700498: step 423, loss 0.379802, acc 0.898438, prec 0.049836, recall 0.848337
2017-12-10T00:20:17.607526: step 424, loss 0.334462, acc 0.912109, prec 0.0499319, recall 0.848706
2017-12-10T00:20:18.516747: step 425, loss 0.337389, acc 0.898438, prec 0.0500045, recall 0.849028
2017-12-10T00:20:19.419871: step 426, loss 0.334484, acc 0.896484, prec 0.050042, recall 0.849257
2017-12-10T00:20:20.333450: step 427, loss 0.465982, acc 0.894531, prec 0.0501303, recall 0.849365
2017-12-10T00:20:21.250866: step 428, loss 0.345312, acc 0.890625, prec 0.0502496, recall 0.849819
2017-12-10T00:20:22.179005: step 429, loss 0.528507, acc 0.884766, prec 0.0503331, recall 0.849925
2017-12-10T00:20:23.086231: step 430, loss 0.300145, acc 0.894531, prec 0.0503861, recall 0.850195
2017-12-10T00:20:24.000084: step 431, loss 0.498855, acc 0.880859, prec 0.0504158, recall 0.85042
2017-12-10T00:20:24.908847: step 432, loss 0.230013, acc 0.90625, prec 0.0504235, recall 0.850554
2017-12-10T00:20:25.816344: step 433, loss 0.487895, acc 0.90625, prec 0.0505489, recall 0.851
2017-12-10T00:20:26.728882: step 434, loss 0.585667, acc 0.900391, prec 0.0507733, recall 0.851456
2017-12-10T00:20:27.640501: step 435, loss 0.430304, acc 0.923828, prec 0.0508064, recall 0.851379
2017-12-10T00:20:28.548299: step 436, loss 1.07005, acc 0.916016, prec 0.0509383, recall 0.851064
2017-12-10T00:20:29.469345: step 437, loss 0.341161, acc 0.882812, prec 0.0510184, recall 0.851415
2017-12-10T00:20:30.395027: step 438, loss 0.405541, acc 0.886719, prec 0.0511002, recall 0.851765
2017-12-10T00:20:31.313142: step 439, loss 0.479141, acc 0.876953, prec 0.0512274, recall 0.852243
2017-12-10T00:20:32.223278: step 440, loss 0.4094, acc 0.886719, prec 0.0513087, recall 0.852588
2017-12-10T00:20:32.397907: step 441, loss 0.443744, acc 0.823529, prec 0.0513172, recall 0.852632
2017-12-10T00:20:33.312100: step 442, loss 0.441497, acc 0.835938, prec 0.0514415, recall 0.853147
2017-12-10T00:20:34.221641: step 443, loss 0.38733, acc 0.861328, prec 0.0514939, recall 0.853446
2017-12-10T00:20:35.125834: step 444, loss 0.381428, acc 0.876953, prec 0.0516531, recall 0.853998
2017-12-10T00:20:36.063393: step 445, loss 0.311668, acc 0.882812, prec 0.0516985, recall 0.854251
2017-12-10T00:20:36.982828: step 446, loss 0.411332, acc 0.900391, prec 0.0518016, recall 0.854629
2017-12-10T00:20:37.906272: step 447, loss 0.380105, acc 0.933594, prec 0.0520027, recall 0.855214
2017-12-10T00:20:38.818241: step 448, loss 0.29633, acc 0.929688, prec 0.0520693, recall 0.855463
2017-12-10T00:20:39.721555: step 449, loss 0.28984, acc 0.923828, prec 0.0521662, recall 0.855794
2017-12-10T00:20:40.631125: step 450, loss 0.420554, acc 0.935547, prec 0.0522362, recall 0.855797
2017-12-10T00:20:41.539896: step 451, loss 0.321523, acc 0.9375, prec 0.0523392, recall 0.856125
2017-12-10T00:20:42.455890: step 452, loss 0.488142, acc 0.957031, prec 0.0525171, recall 0.856616
2017-12-10T00:20:43.359583: step 453, loss 0.202468, acc 0.949219, prec 0.0525592, recall 0.856778
2017-12-10T00:20:44.268261: step 454, loss 0.270888, acc 0.925781, prec 0.0526069, recall 0.856981
2017-12-10T00:20:45.174899: step 455, loss 0.626967, acc 0.931641, prec 0.0526755, recall 0.856739
2017-12-10T00:20:46.087272: step 456, loss 0.25818, acc 0.927734, prec 0.0527732, recall 0.857062
2017-12-10T00:20:47.019405: step 457, loss 0.273789, acc 0.917969, prec 0.0528826, recall 0.857424
2017-12-10T00:20:47.930433: step 458, loss 0.60976, acc 0.904297, prec 0.0528715, recall 0.857263
2017-12-10T00:20:48.841594: step 459, loss 0.544807, acc 0.919922, prec 0.052966, recall 0.857343
2017-12-10T00:20:49.749642: step 460, loss 0.312684, acc 0.927734, prec 0.0530632, recall 0.857662
2017-12-10T00:20:50.653069: step 461, loss 0.37714, acc 0.90625, prec 0.0532156, recall 0.858138
2017-12-10T00:20:51.568370: step 462, loss 0.606034, acc 0.896484, prec 0.0532822, recall 0.858176
2017-12-10T00:20:52.485366: step 463, loss 0.453567, acc 0.861328, prec 0.0533149, recall 0.858412
2017-12-10T00:20:53.392565: step 464, loss 0.346423, acc 0.884766, prec 0.053326, recall 0.858569
2017-12-10T00:20:54.304048: step 465, loss 0.337431, acc 0.884766, prec 0.0533696, recall 0.858804
2017-12-10T00:20:55.221563: step 466, loss 0.412494, acc 0.845703, prec 0.0534435, recall 0.859155
2017-12-10T00:20:56.125804: step 467, loss 0.44158, acc 0.894531, prec 0.0535564, recall 0.859543
2017-12-10T00:20:57.037921: step 468, loss 0.366837, acc 0.917969, prec 0.0536314, recall 0.859813
2017-12-10T00:20:57.942709: step 469, loss 0.255373, acc 0.898438, prec 0.0537133, recall 0.860121
2017-12-10T00:20:58.852902: step 470, loss 0.606435, acc 0.898438, prec 0.0537798, recall 0.860153
2017-12-10T00:20:59.773786: step 471, loss 0.526989, acc 0.902344, prec 0.0538156, recall 0.860109
2017-12-10T00:21:00.708486: step 472, loss 0.239054, acc 0.914062, prec 0.053856, recall 0.8603
2017-12-10T00:21:01.612854: step 473, loss 0.310222, acc 0.921875, prec 0.0539, recall 0.86049
2017-12-10T00:21:02.535291: step 474, loss 0.464485, acc 0.927734, prec 0.0539476, recall 0.860446
2017-12-10T00:21:03.443386: step 475, loss 0.288262, acc 0.917969, prec 0.0540218, recall 0.860711
2017-12-10T00:21:04.350702: step 476, loss 0.221487, acc 0.949219, prec 0.0540946, recall 0.860938
2017-12-10T00:21:05.259428: step 477, loss 0.485437, acc 0.933594, prec 0.0541125, recall 0.860818
2017-12-10T00:21:06.182280: step 478, loss 0.15465, acc 0.947266, prec 0.0541359, recall 0.860931
2017-12-10T00:21:07.095712: step 479, loss 0.246639, acc 0.939453, prec 0.054236, recall 0.861231
2017-12-10T00:21:07.997778: step 480, loss 0.309283, acc 0.939453, prec 0.054336, recall 0.86153
2017-12-10T00:21:08.938358: step 481, loss 0.36395, acc 0.917969, prec 0.0544578, recall 0.861902
2017-12-10T00:21:09.848861: step 482, loss 0.287499, acc 0.955078, prec 0.0545488, recall 0.862161
2017-12-10T00:21:10.759596: step 483, loss 0.179563, acc 0.945312, prec 0.0546031, recall 0.862346
2017-12-10T00:21:11.668695: step 484, loss 0.409708, acc 0.933594, prec 0.0547318, recall 0.862714
2017-12-10T00:21:12.575789: step 485, loss 0.509012, acc 0.9375, prec 0.0548152, recall 0.86274
2017-12-10T00:21:13.479683: step 486, loss 0.290607, acc 0.921875, prec 0.054826, recall 0.86285
2017-12-10T00:21:14.382718: step 487, loss 0.272869, acc 0.931641, prec 0.0548895, recall 0.863068
2017-12-10T00:21:15.292022: step 488, loss 0.235615, acc 0.914062, prec 0.0548966, recall 0.863177
2017-12-10T00:21:16.208496: step 489, loss 0.27972, acc 0.912109, prec 0.0549825, recall 0.863468
2017-12-10T00:21:17.117573: step 490, loss 0.256305, acc 0.908203, prec 0.0550027, recall 0.863612
2017-12-10T00:21:18.032315: step 491, loss 0.683557, acc 0.916016, prec 0.0550752, recall 0.863636
2017-12-10T00:21:18.941559: step 492, loss 0.234533, acc 0.943359, prec 0.0551598, recall 0.863888
2017-12-10T00:21:19.846471: step 493, loss 0.328547, acc 0.90625, prec 0.0552265, recall 0.864139
2017-12-10T00:21:20.764164: step 494, loss 0.371765, acc 0.916016, prec 0.0553295, recall 0.86446
2017-12-10T00:21:21.682111: step 495, loss 0.336892, acc 0.876953, prec 0.0553344, recall 0.864602
2017-12-10T00:21:22.586096: step 496, loss 0.528391, acc 0.919922, prec 0.0555509, recall 0.864943
2017-12-10T00:21:23.515996: step 497, loss 0.387552, acc 0.902344, prec 0.0556785, recall 0.86533
2017-12-10T00:21:24.435555: step 498, loss 0.368182, acc 0.880859, prec 0.0556848, recall 0.86547
2017-12-10T00:21:25.348162: step 499, loss 0.845799, acc 0.943359, prec 0.0558168, recall 0.865594
2017-12-10T00:21:26.264029: step 500, loss 0.424898, acc 0.900391, prec 0.0559112, recall 0.865907
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_1/1512882823/checkpoints/model-500

2017-12-10T00:21:28.199085: step 501, loss 0.721949, acc 0.865234, prec 0.0559581, recall 0.865926
2017-12-10T00:21:29.111781: step 502, loss 0.297585, acc 0.884766, prec 0.0560132, recall 0.866168
2017-12-10T00:21:30.024416: step 503, loss 0.485353, acc 0.888672, prec 0.0560868, recall 0.866221
2017-12-10T00:21:30.207470: step 504, loss 0.306078, acc 0.862745, prec 0.0560802, recall 0.866221
2017-12-10T00:21:31.131268: step 505, loss 0.398773, acc 0.882812, prec 0.056197, recall 0.866598
2017-12-10T00:21:32.056863: step 506, loss 0.471445, acc 0.873047, prec 0.0561843, recall 0.866479
2017-12-10T00:21:32.968080: step 507, loss 0.279759, acc 0.912109, prec 0.0562676, recall 0.866752
2017-12-10T00:21:33.880393: step 508, loss 0.365124, acc 0.925781, prec 0.0564044, recall 0.867126
2017-12-10T00:21:34.786471: step 509, loss 0.254981, acc 0.902344, prec 0.0564828, recall 0.867396
2017-12-10T00:21:35.719162: step 510, loss 0.241186, acc 0.914062, prec 0.0564885, recall 0.867497
2017-12-10T00:21:36.638260: step 511, loss 0.284275, acc 0.898438, prec 0.0564867, recall 0.867598
2017-12-10T00:21:37.554915: step 512, loss 0.505319, acc 0.941406, prec 0.0565844, recall 0.867647
2017-12-10T00:21:38.464654: step 513, loss 0.898882, acc 0.945312, prec 0.056793, recall 0.867929
2017-12-10T00:21:39.371832: step 514, loss 0.166141, acc 0.957031, prec 0.0568659, recall 0.868129
2017-12-10T00:21:40.293434: step 515, loss 0.280995, acc 0.957031, prec 0.0569542, recall 0.868361
2017-12-10T00:21:41.208088: step 516, loss 0.521169, acc 0.921875, prec 0.0569953, recall 0.868309
2017-12-10T00:21:42.120984: step 517, loss 0.230987, acc 0.941406, prec 0.0570604, recall 0.868507
2017-12-10T00:21:43.031641: step 518, loss 0.329621, acc 0.933594, prec 0.0571527, recall 0.86877
2017-12-10T00:21:43.937646: step 519, loss 0.521988, acc 0.933594, prec 0.0572304, recall 0.868783
2017-12-10T00:21:44.853217: step 520, loss 0.323985, acc 0.933594, prec 0.0573379, recall 0.869077
2017-12-10T00:21:45.778001: step 521, loss 0.401267, acc 0.900391, prec 0.0574138, recall 0.869338
2017-12-10T00:21:46.696090: step 522, loss 0.321733, acc 0.912109, prec 0.0574798, recall 0.869565
2017-12-10T00:21:47.614917: step 523, loss 0.505817, acc 0.880859, prec 0.0575769, recall 0.869888
2017-12-10T00:21:48.530493: step 524, loss 0.301815, acc 0.900391, prec 0.0576368, recall 0.870114
2017-12-10T00:21:49.446326: step 525, loss 0.290131, acc 0.896484, prec 0.0577103, recall 0.87037
2017-12-10T00:21:50.381748: step 526, loss 0.351314, acc 0.888672, prec 0.0577951, recall 0.870658
2017-12-10T00:21:51.327621: step 527, loss 0.32277, acc 0.876953, prec 0.0578126, recall 0.870817
2017-12-10T00:21:52.240250: step 528, loss 0.28521, acc 0.921875, prec 0.0578979, recall 0.871071
2017-12-10T00:21:53.149700: step 529, loss 0.357773, acc 0.898438, prec 0.0580024, recall 0.871387
2017-12-10T00:21:54.059148: step 530, loss 0.412736, acc 0.9375, prec 0.0581257, recall 0.871701
2017-12-10T00:21:54.969502: step 531, loss 0.397422, acc 0.929688, prec 0.0582143, recall 0.871951
2017-12-10T00:21:55.882133: step 532, loss 0.323521, acc 0.923828, prec 0.058254, recall 0.872107
2017-12-10T00:21:56.801589: step 533, loss 0.284405, acc 0.921875, prec 0.0583845, recall 0.872449
2017-12-10T00:21:57.723354: step 534, loss 0.432761, acc 0.933594, prec 0.0584288, recall 0.872604
2017-12-10T00:21:58.637990: step 535, loss 0.338646, acc 0.919922, prec 0.0584816, recall 0.872789
2017-12-10T00:21:59.541298: step 536, loss 0.385834, acc 0.927734, prec 0.0585238, recall 0.872732
2017-12-10T00:22:00.466278: step 537, loss 0.424045, acc 0.931641, prec 0.058629, recall 0.872797
2017-12-10T00:22:01.382923: step 538, loss 0.471349, acc 0.898438, prec 0.0586863, recall 0.873012
2017-12-10T00:22:02.288327: step 539, loss 0.308195, acc 0.9375, prec 0.0588083, recall 0.873317
2017-12-10T00:22:03.205252: step 540, loss 0.33599, acc 0.947266, prec 0.0589501, recall 0.873651
2017-12-10T00:22:04.125702: step 541, loss 0.259017, acc 0.9375, prec 0.0589652, recall 0.873742
2017-12-10T00:22:05.027717: step 542, loss 0.240072, acc 0.914062, prec 0.0589841, recall 0.873863
2017-12-10T00:22:05.959980: step 543, loss 0.343387, acc 0.919922, prec 0.059097, recall 0.874164
2017-12-10T00:22:06.869298: step 544, loss 0.700516, acc 0.921875, prec 0.0591063, recall 0.873837
2017-12-10T00:22:07.781982: step 545, loss 0.614615, acc 0.908203, prec 0.0592293, recall 0.87396
2017-12-10T00:22:08.680350: step 546, loss 0.337022, acc 0.880859, prec 0.0593529, recall 0.874318
2017-12-10T00:22:09.596471: step 547, loss 0.365633, acc 0.910156, prec 0.0595207, recall 0.874734
2017-12-10T00:22:10.507214: step 548, loss 0.441862, acc 0.871094, prec 0.0596238, recall 0.875059
2017-12-10T00:22:11.414612: step 549, loss 0.326102, acc 0.892578, prec 0.0596316, recall 0.875177
2017-12-10T00:22:12.326897: step 550, loss 0.405725, acc 0.898438, prec 0.0597024, recall 0.875411
2017-12-10T00:22:13.227858: step 551, loss 0.360384, acc 0.90625, prec 0.0597319, recall 0.875558
2017-12-10T00:22:14.131047: step 552, loss 0.473397, acc 0.904297, prec 0.0597302, recall 0.875645
2017-12-10T00:22:15.050095: step 553, loss 0.402496, acc 0.917969, prec 0.0598254, recall 0.875907
2017-12-10T00:22:15.959839: step 554, loss 0.210598, acc 0.917969, prec 0.0598904, recall 0.87611
2017-12-10T00:22:16.867615: step 555, loss 0.244943, acc 0.916016, prec 0.0598793, recall 0.876168
2017-12-10T00:22:17.775919: step 556, loss 0.293353, acc 0.917969, prec 0.0599142, recall 0.876313
2017-12-10T00:22:18.679110: step 557, loss 0.385005, acc 0.933594, prec 0.0600316, recall 0.876601
2017-12-10T00:22:19.597427: step 558, loss 0.223243, acc 0.921875, prec 0.0600832, recall 0.876773
2017-12-10T00:22:20.515624: step 559, loss 0.201728, acc 0.933594, prec 0.0601255, recall 0.876916
2017-12-10T00:22:21.451590: step 560, loss 0.408317, acc 0.945312, prec 0.0601894, recall 0.876884
2017-12-10T00:22:22.360851: step 561, loss 0.469436, acc 0.962891, prec 0.060232, recall 0.876795
2017-12-10T00:22:23.265061: step 562, loss 0.375238, acc 0.96875, prec 0.0602923, recall 0.876734
2017-12-10T00:22:24.173086: step 563, loss 0.256953, acc 0.939453, prec 0.0603522, recall 0.876905
2017-12-10T00:22:25.088169: step 564, loss 0.386342, acc 0.955078, prec 0.0604645, recall 0.877161
2017-12-10T00:22:26.002943: step 565, loss 0.19548, acc 0.914062, prec 0.0604969, recall 0.877302
2017-12-10T00:22:26.920172: step 566, loss 0.316667, acc 0.947266, prec 0.0606051, recall 0.877556
2017-12-10T00:22:27.099495: step 567, loss 0.313371, acc 0.921569, prec 0.0606162, recall 0.877584
2017-12-10T00:22:28.015115: step 568, loss 0.251853, acc 0.923828, prec 0.0606531, recall 0.877724
2017-12-10T00:22:28.924792: step 569, loss 0.762619, acc 0.896484, prec 0.0607083, recall 0.877518
2017-12-10T00:22:29.827243: step 570, loss 0.320081, acc 0.894531, prec 0.0607753, recall 0.877742
2017-12-10T00:22:30.764653: step 571, loss 0.453092, acc 0.886719, prec 0.0608829, recall 0.878049
2017-12-10T00:22:31.672239: step 572, loss 0.267264, acc 0.912109, prec 0.0609286, recall 0.878215
2017-12-10T00:22:32.584709: step 573, loss 0.297001, acc 0.888672, prec 0.0609923, recall 0.878437
2017-12-10T00:22:33.505960: step 574, loss 0.332503, acc 0.875, prec 0.0610196, recall 0.878602
2017-12-10T00:22:34.421312: step 575, loss 0.282363, acc 0.927734, prec 0.0610728, recall 0.878767
2017-12-10T00:22:35.337628: step 576, loss 0.460026, acc 0.900391, prec 0.0611715, recall 0.879041
2017-12-10T00:22:36.261198: step 577, loss 0.222306, acc 0.929688, prec 0.0612845, recall 0.879314
2017-12-10T00:22:37.185923: step 578, loss 0.272464, acc 0.919922, prec 0.061304, recall 0.879423
2017-12-10T00:22:38.091923: step 579, loss 0.242059, acc 0.945312, prec 0.0613508, recall 0.879559
2017-12-10T00:22:39.004537: step 580, loss 0.200055, acc 0.935547, prec 0.0614074, recall 0.879721
2017-12-10T00:22:39.922052: step 581, loss 0.356677, acc 0.96875, prec 0.0615549, recall 0.879821
2017-12-10T00:22:40.827225: step 582, loss 0.278084, acc 0.941406, prec 0.0616142, recall 0.879982
2017-12-10T00:22:41.734201: step 583, loss 0.204628, acc 0.957031, prec 0.0617106, recall 0.880197
2017-12-10T00:22:42.639088: step 584, loss 0.226075, acc 0.953125, prec 0.0617462, recall 0.880304
2017-12-10T00:22:43.560732: step 585, loss 0.280821, acc 0.964844, prec 0.0618179, recall 0.880268
2017-12-10T00:22:44.471578: step 586, loss 0.141357, acc 0.960938, prec 0.0619013, recall 0.880454
2017-12-10T00:22:45.375273: step 587, loss 0.296578, acc 0.953125, prec 0.0620102, recall 0.880693
2017-12-10T00:22:46.296487: step 588, loss 0.228386, acc 0.945312, prec 0.0620563, recall 0.880826
2017-12-10T00:22:47.204039: step 589, loss 0.372394, acc 0.951172, prec 0.0621503, recall 0.880842
2017-12-10T00:22:48.127749: step 590, loss 0.136107, acc 0.951172, prec 0.062214, recall 0.881
2017-12-10T00:22:49.037279: step 591, loss 0.124721, acc 0.966797, prec 0.0622999, recall 0.881184
2017-12-10T00:22:49.946117: step 592, loss 0.145727, acc 0.962891, prec 0.0623693, recall 0.881341
2017-12-10T00:22:50.883184: step 593, loss 0.213368, acc 0.953125, prec 0.0625068, recall 0.881628
2017-12-10T00:22:51.795985: step 594, loss 0.292625, acc 0.949219, prec 0.06254, recall 0.881732
2017-12-10T00:22:52.716383: step 595, loss 0.276556, acc 0.927734, prec 0.0626646, recall 0.882018
2017-12-10T00:22:53.631678: step 596, loss 0.349159, acc 0.949219, prec 0.0627278, recall 0.881979
2017-12-10T00:22:54.550573: step 597, loss 0.297071, acc 0.951172, prec 0.0628492, recall 0.882237
2017-12-10T00:22:55.465321: step 598, loss 0.256015, acc 0.919922, prec 0.0629403, recall 0.882468
2017-12-10T00:22:56.370652: step 599, loss 0.398069, acc 0.939453, prec 0.063143, recall 0.882877
2017-12-10T00:22:57.283847: step 600, loss 0.216536, acc 0.931641, prec 0.0631523, recall 0.882953

Evaluation:
2017-12-10T00:23:01.950406: step 600, loss 1.69116, acc 0.927069, prec 0.0636757, recall 0.872582

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_1/1512882823/checkpoints/model-600

2017-12-10T00:23:17.782365: step 601, loss 0.327675, acc 0.931641, prec 0.0637564, recall 0.872796
2017-12-10T00:23:18.700729: step 602, loss 0.244283, acc 0.933594, prec 0.0638666, recall 0.873062
2017-12-10T00:23:19.605558: step 603, loss 0.253392, acc 0.927734, prec 0.0638878, recall 0.873169
2017-12-10T00:23:20.515226: step 604, loss 0.452766, acc 0.892578, prec 0.0639916, recall 0.87346
2017-12-10T00:23:21.439264: step 605, loss 0.308087, acc 0.900391, prec 0.0640561, recall 0.873671
2017-12-10T00:23:22.345184: step 606, loss 0.338561, acc 0.917969, prec 0.0641151, recall 0.873855
2017-12-10T00:23:23.255404: step 607, loss 0.32723, acc 0.914062, prec 0.0643005, recall 0.874274
2017-12-10T00:23:24.167140: step 608, loss 0.203875, acc 0.925781, prec 0.0643489, recall 0.87443
2017-12-10T00:23:25.083231: step 609, loss 0.234162, acc 0.921875, prec 0.0643952, recall 0.874586
2017-12-10T00:23:25.987782: step 610, loss 0.45699, acc 0.925781, prec 0.0644444, recall 0.874561
2017-12-10T00:23:26.904233: step 611, loss 0.229292, acc 0.947266, prec 0.0645318, recall 0.874768
2017-12-10T00:23:27.816057: step 612, loss 0.333748, acc 0.941406, prec 0.0645735, recall 0.874897
2017-12-10T00:23:28.734927: step 613, loss 0.202431, acc 0.953125, prec 0.0647064, recall 0.87518
2017-12-10T00:23:29.643484: step 614, loss 0.304651, acc 0.955078, prec 0.0647407, recall 0.875283
2017-12-10T00:23:30.574443: step 615, loss 0.266933, acc 0.941406, prec 0.064768, recall 0.875385
2017-12-10T00:23:31.484588: step 616, loss 0.177228, acc 0.945312, prec 0.0648398, recall 0.875564
2017-12-10T00:23:32.391936: step 617, loss 0.436702, acc 0.925781, prec 0.0648744, recall 0.875512
2017-12-10T00:23:33.307020: step 618, loss 0.289116, acc 0.929688, prec 0.064924, recall 0.875665
2017-12-10T00:23:34.235829: step 619, loss 0.198518, acc 0.933594, prec 0.0649756, recall 0.875817
2017-12-10T00:23:35.146523: step 620, loss 0.21816, acc 0.939453, prec 0.0650159, recall 0.875944
2017-12-10T00:23:36.079130: step 621, loss 0.276912, acc 0.947266, prec 0.0650884, recall 0.876121
2017-12-10T00:23:36.978967: step 622, loss 0.787943, acc 0.9375, prec 0.0651437, recall 0.875915
2017-12-10T00:23:37.883205: step 623, loss 0.339067, acc 0.939453, prec 0.0652131, recall 0.875914
2017-12-10T00:23:38.791790: step 624, loss 0.373088, acc 0.919922, prec 0.0652857, recall 0.876115
2017-12-10T00:23:39.696434: step 625, loss 0.315985, acc 0.917969, prec 0.0653572, recall 0.876316
2017-12-10T00:23:40.613396: step 626, loss 0.189681, acc 0.931641, prec 0.065365, recall 0.876391
2017-12-10T00:23:41.534277: step 627, loss 0.504573, acc 0.933594, prec 0.0654734, recall 0.876463
2017-12-10T00:23:42.452513: step 628, loss 0.347414, acc 0.916016, prec 0.0655999, recall 0.876762
2017-12-10T00:23:43.377313: step 629, loss 0.396199, acc 0.875, prec 0.0656071, recall 0.876886
2017-12-10T00:23:43.561101: step 630, loss 0.0940123, acc 0.980392, prec 0.0656202, recall 0.876911
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 512
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_512_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_2/1512883424

Start training
2017-12-10T00:23:48.093300: step 1, loss 9.43359, acc 0.0625, prec 0.0164271, recall 0.888889
2017-12-10T00:23:48.994721: step 2, loss 4.69989, acc 0.216797, prec 0.0156775, recall 0.875
2017-12-10T00:23:49.897806: step 3, loss 9.03879, acc 0.519531, prec 0.0141343, recall 0.592593
2017-12-10T00:23:50.828738: step 4, loss 3.84086, acc 0.650391, prec 0.0144817, recall 0.59375
2017-12-10T00:23:51.727482: step 5, loss 4.21925, acc 0.636719, prec 0.0146862, recall 0.578947
2017-12-10T00:23:52.639351: step 6, loss 6.47878, acc 0.621094, prec 0.0147754, recall 0.568182
2017-12-10T00:23:53.553552: step 7, loss 2.99739, acc 0.556641, prec 0.0151042, recall 0.568627
2017-12-10T00:23:54.460268: step 8, loss 5.44837, acc 0.482422, prec 0.0141941, recall 0.553571
2017-12-10T00:23:55.365088: step 9, loss 5.7658, acc 0.521484, prec 0.013185, recall 0.533333
2017-12-10T00:23:56.268162: step 10, loss 5.52119, acc 0.435547, prec 0.0121681, recall 0.5
2017-12-10T00:23:57.176681: step 11, loss 4.61535, acc 0.382812, prec 0.0122072, recall 0.521127
2017-12-10T00:23:58.085106: step 12, loss 3.03199, acc 0.378906, prec 0.0131108, recall 0.564103
2017-12-10T00:23:59.001089: step 13, loss 3.83498, acc 0.392578, prec 0.012817, recall 0.559524
2017-12-10T00:23:59.899948: step 14, loss 4.00728, acc 0.457031, prec 0.0129212, recall 0.566667
2017-12-10T00:24:00.825876: step 15, loss 8.18717, acc 0.509766, prec 0.0138095, recall 0.568627
2017-12-10T00:24:01.737730: step 16, loss 4.82915, acc 0.484375, prec 0.0138827, recall 0.574074
2017-12-10T00:24:02.644850: step 17, loss 3.41621, acc 0.570312, prec 0.014919, recall 0.59322
2017-12-10T00:24:03.557313: step 18, loss 2.10422, acc 0.546875, prec 0.0154158, recall 0.612903
2017-12-10T00:24:04.465515: step 19, loss 3.13303, acc 0.601562, prec 0.015768, recall 0.618321
2017-12-10T00:24:05.378734: step 20, loss 2.27286, acc 0.625, prec 0.0153904, recall 0.61194
2017-12-10T00:24:06.323461: step 21, loss 1.46765, acc 0.654297, prec 0.0159681, recall 0.628571
2017-12-10T00:24:07.231972: step 22, loss 3.87013, acc 0.695312, prec 0.0164079, recall 0.624161
2017-12-10T00:24:08.140104: step 23, loss 5.41599, acc 0.763672, prec 0.0165889, recall 0.611465
2017-12-10T00:24:09.058990: step 24, loss 3.15428, acc 0.71875, prec 0.0171862, recall 0.618182
2017-12-10T00:24:09.976109: step 25, loss 1.80071, acc 0.681641, prec 0.0172131, recall 0.621302
2017-12-10T00:24:10.890192: step 26, loss 9.61889, acc 0.642578, prec 0.017823, recall 0.615385
2017-12-10T00:24:11.810514: step 27, loss 3.80858, acc 0.654297, prec 0.0185586, recall 0.621762
2017-12-10T00:24:12.720492: step 28, loss 2.59726, acc 0.511719, prec 0.0188932, recall 0.631841
2017-12-10T00:24:13.636462: step 29, loss 2.63098, acc 0.474609, prec 0.0187277, recall 0.639024
2017-12-10T00:24:14.549367: step 30, loss 3.36515, acc 0.513672, prec 0.0190397, recall 0.64186
2017-12-10T00:24:15.466150: step 31, loss 2.69051, acc 0.515625, prec 0.0194563, recall 0.654709
2017-12-10T00:24:16.373960: step 32, loss 4.60007, acc 0.443359, prec 0.0195072, recall 0.655172
2017-12-10T00:24:17.288893: step 33, loss 3.71122, acc 0.509766, prec 0.019269, recall 0.654008
2017-12-10T00:24:18.188998: step 34, loss 4.2083, acc 0.548828, prec 0.0201593, recall 0.662698
2017-12-10T00:24:19.103186: step 35, loss 2.91058, acc 0.554688, prec 0.0206597, recall 0.669201
2017-12-10T00:24:20.013097: step 36, loss 4.54601, acc 0.525391, prec 0.0207668, recall 0.666667
2017-12-10T00:24:20.948300: step 37, loss 6.17524, acc 0.537109, prec 0.0208819, recall 0.664311
2017-12-10T00:24:21.862993: step 38, loss 2.27392, acc 0.53125, prec 0.0210811, recall 0.672414
2017-12-10T00:24:22.762447: step 39, loss 2.68117, acc 0.560547, prec 0.0215122, recall 0.68
2017-12-10T00:24:23.685564: step 40, loss 3.17432, acc 0.53125, prec 0.0212875, recall 0.678689
2017-12-10T00:24:24.594431: step 41, loss 4.09622, acc 0.550781, prec 0.0216846, recall 0.683544
2017-12-10T00:24:25.505387: step 42, loss 3.1302, acc 0.574219, prec 0.0219931, recall 0.687117
2017-12-10T00:24:26.424783: step 43, loss 2.34089, acc 0.591797, prec 0.0222137, recall 0.689552
2017-12-10T00:24:27.350741: step 44, loss 3.54275, acc 0.599609, prec 0.0224317, recall 0.693878
2017-12-10T00:24:28.260328: step 45, loss 2.21038, acc 0.673828, prec 0.0225397, recall 0.696275
2017-12-10T00:24:29.184511: step 46, loss 2.70145, acc 0.693359, prec 0.022665, recall 0.698592
2017-12-10T00:24:30.096692: step 47, loss 4.65307, acc 0.710938, prec 0.0232516, recall 0.699187
2017-12-10T00:24:31.018664: step 48, loss 2.7393, acc 0.701172, prec 0.0236381, recall 0.698163
2017-12-10T00:24:31.930846: step 49, loss 1.31119, acc 0.714844, prec 0.0239348, recall 0.703608
2017-12-10T00:24:32.839755: step 50, loss 3.30696, acc 0.671875, prec 0.0241016, recall 0.701005
2017-12-10T00:24:33.749091: step 51, loss 3.90593, acc 0.664062, prec 0.0240872, recall 0.698765
2017-12-10T00:24:34.683226: step 52, loss 1.68737, acc 0.642578, prec 0.0237995, recall 0.699507
2017-12-10T00:24:35.591475: step 53, loss 2.87808, acc 0.621094, prec 0.0239037, recall 0.702179
2017-12-10T00:24:36.514417: step 54, loss 2.18033, acc 0.578125, prec 0.0238808, recall 0.705742
2017-12-10T00:24:37.428551: step 55, loss 3.27817, acc 0.599609, prec 0.0238057, recall 0.705189
2017-12-10T00:24:38.342313: step 56, loss 2.34705, acc 0.630859, prec 0.0236844, recall 0.705607
2017-12-10T00:24:39.264994: step 57, loss 1.82508, acc 0.652344, prec 0.0238132, recall 0.708046
2017-12-10T00:24:40.186699: step 58, loss 1.83686, acc 0.699219, prec 0.0238332, recall 0.709091
2017-12-10T00:24:41.102750: step 59, loss 3.36008, acc 0.681641, prec 0.0236945, recall 0.704036
2017-12-10T00:24:42.017251: step 60, loss 3.48179, acc 0.699219, prec 0.0237918, recall 0.702643
2017-12-10T00:24:42.926961: step 61, loss 3.00754, acc 0.648438, prec 0.023771, recall 0.699134
2017-12-10T00:24:43.836275: step 62, loss 2.71076, acc 0.625, prec 0.0238683, recall 0.7
2017-12-10T00:24:44.018631: step 63, loss 1.77107, acc 0.596154, prec 0.0238319, recall 0.7
2017-12-10T00:24:44.936664: step 64, loss 1.81422, acc 0.613281, prec 0.0239149, recall 0.702306
2017-12-10T00:24:45.877256: step 65, loss 1.72797, acc 0.607422, prec 0.0241947, recall 0.707819
2017-12-10T00:24:46.782344: step 66, loss 2.19077, acc 0.632812, prec 0.024221, recall 0.707911
2017-12-10T00:24:47.693227: step 67, loss 1.96875, acc 0.658203, prec 0.0246008, recall 0.712302
2017-12-10T00:24:48.612995: step 68, loss 2.47956, acc 0.724609, prec 0.0248321, recall 0.71345
2017-12-10T00:24:49.523252: step 69, loss 2.92197, acc 0.703125, prec 0.0247146, recall 0.710425
2017-12-10T00:24:50.427544: step 70, loss 3.09386, acc 0.693359, prec 0.0250432, recall 0.712665
2017-12-10T00:24:51.353901: step 71, loss 3.06896, acc 0.699219, prec 0.0248586, recall 0.709193
2017-12-10T00:24:52.279503: step 72, loss 1.38538, acc 0.654297, prec 0.0250796, recall 0.713494
2017-12-10T00:24:53.208745: step 73, loss 2.91079, acc 0.669922, prec 0.0249373, recall 0.710623
2017-12-10T00:24:54.111660: step 74, loss 1.28897, acc 0.666016, prec 0.0250381, recall 0.713768
2017-12-10T00:24:55.010721: step 75, loss 1.67669, acc 0.683594, prec 0.0249686, recall 0.714029
2017-12-10T00:24:55.931633: step 76, loss 1.37874, acc 0.712891, prec 0.0250452, recall 0.715302
2017-12-10T00:24:56.844545: step 77, loss 1.40977, acc 0.710938, prec 0.0252391, recall 0.717544
2017-12-10T00:24:57.767974: step 78, loss 1.16376, acc 0.705078, prec 0.0253636, recall 0.720486
2017-12-10T00:24:58.679878: step 79, loss 1.94633, acc 0.755859, prec 0.0252927, recall 0.718966
2017-12-10T00:24:59.597539: step 80, loss 1.66321, acc 0.789062, prec 0.0254835, recall 0.719388
2017-12-10T00:25:00.521769: step 81, loss 0.736925, acc 0.822266, prec 0.0256364, recall 0.721754
2017-12-10T00:25:01.420361: step 82, loss 2.01428, acc 0.773438, prec 0.0256944, recall 0.721202
2017-12-10T00:25:02.333531: step 83, loss 0.605714, acc 0.826172, prec 0.025732, recall 0.722591
2017-12-10T00:25:03.238956: step 84, loss 6.28264, acc 0.830078, prec 0.0260665, recall 0.720325
2017-12-10T00:25:04.139761: step 85, loss 5.25764, acc 0.796875, prec 0.0263712, recall 0.718153
2017-12-10T00:25:05.046970: step 86, loss 1.63155, acc 0.763672, prec 0.0265846, recall 0.718995
2017-12-10T00:25:05.978253: step 87, loss 3.69187, acc 0.740234, prec 0.026722, recall 0.718266
2017-12-10T00:25:06.893013: step 88, loss 1.85033, acc 0.611328, prec 0.0266978, recall 0.719325
2017-12-10T00:25:07.805500: step 89, loss 3.6893, acc 0.589844, prec 0.0266074, recall 0.716667
2017-12-10T00:25:08.717464: step 90, loss 2.76521, acc 0.447266, prec 0.02646, recall 0.718797
2017-12-10T00:25:09.621647: step 91, loss 2.36212, acc 0.476562, prec 0.0263918, recall 0.721311
2017-12-10T00:25:10.530379: step 92, loss 2.62538, acc 0.509766, prec 0.0266115, recall 0.725806
2017-12-10T00:25:11.438652: step 93, loss 2.57473, acc 0.474609, prec 0.0266963, recall 0.729378
2017-12-10T00:25:12.341896: step 94, loss 2.27446, acc 0.498047, prec 0.0267447, recall 0.732475
2017-12-10T00:25:13.251636: step 95, loss 1.9197, acc 0.574219, prec 0.0266446, recall 0.733997
2017-12-10T00:25:14.159012: step 96, loss 1.33015, acc 0.667969, prec 0.0268113, recall 0.73699
2017-12-10T00:25:15.071465: step 97, loss 3.19951, acc 0.728516, prec 0.0268746, recall 0.734722
2017-12-10T00:25:16.003416: step 98, loss 3.03117, acc 0.785156, prec 0.0272222, recall 0.734332
2017-12-10T00:25:16.920546: step 99, loss 5.16253, acc 0.794922, prec 0.0274275, recall 0.731903
2017-12-10T00:25:17.842397: step 100, loss 1.02506, acc 0.783203, prec 0.0274226, recall 0.732
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_2/1512883424/checkpoints/model-100

2017-12-10T00:25:19.751134: step 101, loss 3.96678, acc 0.730469, prec 0.0273354, recall 0.729801
2017-12-10T00:25:20.666097: step 102, loss 1.04846, acc 0.730469, prec 0.0273892, recall 0.731579
2017-12-10T00:25:21.580755: step 103, loss 1.44113, acc 0.712891, prec 0.0274314, recall 0.732376
2017-12-10T00:25:22.498455: step 104, loss 4.39477, acc 0.699219, prec 0.0274677, recall 0.730323
2017-12-10T00:25:23.414784: step 105, loss 2.489, acc 0.646484, prec 0.0274638, recall 0.731114
2017-12-10T00:25:24.326586: step 106, loss 1.66056, acc 0.611328, prec 0.0274351, recall 0.732824
2017-12-10T00:25:25.243459: step 107, loss 2.02489, acc 0.585938, prec 0.0272997, recall 0.732911
2017-12-10T00:25:26.156096: step 108, loss 1.97394, acc 0.564453, prec 0.0274241, recall 0.73592
2017-12-10T00:25:27.065044: step 109, loss 1.66671, acc 0.603516, prec 0.0274814, recall 0.738213
2017-12-10T00:25:27.970189: step 110, loss 1.39069, acc 0.632812, prec 0.0275565, recall 0.740467
2017-12-10T00:25:28.894189: step 111, loss 1.57318, acc 0.693359, prec 0.0276276, recall 0.74056
2017-12-10T00:25:29.815151: step 112, loss 2.67988, acc 0.726562, prec 0.0277627, recall 0.740964
2017-12-10T00:25:30.758611: step 113, loss 1.10374, acc 0.728516, prec 0.0278949, recall 0.74313
2017-12-10T00:25:31.670794: step 114, loss 4.52606, acc 0.767578, prec 0.0278819, recall 0.740521
2017-12-10T00:25:32.574607: step 115, loss 1.69938, acc 0.775391, prec 0.0280021, recall 0.739742
2017-12-10T00:25:33.495084: step 116, loss 1.95488, acc 0.785156, prec 0.0280844, recall 0.738676
2017-12-10T00:25:34.401396: step 117, loss 1.42339, acc 0.800781, prec 0.0281746, recall 0.738479
2017-12-10T00:25:35.326022: step 118, loss 1.5499, acc 0.757812, prec 0.0283217, recall 0.738883
2017-12-10T00:25:36.246516: step 119, loss 1.88805, acc 0.726562, prec 0.0284473, recall 0.739278
2017-12-10T00:25:37.154951: step 120, loss 1.18579, acc 0.716797, prec 0.0287723, recall 0.742762
2017-12-10T00:25:38.055541: step 121, loss 1.52399, acc 0.646484, prec 0.0288416, recall 0.743929
2017-12-10T00:25:38.958691: step 122, loss 1.4056, acc 0.699219, prec 0.0288592, recall 0.745335
2017-12-10T00:25:39.882680: step 123, loss 1.50432, acc 0.628906, prec 0.0289556, recall 0.747552
2017-12-10T00:25:40.805052: step 124, loss 1.49969, acc 0.699219, prec 0.0290547, recall 0.748652
2017-12-10T00:25:41.724867: step 125, loss 1.9071, acc 0.681641, prec 0.0293433, recall 0.751064
2017-12-10T00:25:41.911140: step 126, loss 1.08662, acc 0.711538, prec 0.029325, recall 0.751064
2017-12-10T00:25:42.829301: step 127, loss 0.88189, acc 0.794922, prec 0.029599, recall 0.753684
2017-12-10T00:25:43.733801: step 128, loss 0.759868, acc 0.789062, prec 0.0296671, recall 0.754974
2017-12-10T00:25:44.634832: step 129, loss 0.684288, acc 0.8125, prec 0.0297095, recall 0.755996
2017-12-10T00:25:45.555073: step 130, loss 4.11088, acc 0.851562, prec 0.029902, recall 0.753086
2017-12-10T00:25:46.471034: step 131, loss 4.09579, acc 0.806641, prec 0.029906, recall 0.75
2017-12-10T00:25:47.393045: step 132, loss 1.10142, acc 0.769531, prec 0.0301166, recall 0.752275
2017-12-10T00:25:48.326521: step 133, loss 2.50748, acc 0.775391, prec 0.0302542, recall 0.751752
2017-12-10T00:25:49.252378: step 134, loss 1.13274, acc 0.714844, prec 0.0306208, recall 0.755183
2017-12-10T00:25:50.176177: step 135, loss 1.4236, acc 0.677734, prec 0.0306909, recall 0.756121
2017-12-10T00:25:51.101945: step 136, loss 1.73587, acc 0.675781, prec 0.0307972, recall 0.757282
2017-12-10T00:25:52.008630: step 137, loss 1.7714, acc 0.615234, prec 0.0309024, recall 0.758654
2017-12-10T00:25:52.921318: step 138, loss 1.57287, acc 0.626953, prec 0.030899, recall 0.760038
2017-12-10T00:25:53.828899: step 139, loss 2.09365, acc 0.619141, prec 0.0308173, recall 0.760228
2017-12-10T00:25:54.740990: step 140, loss 1.49281, acc 0.621094, prec 0.0307739, recall 0.761364
2017-12-10T00:25:55.646291: step 141, loss 1.17809, acc 0.654297, prec 0.0308984, recall 0.76338
2017-12-10T00:25:56.541314: step 142, loss 1.97845, acc 0.681641, prec 0.0309656, recall 0.764212
2017-12-10T00:25:57.439759: step 143, loss 1.0436, acc 0.720703, prec 0.0310176, recall 0.765524
2017-12-10T00:25:58.336751: step 144, loss 2.19101, acc 0.724609, prec 0.0311823, recall 0.766055
2017-12-10T00:25:59.249070: step 145, loss 1.55456, acc 0.761719, prec 0.0311152, recall 0.765082
2017-12-10T00:26:00.166182: step 146, loss 1.4974, acc 0.798828, prec 0.0312141, recall 0.764973
2017-12-10T00:26:01.081749: step 147, loss 1.11198, acc 0.765625, prec 0.0311843, recall 0.764919
2017-12-10T00:26:01.987880: step 148, loss 2.9557, acc 0.806641, prec 0.0313934, recall 0.765443
2017-12-10T00:26:02.895584: step 149, loss 2.20754, acc 0.818359, prec 0.031609, recall 0.765279
2017-12-10T00:26:03.806943: step 150, loss 2.08704, acc 0.794922, prec 0.0315985, recall 0.763204
2017-12-10T00:26:04.717879: step 151, loss 1.78937, acc 0.78125, prec 0.0316118, recall 0.763365
2017-12-10T00:26:05.630443: step 152, loss 1.04699, acc 0.746094, prec 0.0318131, recall 0.765421
2017-12-10T00:26:06.546830: step 153, loss 1.7178, acc 0.742188, prec 0.0317677, recall 0.765368
2017-12-10T00:26:07.451831: step 154, loss 1.22907, acc 0.699219, prec 0.0318362, recall 0.766122
2017-12-10T00:26:08.358180: step 155, loss 0.904045, acc 0.716797, prec 0.0320162, recall 0.768116
2017-12-10T00:26:09.264675: step 156, loss 1.80319, acc 0.679688, prec 0.0320028, recall 0.768448
2017-12-10T00:26:10.175474: step 157, loss 2.13711, acc 0.673828, prec 0.0320553, recall 0.768519
2017-12-10T00:26:11.111860: step 158, loss 1.27157, acc 0.6875, prec 0.032079, recall 0.769682
2017-12-10T00:26:12.022527: step 159, loss 1.40223, acc 0.697266, prec 0.0321091, recall 0.770191
2017-12-10T00:26:12.934573: step 160, loss 1.67695, acc 0.662109, prec 0.0322191, recall 0.771263
2017-12-10T00:26:13.839502: step 161, loss 0.914927, acc 0.753906, prec 0.0322791, recall 0.772391
2017-12-10T00:26:14.741959: step 162, loss 1.52151, acc 0.720703, prec 0.0323539, recall 0.773061
2017-12-10T00:26:15.666314: step 163, loss 3.49205, acc 0.730469, prec 0.0324675, recall 0.773279
2017-12-10T00:26:16.570140: step 164, loss 0.908671, acc 0.761719, prec 0.0326937, recall 0.775281
2017-12-10T00:26:17.481113: step 165, loss 1.39449, acc 0.761719, prec 0.0327245, recall 0.77494
2017-12-10T00:26:18.398354: step 166, loss 1.24505, acc 0.75, prec 0.0326835, recall 0.774245
2017-12-10T00:26:19.309596: step 167, loss 1.14296, acc 0.761719, prec 0.0328422, recall 0.775237
2017-12-10T00:26:20.224904: step 168, loss 1.73267, acc 0.794922, prec 0.0330182, recall 0.776213
2017-12-10T00:26:21.156310: step 169, loss 0.885472, acc 0.761719, prec 0.0331732, recall 0.777778
2017-12-10T00:26:22.060508: step 170, loss 0.77448, acc 0.757812, prec 0.0331331, recall 0.778295
2017-12-10T00:26:22.973411: step 171, loss 0.769517, acc 0.789062, prec 0.0333333, recall 0.78
2017-12-10T00:26:23.887328: step 172, loss 1.36769, acc 0.783203, prec 0.0334032, recall 0.780413
2017-12-10T00:26:24.801743: step 173, loss 0.592515, acc 0.796875, prec 0.0334475, recall 0.78125
2017-12-10T00:26:25.708663: step 174, loss 1.11733, acc 0.826172, prec 0.0334797, recall 0.780136
2017-12-10T00:26:26.621967: step 175, loss 0.543143, acc 0.820312, prec 0.0335052, recall 0.780801
2017-12-10T00:26:27.525993: step 176, loss 1.22195, acc 0.841797, prec 0.0335134, recall 0.78012
2017-12-10T00:26:28.444627: step 177, loss 1.58531, acc 0.841797, prec 0.0334893, recall 0.779865
2017-12-10T00:26:29.352239: step 178, loss 1.41632, acc 0.818359, prec 0.0335145, recall 0.77994
2017-12-10T00:26:30.264382: step 179, loss 1.51124, acc 0.841797, prec 0.0335226, recall 0.779269
2017-12-10T00:26:31.179317: step 180, loss 2.13333, acc 0.808594, prec 0.0334847, recall 0.776706
2017-12-10T00:26:32.097527: step 181, loss 1.15481, acc 0.810547, prec 0.0336903, recall 0.777778
2017-12-10T00:26:33.013069: step 182, loss 0.733228, acc 0.753906, prec 0.0337089, recall 0.778592
2017-12-10T00:26:33.925965: step 183, loss 0.935964, acc 0.75, prec 0.0337253, recall 0.779401
2017-12-10T00:26:34.837907: step 184, loss 1.05159, acc 0.736328, prec 0.0338263, recall 0.780116
2017-12-10T00:26:35.769931: step 185, loss 1.20385, acc 0.724609, prec 0.0340422, recall 0.780891
2017-12-10T00:26:36.678087: step 186, loss 1.11567, acc 0.65625, prec 0.0339157, recall 0.781205
2017-12-10T00:26:37.598263: step 187, loss 1.16414, acc 0.722656, prec 0.0339771, recall 0.78174
2017-12-10T00:26:38.501856: step 188, loss 1.22536, acc 0.730469, prec 0.0339836, recall 0.781405
2017-12-10T00:26:38.686492: step 189, loss 0.387082, acc 0.826923, prec 0.034004, recall 0.78156
2017-12-10T00:26:39.606490: step 190, loss 0.883514, acc 0.761719, prec 0.034115, recall 0.782241
2017-12-10T00:26:40.520242: step 191, loss 1.03764, acc 0.792969, prec 0.0342124, recall 0.782761
2017-12-10T00:26:41.435542: step 192, loss 0.55694, acc 0.837891, prec 0.0343026, recall 0.783671
2017-12-10T00:26:42.340097: step 193, loss 0.567285, acc 0.806641, prec 0.0344933, recall 0.78517
2017-12-10T00:26:43.251028: step 194, loss 1.21084, acc 0.84375, prec 0.0346754, recall 0.78542
2017-12-10T00:26:44.160154: step 195, loss 0.937036, acc 0.898438, prec 0.0347399, recall 0.784931
2017-12-10T00:26:45.072759: step 196, loss 0.673749, acc 0.880859, prec 0.0347937, recall 0.784983
2017-12-10T00:26:45.977764: step 197, loss 3.01848, acc 0.910156, prec 0.0349526, recall 0.784407
2017-12-10T00:26:46.886605: step 198, loss 0.433235, acc 0.867188, prec 0.0351136, recall 0.78557
2017-12-10T00:26:47.786306: step 199, loss 2.37594, acc 0.847656, prec 0.0351806, recall 0.784182
2017-12-10T00:26:48.698743: step 200, loss 0.58738, acc 0.816406, prec 0.035255, recall 0.785047
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_2/1512883424/checkpoints/model-200

2017-12-10T00:26:50.901487: step 201, loss 0.637625, acc 0.796875, prec 0.0352607, recall 0.785619
2017-12-10T00:26:51.810217: step 202, loss 1.56316, acc 0.742188, prec 0.0352391, recall 0.785146
2017-12-10T00:26:52.737574: step 203, loss 0.844772, acc 0.753906, prec 0.0352219, recall 0.785714
2017-12-10T00:26:53.662159: step 204, loss 1.27957, acc 0.744141, prec 0.0354001, recall 0.786745
2017-12-10T00:26:54.579047: step 205, loss 2.40546, acc 0.708984, prec 0.0353311, recall 0.786649
2017-12-10T00:26:55.497426: step 206, loss 0.938418, acc 0.716797, prec 0.035407, recall 0.78776
2017-12-10T00:26:56.442175: step 207, loss 0.83762, acc 0.734375, prec 0.0354635, recall 0.788723
2017-12-10T00:26:57.357193: step 208, loss 1.04908, acc 0.769531, prec 0.035623, recall 0.789575
2017-12-10T00:26:58.263964: step 209, loss 1.67791, acc 0.761719, prec 0.0357773, recall 0.790415
2017-12-10T00:26:59.167671: step 210, loss 0.846289, acc 0.753906, prec 0.0357863, recall 0.791083
2017-12-10T00:27:00.072960: step 211, loss 0.569485, acc 0.806641, prec 0.0357399, recall 0.791349
2017-12-10T00:27:00.996989: step 212, loss 1.86436, acc 0.837891, prec 0.0358238, recall 0.790639
2017-12-10T00:27:01.911398: step 213, loss 0.956263, acc 0.828125, prec 0.0358725, recall 0.7908
2017-12-10T00:27:02.816747: step 214, loss 1.08415, acc 0.830078, prec 0.0360584, recall 0.79211
2017-12-10T00:27:03.719895: step 215, loss 0.621375, acc 0.830078, prec 0.036161, recall 0.793017
2017-12-10T00:27:04.636326: step 216, loss 0.475759, acc 0.832031, prec 0.0361548, recall 0.793404
2017-12-10T00:27:05.552580: step 217, loss 0.495965, acc 0.845703, prec 0.0363467, recall 0.794681
2017-12-10T00:27:06.470904: step 218, loss 0.674116, acc 0.851562, prec 0.0364863, recall 0.795692
2017-12-10T00:27:07.375870: step 219, loss 0.542568, acc 0.861328, prec 0.0366575, recall 0.796818
2017-12-10T00:27:08.280996: step 220, loss 0.841409, acc 0.832031, prec 0.0368405, recall 0.797568
2017-12-10T00:27:09.189974: step 221, loss 0.412144, acc 0.841797, prec 0.0369727, recall 0.798548
2017-12-10T00:27:10.102088: step 222, loss 0.556842, acc 0.859375, prec 0.0370598, recall 0.799277
2017-12-10T00:27:11.020966: step 223, loss 1.10823, acc 0.855469, prec 0.0371465, recall 0.79904
2017-12-10T00:27:11.933806: step 224, loss 2.39254, acc 0.853516, prec 0.0371782, recall 0.798565
2017-12-10T00:27:12.837274: step 225, loss 0.471074, acc 0.847656, prec 0.0372314, recall 0.799166
2017-12-10T00:27:13.753727: step 226, loss 0.444235, acc 0.873047, prec 0.0374044, recall 0.800237
2017-12-10T00:27:14.662828: step 227, loss 1.10349, acc 0.832031, prec 0.0376368, recall 0.800705
2017-12-10T00:27:15.575757: step 228, loss 1.43889, acc 0.841797, prec 0.0377671, recall 0.800701
2017-12-10T00:27:16.485871: step 229, loss 1.29508, acc 0.832031, prec 0.0378396, recall 0.8
2017-12-10T00:27:17.408252: step 230, loss 1.24976, acc 0.785156, prec 0.0380694, recall 0.801038
2017-12-10T00:27:18.317839: step 231, loss 0.820555, acc 0.771484, prec 0.0382367, recall 0.802292
2017-12-10T00:27:19.225433: step 232, loss 0.856598, acc 0.722656, prec 0.0382983, recall 0.803195
2017-12-10T00:27:20.135885: step 233, loss 0.79712, acc 0.746094, prec 0.0383198, recall 0.803866
2017-12-10T00:27:21.074040: step 234, loss 1.10304, acc 0.703125, prec 0.0382155, recall 0.803632
2017-12-10T00:27:21.996163: step 235, loss 0.739413, acc 0.761719, prec 0.0381936, recall 0.804077
2017-12-10T00:27:22.896654: step 236, loss 0.802082, acc 0.759766, prec 0.038145, recall 0.804409
2017-12-10T00:27:23.809292: step 237, loss 0.975721, acc 0.818359, prec 0.0383855, recall 0.805384
2017-12-10T00:27:24.732572: step 238, loss 0.607569, acc 0.810547, prec 0.0383375, recall 0.805602
2017-12-10T00:27:25.644894: step 239, loss 1.09101, acc 0.832031, prec 0.0383797, recall 0.805246
2017-12-10T00:27:26.550305: step 240, loss 0.619496, acc 0.84375, prec 0.0384013, recall 0.805231
2017-12-10T00:27:27.473214: step 241, loss 1.07594, acc 0.820312, prec 0.0384626, recall 0.804986
2017-12-10T00:27:28.385337: step 242, loss 1.49602, acc 0.847656, prec 0.0385621, recall 0.805295
2017-12-10T00:27:29.311517: step 243, loss 0.684391, acc 0.800781, prec 0.0386358, recall 0.806044
2017-12-10T00:27:30.245540: step 244, loss 1.12401, acc 0.849609, prec 0.0386597, recall 0.806027
2017-12-10T00:27:31.157629: step 245, loss 1.17109, acc 0.800781, prec 0.0387095, recall 0.805783
2017-12-10T00:27:32.080092: step 246, loss 0.636358, acc 0.828125, prec 0.0387964, recall 0.806522
2017-12-10T00:27:32.986690: step 247, loss 0.720798, acc 0.767578, prec 0.0388264, recall 0.807151
2017-12-10T00:27:33.890983: step 248, loss 0.716269, acc 0.816406, prec 0.0388814, recall 0.807775
2017-12-10T00:27:34.817837: step 249, loss 0.987351, acc 0.796875, prec 0.0389769, recall 0.808168
2017-12-10T00:27:35.733919: step 250, loss 0.827494, acc 0.802734, prec 0.03905, recall 0.808454
2017-12-10T00:27:36.647627: step 251, loss 0.540551, acc 0.847656, prec 0.039244, recall 0.809574
2017-12-10T00:27:36.832593: step 252, loss 0.388847, acc 0.846154, prec 0.0392359, recall 0.809574
2017-12-10T00:27:37.754532: step 253, loss 1.13029, acc 0.785156, prec 0.0393739, recall 0.809725
2017-12-10T00:27:38.657511: step 254, loss 0.528775, acc 0.849609, prec 0.0393947, recall 0.810127
2017-12-10T00:27:39.558858: step 255, loss 0.856774, acc 0.830078, prec 0.0393572, recall 0.8099
2017-12-10T00:27:40.471519: step 256, loss 0.674844, acc 0.832031, prec 0.0396141, recall 0.811291
2017-12-10T00:27:41.384457: step 257, loss 0.911198, acc 0.841797, prec 0.0398034, recall 0.811526
2017-12-10T00:27:42.300696: step 258, loss 0.949793, acc 0.857422, prec 0.0399756, recall 0.811661
2017-12-10T00:27:43.230644: step 259, loss 0.890749, acc 0.820312, prec 0.0401267, recall 0.812211
2017-12-10T00:27:44.142655: step 260, loss 0.800714, acc 0.794922, prec 0.0401668, recall 0.812372
2017-12-10T00:27:45.056961: step 261, loss 0.535698, acc 0.818359, prec 0.0402663, recall 0.813136
2017-12-10T00:27:45.961355: step 262, loss 1.9933, acc 0.794922, prec 0.0404043, recall 0.812848
2017-12-10T00:27:46.877946: step 263, loss 0.613152, acc 0.794922, prec 0.0406107, recall 0.81407
2017-12-10T00:27:47.788227: step 264, loss 0.666837, acc 0.796875, prec 0.040649, recall 0.814629
2017-12-10T00:27:48.688760: step 265, loss 0.640713, acc 0.769531, prec 0.0406729, recall 0.815185
2017-12-10T00:27:49.580148: step 266, loss 0.73386, acc 0.810547, prec 0.0407656, recall 0.81592
2017-12-10T00:27:50.495554: step 267, loss 0.637104, acc 0.804688, prec 0.040831, recall 0.816559
2017-12-10T00:27:51.414743: step 268, loss 0.525803, acc 0.824219, prec 0.0408824, recall 0.817103
2017-12-10T00:27:52.346480: step 269, loss 0.460401, acc 0.853516, prec 0.0409725, recall 0.817734
2017-12-10T00:27:53.265592: step 270, loss 0.469976, acc 0.882812, prec 0.0410537, recall 0.818271
2017-12-10T00:27:54.171430: step 271, loss 0.571994, acc 0.90625, prec 0.0412422, recall 0.818759
2017-12-10T00:27:55.076286: step 272, loss 0.391346, acc 0.875, prec 0.0412951, recall 0.819201
2017-12-10T00:27:55.989393: step 273, loss 0.830161, acc 0.90625, prec 0.0415062, recall 0.819767
2017-12-10T00:27:56.913444: step 274, loss 0.883532, acc 0.917969, prec 0.041535, recall 0.819632
2017-12-10T00:27:57.815414: step 275, loss 0.989152, acc 0.927734, prec 0.0416871, recall 0.819538
2017-12-10T00:27:58.719829: step 276, loss 0.389701, acc 0.896484, prec 0.0417736, recall 0.820058
2017-12-10T00:27:59.631792: step 277, loss 0.523359, acc 0.904297, prec 0.0419118, recall 0.820354
2017-12-10T00:28:00.558506: step 278, loss 0.399304, acc 0.853516, prec 0.0419286, recall 0.820696
2017-12-10T00:28:01.458587: step 279, loss 0.478535, acc 0.861328, prec 0.0419727, recall 0.821123
2017-12-10T00:28:02.364314: step 280, loss 1.08318, acc 0.855469, prec 0.0420853, recall 0.821023
2017-12-10T00:28:03.264698: step 281, loss 0.63106, acc 0.833984, prec 0.0420925, recall 0.820973
2017-12-10T00:28:04.175752: step 282, loss 0.8166, acc 0.841797, prec 0.0421279, recall 0.820621
2017-12-10T00:28:05.077839: step 283, loss 0.598102, acc 0.849609, prec 0.0422345, recall 0.821295
2017-12-10T00:28:06.017792: step 284, loss 0.588825, acc 0.794922, prec 0.0422891, recall 0.821879
2017-12-10T00:28:06.915874: step 285, loss 0.597641, acc 0.800781, prec 0.0423464, recall 0.82246
2017-12-10T00:28:07.820250: step 286, loss 0.7711, acc 0.763672, prec 0.0424529, recall 0.823284
2017-12-10T00:28:08.736331: step 287, loss 0.490631, acc 0.841797, prec 0.0425537, recall 0.823937
2017-12-10T00:28:09.640643: step 288, loss 0.508299, acc 0.824219, prec 0.0425765, recall 0.824343
2017-12-10T00:28:10.553299: step 289, loss 0.681311, acc 0.853516, prec 0.0426609, recall 0.824529
2017-12-10T00:28:11.459041: step 290, loss 0.463505, acc 0.871094, prec 0.0426622, recall 0.824771
2017-12-10T00:28:12.371413: step 291, loss 0.378215, acc 0.873047, prec 0.0428006, recall 0.825491
2017-12-10T00:28:13.282522: step 292, loss 0.947734, acc 0.888672, prec 0.0428798, recall 0.825592
2017-12-10T00:28:14.192872: step 293, loss 0.914409, acc 0.908203, prec 0.0430141, recall 0.82585
2017-12-10T00:28:15.102586: step 294, loss 0.384513, acc 0.892578, prec 0.0430937, recall 0.826323
2017-12-10T00:28:16.010900: step 295, loss 0.652314, acc 0.890625, prec 0.0432182, recall 0.826577
2017-12-10T00:28:16.923343: step 296, loss 1.12178, acc 0.931641, prec 0.0434772, recall 0.826846
2017-12-10T00:28:17.840962: step 297, loss 0.29552, acc 0.914062, prec 0.0435447, recall 0.827232
2017-12-10T00:28:18.756631: step 298, loss 0.569878, acc 0.853516, prec 0.0435802, recall 0.827617
2017-12-10T00:28:19.657084: step 299, loss 0.436351, acc 0.869141, prec 0.0436686, recall 0.828153
2017-12-10T00:28:20.566815: step 300, loss 0.590683, acc 0.839844, prec 0.0437414, recall 0.828685

Evaluation:
2017-12-10T00:28:25.268644: step 300, loss 1.48921, acc 0.857143, prec 0.0444515, recall 0.819277

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_2/1512883424/checkpoints/model-300

2017-12-10T00:28:27.213053: step 301, loss 0.568334, acc 0.8125, prec 0.0445274, recall 0.819876
2017-12-10T00:28:28.128185: step 302, loss 0.678282, acc 0.833984, prec 0.0445068, recall 0.820099
2017-12-10T00:28:29.036904: step 303, loss 0.460747, acc 0.833984, prec 0.0445077, recall 0.820396
2017-12-10T00:28:29.945561: step 304, loss 0.528379, acc 0.826172, prec 0.0445259, recall 0.820766
2017-12-10T00:28:30.891431: step 305, loss 0.601444, acc 0.845703, prec 0.0446606, recall 0.821502
2017-12-10T00:28:31.802481: step 306, loss 0.710282, acc 0.875, prec 0.0447469, recall 0.821677
2017-12-10T00:28:32.714199: step 307, loss 0.741421, acc 0.859375, prec 0.0448037, recall 0.821778
2017-12-10T00:28:33.617517: step 308, loss 0.630839, acc 0.845703, prec 0.0447473, recall 0.821516
2017-12-10T00:28:34.532269: step 309, loss 0.653322, acc 0.867188, prec 0.0448079, recall 0.821617
2017-12-10T00:28:35.455439: step 310, loss 0.389981, acc 0.886719, prec 0.044856, recall 0.821979
2017-12-10T00:28:36.371630: step 311, loss 0.429596, acc 0.888672, prec 0.0449262, recall 0.822411
2017-12-10T00:28:37.290861: step 312, loss 1.31758, acc 0.873047, prec 0.044928, recall 0.82163
2017-12-10T00:28:38.197125: step 313, loss 2.05916, acc 0.853516, prec 0.0450671, recall 0.821357
2017-12-10T00:28:39.116619: step 314, loss 0.55663, acc 0.835938, prec 0.0451308, recall 0.821858
2017-12-10T00:28:39.295637: step 315, loss 0.346861, acc 0.846154, prec 0.0451229, recall 0.821858
2017-12-10T00:28:40.215216: step 316, loss 0.606989, acc 0.8125, prec 0.0453629, recall 0.822991
2017-12-10T00:28:41.119579: step 317, loss 0.830252, acc 0.802734, prec 0.0454506, recall 0.823623
2017-12-10T00:28:42.035705: step 318, loss 0.735541, acc 0.78125, prec 0.0454645, recall 0.824041
2017-12-10T00:28:42.960606: step 319, loss 0.60357, acc 0.810547, prec 0.0455762, recall 0.824734
2017-12-10T00:28:43.892927: step 320, loss 0.628025, acc 0.765625, prec 0.0456025, recall 0.825216
2017-12-10T00:28:44.805883: step 321, loss 0.559727, acc 0.861328, prec 0.0456978, recall 0.825764
2017-12-10T00:28:45.705281: step 322, loss 1.63753, acc 0.841797, prec 0.0457239, recall 0.825137
2017-12-10T00:28:46.617446: step 323, loss 0.695507, acc 0.847656, prec 0.0457911, recall 0.825613
2017-12-10T00:28:47.520629: step 324, loss 0.536321, acc 0.861328, prec 0.045865, recall 0.826087
2017-12-10T00:28:48.430516: step 325, loss 0.395825, acc 0.857422, prec 0.0458546, recall 0.826289
2017-12-10T00:28:49.339048: step 326, loss 0.446311, acc 0.873047, prec 0.0459135, recall 0.826692
2017-12-10T00:28:50.249932: step 327, loss 0.279648, acc 0.916016, prec 0.0459326, recall 0.826893
2017-12-10T00:28:51.184637: step 328, loss 0.257254, acc 0.902344, prec 0.0459856, recall 0.827227
2017-12-10T00:28:52.090410: step 329, loss 0.658161, acc 0.925781, prec 0.0461127, recall 0.82744
2017-12-10T00:28:53.006207: step 330, loss 0.846302, acc 0.914062, prec 0.0462539, recall 0.827718
2017-12-10T00:28:53.926495: step 331, loss 0.378057, acc 0.935547, prec 0.0463446, recall 0.827797
2017-12-10T00:28:54.830737: step 332, loss 0.342116, acc 0.925781, prec 0.0464088, recall 0.828125
2017-12-10T00:28:55.741149: step 333, loss 0.85179, acc 0.90625, prec 0.046525, recall 0.828333
2017-12-10T00:28:56.651522: step 334, loss 0.508701, acc 0.904297, prec 0.046579, recall 0.828344
2017-12-10T00:28:57.553709: step 335, loss 0.981644, acc 0.890625, prec 0.0466868, recall 0.82855
2017-12-10T00:28:58.467733: step 336, loss 0.833337, acc 0.917969, prec 0.0468285, recall 0.828819
2017-12-10T00:28:59.382471: step 337, loss 0.919566, acc 0.882812, prec 0.0469317, recall 0.829021
2017-12-10T00:29:00.307484: step 338, loss 0.55395, acc 0.833984, prec 0.0469885, recall 0.829469
2017-12-10T00:29:01.217155: step 339, loss 0.538057, acc 0.8125, prec 0.0469939, recall 0.829787
2017-12-10T00:29:02.129829: step 340, loss 0.563372, acc 0.824219, prec 0.046965, recall 0.829978
2017-12-10T00:29:03.036971: step 341, loss 0.722804, acc 0.769531, prec 0.0470088, recall 0.830483
2017-12-10T00:29:03.937500: step 342, loss 0.554246, acc 0.8125, prec 0.0470141, recall 0.830798
2017-12-10T00:29:04.843044: step 343, loss 0.645937, acc 0.822266, prec 0.047224, recall 0.831734
2017-12-10T00:29:05.754382: step 344, loss 0.47529, acc 0.830078, prec 0.0472775, recall 0.832168
2017-12-10T00:29:06.662914: step 345, loss 0.453808, acc 0.861328, prec 0.0473268, recall 0.832538
2017-12-10T00:29:07.583717: step 346, loss 0.382641, acc 0.878906, prec 0.0474046, recall 0.832967
2017-12-10T00:29:08.487415: step 347, loss 0.473399, acc 0.900391, prec 0.0476121, recall 0.833759
2017-12-10T00:29:09.398704: step 348, loss 0.659454, acc 0.888672, prec 0.0476953, recall 0.833879
2017-12-10T00:29:10.313372: step 349, loss 0.736861, acc 0.916016, prec 0.0477526, recall 0.833877
2017-12-10T00:29:11.230603: step 350, loss 0.502725, acc 0.921875, prec 0.047872, recall 0.834056
2017-12-10T00:29:12.140443: step 351, loss 0.616464, acc 0.929688, prec 0.0479755, recall 0.834174
2017-12-10T00:29:13.057352: step 352, loss 0.564832, acc 0.884766, prec 0.0480942, recall 0.834711
2017-12-10T00:29:13.968487: step 353, loss 0.38785, acc 0.904297, prec 0.048203, recall 0.835185
2017-12-10T00:29:14.868834: step 354, loss 0.549729, acc 0.880859, prec 0.0483781, recall 0.83589
2017-12-10T00:29:15.782274: step 355, loss 0.929926, acc 0.876953, prec 0.0483947, recall 0.835826
2017-12-10T00:29:16.692342: step 356, loss 0.418299, acc 0.880859, prec 0.0484123, recall 0.83606
2017-12-10T00:29:17.604328: step 357, loss 0.954736, acc 0.880859, prec 0.0485297, recall 0.83599
2017-12-10T00:29:18.516494: step 358, loss 1.31082, acc 0.837891, prec 0.0485672, recall 0.835452
2017-12-10T00:29:19.421020: step 359, loss 0.574546, acc 0.828125, prec 0.0486356, recall 0.835916
2017-12-10T00:29:20.334064: step 360, loss 0.577723, acc 0.792969, prec 0.0486469, recall 0.836261
2017-12-10T00:29:21.263262: step 361, loss 0.581871, acc 0.791016, prec 0.0486765, recall 0.836663
2017-12-10T00:29:22.170539: step 362, loss 0.664678, acc 0.806641, prec 0.0486753, recall 0.836949
2017-12-10T00:29:23.074680: step 363, loss 0.546517, acc 0.8125, prec 0.048677, recall 0.837234
2017-12-10T00:29:23.987880: step 364, loss 0.494135, acc 0.830078, prec 0.0487454, recall 0.837687
2017-12-10T00:29:24.909938: step 365, loss 0.751141, acc 0.859375, prec 0.0489438, recall 0.838475
2017-12-10T00:29:25.818560: step 366, loss 0.39296, acc 0.863281, prec 0.048913, recall 0.838587
2017-12-10T00:29:26.733674: step 367, loss 0.610262, acc 0.890625, prec 0.0490123, recall 0.838743
2017-12-10T00:29:27.647413: step 368, loss 0.443221, acc 0.859375, prec 0.049037, recall 0.839021
2017-12-10T00:29:28.568658: step 369, loss 0.389534, acc 0.880859, prec 0.0490725, recall 0.839298
2017-12-10T00:29:29.477943: step 370, loss 0.5156, acc 0.908203, prec 0.0492172, recall 0.839849
2017-12-10T00:29:30.398653: step 371, loss 0.380009, acc 0.912109, prec 0.0493064, recall 0.840233
2017-12-10T00:29:31.322308: step 372, loss 0.229062, acc 0.9375, prec 0.0493892, recall 0.84056
2017-12-10T00:29:32.248849: step 373, loss 0.444639, acc 0.935547, prec 0.0495851, recall 0.84121
2017-12-10T00:29:33.160008: step 374, loss 0.254981, acc 0.933594, prec 0.0496085, recall 0.841372
2017-12-10T00:29:34.067829: step 375, loss 0.662013, acc 0.900391, prec 0.0497299, recall 0.841571
2017-12-10T00:29:34.977025: step 376, loss 0.140244, acc 0.953125, prec 0.0497441, recall 0.841678
2017-12-10T00:29:35.922301: step 377, loss 1.71931, acc 0.923828, prec 0.0498432, recall 0.840633
2017-12-10T00:29:36.105820: step 378, loss 0.126728, acc 0.923077, prec 0.0498392, recall 0.840633
2017-12-10T00:29:37.021066: step 379, loss 0.261405, acc 0.921875, prec 0.0499132, recall 0.840955
2017-12-10T00:29:37.927427: step 380, loss 0.416805, acc 0.902344, prec 0.0500528, recall 0.841488
2017-12-10T00:29:38.829633: step 381, loss 0.476956, acc 0.855469, prec 0.0501304, recall 0.841912
2017-12-10T00:29:39.745477: step 382, loss 0.347377, acc 0.888672, prec 0.0502057, recall 0.842281
2017-12-10T00:29:40.671619: step 383, loss 0.388596, acc 0.873047, prec 0.0502917, recall 0.8427
2017-12-10T00:29:41.574946: step 384, loss 0.419405, acc 0.863281, prec 0.0504102, recall 0.843222
2017-12-10T00:29:42.489085: step 385, loss 0.456151, acc 0.871094, prec 0.0505134, recall 0.843688
2017-12-10T00:29:43.410847: step 386, loss 0.324051, acc 0.892578, prec 0.0505524, recall 0.843946
2017-12-10T00:29:44.327225: step 387, loss 0.661623, acc 0.884766, prec 0.0506631, recall 0.84413
2017-12-10T00:29:45.233507: step 388, loss 0.584655, acc 0.878906, prec 0.0507883, recall 0.844641
2017-12-10T00:29:46.145460: step 389, loss 0.434636, acc 0.884766, prec 0.0508228, recall 0.844895
2017-12-10T00:29:47.054643: step 390, loss 0.860824, acc 0.900391, prec 0.0509044, recall 0.844698
2017-12-10T00:29:47.981265: step 391, loss 0.429129, acc 0.884766, prec 0.0509759, recall 0.845052
2017-12-10T00:29:48.892515: step 392, loss 0.39349, acc 0.890625, prec 0.0511061, recall 0.845555
2017-12-10T00:29:49.810542: step 393, loss 0.572464, acc 0.863281, prec 0.0511289, recall 0.845805
2017-12-10T00:29:50.725769: step 394, loss 0.608814, acc 0.873047, prec 0.0511577, recall 0.845781
2017-12-10T00:29:51.645636: step 395, loss 0.656745, acc 0.869141, prec 0.0512029, recall 0.845806
2017-12-10T00:29:52.556966: step 396, loss 0.361124, acc 0.878906, prec 0.0512706, recall 0.846154
2017-12-10T00:29:53.477137: step 397, loss 0.429386, acc 0.878906, prec 0.0513195, recall 0.84645
2017-12-10T00:29:54.394694: step 398, loss 0.361639, acc 0.894531, prec 0.051321, recall 0.846598
2017-12-10T00:29:55.310291: step 399, loss 0.703997, acc 0.888672, prec 0.0513573, recall 0.846573
2017-12-10T00:29:56.214748: step 400, loss 0.348276, acc 0.902344, prec 0.0514547, recall 0.846965
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_2/1512883424/checkpoints/model-400

2017-12-10T00:29:58.235533: step 401, loss 0.406951, acc 0.900391, prec 0.0516061, recall 0.847501
2017-12-10T00:29:59.156206: step 402, loss 0.33389, acc 0.886719, prec 0.0516767, recall 0.84784
2017-12-10T00:30:00.067282: step 403, loss 0.538283, acc 0.908203, prec 0.0518682, recall 0.848466
2017-12-10T00:30:00.992571: step 404, loss 0.383946, acc 0.921875, prec 0.0519197, recall 0.848705
2017-12-10T00:30:01.900618: step 405, loss 0.282708, acc 0.921875, prec 0.0519345, recall 0.848848
2017-12-10T00:30:02.818714: step 406, loss 0.366597, acc 0.919922, prec 0.0520032, recall 0.849134
2017-12-10T00:30:03.729047: step 407, loss 0.396869, acc 0.927734, prec 0.0521132, recall 0.849246
2017-12-10T00:30:04.639600: step 408, loss 0.237405, acc 0.939453, prec 0.0521917, recall 0.84953
2017-12-10T00:30:05.563470: step 409, loss 0.489815, acc 0.9375, prec 0.0524149, recall 0.850187
2017-12-10T00:30:06.479346: step 410, loss 0.277933, acc 0.890625, prec 0.0524859, recall 0.850514
2017-12-10T00:30:07.392809: step 411, loss 0.584326, acc 0.916016, prec 0.0525174, recall 0.850171
2017-12-10T00:30:08.302532: step 412, loss 0.971722, acc 0.910156, prec 0.0526174, recall 0.850279
2017-12-10T00:30:09.222950: step 413, loss 0.42034, acc 0.869141, prec 0.0527132, recall 0.850695
2017-12-10T00:30:10.129281: step 414, loss 0.622053, acc 0.878906, prec 0.0528329, recall 0.850893
2017-12-10T00:30:11.033543: step 415, loss 0.440849, acc 0.84375, prec 0.0528788, recall 0.851214
2017-12-10T00:30:11.947488: step 416, loss 0.491881, acc 0.847656, prec 0.0528905, recall 0.851443
2017-12-10T00:30:12.851624: step 417, loss 0.64439, acc 0.802734, prec 0.052969, recall 0.851897
2017-12-10T00:30:13.758998: step 418, loss 0.669815, acc 0.820312, prec 0.0530394, recall 0.852044
2017-12-10T00:30:14.660247: step 419, loss 0.904997, acc 0.849609, prec 0.0531066, recall 0.852145
2017-12-10T00:30:15.572308: step 420, loss 0.489494, acc 0.837891, prec 0.0531486, recall 0.852459
2017-12-10T00:30:16.472031: step 421, loss 0.436493, acc 0.833984, prec 0.0531706, recall 0.852727
2017-12-10T00:30:17.384453: step 422, loss 0.4172, acc 0.894531, prec 0.0532057, recall 0.85295
2017-12-10T00:30:18.284070: step 423, loss 0.433257, acc 0.878906, prec 0.0532863, recall 0.853305
2017-12-10T00:30:19.210076: step 424, loss 0.867486, acc 0.875, prec 0.0533301, recall 0.853313
2017-12-10T00:30:20.116319: step 425, loss 0.362933, acc 0.904297, prec 0.0534233, recall 0.853666
2017-12-10T00:30:21.046126: step 426, loss 0.279564, acc 0.892578, prec 0.0534393, recall 0.853842
2017-12-10T00:30:21.954209: step 427, loss 0.812838, acc 0.904297, prec 0.0535332, recall 0.853936
2017-12-10T00:30:22.852355: step 428, loss 0.512354, acc 0.900391, prec 0.0536417, recall 0.854328
2017-12-10T00:30:23.753383: step 429, loss 0.498186, acc 0.910156, prec 0.0536674, recall 0.854247
2017-12-10T00:30:24.664792: step 430, loss 0.346071, acc 0.882812, prec 0.0537488, recall 0.854594
2017-12-10T00:30:25.567731: step 431, loss 0.302846, acc 0.90625, prec 0.053842, recall 0.854939
2017-12-10T00:30:26.481752: step 432, loss 0.360351, acc 0.892578, prec 0.053928, recall 0.855283
2017-12-10T00:30:27.390107: step 433, loss 0.812711, acc 0.886719, prec 0.054047, recall 0.855457
2017-12-10T00:30:28.294030: step 434, loss 0.501637, acc 0.902344, prec 0.0541024, recall 0.855713
2017-12-10T00:30:29.208941: step 435, loss 0.301451, acc 0.908203, prec 0.0541606, recall 0.855967
2017-12-10T00:30:30.136722: step 436, loss 0.866167, acc 0.890625, prec 0.0542996, recall 0.85593
2017-12-10T00:30:31.055751: step 437, loss 0.319906, acc 0.896484, prec 0.0543339, recall 0.85614
2017-12-10T00:30:31.956312: step 438, loss 0.401217, acc 0.875, prec 0.0543045, recall 0.856224
2017-12-10T00:30:32.856135: step 439, loss 0.684813, acc 0.886719, prec 0.0543347, recall 0.856184
2017-12-10T00:30:33.762284: step 440, loss 0.800752, acc 0.898438, prec 0.0543884, recall 0.856186
2017-12-10T00:30:33.946558: step 441, loss 1.83065, acc 0.846154, prec 0.0544164, recall 0.856021
2017-12-10T00:30:34.866687: step 442, loss 0.551965, acc 0.822266, prec 0.0545169, recall 0.85648
2017-12-10T00:30:35.793809: step 443, loss 0.702334, acc 0.783203, prec 0.0545796, recall 0.856895
2017-12-10T00:30:36.698404: step 444, loss 0.763215, acc 0.761719, prec 0.0545615, recall 0.857143
2017-12-10T00:30:37.614893: step 445, loss 0.671636, acc 0.783203, prec 0.0545891, recall 0.857472
2017-12-10T00:30:38.542879: step 446, loss 1.05901, acc 0.765625, prec 0.0545741, recall 0.857471
2017-12-10T00:30:39.448788: step 447, loss 0.693149, acc 0.763672, prec 0.0545916, recall 0.857798
2017-12-10T00:30:40.359632: step 448, loss 0.745929, acc 0.794922, prec 0.0546421, recall 0.858164
2017-12-10T00:30:41.267775: step 449, loss 0.510395, acc 0.798828, prec 0.0546086, recall 0.858326
2017-12-10T00:30:42.172741: step 450, loss 0.72764, acc 0.818359, prec 0.0546536, recall 0.858649
2017-12-10T00:30:43.084510: step 451, loss 0.349155, acc 0.884766, prec 0.0546295, recall 0.85873
2017-12-10T00:30:43.991652: step 452, loss 0.370108, acc 0.880859, prec 0.054706, recall 0.859051
2017-12-10T00:30:44.890843: step 453, loss 0.321168, acc 0.919922, prec 0.0547851, recall 0.859331
2017-12-10T00:30:45.808461: step 454, loss 0.281149, acc 0.927734, prec 0.054868, recall 0.859609
2017-12-10T00:30:46.717549: step 455, loss 0.53525, acc 0.931641, prec 0.0549367, recall 0.859605
2017-12-10T00:30:47.619555: step 456, loss 0.442774, acc 0.939453, prec 0.0550093, recall 0.8596
2017-12-10T00:30:48.528240: step 457, loss 0.588225, acc 0.9375, prec 0.0550637, recall 0.859555
2017-12-10T00:30:49.435458: step 458, loss 0.226843, acc 0.958984, prec 0.055128, recall 0.859753
2017-12-10T00:30:50.349213: step 459, loss 0.328724, acc 0.955078, prec 0.0551913, recall 0.859708
2017-12-10T00:30:51.273968: step 460, loss 0.216801, acc 0.949219, prec 0.0552505, recall 0.859905
2017-12-10T00:30:52.181402: step 461, loss 0.902077, acc 0.953125, prec 0.0553646, recall 0.859737
2017-12-10T00:30:53.098289: step 462, loss 0.396209, acc 0.919922, prec 0.0554097, recall 0.859693
2017-12-10T00:30:54.009518: step 463, loss 0.319828, acc 0.941406, prec 0.0555496, recall 0.860083
2017-12-10T00:30:54.912426: step 464, loss 1.09222, acc 0.925781, prec 0.0556503, recall 0.859678
2017-12-10T00:30:55.823206: step 465, loss 0.313021, acc 0.917969, prec 0.0557778, recall 0.860066
2017-12-10T00:30:56.729058: step 466, loss 0.560314, acc 0.871094, prec 0.0558143, recall 0.860061
2017-12-10T00:30:57.635170: step 467, loss 0.611838, acc 0.855469, prec 0.0559093, recall 0.860446
2017-12-10T00:30:58.539181: step 468, loss 0.512251, acc 0.861328, prec 0.056007, recall 0.860829
2017-12-10T00:30:59.456552: step 469, loss 0.70261, acc 0.839844, prec 0.0560776, recall 0.860936
2017-12-10T00:31:00.383804: step 470, loss 0.498159, acc 0.839844, prec 0.056147, recall 0.861278
2017-12-10T00:31:01.289507: step 471, loss 0.869678, acc 0.832031, prec 0.0561964, recall 0.861346
2017-12-10T00:31:02.202312: step 472, loss 0.640284, acc 0.800781, prec 0.0562119, recall 0.86161
2017-12-10T00:31:03.111286: step 473, loss 0.520941, acc 0.806641, prec 0.0561635, recall 0.861722
2017-12-10T00:31:04.025645: step 474, loss 0.48643, acc 0.853516, prec 0.0562058, recall 0.861985
2017-12-10T00:31:04.930888: step 475, loss 0.448205, acc 0.839844, prec 0.056291, recall 0.862358
2017-12-10T00:31:05.845764: step 476, loss 0.446801, acc 0.851562, prec 0.0563154, recall 0.862581
2017-12-10T00:31:06.753995: step 477, loss 0.377492, acc 0.886719, prec 0.0564407, recall 0.862988
2017-12-10T00:31:07.666745: step 478, loss 0.397844, acc 0.929688, prec 0.0565211, recall 0.863246
2017-12-10T00:31:08.574154: step 479, loss 0.364379, acc 0.925781, prec 0.0566326, recall 0.863575
2017-12-10T00:31:09.487068: step 480, loss 0.246422, acc 0.917969, prec 0.0566571, recall 0.863722
2017-12-10T00:31:10.402072: step 481, loss 0.403571, acc 0.917969, prec 0.056814, recall 0.864158
2017-12-10T00:31:11.316644: step 482, loss 0.45146, acc 0.939453, prec 0.0568833, recall 0.864145
2017-12-10T00:31:12.225742: step 483, loss 0.394673, acc 0.929688, prec 0.0568815, recall 0.863987
2017-12-10T00:31:13.133602: step 484, loss 0.754184, acc 0.945312, prec 0.0570518, recall 0.86442
2017-12-10T00:31:14.040022: step 485, loss 0.282423, acc 0.949219, prec 0.0572239, recall 0.864851
2017-12-10T00:31:14.946229: step 486, loss 1.12968, acc 0.941406, prec 0.0573607, recall 0.864751
2017-12-10T00:31:15.858435: step 487, loss 0.223529, acc 0.912109, prec 0.0573815, recall 0.864893
2017-12-10T00:31:16.765575: step 488, loss 0.259875, acc 0.919922, prec 0.0574392, recall 0.865107
2017-12-10T00:31:17.685818: step 489, loss 0.361711, acc 0.886719, prec 0.0574632, recall 0.865284
2017-12-10T00:31:18.604363: step 490, loss 0.296433, acc 0.914062, prec 0.057567, recall 0.865601
2017-12-10T00:31:19.507917: step 491, loss 0.306631, acc 0.890625, prec 0.0575929, recall 0.865777
2017-12-10T00:31:20.438376: step 492, loss 0.40433, acc 0.914062, prec 0.0576308, recall 0.865952
2017-12-10T00:31:21.367817: step 493, loss 0.433644, acc 0.896484, prec 0.0577578, recall 0.866337
2017-12-10T00:31:22.277650: step 494, loss 0.413104, acc 0.894531, prec 0.0578018, recall 0.866545
2017-12-10T00:31:23.185708: step 495, loss 0.308098, acc 0.892578, prec 0.057812, recall 0.866684
2017-12-10T00:31:24.094528: step 496, loss 0.469687, acc 0.894531, prec 0.0578731, recall 0.866701
2017-12-10T00:31:25.018996: step 497, loss 0.258674, acc 0.916016, prec 0.0579605, recall 0.866977
2017-12-10T00:31:25.925037: step 498, loss 0.271664, acc 0.917969, prec 0.0579673, recall 0.86708
2017-12-10T00:31:26.841705: step 499, loss 1.45535, acc 0.900391, prec 0.0580484, recall 0.866907
2017-12-10T00:31:27.750344: step 500, loss 0.377446, acc 0.873047, prec 0.0581133, recall 0.867181
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_2/1512883424/checkpoints/model-500

2017-12-10T00:31:29.694917: step 501, loss 0.574271, acc 0.914062, prec 0.0581838, recall 0.867198
2017-12-10T00:31:30.628830: step 502, loss 0.482835, acc 0.876953, prec 0.0582828, recall 0.867538
2017-12-10T00:31:31.551238: step 503, loss 0.376852, acc 0.894531, prec 0.0582935, recall 0.867673
2017-12-10T00:31:31.732567: step 504, loss 0.516893, acc 0.826923, prec 0.0583007, recall 0.867707
2017-12-10T00:31:32.642001: step 505, loss 0.412617, acc 0.867188, prec 0.0582973, recall 0.867843
2017-12-10T00:31:33.554735: step 506, loss 0.437361, acc 0.902344, prec 0.0584573, recall 0.86828
2017-12-10T00:31:34.462481: step 507, loss 0.927933, acc 0.902344, prec 0.0584899, recall 0.868006
2017-12-10T00:31:35.378311: step 508, loss 0.489766, acc 0.921875, prec 0.0586926, recall 0.868288
2017-12-10T00:31:36.307457: step 509, loss 0.527228, acc 0.875, prec 0.058726, recall 0.868268
2017-12-10T00:31:37.218380: step 510, loss 0.391833, acc 0.902344, prec 0.0587884, recall 0.868501
2017-12-10T00:31:38.125219: step 511, loss 0.379426, acc 0.882812, prec 0.0588085, recall 0.868666
2017-12-10T00:31:39.028782: step 512, loss 0.57288, acc 0.900391, prec 0.0588065, recall 0.868547
2017-12-10T00:31:39.941764: step 513, loss 0.432697, acc 0.894531, prec 0.0589287, recall 0.86891
2017-12-10T00:31:40.843054: step 514, loss 0.2792, acc 0.890625, prec 0.0589686, recall 0.869107
2017-12-10T00:31:41.758711: step 515, loss 0.339007, acc 0.886719, prec 0.0589585, recall 0.869206
2017-12-10T00:31:42.673816: step 516, loss 0.314763, acc 0.902344, prec 0.0590363, recall 0.869467
2017-12-10T00:31:43.582416: step 517, loss 0.356695, acc 0.892578, prec 0.0590929, recall 0.869695
2017-12-10T00:31:44.490381: step 518, loss 0.322775, acc 0.925781, prec 0.0591665, recall 0.869923
2017-12-10T00:31:45.411383: step 519, loss 0.566674, acc 0.941406, prec 0.0592639, recall 0.870182
2017-12-10T00:31:46.320177: step 520, loss 0.303092, acc 0.935547, prec 0.05939, recall 0.870504
2017-12-10T00:31:47.232800: step 521, loss 0.343029, acc 0.908203, prec 0.059486, recall 0.870792
2017-12-10T00:31:48.143510: step 522, loss 0.301077, acc 0.941406, prec 0.059583, recall 0.871047
2017-12-10T00:31:49.053029: step 523, loss 0.300276, acc 0.9375, prec 0.0596778, recall 0.871302
2017-12-10T00:31:49.966294: step 524, loss 0.263319, acc 0.933594, prec 0.0597546, recall 0.871523
2017-12-10T00:31:50.897861: step 525, loss 0.222668, acc 0.927734, prec 0.0597649, recall 0.871618
2017-12-10T00:31:51.805361: step 526, loss 0.204247, acc 0.933594, prec 0.0597782, recall 0.871713
2017-12-10T00:31:52.724776: step 527, loss 0.296983, acc 0.947266, prec 0.0597985, recall 0.871807
2017-12-10T00:31:53.636529: step 528, loss 0.191203, acc 0.935547, prec 0.0598761, recall 0.872027
2017-12-10T00:31:54.542562: step 529, loss 0.211993, acc 0.935547, prec 0.0599852, recall 0.872309
2017-12-10T00:31:55.458068: step 530, loss 0.349784, acc 0.949219, prec 0.0601012, recall 0.87259
2017-12-10T00:31:56.366876: step 531, loss 0.223633, acc 0.943359, prec 0.0601983, recall 0.872838
2017-12-10T00:31:57.272240: step 532, loss 0.564627, acc 0.943359, prec 0.0602331, recall 0.872749
2017-12-10T00:31:58.179085: step 533, loss 0.24729, acc 0.957031, prec 0.0603212, recall 0.872966
2017-12-10T00:31:59.094844: step 534, loss 0.763551, acc 0.945312, prec 0.0604043, recall 0.87297
2017-12-10T00:31:59.997037: step 535, loss 0.464148, acc 0.935547, prec 0.0604821, recall 0.872974
2017-12-10T00:32:00.919849: step 536, loss 0.24939, acc 0.935547, prec 0.060669, recall 0.873402
2017-12-10T00:32:01.827299: step 537, loss 0.657717, acc 0.933594, prec 0.0607927, recall 0.873497
2017-12-10T00:32:02.737005: step 538, loss 0.286954, acc 0.923828, prec 0.060863, recall 0.873709
2017-12-10T00:32:03.640956: step 539, loss 0.342159, acc 0.908203, prec 0.0609408, recall 0.873952
2017-12-10T00:32:04.551007: step 540, loss 0.358113, acc 0.890625, prec 0.0609779, recall 0.874133
2017-12-10T00:32:05.457046: step 541, loss 0.363799, acc 0.886719, prec 0.0610128, recall 0.874313
2017-12-10T00:32:06.377313: step 542, loss 0.46258, acc 0.859375, prec 0.0611117, recall 0.874642
2017-12-10T00:32:07.288027: step 543, loss 0.440211, acc 0.875, prec 0.0611403, recall 0.874821
2017-12-10T00:32:08.218519: step 544, loss 0.398359, acc 0.873047, prec 0.0612147, recall 0.875089
2017-12-10T00:32:09.130654: step 545, loss 0.429444, acc 0.892578, prec 0.0612367, recall 0.875237
2017-12-10T00:32:10.039860: step 546, loss 0.292368, acc 0.919922, prec 0.0613196, recall 0.875473
2017-12-10T00:32:10.952340: step 547, loss 0.446421, acc 0.912109, prec 0.0614449, recall 0.875797
2017-12-10T00:32:11.861810: step 548, loss 0.318251, acc 0.902344, prec 0.0615183, recall 0.876031
2017-12-10T00:32:12.767341: step 549, loss 0.292253, acc 0.908203, prec 0.0615481, recall 0.876177
2017-12-10T00:32:13.690166: step 550, loss 0.372357, acc 0.912109, prec 0.0615644, recall 0.876293
2017-12-10T00:32:14.602439: step 551, loss 0.183062, acc 0.947266, prec 0.0616299, recall 0.876468
2017-12-10T00:32:15.519042: step 552, loss 0.378794, acc 0.927734, prec 0.0616697, recall 0.876613
2017-12-10T00:32:16.430463: step 553, loss 0.255494, acc 0.951172, prec 0.0617526, recall 0.876815
2017-12-10T00:32:17.336079: step 554, loss 0.402217, acc 0.927734, prec 0.0618552, recall 0.876869
2017-12-10T00:32:18.255777: step 555, loss 1.49441, acc 0.953125, prec 0.0620336, recall 0.876833
2017-12-10T00:32:19.183339: step 556, loss 0.319316, acc 0.947266, prec 0.0621152, recall 0.87683
2017-12-10T00:32:20.107108: step 557, loss 0.913772, acc 0.933594, prec 0.0622203, recall 0.876884
2017-12-10T00:32:21.051563: step 558, loss 0.31121, acc 0.912109, prec 0.0622976, recall 0.877112
2017-12-10T00:32:21.972662: step 559, loss 0.337513, acc 0.886719, prec 0.0623461, recall 0.877311
2017-12-10T00:32:22.883194: step 560, loss 0.567854, acc 0.876953, prec 0.0623893, recall 0.877509
2017-12-10T00:32:23.798920: step 561, loss 0.518745, acc 0.828125, prec 0.0623915, recall 0.877678
2017-12-10T00:32:24.718888: step 562, loss 0.544382, acc 0.826172, prec 0.062408, recall 0.877875
2017-12-10T00:32:25.628824: step 563, loss 0.497437, acc 0.833984, prec 0.0624439, recall 0.878099
2017-12-10T00:32:26.536148: step 564, loss 0.448506, acc 0.851562, prec 0.0625041, recall 0.87835
2017-12-10T00:32:27.457950: step 565, loss 0.385503, acc 0.867188, prec 0.0625265, recall 0.878518
2017-12-10T00:32:28.371221: step 566, loss 0.367189, acc 0.894531, prec 0.0625783, recall 0.878712
2017-12-10T00:32:28.550155: step 567, loss 0.287586, acc 0.865385, prec 0.0625712, recall 0.878712
2017-12-10T00:32:29.464816: step 568, loss 0.232002, acc 0.912109, prec 0.0626168, recall 0.878878
2017-12-10T00:32:30.380162: step 569, loss 0.201175, acc 0.9375, prec 0.0627365, recall 0.879153
2017-12-10T00:32:31.291298: step 570, loss 0.300663, acc 0.9375, prec 0.0627658, recall 0.879063
2017-12-10T00:32:32.201856: step 571, loss 0.271151, acc 0.955078, prec 0.0628488, recall 0.879256
2017-12-10T00:32:33.107308: step 572, loss 0.279335, acc 0.966797, prec 0.0629379, recall 0.879447
2017-12-10T00:32:34.017857: step 573, loss 0.292471, acc 0.972656, prec 0.0631059, recall 0.879774
2017-12-10T00:32:34.921830: step 574, loss 1.03905, acc 0.951172, prec 0.0631886, recall 0.879567
2017-12-10T00:32:35.836060: step 575, loss 0.337545, acc 0.941406, prec 0.0632499, recall 0.879532
2017-12-10T00:32:36.761259: step 576, loss 0.278834, acc 0.943359, prec 0.0633264, recall 0.879721
2017-12-10T00:32:37.678904: step 577, loss 0.27917, acc 0.929688, prec 0.0634258, recall 0.879964
2017-12-10T00:32:38.592523: step 578, loss 0.265778, acc 0.941406, prec 0.0634859, recall 0.880125
2017-12-10T00:32:39.496245: step 579, loss 0.328474, acc 0.904297, prec 0.063481, recall 0.880206
2017-12-10T00:32:40.415034: step 580, loss 0.244006, acc 0.919922, prec 0.0635448, recall 0.880393
2017-12-10T00:32:41.333666: step 581, loss 0.188099, acc 0.929688, prec 0.0636438, recall 0.880634
2017-12-10T00:32:42.242599: step 582, loss 0.218623, acc 0.947266, prec 0.0636916, recall 0.880767
2017-12-10T00:32:43.151299: step 583, loss 0.223996, acc 0.935547, prec 0.0637181, recall 0.880873
2017-12-10T00:32:44.062950: step 584, loss 0.32514, acc 0.943359, prec 0.0637938, recall 0.881058
2017-12-10T00:32:44.978801: step 585, loss 0.246311, acc 0.933594, prec 0.0638945, recall 0.881296
2017-12-10T00:32:45.886069: step 586, loss 0.291365, acc 0.935547, prec 0.0640562, recall 0.881637
2017-12-10T00:32:46.796720: step 587, loss 0.52891, acc 0.951172, prec 0.0641217, recall 0.881599
2017-12-10T00:32:47.703614: step 588, loss 0.19853, acc 0.955078, prec 0.0642333, recall 0.881834
2017-12-10T00:32:48.617843: step 589, loss 0.562136, acc 0.949219, prec 0.0642676, recall 0.881744
2017-12-10T00:32:49.529941: step 590, loss 0.291593, acc 0.925781, prec 0.0644085, recall 0.882056
2017-12-10T00:32:50.432505: step 591, loss 0.21985, acc 0.931641, prec 0.0645223, recall 0.882314
2017-12-10T00:32:51.354196: step 592, loss 0.282908, acc 0.917969, prec 0.0645389, recall 0.882417
2017-12-10T00:32:52.272785: step 593, loss 0.200493, acc 0.929688, prec 0.0645915, recall 0.882572
2017-12-10T00:32:53.187452: step 594, loss 0.249458, acc 0.933594, prec 0.0646312, recall 0.8827
2017-12-10T00:32:54.089149: step 595, loss 0.209526, acc 0.955078, prec 0.0647121, recall 0.882879
2017-12-10T00:32:54.999132: step 596, loss 0.379616, acc 0.941406, prec 0.0647857, recall 0.883057
2017-12-10T00:32:55.902887: step 597, loss 0.383715, acc 0.908203, prec 0.0648575, recall 0.883069
2017-12-10T00:32:56.802085: step 598, loss 0.309128, acc 0.941406, prec 0.0649906, recall 0.883348
2017-12-10T00:32:57.707913: step 599, loss 0.224901, acc 0.931641, prec 0.0650736, recall 0.88355
2017-12-10T00:32:58.616396: step 600, loss 0.17736, acc 0.943359, prec 0.0650882, recall 0.883625

Evaluation:
2017-12-10T00:33:03.290267: step 600, loss 2.15994, acc 0.940366, prec 0.0655655, recall 0.871096

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_2/1512883424/checkpoints/model-600

2017-12-10T00:33:05.381798: step 601, loss 0.157176, acc 0.958984, prec 0.0656617, recall 0.871312
2017-12-10T00:33:06.305924: step 602, loss 0.240545, acc 0.933594, prec 0.0656707, recall 0.871393
2017-12-10T00:33:07.223513: step 603, loss 0.222467, acc 0.945312, prec 0.0658036, recall 0.871688
2017-12-10T00:33:08.141884: step 604, loss 0.160805, acc 0.951172, prec 0.0658659, recall 0.871848
2017-12-10T00:33:09.046773: step 605, loss 0.283421, acc 0.949219, prec 0.0660447, recall 0.872221
2017-12-10T00:33:09.967827: step 606, loss 0.344216, acc 0.943359, prec 0.0661027, recall 0.87238
2017-12-10T00:33:10.882241: step 607, loss 0.494244, acc 0.953125, prec 0.0661375, recall 0.872305
2017-12-10T00:33:11.787401: step 608, loss 0.715854, acc 0.9375, prec 0.0662677, recall 0.872235
2017-12-10T00:33:12.692398: step 609, loss 0.156473, acc 0.949219, prec 0.0663579, recall 0.872446
2017-12-10T00:33:13.599126: step 610, loss 0.240796, acc 0.933594, prec 0.066469, recall 0.872709
2017-12-10T00:33:14.519126: step 611, loss 0.404806, acc 0.919922, prec 0.0665433, recall 0.872918
2017-12-10T00:33:15.436282: step 612, loss 0.40853, acc 0.914062, prec 0.0666291, recall 0.873153
2017-12-10T00:33:16.346974: step 613, loss 0.267822, acc 0.904297, prec 0.0667094, recall 0.873387
2017-12-10T00:33:17.259857: step 614, loss 0.278778, acc 0.900391, prec 0.0667146, recall 0.87349
2017-12-10T00:33:18.164610: step 615, loss 0.322486, acc 0.917969, prec 0.0668166, recall 0.873749
2017-12-10T00:33:19.073838: step 616, loss 0.317044, acc 0.908203, prec 0.0668842, recall 0.873955
2017-12-10T00:33:19.984785: step 617, loss 0.247065, acc 0.921875, prec 0.0668715, recall 0.874006
2017-12-10T00:33:20.910926: step 618, loss 0.39862, acc 0.90625, prec 0.0669815, recall 0.874288
2017-12-10T00:33:21.823730: step 619, loss 1.0134, acc 0.933594, prec 0.0670062, recall 0.874035
2017-12-10T00:33:22.737178: step 620, loss 0.241524, acc 0.921875, prec 0.0669935, recall 0.874086
2017-12-10T00:33:23.645901: step 621, loss 0.285887, acc 0.910156, prec 0.0671342, recall 0.874418
2017-12-10T00:33:24.552739: step 622, loss 0.745642, acc 0.921875, prec 0.067195, recall 0.874419
2017-12-10T00:33:25.475146: step 623, loss 0.359747, acc 0.90625, prec 0.0673477, recall 0.874773
2017-12-10T00:33:26.384750: step 624, loss 0.296215, acc 0.896484, prec 0.0673213, recall 0.874824
2017-12-10T00:33:27.288448: step 625, loss 0.543858, acc 0.898438, prec 0.0673114, recall 0.874723
2017-12-10T00:33:28.187193: step 626, loss 0.27899, acc 0.912109, prec 0.0673945, recall 0.87495
2017-12-10T00:33:29.101906: step 627, loss 0.290443, acc 0.921875, prec 0.0675115, recall 0.875226
2017-12-10T00:33:30.009445: step 628, loss 0.224648, acc 0.947266, prec 0.0675699, recall 0.875376
2017-12-10T00:33:30.935700: step 629, loss 0.310463, acc 0.919922, prec 0.0675703, recall 0.875451
2017-12-10T00:33:31.122809: step 630, loss 0.575339, acc 0.807692, prec 0.0675887, recall 0.8755
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 512
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_512_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_3/1512884012

Start training
2017-12-10T00:33:35.593248: step 1, loss 7.63063, acc 0.0859375, prec 0.00849257, recall 0.8
2017-12-10T00:33:36.509409: step 2, loss 5.05611, acc 0.277344, prec 0.00832342, recall 0.636364
2017-12-10T00:33:37.408331: step 3, loss 4.10471, acc 0.585938, prec 0.0141509, recall 0.75
2017-12-10T00:33:38.313907: step 4, loss 5.34308, acc 0.787109, prec 0.0145923, recall 0.607143
2017-12-10T00:33:39.233757: step 5, loss 6.43436, acc 0.824219, prec 0.0143655, recall 0.5625
2017-12-10T00:33:40.138439: step 6, loss 3.91778, acc 0.755859, prec 0.0166667, recall 0.575
2017-12-10T00:33:41.055037: step 7, loss 5.53785, acc 0.746094, prec 0.0165673, recall 0.555556
2017-12-10T00:33:41.957727: step 8, loss 1.2748, acc 0.716797, prec 0.017491, recall 0.591837
2017-12-10T00:33:42.866527: step 9, loss 6.50654, acc 0.652344, prec 0.0174482, recall 0.561404
2017-12-10T00:33:43.778662: step 10, loss 9.44537, acc 0.53125, prec 0.0178571, recall 0.536232
2017-12-10T00:33:44.678106: step 11, loss 3.04328, acc 0.414062, prec 0.0185029, recall 0.571429
2017-12-10T00:33:45.589249: step 12, loss 4.37008, acc 0.326172, prec 0.0190476, recall 0.604651
2017-12-10T00:33:46.497695: step 13, loss 7.24584, acc 0.269531, prec 0.0183516, recall 0.606383
2017-12-10T00:33:47.401139: step 14, loss 4.49704, acc 0.248047, prec 0.0185767, recall 0.637255
2017-12-10T00:33:48.316355: step 15, loss 5.15266, acc 0.242188, prec 0.0174852, recall 0.641509
2017-12-10T00:33:49.230266: step 16, loss 4.46043, acc 0.302734, prec 0.0174118, recall 0.649123
2017-12-10T00:33:50.139452: step 17, loss 3.39054, acc 0.347656, prec 0.0174292, recall 0.666667
2017-12-10T00:33:51.058502: step 18, loss 2.4366, acc 0.474609, prec 0.016869, recall 0.672131
2017-12-10T00:33:51.974952: step 19, loss 3.08646, acc 0.605469, prec 0.0173673, recall 0.676923
2017-12-10T00:33:52.889076: step 20, loss 2.07882, acc 0.664062, prec 0.0175573, recall 0.671533
2017-12-10T00:33:53.788413: step 21, loss 2.49754, acc 0.751953, prec 0.0176942, recall 0.673759
2017-12-10T00:33:54.685181: step 22, loss 3.66352, acc 0.808594, prec 0.0181053, recall 0.668919
2017-12-10T00:33:55.594371: step 23, loss 3.2275, acc 0.814453, prec 0.0188679, recall 0.664557
2017-12-10T00:33:56.505352: step 24, loss 6.00278, acc 0.800781, prec 0.0190678, recall 0.646707
2017-12-10T00:33:57.411014: step 25, loss 5.93106, acc 0.753906, prec 0.0188353, recall 0.633721
2017-12-10T00:33:58.329854: step 26, loss 4.25896, acc 0.732422, prec 0.0192405, recall 0.629834
2017-12-10T00:33:59.244233: step 27, loss 4.02431, acc 0.677734, prec 0.0190445, recall 0.630435
2017-12-10T00:34:00.167165: step 28, loss 3.62211, acc 0.611328, prec 0.0189189, recall 0.626316
2017-12-10T00:34:01.093814: step 29, loss 2.03598, acc 0.535156, prec 0.0189806, recall 0.635897
2017-12-10T00:34:02.005568: step 30, loss 2.39395, acc 0.484375, prec 0.0188208, recall 0.643216
2017-12-10T00:34:02.908443: step 31, loss 3.25874, acc 0.46875, prec 0.0190705, recall 0.652174
2017-12-10T00:34:03.809844: step 32, loss 2.88978, acc 0.470703, prec 0.0189038, recall 0.65566
2017-12-10T00:34:04.712384: step 33, loss 3.69128, acc 0.449219, prec 0.0192383, recall 0.662162
2017-12-10T00:34:05.623657: step 34, loss 3.40296, acc 0.492188, prec 0.0191043, recall 0.665198
2017-12-10T00:34:06.542463: step 35, loss 2.52579, acc 0.542969, prec 0.0196415, recall 0.675106
2017-12-10T00:34:07.459588: step 36, loss 3.08015, acc 0.580078, prec 0.0194906, recall 0.676349
2017-12-10T00:34:08.364689: step 37, loss 1.83377, acc 0.615234, prec 0.0196147, recall 0.682927
2017-12-10T00:34:09.271908: step 38, loss 6.73866, acc 0.638672, prec 0.0198857, recall 0.674419
2017-12-10T00:34:10.181248: step 39, loss 4.25139, acc 0.705078, prec 0.0202202, recall 0.669145
2017-12-10T00:34:11.103730: step 40, loss 4.59318, acc 0.677734, prec 0.0206129, recall 0.672662
2017-12-10T00:34:12.014771: step 41, loss 2.7681, acc 0.621094, prec 0.0203972, recall 0.670213
2017-12-10T00:34:12.919771: step 42, loss 3.98119, acc 0.613281, prec 0.0202874, recall 0.666667
2017-12-10T00:34:13.836096: step 43, loss 2.75516, acc 0.589844, prec 0.0203575, recall 0.667797
2017-12-10T00:34:14.743037: step 44, loss 4.57805, acc 0.541016, prec 0.0203773, recall 0.664474
2017-12-10T00:34:15.651775: step 45, loss 2.22705, acc 0.568359, prec 0.0206094, recall 0.672026
2017-12-10T00:34:16.559841: step 46, loss 2.29227, acc 0.498047, prec 0.0211356, recall 0.68323
2017-12-10T00:34:17.458548: step 47, loss 4.18595, acc 0.533203, prec 0.0210368, recall 0.678788
2017-12-10T00:34:18.361062: step 48, loss 2.68524, acc 0.517578, prec 0.0210111, recall 0.681548
2017-12-10T00:34:19.269390: step 49, loss 3.86132, acc 0.503906, prec 0.0214996, recall 0.689655
2017-12-10T00:34:20.173959: step 50, loss 2.74556, acc 0.521484, prec 0.0218972, recall 0.696379
2017-12-10T00:34:21.104016: step 51, loss 2.60945, acc 0.5, prec 0.0221728, recall 0.701897
2017-12-10T00:34:22.006358: step 52, loss 4.93232, acc 0.574219, prec 0.0222689, recall 0.697368
2017-12-10T00:34:22.925282: step 53, loss 2.61719, acc 0.572266, prec 0.022433, recall 0.701031
2017-12-10T00:34:23.828028: step 54, loss 2.20921, acc 0.560547, prec 0.022501, recall 0.703797
2017-12-10T00:34:24.728811: step 55, loss 2.56724, acc 0.615234, prec 0.0228521, recall 0.706897
2017-12-10T00:34:25.638801: step 56, loss 2.96046, acc 0.617188, prec 0.0229624, recall 0.709443
2017-12-10T00:34:26.553038: step 57, loss 3.37317, acc 0.625, prec 0.0230045, recall 0.707838
2017-12-10T00:34:27.458361: step 58, loss 3.64917, acc 0.667969, prec 0.0232346, recall 0.706019
2017-12-10T00:34:28.371416: step 59, loss 2.01168, acc 0.599609, prec 0.0231721, recall 0.707094
2017-12-10T00:34:29.281244: step 60, loss 2.03577, acc 0.634766, prec 0.0231423, recall 0.708145
2017-12-10T00:34:30.195328: step 61, loss 1.75249, acc 0.625, prec 0.0233168, recall 0.712695
2017-12-10T00:34:31.108941: step 62, loss 3.77597, acc 0.650391, prec 0.0235852, recall 0.711497
2017-12-10T00:34:31.292231: step 63, loss 1.46507, acc 0.788462, prec 0.0237069, recall 0.712743
2017-12-10T00:34:32.206638: step 64, loss 3.24266, acc 0.607422, prec 0.0237876, recall 0.713376
2017-12-10T00:34:33.119343: step 65, loss 1.61687, acc 0.615234, prec 0.0235968, recall 0.714588
2017-12-10T00:34:34.022407: step 66, loss 1.90035, acc 0.583984, prec 0.0235869, recall 0.717573
2017-12-10T00:34:34.927196: step 67, loss 2.46734, acc 0.595703, prec 0.0236546, recall 0.719588
2017-12-10T00:34:35.846155: step 68, loss 1.54253, acc 0.646484, prec 0.0236294, recall 0.721881
2017-12-10T00:34:36.756838: step 69, loss 1.25359, acc 0.697266, prec 0.0236455, recall 0.724138
2017-12-10T00:34:37.664166: step 70, loss 2.89924, acc 0.734375, prec 0.0238876, recall 0.723658
2017-12-10T00:34:38.572554: step 71, loss 2.60841, acc 0.738281, prec 0.0239381, recall 0.721569
2017-12-10T00:34:39.482864: step 72, loss 1.69028, acc 0.777344, prec 0.0244579, recall 0.724665
2017-12-10T00:34:40.389297: step 73, loss 1.11045, acc 0.761719, prec 0.0248912, recall 0.729831
2017-12-10T00:34:41.296507: step 74, loss 3.90341, acc 0.738281, prec 0.0251189, recall 0.727941
2017-12-10T00:34:42.206750: step 75, loss 1.92489, acc 0.71875, prec 0.0252608, recall 0.729583
2017-12-10T00:34:43.121630: step 76, loss 2.3862, acc 0.701172, prec 0.025448, recall 0.730357
2017-12-10T00:34:44.027773: step 77, loss 2.38597, acc 0.652344, prec 0.0255306, recall 0.731922
2017-12-10T00:34:44.939621: step 78, loss 1.48944, acc 0.681641, prec 0.0256332, recall 0.734729
2017-12-10T00:34:45.861181: step 79, loss 5.02937, acc 0.632812, prec 0.0257012, recall 0.732419
2017-12-10T00:34:46.778721: step 80, loss 2.5646, acc 0.617188, prec 0.0258667, recall 0.734797
2017-12-10T00:34:47.705875: step 81, loss 1.81762, acc 0.591797, prec 0.0258352, recall 0.737018
2017-12-10T00:34:48.606838: step 82, loss 1.69796, acc 0.597656, prec 0.0258091, recall 0.739203
2017-12-10T00:34:49.512592: step 83, loss 3.40817, acc 0.597656, prec 0.0261198, recall 0.742671
2017-12-10T00:34:50.419006: step 84, loss 3.56291, acc 0.607422, prec 0.0261564, recall 0.742765
2017-12-10T00:34:51.344095: step 85, loss 2.81732, acc 0.646484, prec 0.0260046, recall 0.740032
2017-12-10T00:34:52.267299: step 86, loss 3.30707, acc 0.6875, prec 0.0264298, recall 0.73913
2017-12-10T00:34:53.180048: step 87, loss 1.4775, acc 0.644531, prec 0.0264864, recall 0.741538
2017-12-10T00:34:54.094453: step 88, loss 3.63409, acc 0.644531, prec 0.0264389, recall 0.740854
2017-12-10T00:34:55.005206: step 89, loss 2.08342, acc 0.669922, prec 0.0264619, recall 0.741692
2017-12-10T00:34:55.916275: step 90, loss 1.39617, acc 0.660156, prec 0.0264773, recall 0.742515
2017-12-10T00:34:56.823110: step 91, loss 2.80138, acc 0.644531, prec 0.0263283, recall 0.741071
2017-12-10T00:34:57.735402: step 92, loss 0.967083, acc 0.712891, prec 0.0264317, recall 0.743363
2017-12-10T00:34:58.643638: step 93, loss 2.88126, acc 0.753906, prec 0.026568, recall 0.741279
2017-12-10T00:34:59.553187: step 94, loss 1.63081, acc 0.732422, prec 0.0265825, recall 0.741703
2017-12-10T00:35:00.469487: step 95, loss 1.87973, acc 0.716797, prec 0.0265859, recall 0.74212
2017-12-10T00:35:01.379168: step 96, loss 0.958498, acc 0.746094, prec 0.0266578, recall 0.743954
2017-12-10T00:35:02.293035: step 97, loss 2.00261, acc 0.730469, prec 0.0267206, recall 0.743662
2017-12-10T00:35:03.202473: step 98, loss 3.39272, acc 0.761719, prec 0.0270026, recall 0.742739
2017-12-10T00:35:04.120579: step 99, loss 1.09904, acc 0.765625, prec 0.0271823, recall 0.744186
2017-12-10T00:35:05.030224: step 100, loss 1.28047, acc 0.677734, prec 0.027106, recall 0.744218
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_3/1512884012/checkpoints/model-100

2017-12-10T00:35:07.017282: step 101, loss 3.03036, acc 0.667969, prec 0.0271213, recall 0.743935
2017-12-10T00:35:07.921451: step 102, loss 1.84246, acc 0.701172, prec 0.0274907, recall 0.746032
2017-12-10T00:35:08.839524: step 103, loss 1.87151, acc 0.693359, prec 0.0274728, recall 0.745407
2017-12-10T00:35:09.743232: step 104, loss 3.96575, acc 0.634766, prec 0.0272784, recall 0.741851
2017-12-10T00:35:10.650443: step 105, loss 1.67928, acc 0.632812, prec 0.0274519, recall 0.743887
2017-12-10T00:35:11.550978: step 106, loss 1.4673, acc 0.613281, prec 0.0275165, recall 0.746173
2017-12-10T00:35:12.451789: step 107, loss 1.60597, acc 0.609375, prec 0.0275332, recall 0.747155
2017-12-10T00:35:13.363829: step 108, loss 1.60196, acc 0.660156, prec 0.0277624, recall 0.749377
2017-12-10T00:35:14.269332: step 109, loss 3.26158, acc 0.673828, prec 0.0278223, recall 0.747537
2017-12-10T00:35:15.176633: step 110, loss 1.71989, acc 0.626953, prec 0.0276718, recall 0.746324
2017-12-10T00:35:16.090770: step 111, loss 1.27411, acc 0.660156, prec 0.0274985, recall 0.746634
2017-12-10T00:35:16.990185: step 112, loss 2.9742, acc 0.679688, prec 0.027692, recall 0.746683
2017-12-10T00:35:17.898270: step 113, loss 2.0471, acc 0.658203, prec 0.0279109, recall 0.747919
2017-12-10T00:35:18.799269: step 114, loss 1.36599, acc 0.642578, prec 0.0278145, recall 0.748815
2017-12-10T00:35:19.700167: step 115, loss 2.1262, acc 0.701172, prec 0.0277996, recall 0.749117
2017-12-10T00:35:20.604087: step 116, loss 1.87261, acc 0.742188, prec 0.0278116, recall 0.748538
2017-12-10T00:35:21.532571: step 117, loss 1.81815, acc 0.732422, prec 0.0279865, recall 0.748268
2017-12-10T00:35:22.437906: step 118, loss 3.0185, acc 0.705078, prec 0.0280602, recall 0.746575
2017-12-10T00:35:23.352046: step 119, loss 2.21948, acc 0.71875, prec 0.0280976, recall 0.746319
2017-12-10T00:35:24.266808: step 120, loss 1.33332, acc 0.6875, prec 0.0280318, recall 0.746336
2017-12-10T00:35:25.178262: step 121, loss 1.6374, acc 0.644531, prec 0.0280637, recall 0.747204
2017-12-10T00:35:26.088349: step 122, loss 1.69148, acc 0.636719, prec 0.0282119, recall 0.748894
2017-12-10T00:35:26.997833: step 123, loss 1.40767, acc 0.625, prec 0.0283896, recall 0.751641
2017-12-10T00:35:27.899592: step 124, loss 1.48655, acc 0.695312, prec 0.0282899, recall 0.750545
2017-12-10T00:35:28.812741: step 125, loss 1.76537, acc 0.724609, prec 0.0283662, recall 0.751351
2017-12-10T00:35:28.998206: step 126, loss 2.24472, acc 0.788462, prec 0.0283931, recall 0.75162
2017-12-10T00:35:29.905729: step 127, loss 0.982358, acc 0.703125, prec 0.0285726, recall 0.754011
2017-12-10T00:35:30.832175: step 128, loss 1.98832, acc 0.714844, prec 0.0285634, recall 0.753454
2017-12-10T00:35:31.732843: step 129, loss 2.64131, acc 0.65625, prec 0.02856, recall 0.752371
2017-12-10T00:35:32.645832: step 130, loss 1.39679, acc 0.679688, prec 0.0287996, recall 0.754422
2017-12-10T00:35:33.564288: step 131, loss 1.18571, acc 0.654297, prec 0.0289049, recall 0.75645
2017-12-10T00:35:34.484286: step 132, loss 1.21681, acc 0.660156, prec 0.0289361, recall 0.757949
2017-12-10T00:35:35.396907: step 133, loss 1.41143, acc 0.712891, prec 0.0290363, recall 0.758901
2017-12-10T00:35:36.332398: step 134, loss 3.57521, acc 0.730469, prec 0.0291499, recall 0.756784
2017-12-10T00:35:37.235795: step 135, loss 1.81426, acc 0.736328, prec 0.0292623, recall 0.756972
2017-12-10T00:35:38.151014: step 136, loss 1.56437, acc 0.703125, prec 0.0293532, recall 0.757905
2017-12-10T00:35:39.069896: step 137, loss 2.28814, acc 0.646484, prec 0.0294844, recall 0.759296
2017-12-10T00:35:39.980027: step 138, loss 0.951382, acc 0.732422, prec 0.0294418, recall 0.76
2017-12-10T00:35:40.884923: step 139, loss 1.69069, acc 0.722656, prec 0.0295776, recall 0.761122
2017-12-10T00:35:41.796889: step 140, loss 1.42003, acc 0.75, prec 0.0297274, recall 0.762224
2017-12-10T00:35:42.716041: step 141, loss 1.7393, acc 0.716797, prec 0.0299651, recall 0.763981
2017-12-10T00:35:43.629644: step 142, loss 1.03951, acc 0.726562, prec 0.0299534, recall 0.764873
2017-12-10T00:35:44.537016: step 143, loss 1.32066, acc 0.722656, prec 0.0299765, recall 0.765258
2017-12-10T00:35:45.444212: step 144, loss 1.20874, acc 0.753906, prec 0.0299458, recall 0.765201
2017-12-10T00:35:46.356223: step 145, loss 1.45552, acc 0.751953, prec 0.0299851, recall 0.765581
2017-12-10T00:35:47.266292: step 146, loss 0.727918, acc 0.808594, prec 0.0301601, recall 0.767313
2017-12-10T00:35:48.179448: step 147, loss 0.830019, acc 0.814453, prec 0.0300926, recall 0.76682
2017-12-10T00:35:49.086792: step 148, loss 1.73243, acc 0.830078, prec 0.0301402, recall 0.76627
2017-12-10T00:35:49.998723: step 149, loss 2.26274, acc 0.84375, prec 0.0301614, recall 0.764813
2017-12-10T00:35:50.933887: step 150, loss 1.03021, acc 0.806641, prec 0.0301945, recall 0.764973
2017-12-10T00:35:51.837722: step 151, loss 1.25591, acc 0.792969, prec 0.0302209, recall 0.76444
2017-12-10T00:35:52.746056: step 152, loss 0.914522, acc 0.792969, prec 0.0303493, recall 0.765233
2017-12-10T00:35:53.661989: step 153, loss 1.43939, acc 0.759766, prec 0.0305614, recall 0.766637
2017-12-10T00:35:54.578293: step 154, loss 1.56488, acc 0.765625, prec 0.0305368, recall 0.765901
2017-12-10T00:35:55.483730: step 155, loss 0.834628, acc 0.753906, prec 0.0306398, recall 0.76734
2017-12-10T00:35:56.386324: step 156, loss 1.50187, acc 0.751953, prec 0.0306076, recall 0.766608
2017-12-10T00:35:57.296044: step 157, loss 1.53324, acc 0.742188, prec 0.0308723, recall 0.768366
2017-12-10T00:35:58.214705: step 158, loss 2.09224, acc 0.679688, prec 0.0308, recall 0.767642
2017-12-10T00:35:59.123125: step 159, loss 1.25576, acc 0.744141, prec 0.0308623, recall 0.768178
2017-12-10T00:36:00.032632: step 160, loss 1.60967, acc 0.732422, prec 0.0310833, recall 0.769687
2017-12-10T00:36:00.953053: step 161, loss 1.06061, acc 0.703125, prec 0.0311862, recall 0.771236
2017-12-10T00:36:01.858185: step 162, loss 1.23426, acc 0.693359, prec 0.0312172, recall 0.772385
2017-12-10T00:36:02.767362: step 163, loss 1.54217, acc 0.703125, prec 0.0311238, recall 0.77212
2017-12-10T00:36:03.676622: step 164, loss 0.987511, acc 0.748047, prec 0.0312165, recall 0.773444
2017-12-10T00:36:04.581947: step 165, loss 1.25895, acc 0.753906, prec 0.031152, recall 0.772539
2017-12-10T00:36:05.489455: step 166, loss 2.56852, acc 0.744141, prec 0.0311483, recall 0.771382
2017-12-10T00:36:06.417243: step 167, loss 0.894384, acc 0.773438, prec 0.0312851, recall 0.772876
2017-12-10T00:36:07.317443: step 168, loss 0.868777, acc 0.775391, prec 0.0313272, recall 0.773171
2017-12-10T00:36:08.217342: step 169, loss 1.91175, acc 0.759766, prec 0.0313628, recall 0.772213
2017-12-10T00:36:09.124461: step 170, loss 1.96037, acc 0.800781, prec 0.0316096, recall 0.772364
2017-12-10T00:36:10.031752: step 171, loss 1.37547, acc 0.78125, prec 0.0317171, recall 0.772403
2017-12-10T00:36:10.945012: step 172, loss 0.742621, acc 0.798828, prec 0.031831, recall 0.773659
2017-12-10T00:36:11.863661: step 173, loss 1.67087, acc 0.78125, prec 0.0319692, recall 0.77326
2017-12-10T00:36:12.777880: step 174, loss 1.17499, acc 0.712891, prec 0.0320059, recall 0.773717
2017-12-10T00:36:13.692507: step 175, loss 1.80478, acc 0.746094, prec 0.0319677, recall 0.773044
2017-12-10T00:36:14.606955: step 176, loss 1.51342, acc 0.726562, prec 0.0319186, recall 0.772973
2017-12-10T00:36:15.513433: step 177, loss 1.3706, acc 0.667969, prec 0.0319932, recall 0.773773
2017-12-10T00:36:16.424929: step 178, loss 2.08486, acc 0.650391, prec 0.0319977, recall 0.773628
2017-12-10T00:36:17.353167: step 179, loss 1.3847, acc 0.671875, prec 0.0320426, recall 0.774242
2017-12-10T00:36:18.255537: step 180, loss 1.38207, acc 0.644531, prec 0.0321626, recall 0.77594
2017-12-10T00:36:19.159713: step 181, loss 1.22169, acc 0.677734, prec 0.0321781, recall 0.776946
2017-12-10T00:36:20.073182: step 182, loss 2.01032, acc 0.701172, prec 0.0321775, recall 0.776619
2017-12-10T00:36:21.004688: step 183, loss 2.31396, acc 0.744141, prec 0.0322581, recall 0.776627
2017-12-10T00:36:21.900574: step 184, loss 1.18251, acc 0.71875, prec 0.0322058, recall 0.776549
2017-12-10T00:36:22.810731: step 185, loss 0.807365, acc 0.75, prec 0.032316, recall 0.777859
2017-12-10T00:36:23.717388: step 186, loss 0.896216, acc 0.767578, prec 0.0324342, recall 0.779155
2017-12-10T00:36:24.627488: step 187, loss 2.66032, acc 0.787109, prec 0.0324473, recall 0.7781
2017-12-10T00:36:25.535513: step 188, loss 1.32012, acc 0.865234, prec 0.0326143, recall 0.778818
2017-12-10T00:36:25.719328: step 189, loss 0.550389, acc 0.903846, prec 0.0326385, recall 0.778978
2017-12-10T00:36:26.646325: step 190, loss 0.934781, acc 0.810547, prec 0.0326316, recall 0.778894
2017-12-10T00:36:27.547653: step 191, loss 0.809927, acc 0.855469, prec 0.0326772, recall 0.778413
2017-12-10T00:36:28.463442: step 192, loss 0.693984, acc 0.853516, prec 0.0328075, recall 0.778962
2017-12-10T00:36:29.385638: step 193, loss 1.01101, acc 0.814453, prec 0.0328319, recall 0.778485
2017-12-10T00:36:30.296427: step 194, loss 1.10143, acc 0.865234, prec 0.0328518, recall 0.778405
2017-12-10T00:36:31.217260: step 195, loss 1.87114, acc 0.853516, prec 0.0330114, recall 0.778011
2017-12-10T00:36:32.131054: step 196, loss 0.49508, acc 0.847656, prec 0.0331357, recall 0.779094
2017-12-10T00:36:33.048776: step 197, loss 1.18704, acc 0.8125, prec 0.0332151, recall 0.778933
2017-12-10T00:36:33.953684: step 198, loss 0.871235, acc 0.796875, prec 0.0333422, recall 0.779614
2017-12-10T00:36:34.871429: step 199, loss 0.685079, acc 0.789062, prec 0.0334067, recall 0.780521
2017-12-10T00:36:35.778339: step 200, loss 1.21352, acc 0.779297, prec 0.0335527, recall 0.780803
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_3/1512884012/checkpoints/model-200

2017-12-10T00:36:37.871373: step 201, loss 0.818612, acc 0.732422, prec 0.0335314, recall 0.781399
2017-12-10T00:36:38.803732: step 202, loss 0.800262, acc 0.785156, prec 0.0336488, recall 0.782579
2017-12-10T00:36:39.704935: step 203, loss 1.03566, acc 0.765625, prec 0.0337846, recall 0.783367
2017-12-10T00:36:40.626215: step 204, loss 0.966537, acc 0.765625, prec 0.0338358, recall 0.783712
2017-12-10T00:36:41.539689: step 205, loss 1.57235, acc 0.767578, prec 0.0339163, recall 0.783676
2017-12-10T00:36:42.448961: step 206, loss 1.12362, acc 0.755859, prec 0.0339065, recall 0.78373
2017-12-10T00:36:43.355459: step 207, loss 0.71472, acc 0.777344, prec 0.0340167, recall 0.784868
2017-12-10T00:36:44.258889: step 208, loss 0.979006, acc 0.830078, prec 0.0340434, recall 0.784918
2017-12-10T00:36:45.158398: step 209, loss 1.19626, acc 0.814453, prec 0.034117, recall 0.785248
2017-12-10T00:36:46.073819: step 210, loss 0.706471, acc 0.832031, prec 0.0343072, recall 0.786641
2017-12-10T00:36:47.005616: step 211, loss 0.712536, acc 0.808594, prec 0.034404, recall 0.787097
2017-12-10T00:36:47.914490: step 212, loss 0.565367, acc 0.835938, prec 0.0344585, recall 0.787781
2017-12-10T00:36:48.823644: step 213, loss 2.36088, acc 0.867188, prec 0.0345302, recall 0.787452
2017-12-10T00:36:49.731945: step 214, loss 0.996591, acc 0.853516, prec 0.0346209, recall 0.787763
2017-12-10T00:36:50.660480: step 215, loss 0.633972, acc 0.851562, prec 0.0347901, recall 0.788973
2017-12-10T00:36:51.577657: step 216, loss 0.749348, acc 0.824219, prec 0.0348374, recall 0.78964
2017-12-10T00:36:52.489059: step 217, loss 0.788221, acc 0.849609, prec 0.0349785, recall 0.790201
2017-12-10T00:36:53.396500: step 218, loss 2.09488, acc 0.810547, prec 0.0350469, recall 0.79
2017-12-10T00:36:54.307095: step 219, loss 0.689931, acc 0.8125, prec 0.0350872, recall 0.790654
2017-12-10T00:36:55.213414: step 220, loss 1.54261, acc 0.800781, prec 0.0352023, recall 0.791202
2017-12-10T00:36:56.129930: step 221, loss 0.949594, acc 0.792969, prec 0.0352864, recall 0.791615
2017-12-10T00:36:57.035491: step 222, loss 0.69083, acc 0.767578, prec 0.035462, recall 0.793019
2017-12-10T00:36:57.939804: step 223, loss 0.933773, acc 0.761719, prec 0.0355281, recall 0.793902
2017-12-10T00:36:58.843723: step 224, loss 0.869391, acc 0.783203, prec 0.0354996, recall 0.794279
2017-12-10T00:36:59.745685: step 225, loss 0.772439, acc 0.791016, prec 0.035475, recall 0.794654
2017-12-10T00:37:00.679048: step 226, loss 0.708476, acc 0.806641, prec 0.0356409, recall 0.795894
2017-12-10T00:37:01.586287: step 227, loss 0.591028, acc 0.828125, prec 0.0357124, recall 0.796631
2017-12-10T00:37:02.499726: step 228, loss 0.603098, acc 0.855469, prec 0.035771, recall 0.797241
2017-12-10T00:37:03.411223: step 229, loss 1.4694, acc 0.861328, prec 0.035886, recall 0.797136
2017-12-10T00:37:04.317347: step 230, loss 1.43129, acc 0.84375, prec 0.0360177, recall 0.797153
2017-12-10T00:37:05.221897: step 231, loss 0.792681, acc 0.84375, prec 0.0361736, recall 0.797759
2017-12-10T00:37:06.160693: step 232, loss 2.07543, acc 0.857422, prec 0.0363098, recall 0.79824
2017-12-10T00:37:07.059095: step 233, loss 0.570409, acc 0.861328, prec 0.0364217, recall 0.798599
2017-12-10T00:37:07.961599: step 234, loss 0.909397, acc 0.794922, prec 0.0364746, recall 0.798837
2017-12-10T00:37:08.864626: step 235, loss 1.87469, acc 0.810547, prec 0.0366135, recall 0.798499
2017-12-10T00:37:09.774524: step 236, loss 0.674306, acc 0.751953, prec 0.036567, recall 0.798847
2017-12-10T00:37:10.674381: step 237, loss 1.05622, acc 0.734375, prec 0.0366399, recall 0.799312
2017-12-10T00:37:11.581515: step 238, loss 1.11626, acc 0.736328, prec 0.0366617, recall 0.8
2017-12-10T00:37:12.493757: step 239, loss 0.804516, acc 0.748047, prec 0.0366388, recall 0.800456
2017-12-10T00:37:13.407304: step 240, loss 0.872, acc 0.744141, prec 0.0366894, recall 0.801249
2017-12-10T00:37:14.316737: step 241, loss 1.36541, acc 0.787109, prec 0.0367874, recall 0.801242
2017-12-10T00:37:15.234836: step 242, loss 0.761784, acc 0.785156, prec 0.036882, recall 0.802136
2017-12-10T00:37:16.147533: step 243, loss 1.18814, acc 0.792969, prec 0.0369807, recall 0.802573
2017-12-10T00:37:17.048959: step 244, loss 1.10767, acc 0.810547, prec 0.0370875, recall 0.803005
2017-12-10T00:37:17.949696: step 245, loss 0.885427, acc 0.794922, prec 0.0371614, recall 0.803324
2017-12-10T00:37:18.853923: step 246, loss 0.933066, acc 0.818359, prec 0.0371724, recall 0.803315
2017-12-10T00:37:19.758510: step 247, loss 0.669218, acc 0.826172, prec 0.0373336, recall 0.804396
2017-12-10T00:37:20.692178: step 248, loss 0.49966, acc 0.841797, prec 0.0373791, recall 0.804932
2017-12-10T00:37:21.603945: step 249, loss 0.889183, acc 0.830078, prec 0.0374442, recall 0.805131
2017-12-10T00:37:22.511363: step 250, loss 0.775171, acc 0.84375, prec 0.0374667, recall 0.805117
2017-12-10T00:37:23.415134: step 251, loss 1.36273, acc 0.828125, prec 0.0377006, recall 0.806051
2017-12-10T00:37:23.598306: step 252, loss 0.661881, acc 0.807692, prec 0.0377154, recall 0.806156
2017-12-10T00:37:24.509520: step 253, loss 0.650315, acc 0.816406, prec 0.0377725, recall 0.806347
2017-12-10T00:37:25.419883: step 254, loss 0.521358, acc 0.833984, prec 0.0378853, recall 0.807177
2017-12-10T00:37:26.330406: step 255, loss 0.451166, acc 0.845703, prec 0.0378102, recall 0.807177
2017-12-10T00:37:27.237904: step 256, loss 0.944901, acc 0.830078, prec 0.0380409, recall 0.808511
2017-12-10T00:37:28.148595: step 257, loss 1.08428, acc 0.873047, prec 0.0381964, recall 0.808995
2017-12-10T00:37:29.060894: step 258, loss 0.550472, acc 0.84375, prec 0.0382401, recall 0.809499
2017-12-10T00:37:29.967748: step 259, loss 1.27361, acc 0.859375, prec 0.0382682, recall 0.809474
2017-12-10T00:37:30.901800: step 260, loss 0.815981, acc 0.863281, prec 0.0383708, recall 0.809324
2017-12-10T00:37:31.807337: step 261, loss 0.468274, acc 0.869141, prec 0.0384263, recall 0.809822
2017-12-10T00:37:32.717154: step 262, loss 0.484303, acc 0.875, prec 0.0385558, recall 0.810614
2017-12-10T00:37:33.631259: step 263, loss 0.566632, acc 0.873047, prec 0.0387551, recall 0.811692
2017-12-10T00:37:34.530853: step 264, loss 1.18714, acc 0.845703, prec 0.0388464, recall 0.811953
2017-12-10T00:37:35.433151: step 265, loss 0.575868, acc 0.830078, prec 0.0388815, recall 0.812436
2017-12-10T00:37:36.351058: step 266, loss 0.948575, acc 0.835938, prec 0.0390617, recall 0.813075
2017-12-10T00:37:37.261158: step 267, loss 0.55212, acc 0.873047, prec 0.0390937, recall 0.813456
2017-12-10T00:37:38.163565: step 268, loss 0.583355, acc 0.841797, prec 0.0392277, recall 0.814307
2017-12-10T00:37:39.073707: step 269, loss 0.671883, acc 0.867188, prec 0.039397, recall 0.815245
2017-12-10T00:37:39.991709: step 270, loss 0.464153, acc 0.859375, prec 0.0394682, recall 0.815803
2017-12-10T00:37:40.892081: step 271, loss 0.53157, acc 0.839844, prec 0.039553, recall 0.816449
2017-12-10T00:37:41.793794: step 272, loss 1.11202, acc 0.865234, prec 0.0396294, recall 0.815776
2017-12-10T00:37:42.707010: step 273, loss 0.647338, acc 0.876953, prec 0.0397327, recall 0.816012
2017-12-10T00:37:43.618467: step 274, loss 0.756328, acc 0.882812, prec 0.0397921, recall 0.816063
2017-12-10T00:37:44.529264: step 275, loss 1.20893, acc 0.867188, prec 0.0399141, recall 0.815984
2017-12-10T00:37:45.442631: step 276, loss 0.952513, acc 0.828125, prec 0.0400154, recall 0.816306
2017-12-10T00:37:46.350287: step 277, loss 0.494083, acc 0.818359, prec 0.0400413, recall 0.816756
2017-12-10T00:37:47.259769: step 278, loss 0.570382, acc 0.832031, prec 0.0401658, recall 0.817561
2017-12-10T00:37:48.159290: step 279, loss 0.431917, acc 0.849609, prec 0.0401837, recall 0.817916
2017-12-10T00:37:49.074008: step 280, loss 0.746505, acc 0.818359, prec 0.0403235, recall 0.818798
2017-12-10T00:37:49.987750: step 281, loss 0.511555, acc 0.84375, prec 0.0404752, recall 0.819672
2017-12-10T00:37:50.920186: step 282, loss 0.83307, acc 0.859375, prec 0.0404991, recall 0.819231
2017-12-10T00:37:51.824413: step 283, loss 0.697322, acc 0.841797, prec 0.0405588, recall 0.819358
2017-12-10T00:37:52.739455: step 284, loss 0.557422, acc 0.865234, prec 0.0406743, recall 0.820048
2017-12-10T00:37:53.639680: step 285, loss 0.775217, acc 0.865234, prec 0.0407449, recall 0.820171
2017-12-10T00:37:54.547478: step 286, loss 0.940547, acc 0.855469, prec 0.0408568, recall 0.820076
2017-12-10T00:37:55.453734: step 287, loss 1.06268, acc 0.875, prec 0.0408865, recall 0.820028
2017-12-10T00:37:56.370091: step 288, loss 0.755351, acc 0.849609, prec 0.0409938, recall 0.82032
2017-12-10T00:37:57.277677: step 289, loss 1.06898, acc 0.830078, prec 0.0410686, recall 0.820525
2017-12-10T00:37:58.194183: step 290, loss 0.765733, acc 0.845703, prec 0.0411732, recall 0.820812
2017-12-10T00:37:59.113510: step 291, loss 0.896133, acc 0.794922, prec 0.0412299, recall 0.821014
2017-12-10T00:38:00.017839: step 292, loss 0.787724, acc 0.773438, prec 0.0411865, recall 0.820882
2017-12-10T00:38:00.947126: step 293, loss 1.32935, acc 0.759766, prec 0.0411821, recall 0.820537
2017-12-10T00:38:01.851087: step 294, loss 0.562173, acc 0.808594, prec 0.0412662, recall 0.821198
2017-12-10T00:38:02.752023: step 295, loss 0.723987, acc 0.771484, prec 0.0412876, recall 0.821691
2017-12-10T00:38:03.660301: step 296, loss 0.664011, acc 0.767578, prec 0.041329, recall 0.822263
2017-12-10T00:38:04.572437: step 297, loss 0.854398, acc 0.806641, prec 0.0413901, recall 0.822456
2017-12-10T00:38:05.493017: step 298, loss 0.581436, acc 0.804688, prec 0.041471, recall 0.823101
2017-12-10T00:38:06.410815: step 299, loss 0.734073, acc 0.830078, prec 0.041499, recall 0.823129
2017-12-10T00:38:07.318619: step 300, loss 1.29582, acc 0.841797, prec 0.0415782, recall 0.822573

Evaluation:
2017-12-10T00:38:12.022454: step 300, loss 1.58827, acc 0.876958, prec 0.0424085, recall 0.808017

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_3/1512884012/checkpoints/model-300

2017-12-10T00:38:13.998592: step 301, loss 0.438577, acc 0.867188, prec 0.0423871, recall 0.808179
2017-12-10T00:38:14.927573: step 302, loss 0.456166, acc 0.884766, prec 0.0426067, recall 0.809224
2017-12-10T00:38:15.832861: step 303, loss 0.532211, acc 0.869141, prec 0.0426493, recall 0.809623
2017-12-10T00:38:16.735189: step 304, loss 0.346994, acc 0.873047, prec 0.0426937, recall 0.810021
2017-12-10T00:38:17.639095: step 305, loss 0.811793, acc 0.886719, prec 0.0427454, recall 0.810079
2017-12-10T00:38:18.542744: step 306, loss 0.929602, acc 0.875, prec 0.0428543, recall 0.810373
2017-12-10T00:38:19.472306: step 307, loss 0.628923, acc 0.861328, prec 0.0429354, recall 0.810587
2017-12-10T00:38:20.382501: step 308, loss 0.765732, acc 0.876953, prec 0.0430655, recall 0.810956
2017-12-10T00:38:21.313134: step 309, loss 1.05967, acc 0.859375, prec 0.0431242, recall 0.811088
2017-12-10T00:38:22.219073: step 310, loss 0.437011, acc 0.849609, prec 0.0431352, recall 0.811398
2017-12-10T00:38:23.130480: step 311, loss 0.646287, acc 0.839844, prec 0.0431633, recall 0.811452
2017-12-10T00:38:24.042055: step 312, loss 0.758904, acc 0.855469, prec 0.0432403, recall 0.811659
2017-12-10T00:38:24.945211: step 313, loss 0.673212, acc 0.822266, prec 0.0432794, recall 0.812119
2017-12-10T00:38:25.854303: step 314, loss 0.744195, acc 0.84375, prec 0.0434124, recall 0.812551
2017-12-10T00:38:26.037860: step 315, loss 0.708619, acc 0.788462, prec 0.0434021, recall 0.812551
2017-12-10T00:38:26.953710: step 316, loss 0.544801, acc 0.832031, prec 0.0434454, recall 0.813005
2017-12-10T00:38:27.867547: step 317, loss 0.765034, acc 0.859375, prec 0.043482, recall 0.813054
2017-12-10T00:38:28.772937: step 318, loss 0.452408, acc 0.84375, prec 0.0435718, recall 0.813655
2017-12-10T00:38:29.692562: step 319, loss 0.819654, acc 0.839844, prec 0.0436192, recall 0.813777
2017-12-10T00:38:30.615106: step 320, loss 0.537542, acc 0.880859, prec 0.0437467, recall 0.814445
2017-12-10T00:38:31.524199: step 321, loss 0.390733, acc 0.865234, prec 0.0438458, recall 0.815036
2017-12-10T00:38:32.430645: step 322, loss 0.401336, acc 0.876953, prec 0.0438276, recall 0.815183
2017-12-10T00:38:33.334262: step 323, loss 0.948519, acc 0.873047, prec 0.0439115, recall 0.81505
2017-12-10T00:38:34.237848: step 324, loss 0.389034, acc 0.880859, prec 0.0439766, recall 0.815488
2017-12-10T00:38:35.163595: step 325, loss 0.52495, acc 0.871094, prec 0.0440166, recall 0.815852
2017-12-10T00:38:36.092635: step 326, loss 0.519666, acc 0.859375, prec 0.0440914, recall 0.816359
2017-12-10T00:38:37.000424: step 327, loss 0.446284, acc 0.892578, prec 0.0442022, recall 0.816935
2017-12-10T00:38:37.915607: step 328, loss 0.548649, acc 0.878906, prec 0.0442665, recall 0.817045
2017-12-10T00:38:38.830368: step 329, loss 0.521657, acc 0.859375, prec 0.0443608, recall 0.817615
2017-12-10T00:38:39.769183: step 330, loss 0.971044, acc 0.871094, prec 0.0443605, recall 0.81751
2017-12-10T00:38:40.686787: step 331, loss 0.618139, acc 0.896484, prec 0.0444922, recall 0.818147
2017-12-10T00:38:41.593985: step 332, loss 0.472706, acc 0.888672, prec 0.0446602, recall 0.818919
2017-12-10T00:38:42.511033: step 333, loss 0.378341, acc 0.878906, prec 0.0447225, recall 0.819337
2017-12-10T00:38:43.425246: step 334, loss 0.643641, acc 0.871094, prec 0.0448018, recall 0.819508
2017-12-10T00:38:44.346554: step 335, loss 0.546345, acc 0.861328, prec 0.0448753, recall 0.819992
2017-12-10T00:38:45.266027: step 336, loss 0.934737, acc 0.849609, prec 0.0449039, recall 0.820023
2017-12-10T00:38:46.175017: step 337, loss 0.468439, acc 0.853516, prec 0.0448934, recall 0.820229
2017-12-10T00:38:47.081401: step 338, loss 0.412754, acc 0.863281, prec 0.0449872, recall 0.820776
2017-12-10T00:38:47.989244: step 339, loss 0.366755, acc 0.884766, prec 0.0449518, recall 0.820844
2017-12-10T00:38:48.897039: step 340, loss 0.368284, acc 0.876953, prec 0.0449922, recall 0.821185
2017-12-10T00:38:49.814231: step 341, loss 0.577362, acc 0.867188, prec 0.0452063, recall 0.82213
2017-12-10T00:38:50.733942: step 342, loss 0.707697, acc 0.912109, prec 0.0453037, recall 0.822289
2017-12-10T00:38:51.646585: step 343, loss 0.469322, acc 0.902344, prec 0.0454743, recall 0.823022
2017-12-10T00:38:52.573036: step 344, loss 0.553346, acc 0.894531, prec 0.0456407, recall 0.823749
2017-12-10T00:38:53.478445: step 345, loss 0.533875, acc 0.914062, prec 0.045757, recall 0.824274
2017-12-10T00:38:54.382037: step 346, loss 0.370009, acc 0.886719, prec 0.045781, recall 0.824535
2017-12-10T00:38:55.287018: step 347, loss 0.284365, acc 0.900391, prec 0.0458509, recall 0.824926
2017-12-10T00:38:56.197373: step 348, loss 1.44775, acc 0.898438, prec 0.0459029, recall 0.824334
2017-12-10T00:38:57.108137: step 349, loss 0.489251, acc 0.859375, prec 0.0460507, recall 0.825046
2017-12-10T00:38:58.020956: step 350, loss 0.419913, acc 0.871094, prec 0.0461058, recall 0.825432
2017-12-10T00:38:58.922895: step 351, loss 1.14314, acc 0.867188, prec 0.0462585, recall 0.82553
2017-12-10T00:38:59.835993: step 352, loss 1.17099, acc 0.873047, prec 0.046355, recall 0.825437
2017-12-10T00:39:00.766976: step 353, loss 0.446444, acc 0.84375, prec 0.0464157, recall 0.825881
2017-12-10T00:39:01.667322: step 354, loss 0.994797, acc 0.765625, prec 0.0464003, recall 0.825898
2017-12-10T00:39:02.590379: step 355, loss 0.662883, acc 0.779297, prec 0.0463906, recall 0.826213
2017-12-10T00:39:03.510053: step 356, loss 0.686005, acc 0.746094, prec 0.046365, recall 0.826527
2017-12-10T00:39:04.407414: step 357, loss 0.840484, acc 0.755859, prec 0.0463057, recall 0.826715
2017-12-10T00:39:05.311117: step 358, loss 0.710288, acc 0.798828, prec 0.0464211, recall 0.8274
2017-12-10T00:39:06.248044: step 359, loss 0.643694, acc 0.783203, prec 0.0464325, recall 0.827772
2017-12-10T00:39:07.152546: step 360, loss 0.712037, acc 0.841797, prec 0.0464537, recall 0.827784
2017-12-10T00:39:08.065392: step 361, loss 0.369878, acc 0.863281, prec 0.0464841, recall 0.828092
2017-12-10T00:39:08.979898: step 362, loss 0.518707, acc 0.892578, prec 0.0466048, recall 0.828643
2017-12-10T00:39:09.883040: step 363, loss 0.224927, acc 0.917969, prec 0.0466038, recall 0.828765
2017-12-10T00:39:10.794953: step 364, loss 0.405471, acc 0.910156, prec 0.0467325, recall 0.829312
2017-12-10T00:39:11.701303: step 365, loss 0.803753, acc 0.943359, prec 0.0468397, recall 0.829441
2017-12-10T00:39:12.609847: step 366, loss 0.534423, acc 0.945312, prec 0.0470048, recall 0.82975
2017-12-10T00:39:13.514946: step 367, loss 0.972704, acc 0.933594, prec 0.0471079, recall 0.829585
2017-12-10T00:39:14.426060: step 368, loss 0.375974, acc 0.923828, prec 0.0472422, recall 0.830123
2017-12-10T00:39:15.344206: step 369, loss 0.369786, acc 0.919922, prec 0.0472985, recall 0.83042
2017-12-10T00:39:16.245287: step 370, loss 1.6073, acc 0.900391, prec 0.0474419, recall 0.830432
2017-12-10T00:39:17.150253: step 371, loss 0.38854, acc 0.869141, prec 0.0475112, recall 0.830844
2017-12-10T00:39:18.060205: step 372, loss 0.611282, acc 0.871094, prec 0.0477135, recall 0.831663
2017-12-10T00:39:18.966339: step 373, loss 0.548377, acc 0.822266, prec 0.0477406, recall 0.832011
2017-12-10T00:39:19.870695: step 374, loss 0.807366, acc 0.792969, prec 0.0478109, recall 0.832245
2017-12-10T00:39:20.797911: step 375, loss 0.56791, acc 0.763672, prec 0.0477907, recall 0.832533
2017-12-10T00:39:21.699799: step 376, loss 0.733195, acc 0.773438, prec 0.0478502, recall 0.833048
2017-12-10T00:39:22.597970: step 377, loss 0.61053, acc 0.800781, prec 0.0479412, recall 0.833617
2017-12-10T00:39:22.775938: step 378, loss 0.84799, acc 0.730769, prec 0.047928, recall 0.833617
2017-12-10T00:39:23.685088: step 379, loss 0.537568, acc 0.8125, prec 0.047987, recall 0.83407
2017-12-10T00:39:24.596819: step 380, loss 0.558503, acc 0.863281, prec 0.0480702, recall 0.83452
2017-12-10T00:39:25.523878: step 381, loss 0.485278, acc 0.896484, prec 0.048169, recall 0.834968
2017-12-10T00:39:26.427096: step 382, loss 0.902385, acc 0.904297, prec 0.0482167, recall 0.834965
2017-12-10T00:39:27.337593: step 383, loss 0.279792, acc 0.898438, prec 0.048205, recall 0.835076
2017-12-10T00:39:28.242992: step 384, loss 0.412729, acc 0.919922, prec 0.0482971, recall 0.835183
2017-12-10T00:39:29.152170: step 385, loss 0.661238, acc 0.919922, prec 0.048389, recall 0.83529
2017-12-10T00:39:30.056473: step 386, loss 0.222581, acc 0.921875, prec 0.0484253, recall 0.835511
2017-12-10T00:39:30.975832: step 387, loss 0.570785, acc 0.931641, prec 0.0485964, recall 0.835836
2017-12-10T00:39:31.882586: step 388, loss 0.508845, acc 0.927734, prec 0.04871, recall 0.835995
2017-12-10T00:39:32.800128: step 389, loss 0.226184, acc 0.925781, prec 0.0487295, recall 0.836158
2017-12-10T00:39:33.718255: step 390, loss 0.434674, acc 0.927734, prec 0.0487508, recall 0.836044
2017-12-10T00:39:34.627352: step 391, loss 0.29923, acc 0.902344, prec 0.0487772, recall 0.836261
2017-12-10T00:39:35.544853: step 392, loss 0.423644, acc 0.898438, prec 0.0488568, recall 0.83664
2017-12-10T00:39:36.463383: step 393, loss 0.384932, acc 0.921875, prec 0.0489659, recall 0.837071
2017-12-10T00:39:37.384249: step 394, loss 0.758507, acc 0.871094, prec 0.0491244, recall 0.837438
2017-12-10T00:39:38.296836: step 395, loss 0.333324, acc 0.902344, prec 0.0491321, recall 0.837598
2017-12-10T00:39:39.219013: step 396, loss 0.334812, acc 0.910156, prec 0.0492166, recall 0.837971
2017-12-10T00:39:40.126566: step 397, loss 0.45786, acc 0.890625, prec 0.0493645, recall 0.838552
2017-12-10T00:39:41.044106: step 398, loss 0.700431, acc 0.869141, prec 0.049466, recall 0.838752
2017-12-10T00:39:41.962387: step 399, loss 0.410156, acc 0.869141, prec 0.0495482, recall 0.83917
2017-12-10T00:39:42.880448: step 400, loss 0.481793, acc 0.871094, prec 0.0495947, recall 0.839482
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_3/1512884012/checkpoints/model-400

2017-12-10T00:39:44.792256: step 401, loss 0.340275, acc 0.876953, prec 0.0495895, recall 0.839638
2017-12-10T00:39:45.693055: step 402, loss 0.413495, acc 0.923828, prec 0.0496976, recall 0.840052
2017-12-10T00:39:46.598868: step 403, loss 0.358086, acc 0.898438, prec 0.0498295, recall 0.840566
2017-12-10T00:39:47.521028: step 404, loss 0.294053, acc 0.902344, prec 0.049981, recall 0.841127
2017-12-10T00:39:48.430588: step 405, loss 0.312008, acc 0.892578, prec 0.050019, recall 0.841381
2017-12-10T00:39:49.347719: step 406, loss 0.833902, acc 0.929688, prec 0.0500769, recall 0.841098
2017-12-10T00:39:50.256442: step 407, loss 0.910208, acc 0.927734, prec 0.0502069, recall 0.84075
2017-12-10T00:39:51.181586: step 408, loss 0.8575, acc 0.894531, prec 0.0502484, recall 0.840203
2017-12-10T00:39:52.087677: step 409, loss 1.27049, acc 0.878906, prec 0.0503002, recall 0.839709
2017-12-10T00:39:52.993222: step 410, loss 0.598241, acc 0.863281, prec 0.0502884, recall 0.839596
2017-12-10T00:39:53.908604: step 411, loss 0.542068, acc 0.84375, prec 0.0503918, recall 0.840101
2017-12-10T00:39:54.822142: step 412, loss 0.569623, acc 0.837891, prec 0.0504203, recall 0.840402
2017-12-10T00:39:55.728255: step 413, loss 0.702182, acc 0.769531, prec 0.050362, recall 0.840552
2017-12-10T00:39:56.632813: step 414, loss 0.647805, acc 0.755859, prec 0.0503686, recall 0.840902
2017-12-10T00:39:57.539414: step 415, loss 0.614295, acc 0.783203, prec 0.0504239, recall 0.841349
2017-12-10T00:39:58.446502: step 416, loss 0.629093, acc 0.763672, prec 0.0503632, recall 0.841498
2017-12-10T00:39:59.355334: step 417, loss 0.487288, acc 0.816406, prec 0.0503989, recall 0.841843
2017-12-10T00:40:00.265944: step 418, loss 0.611599, acc 0.865234, prec 0.0505472, recall 0.842171
2017-12-10T00:40:01.181775: step 419, loss 0.503867, acc 0.878906, prec 0.0506654, recall 0.842658
2017-12-10T00:40:02.094849: step 420, loss 0.373246, acc 0.886719, prec 0.0507342, recall 0.842998
2017-12-10T00:40:03.012895: step 421, loss 0.257978, acc 0.908203, prec 0.0507604, recall 0.843192
2017-12-10T00:40:03.913024: step 422, loss 0.332683, acc 0.917969, prec 0.0508616, recall 0.843577
2017-12-10T00:40:04.824393: step 423, loss 0.554978, acc 0.939453, prec 0.0509564, recall 0.843654
2017-12-10T00:40:05.740430: step 424, loss 0.716892, acc 0.921875, prec 0.0510425, recall 0.843731
2017-12-10T00:40:06.664016: step 425, loss 0.173296, acc 0.939453, prec 0.0510483, recall 0.843826
2017-12-10T00:40:07.561474: step 426, loss 0.231075, acc 0.945312, prec 0.0511622, recall 0.844207
2017-12-10T00:40:08.461771: step 427, loss 0.940305, acc 0.947266, prec 0.0512456, recall 0.843465
2017-12-10T00:40:09.366521: step 428, loss 0.339585, acc 0.933594, prec 0.0513535, recall 0.843845
2017-12-10T00:40:10.271948: step 429, loss 1.32618, acc 0.923828, prec 0.0514943, recall 0.843552
2017-12-10T00:40:11.180736: step 430, loss 0.553964, acc 0.876953, prec 0.0516102, recall 0.843769
2017-12-10T00:40:12.086695: step 431, loss 0.696987, acc 0.849609, prec 0.0515729, recall 0.843609
2017-12-10T00:40:12.986760: step 432, loss 1.28956, acc 0.8125, prec 0.0515352, recall 0.843497
2017-12-10T00:40:13.896968: step 433, loss 0.466962, acc 0.837891, prec 0.0516133, recall 0.843919
2017-12-10T00:40:14.797795: step 434, loss 0.554247, acc 0.794922, prec 0.0517396, recall 0.844524
2017-12-10T00:40:15.712333: step 435, loss 0.705694, acc 0.755859, prec 0.0517081, recall 0.844756
2017-12-10T00:40:16.626476: step 436, loss 0.689403, acc 0.767578, prec 0.0517342, recall 0.845125
2017-12-10T00:40:17.531639: step 437, loss 0.691122, acc 0.791016, prec 0.0518059, recall 0.845584
2017-12-10T00:40:18.461263: step 438, loss 0.593408, acc 0.828125, prec 0.0518435, recall 0.845904
2017-12-10T00:40:19.364442: step 439, loss 0.549282, acc 0.841797, prec 0.0518876, recall 0.846222
2017-12-10T00:40:20.260252: step 440, loss 0.500614, acc 0.859375, prec 0.0519401, recall 0.846539
2017-12-10T00:40:20.441520: step 441, loss 0.334259, acc 0.846154, prec 0.0519497, recall 0.846584
2017-12-10T00:40:21.387402: step 442, loss 0.291747, acc 0.908203, prec 0.0520596, recall 0.84699
2017-12-10T00:40:22.291935: step 443, loss 0.38969, acc 0.925781, prec 0.0521436, recall 0.847304
2017-12-10T00:40:23.208175: step 444, loss 0.434175, acc 0.917969, prec 0.0523262, recall 0.847883
2017-12-10T00:40:24.117541: step 445, loss 0.193021, acc 0.933594, prec 0.0524307, recall 0.848238
2017-12-10T00:40:25.011458: step 446, loss 0.744558, acc 0.939453, prec 0.0525217, recall 0.8483
2017-12-10T00:40:25.924770: step 447, loss 0.642061, acc 0.935547, prec 0.0526108, recall 0.848362
2017-12-10T00:40:26.834546: step 448, loss 0.227883, acc 0.921875, prec 0.0526581, recall 0.848581
2017-12-10T00:40:27.743863: step 449, loss 0.409327, acc 0.933594, prec 0.0527799, recall 0.84873
2017-12-10T00:40:28.642861: step 450, loss 0.449935, acc 0.935547, prec 0.0528176, recall 0.84866
2017-12-10T00:40:29.564948: step 451, loss 0.762496, acc 0.912109, prec 0.0528269, recall 0.848546
2017-12-10T00:40:30.496209: step 452, loss 0.298044, acc 0.917969, prec 0.0529398, recall 0.848937
2017-12-10T00:40:31.405562: step 453, loss 0.324073, acc 0.914062, prec 0.0530337, recall 0.849284
2017-12-10T00:40:32.334044: step 454, loss 0.209267, acc 0.923828, prec 0.0531152, recall 0.849585
2017-12-10T00:40:33.251988: step 455, loss 0.201855, acc 0.925781, prec 0.0531976, recall 0.849886
2017-12-10T00:40:34.159937: step 456, loss 0.825025, acc 0.919922, prec 0.0533117, recall 0.850028
2017-12-10T00:40:35.071017: step 457, loss 0.501496, acc 0.892578, prec 0.053311, recall 0.849915
2017-12-10T00:40:36.002696: step 458, loss 0.483206, acc 0.912109, prec 0.0533198, recall 0.849801
2017-12-10T00:40:36.918526: step 459, loss 0.350968, acc 0.904297, prec 0.0534249, recall 0.850184
2017-12-10T00:40:37.842077: step 460, loss 0.333011, acc 0.888672, prec 0.0534718, recall 0.850438
2017-12-10T00:40:38.752073: step 461, loss 0.272565, acc 0.912109, prec 0.0535467, recall 0.850734
2017-12-10T00:40:39.674742: step 462, loss 0.338484, acc 0.873047, prec 0.0536864, recall 0.851237
2017-12-10T00:40:40.607001: step 463, loss 0.227025, acc 0.923828, prec 0.0537164, recall 0.851404
2017-12-10T00:40:41.518536: step 464, loss 0.26541, acc 0.917969, prec 0.0537434, recall 0.851571
2017-12-10T00:40:42.419506: step 465, loss 0.593223, acc 0.894531, prec 0.05376, recall 0.851499
2017-12-10T00:40:43.320653: step 466, loss 0.625366, acc 0.925781, prec 0.0539098, recall 0.85148
2017-12-10T00:40:44.222015: step 467, loss 0.383467, acc 0.929688, prec 0.0539591, recall 0.851687
2017-12-10T00:40:45.135577: step 468, loss 0.794109, acc 0.914062, prec 0.0541017, recall 0.851903
2017-12-10T00:40:46.045447: step 469, loss 0.259968, acc 0.9375, prec 0.0541546, recall 0.852109
2017-12-10T00:40:46.969985: step 470, loss 0.485859, acc 0.912109, prec 0.054196, recall 0.852078
2017-12-10T00:40:47.881750: step 471, loss 0.410487, acc 0.919922, prec 0.0542077, recall 0.851965
2017-12-10T00:40:48.794630: step 472, loss 0.2419, acc 0.925781, prec 0.054288, recall 0.852251
2017-12-10T00:40:49.697632: step 473, loss 0.899234, acc 0.884766, prec 0.0543665, recall 0.852107
2017-12-10T00:40:50.604150: step 474, loss 0.862064, acc 0.880859, prec 0.0544089, recall 0.852117
2017-12-10T00:40:51.526450: step 475, loss 0.463641, acc 0.865234, prec 0.0544425, recall 0.85236
2017-12-10T00:40:52.435299: step 476, loss 0.441632, acc 0.835938, prec 0.0544121, recall 0.852481
2017-12-10T00:40:53.345898: step 477, loss 0.366362, acc 0.863281, prec 0.0544447, recall 0.852724
2017-12-10T00:40:54.253125: step 478, loss 0.4217, acc 0.839844, prec 0.0544988, recall 0.853046
2017-12-10T00:40:55.161613: step 479, loss 0.48749, acc 0.859375, prec 0.0545128, recall 0.853246
2017-12-10T00:40:56.070798: step 480, loss 0.449587, acc 0.888672, prec 0.0546398, recall 0.853685
2017-12-10T00:40:56.981687: step 481, loss 0.348202, acc 0.890625, prec 0.0546852, recall 0.853923
2017-12-10T00:40:57.885824: step 482, loss 0.360889, acc 0.880859, prec 0.0547422, recall 0.854201
2017-12-10T00:40:58.796862: step 483, loss 0.558662, acc 0.90625, prec 0.0548606, recall 0.854595
2017-12-10T00:40:59.710742: step 484, loss 0.237771, acc 0.919922, prec 0.0549199, recall 0.85483
2017-12-10T00:41:00.634269: step 485, loss 0.271037, acc 0.931641, prec 0.0549685, recall 0.855026
2017-12-10T00:41:01.541987: step 486, loss 0.514289, acc 0.931641, prec 0.0550988, recall 0.855415
2017-12-10T00:41:02.446787: step 487, loss 0.28193, acc 0.933594, prec 0.0551317, recall 0.85557
2017-12-10T00:41:03.345360: step 488, loss 0.255233, acc 0.933594, prec 0.055181, recall 0.855764
2017-12-10T00:41:04.250392: step 489, loss 0.324869, acc 0.962891, prec 0.0552772, recall 0.856034
2017-12-10T00:41:05.159938: step 490, loss 0.224004, acc 0.951172, prec 0.0553186, recall 0.856188
2017-12-10T00:41:06.089530: step 491, loss 0.807109, acc 0.925781, prec 0.0553504, recall 0.855656
2017-12-10T00:41:07.002941: step 492, loss 0.471013, acc 0.925781, prec 0.0554128, recall 0.855659
2017-12-10T00:41:07.915045: step 493, loss 0.564604, acc 0.951172, prec 0.0555201, recall 0.855739
2017-12-10T00:41:08.815556: step 494, loss 0.251632, acc 0.925781, prec 0.0556627, recall 0.856159
2017-12-10T00:41:09.734869: step 495, loss 0.522345, acc 0.90625, prec 0.0557467, recall 0.856463
2017-12-10T00:41:10.641794: step 496, loss 0.559526, acc 0.890625, prec 0.0558878, recall 0.856917
2017-12-10T00:41:11.553352: step 497, loss 0.499243, acc 0.880859, prec 0.0560571, recall 0.857218
2017-12-10T00:41:12.461822: step 498, loss 0.464322, acc 0.861328, prec 0.0561669, recall 0.857629
2017-12-10T00:41:13.365100: step 499, loss 0.455538, acc 0.826172, prec 0.0561782, recall 0.857852
2017-12-10T00:41:14.268555: step 500, loss 0.478837, acc 0.847656, prec 0.0562485, recall 0.858186
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_3/1512884012/checkpoints/model-500

2017-12-10T00:41:16.287616: step 501, loss 0.550004, acc 0.837891, prec 0.0563138, recall 0.858518
2017-12-10T00:41:17.196274: step 502, loss 0.405712, acc 0.875, prec 0.0563327, recall 0.858701
2017-12-10T00:41:18.108284: step 503, loss 0.448796, acc 0.875, prec 0.0563999, recall 0.858994
2017-12-10T00:41:18.295205: step 504, loss 0.247307, acc 0.942308, prec 0.0564131, recall 0.859031
2017-12-10T00:41:19.198218: step 505, loss 0.30942, acc 0.869141, prec 0.0563809, recall 0.859104
2017-12-10T00:41:20.104693: step 506, loss 0.544496, acc 0.896484, prec 0.0564112, recall 0.859064
2017-12-10T00:41:21.039629: step 507, loss 0.277318, acc 0.90625, prec 0.0565414, recall 0.859464
2017-12-10T00:41:21.949209: step 508, loss 0.321934, acc 0.921875, prec 0.0566469, recall 0.859789
2017-12-10T00:41:22.854239: step 509, loss 0.489049, acc 0.941406, prec 0.056715, recall 0.859784
2017-12-10T00:41:23.762239: step 510, loss 0.431431, acc 0.949219, prec 0.0568657, recall 0.860179
2017-12-10T00:41:24.670526: step 511, loss 0.345186, acc 0.939453, prec 0.0569795, recall 0.860501
2017-12-10T00:41:25.577764: step 512, loss 0.228543, acc 0.929688, prec 0.0570564, recall 0.86075
2017-12-10T00:41:26.481314: step 513, loss 0.494447, acc 0.947266, prec 0.0571747, recall 0.86085
2017-12-10T00:41:27.383610: step 514, loss 0.225844, acc 0.951172, prec 0.0572779, recall 0.861132
2017-12-10T00:41:28.284492: step 515, loss 0.354528, acc 0.912109, prec 0.0573776, recall 0.861449
2017-12-10T00:41:29.201362: step 516, loss 0.227956, acc 0.914062, prec 0.0574304, recall 0.861659
2017-12-10T00:41:30.114211: step 517, loss 0.355689, acc 0.90625, prec 0.0575109, recall 0.861938
2017-12-10T00:41:31.029544: step 518, loss 0.476075, acc 0.902344, prec 0.0576538, recall 0.862138
2017-12-10T00:41:31.951729: step 519, loss 0.386915, acc 0.908203, prec 0.0577033, recall 0.862346
2017-12-10T00:41:32.861388: step 520, loss 0.717056, acc 0.921875, prec 0.057713, recall 0.862233
2017-12-10T00:41:33.771709: step 521, loss 0.237382, acc 0.916016, prec 0.0577188, recall 0.862337
2017-12-10T00:41:34.677490: step 522, loss 0.453942, acc 0.869141, prec 0.0578119, recall 0.862681
2017-12-10T00:41:35.586108: step 523, loss 0.416416, acc 0.867188, prec 0.0578565, recall 0.862921
2017-12-10T00:41:36.501883: step 524, loss 0.354492, acc 0.912109, prec 0.0579233, recall 0.86316
2017-12-10T00:41:37.415193: step 525, loss 0.643185, acc 0.880859, prec 0.0578967, recall 0.863014
2017-12-10T00:41:38.335663: step 526, loss 0.541018, acc 0.902344, prec 0.0580371, recall 0.863422
2017-12-10T00:41:39.234630: step 527, loss 0.365383, acc 0.910156, prec 0.0581182, recall 0.863693
2017-12-10T00:41:40.142275: step 528, loss 0.33313, acc 0.898438, prec 0.0582248, recall 0.86403
2017-12-10T00:41:41.068730: step 529, loss 0.277765, acc 0.90625, prec 0.0582723, recall 0.864231
2017-12-10T00:41:41.971413: step 530, loss 0.196652, acc 0.917969, prec 0.0582942, recall 0.864365
2017-12-10T00:41:42.870531: step 531, loss 0.479993, acc 0.902344, prec 0.0583407, recall 0.864353
2017-12-10T00:41:43.782600: step 532, loss 0.276271, acc 0.9375, prec 0.0584973, recall 0.864752
2017-12-10T00:41:44.684695: step 533, loss 0.343369, acc 0.923828, prec 0.0585376, recall 0.864918
2017-12-10T00:41:45.587273: step 534, loss 0.221766, acc 0.9375, prec 0.0586158, recall 0.865149
2017-12-10T00:41:46.492585: step 535, loss 0.302681, acc 0.929688, prec 0.0587056, recall 0.865413
2017-12-10T00:41:47.407365: step 536, loss 0.494559, acc 0.939453, prec 0.0588167, recall 0.865497
2017-12-10T00:41:48.316263: step 537, loss 0.194672, acc 0.9375, prec 0.0589724, recall 0.865889
2017-12-10T00:41:49.217150: step 538, loss 0.263883, acc 0.927734, prec 0.0589986, recall 0.866019
2017-12-10T00:41:50.119994: step 539, loss 0.237438, acc 0.923828, prec 0.0590383, recall 0.866182
2017-12-10T00:41:51.048545: step 540, loss 0.163427, acc 0.941406, prec 0.0590712, recall 0.866311
2017-12-10T00:41:51.947857: step 541, loss 0.570337, acc 0.955078, prec 0.059174, recall 0.866361
2017-12-10T00:41:52.848479: step 542, loss 0.420278, acc 0.935547, prec 0.0592514, recall 0.866377
2017-12-10T00:41:53.757881: step 543, loss 0.285271, acc 0.957031, prec 0.059324, recall 0.866362
2017-12-10T00:41:54.657424: step 544, loss 0.327988, acc 0.953125, prec 0.0594555, recall 0.866683
2017-12-10T00:41:55.587638: step 545, loss 0.209673, acc 0.927734, prec 0.0595277, recall 0.866906
2017-12-10T00:41:56.482065: step 546, loss 0.243991, acc 0.921875, prec 0.0596123, recall 0.867161
2017-12-10T00:41:57.387693: step 547, loss 0.275649, acc 0.919922, prec 0.0597268, recall 0.867478
2017-12-10T00:41:58.297506: step 548, loss 0.4632, acc 0.939453, prec 0.0598364, recall 0.867556
2017-12-10T00:41:59.210902: step 549, loss 0.747243, acc 0.923828, prec 0.0598608, recall 0.867476
2017-12-10T00:42:00.143180: step 550, loss 0.167193, acc 0.941406, prec 0.059893, recall 0.867602
2017-12-10T00:42:01.065834: step 551, loss 0.360145, acc 0.921875, prec 0.0599462, recall 0.86779
2017-12-10T00:42:01.976469: step 552, loss 0.278914, acc 0.929688, prec 0.0600033, recall 0.867978
2017-12-10T00:42:02.883748: step 553, loss 0.428506, acc 0.919922, prec 0.0600707, recall 0.868197
2017-12-10T00:42:03.789441: step 554, loss 0.284149, acc 0.894531, prec 0.0600945, recall 0.868353
2017-12-10T00:42:04.690491: step 555, loss 0.284498, acc 0.90625, prec 0.060201, recall 0.868663
2017-12-10T00:42:05.589685: step 556, loss 0.252143, acc 0.916016, prec 0.0602355, recall 0.868818
2017-12-10T00:42:06.506560: step 557, loss 0.251597, acc 0.923828, prec 0.0602585, recall 0.868941
2017-12-10T00:42:07.409792: step 558, loss 0.267991, acc 0.925781, prec 0.060359, recall 0.869218
2017-12-10T00:42:08.311458: step 559, loss 0.223862, acc 0.933594, prec 0.0604328, recall 0.869433
2017-12-10T00:42:09.209808: step 560, loss 0.307742, acc 0.917969, prec 0.0604985, recall 0.869647
2017-12-10T00:42:10.113915: step 561, loss 0.26723, acc 0.9375, prec 0.0605893, recall 0.86989
2017-12-10T00:42:11.013023: step 562, loss 0.1262, acc 0.966797, prec 0.0606795, recall 0.870103
2017-12-10T00:42:11.920437: step 563, loss 0.655912, acc 0.966797, prec 0.0608479, recall 0.87006
2017-12-10T00:42:12.828376: step 564, loss 0.170632, acc 0.955078, prec 0.0609168, recall 0.870241
2017-12-10T00:42:13.744752: step 565, loss 0.372603, acc 0.953125, prec 0.0610303, recall 0.870512
2017-12-10T00:42:14.649061: step 566, loss 0.339584, acc 0.941406, prec 0.0610625, recall 0.87043
2017-12-10T00:42:14.833601: step 567, loss 0.053958, acc 1, prec 0.0610625, recall 0.87043
2017-12-10T00:42:15.741943: step 568, loss 0.177475, acc 0.949219, prec 0.0611738, recall 0.8707
2017-12-10T00:42:16.644706: step 569, loss 0.297965, acc 0.955078, prec 0.0613185, recall 0.871027
2017-12-10T00:42:17.546126: step 570, loss 0.50829, acc 0.921875, prec 0.0613862, recall 0.871035
2017-12-10T00:42:18.461901: step 571, loss 0.27824, acc 0.921875, prec 0.0614224, recall 0.871183
2017-12-10T00:42:19.371109: step 572, loss 0.224712, acc 0.927734, prec 0.0614616, recall 0.87133
2017-12-10T00:42:20.273894: step 573, loss 0.249199, acc 0.923828, prec 0.0614835, recall 0.871448
2017-12-10T00:42:21.199756: step 574, loss 0.479691, acc 0.90625, prec 0.0615581, recall 0.871484
2017-12-10T00:42:22.113583: step 575, loss 0.210291, acc 0.929688, prec 0.0615981, recall 0.871631
2017-12-10T00:42:23.013360: step 576, loss 0.322158, acc 0.939453, prec 0.0616581, recall 0.871807
2017-12-10T00:42:23.921153: step 577, loss 0.243692, acc 0.916016, prec 0.0617364, recall 0.87204
2017-12-10T00:42:24.829345: step 578, loss 0.310179, acc 0.927734, prec 0.0618507, recall 0.872331
2017-12-10T00:42:25.733843: step 579, loss 0.187832, acc 0.949219, prec 0.0619003, recall 0.872476
2017-12-10T00:42:26.637718: step 580, loss 0.326355, acc 0.935547, prec 0.0620184, recall 0.872764
2017-12-10T00:42:27.545797: step 581, loss 0.199113, acc 0.951172, prec 0.0621141, recall 0.872994
2017-12-10T00:42:28.461350: step 582, loss 0.233624, acc 0.945312, prec 0.0622067, recall 0.873224
2017-12-10T00:42:29.369289: step 583, loss 0.297655, acc 0.921875, prec 0.0622872, recall 0.873452
2017-12-10T00:42:30.282576: step 584, loss 0.17147, acc 0.962891, prec 0.0623736, recall 0.873651
2017-12-10T00:42:31.211905: step 585, loss 0.224169, acc 0.962891, prec 0.0624448, recall 0.873821
2017-12-10T00:42:32.128093: step 586, loss 0.203301, acc 0.9375, prec 0.062503, recall 0.873991
2017-12-10T00:42:33.031493: step 587, loss 0.222211, acc 0.957031, prec 0.0625711, recall 0.87416
2017-12-10T00:42:33.931782: step 588, loss 0.323781, acc 0.945312, prec 0.0626041, recall 0.874077
2017-12-10T00:42:34.834223: step 589, loss 0.199679, acc 0.935547, prec 0.0626311, recall 0.87419
2017-12-10T00:42:35.741925: step 590, loss 0.241305, acc 0.955078, prec 0.062743, recall 0.874442
2017-12-10T00:42:36.660814: step 591, loss 0.269774, acc 0.955078, prec 0.0628099, recall 0.87461
2017-12-10T00:42:37.570384: step 592, loss 0.121533, acc 0.960938, prec 0.0628647, recall 0.87475
2017-12-10T00:42:38.473561: step 593, loss 0.182225, acc 0.960938, prec 0.0630094, recall 0.875055
2017-12-10T00:42:39.392100: step 594, loss 0.211197, acc 0.953125, prec 0.0631049, recall 0.875277
2017-12-10T00:42:40.311000: step 595, loss 0.538039, acc 0.957031, prec 0.0632772, recall 0.875635
2017-12-10T00:42:41.226267: step 596, loss 0.462189, acc 0.947266, prec 0.0633705, recall 0.875661
2017-12-10T00:42:42.129163: step 597, loss 0.401143, acc 0.945312, prec 0.063403, recall 0.875578
2017-12-10T00:42:43.029691: step 598, loss 0.433443, acc 0.943359, prec 0.063508, recall 0.875824
2017-12-10T00:42:43.924447: step 599, loss 0.392691, acc 0.931641, prec 0.0635919, recall 0.876042
2017-12-10T00:42:44.825617: step 600, loss 0.374902, acc 0.945312, prec 0.0636977, recall 0.876286

Evaluation:
2017-12-10T00:42:49.611599: step 600, loss 1.51603, acc 0.898754, prec 0.0640524, recall 0.867217

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_512_fold_3/1512884012/checkpoints/model-600

2017-12-10T00:42:52.007816: step 601, loss 0.351516, acc 0.908203, prec 0.0641663, recall 0.867526
2017-12-10T00:42:52.912735: step 602, loss 0.307898, acc 0.896484, prec 0.0642301, recall 0.867749
2017-12-10T00:42:53.824007: step 603, loss 0.461353, acc 0.857422, prec 0.0642591, recall 0.867944
2017-12-10T00:42:54.734459: step 604, loss 0.424869, acc 0.851562, prec 0.0643579, recall 0.868277
2017-12-10T00:42:55.650953: step 605, loss 0.424308, acc 0.871094, prec 0.0644083, recall 0.868498
2017-12-10T00:42:56.573150: step 606, loss 0.373061, acc 0.888672, prec 0.064453, recall 0.868691
2017-12-10T00:42:57.485444: step 607, loss 0.353397, acc 0.888672, prec 0.064454, recall 0.868801
2017-12-10T00:42:58.389587: step 608, loss 0.261602, acc 0.900391, prec 0.0644756, recall 0.868938
2017-12-10T00:42:59.303558: step 609, loss 0.270854, acc 0.910156, prec 0.0645891, recall 0.869239
2017-12-10T00:43:00.216487: step 610, loss 0.238146, acc 0.923828, prec 0.0646225, recall 0.869375
2017-12-10T00:43:01.132715: step 611, loss 0.257252, acc 0.9375, prec 0.0646919, recall 0.869565
2017-12-10T00:43:02.039489: step 612, loss 0.455522, acc 0.941406, prec 0.0648354, recall 0.86989
2017-12-10T00:43:02.950435: step 613, loss 0.283059, acc 0.943359, prec 0.0649076, recall 0.870079
2017-12-10T00:43:03.864090: step 614, loss 0.334765, acc 0.958984, prec 0.0650165, recall 0.870321
2017-12-10T00:43:04.770121: step 615, loss 0.636635, acc 0.951172, prec 0.0650502, recall 0.870248
2017-12-10T00:43:05.682402: step 616, loss 0.255516, acc 0.957031, prec 0.065158, recall 0.870489
2017-12-10T00:43:06.592832: step 617, loss 0.186246, acc 0.947266, prec 0.065203, recall 0.870622
2017-12-10T00:43:07.500378: step 618, loss 0.205972, acc 0.935547, prec 0.0652563, recall 0.870782
2017-12-10T00:43:08.416169: step 619, loss 0.237234, acc 0.925781, prec 0.0652901, recall 0.870915
2017-12-10T00:43:09.339516: step 620, loss 0.199715, acc 0.943359, prec 0.0653473, recall 0.871074
2017-12-10T00:43:10.245552: step 621, loss 0.201189, acc 0.951172, prec 0.0654372, recall 0.871285
2017-12-10T00:43:11.154775: step 622, loss 0.66926, acc 0.931641, prec 0.0655324, recall 0.871344
2017-12-10T00:43:12.067700: step 623, loss 0.200033, acc 0.943359, prec 0.0655607, recall 0.871449
2017-12-10T00:43:12.977332: step 624, loss 0.193297, acc 0.945312, prec 0.0656617, recall 0.871685
2017-12-10T00:43:13.888526: step 625, loss 0.281443, acc 0.921875, prec 0.0656931, recall 0.871816
2017-12-10T00:43:14.793112: step 626, loss 0.305615, acc 0.933594, prec 0.0657735, recall 0.872024
2017-12-10T00:43:15.693102: step 627, loss 0.404858, acc 0.916016, prec 0.0658304, recall 0.872206
2017-12-10T00:43:16.598303: step 628, loss 0.252549, acc 0.931641, prec 0.065924, recall 0.87244
2017-12-10T00:43:17.498567: step 629, loss 0.351929, acc 0.935547, prec 0.0660194, recall 0.872672
2017-12-10T00:43:17.685288: step 630, loss 0.0692179, acc 0.980769, prec 0.0660184, recall 0.872672
Training finished
Starting Experiment - batch_size_1024 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 1024
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_1024_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_0/1512884598

Start training
2017-12-10T00:43:23.022119: step 1, loss 6.96493, acc 0.81543, prec 0.0112994, recall 0.125
2017-12-10T00:43:24.751750: step 2, loss 7.77306, acc 0.526367, prec 0.0121581, recall 0.25
2017-12-10T00:43:26.470451: step 3, loss 6.05348, acc 0.229492, prec 0.0110269, recall 0.363636
2017-12-10T00:43:28.168661: step 4, loss 6.06498, acc 0.15625, prec 0.0128811, recall 0.517241
2017-12-10T00:43:29.878954: step 5, loss 6.63483, acc 0.146484, prec 0.0133873, recall 0.573333
2017-12-10T00:43:31.601082: step 6, loss 6.16536, acc 0.233398, prec 0.0132335, recall 0.609195
2017-12-10T00:43:33.306269: step 7, loss 4.87952, acc 0.324219, prec 0.0129787, recall 0.61
2017-12-10T00:43:34.999742: step 8, loss 2.93928, acc 0.461914, prec 0.0138783, recall 0.634783
2017-12-10T00:43:36.737031: step 9, loss 5.63271, acc 0.62793, prec 0.0147111, recall 0.619403
2017-12-10T00:43:38.462204: step 10, loss 3.3219, acc 0.669922, prec 0.0152123, recall 0.614865
2017-12-10T00:43:40.162415: step 11, loss 4.09453, acc 0.700195, prec 0.0158957, recall 0.609756
2017-12-10T00:43:41.881562: step 12, loss 3.07733, acc 0.736328, prec 0.016161, recall 0.595506
2017-12-10T00:43:43.605505: step 13, loss 6.60448, acc 0.700195, prec 0.0157503, recall 0.565445
2017-12-10T00:43:45.329116: step 14, loss 3.11902, acc 0.611328, prec 0.0158511, recall 0.560976
2017-12-10T00:43:47.055343: step 15, loss 4.20466, acc 0.564453, prec 0.0163509, recall 0.567568
2017-12-10T00:43:48.761172: step 16, loss 4.19223, acc 0.467773, prec 0.0169533, recall 0.576132
2017-12-10T00:43:50.472489: step 17, loss 3.69656, acc 0.442383, prec 0.0172004, recall 0.586873
2017-12-10T00:43:52.201088: step 18, loss 4.19066, acc 0.363281, prec 0.0174718, recall 0.603636
2017-12-10T00:43:53.904602: step 19, loss 3.89651, acc 0.351562, prec 0.0168175, recall 0.606383
2017-12-10T00:43:55.611399: step 20, loss 4.16717, acc 0.366211, prec 0.0169022, recall 0.618243
2017-12-10T00:43:57.330963: step 21, loss 3.1934, acc 0.407227, prec 0.0172941, recall 0.636656
2017-12-10T00:43:59.050864: step 22, loss 2.68959, acc 0.489258, prec 0.0178512, recall 0.654434
2017-12-10T00:44:00.795639: step 23, loss 4.40213, acc 0.576172, prec 0.0181057, recall 0.65407
2017-12-10T00:44:02.519137: step 24, loss 4.60954, acc 0.654297, prec 0.01792, recall 0.646893
2017-12-10T00:44:04.222314: step 25, loss 1.87524, acc 0.71582, prec 0.0180511, recall 0.648352
2017-12-10T00:44:05.950982: step 26, loss 3.76926, acc 0.727539, prec 0.0180497, recall 0.642667
2017-12-10T00:44:07.659699: step 27, loss 4.18867, acc 0.74707, prec 0.0182913, recall 0.640103
2017-12-10T00:44:09.378071: step 28, loss 3.28551, acc 0.723633, prec 0.0187037, recall 0.640394
2017-12-10T00:44:11.090668: step 29, loss 3.04466, acc 0.688477, prec 0.0187034, recall 0.639423
2017-12-10T00:44:12.810663: step 30, loss 4.89776, acc 0.712891, prec 0.0190798, recall 0.635321
2017-12-10T00:44:14.517389: step 31, loss 3.2751, acc 0.613281, prec 0.0194988, recall 0.640969
2017-12-10T00:44:14.701295: step 32, loss 1.96512, acc 0.568627, prec 0.0194701, recall 0.640969
2017-12-10T00:44:16.416139: step 33, loss 4.26618, acc 0.575195, prec 0.0195607, recall 0.64454
2017-12-10T00:44:18.114681: step 34, loss 2.96629, acc 0.505859, prec 0.0198667, recall 0.651546
2017-12-10T00:44:19.820855: step 35, loss 2.79677, acc 0.53125, prec 0.0201244, recall 0.658683
2017-12-10T00:44:21.566045: step 36, loss 3.15222, acc 0.512695, prec 0.0202283, recall 0.664078
2017-12-10T00:44:23.273727: step 37, loss 2.88831, acc 0.519531, prec 0.0202792, recall 0.667297
2017-12-10T00:44:24.985754: step 38, loss 1.96911, acc 0.605469, prec 0.0203154, recall 0.671614
2017-12-10T00:44:26.692952: step 39, loss 2.26272, acc 0.620117, prec 0.0204765, recall 0.674503
2017-12-10T00:44:28.399258: step 40, loss 2.6677, acc 0.682617, prec 0.0204907, recall 0.674956
2017-12-10T00:44:30.105235: step 41, loss 2.38339, acc 0.6875, prec 0.0207715, recall 0.675862
2017-12-10T00:44:31.813007: step 42, loss 1.91922, acc 0.720703, prec 0.0207703, recall 0.676871
2017-12-10T00:44:33.536330: step 43, loss 2.1077, acc 0.727539, prec 0.0210808, recall 0.678808
2017-12-10T00:44:35.262077: step 44, loss 2.35848, acc 0.758789, prec 0.0212172, recall 0.679675
2017-12-10T00:44:36.986046: step 45, loss 3.47461, acc 0.767578, prec 0.0216563, recall 0.681388
2017-12-10T00:44:38.703256: step 46, loss 2.67271, acc 0.771484, prec 0.0217973, recall 0.681115
2017-12-10T00:44:40.416290: step 47, loss 2.03651, acc 0.738281, prec 0.0217529, recall 0.68147
2017-12-10T00:44:42.112280: step 48, loss 2.02027, acc 0.705078, prec 0.0218156, recall 0.683258
2017-12-10T00:44:43.831053: step 49, loss 2.55022, acc 0.717773, prec 0.0221231, recall 0.686303
2017-12-10T00:44:45.550354: step 50, loss 2.89674, acc 0.701172, prec 0.0223605, recall 0.686782
2017-12-10T00:44:47.254131: step 51, loss 1.65437, acc 0.697266, prec 0.0224476, recall 0.689802
2017-12-10T00:44:48.973301: step 52, loss 1.79414, acc 0.655273, prec 0.0225768, recall 0.693593
2017-12-10T00:44:50.706232: step 53, loss 1.57572, acc 0.698242, prec 0.0229649, recall 0.69932
2017-12-10T00:44:52.418732: step 54, loss 2.70495, acc 0.667969, prec 0.0231852, recall 0.699867
2017-12-10T00:44:54.135072: step 55, loss 3.30337, acc 0.679688, prec 0.0233274, recall 0.698701
2017-12-10T00:44:55.854726: step 56, loss 2.88413, acc 0.657227, prec 0.0235666, recall 0.69962
2017-12-10T00:44:57.554107: step 57, loss 1.86306, acc 0.589844, prec 0.023563, recall 0.701623
2017-12-10T00:44:59.259693: step 58, loss 3.63872, acc 0.592773, prec 0.0235643, recall 0.701841
2017-12-10T00:45:01.025853: step 59, loss 3.20813, acc 0.62793, prec 0.0240363, recall 0.705113
2017-12-10T00:45:02.751327: step 60, loss 2.50214, acc 0.574219, prec 0.0242038, recall 0.708625
2017-12-10T00:45:04.462787: step 61, loss 1.98606, acc 0.598633, prec 0.024311, recall 0.712974
2017-12-10T00:45:06.196077: step 62, loss 3.40124, acc 0.592773, prec 0.0246015, recall 0.714765
2017-12-10T00:45:07.903144: step 63, loss 2.00518, acc 0.614258, prec 0.0247148, recall 0.718062
2017-12-10T00:45:08.083858: step 64, loss 1.68259, acc 0.647059, prec 0.0246979, recall 0.718062
2017-12-10T00:45:09.795839: step 65, loss 2.04014, acc 0.665039, prec 0.0248215, recall 0.719393
2017-12-10T00:45:11.499157: step 66, loss 1.22611, acc 0.725586, prec 0.0250333, recall 0.722519
2017-12-10T00:45:13.208356: step 67, loss 2.33554, acc 0.749023, prec 0.0252298, recall 0.72298
2017-12-10T00:45:14.916337: step 68, loss 1.77028, acc 0.789062, prec 0.0254585, recall 0.724922
2017-12-10T00:45:16.623013: step 69, loss 2.64683, acc 0.800781, prec 0.0257695, recall 0.723684
2017-12-10T00:45:18.321320: step 70, loss 3.22358, acc 0.787109, prec 0.0259225, recall 0.722112
2017-12-10T00:45:20.021076: step 71, loss 1.25368, acc 0.822266, prec 0.0262401, recall 0.725221
2017-12-10T00:45:21.776185: step 72, loss 1.04387, acc 0.775391, prec 0.0265094, recall 0.727536
2017-12-10T00:45:23.473077: step 73, loss 1.8621, acc 0.75, prec 0.0265811, recall 0.727794
2017-12-10T00:45:25.180161: step 74, loss 2.43902, acc 0.739258, prec 0.0267095, recall 0.727872
2017-12-10T00:45:26.909676: step 75, loss 1.79873, acc 0.743164, prec 0.0270714, recall 0.730379
2017-12-10T00:45:28.616244: step 76, loss 1.53241, acc 0.703125, prec 0.0270261, recall 0.730101
2017-12-10T00:45:30.334959: step 77, loss 1.72373, acc 0.682617, prec 0.0271257, recall 0.731047
2017-12-10T00:45:32.054174: step 78, loss 1.85289, acc 0.663086, prec 0.0271719, recall 0.732382
2017-12-10T00:45:33.759741: step 79, loss 2.99686, acc 0.65918, prec 0.027279, recall 0.732865
2017-12-10T00:45:35.483852: step 80, loss 1.30998, acc 0.658203, prec 0.0273162, recall 0.735422
2017-12-10T00:45:37.207659: step 81, loss 1.46234, acc 0.649414, prec 0.0272213, recall 0.736387
2017-12-10T00:45:38.928105: step 82, loss 1.5333, acc 0.670898, prec 0.0271475, recall 0.737339
2017-12-10T00:45:40.638861: step 83, loss 1.0374, acc 0.739258, prec 0.0273166, recall 0.740238
2017-12-10T00:45:42.347214: step 84, loss 2.02354, acc 0.765625, prec 0.0275395, recall 0.740803
2017-12-10T00:45:44.050189: step 85, loss 2.1811, acc 0.791016, prec 0.027662, recall 0.739884
2017-12-10T00:45:45.764452: step 86, loss 1.99298, acc 0.768555, prec 0.0277642, recall 0.738386
2017-12-10T00:45:47.462325: step 87, loss 1.89298, acc 0.786133, prec 0.0278775, recall 0.73871
2017-12-10T00:45:49.177752: step 88, loss 2.17282, acc 0.751953, prec 0.0281367, recall 0.739683
2017-12-10T00:45:50.888552: step 89, loss 1.03621, acc 0.722656, prec 0.0281012, recall 0.741121
2017-12-10T00:45:52.603118: step 90, loss 1.69623, acc 0.726562, prec 0.0283036, recall 0.741835
2017-12-10T00:45:54.303067: step 91, loss 1.89267, acc 0.680664, prec 0.0284615, recall 0.743668
2017-12-10T00:45:56.021282: step 92, loss 1.84342, acc 0.707031, prec 0.0286388, recall 0.745455
2017-12-10T00:45:57.770388: step 93, loss 1.21036, acc 0.69043, prec 0.0286011, recall 0.746988
2017-12-10T00:45:59.482697: step 94, loss 2.89775, acc 0.734375, prec 0.0287725, recall 0.746291
2017-12-10T00:46:01.223174: step 95, loss 1.77525, acc 0.696289, prec 0.0287966, recall 0.74651
2017-12-10T00:46:01.404374: step 96, loss 0.618423, acc 0.784314, prec 0.0288151, recall 0.746696
2017-12-10T00:46:03.118590: step 97, loss 1.59451, acc 0.68457, prec 0.0288273, recall 0.747997
2017-12-10T00:46:04.820289: step 98, loss 1.42714, acc 0.686523, prec 0.0289226, recall 0.74928
2017-12-10T00:46:06.545948: step 99, loss 2.19219, acc 0.727539, prec 0.0290255, recall 0.748754
2017-12-10T00:46:08.242214: step 100, loss 1.86696, acc 0.716797, prec 0.0290093, recall 0.749117
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_0/1512884598/checkpoints/model-100

2017-12-10T00:46:11.242250: step 101, loss 1.2647, acc 0.731445, prec 0.0291634, recall 0.750524
2017-12-10T00:46:12.953380: step 102, loss 2.40008, acc 0.732422, prec 0.0293444, recall 0.750517
2017-12-10T00:46:14.668921: step 103, loss 1.54318, acc 0.731445, prec 0.0296233, recall 0.752717
2017-12-10T00:46:16.363697: step 104, loss 1.09715, acc 0.731445, prec 0.0297161, recall 0.754717
2017-12-10T00:46:18.071850: step 105, loss 1.59248, acc 0.72168, prec 0.0296991, recall 0.75502
2017-12-10T00:46:19.788744: step 106, loss 1.25268, acc 0.710938, prec 0.0296737, recall 0.755319
2017-12-10T00:46:21.510919: step 107, loss 1.34755, acc 0.738281, prec 0.0296452, recall 0.755453
2017-12-10T00:46:23.221384: step 108, loss 1.0575, acc 0.764648, prec 0.0297619, recall 0.756881
2017-12-10T00:46:24.939678: step 109, loss 1.16791, acc 0.78418, prec 0.0299173, recall 0.758442
2017-12-10T00:46:26.636243: step 110, loss 1.83266, acc 0.783203, prec 0.0302452, recall 0.759591
2017-12-10T00:46:28.350952: step 111, loss 1.90752, acc 0.780273, prec 0.0303721, recall 0.758533
2017-12-10T00:46:30.076370: step 112, loss 1.7972, acc 0.798828, prec 0.0304373, recall 0.757994
2017-12-10T00:46:31.798579: step 113, loss 1.29008, acc 0.773438, prec 0.0305775, recall 0.759006
2017-12-10T00:46:33.503634: step 114, loss 1.62927, acc 0.737305, prec 0.0308805, recall 0.761176
2017-12-10T00:46:35.197129: step 115, loss 1.27171, acc 0.716797, prec 0.0308762, recall 0.761557
2017-12-10T00:46:36.912731: step 116, loss 1.59331, acc 0.673828, prec 0.031004, recall 0.763396
2017-12-10T00:46:38.630563: step 117, loss 1.37472, acc 0.681641, prec 0.0310651, recall 0.764776
2017-12-10T00:46:40.349783: step 118, loss 2.07968, acc 0.702148, prec 0.031165, recall 0.765819
2017-12-10T00:46:42.070590: step 119, loss 1.80605, acc 0.696289, prec 0.0312828, recall 0.76653
2017-12-10T00:46:43.772190: step 120, loss 1.01099, acc 0.698242, prec 0.0313522, recall 0.768293
2017-12-10T00:46:45.469775: step 121, loss 1.09596, acc 0.713867, prec 0.0314331, recall 0.769585
2017-12-10T00:46:47.177866: step 122, loss 1.12736, acc 0.743164, prec 0.0315349, recall 0.770857
2017-12-10T00:46:48.901998: step 123, loss 1.05831, acc 0.775391, prec 0.0316154, recall 0.771412
2017-12-10T00:46:50.626031: step 124, loss 0.939804, acc 0.783203, prec 0.0317233, recall 0.772088
2017-12-10T00:46:52.347465: step 125, loss 0.949742, acc 0.822266, prec 0.0318363, recall 0.773058
2017-12-10T00:46:54.051409: step 126, loss 1.38087, acc 0.829102, prec 0.0319105, recall 0.772904
2017-12-10T00:46:55.767667: step 127, loss 2.01136, acc 0.823242, prec 0.0320247, recall 0.772577
2017-12-10T00:46:55.953624: step 128, loss 0.405674, acc 0.901961, prec 0.032021, recall 0.772577
2017-12-10T00:46:57.660081: step 129, loss 2.03832, acc 0.839844, prec 0.0323013, recall 0.772703
2017-12-10T00:46:59.368337: step 130, loss 1.06232, acc 0.827148, prec 0.0323712, recall 0.772555
2017-12-10T00:47:01.094501: step 131, loss 1.22775, acc 0.842773, prec 0.0326043, recall 0.773676
2017-12-10T00:47:02.814055: step 132, loss 1.24463, acc 0.800781, prec 0.0327387, recall 0.774416
2017-12-10T00:47:04.523843: step 133, loss 1.53168, acc 0.755859, prec 0.0328597, recall 0.775263
2017-12-10T00:47:06.289132: step 134, loss 0.994892, acc 0.74707, prec 0.0329933, recall 0.776618
2017-12-10T00:47:07.989052: step 135, loss 0.932175, acc 0.726562, prec 0.0329816, recall 0.777662
2017-12-10T00:47:09.698204: step 136, loss 1.12199, acc 0.713867, prec 0.033194, recall 0.779548
2017-12-10T00:47:11.404254: step 137, loss 1.46899, acc 0.725586, prec 0.0332239, recall 0.77999
2017-12-10T00:47:13.114517: step 138, loss 0.998908, acc 0.725586, prec 0.0333773, recall 0.781883
2017-12-10T00:47:14.829265: step 139, loss 0.873772, acc 0.767578, prec 0.0335389, recall 0.783635
2017-12-10T00:47:16.536769: step 140, loss 1.21264, acc 0.822266, prec 0.0335766, recall 0.782934
2017-12-10T00:47:18.245295: step 141, loss 0.879699, acc 0.826172, prec 0.0336566, recall 0.783234
2017-12-10T00:47:19.952792: step 142, loss 1.01754, acc 0.826172, prec 0.0337983, recall 0.783465
2017-12-10T00:47:21.682365: step 143, loss 1.38711, acc 0.863281, prec 0.0339887, recall 0.782651
2017-12-10T00:47:23.381060: step 144, loss 1.31717, acc 0.832031, prec 0.0342152, recall 0.782546
2017-12-10T00:47:25.098387: step 145, loss 1.13508, acc 0.800781, prec 0.0342931, recall 0.782942
2017-12-10T00:47:26.803035: step 146, loss 1.05235, acc 0.797852, prec 0.0345289, recall 0.784528
2017-12-10T00:47:28.537851: step 147, loss 1.39889, acc 0.753906, prec 0.0345709, recall 0.784536
2017-12-10T00:47:30.268935: step 148, loss 0.923957, acc 0.737305, prec 0.0345782, recall 0.785547
2017-12-10T00:47:31.984064: step 149, loss 1.02822, acc 0.720703, prec 0.0346527, recall 0.786946
2017-12-10T00:47:33.679187: step 150, loss 0.87558, acc 0.755859, prec 0.0347911, recall 0.788524
2017-12-10T00:47:35.418236: step 151, loss 0.838492, acc 0.780273, prec 0.0348681, recall 0.789328
2017-12-10T00:47:37.133186: step 152, loss 0.737024, acc 0.765625, prec 0.0348746, recall 0.790197
2017-12-10T00:47:38.830745: step 153, loss 1.29732, acc 0.792969, prec 0.0350192, recall 0.79055
2017-12-10T00:47:40.552786: step 154, loss 1.07993, acc 0.817383, prec 0.0350441, recall 0.790592
2017-12-10T00:47:42.272226: step 155, loss 1.02502, acc 0.84082, prec 0.0350849, recall 0.790991
2017-12-10T00:47:43.983969: step 156, loss 1.16786, acc 0.84082, prec 0.0351261, recall 0.791031
2017-12-10T00:47:45.685783: step 157, loss 1.3837, acc 0.84668, prec 0.0351713, recall 0.791071
2017-12-10T00:47:47.403676: step 158, loss 1.71147, acc 0.838867, prec 0.0352876, recall 0.791131
2017-12-10T00:47:49.114062: step 159, loss 1.0283, acc 0.829102, prec 0.0353574, recall 0.791354
2017-12-10T00:47:49.300651: step 160, loss 6.70459, acc 0.784314, prec 0.0353885, recall 0.791189
2017-12-10T00:47:51.038504: step 161, loss 0.923725, acc 0.723633, prec 0.0355324, recall 0.792832
2017-12-10T00:47:52.744564: step 162, loss 1.40299, acc 0.608398, prec 0.0357053, recall 0.794983
2017-12-10T00:47:54.464374: step 163, loss 1.7638, acc 0.532227, prec 0.0356359, recall 0.796217
2017-12-10T00:47:56.170785: step 164, loss 2.01029, acc 0.495117, prec 0.035634, recall 0.797868
2017-12-10T00:47:57.874461: step 165, loss 2.05405, acc 0.516602, prec 0.0354468, recall 0.798555
2017-12-10T00:47:59.588753: step 166, loss 1.89726, acc 0.530273, prec 0.0354526, recall 0.800084
2017-12-10T00:48:01.312297: step 167, loss 1.62462, acc 0.598633, prec 0.0353792, recall 0.801008
2017-12-10T00:48:03.001519: step 168, loss 1.43783, acc 0.662109, prec 0.0353676, recall 0.80167
2017-12-10T00:48:04.701571: step 169, loss 1.02251, acc 0.723633, prec 0.0353433, recall 0.802413
2017-12-10T00:48:06.427125: step 170, loss 1.56819, acc 0.779297, prec 0.0354283, recall 0.802479
2017-12-10T00:48:08.139667: step 171, loss 0.66372, acc 0.832031, prec 0.0354928, recall 0.803292
2017-12-10T00:48:09.847294: step 172, loss 1.65443, acc 0.87793, prec 0.0356774, recall 0.803185
2017-12-10T00:48:11.555176: step 173, loss 0.716052, acc 0.882812, prec 0.0358455, recall 0.803651
2017-12-10T00:48:13.262288: step 174, loss 1.49112, acc 0.866211, prec 0.0359515, recall 0.802902
2017-12-10T00:48:14.970857: step 175, loss 1.41034, acc 0.886719, prec 0.0361053, recall 0.802321
2017-12-10T00:48:16.692875: step 176, loss 1.42269, acc 0.887695, prec 0.0362931, recall 0.802224
2017-12-10T00:48:18.395731: step 177, loss 1.35402, acc 0.860352, prec 0.036548, recall 0.802518
2017-12-10T00:48:20.115095: step 178, loss 0.976987, acc 0.837891, prec 0.0367335, recall 0.803202
2017-12-10T00:48:21.843646: step 179, loss 1.21644, acc 0.819336, prec 0.0368538, recall 0.803648
2017-12-10T00:48:23.551766: step 180, loss 1.5477, acc 0.751953, prec 0.0369459, recall 0.803854
2017-12-10T00:48:25.276163: step 181, loss 1.69169, acc 0.738281, prec 0.0368229, recall 0.803771
2017-12-10T00:48:26.981303: step 182, loss 1.53699, acc 0.717773, prec 0.0367389, recall 0.803606
2017-12-10T00:48:28.689096: step 183, loss 1.18863, acc 0.710938, prec 0.0368517, recall 0.804952
2017-12-10T00:48:30.411169: step 184, loss 1.24049, acc 0.676758, prec 0.0367905, recall 0.805619
2017-12-10T00:48:32.108487: step 185, loss 1.1716, acc 0.711914, prec 0.03677, recall 0.806049
2017-12-10T00:48:33.803493: step 186, loss 1.17923, acc 0.696289, prec 0.0367391, recall 0.80678
2017-12-10T00:48:35.504893: step 187, loss 1.25035, acc 0.723633, prec 0.0367924, recall 0.807491
2017-12-10T00:48:37.232278: step 188, loss 0.984762, acc 0.763672, prec 0.0368709, recall 0.808194
2017-12-10T00:48:38.935340: step 189, loss 0.8587, acc 0.799805, prec 0.0369718, recall 0.808889
2017-12-10T00:48:40.654012: step 190, loss 1.3858, acc 0.830078, prec 0.037012, recall 0.80833
2017-12-10T00:48:42.351169: step 191, loss 1.02143, acc 0.84668, prec 0.0370458, recall 0.808003
2017-12-10T00:48:42.538135: step 192, loss 0.420965, acc 0.862745, prec 0.0370414, recall 0.808003
2017-12-10T00:48:44.250002: step 193, loss 0.598746, acc 0.87793, prec 0.0371098, recall 0.808339
2017-12-10T00:48:45.963749: step 194, loss 1.54266, acc 0.883789, prec 0.0372642, recall 0.80814
2017-12-10T00:48:47.675348: step 195, loss 0.372364, acc 0.894531, prec 0.037374, recall 0.808903
2017-12-10T00:48:49.390064: step 196, loss 0.827185, acc 0.884766, prec 0.037558, recall 0.809712
2017-12-10T00:48:51.126523: step 197, loss 0.892854, acc 0.893555, prec 0.037684, recall 0.80966
2017-12-10T00:48:52.836808: step 198, loss 1.054, acc 0.875977, prec 0.0378296, recall 0.810032
2017-12-10T00:48:54.547803: step 199, loss 0.5658, acc 0.879883, prec 0.0378973, recall 0.810064
2017-12-10T00:48:56.257348: step 200, loss 0.637296, acc 0.868164, prec 0.0380519, recall 0.810782
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_0/1512884598/checkpoints/model-200

2017-12-10T00:48:58.889163: step 201, loss 0.812809, acc 0.864258, prec 0.0381088, recall 0.810811
2017-12-10T00:49:00.612660: step 202, loss 0.721708, acc 0.851562, prec 0.038173, recall 0.810905
2017-12-10T00:49:02.313847: step 203, loss 1.33837, acc 0.793945, prec 0.0383108, recall 0.811177
2017-12-10T00:49:04.030536: step 204, loss 1.65929, acc 0.800781, prec 0.0383887, recall 0.811464
2017-12-10T00:49:05.739490: step 205, loss 0.763134, acc 0.798828, prec 0.0383858, recall 0.811704
2017-12-10T00:49:07.444362: step 206, loss 0.689197, acc 0.787109, prec 0.0384684, recall 0.812607
2017-12-10T00:49:09.156444: step 207, loss 0.943934, acc 0.789062, prec 0.0385523, recall 0.813224
2017-12-10T00:49:10.861047: step 208, loss 0.750503, acc 0.785156, prec 0.0386169, recall 0.814048
2017-12-10T00:49:12.565843: step 209, loss 1.00376, acc 0.772461, prec 0.0386897, recall 0.814377
2017-12-10T00:49:14.261300: step 210, loss 0.625484, acc 0.816406, prec 0.0389268, recall 0.815807
2017-12-10T00:49:15.962148: step 211, loss 0.919527, acc 0.817383, prec 0.0390723, recall 0.816306
2017-12-10T00:49:17.665314: step 212, loss 0.600025, acc 0.821289, prec 0.0391578, recall 0.816827
2017-12-10T00:49:19.369294: step 213, loss 0.94733, acc 0.838867, prec 0.0392089, recall 0.816892
2017-12-10T00:49:21.091605: step 214, loss 0.860528, acc 0.87207, prec 0.0394186, recall 0.816961
2017-12-10T00:49:22.787594: step 215, loss 0.599832, acc 0.855469, prec 0.0395392, recall 0.81753
2017-12-10T00:49:24.481342: step 216, loss 0.562726, acc 0.832031, prec 0.0395682, recall 0.818064
2017-12-10T00:49:26.218574: step 217, loss 0.957344, acc 0.847656, prec 0.039669, recall 0.818035
2017-12-10T00:49:27.922464: step 218, loss 0.654098, acc 0.844727, prec 0.0397511, recall 0.818474
2017-12-10T00:49:29.623798: step 219, loss 0.753343, acc 0.836914, prec 0.0398578, recall 0.819026
2017-12-10T00:49:31.346045: step 220, loss 0.620913, acc 0.84668, prec 0.0399254, recall 0.8194
2017-12-10T00:49:33.064453: step 221, loss 1.10342, acc 0.850586, prec 0.0400105, recall 0.819568
2017-12-10T00:49:34.762988: step 222, loss 0.655398, acc 0.848633, prec 0.0400043, recall 0.819651
2017-12-10T00:49:36.478704: step 223, loss 0.968077, acc 0.845703, prec 0.0401901, recall 0.819956
2017-12-10T00:49:36.663073: step 224, loss 0.72491, acc 0.784314, prec 0.0401981, recall 0.820013
2017-12-10T00:49:38.374800: step 225, loss 0.515024, acc 0.837891, prec 0.0402135, recall 0.820465
2017-12-10T00:49:40.095044: step 226, loss 0.510169, acc 0.837891, prec 0.0402436, recall 0.82097
2017-12-10T00:49:41.800141: step 227, loss 0.57253, acc 0.849609, prec 0.0403257, recall 0.821384
2017-12-10T00:49:43.505344: step 228, loss 0.979417, acc 0.851562, prec 0.0404232, recall 0.82185
2017-12-10T00:49:45.218389: step 229, loss 0.488209, acc 0.854492, prec 0.0404045, recall 0.822126
2017-12-10T00:49:46.926975: step 230, loss 0.674457, acc 0.866211, prec 0.0405689, recall 0.822804
2017-12-10T00:49:48.627594: step 231, loss 0.653132, acc 0.870117, prec 0.0406774, recall 0.823006
2017-12-10T00:49:50.347455: step 232, loss 0.751374, acc 0.87793, prec 0.0407475, recall 0.822793
2017-12-10T00:49:52.063811: step 233, loss 0.979456, acc 0.899414, prec 0.0408894, recall 0.822546
2017-12-10T00:49:53.775436: step 234, loss 0.876289, acc 0.880859, prec 0.0411046, recall 0.823121
2017-12-10T00:49:55.513494: step 235, loss 0.495996, acc 0.882812, prec 0.0412612, recall 0.823971
2017-12-10T00:49:57.223008: step 236, loss 0.386877, acc 0.884766, prec 0.0413896, recall 0.824708
2017-12-10T00:49:58.926578: step 237, loss 0.751075, acc 0.856445, prec 0.0415868, recall 0.82526
2017-12-10T00:50:00.662162: step 238, loss 0.702515, acc 0.854492, prec 0.0416667, recall 0.825636
2017-12-10T00:50:02.390899: step 239, loss 0.563355, acc 0.831055, prec 0.0418305, recall 0.826612
2017-12-10T00:50:04.109337: step 240, loss 0.705234, acc 0.84668, prec 0.0418475, recall 0.826776
2017-12-10T00:50:05.813662: step 241, loss 0.911959, acc 0.852539, prec 0.0418254, recall 0.826788
2017-12-10T00:50:07.514119: step 242, loss 1.35123, acc 0.832031, prec 0.0418774, recall 0.826379
2017-12-10T00:50:09.223042: step 243, loss 0.944478, acc 0.837891, prec 0.0420157, recall 0.826996
2017-12-10T00:50:10.926822: step 244, loss 0.671416, acc 0.834961, prec 0.0421937, recall 0.827755
2017-12-10T00:50:12.619748: step 245, loss 0.685139, acc 0.849609, prec 0.0422816, recall 0.828161
2017-12-10T00:50:14.315355: step 246, loss 0.547401, acc 0.847656, prec 0.0423813, recall 0.828849
2017-12-10T00:50:16.027268: step 247, loss 0.779572, acc 0.8125, prec 0.0425007, recall 0.829442
2017-12-10T00:50:17.726876: step 248, loss 0.599074, acc 0.833008, prec 0.0426736, recall 0.830408
2017-12-10T00:50:19.451890: step 249, loss 0.453845, acc 0.851562, prec 0.0427462, recall 0.830982
2017-12-10T00:50:21.211367: step 250, loss 0.712628, acc 0.858398, prec 0.042768, recall 0.831129
2017-12-10T00:50:22.901229: step 251, loss 0.477262, acc 0.857422, prec 0.0428714, recall 0.831791
2017-12-10T00:50:24.590684: step 252, loss 0.759957, acc 0.855469, prec 0.0429743, recall 0.831984
2017-12-10T00:50:26.298287: step 253, loss 0.770189, acc 0.897461, prec 0.0431448, recall 0.832315
2017-12-10T00:50:27.995496: step 254, loss 0.879837, acc 0.869141, prec 0.0432547, recall 0.832734
2017-12-10T00:50:29.713615: step 255, loss 0.562541, acc 0.885742, prec 0.0433748, recall 0.83315
2017-12-10T00:50:29.894963: step 256, loss 0.177971, acc 0.960784, prec 0.0433736, recall 0.83315
2017-12-10T00:50:31.620918: step 257, loss 0.479422, acc 0.880859, prec 0.0435038, recall 0.833607
2017-12-10T00:50:33.329937: step 258, loss 0.687403, acc 0.883789, prec 0.0436217, recall 0.834016
2017-12-10T00:50:35.034690: step 259, loss 0.471213, acc 0.890625, prec 0.0437156, recall 0.834558
2017-12-10T00:50:36.759736: step 260, loss 0.603991, acc 0.873047, prec 0.0438122, recall 0.834915
2017-12-10T00:50:38.456267: step 261, loss 0.572496, acc 0.896484, prec 0.0439642, recall 0.835402
2017-12-10T00:50:40.173730: step 262, loss 0.404688, acc 0.875977, prec 0.0440747, recall 0.836021
2017-12-10T00:50:41.879781: step 263, loss 0.529378, acc 0.893555, prec 0.0442637, recall 0.836855
2017-12-10T00:50:43.581098: step 264, loss 0.418614, acc 0.865234, prec 0.0443528, recall 0.83742
2017-12-10T00:50:45.281556: step 265, loss 0.467222, acc 0.878906, prec 0.0444645, recall 0.837802
2017-12-10T00:50:46.986723: step 266, loss 0.423827, acc 0.882812, prec 0.0446179, recall 0.838531
2017-12-10T00:50:48.709834: step 267, loss 0.351284, acc 0.885742, prec 0.0447457, recall 0.839168
2017-12-10T00:50:50.429714: step 268, loss 0.495095, acc 0.90332, prec 0.0448583, recall 0.839496
2017-12-10T00:50:52.160721: step 269, loss 0.423248, acc 0.90625, prec 0.0449584, recall 0.84
2017-12-10T00:50:53.882498: step 270, loss 0.517029, acc 0.887695, prec 0.0450203, recall 0.840198
2017-12-10T00:50:55.599250: step 271, loss 0.631037, acc 0.90625, prec 0.0451211, recall 0.84026
2017-12-10T00:50:57.315984: step 272, loss 0.559054, acc 0.90625, prec 0.0453406, recall 0.840909
2017-12-10T00:50:59.032180: step 273, loss 1.01914, acc 0.90332, prec 0.0455323, recall 0.841038
2017-12-10T00:51:00.760434: step 274, loss 0.296314, acc 0.900391, prec 0.04564, recall 0.841566
2017-12-10T00:51:02.493152: step 275, loss 0.440264, acc 0.884766, prec 0.0457373, recall 0.842092
2017-12-10T00:51:04.208477: step 276, loss 0.313446, acc 0.893555, prec 0.0458004, recall 0.842494
2017-12-10T00:51:05.927877: step 277, loss 0.507166, acc 0.852539, prec 0.0459815, recall 0.843331
2017-12-10T00:51:07.619327: step 278, loss 0.674075, acc 0.866211, prec 0.0460924, recall 0.843711
2017-12-10T00:51:09.325700: step 279, loss 0.537779, acc 0.870117, prec 0.0461792, recall 0.844009
2017-12-10T00:51:11.037202: step 280, loss 0.367949, acc 0.895508, prec 0.0462161, recall 0.844322
2017-12-10T00:51:12.736695: step 281, loss 0.648691, acc 0.865234, prec 0.0463123, recall 0.844655
2017-12-10T00:51:14.468403: step 282, loss 0.747877, acc 0.875977, prec 0.0463896, recall 0.844699
2017-12-10T00:51:16.184172: step 283, loss 0.392439, acc 0.893555, prec 0.0464897, recall 0.8452
2017-12-10T00:51:17.886198: step 284, loss 0.534533, acc 0.890625, prec 0.046563, recall 0.845203
2017-12-10T00:51:19.618942: step 285, loss 0.671969, acc 0.898438, prec 0.0467324, recall 0.845264
2017-12-10T00:51:21.358638: step 286, loss 0.561126, acc 0.867188, prec 0.0467503, recall 0.845361
2017-12-10T00:51:23.048464: step 287, loss 0.386431, acc 0.886719, prec 0.0468189, recall 0.845777
2017-12-10T00:51:23.235186: step 288, loss 0.42692, acc 0.921569, prec 0.0468293, recall 0.845815
2017-12-10T00:51:24.950822: step 289, loss 0.421255, acc 0.875977, prec 0.0469422, recall 0.846379
2017-12-10T00:51:26.658534: step 290, loss 0.558804, acc 0.864258, prec 0.0469712, recall 0.846304
2017-12-10T00:51:28.357586: step 291, loss 0.456724, acc 0.893555, prec 0.0470954, recall 0.846657
2017-12-10T00:51:30.053931: step 292, loss 0.520208, acc 0.887695, prec 0.0472539, recall 0.847118
2017-12-10T00:51:31.809321: step 293, loss 0.336506, acc 0.889648, prec 0.0473614, recall 0.847633
2017-12-10T00:51:33.529691: step 294, loss 0.425182, acc 0.886719, prec 0.0474544, recall 0.847904
2017-12-10T00:51:35.250716: step 295, loss 0.50626, acc 0.90332, prec 0.0475196, recall 0.848065
2017-12-10T00:51:36.982807: step 296, loss 0.438473, acc 0.887695, prec 0.0475872, recall 0.848261
2017-12-10T00:51:38.710784: step 297, loss 0.278648, acc 0.90918, prec 0.0476807, recall 0.848694
2017-12-10T00:51:40.424129: step 298, loss 0.454943, acc 0.932617, prec 0.0478152, recall 0.848994
2017-12-10T00:51:42.139190: step 299, loss 0.299586, acc 0.920898, prec 0.0478777, recall 0.849315
2017-12-10T00:51:43.891542: step 300, loss 0.347568, acc 0.913086, prec 0.0479983, recall 0.849812

Evaluation:
2017-12-10T00:51:48.543872: step 300, loss 2.29263, acc 0.935843, prec 0.0486963, recall 0.837715

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_0/1512884598/checkpoints/model-300

2017-12-10T00:51:52.987305: step 301, loss 0.655206, acc 0.916016, prec 0.0488434, recall 0.837734
2017-12-10T00:51:54.740924: step 302, loss 0.456791, acc 0.930664, prec 0.048961, recall 0.83802
2017-12-10T00:51:56.466650: step 303, loss 0.635116, acc 0.915039, prec 0.0491054, recall 0.838413
2017-12-10T00:51:58.173916: step 304, loss 0.60401, acc 0.913086, prec 0.0492115, recall 0.838508
2017-12-10T00:51:59.889690: step 305, loss 0.450033, acc 0.911133, prec 0.049278, recall 0.838681
2017-12-10T00:52:01.614802: step 306, loss 0.370032, acc 0.869141, prec 0.0493035, recall 0.839004
2017-12-10T00:52:03.327412: step 307, loss 0.368701, acc 0.886719, prec 0.0494273, recall 0.839575
2017-12-10T00:52:05.027358: step 308, loss 0.444794, acc 0.87207, prec 0.0495169, recall 0.839885
2017-12-10T00:52:06.772146: step 309, loss 0.574468, acc 0.886719, prec 0.04969, recall 0.840405
2017-12-10T00:52:08.484104: step 310, loss 0.357933, acc 0.879883, prec 0.0497833, recall 0.840894
2017-12-10T00:52:10.237504: step 311, loss 0.547377, acc 0.870117, prec 0.0498452, recall 0.841311
2017-12-10T00:52:11.989908: step 312, loss 0.557437, acc 0.856445, prec 0.0500821, recall 0.842242
2017-12-10T00:52:13.710120: step 313, loss 0.333349, acc 0.881836, prec 0.0501877, recall 0.842755
2017-12-10T00:52:15.446377: step 314, loss 0.478051, acc 0.876953, prec 0.0502775, recall 0.84323
2017-12-10T00:52:17.148836: step 315, loss 0.598525, acc 0.881836, prec 0.0503952, recall 0.843589
2017-12-10T00:52:18.848118: step 316, loss 0.714934, acc 0.902344, prec 0.050539, recall 0.843797
2017-12-10T00:52:20.570007: step 317, loss 0.502772, acc 0.887695, prec 0.0505875, recall 0.84377
2017-12-10T00:52:22.336469: step 318, loss 0.443487, acc 0.901367, prec 0.0506679, recall 0.84417
2017-12-10T00:52:24.042761: step 319, loss 0.558298, acc 0.888672, prec 0.0507403, recall 0.844388
2017-12-10T00:52:24.225910: step 320, loss 0.42601, acc 0.862745, prec 0.0507357, recall 0.844388
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 1024
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_1024_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_1/1512885145

Start training
2017-12-10T00:52:29.739333: step 1, loss 5.02813, acc 0.525391, prec 0.0163934, recall 0.571429
2017-12-10T00:52:31.471434: step 2, loss 5.67206, acc 0.359375, prec 0.0139494, recall 0.592593
2017-12-10T00:52:33.179963: step 3, loss 4.45571, acc 0.353516, prec 0.0143409, recall 0.604651
2017-12-10T00:52:34.895018: step 4, loss 4.74498, acc 0.445312, prec 0.0150943, recall 0.610169
2017-12-10T00:52:36.665201: step 5, loss 3.54789, acc 0.506836, prec 0.0148635, recall 0.614286
2017-12-10T00:52:38.378490: step 6, loss 6.51983, acc 0.582031, prec 0.0147724, recall 0.569767
2017-12-10T00:52:40.121432: step 7, loss 5.41817, acc 0.555664, prec 0.0151033, recall 0.57
2017-12-10T00:52:41.855602: step 8, loss 6.31832, acc 0.539062, prec 0.0164551, recall 0.59322
2017-12-10T00:52:43.552766: step 9, loss 3.0454, acc 0.512695, prec 0.0168032, recall 0.615385
2017-12-10T00:52:45.249516: step 10, loss 5.38255, acc 0.523438, prec 0.0171363, recall 0.612245
2017-12-10T00:52:46.965689: step 11, loss 4.22102, acc 0.500977, prec 0.0176838, recall 0.614458
2017-12-10T00:52:48.671378: step 12, loss 2.45212, acc 0.496094, prec 0.0184185, recall 0.644444
2017-12-10T00:52:50.400601: step 13, loss 3.6216, acc 0.547852, prec 0.0190518, recall 0.658163
2017-12-10T00:52:52.126675: step 14, loss 4.06377, acc 0.540039, prec 0.0190424, recall 0.660287
2017-12-10T00:52:53.847462: step 15, loss 3.7404, acc 0.599609, prec 0.0190576, recall 0.660634
2017-12-10T00:52:55.562037: step 16, loss 2.4148, acc 0.623047, prec 0.0197344, recall 0.670886
2017-12-10T00:52:57.282442: step 17, loss 3.50917, acc 0.682617, prec 0.019802, recall 0.664
2017-12-10T00:52:58.992853: step 18, loss 3.62737, acc 0.685547, prec 0.0201011, recall 0.655431
2017-12-10T00:53:00.732697: step 19, loss 4.91, acc 0.617188, prec 0.0203297, recall 0.649123
2017-12-10T00:53:02.443974: step 20, loss 3.43205, acc 0.591797, prec 0.0201659, recall 0.648649
2017-12-10T00:53:04.164635: step 21, loss 2.47278, acc 0.556641, prec 0.0202324, recall 0.65798
2017-12-10T00:53:05.866056: step 22, loss 3.34111, acc 0.524414, prec 0.0200477, recall 0.658307
2017-12-10T00:53:07.576266: step 23, loss 4.19624, acc 0.493164, prec 0.0198218, recall 0.65861
2017-12-10T00:53:09.281086: step 24, loss 3.78466, acc 0.527344, prec 0.0201044, recall 0.661891
2017-12-10T00:53:11.000957: step 25, loss 3.4946, acc 0.527344, prec 0.0202804, recall 0.665753
2017-12-10T00:53:12.723782: step 26, loss 3.44914, acc 0.558594, prec 0.0205738, recall 0.670157
2017-12-10T00:53:14.428826: step 27, loss 3.32298, acc 0.582031, prec 0.0212634, recall 0.679901
2017-12-10T00:53:16.134473: step 28, loss 3.73261, acc 0.5625, prec 0.0215844, recall 0.682464
2017-12-10T00:53:17.820484: step 29, loss 4.00113, acc 0.564453, prec 0.0216745, recall 0.682648
2017-12-10T00:53:19.535784: step 30, loss 2.99017, acc 0.592773, prec 0.0216657, recall 0.681416
2017-12-10T00:53:21.276887: step 31, loss 2.91627, acc 0.570312, prec 0.0218207, recall 0.685225
2017-12-10T00:53:21.458196: step 32, loss 1.90599, acc 0.588235, prec 0.0217895, recall 0.685225
2017-12-10T00:53:23.164170: step 33, loss 3.21662, acc 0.629883, prec 0.0220261, recall 0.687371
2017-12-10T00:53:24.863777: step 34, loss 2.76061, acc 0.641602, prec 0.0223315, recall 0.69
2017-12-10T00:53:26.588179: step 35, loss 4.23838, acc 0.688477, prec 0.0222631, recall 0.682879
2017-12-10T00:53:28.304375: step 36, loss 3.79333, acc 0.652344, prec 0.0225088, recall 0.681051
2017-12-10T00:53:30.021273: step 37, loss 3.62152, acc 0.651367, prec 0.0226818, recall 0.68
2017-12-10T00:53:31.762351: step 38, loss 2.14239, acc 0.589844, prec 0.0228737, recall 0.683746
2017-12-10T00:53:33.474733: step 39, loss 3.36002, acc 0.556641, prec 0.0232358, recall 0.68942
2017-12-10T00:53:35.193942: step 40, loss 4.03663, acc 0.547852, prec 0.0232975, recall 0.688742
2017-12-10T00:53:36.918009: step 41, loss 2.66293, acc 0.510742, prec 0.0231418, recall 0.692182
2017-12-10T00:53:38.623260: step 42, loss 2.17048, acc 0.535156, prec 0.0234383, recall 0.700475
2017-12-10T00:53:40.349112: step 43, loss 2.99389, acc 0.574219, prec 0.0233691, recall 0.700311
2017-12-10T00:53:42.070960: step 44, loss 2.8867, acc 0.630859, prec 0.0232724, recall 0.699237
2017-12-10T00:53:43.770255: step 45, loss 2.88808, acc 0.670898, prec 0.0236158, recall 0.702823
2017-12-10T00:53:45.469361: step 46, loss 1.91299, acc 0.708008, prec 0.0237522, recall 0.704082
2017-12-10T00:53:47.173706: step 47, loss 4.13323, acc 0.739258, prec 0.0239713, recall 0.703704
2017-12-10T00:53:48.888569: step 48, loss 3.48649, acc 0.728516, prec 0.0241754, recall 0.700416
2017-12-10T00:53:50.617665: step 49, loss 2.55409, acc 0.693359, prec 0.0240981, recall 0.699042
2017-12-10T00:53:52.333304: step 50, loss 2.02262, acc 0.668945, prec 0.0245826, recall 0.703851
2017-12-10T00:53:54.040461: step 51, loss 1.89152, acc 0.644531, prec 0.0247538, recall 0.707953
2017-12-10T00:53:55.751231: step 52, loss 1.67778, acc 0.632812, prec 0.0248175, recall 0.712082
2017-12-10T00:53:57.458177: step 53, loss 2.56584, acc 0.599609, prec 0.0249736, recall 0.714465
2017-12-10T00:53:59.158565: step 54, loss 1.90961, acc 0.635742, prec 0.0249524, recall 0.715881
2017-12-10T00:54:00.891481: step 55, loss 2.6588, acc 0.658203, prec 0.025083, recall 0.716545
2017-12-10T00:54:02.604722: step 56, loss 1.78268, acc 0.679688, prec 0.0252718, recall 0.719235
2017-12-10T00:54:04.300396: step 57, loss 1.51379, acc 0.688477, prec 0.0253448, recall 0.72
2017-12-10T00:54:06.057183: step 58, loss 2.09985, acc 0.706055, prec 0.0257134, recall 0.722988
2017-12-10T00:54:07.779145: step 59, loss 1.42745, acc 0.741211, prec 0.0259914, recall 0.725734
2017-12-10T00:54:09.486905: step 60, loss 1.41254, acc 0.754883, prec 0.02624, recall 0.727273
2017-12-10T00:54:11.183957: step 61, loss 1.53937, acc 0.740234, prec 0.0261971, recall 0.727473
2017-12-10T00:54:12.901752: step 62, loss 2.53197, acc 0.760742, prec 0.0263302, recall 0.728061
2017-12-10T00:54:14.592305: step 63, loss 3.4953, acc 0.738281, prec 0.026177, recall 0.723473
2017-12-10T00:54:14.776592: step 64, loss 1.09742, acc 0.764706, prec 0.0262026, recall 0.723769
2017-12-10T00:54:16.493125: step 65, loss 1.77151, acc 0.725586, prec 0.0263733, recall 0.723449
2017-12-10T00:54:18.212167: step 66, loss 2.57035, acc 0.717773, prec 0.0264584, recall 0.722567
2017-12-10T00:54:19.920512: step 67, loss 1.71727, acc 0.688477, prec 0.0266182, recall 0.724771
2017-12-10T00:54:21.647332: step 68, loss 1.91452, acc 0.610352, prec 0.0265885, recall 0.725352
2017-12-10T00:54:23.349803: step 69, loss 1.98871, acc 0.624023, prec 0.0266439, recall 0.726462
2017-12-10T00:54:25.067907: step 70, loss 1.73529, acc 0.631836, prec 0.0265997, recall 0.727451
2017-12-10T00:54:26.777341: step 71, loss 2.142, acc 0.59375, prec 0.0266587, recall 0.728764
2017-12-10T00:54:28.478761: step 72, loss 2.31691, acc 0.601562, prec 0.0267901, recall 0.731244
2017-12-10T00:54:30.188700: step 73, loss 2.06889, acc 0.646484, prec 0.0267262, recall 0.731891
2017-12-10T00:54:31.897881: step 74, loss 1.35376, acc 0.675781, prec 0.0267884, recall 0.734637
2017-12-10T00:54:33.604160: step 75, loss 1.53832, acc 0.734375, prec 0.026938, recall 0.736213
2017-12-10T00:54:35.312559: step 76, loss 1.5225, acc 0.785156, prec 0.0271335, recall 0.736413
2017-12-10T00:54:37.034523: step 77, loss 0.91009, acc 0.777344, prec 0.0273483, recall 0.738819
2017-12-10T00:54:38.750469: step 78, loss 2.50908, acc 0.832031, prec 0.0275492, recall 0.738747
2017-12-10T00:54:40.466896: step 79, loss 2.71054, acc 0.816406, prec 0.0276406, recall 0.736063
2017-12-10T00:54:42.179233: step 80, loss 1.29569, acc 0.790039, prec 0.0276704, recall 0.736387
2017-12-10T00:54:43.882918: step 81, loss 1.49213, acc 0.772461, prec 0.0278422, recall 0.736573
2017-12-10T00:54:45.586116: step 82, loss 1.55718, acc 0.726562, prec 0.0282167, recall 0.73913
2017-12-10T00:54:47.311475: step 83, loss 1.288, acc 0.706055, prec 0.0282256, recall 0.740464
2017-12-10T00:54:48.999301: step 84, loss 1.13791, acc 0.69043, prec 0.0286143, recall 0.745114
2017-12-10T00:54:50.714366: step 85, loss 1.68143, acc 0.69043, prec 0.0285767, recall 0.744956
2017-12-10T00:54:52.443634: step 86, loss 2.0718, acc 0.709961, prec 0.0286188, recall 0.744019
2017-12-10T00:54:54.147586: step 87, loss 1.63415, acc 0.692383, prec 0.0287007, recall 0.745268
2017-12-10T00:54:55.856240: step 88, loss 1.30691, acc 0.702148, prec 0.0289925, recall 0.749029
2017-12-10T00:54:57.580665: step 89, loss 1.46212, acc 0.728516, prec 0.0292171, recall 0.750958
2017-12-10T00:54:59.299248: step 90, loss 2.62621, acc 0.729492, prec 0.02927, recall 0.749432
2017-12-10T00:55:01.060360: step 91, loss 1.44764, acc 0.726562, prec 0.02946, recall 0.75
2017-12-10T00:55:02.811756: step 92, loss 2.00024, acc 0.701172, prec 0.0295117, recall 0.749816
2017-12-10T00:55:04.515197: step 93, loss 1.75844, acc 0.682617, prec 0.0294608, recall 0.750183
2017-12-10T00:55:06.255066: step 94, loss 1.82888, acc 0.706055, prec 0.0295711, recall 0.750362
2017-12-10T00:55:07.977515: step 95, loss 2.18292, acc 0.661133, prec 0.0297214, recall 0.752143
2017-12-10T00:55:08.162860: step 96, loss 1.35734, acc 0.666667, prec 0.0297345, recall 0.75232
2017-12-10T00:55:09.879302: step 97, loss 1.47221, acc 0.681641, prec 0.0298712, recall 0.754411
2017-12-10T00:55:11.612212: step 98, loss 1.36776, acc 0.708984, prec 0.0298681, recall 0.754902
2017-12-10T00:55:13.316665: step 99, loss 1.5559, acc 0.754883, prec 0.0299579, recall 0.755201
2017-12-10T00:55:15.037350: step 100, loss 1.44814, acc 0.787109, prec 0.0302053, recall 0.756849
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_1/1512885145/checkpoints/model-100

2017-12-10T00:55:17.773441: step 101, loss 1.14847, acc 0.811523, prec 0.0303121, recall 0.757473
2017-12-10T00:55:19.465454: step 102, loss 2.02285, acc 0.813477, prec 0.0305267, recall 0.75721
2017-12-10T00:55:21.189320: step 103, loss 1.39276, acc 0.78125, prec 0.0306312, recall 0.757979
2017-12-10T00:55:22.892097: step 104, loss 1.31095, acc 0.785156, prec 0.0308428, recall 0.758372
2017-12-10T00:55:24.585573: step 105, loss 1.604, acc 0.757812, prec 0.0309516, recall 0.758285
2017-12-10T00:55:26.295549: step 106, loss 1.37971, acc 0.749023, prec 0.0309232, recall 0.757908
2017-12-10T00:55:27.984682: step 107, loss 1.29255, acc 0.733398, prec 0.0310082, recall 0.758797
2017-12-10T00:55:29.693788: step 108, loss 1.42361, acc 0.72168, prec 0.0311592, recall 0.759166
2017-12-10T00:55:31.421237: step 109, loss 1.21125, acc 0.675781, prec 0.0311423, recall 0.760678
2017-12-10T00:55:33.126848: step 110, loss 1.59507, acc 0.705078, prec 0.0313002, recall 0.761639
2017-12-10T00:55:34.857655: step 111, loss 1.06887, acc 0.711914, prec 0.0314343, recall 0.763838
2017-12-10T00:55:36.594036: step 112, loss 1.03674, acc 0.700195, prec 0.0316297, recall 0.766423
2017-12-10T00:55:38.294352: step 113, loss 0.954747, acc 0.771484, prec 0.0318799, recall 0.768953
2017-12-10T00:55:40.005670: step 114, loss 0.888096, acc 0.797852, prec 0.03203, recall 0.769827
2017-12-10T00:55:41.737642: step 115, loss 1.75463, acc 0.797852, prec 0.0322995, recall 0.770453
2017-12-10T00:55:43.457918: step 116, loss 1.02174, acc 0.830078, prec 0.0323785, recall 0.76986
2017-12-10T00:55:45.160491: step 117, loss 1.77714, acc 0.800781, prec 0.0326214, recall 0.770785
2017-12-10T00:55:46.870571: step 118, loss 1.11862, acc 0.813477, prec 0.0328251, recall 0.771429
2017-12-10T00:55:48.579250: step 119, loss 0.973147, acc 0.768555, prec 0.0328725, recall 0.77185
2017-12-10T00:55:50.308809: step 120, loss 1.21072, acc 0.75293, prec 0.0329066, recall 0.772266
2017-12-10T00:55:52.021512: step 121, loss 0.745848, acc 0.782227, prec 0.0331009, recall 0.774302
2017-12-10T00:55:53.723706: step 122, loss 0.765286, acc 0.756836, prec 0.0331347, recall 0.775556
2017-12-10T00:55:55.415139: step 123, loss 1.38966, acc 0.763672, prec 0.0332217, recall 0.775758
2017-12-10T00:55:57.137277: step 124, loss 2.09747, acc 0.780273, prec 0.0332762, recall 0.775287
2017-12-10T00:55:58.844005: step 125, loss 0.754159, acc 0.798828, prec 0.0333645, recall 0.77663
2017-12-10T00:56:00.579384: step 126, loss 1.07358, acc 0.793945, prec 0.0334278, recall 0.776579
2017-12-10T00:56:02.313612: step 127, loss 0.873386, acc 0.80957, prec 0.0335684, recall 0.777718
2017-12-10T00:56:02.495961: step 128, loss 0.677221, acc 0.72549, prec 0.0335798, recall 0.777837
2017-12-10T00:56:04.210920: step 129, loss 0.793126, acc 0.819336, prec 0.0336386, recall 0.778073
2017-12-10T00:56:05.939008: step 130, loss 2.09234, acc 0.868164, prec 0.0339158, recall 0.777603
2017-12-10T00:56:07.648124: step 131, loss 1.15333, acc 0.850586, prec 0.0340195, recall 0.777952
2017-12-10T00:56:09.360470: step 132, loss 0.749973, acc 0.838867, prec 0.0342442, recall 0.779389
2017-12-10T00:56:11.063353: step 133, loss 1.29239, acc 0.8125, prec 0.0344257, recall 0.779887
2017-12-10T00:56:12.781734: step 134, loss 0.9334, acc 0.800781, prec 0.0347034, recall 0.781726
2017-12-10T00:56:14.480604: step 135, loss 1.20658, acc 0.758789, prec 0.0348149, recall 0.782872
2017-12-10T00:56:16.177586: step 136, loss 0.824125, acc 0.738281, prec 0.0347361, recall 0.783526
2017-12-10T00:56:17.887263: step 137, loss 0.76247, acc 0.757812, prec 0.0347805, recall 0.784715
2017-12-10T00:56:19.599249: step 138, loss 0.710817, acc 0.786133, prec 0.0348891, recall 0.786104
2017-12-10T00:56:21.336451: step 139, loss 0.987134, acc 0.795898, prec 0.0350904, recall 0.787119
2017-12-10T00:56:23.036461: step 140, loss 0.825147, acc 0.800781, prec 0.0352928, recall 0.788499
2017-12-10T00:56:24.746532: step 141, loss 0.885168, acc 0.828125, prec 0.03539, recall 0.788862
2017-12-10T00:56:26.450176: step 142, loss 1.00374, acc 0.842773, prec 0.0356447, recall 0.789549
2017-12-10T00:56:28.131902: step 143, loss 1.10954, acc 0.875, prec 0.0359031, recall 0.789748
2017-12-10T00:56:29.872855: step 144, loss 1.08169, acc 0.830078, prec 0.0360597, recall 0.790763
2017-12-10T00:56:31.604859: step 145, loss 0.943429, acc 0.816406, prec 0.0361637, recall 0.791198
2017-12-10T00:56:33.332699: step 146, loss 0.812086, acc 0.794922, prec 0.0362901, recall 0.79219
2017-12-10T00:56:35.051495: step 147, loss 0.783188, acc 0.799805, prec 0.0363579, recall 0.79288
2017-12-10T00:56:36.768359: step 148, loss 1.33723, acc 0.78125, prec 0.0363705, recall 0.793008
2017-12-10T00:56:38.475768: step 149, loss 0.560181, acc 0.819336, prec 0.0363911, recall 0.793767
2017-12-10T00:56:40.180651: step 150, loss 1.06278, acc 0.799805, prec 0.03658, recall 0.794278
2017-12-10T00:56:41.894133: step 151, loss 1.18832, acc 0.818359, prec 0.0366405, recall 0.79449
2017-12-10T00:56:43.624077: step 152, loss 0.814205, acc 0.795898, prec 0.0366023, recall 0.794687
2017-12-10T00:56:45.326486: step 153, loss 0.606763, acc 0.829102, prec 0.036868, recall 0.796519
2017-12-10T00:56:47.070408: step 154, loss 0.579144, acc 0.821289, prec 0.0369669, recall 0.797603
2017-12-10T00:56:48.790835: step 155, loss 0.829468, acc 0.825195, prec 0.0371281, recall 0.79859
2017-12-10T00:56:50.506167: step 156, loss 0.629323, acc 0.854492, prec 0.0372912, recall 0.799475
2017-12-10T00:56:52.236013: step 157, loss 0.470293, acc 0.865234, prec 0.0374412, recall 0.800609
2017-12-10T00:56:53.952135: step 158, loss 1.24609, acc 0.868164, prec 0.0377316, recall 0.801293
2017-12-10T00:56:55.655959: step 159, loss 1.01836, acc 0.842773, prec 0.0378049, recall 0.801458
2017-12-10T00:56:55.834346: step 160, loss 4.891, acc 0.784314, prec 0.0378175, recall 0.800857
2017-12-10T00:56:57.549893: step 161, loss 0.871264, acc 0.732422, prec 0.0379767, recall 0.802464
2017-12-10T00:56:59.257701: step 162, loss 1.27209, acc 0.633789, prec 0.0380764, recall 0.804128
2017-12-10T00:57:00.988857: step 163, loss 1.68837, acc 0.525391, prec 0.0379579, recall 0.805195
2017-12-10T00:57:02.684649: step 164, loss 1.88132, acc 0.474609, prec 0.0379537, recall 0.806894
2017-12-10T00:57:04.389775: step 165, loss 1.98391, acc 0.457031, prec 0.0377504, recall 0.807772
2017-12-10T00:57:06.133720: step 166, loss 1.78072, acc 0.493164, prec 0.0375229, recall 0.808405
2017-12-10T00:57:07.849691: step 167, loss 1.39601, acc 0.584961, prec 0.0374578, recall 0.809426
2017-12-10T00:57:09.576458: step 168, loss 1.28749, acc 0.6875, prec 0.0375236, recall 0.810008
2017-12-10T00:57:11.286015: step 169, loss 0.820485, acc 0.746094, prec 0.0376655, recall 0.811389
2017-12-10T00:57:12.988043: step 170, loss 0.666792, acc 0.832031, prec 0.037886, recall 0.812826
2017-12-10T00:57:15.435106: step 171, loss 0.477493, acc 0.874023, prec 0.0380459, recall 0.81387
2017-12-10T00:57:17.159117: step 172, loss 1.13649, acc 0.910156, prec 0.0381275, recall 0.812847
2017-12-10T00:57:18.875253: step 173, loss 0.858, acc 0.922852, prec 0.0382158, recall 0.812796
2017-12-10T00:57:20.575243: step 174, loss 2.64211, acc 0.93457, prec 0.0384423, recall 0.811033
2017-12-10T00:57:22.330049: step 175, loss 1.08369, acc 0.925781, prec 0.0385513, recall 0.810432
2017-12-10T00:57:24.041210: step 176, loss 0.59321, acc 0.887695, prec 0.0386484, recall 0.810539
2017-12-10T00:57:25.782696: step 177, loss 1.28808, acc 0.87793, prec 0.0387394, recall 0.810019
2017-12-10T00:57:27.524955: step 178, loss 0.489281, acc 0.852539, prec 0.0388085, recall 0.810749
2017-12-10T00:57:29.240218: step 179, loss 1.21582, acc 0.823242, prec 0.0388222, recall 0.810707
2017-12-10T00:57:30.988177: step 180, loss 1.1296, acc 0.758789, prec 0.0388065, recall 0.810739
2017-12-10T00:57:32.710118: step 181, loss 0.836676, acc 0.765625, prec 0.0390037, recall 0.81224
2017-12-10T00:57:34.432434: step 182, loss 0.92531, acc 0.737305, prec 0.0391265, recall 0.813508
2017-12-10T00:57:36.177071: step 183, loss 1.49919, acc 0.745117, prec 0.0391516, recall 0.813736
2017-12-10T00:57:37.906950: step 184, loss 1.08375, acc 0.736328, prec 0.0393754, recall 0.815089
2017-12-10T00:57:39.621655: step 185, loss 1.70222, acc 0.74707, prec 0.0394026, recall 0.814406
2017-12-10T00:57:41.321564: step 186, loss 0.874135, acc 0.742188, prec 0.0394735, recall 0.815424
2017-12-10T00:57:43.032198: step 187, loss 0.828954, acc 0.775391, prec 0.0395166, recall 0.81623
2017-12-10T00:57:44.748289: step 188, loss 1.07636, acc 0.768555, prec 0.0395552, recall 0.816733
2017-12-10T00:57:46.467651: step 189, loss 0.917552, acc 0.767578, prec 0.0396591, recall 0.817789
2017-12-10T00:57:48.170695: step 190, loss 1.068, acc 0.804688, prec 0.0396736, recall 0.817497
2017-12-10T00:57:49.892408: step 191, loss 0.981371, acc 0.81543, prec 0.0397281, recall 0.81763
2017-12-10T00:57:50.075800: step 192, loss 0.839195, acc 0.72549, prec 0.0397185, recall 0.81763
2017-12-10T00:57:51.830164: step 193, loss 0.607476, acc 0.857422, prec 0.0399336, recall 0.818859
2017-12-10T00:57:53.561182: step 194, loss 0.98657, acc 0.855469, prec 0.0400655, recall 0.818887
2017-12-10T00:57:55.260217: step 195, loss 0.807757, acc 0.870117, prec 0.0402229, recall 0.819264
2017-12-10T00:57:56.966632: step 196, loss 1.60374, acc 0.874023, prec 0.0404014, recall 0.818561
2017-12-10T00:57:58.682381: step 197, loss 0.878377, acc 0.854492, prec 0.0405133, recall 0.818811
2017-12-10T00:58:00.395200: step 198, loss 0.644059, acc 0.854492, prec 0.0406074, recall 0.819277
2017-12-10T00:58:02.123915: step 199, loss 0.755772, acc 0.800781, prec 0.0407601, recall 0.82039
2017-12-10T00:58:03.828738: step 200, loss 0.820605, acc 0.774414, prec 0.0406989, recall 0.820478
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_1/1512885145/checkpoints/model-200

2017-12-10T00:58:20.170641: step 201, loss 0.661193, acc 0.782227, prec 0.0407561, recall 0.821271
2017-12-10T00:58:21.888039: step 202, loss 0.889324, acc 0.760742, prec 0.0407341, recall 0.821537
2017-12-10T00:58:23.583502: step 203, loss 0.931828, acc 0.767578, prec 0.0408293, recall 0.822222
2017-12-10T00:58:25.304928: step 204, loss 1.14237, acc 0.791992, prec 0.0408136, recall 0.822148
2017-12-10T00:58:27.015070: step 205, loss 0.849314, acc 0.821289, prec 0.0409132, recall 0.822704
2017-12-10T00:58:28.709099: step 206, loss 0.911408, acc 0.814453, prec 0.0410875, recall 0.823276
2017-12-10T00:58:30.455788: step 207, loss 0.816935, acc 0.854492, prec 0.0412245, recall 0.823879
2017-12-10T00:58:32.175147: step 208, loss 0.624702, acc 0.835938, prec 0.041269, recall 0.824187
2017-12-10T00:58:33.895874: step 209, loss 1.17921, acc 0.830078, prec 0.0413577, recall 0.824126
2017-12-10T00:58:35.620388: step 210, loss 0.786156, acc 0.792969, prec 0.0414964, recall 0.825154
2017-12-10T00:58:37.352409: step 211, loss 0.965351, acc 0.819336, prec 0.0415927, recall 0.824879
2017-12-10T00:58:39.054612: step 212, loss 0.956921, acc 0.799805, prec 0.0416268, recall 0.824968
2017-12-10T00:58:40.780444: step 213, loss 0.681216, acc 0.808594, prec 0.0417583, recall 0.82592
2017-12-10T00:58:42.491124: step 214, loss 0.614914, acc 0.821289, prec 0.0418051, recall 0.826531
2017-12-10T00:58:44.197114: step 215, loss 0.671567, acc 0.847656, prec 0.0419321, recall 0.827094
2017-12-10T00:58:45.927838: step 216, loss 0.537131, acc 0.833984, prec 0.0420175, recall 0.827804
2017-12-10T00:58:47.648269: step 217, loss 0.385415, acc 0.859375, prec 0.0421199, recall 0.828508
2017-12-10T00:58:49.353506: step 218, loss 1.11885, acc 0.848633, prec 0.0422317, recall 0.828482
2017-12-10T00:58:51.106768: step 219, loss 0.536804, acc 0.890625, prec 0.0424468, recall 0.829238
2017-12-10T00:58:52.838981: step 220, loss 0.48861, acc 0.857422, prec 0.0425765, recall 0.830031
2017-12-10T00:58:54.551658: step 221, loss 0.743658, acc 0.885742, prec 0.0426955, recall 0.830456
2017-12-10T00:58:56.250734: step 222, loss 0.742592, acc 0.878906, prec 0.042779, recall 0.830774
2017-12-10T00:58:57.952295: step 223, loss 0.963458, acc 0.894531, prec 0.0428441, recall 0.830477
2017-12-10T00:58:58.138755: step 224, loss 8.09005, acc 0.882353, prec 0.0428407, recall 0.830223
2017-12-10T00:58:59.865885: step 225, loss 0.834079, acc 0.814453, prec 0.0429994, recall 0.830952
2017-12-10T00:59:01.595737: step 226, loss 0.813738, acc 0.748047, prec 0.0430655, recall 0.83177
2017-12-10T00:59:03.305162: step 227, loss 0.88755, acc 0.711914, prec 0.0429569, recall 0.832075
2017-12-10T00:59:05.018284: step 228, loss 1.03818, acc 0.668945, prec 0.0429092, recall 0.832681
2017-12-10T00:59:06.754259: step 229, loss 1.11142, acc 0.652344, prec 0.042895, recall 0.833433
2017-12-10T00:59:08.465087: step 230, loss 1.03992, acc 0.660156, prec 0.0429156, recall 0.834277
2017-12-10T00:59:10.181194: step 231, loss 0.904612, acc 0.6875, prec 0.0429105, recall 0.834966
2017-12-10T00:59:11.881047: step 232, loss 0.862567, acc 0.734375, prec 0.0429513, recall 0.835697
2017-12-10T00:59:13.606314: step 233, loss 0.808006, acc 0.768555, prec 0.0430145, recall 0.836423
2017-12-10T00:59:15.325170: step 234, loss 0.569349, acc 0.826172, prec 0.0431155, recall 0.837141
2017-12-10T00:59:17.022244: step 235, loss 0.910688, acc 0.847656, prec 0.0431452, recall 0.83708
2017-12-10T00:59:18.740371: step 236, loss 0.566431, acc 0.892578, prec 0.0432902, recall 0.837547
2017-12-10T00:59:20.442061: step 237, loss 0.645971, acc 0.901367, prec 0.0434124, recall 0.837674
2017-12-10T00:59:22.161308: step 238, loss 0.792681, acc 0.913086, prec 0.043585, recall 0.837939
2017-12-10T00:59:23.873712: step 239, loss 1.32601, acc 0.925781, prec 0.0438241, recall 0.837907
2017-12-10T00:59:25.589500: step 240, loss 0.835425, acc 0.922852, prec 0.0440443, recall 0.838545
2017-12-10T00:59:27.342825: step 241, loss 0.529073, acc 0.914062, prec 0.0441725, recall 0.838901
2017-12-10T00:59:29.066486: step 242, loss 0.794393, acc 0.886719, prec 0.0442973, recall 0.838828
2017-12-10T00:59:30.813256: step 243, loss 1.0903, acc 0.875977, prec 0.0443435, recall 0.838529
2017-12-10T00:59:32.589761: step 244, loss 0.499167, acc 0.855469, prec 0.0444451, recall 0.838926
2017-12-10T00:59:34.307442: step 245, loss 0.874946, acc 0.833008, prec 0.0445311, recall 0.839321
2017-12-10T00:59:36.024893: step 246, loss 0.67078, acc 0.807617, prec 0.0445989, recall 0.839945
2017-12-10T00:59:37.734707: step 247, loss 0.646309, acc 0.791992, prec 0.0445997, recall 0.840387
2017-12-10T00:59:39.435026: step 248, loss 0.654317, acc 0.786133, prec 0.0446805, recall 0.841091
2017-12-10T00:59:41.142616: step 249, loss 0.717207, acc 0.797852, prec 0.0448381, recall 0.842004
2017-12-10T00:59:42.865514: step 250, loss 0.862054, acc 0.803711, prec 0.0449299, recall 0.842464
2017-12-10T00:59:44.577248: step 251, loss 0.595506, acc 0.834961, prec 0.0449583, recall 0.842892
2017-12-10T00:59:46.302449: step 252, loss 0.708601, acc 0.810547, prec 0.0449848, recall 0.843132
2017-12-10T00:59:48.044878: step 253, loss 0.588234, acc 0.84082, prec 0.0451544, recall 0.843977
2017-12-10T00:59:49.753271: step 254, loss 0.561537, acc 0.861328, prec 0.0452551, recall 0.844337
2017-12-10T00:59:51.482810: step 255, loss 0.559124, acc 0.875, prec 0.0452692, recall 0.844177
2017-12-10T00:59:51.663899: step 256, loss 0.330344, acc 0.901961, prec 0.0452797, recall 0.844218
2017-12-10T00:59:53.404915: step 257, loss 0.654351, acc 0.897461, prec 0.045336, recall 0.844142
2017-12-10T00:59:55.133678: step 258, loss 0.619776, acc 0.894531, prec 0.0454584, recall 0.844273
2017-12-10T00:59:56.829391: step 259, loss 0.764436, acc 0.900391, prec 0.0455844, recall 0.844403
2017-12-10T00:59:58.539692: step 260, loss 0.659643, acc 0.904297, prec 0.0457534, recall 0.844655
2017-12-10T01:00:00.256983: step 261, loss 1.02035, acc 0.878906, prec 0.0457969, recall 0.844357
2017-12-10T01:00:01.979693: step 262, loss 0.411718, acc 0.900391, prec 0.0459881, recall 0.845129
2017-12-10T01:00:03.694481: step 263, loss 0.698021, acc 0.876953, prec 0.0460432, recall 0.844872
2017-12-10T01:00:05.422996: step 264, loss 0.4112, acc 0.869141, prec 0.0461179, recall 0.845355
2017-12-10T01:00:07.153976: step 265, loss 0.447962, acc 0.849609, prec 0.0462331, recall 0.845995
2017-12-10T01:00:08.867740: step 266, loss 0.49019, acc 0.862305, prec 0.0463026, recall 0.846471
2017-12-10T01:00:10.576504: step 267, loss 0.542231, acc 0.868164, prec 0.0464433, recall 0.846923
2017-12-10T01:00:12.307771: step 268, loss 0.593203, acc 0.848633, prec 0.0465563, recall 0.847549
2017-12-10T01:00:14.035358: step 269, loss 0.545219, acc 0.862305, prec 0.0466913, recall 0.848207
2017-12-10T01:00:15.732927: step 270, loss 0.534406, acc 0.865234, prec 0.0468676, recall 0.848975
2017-12-10T01:00:17.447987: step 271, loss 0.575319, acc 0.876953, prec 0.0469188, recall 0.849142
2017-12-10T01:00:19.163734: step 272, loss 0.585035, acc 0.883789, prec 0.0470672, recall 0.849573
2017-12-10T01:00:20.867890: step 273, loss 0.442007, acc 0.885742, prec 0.0472024, recall 0.850175
2017-12-10T01:00:22.585366: step 274, loss 0.640837, acc 0.880859, prec 0.047361, recall 0.850635
2017-12-10T01:00:24.289291: step 275, loss 0.520363, acc 0.87793, prec 0.0474511, recall 0.850905
2017-12-10T01:00:26.020300: step 276, loss 0.646729, acc 0.887695, prec 0.0476526, recall 0.851468
2017-12-10T01:00:27.741708: step 277, loss 0.484817, acc 0.885742, prec 0.0477602, recall 0.85177
2017-12-10T01:00:29.439621: step 278, loss 0.548062, acc 0.884766, prec 0.0478281, recall 0.851752
2017-12-10T01:00:31.170173: step 279, loss 0.604883, acc 0.875, prec 0.0480192, recall 0.852303
2017-12-10T01:00:32.873142: step 280, loss 0.557798, acc 0.876953, prec 0.0480159, recall 0.852104
2017-12-10T01:00:34.581434: step 281, loss 0.544938, acc 0.883789, prec 0.0480947, recall 0.852328
2017-12-10T01:00:36.320780: step 282, loss 0.376092, acc 0.884766, prec 0.0481732, recall 0.852756
2017-12-10T01:00:38.032409: step 283, loss 0.403554, acc 0.896484, prec 0.0482982, recall 0.853288
2017-12-10T01:00:39.761283: step 284, loss 0.500916, acc 0.861328, prec 0.0483344, recall 0.853641
2017-12-10T01:00:41.481608: step 285, loss 0.645511, acc 0.867188, prec 0.0484791, recall 0.853863
2017-12-10T01:00:43.227239: step 286, loss 0.461235, acc 0.87207, prec 0.0484582, recall 0.853833
2017-12-10T01:00:44.937134: step 287, loss 0.500056, acc 0.876953, prec 0.0485178, recall 0.854013
2017-12-10T01:00:45.120067: step 288, loss 0.989204, acc 0.882353, prec 0.0485654, recall 0.854152
2017-12-10T01:00:46.842322: step 289, loss 0.353744, acc 0.892578, prec 0.0486988, recall 0.854705
2017-12-10T01:00:48.546868: step 290, loss 0.42283, acc 0.862305, prec 0.0486704, recall 0.854877
2017-12-10T01:00:50.266087: step 291, loss 0.513487, acc 0.856445, prec 0.0488172, recall 0.855527
2017-12-10T01:00:52.004779: step 292, loss 0.409882, acc 0.875, prec 0.0489632, recall 0.856137
2017-12-10T01:00:53.715478: step 293, loss 0.447138, acc 0.880859, prec 0.0489857, recall 0.856206
2017-12-10T01:00:55.446552: step 294, loss 0.37264, acc 0.880859, prec 0.0491093, recall 0.856743
2017-12-10T01:00:57.164393: step 295, loss 0.469308, acc 0.882812, prec 0.0492845, recall 0.857408
2017-12-10T01:00:58.874761: step 296, loss 0.699542, acc 0.895508, prec 0.0493802, recall 0.85744
2017-12-10T01:01:00.613325: step 297, loss 0.386855, acc 0.875977, prec 0.0494106, recall 0.857737
2017-12-10T01:01:02.311806: step 298, loss 0.485372, acc 0.908203, prec 0.0496141, recall 0.858423
2017-12-10T01:01:04.032996: step 299, loss 0.378168, acc 0.893555, prec 0.0497062, recall 0.858845
2017-12-10T01:01:05.748143: step 300, loss 0.434754, acc 0.898438, prec 0.0498014, recall 0.859264

Evaluation:
2017-12-10T01:01:10.438942: step 300, loss 1.6282, acc 0.918483, prec 0.0503854, recall 0.850265

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_1/1512885145/checkpoints/model-300

2017-12-10T01:01:13.109873: step 301, loss 0.667, acc 0.886719, prec 0.050471, recall 0.850506
2017-12-10T01:01:14.814579: step 302, loss 0.818059, acc 0.90918, prec 0.0505108, recall 0.850209
2017-12-10T01:01:16.529352: step 303, loss 0.407831, acc 0.901367, prec 0.0506182, recall 0.850482
2017-12-10T01:01:18.237099: step 304, loss 0.47663, acc 0.885742, prec 0.0507518, recall 0.85085
2017-12-10T01:01:19.945293: step 305, loss 1.03079, acc 0.885742, prec 0.0508252, recall 0.8505
2017-12-10T01:01:21.676251: step 306, loss 0.690999, acc 0.857422, prec 0.0508527, recall 0.85064
2017-12-10T01:01:23.376567: step 307, loss 0.524132, acc 0.871094, prec 0.0509507, recall 0.85094
2017-12-10T01:01:25.068686: step 308, loss 0.652847, acc 0.825195, prec 0.0510412, recall 0.851485
2017-12-10T01:01:26.776080: step 309, loss 0.47669, acc 0.84668, prec 0.0511091, recall 0.851931
2017-12-10T01:01:28.477919: step 310, loss 0.539813, acc 0.833008, prec 0.0512649, recall 0.852627
2017-12-10T01:01:30.237014: step 311, loss 0.644078, acc 0.851562, prec 0.051421, recall 0.853104
2017-12-10T01:01:31.969441: step 312, loss 0.501264, acc 0.842773, prec 0.051485, recall 0.85354
2017-12-10T01:01:33.676560: step 313, loss 0.588133, acc 0.84375, prec 0.0515493, recall 0.853973
2017-12-10T01:01:35.395387: step 314, loss 0.497115, acc 0.84668, prec 0.0515549, recall 0.85425
2017-12-10T01:01:37.123515: step 315, loss 0.818077, acc 0.871094, prec 0.0516023, recall 0.854228
2017-12-10T01:01:38.844664: step 316, loss 0.799576, acc 0.863281, prec 0.0516811, recall 0.854119
2017-12-10T01:01:40.562403: step 317, loss 0.54972, acc 0.870117, prec 0.0517861, recall 0.854606
2017-12-10T01:01:42.267895: step 318, loss 0.533464, acc 0.873047, prec 0.0518934, recall 0.854913
2017-12-10T01:01:43.966241: step 319, loss 0.445215, acc 0.866211, prec 0.052007, recall 0.855424
2017-12-10T01:01:44.149401: step 320, loss 0.328909, acc 0.921569, prec 0.0520044, recall 0.855424
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 1024
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_1024_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_2/1512885705

Start training
2017-12-10T01:01:49.581362: step 1, loss 8.81362, acc 0.726562, prec 0.0147601, recall 0.235294
2017-12-10T01:01:51.317878: step 2, loss 4.97723, acc 0.416016, prec 0.0182857, recall 0.457143
2017-12-10T01:01:53.029971: step 3, loss 6.15943, acc 0.199219, prec 0.0129641, recall 0.488889
2017-12-10T01:01:54.745379: step 4, loss 5.83458, acc 0.152344, prec 0.0116595, recall 0.566038
2017-12-10T01:01:56.454186: step 5, loss 6.16826, acc 0.208984, prec 0.0129679, recall 0.619718
2017-12-10T01:01:58.155442: step 6, loss 5.51288, acc 0.319336, prec 0.0131836, recall 0.635294
2017-12-10T01:01:59.857640: step 7, loss 2.96801, acc 0.447266, prec 0.0143408, recall 0.663366
2017-12-10T01:02:01.581627: step 8, loss 4.33097, acc 0.603516, prec 0.0143814, recall 0.634783
2017-12-10T01:02:03.277509: step 9, loss 4.60846, acc 0.713867, prec 0.0152757, recall 0.61194
2017-12-10T01:02:04.990768: step 10, loss 4.90428, acc 0.694336, prec 0.0149727, recall 0.590278
2017-12-10T01:02:06.713644: step 11, loss 7.14749, acc 0.674805, prec 0.0158149, recall 0.568862
2017-12-10T01:02:08.422984: step 12, loss 4.02082, acc 0.621094, prec 0.0157936, recall 0.564246
2017-12-10T01:02:10.117908: step 13, loss 3.6594, acc 0.529297, prec 0.0159884, recall 0.56701
2017-12-10T01:02:11.817366: step 14, loss 4.14006, acc 0.435547, prec 0.0156815, recall 0.570732
2017-12-10T01:02:13.514434: step 15, loss 4.84799, acc 0.4375, prec 0.0164036, recall 0.586667
2017-12-10T01:02:15.224597: step 16, loss 3.77981, acc 0.396484, prec 0.016488, recall 0.598326
2017-12-10T01:02:16.928103: step 17, loss 5.23289, acc 0.382812, prec 0.0172765, recall 0.616858
2017-12-10T01:02:18.629233: step 18, loss 3.34701, acc 0.378906, prec 0.0172587, recall 0.632353
2017-12-10T01:02:20.337924: step 19, loss 3.44492, acc 0.431641, prec 0.0175189, recall 0.646853
2017-12-10T01:02:22.057444: step 20, loss 5.47219, acc 0.466797, prec 0.0178218, recall 0.642857
2017-12-10T01:02:23.769830: step 21, loss 5.0735, acc 0.545898, prec 0.017962, recall 0.643963
2017-12-10T01:02:25.479390: step 22, loss 2.52366, acc 0.570312, prec 0.0185339, recall 0.653959
2017-12-10T01:02:27.206789: step 23, loss 2.36908, acc 0.601562, prec 0.0185587, recall 0.66
2017-12-10T01:02:28.921980: step 24, loss 2.8803, acc 0.645508, prec 0.0188076, recall 0.65847
2017-12-10T01:02:30.652999: step 25, loss 4.57958, acc 0.638672, prec 0.0190296, recall 0.660526
2017-12-10T01:02:32.366489: step 26, loss 3.82661, acc 0.675781, prec 0.0192947, recall 0.66076
2017-12-10T01:02:34.078801: step 27, loss 4.16124, acc 0.661133, prec 0.0196008, recall 0.657005
2017-12-10T01:02:35.807119: step 28, loss 4.80973, acc 0.694336, prec 0.0198689, recall 0.654292
2017-12-10T01:02:37.526094: step 29, loss 4.1765, acc 0.624023, prec 0.0202235, recall 0.658482
2017-12-10T01:02:39.232271: step 30, loss 2.38739, acc 0.614258, prec 0.0202802, recall 0.663755
2017-12-10T01:02:40.956413: step 31, loss 2.66215, acc 0.611328, prec 0.02033, recall 0.668803
2017-12-10T01:02:41.140829: step 32, loss 1.88923, acc 0.576923, prec 0.020428, recall 0.670213
2017-12-10T01:02:42.856035: step 33, loss 2.85663, acc 0.573242, prec 0.0203631, recall 0.671518
2017-12-10T01:02:44.569056: step 34, loss 2.04239, acc 0.647461, prec 0.0207563, recall 0.679435
2017-12-10T01:02:46.283327: step 35, loss 2.38094, acc 0.65625, prec 0.0206739, recall 0.679208
2017-12-10T01:02:48.011628: step 36, loss 2.42927, acc 0.662109, prec 0.0207755, recall 0.680851
2017-12-10T01:02:49.731041: step 37, loss 2.73569, acc 0.707031, prec 0.0210435, recall 0.682331
2017-12-10T01:02:51.465665: step 38, loss 3.13235, acc 0.71875, prec 0.0211504, recall 0.680734
2017-12-10T01:02:53.188880: step 39, loss 2.7061, acc 0.729492, prec 0.0214306, recall 0.682143
2017-12-10T01:02:54.909117: step 40, loss 3.29975, acc 0.739258, prec 0.0215517, recall 0.681818
2017-12-10T01:02:56.605199: step 41, loss 1.37465, acc 0.748047, prec 0.0217308, recall 0.684391
2017-12-10T01:02:58.303275: step 42, loss 2.7927, acc 0.719727, prec 0.0220852, recall 0.684385
2017-12-10T01:03:00.002741: step 43, loss 2.50337, acc 0.698242, prec 0.0225045, recall 0.686495
2017-12-10T01:03:01.724004: step 44, loss 1.797, acc 0.673828, prec 0.0226237, recall 0.689274
2017-12-10T01:03:03.425669: step 45, loss 2.69159, acc 0.689453, prec 0.0229089, recall 0.691244
2017-12-10T01:03:05.121786: step 46, loss 3.1595, acc 0.654297, prec 0.0229466, recall 0.692308
2017-12-10T01:03:06.844150: step 47, loss 1.82961, acc 0.65918, prec 0.0232261, recall 0.69764
2017-12-10T01:03:08.547348: step 48, loss 3.49284, acc 0.648438, prec 0.0234891, recall 0.697708
2017-12-10T01:03:10.270521: step 49, loss 2.06152, acc 0.633789, prec 0.0235367, recall 0.7
2017-12-10T01:03:11.986736: step 50, loss 2.95092, acc 0.648438, prec 0.023784, recall 0.70096
2017-12-10T01:03:13.693589: step 51, loss 2.17231, acc 0.623047, prec 0.023902, recall 0.703903
2017-12-10T01:03:15.422123: step 52, loss 1.79958, acc 0.628906, prec 0.0241526, recall 0.708827
2017-12-10T01:03:17.143169: step 53, loss 2.33643, acc 0.675781, prec 0.0242328, recall 0.709845
2017-12-10T01:03:18.848473: step 54, loss 2.874, acc 0.65625, prec 0.0244157, recall 0.712834
2017-12-10T01:03:20.559880: step 55, loss 1.59323, acc 0.676758, prec 0.0246162, recall 0.714819
2017-12-10T01:03:22.282106: step 56, loss 2.42811, acc 0.723633, prec 0.0250286, recall 0.716364
2017-12-10T01:03:23.989850: step 57, loss 2.16106, acc 0.736328, prec 0.0254028, recall 0.718343
2017-12-10T01:03:25.708101: step 58, loss 2.14347, acc 0.720703, prec 0.0256696, recall 0.720418
2017-12-10T01:03:27.422716: step 59, loss 2.2322, acc 0.716797, prec 0.0256484, recall 0.719359
2017-12-10T01:03:29.124828: step 60, loss 2.51004, acc 0.71582, prec 0.0258624, recall 0.720225
2017-12-10T01:03:30.858304: step 61, loss 1.336, acc 0.692383, prec 0.0260423, recall 0.724252
2017-12-10T01:03:32.556657: step 62, loss 2.95628, acc 0.695312, prec 0.0260353, recall 0.721919
2017-12-10T01:03:34.265981: step 63, loss 2.0653, acc 0.664062, prec 0.0263719, recall 0.724175
2017-12-10T01:03:34.448041: step 64, loss 0.923347, acc 0.711538, prec 0.0263943, recall 0.724468
2017-12-10T01:03:36.184962: step 65, loss 2.058, acc 0.645508, prec 0.0265128, recall 0.727463
2017-12-10T01:03:37.884496: step 66, loss 1.76107, acc 0.640625, prec 0.0266963, recall 0.730928
2017-12-10T01:03:39.611758: step 67, loss 1.63997, acc 0.678711, prec 0.0268773, recall 0.73401
2017-12-10T01:03:41.323117: step 68, loss 3.44975, acc 0.711914, prec 0.0268084, recall 0.72973
2017-12-10T01:03:43.020483: step 69, loss 3.57441, acc 0.6875, prec 0.0269966, recall 0.729862
2017-12-10T01:03:44.742381: step 70, loss 1.45966, acc 0.691406, prec 0.0269717, recall 0.730545
2017-12-10T01:03:46.453507: step 71, loss 2.5372, acc 0.68457, prec 0.0271834, recall 0.732314
2017-12-10T01:03:48.162442: step 72, loss 2.68563, acc 0.689453, prec 0.0272603, recall 0.731638
2017-12-10T01:03:49.887922: step 73, loss 2.73955, acc 0.707031, prec 0.0274876, recall 0.731978
2017-12-10T01:03:51.609592: step 74, loss 1.51976, acc 0.649414, prec 0.027516, recall 0.734675
2017-12-10T01:03:53.323865: step 75, loss 1.44273, acc 0.661133, prec 0.0275549, recall 0.737319
2017-12-10T01:03:55.029002: step 76, loss 2.61703, acc 0.677734, prec 0.0279702, recall 0.739823
2017-12-10T01:03:56.742218: step 77, loss 1.42, acc 0.666992, prec 0.0280405, recall 0.742557
2017-12-10T01:03:58.445715: step 78, loss 2.64568, acc 0.716797, prec 0.0281598, recall 0.74266
2017-12-10T01:04:00.157074: step 79, loss 2.09087, acc 0.717773, prec 0.0282444, recall 0.743809
2017-12-10T01:04:01.896972: step 80, loss 2.93799, acc 0.758789, prec 0.0282746, recall 0.742399
2017-12-10T01:04:03.602244: step 81, loss 2.41591, acc 0.730469, prec 0.0284312, recall 0.743333
2017-12-10T01:04:05.307828: step 82, loss 1.49452, acc 0.737305, prec 0.0286518, recall 0.745275
2017-12-10T01:04:07.020893: step 83, loss 2.0741, acc 0.732422, prec 0.028804, recall 0.746148
2017-12-10T01:04:08.732160: step 84, loss 2.19939, acc 0.712891, prec 0.0289657, recall 0.7472
2017-12-10T01:04:10.449936: step 85, loss 1.74898, acc 0.733398, prec 0.0290537, recall 0.747627
2017-12-10T01:04:12.168906: step 86, loss 1.53403, acc 0.714844, prec 0.0292415, recall 0.74883
2017-12-10T01:04:13.903191: step 87, loss 1.44748, acc 0.697266, prec 0.0295253, recall 0.75192
2017-12-10T01:04:15.610382: step 88, loss 1.89799, acc 0.708008, prec 0.0295243, recall 0.752475
2017-12-10T01:04:17.306397: step 89, loss 1.59288, acc 0.709961, prec 0.0296695, recall 0.753383
2017-12-10T01:04:19.032535: step 90, loss 1.18085, acc 0.717773, prec 0.0297605, recall 0.755026
2017-12-10T01:04:20.760772: step 91, loss 1.11741, acc 0.748047, prec 0.0297073, recall 0.755556
2017-12-10T01:04:22.478598: step 92, loss 1.3458, acc 0.749023, prec 0.0298249, recall 0.756598
2017-12-10T01:04:24.188251: step 93, loss 3.51982, acc 0.769531, prec 0.0298525, recall 0.753623
2017-12-10T01:04:25.902888: step 94, loss 1.72476, acc 0.763672, prec 0.0299809, recall 0.754122
2017-12-10T01:04:27.613551: step 95, loss 1.23446, acc 0.78418, prec 0.0301254, recall 0.75461
2017-12-10T01:04:27.798136: step 96, loss 0.53828, acc 0.807692, prec 0.0301169, recall 0.75461
2017-12-10T01:04:29.512732: step 97, loss 1.29307, acc 0.763672, prec 0.0304035, recall 0.757173
2017-12-10T01:04:31.239314: step 98, loss 1.79941, acc 0.736328, prec 0.0305286, recall 0.757785
2017-12-10T01:04:32.961865: step 99, loss 1.63962, acc 0.771484, prec 0.0306823, recall 0.758385
2017-12-10T01:04:34.672112: step 100, loss 0.96522, acc 0.763672, prec 0.0307988, recall 0.759837
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_2/1512885705/checkpoints/model-100

2017-12-10T01:04:37.651578: step 101, loss 1.3304, acc 0.760742, prec 0.0309915, recall 0.761234
2017-12-10T01:04:39.368594: step 102, loss 1.23472, acc 0.748047, prec 0.0309865, recall 0.761492
2017-12-10T01:04:41.083647: step 103, loss 2.21423, acc 0.771484, prec 0.0311599, recall 0.761685
2017-12-10T01:04:42.797058: step 104, loss 1.12867, acc 0.757812, prec 0.0313428, recall 0.763518
2017-12-10T01:04:44.509915: step 105, loss 1.45942, acc 0.748047, prec 0.0313363, recall 0.76326
2017-12-10T01:04:46.229827: step 106, loss 1.20168, acc 0.727539, prec 0.0314131, recall 0.764593
2017-12-10T01:04:47.935704: step 107, loss 1.98071, acc 0.75, prec 0.0315869, recall 0.764408
2017-12-10T01:04:49.649645: step 108, loss 1.98261, acc 0.725586, prec 0.0316112, recall 0.763967
2017-12-10T01:04:51.381178: step 109, loss 1.2951, acc 0.711914, prec 0.0316221, recall 0.764486
2017-12-10T01:04:53.098141: step 110, loss 1.23202, acc 0.676758, prec 0.0316268, recall 0.766089
2017-12-10T01:04:54.802365: step 111, loss 1.89389, acc 0.676758, prec 0.0316592, recall 0.765931
2017-12-10T01:04:56.532424: step 112, loss 1.29097, acc 0.6875, prec 0.031648, recall 0.767357
2017-12-10T01:04:58.228171: step 113, loss 1.26438, acc 0.712891, prec 0.0316099, recall 0.768019
2017-12-10T01:04:59.937123: step 114, loss 1.03996, acc 0.726562, prec 0.0317272, recall 0.769508
2017-12-10T01:05:01.650873: step 115, loss 1.11617, acc 0.771484, prec 0.0318081, recall 0.770101
2017-12-10T01:05:03.352741: step 116, loss 1.78766, acc 0.789062, prec 0.0322589, recall 0.771796
2017-12-10T01:05:05.060951: step 117, loss 2.35621, acc 0.811523, prec 0.0323703, recall 0.771015
2017-12-10T01:05:06.788617: step 118, loss 2.41331, acc 0.788086, prec 0.0325095, recall 0.770069
2017-12-10T01:05:08.493145: step 119, loss 0.964448, acc 0.765625, prec 0.0325553, recall 0.77094
2017-12-10T01:05:10.202434: step 120, loss 1.64888, acc 0.764648, prec 0.032694, recall 0.771445
2017-12-10T01:05:11.907878: step 121, loss 1.05465, acc 0.749023, prec 0.032702, recall 0.772166
2017-12-10T01:05:13.627736: step 122, loss 1.46385, acc 0.709961, prec 0.0327938, recall 0.773081
2017-12-10T01:05:15.360956: step 123, loss 1.67629, acc 0.724609, prec 0.0328741, recall 0.773429
2017-12-10T01:05:17.066747: step 124, loss 1.82553, acc 0.713867, prec 0.0330348, recall 0.774264
2017-12-10T01:05:18.779719: step 125, loss 1.30621, acc 0.716797, prec 0.033194, recall 0.775918
2017-12-10T01:05:20.489614: step 126, loss 1.05348, acc 0.686523, prec 0.0331492, recall 0.777002
2017-12-10T01:05:22.215910: step 127, loss 2.16272, acc 0.738281, prec 0.0332377, recall 0.776064
2017-12-10T01:05:22.400484: step 128, loss 0.606803, acc 0.730769, prec 0.0332271, recall 0.776064
2017-12-10T01:05:24.119083: step 129, loss 1.13398, acc 0.757812, prec 0.033436, recall 0.777368
2017-12-10T01:05:25.820580: step 130, loss 1.36251, acc 0.77832, prec 0.0335487, recall 0.778474
2017-12-10T01:05:27.521122: step 131, loss 1.25536, acc 0.794922, prec 0.033588, recall 0.778297
2017-12-10T01:05:29.222718: step 132, loss 1.1002, acc 0.801758, prec 0.0336099, recall 0.778409
2017-12-10T01:05:30.958401: step 133, loss 1.46598, acc 0.825195, prec 0.0337368, recall 0.778176
2017-12-10T01:05:32.678061: step 134, loss 0.658165, acc 0.822266, prec 0.0338796, recall 0.779247
2017-12-10T01:05:34.394539: step 135, loss 0.713659, acc 0.811523, prec 0.0339065, recall 0.779747
2017-12-10T01:05:36.106279: step 136, loss 1.12306, acc 0.84375, prec 0.0340652, recall 0.78001
2017-12-10T01:05:37.813163: step 137, loss 1.35519, acc 0.845703, prec 0.0341195, recall 0.779331
2017-12-10T01:05:39.538089: step 138, loss 1.09173, acc 0.830078, prec 0.0343064, recall 0.780584
2017-12-10T01:05:41.253632: step 139, loss 0.761838, acc 0.816406, prec 0.034251, recall 0.780632
2017-12-10T01:05:42.967335: step 140, loss 1.19211, acc 0.817383, prec 0.0343652, recall 0.780775
2017-12-10T01:05:44.681397: step 141, loss 1.35126, acc 0.808594, prec 0.0346176, recall 0.78128
2017-12-10T01:05:46.390658: step 142, loss 1.40955, acc 0.793945, prec 0.0348161, recall 0.78119
2017-12-10T01:05:48.118142: step 143, loss 0.962902, acc 0.731445, prec 0.0349408, recall 0.782857
2017-12-10T01:05:49.839871: step 144, loss 1.21804, acc 0.760742, prec 0.0350258, recall 0.783822
2017-12-10T01:05:51.590183: step 145, loss 1.17692, acc 0.745117, prec 0.0350379, recall 0.784102
2017-12-10T01:05:53.295572: step 146, loss 1.15465, acc 0.730469, prec 0.0353607, recall 0.786345
2017-12-10T01:05:55.006097: step 147, loss 1.00551, acc 0.740234, prec 0.0355259, recall 0.788116
2017-12-10T01:05:56.712392: step 148, loss 1.33858, acc 0.741211, prec 0.035572, recall 0.788558
2017-12-10T01:05:58.426693: step 149, loss 0.970533, acc 0.785156, prec 0.0356497, recall 0.789354
2017-12-10T01:06:00.147111: step 150, loss 1.10451, acc 0.790039, prec 0.0358092, recall 0.790519
2017-12-10T01:06:01.857013: step 151, loss 0.918549, acc 0.790039, prec 0.0358495, recall 0.791105
2017-12-10T01:06:03.557907: step 152, loss 0.856943, acc 0.807617, prec 0.0360589, recall 0.792428
2017-12-10T01:06:05.278357: step 153, loss 2.7324, acc 0.84375, prec 0.0361227, recall 0.790801
2017-12-10T01:06:07.011298: step 154, loss 1.51815, acc 0.836914, prec 0.0363153, recall 0.790535
2017-12-10T01:06:08.715294: step 155, loss 1.0737, acc 0.8125, prec 0.0364475, recall 0.791123
2017-12-10T01:06:10.456016: step 156, loss 0.773209, acc 0.786133, prec 0.0364228, recall 0.791757
2017-12-10T01:06:12.152165: step 157, loss 0.868436, acc 0.791992, prec 0.036653, recall 0.792866
2017-12-10T01:06:13.873685: step 158, loss 0.780207, acc 0.750977, prec 0.0366777, recall 0.793841
2017-12-10T01:06:15.573258: step 159, loss 0.700126, acc 0.798828, prec 0.0367375, recall 0.794806
2017-12-10T01:06:15.759396: step 160, loss 0.482203, acc 0.865385, prec 0.0367514, recall 0.794894
2017-12-10T01:06:17.449970: step 161, loss 1.04211, acc 0.826172, prec 0.0368701, recall 0.795349
2017-12-10T01:06:19.162344: step 162, loss 0.873933, acc 0.805664, prec 0.0370472, recall 0.796475
2017-12-10T01:06:20.857544: step 163, loss 0.907912, acc 0.858398, prec 0.0370933, recall 0.796823
2017-12-10T01:06:22.561004: step 164, loss 1.14164, acc 0.850586, prec 0.0371915, recall 0.796427
2017-12-10T01:06:24.280710: step 165, loss 0.596081, acc 0.836914, prec 0.0373134, recall 0.797521
2017-12-10T01:06:25.990347: step 166, loss 0.640201, acc 0.848633, prec 0.0373882, recall 0.798025
2017-12-10T01:06:27.704069: step 167, loss 0.702978, acc 0.839844, prec 0.0375847, recall 0.799428
2017-12-10T01:06:29.413453: step 168, loss 0.720078, acc 0.834961, prec 0.0376857, recall 0.799756
2017-12-10T01:06:31.154673: step 169, loss 0.935526, acc 0.837891, prec 0.0378064, recall 0.800161
2017-12-10T01:06:32.865045: step 170, loss 1.72179, acc 0.850586, prec 0.038011, recall 0.79992
2017-12-10T01:06:34.586826: step 171, loss 0.990352, acc 0.820312, prec 0.0380637, recall 0.799443
2017-12-10T01:06:36.319313: step 172, loss 1.43286, acc 0.807617, prec 0.0382874, recall 0.800079
2017-12-10T01:06:38.046072: step 173, loss 1.02935, acc 0.746094, prec 0.0383359, recall 0.800784
2017-12-10T01:06:39.779594: step 174, loss 0.941038, acc 0.726562, prec 0.0383869, recall 0.801872
2017-12-10T01:06:41.490350: step 175, loss 0.992863, acc 0.709961, prec 0.0383716, recall 0.802718
2017-12-10T01:06:43.194047: step 176, loss 0.91842, acc 0.729492, prec 0.0384239, recall 0.803785
2017-12-10T01:06:44.915866: step 177, loss 0.941984, acc 0.739258, prec 0.0384827, recall 0.804841
2017-12-10T01:06:46.636594: step 178, loss 1.10215, acc 0.746094, prec 0.0384953, recall 0.804738
2017-12-10T01:06:48.386086: step 179, loss 1.06553, acc 0.785156, prec 0.0386575, recall 0.805461
2017-12-10T01:06:50.094571: step 180, loss 1.01967, acc 0.819336, prec 0.0387747, recall 0.805273
2017-12-10T01:06:51.832785: step 181, loss 0.569259, acc 0.821289, prec 0.0388383, recall 0.805774
2017-12-10T01:06:53.542928: step 182, loss 0.53429, acc 0.864258, prec 0.0389669, recall 0.806415
2017-12-10T01:06:55.252770: step 183, loss 0.963553, acc 0.874023, prec 0.0390521, recall 0.805937
2017-12-10T01:06:56.968982: step 184, loss 1.03325, acc 0.881836, prec 0.0391942, recall 0.805678
2017-12-10T01:06:58.677526: step 185, loss 0.693809, acc 0.848633, prec 0.0393268, recall 0.806381
2017-12-10T01:07:00.373403: step 186, loss 0.586099, acc 0.854492, prec 0.0394622, recall 0.80737
2017-12-10T01:07:02.093711: step 187, loss 1.44505, acc 0.851562, prec 0.0396487, recall 0.807386
2017-12-10T01:07:03.791775: step 188, loss 0.663119, acc 0.837891, prec 0.0398221, recall 0.808273
2017-12-10T01:07:05.492425: step 189, loss 0.750566, acc 0.8125, prec 0.039976, recall 0.809149
2017-12-10T01:07:07.220095: step 190, loss 0.622426, acc 0.798828, prec 0.0399156, recall 0.80949
2017-12-10T01:07:08.920810: step 191, loss 1.12991, acc 0.802734, prec 0.0400273, recall 0.810216
2017-12-10T01:07:09.110055: step 192, loss 0.434637, acc 0.788462, prec 0.0400364, recall 0.810284
2017-12-10T01:07:10.818255: step 193, loss 0.630675, acc 0.822266, prec 0.0401271, recall 0.811154
2017-12-10T01:07:12.521602: step 194, loss 0.840013, acc 0.827148, prec 0.0402381, recall 0.811798
2017-12-10T01:07:14.230823: step 195, loss 1.28979, acc 0.836914, prec 0.0403075, recall 0.811387
2017-12-10T01:07:15.941375: step 196, loss 1.02707, acc 0.830078, prec 0.0403881, recall 0.811046
2017-12-10T01:07:17.653188: step 197, loss 0.60203, acc 0.848633, prec 0.0405284, recall 0.812025
2017-12-10T01:07:19.361642: step 198, loss 0.724852, acc 0.84375, prec 0.0405991, recall 0.812457
2017-12-10T01:07:21.095746: step 199, loss 0.591193, acc 0.851562, prec 0.0406749, recall 0.812886
2017-12-10T01:07:22.797757: step 200, loss 0.882222, acc 0.87207, prec 0.0408149, recall 0.813224
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_2/1512885705/checkpoints/model-200

2017-12-10T01:07:25.613559: step 201, loss 0.505063, acc 0.850586, prec 0.0409538, recall 0.814174
2017-12-10T01:07:27.323972: step 202, loss 0.448837, acc 0.854492, prec 0.0410784, recall 0.815052
2017-12-10T01:07:29.009937: step 203, loss 0.385254, acc 0.862305, prec 0.041094, recall 0.815488
2017-12-10T01:07:30.729686: step 204, loss 0.68374, acc 0.874023, prec 0.0412322, recall 0.81608
2017-12-10T01:07:32.454134: step 205, loss 0.585395, acc 0.884766, prec 0.0415069, recall 0.817154
2017-12-10T01:07:34.163328: step 206, loss 1.02354, acc 0.860352, prec 0.0416351, recall 0.81719
2017-12-10T01:07:35.877868: step 207, loss 0.484442, acc 0.888672, prec 0.0417808, recall 0.818032
2017-12-10T01:07:37.600692: step 208, loss 0.641511, acc 0.891602, prec 0.041833, recall 0.817973
2017-12-10T01:07:39.314468: step 209, loss 0.485671, acc 0.855469, prec 0.0418897, recall 0.818568
2017-12-10T01:07:41.020041: step 210, loss 0.815358, acc 0.851562, prec 0.0421372, recall 0.819066
2017-12-10T01:07:42.739570: step 211, loss 0.60167, acc 0.845703, prec 0.0422814, recall 0.82
2017-12-10T01:07:44.448304: step 212, loss 0.656957, acc 0.850586, prec 0.0424138, recall 0.82034
2017-12-10T01:07:46.172010: step 213, loss 0.426149, acc 0.859375, prec 0.0425029, recall 0.821029
2017-12-10T01:07:47.868242: step 214, loss 0.544746, acc 0.841797, prec 0.0425788, recall 0.821713
2017-12-10T01:07:49.575872: step 215, loss 0.607178, acc 0.842773, prec 0.0427653, recall 0.822785
2017-12-10T01:07:51.295330: step 216, loss 0.596114, acc 0.862305, prec 0.0428869, recall 0.823307
2017-12-10T01:07:53.013883: step 217, loss 0.898982, acc 0.842773, prec 0.0429789, recall 0.823511
2017-12-10T01:07:54.722720: step 218, loss 0.647162, acc 0.880859, prec 0.0432072, recall 0.824097
2017-12-10T01:07:56.465831: step 219, loss 1.40972, acc 0.862305, prec 0.0433296, recall 0.823584
2017-12-10T01:07:58.180665: step 220, loss 0.618742, acc 0.856445, prec 0.0433519, recall 0.823511
2017-12-10T01:07:59.889158: step 221, loss 0.478492, acc 0.857422, prec 0.0434353, recall 0.824162
2017-12-10T01:08:01.614914: step 222, loss 0.999374, acc 0.835938, prec 0.0435035, recall 0.824556
2017-12-10T01:08:03.345755: step 223, loss 0.929123, acc 0.834961, prec 0.0437254, recall 0.825228
2017-12-10T01:08:03.525686: step 224, loss 0.703985, acc 0.788462, prec 0.0437177, recall 0.825228
2017-12-10T01:08:05.235624: step 225, loss 0.609756, acc 0.813477, prec 0.0437986, recall 0.825969
2017-12-10T01:08:06.957546: step 226, loss 0.647005, acc 0.821289, prec 0.0438699, recall 0.826401
2017-12-10T01:08:08.689046: step 227, loss 0.834308, acc 0.828125, prec 0.0440531, recall 0.826946
2017-12-10T01:08:10.403173: step 228, loss 0.54704, acc 0.857422, prec 0.044134, recall 0.827319
2017-12-10T01:08:12.107819: step 229, loss 0.943203, acc 0.874023, prec 0.0443326, recall 0.828046
2017-12-10T01:08:13.797730: step 230, loss 0.453881, acc 0.884766, prec 0.0444466, recall 0.828706
2017-12-10T01:08:15.527014: step 231, loss 0.386977, acc 0.881836, prec 0.044573, recall 0.829412
2017-12-10T01:08:17.236907: step 232, loss 0.46123, acc 0.890625, prec 0.0448409, recall 0.830558
2017-12-10T01:08:18.948981: step 233, loss 0.864805, acc 0.882812, prec 0.0448936, recall 0.830277
2017-12-10T01:08:20.664287: step 234, loss 0.415109, acc 0.90625, prec 0.0450367, recall 0.830725
2017-12-10T01:08:22.380775: step 235, loss 0.841953, acc 0.891602, prec 0.0451544, recall 0.83088
2017-12-10T01:08:24.082560: step 236, loss 0.655962, acc 0.90918, prec 0.0454033, recall 0.83166
2017-12-10T01:08:25.789349: step 237, loss 0.702079, acc 0.895508, prec 0.0455078, recall 0.831762
2017-12-10T01:08:27.531090: step 238, loss 0.719953, acc 0.857422, prec 0.0456437, recall 0.832055
2017-12-10T01:08:29.240215: step 239, loss 0.537873, acc 0.84668, prec 0.0457252, recall 0.832673
2017-12-10T01:08:30.993962: step 240, loss 0.594403, acc 0.823242, prec 0.0458336, recall 0.833427
2017-12-10T01:08:32.701329: step 241, loss 0.868066, acc 0.810547, prec 0.0458886, recall 0.833801
2017-12-10T01:08:34.406044: step 242, loss 0.60843, acc 0.832031, prec 0.0458993, recall 0.83422
2017-12-10T01:08:36.139924: step 243, loss 0.715305, acc 0.801758, prec 0.046006, recall 0.834773
2017-12-10T01:08:37.854757: step 244, loss 0.45939, acc 0.84082, prec 0.0459934, recall 0.835095
2017-12-10T01:08:39.551521: step 245, loss 0.507199, acc 0.849609, prec 0.0461329, recall 0.83587
2017-12-10T01:08:41.272575: step 246, loss 0.490093, acc 0.858398, prec 0.0461916, recall 0.836138
2017-12-10T01:08:42.982562: step 247, loss 0.588338, acc 0.900391, prec 0.0463673, recall 0.836673
2017-12-10T01:08:44.692392: step 248, loss 0.844804, acc 0.907227, prec 0.0464611, recall 0.836707
2017-12-10T01:08:46.407184: step 249, loss 0.683477, acc 0.901367, prec 0.0466082, recall 0.836918
2017-12-10T01:08:48.121744: step 250, loss 0.483268, acc 0.913086, prec 0.0467193, recall 0.837222
2017-12-10T01:08:49.833198: step 251, loss 0.595763, acc 0.916016, prec 0.0468185, recall 0.837253
2017-12-10T01:08:51.563806: step 252, loss 0.841526, acc 0.900391, prec 0.0469644, recall 0.837234
2017-12-10T01:08:53.269015: step 253, loss 0.406389, acc 0.870117, prec 0.0470282, recall 0.837715
2017-12-10T01:08:54.961869: step 254, loss 0.582655, acc 0.87207, prec 0.047179, recall 0.838451
2017-12-10T01:08:56.683158: step 255, loss 0.578274, acc 0.881836, prec 0.0472799, recall 0.838787
2017-12-10T01:08:56.868109: step 256, loss 0.351943, acc 0.884615, prec 0.0472899, recall 0.83883
2017-12-10T01:08:58.581317: step 257, loss 0.483494, acc 0.84375, prec 0.0473477, recall 0.839343
2017-12-10T01:09:00.292053: step 258, loss 0.544099, acc 0.839844, prec 0.047374, recall 0.839767
2017-12-10T01:09:02.012547: step 259, loss 0.541234, acc 0.864258, prec 0.0475035, recall 0.840221
2017-12-10T01:09:03.737232: step 260, loss 0.542471, acc 0.871094, prec 0.0476091, recall 0.840587
2017-12-10T01:09:05.438535: step 261, loss 0.55084, acc 0.873047, prec 0.0477298, recall 0.840992
2017-12-10T01:09:07.168368: step 262, loss 0.48232, acc 0.888672, prec 0.0479169, recall 0.841777
2017-12-10T01:09:08.880301: step 263, loss 0.390474, acc 0.899414, prec 0.0480549, recall 0.842391
2017-12-10T01:09:10.581900: step 264, loss 0.322993, acc 0.915039, prec 0.0481336, recall 0.842798
2017-12-10T01:09:12.285694: step 265, loss 0.413826, acc 0.895508, prec 0.0482679, recall 0.843404
2017-12-10T01:09:13.992005: step 266, loss 0.455438, acc 0.897461, prec 0.0484598, recall 0.84395
2017-12-10T01:09:15.693410: step 267, loss 0.373923, acc 0.927734, prec 0.0485475, recall 0.844133
2017-12-10T01:09:17.404982: step 268, loss 0.415944, acc 0.911133, prec 0.0487058, recall 0.844766
2017-12-10T01:09:19.126439: step 269, loss 0.542683, acc 0.916992, prec 0.0488269, recall 0.845063
2017-12-10T01:09:20.848391: step 270, loss 0.276984, acc 0.919922, prec 0.0489907, recall 0.845688
2017-12-10T01:09:22.560464: step 271, loss 0.353967, acc 0.926758, prec 0.0491729, recall 0.846347
2017-12-10T01:09:24.272465: step 272, loss 0.562298, acc 0.904297, prec 0.0493257, recall 0.846538
2017-12-10T01:09:25.997365: step 273, loss 0.556703, acc 0.90918, prec 0.0494533, recall 0.846863
2017-12-10T01:09:27.704331: step 274, loss 0.434112, acc 0.891602, prec 0.0496495, recall 0.847621
2017-12-10T01:09:29.415972: step 275, loss 0.410052, acc 0.883789, prec 0.0497161, recall 0.847826
2017-12-10T01:09:31.147817: step 276, loss 0.459298, acc 0.889648, prec 0.0498135, recall 0.848313
2017-12-10T01:09:32.861886: step 277, loss 0.456355, acc 0.882812, prec 0.0500014, recall 0.849057
2017-12-10T01:09:34.567321: step 278, loss 0.353685, acc 0.898438, prec 0.0501318, recall 0.849609
2017-12-10T01:09:36.316087: step 279, loss 0.528747, acc 0.880859, prec 0.0502358, recall 0.849915
2017-12-10T01:09:38.006612: step 280, loss 0.615037, acc 0.883789, prec 0.0503143, recall 0.850146
2017-12-10T01:09:39.716785: step 281, loss 0.849786, acc 0.882812, prec 0.0503782, recall 0.850339
2017-12-10T01:09:41.444519: step 282, loss 0.688373, acc 0.857422, prec 0.0505045, recall 0.850746
2017-12-10T01:09:43.193908: step 283, loss 0.422663, acc 0.882812, prec 0.050567, recall 0.85114
2017-12-10T01:09:44.944141: step 284, loss 0.611982, acc 0.870117, prec 0.0507153, recall 0.851577
2017-12-10T01:09:46.672173: step 285, loss 0.441015, acc 0.882812, prec 0.0508581, recall 0.852178
2017-12-10T01:09:48.385446: step 286, loss 0.508605, acc 0.864258, prec 0.0509731, recall 0.852739
2017-12-10T01:09:50.120023: step 287, loss 0.334744, acc 0.892578, prec 0.0510683, recall 0.853191
2017-12-10T01:09:50.302639: step 288, loss 0.457416, acc 0.865385, prec 0.0510633, recall 0.853191
2017-12-10T01:09:52.027507: step 289, loss 0.263377, acc 0.919922, prec 0.051165, recall 0.853607
2017-12-10T01:09:53.745772: step 290, loss 0.438541, acc 0.917969, prec 0.0512925, recall 0.853888
2017-12-10T01:09:55.454157: step 291, loss 0.697557, acc 0.928711, prec 0.0514678, recall 0.854269
2017-12-10T01:09:57.164622: step 292, loss 0.480562, acc 0.933594, prec 0.0515936, recall 0.854312
2017-12-10T01:09:58.881764: step 293, loss 0.324905, acc 0.928711, prec 0.0517673, recall 0.854887
2017-12-10T01:10:00.597686: step 294, loss 0.343016, acc 0.933594, prec 0.0519442, recall 0.855458
2017-12-10T01:10:02.312628: step 295, loss 0.862891, acc 0.933594, prec 0.0520025, recall 0.85533
2017-12-10T01:10:04.027041: step 296, loss 0.383705, acc 0.914062, prec 0.0521118, recall 0.855566
2017-12-10T01:10:05.727224: step 297, loss 0.505854, acc 0.901367, prec 0.0522245, recall 0.855833
2017-12-10T01:10:07.444819: step 298, loss 0.34502, acc 0.895508, prec 0.052345, recall 0.856327
2017-12-10T01:10:09.162557: step 299, loss 0.522992, acc 0.882812, prec 0.0524966, recall 0.856526
2017-12-10T01:10:10.858203: step 300, loss 1.62522, acc 0.87207, prec 0.0525877, recall 0.856399

Evaluation:
2017-12-10T01:10:15.555197: step 300, loss 1.43689, acc 0.883752, prec 0.0529704, recall 0.849441

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_2/1512885705/checkpoints/model-300

2017-12-10T01:10:18.417359: step 301, loss 0.431071, acc 0.864258, prec 0.0530895, recall 0.85
2017-12-10T01:10:20.126132: step 302, loss 0.389922, acc 0.866211, prec 0.053158, recall 0.850425
2017-12-10T01:10:21.846770: step 303, loss 0.63463, acc 0.853516, prec 0.0533205, recall 0.850921
2017-12-10T01:10:23.569695: step 304, loss 0.576936, acc 0.853516, prec 0.0533796, recall 0.851156
2017-12-10T01:10:25.287555: step 305, loss 0.404925, acc 0.854492, prec 0.0534256, recall 0.851541
2017-12-10T01:10:27.009323: step 306, loss 0.636959, acc 0.858398, prec 0.0536162, recall 0.851907
2017-12-10T01:10:28.715313: step 307, loss 0.676038, acc 0.864258, prec 0.0537219, recall 0.851868
2017-12-10T01:10:30.450437: step 308, loss 0.558787, acc 0.856445, prec 0.0538319, recall 0.852403
2017-12-10T01:10:32.164429: step 309, loss 0.335785, acc 0.895508, prec 0.0538942, recall 0.852748
2017-12-10T01:10:33.869801: step 310, loss 0.469119, acc 0.881836, prec 0.053985, recall 0.853003
2017-12-10T01:10:35.615742: step 311, loss 0.386296, acc 0.889648, prec 0.0540299, recall 0.853314
2017-12-10T01:10:37.353053: step 312, loss 0.415535, acc 0.907227, prec 0.0542138, recall 0.85393
2017-12-10T01:10:39.062554: step 313, loss 0.232895, acc 0.912109, prec 0.0542748, recall 0.854237
2017-12-10T01:10:40.786443: step 314, loss 0.398699, acc 0.936523, prec 0.0544174, recall 0.854515
2017-12-10T01:10:42.509857: step 315, loss 0.488166, acc 0.925781, prec 0.0545273, recall 0.854553
2017-12-10T01:10:44.232326: step 316, loss 0.439621, acc 0.915039, prec 0.0545788, recall 0.85447
2017-12-10T01:10:45.923326: step 317, loss 0.5146, acc 0.927734, prec 0.0546144, recall 0.854327
2017-12-10T01:10:47.639325: step 318, loss 0.483738, acc 0.928711, prec 0.0547377, recall 0.854572
2017-12-10T01:10:49.339034: step 319, loss 0.393352, acc 0.908203, prec 0.0548322, recall 0.854962
2017-12-10T01:10:49.526253: step 320, loss 0.114249, acc 0.961538, prec 0.0548433, recall 0.854992
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 1024
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_1024_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_3/1512886250

Start training
2017-12-10T01:10:54.892510: step 1, loss 7.33035, acc 0.108398, prec 0.018319, recall 0.894737
2017-12-10T01:10:56.620593: step 2, loss 6.16139, acc 0.415039, prec 0.0221643, recall 0.73913
2017-12-10T01:10:58.334863: step 3, loss 9.17422, acc 0.547852, prec 0.0210738, recall 0.636364
2017-12-10T01:11:00.047364: step 4, loss 6.07891, acc 0.461914, prec 0.0196464, recall 0.617284
2017-12-10T01:11:01.760286: step 5, loss 5.08836, acc 0.404297, prec 0.020234, recall 0.633663
2017-12-10T01:11:03.477600: step 6, loss 4.3635, acc 0.348633, prec 0.0185234, recall 0.633929
2017-12-10T01:11:05.189518: step 7, loss 5.58485, acc 0.375, prec 0.0174302, recall 0.629032
2017-12-10T01:11:06.924459: step 8, loss 4.25229, acc 0.37207, prec 0.0173692, recall 0.635714
2017-12-10T01:11:08.618061: step 9, loss 3.09361, acc 0.426758, prec 0.0171389, recall 0.649007
2017-12-10T01:11:10.312879: step 10, loss 4.32452, acc 0.489258, prec 0.0165038, recall 0.639752
2017-12-10T01:11:12.022848: step 11, loss 4.70642, acc 0.570312, prec 0.0179319, recall 0.652174
2017-12-10T01:11:13.724926: step 12, loss 3.06506, acc 0.645508, prec 0.0180014, recall 0.641414
2017-12-10T01:11:15.426398: step 13, loss 3.55682, acc 0.626953, prec 0.0181427, recall 0.642857
2017-12-10T01:11:17.132064: step 14, loss 4.41708, acc 0.635742, prec 0.018419, recall 0.642857
2017-12-10T01:11:18.828963: step 15, loss 3.39121, acc 0.625, prec 0.0188817, recall 0.648536
2017-12-10T01:11:20.533862: step 16, loss 1.99436, acc 0.619141, prec 0.0192821, recall 0.661355
2017-12-10T01:11:22.265565: step 17, loss 2.32806, acc 0.599609, prec 0.0193884, recall 0.667939
2017-12-10T01:11:23.986262: step 18, loss 2.9367, acc 0.605469, prec 0.0197055, recall 0.676364
2017-12-10T01:11:25.694254: step 19, loss 3.90279, acc 0.668945, prec 0.0208376, recall 0.682274
2017-12-10T01:11:27.402058: step 20, loss 3.03322, acc 0.657227, prec 0.0208005, recall 0.680645
2017-12-10T01:11:29.125092: step 21, loss 2.99137, acc 0.636719, prec 0.0209125, recall 0.679012
2017-12-10T01:11:30.889682: step 22, loss 2.53898, acc 0.652344, prec 0.0213137, recall 0.684366
2017-12-10T01:11:32.607383: step 23, loss 4.95066, acc 0.628906, prec 0.0216542, recall 0.677778
2017-12-10T01:11:34.325566: step 24, loss 2.25146, acc 0.592773, prec 0.0214713, recall 0.680217
2017-12-10T01:11:36.057098: step 25, loss 5.38208, acc 0.580078, prec 0.0216118, recall 0.675258
2017-12-10T01:11:37.781271: step 26, loss 3.96909, acc 0.474609, prec 0.0215504, recall 0.677419
2017-12-10T01:11:39.498496: step 27, loss 3.04955, acc 0.461914, prec 0.0212476, recall 0.680387
2017-12-10T01:11:41.192678: step 28, loss 2.65078, acc 0.46582, prec 0.0209724, recall 0.686461
2017-12-10T01:11:42.895876: step 29, loss 2.49894, acc 0.50293, prec 0.0210475, recall 0.69515
2017-12-10T01:11:44.628527: step 30, loss 3.07954, acc 0.584961, prec 0.0214426, recall 0.699115
2017-12-10T01:11:46.353193: step 31, loss 2.809, acc 0.626953, prec 0.0214215, recall 0.701299
2017-12-10T01:11:46.538479: step 32, loss 0.950012, acc 0.730769, prec 0.0214663, recall 0.701944
2017-12-10T01:11:48.256796: step 33, loss 2.3303, acc 0.749023, prec 0.0216869, recall 0.70021
2017-12-10T01:11:49.974343: step 34, loss 3.68508, acc 0.783203, prec 0.0220772, recall 0.69697
2017-12-10T01:11:51.712185: step 35, loss 5.20045, acc 0.793945, prec 0.0221029, recall 0.688976
2017-12-10T01:11:53.441349: step 36, loss 3.71474, acc 0.770508, prec 0.022337, recall 0.685115
2017-12-10T01:11:55.153937: step 37, loss 2.36972, acc 0.723633, prec 0.0225536, recall 0.685874
2017-12-10T01:11:56.864647: step 38, loss 3.44934, acc 0.678711, prec 0.0231695, recall 0.68984
2017-12-10T01:11:58.570627: step 39, loss 2.08992, acc 0.601562, prec 0.0235315, recall 0.697232
2017-12-10T01:12:00.280929: step 40, loss 2.64095, acc 0.517578, prec 0.0235394, recall 0.701014
2017-12-10T01:12:01.985968: step 41, loss 2.61762, acc 0.498047, prec 0.0237348, recall 0.707718
2017-12-10T01:12:03.728557: step 42, loss 2.37088, acc 0.530273, prec 0.0236985, recall 0.712903
2017-12-10T01:12:05.438079: step 43, loss 2.9228, acc 0.53125, prec 0.0235134, recall 0.715421
2017-12-10T01:12:07.168862: step 44, loss 1.96483, acc 0.583984, prec 0.0237001, recall 0.721617
2017-12-10T01:12:08.878970: step 45, loss 2.59533, acc 0.677734, prec 0.0239482, recall 0.723824
2017-12-10T01:12:10.593209: step 46, loss 1.74253, acc 0.716797, prec 0.0241381, recall 0.72619
2017-12-10T01:12:12.297036: step 47, loss 1.66997, acc 0.760742, prec 0.0243759, recall 0.728467
2017-12-10T01:12:14.001101: step 48, loss 1.77174, acc 0.786133, prec 0.0245941, recall 0.728183
2017-12-10T01:12:15.698638: step 49, loss 3.15141, acc 0.830078, prec 0.0250084, recall 0.722992
2017-12-10T01:12:17.404960: step 50, loss 2.47165, acc 0.785156, prec 0.025032, recall 0.719346
2017-12-10T01:12:19.110486: step 51, loss 1.70856, acc 0.779297, prec 0.0252298, recall 0.718291
2017-12-10T01:12:20.814972: step 52, loss 1.68732, acc 0.756836, prec 0.0255282, recall 0.721204
2017-12-10T01:12:22.529912: step 53, loss 2.59384, acc 0.716797, prec 0.0255953, recall 0.720721
2017-12-10T01:12:24.228895: step 54, loss 1.53847, acc 0.671875, prec 0.0254725, recall 0.721939
2017-12-10T01:12:25.941344: step 55, loss 2.8564, acc 0.652344, prec 0.0255502, recall 0.722153
2017-12-10T01:12:27.648938: step 56, loss 1.6934, acc 0.638672, prec 0.0257326, recall 0.726937
2017-12-10T01:12:29.336579: step 57, loss 3.29457, acc 0.628906, prec 0.0258211, recall 0.725632
2017-12-10T01:12:31.093161: step 58, loss 2.12969, acc 0.606445, prec 0.0260822, recall 0.729412
2017-12-10T01:12:32.811931: step 59, loss 1.83361, acc 0.611328, prec 0.0260959, recall 0.732869
2017-12-10T01:12:34.543434: step 60, loss 2.38603, acc 0.607422, prec 0.0260707, recall 0.731735
2017-12-10T01:12:36.300020: step 61, loss 2.20499, acc 0.658203, prec 0.0262145, recall 0.734007
2017-12-10T01:12:38.012851: step 62, loss 2.02, acc 0.649414, prec 0.0265766, recall 0.73713
2017-12-10T01:12:39.741692: step 63, loss 1.80556, acc 0.682617, prec 0.0266963, recall 0.739741
2017-12-10T01:12:39.926937: step 64, loss 1.09776, acc 0.730769, prec 0.0266817, recall 0.739741
2017-12-10T01:12:41.638448: step 65, loss 1.31577, acc 0.762695, prec 0.0267716, recall 0.740662
2017-12-10T01:12:43.348810: step 66, loss 3.16289, acc 0.775391, prec 0.0268823, recall 0.734587
2017-12-10T01:12:45.080607: step 67, loss 1.9404, acc 0.779297, prec 0.0269871, recall 0.734778
2017-12-10T01:12:46.797661: step 68, loss 1.90545, acc 0.795898, prec 0.0273257, recall 0.737323
2017-12-10T01:12:48.510558: step 69, loss 1.94308, acc 0.753906, prec 0.0275079, recall 0.737525
2017-12-10T01:12:50.216967: step 70, loss 1.68231, acc 0.75, prec 0.0278965, recall 0.73998
2017-12-10T01:12:51.958061: step 71, loss 1.61998, acc 0.730469, prec 0.0279723, recall 0.741063
2017-12-10T01:12:53.667564: step 72, loss 3.99072, acc 0.681641, prec 0.028103, recall 0.741445
2017-12-10T01:12:55.392449: step 73, loss 1.94606, acc 0.675781, prec 0.0282562, recall 0.744142
2017-12-10T01:12:57.118999: step 74, loss 2.30053, acc 0.65332, prec 0.0282502, recall 0.743068
2017-12-10T01:12:58.812660: step 75, loss 2.13884, acc 0.649414, prec 0.0283732, recall 0.744313
2017-12-10T01:13:00.526483: step 76, loss 1.64768, acc 0.626953, prec 0.02837, recall 0.745504
2017-12-10T01:13:02.230087: step 77, loss 1.62485, acc 0.634766, prec 0.0285705, recall 0.748673
2017-12-10T01:13:03.942332: step 78, loss 1.89088, acc 0.666016, prec 0.0287316, recall 0.751309
2017-12-10T01:13:05.660468: step 79, loss 2.03924, acc 0.686523, prec 0.0287497, recall 0.752159
2017-12-10T01:13:07.390225: step 80, loss 2.57058, acc 0.711914, prec 0.0289216, recall 0.751912
2017-12-10T01:13:09.112054: step 81, loss 1.5667, acc 0.74707, prec 0.0290268, recall 0.752941
2017-12-10T01:13:10.811383: step 82, loss 2.06042, acc 0.738281, prec 0.0291228, recall 0.753322
2017-12-10T01:13:12.519340: step 83, loss 1.25747, acc 0.757812, prec 0.0293265, recall 0.755537
2017-12-10T01:13:14.223799: step 84, loss 1.67396, acc 0.768555, prec 0.0293848, recall 0.756098
2017-12-10T01:13:15.924425: step 85, loss 2.11562, acc 0.790039, prec 0.0293121, recall 0.754443
2017-12-10T01:13:17.626904: step 86, loss 1.80099, acc 0.764648, prec 0.0293064, recall 0.754006
2017-12-10T01:13:19.325750: step 87, loss 2.11289, acc 0.763672, prec 0.0292709, recall 0.752782
2017-12-10T01:13:21.048679: step 88, loss 1.41223, acc 0.758789, prec 0.0293189, recall 0.753349
2017-12-10T01:13:22.766532: step 89, loss 3.54616, acc 0.738281, prec 0.0295271, recall 0.753297
2017-12-10T01:13:24.486131: step 90, loss 2.41472, acc 0.716797, prec 0.0298008, recall 0.753237
2017-12-10T01:13:26.199865: step 91, loss 1.65784, acc 0.65332, prec 0.029949, recall 0.755639
2017-12-10T01:13:27.898732: step 92, loss 2.42991, acc 0.624023, prec 0.030013, recall 0.755935
2017-12-10T01:13:29.605321: step 93, loss 2.11922, acc 0.584961, prec 0.0299823, recall 0.756975
2017-12-10T01:13:31.335424: step 94, loss 1.63904, acc 0.581055, prec 0.0299193, recall 0.758922
2017-12-10T01:13:33.043649: step 95, loss 1.58686, acc 0.609375, prec 0.0300201, recall 0.761699
2017-12-10T01:13:33.231749: step 96, loss 1.14238, acc 0.711538, prec 0.0300074, recall 0.761699
2017-12-10T01:13:34.949775: step 97, loss 1.50175, acc 0.69043, prec 0.0301498, recall 0.763701
2017-12-10T01:13:36.683472: step 98, loss 1.05905, acc 0.754883, prec 0.0303182, recall 0.765493
2017-12-10T01:13:38.387511: step 99, loss 1.14953, acc 0.791016, prec 0.0305693, recall 0.767571
2017-12-10T01:13:40.107825: step 100, loss 2.98086, acc 0.834961, prec 0.0306733, recall 0.765313
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_3/1512886250/checkpoints/model-100

2017-12-10T01:13:42.875750: step 101, loss 1.6637, acc 0.84668, prec 0.0308115, recall 0.764305
2017-12-10T01:13:44.574858: step 102, loss 1.3163, acc 0.867188, prec 0.0309935, recall 0.762963
2017-12-10T01:13:46.300050: step 103, loss 1.34379, acc 0.829102, prec 0.0309548, recall 0.761554
2017-12-10T01:13:48.043968: step 104, loss 2.62557, acc 0.799805, prec 0.0310241, recall 0.759947
2017-12-10T01:13:49.763752: step 105, loss 1.03837, acc 0.733398, prec 0.0310317, recall 0.76087
2017-12-10T01:13:51.485820: step 106, loss 1.38005, acc 0.72168, prec 0.0310034, recall 0.761624
2017-12-10T01:13:53.212515: step 107, loss 1.50133, acc 0.649414, prec 0.0310174, recall 0.762987
2017-12-10T01:13:54.931440: step 108, loss 1.31099, acc 0.655273, prec 0.0312639, recall 0.765685
2017-12-10T01:13:56.649647: step 109, loss 1.28616, acc 0.662109, prec 0.0313851, recall 0.768061
2017-12-10T01:13:58.346262: step 110, loss 1.20515, acc 0.664062, prec 0.0313815, recall 0.769666
2017-12-10T01:14:00.051840: step 111, loss 1.18167, acc 0.699219, prec 0.0315061, recall 0.77134
2017-12-10T01:14:01.778852: step 112, loss 1.58195, acc 0.776367, prec 0.0316689, recall 0.771887
2017-12-10T01:14:03.488728: step 113, loss 1.37109, acc 0.797852, prec 0.0318459, recall 0.773366
2017-12-10T01:14:05.196510: step 114, loss 0.768439, acc 0.801758, prec 0.0320232, recall 0.775288
2017-12-10T01:14:06.946275: step 115, loss 1.5762, acc 0.841797, prec 0.0321857, recall 0.77551
2017-12-10T01:14:08.651297: step 116, loss 1.94653, acc 0.835938, prec 0.0322717, recall 0.774405
2017-12-10T01:14:10.362709: step 117, loss 1.02989, acc 0.837891, prec 0.0323808, recall 0.774365
2017-12-10T01:14:12.076258: step 118, loss 2.57398, acc 0.842773, prec 0.0325453, recall 0.771879
2017-12-10T01:14:13.790758: step 119, loss 1.97918, acc 0.790039, prec 0.0326148, recall 0.770966
2017-12-10T01:14:15.518054: step 120, loss 1.48224, acc 0.744141, prec 0.0327865, recall 0.771298
2017-12-10T01:14:17.233069: step 121, loss 1.23929, acc 0.673828, prec 0.032779, recall 0.772727
2017-12-10T01:14:18.952860: step 122, loss 1.99646, acc 0.657227, prec 0.0328284, recall 0.774085
2017-12-10T01:14:20.674947: step 123, loss 1.55371, acc 0.632812, prec 0.0328338, recall 0.775727
2017-12-10T01:14:22.417814: step 124, loss 1.51234, acc 0.585938, prec 0.0328023, recall 0.777346
2017-12-10T01:14:24.114379: step 125, loss 1.50488, acc 0.62207, prec 0.0329341, recall 0.77967
2017-12-10T01:14:25.827056: step 126, loss 1.55785, acc 0.676758, prec 0.0329956, recall 0.780926
2017-12-10T01:14:27.536650: step 127, loss 0.91331, acc 0.738281, prec 0.0331473, recall 0.78282
2017-12-10T01:14:27.719853: step 128, loss 1.11874, acc 0.769231, prec 0.0331603, recall 0.782937
2017-12-10T01:14:29.447025: step 129, loss 1.15656, acc 0.837891, prec 0.0332354, recall 0.782725
2017-12-10T01:14:31.226352: step 130, loss 1.1622, acc 0.841797, prec 0.0334438, recall 0.783626
2017-12-10T01:14:32.954523: step 131, loss 1.12696, acc 0.876953, prec 0.0335701, recall 0.783113
2017-12-10T01:14:34.647486: step 132, loss 1.17358, acc 0.891602, prec 0.0336853, recall 0.782495
2017-12-10T01:14:36.394877: step 133, loss 1.66395, acc 0.887695, prec 0.0337106, recall 0.781022
2017-12-10T01:14:38.104524: step 134, loss 0.940983, acc 0.873047, prec 0.0338538, recall 0.780652
2017-12-10T01:14:39.832858: step 135, loss 0.884992, acc 0.849609, prec 0.034062, recall 0.781939
2017-12-10T01:14:41.547030: step 136, loss 1.23718, acc 0.835938, prec 0.0344728, recall 0.784304
2017-12-10T01:14:43.264559: step 137, loss 1.02689, acc 0.814453, prec 0.0346707, recall 0.785643
2017-12-10T01:14:44.982052: step 138, loss 0.947724, acc 0.77832, prec 0.0347966, recall 0.786355
2017-12-10T01:14:46.709553: step 139, loss 0.845596, acc 0.761719, prec 0.0348644, recall 0.787624
2017-12-10T01:14:48.423673: step 140, loss 1.15608, acc 0.740234, prec 0.0350415, recall 0.789112
2017-12-10T01:14:50.145793: step 141, loss 0.953397, acc 0.736328, prec 0.0351082, recall 0.790448
2017-12-10T01:14:51.879125: step 142, loss 1.16853, acc 0.742188, prec 0.0351587, recall 0.791283
2017-12-10T01:14:53.600338: step 143, loss 0.777892, acc 0.791016, prec 0.0353694, recall 0.793087
2017-12-10T01:14:55.312600: step 144, loss 0.786587, acc 0.797852, prec 0.0354601, recall 0.794272
2017-12-10T01:14:57.019509: step 145, loss 0.745553, acc 0.817383, prec 0.0354634, recall 0.794579
2017-12-10T01:14:58.729742: step 146, loss 0.86146, acc 0.827148, prec 0.035515, recall 0.795078
2017-12-10T01:15:00.465871: step 147, loss 1.19616, acc 0.870117, prec 0.0356202, recall 0.795294
2017-12-10T01:15:02.179387: step 148, loss 0.911625, acc 0.878906, prec 0.0357713, recall 0.796071
2017-12-10T01:15:03.898112: step 149, loss 2.31403, acc 0.865234, prec 0.0359365, recall 0.794349
2017-12-10T01:15:05.630319: step 150, loss 2.6065, acc 0.856445, prec 0.0360955, recall 0.791934
2017-12-10T01:15:07.366380: step 151, loss 0.662142, acc 0.838867, prec 0.0362323, recall 0.793166
2017-12-10T01:15:09.076114: step 152, loss 0.947293, acc 0.785156, prec 0.0363874, recall 0.794304
2017-12-10T01:15:10.788689: step 153, loss 0.910643, acc 0.755859, prec 0.0365572, recall 0.795964
2017-12-10T01:15:12.525476: step 154, loss 0.967865, acc 0.713867, prec 0.0366141, recall 0.797237
2017-12-10T01:15:14.239367: step 155, loss 1.07449, acc 0.730469, prec 0.0365856, recall 0.797693
2017-12-10T01:15:15.964337: step 156, loss 1.07858, acc 0.711914, prec 0.0366206, recall 0.798853
2017-12-10T01:15:17.683657: step 157, loss 1.04515, acc 0.726562, prec 0.0368219, recall 0.800349
2017-12-10T01:15:19.400070: step 158, loss 1.00043, acc 0.767578, prec 0.0368785, recall 0.801043
2017-12-10T01:15:21.142330: step 159, loss 1.73082, acc 0.786133, prec 0.0368733, recall 0.800692
2017-12-10T01:15:21.325984: step 160, loss 0.926449, acc 0.788462, prec 0.0369036, recall 0.800864
2017-12-10T01:15:23.036278: step 161, loss 0.998459, acc 0.819336, prec 0.0370554, recall 0.801802
2017-12-10T01:15:24.756819: step 162, loss 0.66744, acc 0.821289, prec 0.0372067, recall 0.803069
2017-12-10T01:15:26.475459: step 163, loss 1.00474, acc 0.84082, prec 0.0372594, recall 0.80314
2017-12-10T01:15:28.186278: step 164, loss 0.645718, acc 0.832031, prec 0.0374924, recall 0.804714
2017-12-10T01:15:29.906297: step 165, loss 0.483896, acc 0.859375, prec 0.0376315, recall 0.805776
2017-12-10T01:15:31.639100: step 166, loss 0.656439, acc 0.876953, prec 0.0377469, recall 0.805995
2017-12-10T01:15:33.352364: step 167, loss 0.60206, acc 0.887695, prec 0.0377762, recall 0.805809
2017-12-10T01:15:35.056547: step 168, loss 1.3715, acc 0.890625, prec 0.0379771, recall 0.805681
2017-12-10T01:15:36.778371: step 169, loss 1.21446, acc 0.883789, prec 0.0383201, recall 0.806517
2017-12-10T01:15:38.490074: step 170, loss 0.558823, acc 0.871094, prec 0.0384081, recall 0.807302
2017-12-10T01:15:40.219998: step 171, loss 0.651916, acc 0.856445, prec 0.0384482, recall 0.807599
2017-12-10T01:15:41.926868: step 172, loss 0.679195, acc 0.81543, prec 0.0385117, recall 0.808451
2017-12-10T01:15:43.657508: step 173, loss 0.821659, acc 0.789062, prec 0.0385188, recall 0.808818
2017-12-10T01:15:45.393900: step 174, loss 1.27782, acc 0.77832, prec 0.0385734, recall 0.809087
2017-12-10T01:15:47.141627: step 175, loss 0.86103, acc 0.789062, prec 0.0387075, recall 0.809976
2017-12-10T01:15:48.857871: step 176, loss 0.887358, acc 0.777344, prec 0.0388498, recall 0.810928
2017-12-10T01:15:50.576710: step 177, loss 0.687592, acc 0.801758, prec 0.0390083, recall 0.812183
2017-12-10T01:15:52.304521: step 178, loss 1.31885, acc 0.801758, prec 0.0391498, recall 0.812403
2017-12-10T01:15:54.011610: step 179, loss 0.6356, acc 0.811523, prec 0.0394382, recall 0.814132
2017-12-10T01:15:55.715229: step 180, loss 0.648853, acc 0.818359, prec 0.0395522, recall 0.814815
2017-12-10T01:15:57.435924: step 181, loss 0.64624, acc 0.832031, prec 0.0396756, recall 0.81549
2017-12-10T01:15:59.148142: step 182, loss 0.768781, acc 0.853516, prec 0.0397436, recall 0.815879
2017-12-10T01:16:00.882846: step 183, loss 1.38666, acc 0.869141, prec 0.0398082, recall 0.814968
2017-12-10T01:16:02.606105: step 184, loss 0.488002, acc 0.858398, prec 0.039896, recall 0.81573
2017-12-10T01:16:04.342060: step 185, loss 0.503457, acc 0.876953, prec 0.0399803, recall 0.816113
2017-12-10T01:16:06.072876: step 186, loss 0.949505, acc 0.859375, prec 0.0401917, recall 0.816735
2017-12-10T01:16:07.791587: step 187, loss 1.22398, acc 0.878906, prec 0.0402952, recall 0.816575
2017-12-10T01:16:09.535591: step 188, loss 0.637408, acc 0.845703, prec 0.0403899, recall 0.816783
2017-12-10T01:16:11.242671: step 189, loss 1.02678, acc 0.848633, prec 0.0405737, recall 0.817024
2017-12-10T01:16:12.947634: step 190, loss 0.50817, acc 0.855469, prec 0.0406211, recall 0.817621
2017-12-10T01:16:14.671103: step 191, loss 0.717264, acc 0.851562, prec 0.0408211, recall 0.818509
2017-12-10T01:16:14.858233: step 192, loss 0.430251, acc 0.884615, prec 0.0408339, recall 0.818574
2017-12-10T01:16:16.574568: step 193, loss 0.924618, acc 0.844727, prec 0.040873, recall 0.818867
2017-12-10T01:16:18.308829: step 194, loss 0.943937, acc 0.824219, prec 0.0409492, recall 0.818766
2017-12-10T01:16:20.040299: step 195, loss 0.938623, acc 0.850586, prec 0.0410276, recall 0.818601
2017-12-10T01:16:21.776637: step 196, loss 0.57557, acc 0.813477, prec 0.0410417, recall 0.819179
2017-12-10T01:16:23.510043: step 197, loss 0.483377, acc 0.867188, prec 0.0412821, recall 0.82045
2017-12-10T01:16:25.219182: step 198, loss 0.873985, acc 0.862305, prec 0.0414345, recall 0.820817
2017-12-10T01:16:26.928618: step 199, loss 0.649198, acc 0.839844, prec 0.041518, recall 0.82128
2017-12-10T01:16:28.660085: step 200, loss 0.950151, acc 0.850586, prec 0.0416762, recall 0.821984
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_3/1512886250/checkpoints/model-200

2017-12-10T01:16:31.656174: step 201, loss 0.433374, acc 0.853516, prec 0.0417679, recall 0.822719
2017-12-10T01:16:33.372207: step 202, loss 0.473302, acc 0.860352, prec 0.041931, recall 0.823691
2017-12-10T01:16:35.090031: step 203, loss 0.487791, acc 0.883789, prec 0.0420941, recall 0.824591
2017-12-10T01:16:36.827941: step 204, loss 0.644534, acc 0.881836, prec 0.0421575, recall 0.824288
2017-12-10T01:16:38.551261: step 205, loss 0.729584, acc 0.913086, prec 0.0423925, recall 0.824798
2017-12-10T01:16:40.282397: step 206, loss 0.527001, acc 0.875, prec 0.0424973, recall 0.825503
2017-12-10T01:16:42.005909: step 207, loss 0.494636, acc 0.898438, prec 0.042686, recall 0.826159
2017-12-10T01:16:43.711339: step 208, loss 0.386329, acc 0.889648, prec 0.0428006, recall 0.826853
2017-12-10T01:16:45.426279: step 209, loss 0.37854, acc 0.898438, prec 0.0428728, recall 0.827095
2017-12-10T01:16:47.164607: step 210, loss 1.01158, acc 0.894531, prec 0.0431729, recall 0.827586
2017-12-10T01:16:48.924079: step 211, loss 0.656782, acc 0.897461, prec 0.0433905, recall 0.828329
2017-12-10T01:16:50.646222: step 212, loss 0.476216, acc 0.860352, prec 0.043562, recall 0.829276
2017-12-10T01:16:52.382270: step 213, loss 0.755059, acc 0.853516, prec 0.0436793, recall 0.82978
2017-12-10T01:16:54.138683: step 214, loss 0.399668, acc 0.861328, prec 0.0438012, recall 0.830547
2017-12-10T01:16:55.840163: step 215, loss 0.807706, acc 0.84375, prec 0.0438628, recall 0.830346
2017-12-10T01:16:57.573605: step 216, loss 0.461581, acc 0.84082, prec 0.0438873, recall 0.830833
2017-12-10T01:16:59.276219: step 217, loss 0.786379, acc 0.84668, prec 0.0440141, recall 0.831111
2017-12-10T01:17:01.015974: step 218, loss 0.852079, acc 0.860352, prec 0.044071, recall 0.830857
2017-12-10T01:17:02.755806: step 219, loss 0.939542, acc 0.827148, prec 0.0441818, recall 0.831132
2017-12-10T01:17:04.462379: step 220, loss 0.460056, acc 0.849609, prec 0.0442277, recall 0.831661
2017-12-10T01:17:06.195736: step 221, loss 0.800165, acc 0.834961, prec 0.0443743, recall 0.832294
2017-12-10T01:17:07.931289: step 222, loss 0.83819, acc 0.833984, prec 0.0444559, recall 0.832713
2017-12-10T01:17:09.645029: step 223, loss 0.560881, acc 0.822266, prec 0.044559, recall 0.833488
2017-12-10T01:17:09.833348: step 224, loss 0.55655, acc 0.807692, prec 0.0446147, recall 0.833693
2017-12-10T01:17:11.580635: step 225, loss 0.518404, acc 0.848633, prec 0.0448154, recall 0.834713
2017-12-10T01:17:13.289925: step 226, loss 0.778321, acc 0.870117, prec 0.0450007, recall 0.835366
2017-12-10T01:17:15.004980: step 227, loss 0.453778, acc 0.894531, prec 0.0451871, recall 0.836215
2017-12-10T01:17:16.707969: step 228, loss 0.588683, acc 0.891602, prec 0.0453557, recall 0.836753
2017-12-10T01:17:18.419626: step 229, loss 0.647805, acc 0.897461, prec 0.0454041, recall 0.836643
2017-12-10T01:17:20.131295: step 230, loss 0.620248, acc 0.899414, prec 0.045516, recall 0.836729
2017-12-10T01:17:21.864004: step 231, loss 0.326212, acc 0.897461, prec 0.0456246, recall 0.837313
2017-12-10T01:17:23.584943: step 232, loss 0.362826, acc 0.930664, prec 0.045758, recall 0.837894
2017-12-10T01:17:25.301173: step 233, loss 0.564587, acc 0.919922, prec 0.046084, recall 0.839091
2017-12-10T01:17:27.020053: step 234, loss 0.846772, acc 0.90918, prec 0.0462169, recall 0.839212
2017-12-10T01:17:28.725905: step 235, loss 0.4666, acc 0.870117, prec 0.0463332, recall 0.839871
2017-12-10T01:17:30.441250: step 236, loss 0.557393, acc 0.875, prec 0.0464535, recall 0.84028
2017-12-10T01:17:32.160072: step 237, loss 0.364803, acc 0.880859, prec 0.0465004, recall 0.840698
2017-12-10T01:17:33.863913: step 238, loss 0.530626, acc 0.888672, prec 0.0466448, recall 0.841389
2017-12-10T01:17:35.584722: step 239, loss 0.514734, acc 0.845703, prec 0.0467557, recall 0.842075
2017-12-10T01:17:37.299829: step 240, loss 0.859491, acc 0.869141, prec 0.0468246, recall 0.84209
2017-12-10T01:17:39.024146: step 241, loss 0.686496, acc 0.848633, prec 0.0468919, recall 0.842391
2017-12-10T01:17:40.737806: step 242, loss 0.603091, acc 0.849609, prec 0.0470655, recall 0.843003
2017-12-10T01:17:42.468780: step 243, loss 0.562456, acc 0.84668, prec 0.0471447, recall 0.843582
2017-12-10T01:17:44.204235: step 244, loss 0.485051, acc 0.864258, prec 0.0472218, recall 0.844112
2017-12-10T01:17:45.916914: step 245, loss 0.572275, acc 0.869141, prec 0.0473323, recall 0.844726
2017-12-10T01:17:47.639631: step 246, loss 0.662838, acc 0.880859, prec 0.047482, recall 0.845185
2017-12-10T01:17:49.374012: step 247, loss 0.501954, acc 0.867188, prec 0.0475608, recall 0.845467
2017-12-10T01:17:51.137335: step 248, loss 1.29395, acc 0.887695, prec 0.0476273, recall 0.844957
2017-12-10T01:17:52.854408: step 249, loss 1.09995, acc 0.87793, prec 0.0477731, recall 0.845409
2017-12-10T01:17:54.568715: step 250, loss 1.12535, acc 0.854492, prec 0.0479612, recall 0.845562
2017-12-10T01:17:56.278699: step 251, loss 0.544926, acc 0.813477, prec 0.047952, recall 0.845943
2017-12-10T01:17:57.982965: step 252, loss 0.526754, acc 0.821289, prec 0.0479341, recall 0.84628
2017-12-10T01:17:59.700170: step 253, loss 0.498674, acc 0.811523, prec 0.0479382, recall 0.846699
2017-12-10T01:18:01.431747: step 254, loss 0.688191, acc 0.81543, prec 0.0480486, recall 0.847177
2017-12-10T01:18:03.140794: step 255, loss 0.605897, acc 0.849609, prec 0.0482128, recall 0.847961
2017-12-10T01:18:03.328756: step 256, loss 0.224436, acc 0.923077, prec 0.0482244, recall 0.848002
2017-12-10T01:18:05.067649: step 257, loss 0.379229, acc 0.875977, prec 0.0483493, recall 0.848615
2017-12-10T01:18:06.879625: step 258, loss 0.439157, acc 0.875, prec 0.0484584, recall 0.849183
2017-12-10T01:18:08.573008: step 259, loss 0.288291, acc 0.908203, prec 0.0485631, recall 0.849666
2017-12-10T01:18:10.267944: step 260, loss 0.657689, acc 0.915039, prec 0.0487177, recall 0.849814
2017-12-10T01:18:11.982577: step 261, loss 0.30568, acc 0.929688, prec 0.048838, recall 0.850291
2017-12-10T01:18:13.717311: step 262, loss 0.285856, acc 0.916016, prec 0.0488898, recall 0.850608
2017-12-10T01:18:15.459194: step 263, loss 0.471009, acc 0.929688, prec 0.0490689, recall 0.850789
2017-12-10T01:18:17.175773: step 264, loss 0.855592, acc 0.933594, prec 0.0492526, recall 0.850301
2017-12-10T01:18:18.898822: step 265, loss 1.17203, acc 0.928711, prec 0.0493883, recall 0.849922
2017-12-10T01:18:20.618475: step 266, loss 0.648998, acc 0.916016, prec 0.0495548, recall 0.850324
2017-12-10T01:18:22.366340: step 267, loss 0.571232, acc 0.908203, prec 0.0496861, recall 0.850646
2017-12-10T01:18:24.084771: step 268, loss 0.512474, acc 0.847656, prec 0.0497704, recall 0.850965
2017-12-10T01:18:25.810812: step 269, loss 0.487161, acc 0.856445, prec 0.0498461, recall 0.851462
2017-12-10T01:18:27.524317: step 270, loss 0.46631, acc 0.849609, prec 0.0499161, recall 0.851956
2017-12-10T01:18:29.254660: step 271, loss 0.549849, acc 0.825195, prec 0.0499529, recall 0.852409
2017-12-10T01:18:30.998448: step 272, loss 0.627348, acc 0.837891, prec 0.0500708, recall 0.852829
2017-12-10T01:18:32.705993: step 273, loss 0.604463, acc 0.81543, prec 0.0501136, recall 0.853313
2017-12-10T01:18:34.426407: step 274, loss 0.435811, acc 0.874023, prec 0.0501586, recall 0.853683
2017-12-10T01:18:36.187013: step 275, loss 0.418834, acc 0.874023, prec 0.0502455, recall 0.854161
2017-12-10T01:18:37.898210: step 276, loss 1.06335, acc 0.874023, prec 0.0503343, recall 0.853994
2017-12-10T01:18:39.616383: step 277, loss 0.612988, acc 0.891602, prec 0.0505193, recall 0.85426
2017-12-10T01:18:41.316615: step 278, loss 0.619997, acc 0.898438, prec 0.0506949, recall 0.854487
2017-12-10T01:18:43.020465: step 279, loss 0.357606, acc 0.893555, prec 0.0508228, recall 0.855026
2017-12-10T01:18:44.732918: step 280, loss 0.317275, acc 0.914062, prec 0.0510355, recall 0.855739
2017-12-10T01:18:46.451794: step 281, loss 0.328021, acc 0.90332, prec 0.0511421, recall 0.856198
2017-12-10T01:18:48.162149: step 282, loss 0.391413, acc 0.912109, prec 0.0512967, recall 0.856759
2017-12-10T01:18:49.887485: step 283, loss 0.494745, acc 0.912109, prec 0.0514792, recall 0.857178
2017-12-10T01:18:51.619452: step 284, loss 0.415176, acc 0.93457, prec 0.0516232, recall 0.857454
2017-12-10T01:18:53.343756: step 285, loss 0.297739, acc 0.907227, prec 0.0516622, recall 0.857729
2017-12-10T01:18:55.044848: step 286, loss 0.371911, acc 0.918945, prec 0.0517522, recall 0.8579
2017-12-10T01:18:56.744607: step 287, loss 0.343585, acc 0.916992, prec 0.051868, recall 0.858137
2017-12-10T01:18:56.929933: step 288, loss 1.28556, acc 0.942308, prec 0.0518795, recall 0.858171
2017-12-10T01:18:58.666373: step 289, loss 0.596053, acc 0.924805, prec 0.0520696, recall 0.858576
2017-12-10T01:19:00.387857: step 290, loss 0.485046, acc 0.900391, prec 0.0521581, recall 0.858776
2017-12-10T01:19:02.099595: step 291, loss 0.52016, acc 0.87207, prec 0.0523201, recall 0.859208
2017-12-10T01:19:03.814446: step 292, loss 0.363748, acc 0.876953, prec 0.0524026, recall 0.859641
2017-12-10T01:19:05.531032: step 293, loss 0.401626, acc 0.860352, prec 0.0524584, recall 0.860038
2017-12-10T01:19:07.267163: step 294, loss 0.435269, acc 0.883789, prec 0.0525863, recall 0.860563
2017-12-10T01:19:08.971809: step 295, loss 0.580713, acc 0.868164, prec 0.0526346, recall 0.860721
2017-12-10T01:19:10.688221: step 296, loss 0.487275, acc 0.883789, prec 0.0527895, recall 0.861105
2017-12-10T01:19:12.390468: step 297, loss 0.403948, acc 0.864258, prec 0.0528469, recall 0.861492
2017-12-10T01:19:14.090073: step 298, loss 0.34589, acc 0.87207, prec 0.0529775, recall 0.862037
2017-12-10T01:19:15.788525: step 299, loss 0.34901, acc 0.905273, prec 0.0530659, recall 0.862419
2017-12-10T01:19:17.508215: step 300, loss 0.458391, acc 0.922852, prec 0.0532087, recall 0.862696

Evaluation:
2017-12-10T01:19:22.242845: step 300, loss 2.0292, acc 0.936592, prec 0.0537263, recall 0.84899

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_1024_fold_3/1512886250/checkpoints/model-300

2017-12-10T01:19:24.996324: step 301, loss 0.266448, acc 0.926758, prec 0.0538689, recall 0.849491
2017-12-10T01:19:26.715663: step 302, loss 0.312706, acc 0.935547, prec 0.0540181, recall 0.849989
2017-12-10T01:19:28.427050: step 303, loss 0.294511, acc 0.942383, prec 0.0542119, recall 0.850582
2017-12-10T01:19:30.136917: step 304, loss 0.598508, acc 0.942383, prec 0.0544201, recall 0.850831
2017-12-10T01:19:31.866318: step 305, loss 0.44102, acc 0.938477, prec 0.0545447, recall 0.851068
2017-12-10T01:19:33.586375: step 306, loss 0.381862, acc 0.933594, prec 0.0546125, recall 0.851175
2017-12-10T01:19:35.311259: step 307, loss 1.25457, acc 0.926758, prec 0.0547181, recall 0.850455
2017-12-10T01:19:37.044762: step 308, loss 0.333956, acc 0.90918, prec 0.0547526, recall 0.850714
2017-12-10T01:19:38.766445: step 309, loss 0.682546, acc 0.898438, prec 0.0548983, recall 0.850894
2017-12-10T01:19:40.515436: step 310, loss 0.438851, acc 0.87207, prec 0.0550214, recall 0.851438
2017-12-10T01:19:42.234150: step 311, loss 0.483764, acc 0.84668, prec 0.0551111, recall 0.851947
2017-12-10T01:19:43.953113: step 312, loss 0.544783, acc 0.817383, prec 0.0550863, recall 0.852232
2017-12-10T01:19:45.656242: step 313, loss 0.558727, acc 0.819336, prec 0.0551931, recall 0.852829
2017-12-10T01:19:47.388041: step 314, loss 0.498011, acc 0.84375, prec 0.0552535, recall 0.853265
2017-12-10T01:19:49.129504: step 315, loss 0.491001, acc 0.845703, prec 0.0554056, recall 0.853916
2017-12-10T01:19:50.866032: step 316, loss 0.628904, acc 0.852539, prec 0.0554083, recall 0.854013
2017-12-10T01:19:52.610719: step 317, loss 0.350695, acc 0.886719, prec 0.0554623, recall 0.854351
2017-12-10T01:19:54.319730: step 318, loss 0.373373, acc 0.891602, prec 0.0555586, recall 0.854778
2017-12-10T01:19:56.044353: step 319, loss 0.332921, acc 0.908203, prec 0.0556545, recall 0.855172
2017-12-10T01:19:56.230158: step 320, loss 0.215937, acc 0.903846, prec 0.0556508, recall 0.855172
Training finished
Starting Experiment - batch_size_2048 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 2048
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_2048_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_0/1512886797

Start training
2017-12-10T01:20:03.180955: step 1, loss 7.95425, acc 0.804688, prec 0.0235602, recall 0.25
2017-12-10T01:20:06.520052: step 2, loss 4.55924, acc 0.477051, prec 0.0164835, recall 0.380952
2017-12-10T01:20:09.830289: step 3, loss 5.09943, acc 0.247559, prec 0.0159151, recall 0.521739
2017-12-10T01:20:13.160053: step 4, loss 5.71517, acc 0.205078, prec 0.0152262, recall 0.596639
2017-12-10T01:20:16.466432: step 5, loss 6.48411, acc 0.218262, prec 0.0146543, recall 0.62585
2017-12-10T01:20:19.834173: step 6, loss 4.32488, acc 0.294434, prec 0.014856, recall 0.657143
2017-12-10T01:20:23.172909: step 7, loss 4.15418, acc 0.427734, prec 0.0157842, recall 0.681159
2017-12-10T01:20:26.521906: step 8, loss 3.72517, acc 0.56543, prec 0.0157745, recall 0.668103
2017-12-10T01:20:29.840332: step 9, loss 3.27308, acc 0.705566, prec 0.0162976, recall 0.653846
2017-12-10T01:20:33.160272: step 10, loss 6.02582, acc 0.751465, prec 0.0165569, recall 0.624138
2017-12-10T01:20:36.499148: step 11, loss 4.983, acc 0.746582, prec 0.0167803, recall 0.598131
2017-12-10T01:20:39.822816: step 12, loss 2.92997, acc 0.700195, prec 0.0169998, recall 0.59593
2017-12-10T01:20:43.159851: step 13, loss 3.65342, acc 0.626465, prec 0.0173893, recall 0.586842
2017-12-10T01:20:46.467878: step 14, loss 3.22152, acc 0.556152, prec 0.0177519, recall 0.595122
2017-12-10T01:20:49.795490: step 15, loss 3.66855, acc 0.47168, prec 0.0181181, recall 0.611364
2017-12-10T01:20:51.603359: step 16, loss 4.38455, acc 0.44186, prec 0.0181795, recall 0.618943
2017-12-10T01:20:54.966997: step 17, loss 3.83252, acc 0.400879, prec 0.0185562, recall 0.632653
2017-12-10T01:20:58.312043: step 18, loss 3.23601, acc 0.394531, prec 0.0189141, recall 0.653846
2017-12-10T01:21:01.660640: step 19, loss 3.19224, acc 0.439453, prec 0.0193724, recall 0.668468
2017-12-10T01:21:04.953380: step 20, loss 3.25019, acc 0.509766, prec 0.0196768, recall 0.677474
2017-12-10T01:21:08.268989: step 21, loss 3.94637, acc 0.55957, prec 0.0200957, recall 0.684976
2017-12-10T01:21:11.593755: step 22, loss 2.67351, acc 0.632324, prec 0.0201711, recall 0.686916
2017-12-10T01:21:14.915749: step 23, loss 2.74879, acc 0.691895, prec 0.0204417, recall 0.685544
2017-12-10T01:21:18.224488: step 24, loss 2.54223, acc 0.718262, prec 0.0211661, recall 0.692635
2017-12-10T01:21:21.550932: step 25, loss 2.48, acc 0.729004, prec 0.0214645, recall 0.691156
2017-12-10T01:21:24.850275: step 26, loss 3.54249, acc 0.733398, prec 0.0217993, recall 0.688396
2017-12-10T01:21:28.162570: step 27, loss 1.60247, acc 0.750488, prec 0.0222231, recall 0.692695
2017-12-10T01:21:31.495738: step 28, loss 2.44942, acc 0.73584, prec 0.0221809, recall 0.689189
2017-12-10T01:21:34.826791: step 29, loss 1.98992, acc 0.727051, prec 0.0223125, recall 0.689367
2017-12-10T01:21:38.223503: step 30, loss 2.22458, acc 0.702637, prec 0.0225822, recall 0.689735
2017-12-10T01:21:41.570131: step 31, loss 3.33806, acc 0.674316, prec 0.0226494, recall 0.686384
2017-12-10T01:21:43.367753: step 32, loss 2.30262, acc 0.637209, prec 0.0226852, recall 0.688326
2017-12-10T01:21:46.684768: step 33, loss 2.46907, acc 0.622559, prec 0.0228291, recall 0.691979
2017-12-10T01:21:50.015460: step 34, loss 2.56156, acc 0.592285, prec 0.023118, recall 0.696594
2017-12-10T01:21:53.368653: step 35, loss 1.98835, acc 0.592773, prec 0.0235188, recall 0.704183
2017-12-10T01:21:56.689805: step 36, loss 2.15462, acc 0.581543, prec 0.0236264, recall 0.709709
2017-12-10T01:21:59.993533: step 37, loss 2.55362, acc 0.626465, prec 0.0238606, recall 0.713478
2017-12-10T01:22:03.303057: step 38, loss 2.23783, acc 0.669922, prec 0.0239696, recall 0.714811
2017-12-10T01:22:06.629285: step 39, loss 2.46013, acc 0.674805, prec 0.0241102, recall 0.716981
2017-12-10T01:22:09.950035: step 40, loss 2.05573, acc 0.685059, prec 0.0244343, recall 0.719895
2017-12-10T01:22:13.255324: step 41, loss 1.37074, acc 0.71875, prec 0.0247075, recall 0.724403
2017-12-10T01:22:16.664116: step 42, loss 1.56855, acc 0.744629, prec 0.0252642, recall 0.727723
2017-12-10T01:22:19.978516: step 43, loss 3.01757, acc 0.758789, prec 0.0253615, recall 0.72361
2017-12-10T01:22:23.278302: step 44, loss 2.78458, acc 0.776855, prec 0.0254823, recall 0.719685
2017-12-10T01:22:26.698372: step 45, loss 1.70047, acc 0.773926, prec 0.0256699, recall 0.721578
2017-12-10T01:22:30.025375: step 46, loss 2.19822, acc 0.74707, prec 0.0258413, recall 0.723065
2017-12-10T01:22:33.447002: step 47, loss 1.47987, acc 0.737305, prec 0.0259931, recall 0.725037
2017-12-10T01:22:35.260340: step 48, loss 2.3123, acc 0.715349, prec 0.0261478, recall 0.72467
2017-12-10T01:22:38.639666: step 49, loss 2.2193, acc 0.694824, prec 0.0261327, recall 0.724188
2017-12-10T01:22:41.999964: step 50, loss 1.88192, acc 0.64502, prec 0.0263474, recall 0.72708
2017-12-10T01:22:45.311780: step 51, loss 1.53413, acc 0.648926, prec 0.0265804, recall 0.73255
2017-12-10T01:22:48.608612: step 52, loss 1.81991, acc 0.646484, prec 0.0268508, recall 0.737162
2017-12-10T01:22:51.985729: step 53, loss 2.3285, acc 0.664062, prec 0.0270676, recall 0.738614
2017-12-10T01:22:55.294772: step 54, loss 1.91899, acc 0.67334, prec 0.0271042, recall 0.738651
2017-12-10T01:22:58.608975: step 55, loss 1.65727, acc 0.700684, prec 0.0271968, recall 0.739796
2017-12-10T01:23:01.956289: step 56, loss 2.35199, acc 0.70166, prec 0.0272007, recall 0.738394
2017-12-10T01:23:05.284021: step 57, loss 1.91091, acc 0.737305, prec 0.027315, recall 0.738433
2017-12-10T01:23:08.627284: step 58, loss 1.60481, acc 0.754883, prec 0.0273392, recall 0.737675
2017-12-10T01:23:11.933187: step 59, loss 1.38235, acc 0.76416, prec 0.0277858, recall 0.740345
2017-12-10T01:23:15.278851: step 60, loss 1.48461, acc 0.744141, prec 0.0278965, recall 0.741218
2017-12-10T01:23:18.610439: step 61, loss 1.54857, acc 0.760742, prec 0.0280677, recall 0.742363
2017-12-10T01:23:21.941365: step 62, loss 2.54868, acc 0.754395, prec 0.0283129, recall 0.742793
2017-12-10T01:23:25.260381: step 63, loss 1.31196, acc 0.738281, prec 0.0284674, recall 0.744847
2017-12-10T01:23:27.036763: step 64, loss 1.46757, acc 0.76093, prec 0.0286845, recall 0.746145
2017-12-10T01:23:30.388322: step 65, loss 1.60786, acc 0.736816, prec 0.0290131, recall 0.749325
2017-12-10T01:23:33.712923: step 66, loss 1.29998, acc 0.730469, prec 0.0291248, recall 0.751467
2017-12-10T01:23:37.070962: step 67, loss 1.66349, acc 0.738281, prec 0.0293637, recall 0.75354
2017-12-10T01:23:40.417463: step 68, loss 1.27342, acc 0.753418, prec 0.0295747, recall 0.756463
2017-12-10T01:23:43.737538: step 69, loss 1.56338, acc 0.759277, prec 0.0297525, recall 0.757128
2017-12-10T01:23:47.070183: step 70, loss 0.871553, acc 0.78418, prec 0.029915, recall 0.759819
2017-12-10T01:23:50.428941: step 71, loss 1.7053, acc 0.779297, prec 0.0299965, recall 0.759324
2017-12-10T01:23:53.769735: step 72, loss 2.45261, acc 0.796875, prec 0.0301755, recall 0.757828
2017-12-10T01:23:57.098890: step 73, loss 1.31435, acc 0.797852, prec 0.0306296, recall 0.760671
2017-12-10T01:24:00.429164: step 74, loss 0.897514, acc 0.770996, prec 0.0308746, recall 0.763482
2017-12-10T01:24:03.754157: step 75, loss 1.60604, acc 0.779297, prec 0.0310545, recall 0.763993
2017-12-10T01:24:07.096335: step 76, loss 1.44493, acc 0.765137, prec 0.0312142, recall 0.764489
2017-12-10T01:24:10.382886: step 77, loss 1.09929, acc 0.734863, prec 0.0312965, recall 0.766151
2017-12-10T01:24:13.687902: step 78, loss 1.50869, acc 0.720703, prec 0.0314855, recall 0.768161
2017-12-10T01:24:17.030720: step 79, loss 1.33382, acc 0.748535, prec 0.0316157, recall 0.769265
2017-12-10T01:24:18.825415: step 80, loss 1.34659, acc 0.745116, prec 0.0316523, recall 0.770044
2017-12-10T01:24:22.151679: step 81, loss 1.08942, acc 0.77002, prec 0.0317856, recall 0.771678
2017-12-10T01:24:25.470847: step 82, loss 1.18926, acc 0.769531, prec 0.0319344, recall 0.772708
2017-12-10T01:24:28.773586: step 83, loss 1.32751, acc 0.805664, prec 0.0322085, recall 0.773865
2017-12-10T01:24:32.106866: step 84, loss 1.25695, acc 0.804688, prec 0.032461, recall 0.774571
2017-12-10T01:24:35.409061: step 85, loss 1.2923, acc 0.810547, prec 0.0327504, recall 0.775443
2017-12-10T01:24:38.754476: step 86, loss 1.22139, acc 0.801758, prec 0.0329418, recall 0.775834
2017-12-10T01:24:42.052279: step 87, loss 1.39653, acc 0.820801, prec 0.0332338, recall 0.777599
2017-12-10T01:24:45.379758: step 88, loss 1.42697, acc 0.800781, prec 0.0334168, recall 0.778571
2017-12-10T01:24:48.735567: step 89, loss 1.02293, acc 0.766113, prec 0.0335073, recall 0.779568
2017-12-10T01:24:52.137579: step 90, loss 1.24828, acc 0.75, prec 0.0336753, recall 0.780753
2017-12-10T01:24:55.460922: step 91, loss 1.19815, acc 0.77832, prec 0.0337612, recall 0.781022
2017-12-10T01:24:58.766546: step 92, loss 0.995409, acc 0.773438, prec 0.0338221, recall 0.782393
2017-12-10T01:25:02.067076: step 93, loss 1.01596, acc 0.779785, prec 0.034064, recall 0.784048
2017-12-10T01:25:05.404301: step 94, loss 0.884713, acc 0.809082, prec 0.0343199, recall 0.785582
2017-12-10T01:25:08.801938: step 95, loss 0.936729, acc 0.804199, prec 0.0344422, recall 0.786451
2017-12-10T01:25:10.599205: step 96, loss 0.925991, acc 0.807442, prec 0.0344368, recall 0.786711
2017-12-10T01:25:13.919923: step 97, loss 0.860419, acc 0.821289, prec 0.0346535, recall 0.787945
2017-12-10T01:25:17.237933: step 98, loss 1.10838, acc 0.837891, prec 0.034917, recall 0.789304
2017-12-10T01:25:20.559909: step 99, loss 0.630174, acc 0.84668, prec 0.0350955, recall 0.790466
2017-12-10T01:25:23.894050: step 100, loss 1.42003, acc 0.857422, prec 0.0353331, recall 0.790158
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_0/1512886797/checkpoints/model-100

2017-12-10T01:25:28.582135: step 101, loss 1.15215, acc 0.851562, prec 0.0355305, recall 0.790261
2017-12-10T01:25:31.952213: step 102, loss 1.21316, acc 0.841309, prec 0.0357143, recall 0.790361
2017-12-10T01:25:35.268319: step 103, loss 0.825626, acc 0.798828, prec 0.0358754, recall 0.791681
2017-12-10T01:25:38.623705: step 104, loss 1.02701, acc 0.813965, prec 0.0360828, recall 0.792313
2017-12-10T01:25:41.961283: step 105, loss 0.995151, acc 0.796875, prec 0.0363847, recall 0.79401
2017-12-10T01:25:45.286691: step 106, loss 1.11654, acc 0.75293, prec 0.0365452, recall 0.79526
2017-12-10T01:25:48.600284: step 107, loss 0.976578, acc 0.765625, prec 0.0366006, recall 0.796472
2017-12-10T01:25:51.975205: step 108, loss 1.23908, acc 0.74707, prec 0.0365924, recall 0.796951
2017-12-10T01:25:55.297659: step 109, loss 1.14532, acc 0.762695, prec 0.0367726, recall 0.798203
2017-12-10T01:25:58.581243: step 110, loss 1.0424, acc 0.79834, prec 0.0368483, recall 0.79879
2017-12-10T01:26:01.938091: step 111, loss 0.833323, acc 0.794434, prec 0.0370019, recall 0.800252
2017-12-10T01:26:03.722756: step 112, loss 0.837774, acc 0.783256, prec 0.0369756, recall 0.800189
2017-12-10T01:26:07.085842: step 113, loss 0.831847, acc 0.831055, prec 0.0373209, recall 0.802051
2017-12-10T01:26:10.392494: step 114, loss 1.12, acc 0.851074, prec 0.0374363, recall 0.802283
2017-12-10T01:26:13.679869: step 115, loss 1.00174, acc 0.862305, prec 0.0375228, recall 0.801838
2017-12-10T01:26:16.972608: step 116, loss 1.13735, acc 0.861816, prec 0.0377859, recall 0.802424
2017-12-10T01:26:20.269974: step 117, loss 0.849525, acc 0.864258, prec 0.0378986, recall 0.802588
2017-12-10T01:26:23.613723: step 118, loss 1.09011, acc 0.873047, prec 0.0381572, recall 0.802859
2017-12-10T01:26:26.929831: step 119, loss 0.800967, acc 0.847168, prec 0.03837, recall 0.803777
2017-12-10T01:26:30.244261: step 120, loss 0.710308, acc 0.818848, prec 0.0385488, recall 0.804914
2017-12-10T01:26:33.540957: step 121, loss 0.674355, acc 0.810059, prec 0.0387015, recall 0.806442
2017-12-10T01:26:36.894202: step 122, loss 1.06629, acc 0.819824, prec 0.0387578, recall 0.807038
2017-12-10T01:26:40.202013: step 123, loss 0.958461, acc 0.815918, prec 0.0389018, recall 0.808011
2017-12-10T01:26:43.565301: step 124, loss 0.974558, acc 0.817871, prec 0.0390875, recall 0.808444
2017-12-10T01:26:46.883589: step 125, loss 1.00856, acc 0.83252, prec 0.0391684, recall 0.80884
2017-12-10T01:26:50.210296: step 126, loss 0.641657, acc 0.818848, prec 0.0393361, recall 0.810335
2017-12-10T01:26:53.558397: step 127, loss 0.799181, acc 0.834961, prec 0.0395088, recall 0.810856
2017-12-10T01:26:55.329842: step 128, loss 0.608448, acc 0.84186, prec 0.0396899, recall 0.811949
2017-12-10T01:26:58.733362: step 129, loss 0.774373, acc 0.841309, prec 0.039866, recall 0.812671
2017-12-10T01:27:02.076815: step 130, loss 0.62581, acc 0.864746, prec 0.0400918, recall 0.813481
2017-12-10T01:27:05.380048: step 131, loss 0.6864, acc 0.859863, prec 0.0403364, recall 0.814159
2017-12-10T01:27:08.736030: step 132, loss 0.642026, acc 0.878418, prec 0.0404343, recall 0.814183
2017-12-10T01:27:12.060134: step 133, loss 0.655609, acc 0.876465, prec 0.0406559, recall 0.814697
2017-12-10T01:27:15.374292: step 134, loss 0.704285, acc 0.86377, prec 0.0408488, recall 0.815368
2017-12-10T01:27:18.705756: step 135, loss 0.799326, acc 0.853516, prec 0.0410534, recall 0.816337
2017-12-10T01:27:22.060698: step 136, loss 0.498088, acc 0.866211, prec 0.0411818, recall 0.817382
2017-12-10T01:27:25.387010: step 137, loss 0.702645, acc 0.859863, prec 0.0412789, recall 0.817691
2017-12-10T01:27:28.679011: step 138, loss 0.69687, acc 0.856934, prec 0.0414345, recall 0.818019
2017-12-10T01:27:32.038135: step 139, loss 0.717505, acc 0.848145, prec 0.0415782, recall 0.818758
2017-12-10T01:27:35.360328: step 140, loss 0.501669, acc 0.850586, prec 0.0418082, recall 0.820216
2017-12-10T01:27:38.688013: step 141, loss 0.525574, acc 0.852051, prec 0.0418672, recall 0.820821
2017-12-10T01:27:42.047029: step 142, loss 0.579412, acc 0.862305, prec 0.0420714, recall 0.821703
2017-12-10T01:27:45.382605: step 143, loss 1.00494, acc 0.872559, prec 0.0423349, recall 0.822343
2017-12-10T01:27:47.187918: step 144, loss 0.981175, acc 0.875349, prec 0.0424832, recall 0.82232
2017-12-10T01:27:50.543107: step 145, loss 0.74153, acc 0.855469, prec 0.0425912, recall 0.822871
2017-12-10T01:27:53.868067: step 146, loss 0.607946, acc 0.855957, prec 0.0428425, recall 0.824125
2017-12-10T01:27:57.174054: step 147, loss 0.663639, acc 0.85791, prec 0.0430106, recall 0.824868
2017-12-10T01:28:00.576616: step 148, loss 0.802056, acc 0.842285, prec 0.0430775, recall 0.825113
2017-12-10T01:28:03.993929: step 149, loss 0.511918, acc 0.847656, prec 0.0432192, recall 0.82619
2017-12-10T01:28:07.333576: step 150, loss 0.638357, acc 0.84668, prec 0.0434182, recall 0.827262
2017-12-10T01:28:10.652324: step 151, loss 0.746259, acc 0.858398, prec 0.0436764, recall 0.828092
2017-12-10T01:28:13.967652: step 152, loss 0.713985, acc 0.851562, prec 0.0437959, recall 0.828664
2017-12-10T01:28:17.263971: step 153, loss 0.61215, acc 0.865234, prec 0.0441153, recall 0.830046
2017-12-10T01:28:20.600837: step 154, loss 0.465029, acc 0.862793, prec 0.0442671, recall 0.830864
2017-12-10T01:28:23.955921: step 155, loss 0.621853, acc 0.862793, prec 0.0443847, recall 0.830992
2017-12-10T01:28:27.295616: step 156, loss 0.550273, acc 0.861816, prec 0.0445449, recall 0.831833
2017-12-10T01:28:30.621732: step 157, loss 0.752125, acc 0.875488, prec 0.0446965, recall 0.832401
2017-12-10T01:28:33.924230: step 158, loss 0.497331, acc 0.885742, prec 0.0448464, recall 0.833111
2017-12-10T01:28:37.312728: step 159, loss 0.627053, acc 0.886719, prec 0.0449644, recall 0.832965
2017-12-10T01:28:39.119577: step 160, loss 0.607178, acc 0.893954, prec 0.0451093, recall 0.83326
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 2048
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_2048_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_1/1512887320

Start training
2017-12-10T01:28:46.042677: step 1, loss 6.02294, acc 0.656738, prec 0.0242857, recall 0.459459
2017-12-10T01:28:49.372239: step 2, loss 4.53939, acc 0.337891, prec 0.0159729, recall 0.559322
2017-12-10T01:28:52.686637: step 3, loss 5.58349, acc 0.300781, prec 0.0156695, recall 0.604396
2017-12-10T01:28:56.034516: step 4, loss 5.2691, acc 0.378418, prec 0.0174891, recall 0.651163
2017-12-10T01:28:59.345953: step 5, loss 4.93375, acc 0.439941, prec 0.0182794, recall 0.656627
2017-12-10T01:29:02.692606: step 6, loss 4.30063, acc 0.490234, prec 0.0174037, recall 0.645503
2017-12-10T01:29:06.002785: step 7, loss 5.31789, acc 0.559082, prec 0.0171934, recall 0.618182
2017-12-10T01:29:09.320379: step 8, loss 5.918, acc 0.512207, prec 0.0166217, recall 0.594378
2017-12-10T01:29:12.634406: step 9, loss 5.09769, acc 0.487305, prec 0.0170631, recall 0.598592
2017-12-10T01:29:15.929528: step 10, loss 4.78652, acc 0.429199, prec 0.0169659, recall 0.601911
2017-12-10T01:29:19.237526: step 11, loss 4.76705, acc 0.361816, prec 0.0168553, recall 0.610465
2017-12-10T01:29:22.570610: step 12, loss 4.82268, acc 0.343262, prec 0.0166401, recall 0.628415
2017-12-10T01:29:25.883649: step 13, loss 4.19396, acc 0.390625, prec 0.0171625, recall 0.64268
2017-12-10T01:29:29.176483: step 14, loss 3.31207, acc 0.471191, prec 0.0174756, recall 0.656613
2017-12-10T01:29:32.528212: step 15, loss 2.20266, acc 0.599609, prec 0.017618, recall 0.665188
2017-12-10T01:29:34.321391: step 16, loss 4.00298, acc 0.67814, prec 0.0178941, recall 0.665953
2017-12-10T01:29:37.654985: step 17, loss 4.40179, acc 0.738281, prec 0.0183624, recall 0.655378
2017-12-10T01:29:40.979365: step 18, loss 2.13575, acc 0.750488, prec 0.0188188, recall 0.658444
2017-12-10T01:29:44.285491: step 19, loss 4.07464, acc 0.728516, prec 0.0188481, recall 0.650909
2017-12-10T01:29:47.614601: step 20, loss 3.58967, acc 0.717285, prec 0.0195548, recall 0.65247
2017-12-10T01:29:50.932427: step 21, loss 2.20674, acc 0.665039, prec 0.0201547, recall 0.661812
2017-12-10T01:29:54.228969: step 22, loss 2.58713, acc 0.625977, prec 0.0207266, recall 0.673344
2017-12-10T01:29:57.542285: step 23, loss 3.068, acc 0.604492, prec 0.0210853, recall 0.677419
2017-12-10T01:30:00.881398: step 24, loss 2.97457, acc 0.569824, prec 0.0211357, recall 0.679831
2017-12-10T01:30:04.190464: step 25, loss 3.45851, acc 0.567871, prec 0.0214686, recall 0.68414
2017-12-10T01:30:07.497635: step 26, loss 3.31407, acc 0.550781, prec 0.0214665, recall 0.687013
2017-12-10T01:30:10.815915: step 27, loss 3.62779, acc 0.571777, prec 0.0215799, recall 0.68789
2017-12-10T01:30:14.111968: step 28, loss 2.59378, acc 0.583008, prec 0.0219967, recall 0.695808
2017-12-10T01:30:17.470003: step 29, loss 2.84155, acc 0.587402, prec 0.0220731, recall 0.698376
2017-12-10T01:30:20.831801: step 30, loss 2.89776, acc 0.64502, prec 0.0222746, recall 0.699552
2017-12-10T01:30:24.158804: step 31, loss 2.40814, acc 0.631836, prec 0.0226104, recall 0.706074
2017-12-10T01:30:25.939209: step 32, loss 2.98988, acc 0.68186, prec 0.0226493, recall 0.706638
2017-12-10T01:30:29.240392: step 33, loss 2.61387, acc 0.692383, prec 0.0226677, recall 0.706067
2017-12-10T01:30:32.540404: step 34, loss 3.13353, acc 0.726074, prec 0.0230934, recall 0.706653
2017-12-10T01:30:35.873844: step 35, loss 2.42413, acc 0.735352, prec 0.0231986, recall 0.707101
2017-12-10T01:30:39.164262: step 36, loss 1.91038, acc 0.749023, prec 0.0235073, recall 0.709213
2017-12-10T01:30:42.517740: step 37, loss 2.11042, acc 0.727051, prec 0.0238028, recall 0.711485
2017-12-10T01:30:45.859996: step 38, loss 2.28477, acc 0.719238, prec 0.0239563, recall 0.712591
2017-12-10T01:30:49.166281: step 39, loss 2.54875, acc 0.697754, prec 0.0244841, recall 0.716549
2017-12-10T01:30:52.533898: step 40, loss 2.39133, acc 0.671875, prec 0.024723, recall 0.718322
2017-12-10T01:30:55.836599: step 41, loss 3.10702, acc 0.63916, prec 0.024822, recall 0.7175
2017-12-10T01:30:59.145098: step 42, loss 2.30238, acc 0.614746, prec 0.0249866, recall 0.721725
2017-12-10T01:31:02.482487: step 43, loss 2.31881, acc 0.595215, prec 0.0253053, recall 0.726125
2017-12-10T01:31:05.824759: step 44, loss 2.56514, acc 0.589844, prec 0.0254185, recall 0.728814
2017-12-10T01:31:09.134700: step 45, loss 1.80889, acc 0.608398, prec 0.0254975, recall 0.733737
2017-12-10T01:31:12.429712: step 46, loss 2.18274, acc 0.656738, prec 0.0254909, recall 0.734572
2017-12-10T01:31:15.744186: step 47, loss 2.11007, acc 0.70459, prec 0.0258708, recall 0.736804
2017-12-10T01:31:17.537401: step 48, loss 2.67721, acc 0.732093, prec 0.0260293, recall 0.73733
2017-12-10T01:31:20.900794: step 49, loss 2.64403, acc 0.72998, prec 0.0262334, recall 0.73743
2017-12-10T01:31:24.213503: step 50, loss 1.80144, acc 0.754395, prec 0.0266294, recall 0.740286
2017-12-10T01:31:27.520680: step 51, loss 1.98156, acc 0.722168, prec 0.0266931, recall 0.738462
2017-12-10T01:31:30.824586: step 52, loss 1.93191, acc 0.716309, prec 0.0268138, recall 0.739645
2017-12-10T01:31:34.186905: step 53, loss 1.7133, acc 0.703613, prec 0.0270054, recall 0.741935
2017-12-10T01:31:37.538738: step 54, loss 1.90654, acc 0.688965, prec 0.0269933, recall 0.742366
2017-12-10T01:31:40.838741: step 55, loss 1.67078, acc 0.685059, prec 0.0271976, recall 0.744853
2017-12-10T01:31:44.132712: step 56, loss 1.98393, acc 0.706055, prec 0.0274234, recall 0.746333
2017-12-10T01:31:47.463469: step 57, loss 1.68508, acc 0.709961, prec 0.0275611, recall 0.747596
2017-12-10T01:31:50.780771: step 58, loss 2.40422, acc 0.69873, prec 0.0278112, recall 0.747944
2017-12-10T01:31:54.085614: step 59, loss 2.13802, acc 0.688965, prec 0.0279788, recall 0.748272
2017-12-10T01:31:57.414702: step 60, loss 1.91626, acc 0.695312, prec 0.0280646, recall 0.749716
2017-12-10T01:32:00.766479: step 61, loss 1.58339, acc 0.70166, prec 0.0283588, recall 0.752922
2017-12-10T01:32:04.058083: step 62, loss 1.77536, acc 0.702637, prec 0.0284266, recall 0.753703
2017-12-10T01:32:07.392669: step 63, loss 1.5613, acc 0.700684, prec 0.0284699, recall 0.754737
2017-12-10T01:32:09.179573: step 64, loss 1.40359, acc 0.708837, prec 0.0286647, recall 0.756424
2017-12-10T01:32:12.476548: step 65, loss 1.70634, acc 0.75293, prec 0.0288651, recall 0.756842
2017-12-10T01:32:15.804622: step 66, loss 1.42752, acc 0.76123, prec 0.0291272, recall 0.758799
2017-12-10T01:32:19.094942: step 67, loss 1.28055, acc 0.791016, prec 0.0293428, recall 0.760204
2017-12-10T01:32:22.443242: step 68, loss 1.49404, acc 0.796387, prec 0.0295622, recall 0.760804
2017-12-10T01:32:25.761722: step 69, loss 1.48157, acc 0.779297, prec 0.0297373, recall 0.762401
2017-12-10T01:32:29.062195: step 70, loss 1.32712, acc 0.781738, prec 0.0299323, recall 0.762952
2017-12-10T01:32:32.385498: step 71, loss 1.19123, acc 0.766602, prec 0.0300118, recall 0.764393
2017-12-10T01:32:35.721984: step 72, loss 1.35941, acc 0.773926, prec 0.03008, recall 0.765692
2017-12-10T01:32:39.123657: step 73, loss 1.66318, acc 0.800293, prec 0.0304694, recall 0.767278
2017-12-10T01:32:42.442190: step 74, loss 1.15813, acc 0.773926, prec 0.030552, recall 0.76755
2017-12-10T01:32:45.745599: step 75, loss 1.17544, acc 0.753906, prec 0.0307517, recall 0.769019
2017-12-10T01:32:49.101853: step 76, loss 1.69457, acc 0.740723, prec 0.0310215, recall 0.77027
2017-12-10T01:32:52.484177: step 77, loss 1.50868, acc 0.743164, prec 0.031323, recall 0.772023
2017-12-10T01:32:55.804830: step 78, loss 1.27662, acc 0.742188, prec 0.0314623, recall 0.772827
2017-12-10T01:32:59.096608: step 79, loss 1.16646, acc 0.712402, prec 0.0315975, recall 0.774806
2017-12-10T01:33:00.922079: step 80, loss 1.07478, acc 0.746977, prec 0.0317188, recall 0.776017
2017-12-10T01:33:04.214611: step 81, loss 1.27983, acc 0.734863, prec 0.0318079, recall 0.777213
2017-12-10T01:33:07.525538: step 82, loss 1.293, acc 0.782227, prec 0.0320823, recall 0.778798
2017-12-10T01:33:10.832966: step 83, loss 1.18823, acc 0.800781, prec 0.0322415, recall 0.77929
2017-12-10T01:33:14.153395: step 84, loss 1.95536, acc 0.80127, prec 0.0324669, recall 0.778862
2017-12-10T01:33:17.466998: step 85, loss 1.03001, acc 0.798828, prec 0.0326178, recall 0.779968
2017-12-10T01:33:20.797376: step 86, loss 1.93978, acc 0.799316, prec 0.0327064, recall 0.778529
2017-12-10T01:33:24.153068: step 87, loss 0.940709, acc 0.774902, prec 0.0329699, recall 0.781005
2017-12-10T01:33:27.463588: step 88, loss 0.942197, acc 0.760254, prec 0.0331187, recall 0.782305
2017-12-10T01:33:30.800355: step 89, loss 1.01049, acc 0.75293, prec 0.0333816, recall 0.784839
2017-12-10T01:33:34.115554: step 90, loss 0.874533, acc 0.751465, prec 0.0335131, recall 0.78696
2017-12-10T01:33:37.460663: step 91, loss 1.43322, acc 0.767578, prec 0.0336632, recall 0.787266
2017-12-10T01:33:40.782056: step 92, loss 1.14144, acc 0.791016, prec 0.0338051, recall 0.787991
2017-12-10T01:33:44.097038: step 93, loss 1.74037, acc 0.820312, prec 0.0341166, recall 0.788244
2017-12-10T01:33:47.420752: step 94, loss 1.13542, acc 0.800293, prec 0.034309, recall 0.78917
2017-12-10T01:33:50.756182: step 95, loss 1.26328, acc 0.806152, prec 0.0343107, recall 0.78853
2017-12-10T01:33:52.553595: step 96, loss 0.971679, acc 0.803721, prec 0.0343493, recall 0.788722
2017-12-10T01:33:55.905101: step 97, loss 1.15029, acc 0.774902, prec 0.0345237, recall 0.789975
2017-12-10T01:33:59.222654: step 98, loss 1.02533, acc 0.763672, prec 0.0347857, recall 0.792262
2017-12-10T01:34:02.561884: step 99, loss 0.937263, acc 0.793457, prec 0.0347983, recall 0.79259
2017-12-10T01:34:05.862185: step 100, loss 0.893261, acc 0.785645, prec 0.0348748, recall 0.793542
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_1/1512887320/checkpoints/model-100

2017-12-10T01:34:10.338997: step 101, loss 0.860091, acc 0.796387, prec 0.0350335, recall 0.795099
2017-12-10T01:34:13.647530: step 102, loss 1.32394, acc 0.839355, prec 0.0353547, recall 0.795569
2017-12-10T01:34:16.994206: step 103, loss 0.744592, acc 0.840332, prec 0.0354275, recall 0.796197
2017-12-10T01:34:20.318329: step 104, loss 1.05052, acc 0.849121, prec 0.0355544, recall 0.795702
2017-12-10T01:34:23.688849: step 105, loss 0.87852, acc 0.829102, prec 0.0357988, recall 0.796666
2017-12-10T01:34:26.994273: step 106, loss 0.807097, acc 0.81543, prec 0.0358838, recall 0.797468
2017-12-10T01:34:30.338915: step 107, loss 0.757712, acc 0.828125, prec 0.0361775, recall 0.799166
2017-12-10T01:34:33.642901: step 108, loss 1.18764, acc 0.811035, prec 0.0363114, recall 0.799682
2017-12-10T01:34:36.967083: step 109, loss 1.15299, acc 0.797363, prec 0.0365684, recall 0.800314
2017-12-10T01:34:40.296631: step 110, loss 0.823002, acc 0.796875, prec 0.0366959, recall 0.80137
2017-12-10T01:34:43.666065: step 111, loss 0.862654, acc 0.78125, prec 0.036982, recall 0.803445
2017-12-10T01:34:45.462399: step 112, loss 1.65479, acc 0.750698, prec 0.0370344, recall 0.803304
2017-12-10T01:34:48.778525: step 113, loss 1.02448, acc 0.743652, prec 0.0370998, recall 0.804552
2017-12-10T01:34:52.115029: step 114, loss 1.20429, acc 0.765625, prec 0.0372012, recall 0.805597
2017-12-10T01:34:55.435123: step 115, loss 1.02213, acc 0.771484, prec 0.0372676, recall 0.806452
2017-12-10T01:34:58.754156: step 116, loss 0.917668, acc 0.794922, prec 0.037477, recall 0.807567
2017-12-10T01:35:02.097943: step 117, loss 0.706556, acc 0.814453, prec 0.0376504, recall 0.809147
2017-12-10T01:35:05.419688: step 118, loss 1.34808, acc 0.840332, prec 0.0378265, recall 0.808943
2017-12-10T01:35:08.733684: step 119, loss 1.05625, acc 0.842773, prec 0.0380806, recall 0.809537
2017-12-10T01:35:12.031234: step 120, loss 1.04927, acc 0.850586, prec 0.0382245, recall 0.809402
2017-12-10T01:35:15.332264: step 121, loss 0.834456, acc 0.833496, prec 0.0383992, recall 0.81017
2017-12-10T01:35:18.623628: step 122, loss 0.897988, acc 0.818848, prec 0.0384036, recall 0.810059
2017-12-10T01:35:21.985620: step 123, loss 0.828317, acc 0.813477, prec 0.0386057, recall 0.811021
2017-12-10T01:35:25.321400: step 124, loss 0.842934, acc 0.803223, prec 0.0387944, recall 0.81219
2017-12-10T01:35:28.630115: step 125, loss 0.748633, acc 0.806152, prec 0.0389074, recall 0.813476
2017-12-10T01:35:31.967350: step 126, loss 0.98023, acc 0.81543, prec 0.0390565, recall 0.813739
2017-12-10T01:35:35.260787: step 127, loss 1.13214, acc 0.807129, prec 0.0392322, recall 0.814366
2017-12-10T01:35:37.065146: step 128, loss 0.754627, acc 0.804651, prec 0.0393497, recall 0.815043
2017-12-10T01:35:40.391740: step 129, loss 0.737711, acc 0.818359, prec 0.0395079, recall 0.816202
2017-12-10T01:35:43.694541: step 130, loss 0.758011, acc 0.808105, prec 0.0396911, recall 0.817272
2017-12-10T01:35:47.025951: step 131, loss 0.84119, acc 0.839355, prec 0.0398447, recall 0.817659
2017-12-10T01:35:50.405583: step 132, loss 0.562573, acc 0.850098, prec 0.0400188, recall 0.818724
2017-12-10T01:35:53.751820: step 133, loss 0.753039, acc 0.865723, prec 0.040149, recall 0.818697
2017-12-10T01:35:57.060227: step 134, loss 0.556568, acc 0.897949, prec 0.040407, recall 0.819668
2017-12-10T01:36:00.365822: step 135, loss 0.765169, acc 0.883789, prec 0.0405784, recall 0.819518
2017-12-10T01:36:03.677580: step 136, loss 0.627901, acc 0.856934, prec 0.0407426, recall 0.820287
2017-12-10T01:36:07.003891: step 137, loss 0.700198, acc 0.855469, prec 0.0409279, recall 0.821134
2017-12-10T01:36:10.348050: step 138, loss 0.701597, acc 0.853027, prec 0.0410854, recall 0.82188
2017-12-10T01:36:13.665182: step 139, loss 0.517321, acc 0.846191, prec 0.0411861, recall 0.822847
2017-12-10T01:36:16.969574: step 140, loss 0.698022, acc 0.830078, prec 0.0412702, recall 0.8234
2017-12-10T01:36:20.275515: step 141, loss 0.683221, acc 0.852539, prec 0.041506, recall 0.824416
2017-12-10T01:36:23.616452: step 142, loss 0.588335, acc 0.840332, prec 0.0417149, recall 0.825573
2017-12-10T01:36:26.952285: step 143, loss 0.637612, acc 0.84375, prec 0.0419959, recall 0.826762
2017-12-10T01:36:28.738206: step 144, loss 0.694335, acc 0.858605, prec 0.0421164, recall 0.827266
2017-12-10T01:36:32.101179: step 145, loss 0.803264, acc 0.864258, prec 0.0424274, recall 0.828269
2017-12-10T01:36:35.421251: step 146, loss 0.647143, acc 0.866211, prec 0.042645, recall 0.82932
2017-12-10T01:36:38.750639: step 147, loss 0.902775, acc 0.864746, prec 0.0428152, recall 0.829619
2017-12-10T01:36:42.066111: step 148, loss 0.838613, acc 0.850586, prec 0.0429226, recall 0.830141
2017-12-10T01:36:45.388821: step 149, loss 0.767899, acc 0.840332, prec 0.0430872, recall 0.830699
2017-12-10T01:36:48.730684: step 150, loss 0.628819, acc 0.867676, prec 0.0431992, recall 0.831169
2017-12-10T01:36:52.103538: step 151, loss 0.617439, acc 0.849609, prec 0.0433598, recall 0.831674
2017-12-10T01:36:55.434746: step 152, loss 0.574816, acc 0.856445, prec 0.0434578, recall 0.832321
2017-12-10T01:36:58.740950: step 153, loss 0.65265, acc 0.857422, prec 0.0436793, recall 0.833371
2017-12-10T01:37:02.074546: step 154, loss 0.714633, acc 0.868164, prec 0.0438217, recall 0.833925
2017-12-10T01:37:05.378980: step 155, loss 0.548702, acc 0.867188, prec 0.0439621, recall 0.834474
2017-12-10T01:37:08.694500: step 156, loss 0.615404, acc 0.871582, prec 0.0441942, recall 0.835487
2017-12-10T01:37:12.013409: step 157, loss 0.897918, acc 0.861816, prec 0.044438, recall 0.836012
2017-12-10T01:37:15.322738: step 158, loss 0.907608, acc 0.859375, prec 0.0445792, recall 0.83603
2017-12-10T01:37:18.608017: step 159, loss 0.495942, acc 0.844238, prec 0.0446899, recall 0.83691
2017-12-10T01:37:20.394948: step 160, loss 0.909642, acc 0.84093, prec 0.0446805, recall 0.836617
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 2048
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_2048_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_2/1512887841

Start training
2017-12-10T01:37:27.316643: step 1, loss 8.17672, acc 0.827637, prec 0.0149701, recall 0.172414
2017-12-10T01:37:30.624717: step 2, loss 7.02265, acc 0.46875, prec 0.00994318, recall 0.229508
2017-12-10T01:37:33.926958: step 3, loss 5.52475, acc 0.196777, prec 0.0136452, recall 0.456522
2017-12-10T01:37:37.236840: step 4, loss 6.11631, acc 0.151367, prec 0.0140438, recall 0.576271
2017-12-10T01:37:40.548042: step 5, loss 6.29093, acc 0.185059, prec 0.0145482, recall 0.620915
2017-12-10T01:37:43.848956: step 6, loss 6.07667, acc 0.274414, prec 0.0151855, recall 0.645503
2017-12-10T01:37:47.176551: step 7, loss 3.82506, acc 0.37207, prec 0.0155313, recall 0.6621
2017-12-10T01:37:50.493571: step 8, loss 4.09935, acc 0.504883, prec 0.0161228, recall 0.654902
2017-12-10T01:37:53.817209: step 9, loss 5.29268, acc 0.604004, prec 0.0161189, recall 0.636042
2017-12-10T01:37:57.138658: step 10, loss 3.89039, acc 0.68457, prec 0.0159214, recall 0.618421
2017-12-10T01:38:00.438924: step 11, loss 3.88961, acc 0.71875, prec 0.0163881, recall 0.613293
2017-12-10T01:38:03.740440: step 12, loss 4.48245, acc 0.712402, prec 0.0174034, recall 0.615804
2017-12-10T01:38:07.083754: step 13, loss 3.97559, acc 0.666992, prec 0.0177147, recall 0.596059
2017-12-10T01:38:10.379652: step 14, loss 2.96052, acc 0.634766, prec 0.0183696, recall 0.609195
2017-12-10T01:38:13.681007: step 15, loss 3.31574, acc 0.544434, prec 0.0180907, recall 0.613687
2017-12-10T01:38:15.482189: step 16, loss 3.96432, acc 0.5, prec 0.0181647, recall 0.614894
2017-12-10T01:38:18.797340: step 17, loss 4.71515, acc 0.456543, prec 0.018312, recall 0.622755
2017-12-10T01:38:22.113804: step 18, loss 3.91564, acc 0.450684, prec 0.0185318, recall 0.637051
2017-12-10T01:38:25.423324: step 19, loss 3.44243, acc 0.421875, prec 0.018971, recall 0.654804
2017-12-10T01:38:28.723795: step 20, loss 3.36282, acc 0.474121, prec 0.0193649, recall 0.66835
2017-12-10T01:38:32.052912: step 21, loss 2.99597, acc 0.530762, prec 0.0193248, recall 0.672609
2017-12-10T01:38:35.361284: step 22, loss 2.3061, acc 0.604004, prec 0.0193282, recall 0.677673
2017-12-10T01:38:38.719303: step 23, loss 2.55295, acc 0.669434, prec 0.0199156, recall 0.684604
2017-12-10T01:38:42.037687: step 24, loss 3.15098, acc 0.710938, prec 0.0204618, recall 0.687055
2017-12-10T01:38:45.389163: step 25, loss 4.60223, acc 0.754883, prec 0.0209051, recall 0.678331
2017-12-10T01:38:48.705081: step 26, loss 4.08188, acc 0.73877, prec 0.021059, recall 0.67228
2017-12-10T01:38:52.033085: step 27, loss 2.06389, acc 0.723633, prec 0.0216004, recall 0.677861
2017-12-10T01:38:55.377781: step 28, loss 2.86359, acc 0.676758, prec 0.0219992, recall 0.681004
2017-12-10T01:38:58.692866: step 29, loss 2.99649, acc 0.677734, prec 0.0220868, recall 0.679398
2017-12-10T01:39:02.062996: step 30, loss 2.16611, acc 0.625488, prec 0.0225438, recall 0.686318
2017-12-10T01:39:05.376771: step 31, loss 2.38791, acc 0.597168, prec 0.0226846, recall 0.691145
2017-12-10T01:39:07.184551: step 32, loss 2.63802, acc 0.597584, prec 0.0227527, recall 0.693617
2017-12-10T01:39:10.482111: step 33, loss 2.46675, acc 0.606934, prec 0.023162, recall 0.700513
2017-12-10T01:39:13.789001: step 34, loss 2.50568, acc 0.597656, prec 0.0232765, recall 0.704591
2017-12-10T01:39:17.084702: step 35, loss 3.38496, acc 0.633301, prec 0.0235983, recall 0.708494
2017-12-10T01:39:20.386427: step 36, loss 2.98024, acc 0.661621, prec 0.0237646, recall 0.709859
2017-12-10T01:39:23.688161: step 37, loss 1.614, acc 0.695801, prec 0.0240619, recall 0.715201
2017-12-10T01:39:27.000258: step 38, loss 2.95699, acc 0.689941, prec 0.0245508, recall 0.716931
2017-12-10T01:39:30.369020: step 39, loss 2.81086, acc 0.69043, prec 0.0246438, recall 0.717241
2017-12-10T01:39:33.673229: step 40, loss 2.89812, acc 0.712402, prec 0.0249367, recall 0.718357
2017-12-10T01:39:37.011720: step 41, loss 1.86051, acc 0.691895, prec 0.0249929, recall 0.718981
2017-12-10T01:39:40.312655: step 42, loss 1.94499, acc 0.720703, prec 0.0253083, recall 0.7208
2017-12-10T01:39:43.619592: step 43, loss 1.95591, acc 0.726562, prec 0.0256736, recall 0.724649
2017-12-10T01:39:46.920149: step 44, loss 2.53393, acc 0.719238, prec 0.0258593, recall 0.725954
2017-12-10T01:39:50.231194: step 45, loss 2.47731, acc 0.706543, prec 0.0260716, recall 0.7287
2017-12-10T01:39:53.608030: step 46, loss 1.62047, acc 0.704102, prec 0.0262985, recall 0.732064
2017-12-10T01:39:56.911868: step 47, loss 2.36446, acc 0.711426, prec 0.0264548, recall 0.733142
2017-12-10T01:39:58.690266: step 48, loss 3.79176, acc 0.711896, prec 0.0264739, recall 0.731206
2017-12-10T01:40:02.061589: step 49, loss 1.88211, acc 0.710938, prec 0.0266225, recall 0.732777
2017-12-10T01:40:05.371372: step 50, loss 2.23415, acc 0.70459, prec 0.026879, recall 0.735194
2017-12-10T01:40:08.689674: step 51, loss 1.56286, acc 0.689453, prec 0.0269867, recall 0.737617
2017-12-10T01:40:12.077351: step 52, loss 1.6494, acc 0.665527, prec 0.0271747, recall 0.741787
2017-12-10T01:40:15.380848: step 53, loss 2.03256, acc 0.686035, prec 0.0275468, recall 0.745513
2017-12-10T01:40:18.655749: step 54, loss 1.86664, acc 0.693848, prec 0.0276903, recall 0.747952
2017-12-10T01:40:21.960087: step 55, loss 1.79965, acc 0.731934, prec 0.0281269, recall 0.751076
2017-12-10T01:40:25.361057: step 56, loss 2.57374, acc 0.725098, prec 0.0283677, recall 0.751957
2017-12-10T01:40:28.677077: step 57, loss 1.38598, acc 0.73584, prec 0.0285272, recall 0.753555
2017-12-10T01:40:32.008146: step 58, loss 1.87391, acc 0.753418, prec 0.0287931, recall 0.754794
2017-12-10T01:40:35.314708: step 59, loss 2.59097, acc 0.759277, prec 0.0289147, recall 0.752854
2017-12-10T01:40:38.656425: step 60, loss 1.87701, acc 0.733398, prec 0.0289755, recall 0.753378
2017-12-10T01:40:41.999428: step 61, loss 1.9983, acc 0.738281, prec 0.0291871, recall 0.754425
2017-12-10T01:40:45.326214: step 62, loss 1.40093, acc 0.737305, prec 0.0294111, recall 0.756801
2017-12-10T01:40:48.630546: step 63, loss 1.61644, acc 0.726562, prec 0.0294972, recall 0.756699
2017-12-10T01:40:50.431442: step 64, loss 1.64999, acc 0.712825, prec 0.0295509, recall 0.757447
2017-12-10T01:40:53.759751: step 65, loss 1.47998, acc 0.715332, prec 0.0298373, recall 0.75952
2017-12-10T01:40:57.045210: step 66, loss 1.52024, acc 0.740234, prec 0.0299702, recall 0.760802
2017-12-10T01:41:00.366279: step 67, loss 1.20375, acc 0.699707, prec 0.0301066, recall 0.763572
2017-12-10T01:41:03.712585: step 68, loss 1.60601, acc 0.741211, prec 0.0302149, recall 0.76503
2017-12-10T01:41:07.069290: step 69, loss 0.98674, acc 0.759766, prec 0.0305891, recall 0.768701
2017-12-10T01:41:10.396497: step 70, loss 1.13769, acc 0.794922, prec 0.0307555, recall 0.770053
2017-12-10T01:41:13.740152: step 71, loss 1.90803, acc 0.815918, prec 0.0310597, recall 0.770186
2017-12-10T01:41:17.112244: step 72, loss 1.42975, acc 0.814941, prec 0.0312106, recall 0.769086
2017-12-10T01:41:20.478097: step 73, loss 2.5136, acc 0.82373, prec 0.0315348, recall 0.769338
2017-12-10T01:41:23.788842: step 74, loss 1.44237, acc 0.796387, prec 0.031619, recall 0.769125
2017-12-10T01:41:27.140319: step 75, loss 1.34269, acc 0.789062, prec 0.0318009, recall 0.769892
2017-12-10T01:41:30.452830: step 76, loss 1.18126, acc 0.770996, prec 0.0318497, recall 0.770367
2017-12-10T01:41:33.794092: step 77, loss 1.37256, acc 0.741699, prec 0.031933, recall 0.77158
2017-12-10T01:41:37.093144: step 78, loss 1.3615, acc 0.731934, prec 0.032144, recall 0.773223
2017-12-10T01:41:40.453510: step 79, loss 1.86163, acc 0.705078, prec 0.032372, recall 0.774443
2017-12-10T01:41:42.248013: step 80, loss 1.27401, acc 0.740706, prec 0.0324881, recall 0.775319
2017-12-10T01:41:45.571055: step 81, loss 1.30783, acc 0.736328, prec 0.0326242, recall 0.776797
2017-12-10T01:41:48.894379: step 82, loss 1.85489, acc 0.748535, prec 0.0328752, recall 0.777824
2017-12-10T01:41:52.256933: step 83, loss 1.46725, acc 0.734375, prec 0.0331031, recall 0.779772
2017-12-10T01:41:55.630300: step 84, loss 1.12039, acc 0.720215, prec 0.0330768, recall 0.78106
2017-12-10T01:41:58.924707: step 85, loss 1.09074, acc 0.76709, prec 0.033237, recall 0.782713
2017-12-10T01:42:02.250284: step 86, loss 0.94548, acc 0.791992, prec 0.0334401, recall 0.784104
2017-12-10T01:42:05.582861: step 87, loss 1.61401, acc 0.810059, prec 0.0336945, recall 0.785018
2017-12-10T01:42:08.919761: step 88, loss 1.46482, acc 0.827148, prec 0.0338382, recall 0.784336
2017-12-10T01:42:12.203429: step 89, loss 1.16018, acc 0.830078, prec 0.0341251, recall 0.785605
2017-12-10T01:42:15.506166: step 90, loss 1.51714, acc 0.828613, prec 0.0343609, recall 0.785714
2017-12-10T01:42:18.799165: step 91, loss 0.85158, acc 0.816895, prec 0.0345616, recall 0.787202
2017-12-10T01:42:22.126598: step 92, loss 1.08956, acc 0.798828, prec 0.0348502, recall 0.788335
2017-12-10T01:42:25.422091: step 93, loss 1.41783, acc 0.783203, prec 0.0349606, recall 0.788957
2017-12-10T01:42:28.749271: step 94, loss 0.95854, acc 0.779297, prec 0.0350325, recall 0.79027
2017-12-10T01:42:32.088893: step 95, loss 1.69297, acc 0.766602, prec 0.035061, recall 0.79
2017-12-10T01:42:33.872427: step 96, loss 1.79566, acc 0.77881, prec 0.0351899, recall 0.790426
2017-12-10T01:42:37.208487: step 97, loss 1.17462, acc 0.772461, prec 0.0354038, recall 0.791594
2017-12-10T01:42:40.482101: step 98, loss 0.892213, acc 0.766602, prec 0.035606, recall 0.793557
2017-12-10T01:42:43.781354: step 99, loss 1.03619, acc 0.782715, prec 0.0358245, recall 0.794933
2017-12-10T01:42:47.119669: step 100, loss 1.10057, acc 0.785156, prec 0.0359835, recall 0.796001
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_2/1512887841/checkpoints/model-100

2017-12-10T01:42:51.480521: step 101, loss 1.35, acc 0.793457, prec 0.0360778, recall 0.795905
2017-12-10T01:42:54.771371: step 102, loss 0.833199, acc 0.802246, prec 0.0361932, recall 0.796937
2017-12-10T01:42:58.061403: step 103, loss 0.8941, acc 0.813965, prec 0.0363207, recall 0.79769
2017-12-10T01:43:01.405342: step 104, loss 1.24016, acc 0.818359, prec 0.0365535, recall 0.798369
2017-12-10T01:43:04.711755: step 105, loss 1.2608, acc 0.827148, prec 0.0367653, recall 0.798645
2017-12-10T01:43:08.038430: step 106, loss 0.755028, acc 0.838379, prec 0.0369002, recall 0.799296
2017-12-10T01:43:11.354159: step 107, loss 1.01713, acc 0.814941, prec 0.0370082, recall 0.799683
2017-12-10T01:43:14.674545: step 108, loss 0.683839, acc 0.822754, prec 0.0373329, recall 0.801757
2017-12-10T01:43:18.013026: step 109, loss 1.02634, acc 0.819336, prec 0.0375114, recall 0.802672
2017-12-10T01:43:21.369222: step 110, loss 1.11916, acc 0.825195, prec 0.0376538, recall 0.802895
2017-12-10T01:43:24.680298: step 111, loss 1.01775, acc 0.824219, prec 0.0378483, recall 0.803599
2017-12-10T01:43:26.489001: step 112, loss 0.981355, acc 0.818773, prec 0.0378677, recall 0.803647
2017-12-10T01:43:29.798222: step 113, loss 0.893597, acc 0.79248, prec 0.037832, recall 0.80375
2017-12-10T01:43:33.121706: step 114, loss 0.996665, acc 0.822266, prec 0.0381159, recall 0.804842
2017-12-10T01:43:36.481196: step 115, loss 1.13608, acc 0.813477, prec 0.0382651, recall 0.805391
2017-12-10T01:43:39.834598: step 116, loss 0.948853, acc 0.797363, prec 0.0384486, recall 0.806158
2017-12-10T01:43:43.129183: step 117, loss 0.73268, acc 0.785156, prec 0.0385342, recall 0.807513
2017-12-10T01:43:46.446397: step 118, loss 1.05364, acc 0.814941, prec 0.0387592, recall 0.80859
2017-12-10T01:43:49.759446: step 119, loss 1.06258, acc 0.812988, prec 0.039019, recall 0.809809
2017-12-10T01:43:53.054207: step 120, loss 1.16877, acc 0.837402, prec 0.0392639, recall 0.810612
2017-12-10T01:43:56.344801: step 121, loss 0.82868, acc 0.800781, prec 0.0393873, recall 0.81131
2017-12-10T01:43:59.643434: step 122, loss 0.793795, acc 0.835449, prec 0.0395337, recall 0.81217
2017-12-10T01:44:02.975384: step 123, loss 0.923937, acc 0.82666, prec 0.039669, recall 0.813017
2017-12-10T01:44:06.289663: step 124, loss 0.825343, acc 0.836914, prec 0.0397895, recall 0.813304
2017-12-10T01:44:09.592306: step 125, loss 0.98063, acc 0.849609, prec 0.0399739, recall 0.813789
2017-12-10T01:44:12.912783: step 126, loss 1.10716, acc 0.838379, prec 0.0402081, recall 0.814516
2017-12-10T01:44:16.215952: step 127, loss 0.993171, acc 0.834473, prec 0.0403339, recall 0.815052
2017-12-10T01:44:18.013763: step 128, loss 0.640315, acc 0.826208, prec 0.0403989, recall 0.815691
2017-12-10T01:44:21.317376: step 129, loss 0.764404, acc 0.834473, prec 0.0406234, recall 0.816601
2017-12-10T01:44:24.619922: step 130, loss 0.588966, acc 0.834961, prec 0.0407828, recall 0.817682
2017-12-10T01:44:27.945795: step 131, loss 0.722616, acc 0.854004, prec 0.0409375, recall 0.818229
2017-12-10T01:44:31.311914: step 132, loss 0.651298, acc 0.845703, prec 0.0410571, recall 0.818674
2017-12-10T01:44:34.612464: step 133, loss 0.95121, acc 0.873047, prec 0.0412075, recall 0.818275
2017-12-10T01:44:37.954087: step 134, loss 0.751037, acc 0.844727, prec 0.0413116, recall 0.818667
2017-12-10T01:44:41.249323: step 135, loss 0.551095, acc 0.868164, prec 0.0416107, recall 0.820106
2017-12-10T01:44:44.591166: step 136, loss 0.971205, acc 0.852051, prec 0.0418051, recall 0.821
2017-12-10T01:44:47.873699: step 137, loss 0.960413, acc 0.863281, prec 0.0419393, recall 0.820799
2017-12-10T01:44:51.246179: step 138, loss 0.503439, acc 0.871094, prec 0.0421631, recall 0.821921
2017-12-10T01:44:54.562021: step 139, loss 0.857164, acc 0.861328, prec 0.0422672, recall 0.822233
2017-12-10T01:44:57.853871: step 140, loss 0.78802, acc 0.855469, prec 0.0424716, recall 0.823129
2017-12-10T01:45:01.181145: step 141, loss 0.767655, acc 0.846191, prec 0.0426045, recall 0.823799
2017-12-10T01:45:04.488800: step 142, loss 1.15547, acc 0.840332, prec 0.0428147, recall 0.824163
2017-12-10T01:45:07.821453: step 143, loss 0.639386, acc 0.841309, prec 0.0430576, recall 0.825427
2017-12-10T01:45:09.614395: step 144, loss 0.693643, acc 0.839219, prec 0.0431197, recall 0.825768
2017-12-10T01:45:12.912066: step 145, loss 0.594477, acc 0.826172, prec 0.0432838, recall 0.826995
2017-12-10T01:45:16.217282: step 146, loss 0.63291, acc 0.834473, prec 0.043374, recall 0.827731
2017-12-10T01:45:19.486901: step 147, loss 0.85407, acc 0.845215, prec 0.0435471, recall 0.82793
2017-12-10T01:45:22.826545: step 148, loss 0.736367, acc 0.857422, prec 0.0437884, recall 0.828893
2017-12-10T01:45:26.141755: step 149, loss 0.66796, acc 0.876465, prec 0.0440371, recall 0.829802
2017-12-10T01:45:29.493896: step 150, loss 0.831618, acc 0.867188, prec 0.0442054, recall 0.830278
2017-12-10T01:45:32.830706: step 151, loss 0.686558, acc 0.866699, prec 0.0444183, recall 0.830714
2017-12-10T01:45:36.154337: step 152, loss 0.5659, acc 0.869629, prec 0.0445295, recall 0.830989
2017-12-10T01:45:39.474819: step 153, loss 0.466842, acc 0.879883, prec 0.0447071, recall 0.831817
2017-12-10T01:45:42.815055: step 154, loss 0.520298, acc 0.875, prec 0.0448105, recall 0.83223
2017-12-10T01:45:46.116471: step 155, loss 0.968151, acc 0.871094, prec 0.0450475, recall 0.832166
2017-12-10T01:45:49.429810: step 156, loss 1.34339, acc 0.87793, prec 0.0452683, recall 0.831851
2017-12-10T01:45:52.817732: step 157, loss 0.630134, acc 0.854492, prec 0.0453128, recall 0.832145
2017-12-10T01:45:56.137642: step 158, loss 0.628221, acc 0.850098, prec 0.0454189, recall 0.832832
2017-12-10T01:45:59.456571: step 159, loss 0.811792, acc 0.833496, prec 0.045585, recall 0.833404
2017-12-10T01:46:01.264925: step 160, loss 0.600037, acc 0.821561, prec 0.0456164, recall 0.83383
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 2048
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_2048_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_3/1512888362

Start training
2017-12-10T01:46:08.191980: step 1, loss 5.05616, acc 0.489258, prec 0.0142993, recall 0.555556
2017-12-10T01:46:11.481894: step 2, loss 4.10994, acc 0.375488, prec 0.0137104, recall 0.581818
2017-12-10T01:46:14.769081: step 3, loss 3.9982, acc 0.443848, prec 0.0143637, recall 0.60241
2017-12-10T01:46:18.061239: step 4, loss 3.39767, acc 0.50293, prec 0.0166076, recall 0.646552
2017-12-10T01:46:21.398134: step 5, loss 5.59507, acc 0.59082, prec 0.0179238, recall 0.615385
2017-12-10T01:46:24.721833: step 6, loss 3.87457, acc 0.510742, prec 0.0183702, recall 0.625668
2017-12-10T01:46:28.003068: step 7, loss 3.42833, acc 0.458008, prec 0.01816, recall 0.632558
2017-12-10T01:46:31.348395: step 8, loss 3.57927, acc 0.438477, prec 0.0183666, recall 0.654321
2017-12-10T01:46:34.670330: step 9, loss 4.30143, acc 0.512207, prec 0.0191274, recall 0.663082
2017-12-10T01:46:37.993865: step 10, loss 3.43684, acc 0.536621, prec 0.018637, recall 0.655629
2017-12-10T01:46:41.334668: step 11, loss 4.34494, acc 0.582031, prec 0.0189747, recall 0.654655
2017-12-10T01:46:44.623000: step 12, loss 3.00538, acc 0.597656, prec 0.0196285, recall 0.663014
2017-12-10T01:46:47.934573: step 13, loss 2.52755, acc 0.613281, prec 0.0196467, recall 0.668394
2017-12-10T01:46:51.274855: step 14, loss 3.22328, acc 0.64209, prec 0.0201773, recall 0.669856
2017-12-10T01:46:54.624333: step 15, loss 3.18376, acc 0.636719, prec 0.0205699, recall 0.671875
2017-12-10T01:46:56.410631: step 16, loss 3.37486, acc 0.631041, prec 0.0207488, recall 0.673866
2017-12-10T01:46:59.727806: step 17, loss 2.27638, acc 0.631836, prec 0.0208188, recall 0.678351
2017-12-10T01:47:03.051023: step 18, loss 3.06083, acc 0.612305, prec 0.0212497, recall 0.681467
2017-12-10T01:47:06.419623: step 19, loss 3.35536, acc 0.622559, prec 0.0212191, recall 0.677064
2017-12-10T01:47:09.716575: step 20, loss 2.60108, acc 0.578125, prec 0.0211869, recall 0.680141
2017-12-10T01:47:13.008733: step 21, loss 1.85482, acc 0.578125, prec 0.0216109, recall 0.694631
2017-12-10T01:47:16.322651: step 22, loss 2.91888, acc 0.608887, prec 0.0221243, recall 0.699367
2017-12-10T01:47:19.616920: step 23, loss 1.7147, acc 0.652344, prec 0.0225484, recall 0.706505
2017-12-10T01:47:22.960102: step 24, loss 3.47486, acc 0.67627, prec 0.0229133, recall 0.704023
2017-12-10T01:47:26.274670: step 25, loss 2.44819, acc 0.707031, prec 0.0231427, recall 0.702069
2017-12-10T01:47:29.577038: step 26, loss 2.36402, acc 0.6875, prec 0.023531, recall 0.705026
2017-12-10T01:47:32.912634: step 27, loss 3.32633, acc 0.697754, prec 0.0240498, recall 0.704403
2017-12-10T01:47:36.199751: step 28, loss 2.02754, acc 0.659668, prec 0.0246168, recall 0.711191
2017-12-10T01:47:39.486660: step 29, loss 2.11155, acc 0.650879, prec 0.0249747, recall 0.716107
2017-12-10T01:47:42.785271: step 30, loss 3.82906, acc 0.637207, prec 0.0248342, recall 0.709641
2017-12-10T01:47:46.095327: step 31, loss 2.16933, acc 0.594238, prec 0.024757, recall 0.713348
2017-12-10T01:47:47.880093: step 32, loss 2.1161, acc 0.590149, prec 0.0247153, recall 0.714903
2017-12-10T01:47:51.202620: step 33, loss 1.70896, acc 0.617676, prec 0.0246864, recall 0.719873
2017-12-10T01:47:54.526994: step 34, loss 1.9901, acc 0.662598, prec 0.0247084, recall 0.721362
2017-12-10T01:47:57.818495: step 35, loss 1.69204, acc 0.731934, prec 0.0250528, recall 0.725175
2017-12-10T01:48:01.163446: step 36, loss 1.14491, acc 0.769043, prec 0.0252172, recall 0.726916
2017-12-10T01:48:04.444890: step 37, loss 2.87458, acc 0.802734, prec 0.0256393, recall 0.723909
2017-12-10T01:48:07.748921: step 38, loss 1.30118, acc 0.812988, prec 0.0259004, recall 0.72449
2017-12-10T01:48:11.050618: step 39, loss 2.28159, acc 0.805176, prec 0.0261772, recall 0.722022
2017-12-10T01:48:14.431092: step 40, loss 2.11524, acc 0.759766, prec 0.0267392, recall 0.725764
2017-12-10T01:48:17.734729: step 41, loss 2.41147, acc 0.709473, prec 0.0269179, recall 0.727195
2017-12-10T01:48:21.012019: step 42, loss 2.16435, acc 0.670898, prec 0.0271447, recall 0.728856
2017-12-10T01:48:24.333555: step 43, loss 2.051, acc 0.605469, prec 0.0274843, recall 0.733333
2017-12-10T01:48:27.646912: step 44, loss 2.05501, acc 0.582031, prec 0.0275969, recall 0.736307
2017-12-10T01:48:30.955573: step 45, loss 2.70737, acc 0.564453, prec 0.0278975, recall 0.740713
2017-12-10T01:48:34.253687: step 46, loss 2.31653, acc 0.575684, prec 0.0279015, recall 0.743875
2017-12-10T01:48:37.593069: step 47, loss 1.96003, acc 0.598633, prec 0.0279146, recall 0.746725
2017-12-10T01:48:39.381257: step 48, loss 1.64311, acc 0.684015, prec 0.0280009, recall 0.74802
2017-12-10T01:48:42.694677: step 49, loss 1.17291, acc 0.713379, prec 0.0280555, recall 0.750887
2017-12-10T01:48:46.007583: step 50, loss 1.9819, acc 0.780762, prec 0.0284464, recall 0.75
2017-12-10T01:48:49.333913: step 51, loss 2.0193, acc 0.79541, prec 0.0287505, recall 0.747978
2017-12-10T01:48:52.654988: step 52, loss 1.86913, acc 0.807129, prec 0.0290854, recall 0.74967
2017-12-10T01:48:55.955647: step 53, loss 2.03311, acc 0.803711, prec 0.029414, recall 0.747423
2017-12-10T01:48:59.244144: step 54, loss 1.58709, acc 0.758301, prec 0.0294663, recall 0.747776
2017-12-10T01:49:02.556726: step 55, loss 1.81568, acc 0.70459, prec 0.029651, recall 0.750468
2017-12-10T01:49:05.867587: step 56, loss 1.6726, acc 0.688965, prec 0.0296451, recall 0.750153
2017-12-10T01:49:09.192610: step 57, loss 1.20055, acc 0.653809, prec 0.0298379, recall 0.754672
2017-12-10T01:49:12.491077: step 58, loss 1.62857, acc 0.671387, prec 0.0297787, recall 0.755952
2017-12-10T01:49:15.796859: step 59, loss 1.40979, acc 0.694336, prec 0.0299776, recall 0.758621
2017-12-10T01:49:19.114525: step 60, loss 1.27172, acc 0.719238, prec 0.0300738, recall 0.759931
2017-12-10T01:49:22.465716: step 61, loss 1.08358, acc 0.759277, prec 0.0302437, recall 0.762202
2017-12-10T01:49:25.803107: step 62, loss 1.08793, acc 0.782715, prec 0.0305076, recall 0.764377
2017-12-10T01:49:29.100136: step 63, loss 1.86301, acc 0.813965, prec 0.0308365, recall 0.763259
2017-12-10T01:49:30.917247: step 64, loss 2.12089, acc 0.807621, prec 0.0310418, recall 0.762419
2017-12-10T01:49:34.283187: step 65, loss 1.49734, acc 0.781738, prec 0.0313343, recall 0.763521
2017-12-10T01:49:37.613304: step 66, loss 1.76169, acc 0.736328, prec 0.0316624, recall 0.764798
2017-12-10T01:49:40.920995: step 67, loss 1.1775, acc 0.682129, prec 0.0318002, recall 0.768168
2017-12-10T01:49:44.237610: step 68, loss 1.42166, acc 0.648438, prec 0.03189, recall 0.770277
2017-12-10T01:49:47.626891: step 69, loss 1.45072, acc 0.615234, prec 0.0318518, recall 0.772637
2017-12-10T01:49:50.944088: step 70, loss 1.24721, acc 0.644531, prec 0.0317746, recall 0.774877
2017-12-10T01:49:54.336021: step 71, loss 1.37763, acc 0.711426, prec 0.0318468, recall 0.775887
2017-12-10T01:49:57.642067: step 72, loss 1.05048, acc 0.779785, prec 0.0320049, recall 0.777244
2017-12-10T01:50:00.953926: step 73, loss 0.959854, acc 0.797852, prec 0.0322777, recall 0.77946
2017-12-10T01:50:04.280125: step 74, loss 1.00769, acc 0.820801, prec 0.032615, recall 0.780726
2017-12-10T01:50:07.575686: step 75, loss 1.5527, acc 0.854492, prec 0.0328442, recall 0.779715
2017-12-10T01:50:10.895595: step 76, loss 1.27664, acc 0.85791, prec 0.0331315, recall 0.778681
2017-12-10T01:50:14.234730: step 77, loss 0.979406, acc 0.833984, prec 0.0332506, recall 0.779418
2017-12-10T01:50:17.513276: step 78, loss 1.6588, acc 0.82959, prec 0.0335121, recall 0.779198
2017-12-10T01:50:20.816559: step 79, loss 0.821813, acc 0.80127, prec 0.0336755, recall 0.780392
2017-12-10T01:50:22.636731: step 80, loss 1.27182, acc 0.780669, prec 0.0338361, recall 0.780994
2017-12-10T01:50:25.947292: step 81, loss 1.11386, acc 0.756348, prec 0.0340623, recall 0.782794
2017-12-10T01:50:29.258065: step 82, loss 1.31101, acc 0.707031, prec 0.0340089, recall 0.783122
2017-12-10T01:50:32.567749: step 83, loss 1.20298, acc 0.697266, prec 0.0342405, recall 0.785625
2017-12-10T01:50:35.850616: step 84, loss 1.24624, acc 0.708008, prec 0.0344124, recall 0.78706
2017-12-10T01:50:39.146839: step 85, loss 1.18611, acc 0.737793, prec 0.0345494, recall 0.788112
2017-12-10T01:50:42.483806: step 86, loss 0.915543, acc 0.736328, prec 0.0345784, recall 0.789579
2017-12-10T01:50:45.805487: step 87, loss 0.703348, acc 0.791504, prec 0.034708, recall 0.791501
2017-12-10T01:50:49.097609: step 88, loss 0.999158, acc 0.843262, prec 0.034986, recall 0.792238
2017-12-10T01:50:52.419179: step 89, loss 0.964119, acc 0.838867, prec 0.0353382, recall 0.793357
2017-12-10T01:50:55.756670: step 90, loss 1.87417, acc 0.848145, prec 0.035487, recall 0.791603
2017-12-10T01:50:59.195726: step 91, loss 0.861785, acc 0.849609, prec 0.0357301, recall 0.792453
2017-12-10T01:51:02.543951: step 92, loss 1.43858, acc 0.835938, prec 0.0359221, recall 0.792537
2017-12-10T01:51:05.841458: step 93, loss 1.72226, acc 0.799316, prec 0.0360837, recall 0.792112
2017-12-10T01:51:09.148280: step 94, loss 1.04643, acc 0.752441, prec 0.0361173, recall 0.793129
2017-12-10T01:51:12.459191: step 95, loss 1.09124, acc 0.709961, prec 0.0361773, recall 0.794788
2017-12-10T01:51:14.240034: step 96, loss 0.998716, acc 0.70539, prec 0.0362263, recall 0.795896
2017-12-10T01:51:17.554262: step 97, loss 1.05314, acc 0.694824, prec 0.0362661, recall 0.797504
2017-12-10T01:51:20.847528: step 98, loss 0.938047, acc 0.730469, prec 0.0364714, recall 0.799648
2017-12-10T01:51:24.170450: step 99, loss 0.857924, acc 0.78125, prec 0.0365961, recall 0.800837
2017-12-10T01:51:27.488354: step 100, loss 0.745442, acc 0.791016, prec 0.0367147, recall 0.802213
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_2048_fold_3/1512888362/checkpoints/model-100

2017-12-10T01:51:31.800465: step 101, loss 1.19709, acc 0.853516, prec 0.037135, recall 0.803748
2017-12-10T01:51:35.108541: step 102, loss 0.857009, acc 0.862793, prec 0.0372158, recall 0.803451
2017-12-10T01:51:38.433536: step 103, loss 1.00792, acc 0.873535, prec 0.0373399, recall 0.802751
2017-12-10T01:51:41.799913: step 104, loss 0.91113, acc 0.854004, prec 0.0376465, recall 0.804045
2017-12-10T01:51:45.117715: step 105, loss 1.08086, acc 0.864746, prec 0.0378309, recall 0.803941
2017-12-10T01:51:48.461962: step 106, loss 1.34069, acc 0.856934, prec 0.03808, recall 0.803374
2017-12-10T01:51:51.798800: step 107, loss 0.823412, acc 0.806641, prec 0.0381159, recall 0.803738
2017-12-10T01:51:55.120543: step 108, loss 0.904054, acc 0.804688, prec 0.0382799, recall 0.804917
2017-12-10T01:51:58.444043: step 109, loss 0.899109, acc 0.78125, prec 0.038443, recall 0.806197
2017-12-10T01:52:01.769620: step 110, loss 0.815373, acc 0.752441, prec 0.0384972, recall 0.807403
2017-12-10T01:52:05.072831: step 111, loss 1.0155, acc 0.791992, prec 0.038613, recall 0.807896
2017-12-10T01:52:06.883351: step 112, loss 0.939893, acc 0.769517, prec 0.038799, recall 0.80901
2017-12-10T01:52:10.197626: step 113, loss 0.681675, acc 0.77002, prec 0.0388836, recall 0.810472
2017-12-10T01:52:13.593889: step 114, loss 0.660256, acc 0.809082, prec 0.0390826, recall 0.812197
2017-12-10T01:52:16.905856: step 115, loss 0.822087, acc 0.838379, prec 0.0393151, recall 0.813157
2017-12-10T01:52:20.225428: step 116, loss 0.798803, acc 0.846191, prec 0.0393738, recall 0.813377
2017-12-10T01:52:23.572491: step 117, loss 1.01761, acc 0.875977, prec 0.039662, recall 0.813644
2017-12-10T01:52:26.932738: step 118, loss 0.553892, acc 0.89209, prec 0.039841, recall 0.814131
2017-12-10T01:52:30.244788: step 119, loss 0.796394, acc 0.870605, prec 0.0400623, recall 0.814879
2017-12-10T01:52:33.571227: step 120, loss 0.700787, acc 0.884766, prec 0.040217, recall 0.815061
2017-12-10T01:52:36.894059: step 121, loss 0.719796, acc 0.869141, prec 0.040584, recall 0.815902
2017-12-10T01:52:40.200780: step 122, loss 0.851755, acc 0.854004, prec 0.0407671, recall 0.816332
2017-12-10T01:52:43.463232: step 123, loss 0.563817, acc 0.847656, prec 0.0409528, recall 0.817494
2017-12-10T01:52:46.754370: step 124, loss 0.658634, acc 0.827637, prec 0.0411941, recall 0.818712
2017-12-10T01:52:50.060474: step 125, loss 0.827035, acc 0.82373, prec 0.0413494, recall 0.819158
2017-12-10T01:52:53.394631: step 126, loss 0.752771, acc 0.816406, prec 0.0414536, recall 0.819896
2017-12-10T01:52:56.700937: step 127, loss 0.625929, acc 0.831055, prec 0.0416254, recall 0.821264
2017-12-10T01:52:58.513572: step 128, loss 0.639511, acc 0.814126, prec 0.0417221, recall 0.821814
2017-12-10T01:53:01.821342: step 129, loss 0.599358, acc 0.829102, prec 0.0419291, recall 0.822853
2017-12-10T01:53:05.154759: step 130, loss 0.91046, acc 0.844238, prec 0.0420755, recall 0.822936
2017-12-10T01:53:08.490339: step 131, loss 0.606031, acc 0.847168, prec 0.0422864, recall 0.824118
2017-12-10T01:53:11.776637: step 132, loss 0.641215, acc 0.84668, prec 0.042496, recall 0.82485
2017-12-10T01:53:15.074125: step 133, loss 0.473202, acc 0.864258, prec 0.0425563, recall 0.825409
2017-12-10T01:53:18.395918: step 134, loss 0.487338, acc 0.868652, prec 0.0428379, recall 0.826938
2017-12-10T01:53:21.694387: step 135, loss 0.826053, acc 0.872559, prec 0.0430611, recall 0.827164
2017-12-10T01:53:25.026436: step 136, loss 0.676304, acc 0.89209, prec 0.0432784, recall 0.827927
2017-12-10T01:53:28.360818: step 137, loss 0.484388, acc 0.880371, prec 0.0433916, recall 0.828586
2017-12-10T01:53:31.681469: step 138, loss 0.573662, acc 0.884766, prec 0.0436361, recall 0.82925
2017-12-10T01:53:34.991229: step 139, loss 0.571927, acc 0.867676, prec 0.0438213, recall 0.829777
2017-12-10T01:53:38.338039: step 140, loss 0.490595, acc 0.870605, prec 0.0439948, recall 0.830663
2017-12-10T01:53:41.643963: step 141, loss 0.619565, acc 0.874512, prec 0.0443225, recall 0.831422
2017-12-10T01:53:45.023544: step 142, loss 0.555518, acc 0.870117, prec 0.0444557, recall 0.831959
2017-12-10T01:53:48.345185: step 143, loss 0.590401, acc 0.854492, prec 0.0445818, recall 0.83253
2017-12-10T01:53:50.154287: step 144, loss 0.708801, acc 0.857807, prec 0.0446914, recall 0.832973
2017-12-10T01:53:53.487413: step 145, loss 0.542284, acc 0.865234, prec 0.0449381, recall 0.834087
2017-12-10T01:53:56.788962: step 146, loss 0.762221, acc 0.853027, prec 0.0451933, recall 0.835064
2017-12-10T01:54:00.102441: step 147, loss 0.500141, acc 0.835449, prec 0.045376, recall 0.836261
2017-12-10T01:54:03.500793: step 148, loss 0.633673, acc 0.82959, prec 0.0455149, recall 0.836939
2017-12-10T01:54:06.856273: step 149, loss 0.506089, acc 0.858887, prec 0.0456378, recall 0.837844
2017-12-10T01:54:10.158995: step 150, loss 0.544373, acc 0.853027, prec 0.0458612, recall 0.83888
2017-12-10T01:54:13.494053: step 151, loss 0.464847, acc 0.874023, prec 0.0461308, recall 0.840164
2017-12-10T01:54:16.818890: step 152, loss 0.491603, acc 0.879883, prec 0.0463827, recall 0.840976
2017-12-10T01:54:20.138571: step 153, loss 0.459347, acc 0.89209, prec 0.0465408, recall 0.841455
2017-12-10T01:54:23.426474: step 154, loss 0.437942, acc 0.898438, prec 0.0466931, recall 0.842082
2017-12-10T01:54:26.739889: step 155, loss 0.511905, acc 0.89209, prec 0.0467675, recall 0.842117
2017-12-10T01:54:30.034498: step 156, loss 0.521722, acc 0.920898, prec 0.0470171, recall 0.842384
2017-12-10T01:54:33.394481: step 157, loss 0.672097, acc 0.913574, prec 0.047152, recall 0.842151
2017-12-10T01:54:36.750647: step 158, loss 0.597492, acc 0.906738, prec 0.0474177, recall 0.842518
2017-12-10T01:54:40.051752: step 159, loss 0.341982, acc 0.899414, prec 0.047601, recall 0.843405
2017-12-10T01:54:41.834352: step 160, loss 0.805272, acc 0.874535, prec 0.0476516, recall 0.843413
Training finished
Starting Experiment - batch_size_4096 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 4096
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_4096_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_4096_fold_0/1512888882

Start training
2017-12-10T01:54:51.966409: step 1, loss 6.03065, acc 0.185791, prec 0.0145056, recall 0.890909
2017-12-10T01:54:58.509075: step 2, loss 5.27484, acc 0.555664, prec 0.0157662, recall 0.694915
2017-12-10T01:55:04.986242: step 3, loss 5.38011, acc 0.653564, prec 0.0166238, recall 0.621469
2017-12-10T01:55:11.569717: step 4, loss 6.69932, acc 0.639893, prec 0.017186, recall 0.58159
2017-12-10T01:55:18.105676: step 5, loss 4.78735, acc 0.52832, prec 0.0177397, recall 0.587459
2017-12-10T01:55:24.647470: step 6, loss 3.29791, acc 0.43042, prec 0.0178183, recall 0.626062
2017-12-10T01:55:31.221801: step 7, loss 4.3777, acc 0.379639, prec 0.0178321, recall 0.641827
2017-12-10T01:55:36.214484: step 8, loss 3.56199, acc 0.379763, prec 0.0175977, recall 0.656388
2017-12-10T01:55:42.780535: step 9, loss 3.70841, acc 0.472412, prec 0.0180336, recall 0.673828
2017-12-10T01:55:49.366655: step 10, loss 3.79543, acc 0.533203, prec 0.0179046, recall 0.669627
2017-12-10T01:55:55.988819: step 11, loss 3.27747, acc 0.603271, prec 0.0186718, recall 0.673016
2017-12-10T01:56:02.571424: step 12, loss 3.51048, acc 0.669189, prec 0.0191837, recall 0.673469
2017-12-10T01:56:09.161025: step 13, loss 2.96959, acc 0.686523, prec 0.019926, recall 0.677376
2017-12-10T01:56:15.694546: step 14, loss 3.54528, acc 0.694336, prec 0.0202558, recall 0.673317
2017-12-10T01:56:22.292330: step 15, loss 3.26024, acc 0.698975, prec 0.0204981, recall 0.670574
2017-12-10T01:56:27.266952: step 16, loss 2.69199, acc 0.665066, prec 0.0212861, recall 0.679515
2017-12-10T01:56:33.751682: step 17, loss 3.35481, acc 0.64209, prec 0.021814, recall 0.684861
2017-12-10T01:56:40.384782: step 18, loss 2.61134, acc 0.610107, prec 0.0223211, recall 0.692754
2017-12-10T01:56:46.893858: step 19, loss 2.82136, acc 0.593994, prec 0.0225297, recall 0.700368
2017-12-10T01:56:53.476533: step 20, loss 2.80549, acc 0.61084, prec 0.0229849, recall 0.707465
2017-12-10T01:56:59.977246: step 21, loss 3.02202, acc 0.604492, prec 0.023017, recall 0.708714
2017-12-10T01:57:06.573369: step 22, loss 2.60569, acc 0.630615, prec 0.023562, recall 0.716758
2017-12-10T01:57:13.107231: step 23, loss 1.84775, acc 0.647461, prec 0.0238095, recall 0.725892
2017-12-10T01:57:18.079000: step 24, loss 3.85331, acc 0.696126, prec 0.0239346, recall 0.722467
2017-12-10T01:57:24.603242: step 25, loss 2.38914, acc 0.715576, prec 0.0239987, recall 0.721393
2017-12-10T01:57:31.108899: step 26, loss 2.4803, acc 0.723877, prec 0.0241252, recall 0.718793
2017-12-10T01:57:37.642708: step 27, loss 2.96587, acc 0.729736, prec 0.0245227, recall 0.717192
2017-12-10T01:57:44.208073: step 28, loss 1.98173, acc 0.708496, prec 0.0251681, recall 0.721979
2017-12-10T01:57:50.736637: step 29, loss 2.30325, acc 0.677246, prec 0.0254007, recall 0.722993
2017-12-10T01:57:57.239964: step 30, loss 2.16739, acc 0.665527, prec 0.0256336, recall 0.725524
2017-12-10T01:58:03.808770: step 31, loss 1.90027, acc 0.648193, prec 0.0257937, recall 0.730617
2017-12-10T01:58:08.829996: step 32, loss 2.20521, acc 0.632725, prec 0.0260369, recall 0.734581
2017-12-10T01:58:15.347180: step 33, loss 2.02647, acc 0.661377, prec 0.0263338, recall 0.740128
2017-12-10T01:58:21.957869: step 34, loss 2.20451, acc 0.664795, prec 0.0265873, recall 0.743921
2017-12-10T01:58:28.506321: step 35, loss 2.05803, acc 0.70874, prec 0.0269, recall 0.745491
2017-12-10T01:58:35.030378: step 36, loss 1.98586, acc 0.74585, prec 0.0270467, recall 0.746451
2017-12-10T01:58:41.585023: step 37, loss 2.40425, acc 0.755859, prec 0.0272102, recall 0.745229
2017-12-10T01:58:48.139521: step 38, loss 1.77668, acc 0.762207, prec 0.0275619, recall 0.746407
2017-12-10T01:58:54.674771: step 39, loss 2.12696, acc 0.757812, prec 0.027911, recall 0.746625
2017-12-10T01:58:59.649877: step 40, loss 2.05969, acc 0.755043, prec 0.0282214, recall 0.748899
2017-12-10T01:59:06.249630: step 41, loss 1.97693, acc 0.729736, prec 0.0284932, recall 0.750644
2017-12-10T01:59:12.868982: step 42, loss 2.0788, acc 0.7146, prec 0.0289598, recall 0.753533
2017-12-10T01:59:19.381681: step 43, loss 2.09488, acc 0.6875, prec 0.0290984, recall 0.755177
2017-12-10T01:59:26.025536: step 44, loss 2.05123, acc 0.673584, prec 0.0292789, recall 0.758128
2017-12-10T01:59:32.644608: step 45, loss 1.84629, acc 0.674316, prec 0.0293809, recall 0.760186
2017-12-10T01:59:39.157397: step 46, loss 1.43556, acc 0.702637, prec 0.0295841, recall 0.763968
2017-12-10T01:59:45.683024: step 47, loss 1.87069, acc 0.718506, prec 0.0296705, recall 0.764728
2017-12-10T01:59:50.733442: step 48, loss 1.64965, acc 0.742555, prec 0.0298179, recall 0.765786
2017-12-10T01:59:57.250595: step 49, loss 1.49013, acc 0.768066, prec 0.0301061, recall 0.767517
2017-12-10T02:00:03.853708: step 50, loss 2.08228, acc 0.796631, prec 0.0303435, recall 0.767254
2017-12-10T02:00:10.364996: step 51, loss 1.8241, acc 0.806396, prec 0.0306464, recall 0.766793
2017-12-10T02:00:16.861566: step 52, loss 1.95171, acc 0.774658, prec 0.0307145, recall 0.766621
2017-12-10T02:00:23.528709: step 53, loss 1.26248, acc 0.767578, prec 0.0310912, recall 0.76992
2017-12-10T02:00:30.110724: step 54, loss 1.4143, acc 0.763672, prec 0.0314649, recall 0.772653
2017-12-10T02:00:36.697229: step 55, loss 1.84096, acc 0.712891, prec 0.0314416, recall 0.772553
2017-12-10T02:00:41.680343: step 56, loss 1.37541, acc 0.720461, prec 0.0317003, recall 0.77533
2017-12-10T02:00:48.271254: step 57, loss 1.36246, acc 0.71582, prec 0.0318976, recall 0.777641
2017-12-10T02:00:54.783872: step 58, loss 1.12324, acc 0.73877, prec 0.0319078, recall 0.779371
2017-12-10T02:01:01.349198: step 59, loss 1.30132, acc 0.765381, prec 0.0322246, recall 0.781802
2017-12-10T02:01:07.870839: step 60, loss 1.57781, acc 0.790039, prec 0.0325411, recall 0.782341
2017-12-10T02:01:14.415032: step 61, loss 1.21843, acc 0.804443, prec 0.0328031, recall 0.782935
2017-12-10T02:01:20.969598: step 62, loss 1.01852, acc 0.808594, prec 0.033146, recall 0.784824
2017-12-10T02:01:27.496148: step 63, loss 1.66387, acc 0.797119, prec 0.0333057, recall 0.7845
2017-12-10T02:01:32.496473: step 64, loss 1.46301, acc 0.826129, prec 0.0335058, recall 0.784692
2017-12-10T02:01:39.125682: step 65, loss 1.11059, acc 0.793701, prec 0.033765, recall 0.785598
2017-12-10T02:01:45.671524: step 66, loss 1.12664, acc 0.772461, prec 0.0339825, recall 0.787733
2017-12-10T02:01:52.298475: step 67, loss 1.04202, acc 0.785156, prec 0.0342174, recall 0.788769
2017-12-10T02:01:58.821636: step 68, loss 1.00747, acc 0.77124, prec 0.0343034, recall 0.790402
2017-12-10T02:02:05.329902: step 69, loss 1.24757, acc 0.779541, prec 0.0347251, recall 0.792578
2017-12-10T02:02:11.866067: step 70, loss 0.877679, acc 0.792725, prec 0.0348912, recall 0.794177
2017-12-10T02:02:18.369100: step 71, loss 0.95764, acc 0.809326, prec 0.0352073, recall 0.795949
2017-12-10T02:02:23.453543: step 72, loss 0.828009, acc 0.817803, prec 0.0353677, recall 0.797357
2017-12-10T02:02:29.977307: step 73, loss 0.896902, acc 0.840576, prec 0.0357547, recall 0.799374
2017-12-10T02:02:36.552330: step 74, loss 1.00728, acc 0.848633, prec 0.0360573, recall 0.800142
2017-12-10T02:02:43.073396: step 75, loss 1.20916, acc 0.855957, prec 0.0363772, recall 0.801123
2017-12-10T02:02:49.564188: step 76, loss 1.33633, acc 0.860107, prec 0.0365596, recall 0.800139
2017-12-10T02:02:56.165166: step 77, loss 1.10841, acc 0.827881, prec 0.0367662, recall 0.801187
2017-12-10T02:03:02.673251: step 78, loss 0.863612, acc 0.820312, prec 0.0370382, recall 0.802386
2017-12-10T02:03:09.266109: step 79, loss 1.06667, acc 0.778076, prec 0.0371209, recall 0.802848
2017-12-10T02:03:14.288742: step 80, loss 1.36962, acc 0.770733, prec 0.0372545, recall 0.803744
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 4096
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_4096_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_4096_fold_1/1512889395

Start training
2017-12-10T02:03:24.351815: step 1, loss 4.11593, acc 0.374268, prec 0.011646, recall 0.638298
2017-12-10T02:03:30.870199: step 2, loss 7.04942, acc 0.697021, prec 0.0126616, recall 0.440367
2017-12-10T02:03:37.433573: step 3, loss 4.92272, acc 0.56958, prec 0.0143988, recall 0.467836
2017-12-10T02:03:43.972622: step 4, loss 4.27083, acc 0.380615, prec 0.0156327, recall 0.542735
2017-12-10T02:03:50.480542: step 5, loss 5.31107, acc 0.315186, prec 0.0157818, recall 0.590444
2017-12-10T02:03:56.997222: step 6, loss 4.74975, acc 0.306641, prec 0.0163994, recall 0.628809
2017-12-10T02:04:03.490714: step 7, loss 3.65149, acc 0.390381, prec 0.0167308, recall 0.657074
2017-12-10T02:04:08.504907: step 8, loss 3.72483, acc 0.495037, prec 0.0171915, recall 0.66167
2017-12-10T02:04:15.095853: step 9, loss 4.19987, acc 0.586426, prec 0.0173299, recall 0.653257
2017-12-10T02:04:21.677616: step 10, loss 5.53518, acc 0.660889, prec 0.018036, recall 0.636516
2017-12-10T02:04:28.208180: step 11, loss 3.70175, acc 0.635498, prec 0.0185981, recall 0.640244
2017-12-10T02:04:34.777618: step 12, loss 3.10213, acc 0.618408, prec 0.0187516, recall 0.638028
2017-12-10T02:04:41.337411: step 13, loss 3.48552, acc 0.584961, prec 0.0192731, recall 0.648895
2017-12-10T02:04:47.881506: step 14, loss 3.22925, acc 0.544189, prec 0.0199993, recall 0.661905
2017-12-10T02:04:54.401718: step 15, loss 3.12497, acc 0.533447, prec 0.0201049, recall 0.670404
2017-12-10T02:04:59.460739: step 16, loss 2.95943, acc 0.543068, prec 0.0202584, recall 0.67666
2017-12-10T02:05:06.106023: step 17, loss 2.86448, acc 0.58252, prec 0.020314, recall 0.681263
2017-12-10T02:05:12.623568: step 18, loss 2.85747, acc 0.618652, prec 0.0207374, recall 0.685824
2017-12-10T02:05:19.193448: step 19, loss 2.72534, acc 0.645752, prec 0.0210243, recall 0.689435
2017-12-10T02:05:25.773904: step 20, loss 3.3661, acc 0.673828, prec 0.0214884, recall 0.690456
2017-12-10T02:05:32.305570: step 21, loss 2.32895, acc 0.687744, prec 0.0219499, recall 0.693627
2017-12-10T02:05:38.832746: step 22, loss 3.10736, acc 0.689697, prec 0.0223406, recall 0.692248
2017-12-10T02:05:45.346719: step 23, loss 2.56664, acc 0.681396, prec 0.0227328, recall 0.694013
2017-12-10T02:05:50.367219: step 24, loss 2.63448, acc 0.652898, prec 0.0230327, recall 0.697359
2017-12-10T02:05:56.959137: step 25, loss 3.08038, acc 0.64502, prec 0.0233241, recall 0.699454
2017-12-10T02:06:03.491979: step 26, loss 2.45768, acc 0.623291, prec 0.02372, recall 0.704768
2017-12-10T02:06:10.064873: step 27, loss 2.44888, acc 0.61084, prec 0.0238131, recall 0.708781
2017-12-10T02:06:16.581501: step 28, loss 2.38467, acc 0.60498, prec 0.0240287, recall 0.713329
2017-12-10T02:06:23.211495: step 29, loss 2.53384, acc 0.635742, prec 0.0243292, recall 0.71831
2017-12-10T02:06:29.706832: step 30, loss 2.7367, acc 0.652832, prec 0.0245702, recall 0.721907
2017-12-10T02:06:36.283084: step 31, loss 3.02577, acc 0.656982, prec 0.0247547, recall 0.721644
2017-12-10T02:06:41.243720: step 32, loss 2.40588, acc 0.683317, prec 0.0248838, recall 0.722163
2017-12-10T02:06:47.788270: step 33, loss 1.97722, acc 0.694092, prec 0.0250649, recall 0.724102
2017-12-10T02:06:54.492055: step 34, loss 2.57526, acc 0.70166, prec 0.0252877, recall 0.724747
2017-12-10T02:07:01.069896: step 35, loss 2.40764, acc 0.720215, prec 0.0256853, recall 0.726917
2017-12-10T02:07:07.650442: step 36, loss 2.29053, acc 0.707764, prec 0.0260925, recall 0.730369
2017-12-10T02:07:14.171052: step 37, loss 2.46672, acc 0.69873, prec 0.0265334, recall 0.73242
2017-12-10T02:07:20.725394: step 38, loss 1.64142, acc 0.668945, prec 0.0265047, recall 0.734978
2017-12-10T02:07:27.299286: step 39, loss 2.18356, acc 0.667236, prec 0.0267702, recall 0.737048
2017-12-10T02:07:32.381881: step 40, loss 1.74592, acc 0.676273, prec 0.0268509, recall 0.739186
2017-12-10T02:07:38.981619: step 41, loss 1.7217, acc 0.67041, prec 0.0271468, recall 0.742917
2017-12-10T02:07:45.543291: step 42, loss 1.93094, acc 0.681396, prec 0.0275938, recall 0.747475
2017-12-10T02:07:52.301972: step 43, loss 1.57131, acc 0.724365, prec 0.0278099, recall 0.750099
2017-12-10T02:07:58.842480: step 44, loss 1.81535, acc 0.731689, prec 0.0279492, recall 0.75029
2017-12-10T02:08:05.468777: step 45, loss 2.07218, acc 0.760498, prec 0.0282567, recall 0.750472
2017-12-10T02:08:11.988863: step 46, loss 1.59193, acc 0.767822, prec 0.028579, recall 0.752121
2017-12-10T02:08:18.608149: step 47, loss 1.25421, acc 0.76001, prec 0.02877, recall 0.754621
2017-12-10T02:08:23.605773: step 48, loss 1.37029, acc 0.755043, prec 0.0289885, recall 0.756959
2017-12-10T02:08:30.137278: step 49, loss 1.33503, acc 0.765869, prec 0.0291409, recall 0.75798
2017-12-10T02:08:36.715456: step 50, loss 1.46558, acc 0.774658, prec 0.0294329, recall 0.759533
2017-12-10T02:08:43.214536: step 51, loss 1.00834, acc 0.799316, prec 0.0295763, recall 0.761179
2017-12-10T02:08:49.755507: step 52, loss 1.07391, acc 0.791504, prec 0.0298311, recall 0.763062
2017-12-10T02:08:56.306067: step 53, loss 1.38214, acc 0.79126, prec 0.030243, recall 0.765127
2017-12-10T02:09:02.826027: step 54, loss 1.73899, acc 0.788818, prec 0.0306058, recall 0.766147
2017-12-10T02:09:09.437294: step 55, loss 1.90649, acc 0.772705, prec 0.031057, recall 0.76737
2017-12-10T02:09:14.457460: step 56, loss 1.34919, acc 0.757925, prec 0.0312139, recall 0.768125
2017-12-10T02:09:21.006920: step 57, loss 1.3609, acc 0.700195, prec 0.031505, recall 0.771651
2017-12-10T02:09:27.560183: step 58, loss 1.51259, acc 0.677002, prec 0.0316463, recall 0.774213
2017-12-10T02:09:34.242488: step 59, loss 1.38781, acc 0.676758, prec 0.0318511, recall 0.777296
2017-12-10T02:09:40.811316: step 60, loss 1.435, acc 0.687256, prec 0.0319528, recall 0.77942
2017-12-10T02:09:47.352722: step 61, loss 1.5113, acc 0.707764, prec 0.0321829, recall 0.781808
2017-12-10T02:09:53.934594: step 62, loss 1.11264, acc 0.746826, prec 0.0323444, recall 0.783457
2017-12-10T02:10:00.475827: step 63, loss 1.28594, acc 0.789062, prec 0.0324917, recall 0.783162
2017-12-10T02:10:05.462899: step 64, loss 1.43864, acc 0.819404, prec 0.0326468, recall 0.783191
2017-12-10T02:10:11.965599: step 65, loss 1.16686, acc 0.828125, prec 0.0328446, recall 0.784148
2017-12-10T02:10:18.501068: step 66, loss 1.39925, acc 0.839111, prec 0.0331649, recall 0.783805
2017-12-10T02:10:24.978579: step 67, loss 1.02078, acc 0.83252, prec 0.0334568, recall 0.785221
2017-12-10T02:10:31.502068: step 68, loss 1.44577, acc 0.816162, prec 0.03367, recall 0.784941
2017-12-10T02:10:38.054677: step 69, loss 1.1483, acc 0.77832, prec 0.0338216, recall 0.785643
2017-12-10T02:10:44.676093: step 70, loss 1.22383, acc 0.76123, prec 0.0340885, recall 0.786865
2017-12-10T02:10:51.304887: step 71, loss 1.25817, acc 0.731934, prec 0.0343051, recall 0.788803
2017-12-10T02:10:56.331783: step 72, loss 1.06213, acc 0.7195, prec 0.0343846, recall 0.790388
2017-12-10T02:11:02.909775: step 73, loss 1.20366, acc 0.723389, prec 0.0344817, recall 0.791921
2017-12-10T02:11:09.425044: step 74, loss 1.24176, acc 0.750244, prec 0.0346648, recall 0.793103
2017-12-10T02:11:16.037492: step 75, loss 1.09566, acc 0.795654, prec 0.0349771, recall 0.794399
2017-12-10T02:11:22.620832: step 76, loss 1.19545, acc 0.797119, prec 0.0350945, recall 0.794554
2017-12-10T02:11:29.129537: step 77, loss 0.987314, acc 0.835205, prec 0.0354147, recall 0.796317
2017-12-10T02:11:35.684090: step 78, loss 1.15949, acc 0.828125, prec 0.0357412, recall 0.797073
2017-12-10T02:11:42.345838: step 79, loss 1.07527, acc 0.830078, prec 0.0359244, recall 0.797496
2017-12-10T02:11:47.332980: step 80, loss 1.11475, acc 0.810118, prec 0.0360278, recall 0.798073
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 4096
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_4096_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_4096_fold_2/1512889908

Start training
2017-12-10T02:11:57.531223: step 1, loss 10.2386, acc 0.755371, prec 0.0105042, recall 0.142857
2017-12-10T02:12:04.151151: step 2, loss 5.60716, acc 0.385498, prec 0.011802, recall 0.322835
2017-12-10T02:12:10.651841: step 3, loss 5.85043, acc 0.187012, prec 0.0125694, recall 0.480447
2017-12-10T02:12:17.142325: step 4, loss 5.8751, acc 0.181641, prec 0.0135716, recall 0.586498
2017-12-10T02:12:23.684941: step 5, loss 5.51131, acc 0.238037, prec 0.0143209, recall 0.64214
2017-12-10T02:12:30.248258: step 6, loss 5.14326, acc 0.349121, prec 0.0150245, recall 0.663014
2017-12-10T02:12:36.912916: step 7, loss 3.81268, acc 0.48999, prec 0.0152098, recall 0.661098
2017-12-10T02:12:41.912710: step 8, loss 4.53052, acc 0.605634, prec 0.0157318, recall 0.651064
2017-12-10T02:12:48.451350: step 9, loss 3.06985, acc 0.683594, prec 0.0160902, recall 0.648544
2017-12-10T02:12:55.117479: step 10, loss 5.01384, acc 0.721924, prec 0.0165844, recall 0.623711
2017-12-10T02:13:01.671649: step 11, loss 3.68345, acc 0.713623, prec 0.0169102, recall 0.615142
2017-12-10T02:13:08.231310: step 12, loss 3.16791, acc 0.669434, prec 0.0172357, recall 0.612809
2017-12-10T02:13:14.791403: step 13, loss 2.63183, acc 0.631592, prec 0.0178723, recall 0.621984
2017-12-10T02:13:21.316411: step 14, loss 3.61485, acc 0.598877, prec 0.0182799, recall 0.625775
2017-12-10T02:13:27.967455: step 15, loss 3.51237, acc 0.553955, prec 0.0189888, recall 0.637088
2017-12-10T02:13:32.986318: step 16, loss 2.99295, acc 0.503521, prec 0.0197806, recall 0.654255
2017-12-10T02:13:39.535927: step 17, loss 3.29295, acc 0.51001, prec 0.0199789, recall 0.663992
2017-12-10T02:13:46.130426: step 18, loss 3.3093, acc 0.546875, prec 0.0201011, recall 0.671115
2017-12-10T02:13:52.683561: step 19, loss 3.51059, acc 0.536865, prec 0.0204032, recall 0.67684
2017-12-10T02:13:59.313527: step 20, loss 2.68599, acc 0.596436, prec 0.0207514, recall 0.68547
2017-12-10T02:14:05.848924: step 21, loss 2.99101, acc 0.633789, prec 0.0212983, recall 0.693117
