Vocabulary Size: 33447
encodedPathwayA = 8 encodedPathwayB = 53
Varying batch size
Starting Experiment - batch_size_128 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 128
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_128_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785

Start training
2017-12-09T22:16:28.598166: step 1, loss 6.55866, acc 0.125, prec 0, recall 0
2017-12-09T22:16:28.899483: step 2, loss 2.23368, acc 0.515625, prec 0.00571429, recall 1
2017-12-09T22:16:29.200340: step 3, loss 9.94834, acc 0.820312, prec 0.00507614, recall 0.5
2017-12-09T22:16:29.503589: step 4, loss 7.42655, acc 0.890625, prec 0.00478469, recall 0.25
2017-12-09T22:16:29.811780: step 5, loss 2.68158, acc 0.875, prec 0.00446429, recall 0.2
2017-12-09T22:16:30.123452: step 6, loss 7.02969, acc 0.835938, prec 0.00411523, recall 0.142857
2017-12-09T22:16:30.431967: step 7, loss 25.8904, acc 0.742188, prec 0.003663, recall 0.1
2017-12-09T22:16:30.736391: step 8, loss 5.41813, acc 0.609375, prec 0.00310559, recall 0.0909091
2017-12-09T22:16:31.034311: step 9, loss 8.41496, acc 0.484375, prec 0.00771208, recall 0.214286
2017-12-09T22:16:31.334922: step 10, loss 8.43773, acc 0.382812, prec 0.010661, recall 0.294118
2017-12-09T22:16:31.632995: step 11, loss 7.60528, acc 0.203125, prec 0.00877193, recall 0.277778
2017-12-09T22:16:31.933973: step 12, loss 6.51333, acc 0.179688, prec 0.00890208, recall 0.285714
2017-12-09T22:16:32.232974: step 13, loss 6.5728, acc 0.109375, prec 0.011378, recall 0.375
2017-12-09T22:16:32.530014: step 14, loss 8.17455, acc 0.078125, prec 0.0110011, recall 0.384615
2017-12-09T22:16:32.826070: step 15, loss 7.87421, acc 0.0546875, prec 0.0106693, recall 0.407407
2017-12-09T22:16:33.126544: step 16, loss 7.89121, acc 0.0546875, prec 0.0112652, recall 0.448276
2017-12-09T22:16:33.428358: step 17, loss 6.82076, acc 0.117188, prec 0.011041, recall 0.466667
2017-12-09T22:16:33.724024: step 18, loss 6.30788, acc 0.0859375, prec 0.0101083, recall 0.466667
2017-12-09T22:16:34.015835: step 19, loss 4.74558, acc 0.210938, prec 0.0114171, recall 0.515152
2017-12-09T22:16:34.315080: step 20, loss 6.10038, acc 0.40625, prec 0.0121328, recall 0.527778
2017-12-09T22:16:34.612179: step 21, loss 3.33851, acc 0.351562, prec 0.0127196, recall 0.552632
2017-12-09T22:16:34.905101: step 22, loss 6.33357, acc 0.445312, prec 0.0133488, recall 0.560976
2017-12-09T22:16:35.207952: step 23, loss 2.54661, acc 0.523438, prec 0.0139978, recall 0.581395
2017-12-09T22:16:35.509431: step 24, loss 4.60105, acc 0.679688, prec 0.0136911, recall 0.568182
2017-12-09T22:16:35.812087: step 25, loss 1.17828, acc 0.671875, prec 0.0139112, recall 0.577778
2017-12-09T22:16:36.115751: step 26, loss 9.34453, acc 0.765625, prec 0.0136986, recall 0.565217
2017-12-09T22:16:36.416028: step 27, loss 7.85832, acc 0.765625, prec 0.0150259, recall 0.58
2017-12-09T22:16:36.716892: step 28, loss 7.54147, acc 0.796875, prec 0.0148414, recall 0.557692
2017-12-09T22:16:37.016781: step 29, loss 4.74406, acc 0.796875, prec 0.0146539, recall 0.54717
2017-12-09T22:16:37.319091: step 30, loss 3.05854, acc 0.710938, prec 0.014881, recall 0.545455
2017-12-09T22:16:37.619503: step 31, loss 5.10154, acc 0.726562, prec 0.0146341, recall 0.535714
2017-12-09T22:16:37.919955: step 32, loss 1.43806, acc 0.671875, prec 0.0148113, recall 0.54386
2017-12-09T22:16:38.216400: step 33, loss 3.43008, acc 0.554688, prec 0.0148837, recall 0.542373
2017-12-09T22:16:38.515230: step 34, loss 1.87975, acc 0.554688, prec 0.0153916, recall 0.557377
2017-12-09T22:16:38.819053: step 35, loss 2.3509, acc 0.476562, prec 0.0153711, recall 0.564516
2017-12-09T22:16:39.121162: step 36, loss 8.67746, acc 0.578125, prec 0.015444, recall 0.5625
2017-12-09T22:16:39.420589: step 37, loss 1.94819, acc 0.539062, prec 0.0150628, recall 0.5625
2017-12-09T22:16:39.713272: step 38, loss 3.15321, acc 0.4375, prec 0.0146223, recall 0.5625
2017-12-09T22:16:40.024298: step 39, loss 2.64757, acc 0.515625, prec 0.0158228, recall 0.588235
2017-12-09T22:16:40.330919: step 40, loss 2.13385, acc 0.53125, prec 0.0158362, recall 0.594203
2017-12-09T22:16:40.625288: step 41, loss 1.5524, acc 0.578125, prec 0.0155127, recall 0.594203
2017-12-09T22:16:40.928849: step 42, loss 2.47772, acc 0.5, prec 0.0155096, recall 0.6
2017-12-09T22:16:41.231519: step 43, loss 4.78909, acc 0.601562, prec 0.0162984, recall 0.608108
2017-12-09T22:16:41.530789: step 44, loss 1.43337, acc 0.703125, prec 0.0171306, recall 0.623377
2017-12-09T22:16:41.831460: step 45, loss 3.48452, acc 0.65625, prec 0.0168717, recall 0.615385
2017-12-09T22:16:42.134037: step 46, loss 1.93668, acc 0.71875, prec 0.0166667, recall 0.607595
2017-12-09T22:16:42.433284: step 47, loss 1.31644, acc 0.664062, prec 0.0167579, recall 0.6125
2017-12-09T22:16:42.729642: step 48, loss 1.26124, acc 0.703125, prec 0.0168748, recall 0.617284
2017-12-09T22:16:43.030649: step 49, loss 9.4385, acc 0.78125, prec 0.0170626, recall 0.6
2017-12-09T22:16:43.326336: step 50, loss 1.73785, acc 0.640625, prec 0.0174514, recall 0.609195
2017-12-09T22:16:43.622809: step 51, loss 6.82941, acc 0.6875, prec 0.0178687, recall 0.611111
2017-12-09T22:16:43.924428: step 52, loss 5.53295, acc 0.59375, prec 0.018205, recall 0.612903
2017-12-09T22:16:44.223393: step 53, loss 1.57419, acc 0.609375, prec 0.0179189, recall 0.612903
2017-12-09T22:16:44.519779: step 54, loss 2.38811, acc 0.492188, prec 0.0178626, recall 0.617021
2017-12-09T22:16:44.819315: step 55, loss 2.82953, acc 0.398438, prec 0.0177444, recall 0.621053
2017-12-09T22:16:45.123272: step 56, loss 3.15901, acc 0.445312, prec 0.0191064, recall 0.643564
2017-12-09T22:16:45.425004: step 57, loss 2.31953, acc 0.421875, prec 0.0186997, recall 0.643564
2017-12-09T22:16:45.725661: step 58, loss 6.15749, acc 0.46875, prec 0.0188999, recall 0.644231
2017-12-09T22:16:46.025491: step 59, loss 3.11441, acc 0.445312, prec 0.0190713, recall 0.650943
2017-12-09T22:16:46.330647: step 60, loss 6.1061, acc 0.453125, prec 0.0195122, recall 0.654545
2017-12-09T22:16:46.630899: step 61, loss 3.15409, acc 0.476562, prec 0.0199468, recall 0.663717
2017-12-09T22:16:46.928151: step 62, loss 3.85722, acc 0.5, prec 0.0201307, recall 0.663793
2017-12-09T22:16:47.227054: step 63, loss 3.09889, acc 0.476562, prec 0.0202876, recall 0.669492
2017-12-09T22:16:47.529605: step 64, loss 6.43032, acc 0.460938, prec 0.0209279, recall 0.674797
2017-12-09T22:16:47.831160: step 65, loss 2.21107, acc 0.5, prec 0.0215667, recall 0.685039
2017-12-09T22:16:48.137464: step 66, loss 2.38481, acc 0.53125, prec 0.0214896, recall 0.6875
2017-12-09T22:16:48.435138: step 67, loss 5.44085, acc 0.507812, prec 0.0211691, recall 0.682171
2017-12-09T22:16:48.731946: step 68, loss 2.21245, acc 0.546875, prec 0.0213422, recall 0.687023
2017-12-09T22:16:49.029591: step 69, loss 2.74946, acc 0.65625, prec 0.0215861, recall 0.686567
2017-12-09T22:16:49.330939: step 70, loss 2.98462, acc 0.585938, prec 0.0217794, recall 0.686131
2017-12-09T22:16:49.628616: step 71, loss 2.16993, acc 0.554688, prec 0.0217193, recall 0.688406
2017-12-09T22:16:49.929682: step 72, loss 4.22326, acc 0.640625, prec 0.0214981, recall 0.683453
2017-12-09T22:16:50.244567: step 73, loss 4.02922, acc 0.601562, prec 0.0217002, recall 0.678322
2017-12-09T22:16:50.553880: step 74, loss 1.46999, acc 0.648438, prec 0.0219172, recall 0.682759
2017-12-09T22:16:50.856104: step 75, loss 2.65064, acc 0.59375, prec 0.0216725, recall 0.678082
2017-12-09T22:16:51.157355: step 76, loss 5.25151, acc 0.546875, prec 0.0218331, recall 0.673333
2017-12-09T22:16:51.452620: step 77, loss 2.17641, acc 0.59375, prec 0.0215904, recall 0.673333
2017-12-09T22:16:51.751060: step 78, loss 2.76509, acc 0.53125, prec 0.0215281, recall 0.671053
2017-12-09T22:16:52.052001: step 79, loss 2.22045, acc 0.507812, prec 0.0214494, recall 0.673203
2017-12-09T22:16:52.353125: step 80, loss 2.2293, acc 0.601562, prec 0.0216272, recall 0.677419
2017-12-09T22:16:52.653618: step 81, loss 2.71581, acc 0.539062, prec 0.0217701, recall 0.677215
2017-12-09T22:16:52.956528: step 82, loss 2.00075, acc 0.570312, prec 0.0215292, recall 0.677215
2017-12-09T22:16:53.265133: step 83, loss 6.96515, acc 0.476562, prec 0.0214413, recall 0.675
2017-12-09T22:16:53.564917: step 84, loss 2.42497, acc 0.53125, prec 0.0215729, recall 0.679012
2017-12-09T22:16:53.867953: step 85, loss 13.2073, acc 0.632812, prec 0.0215743, recall 0.672727
2017-12-09T22:16:54.170327: step 86, loss 3.58549, acc 0.59375, prec 0.0213626, recall 0.668675
2017-12-09T22:16:54.466273: step 87, loss 2.1256, acc 0.585938, prec 0.0213333, recall 0.670659
2017-12-09T22:16:54.765682: step 88, loss 2.10835, acc 0.523438, prec 0.0210883, recall 0.670659
2017-12-09T22:16:55.070976: step 89, loss 4.96123, acc 0.515625, prec 0.0210311, recall 0.668639
2017-12-09T22:16:55.370622: step 90, loss 7.02131, acc 0.53125, prec 0.0213431, recall 0.67052
2017-12-09T22:16:55.668982: step 91, loss 12.8284, acc 0.445312, prec 0.0210794, recall 0.659091
2017-12-09T22:16:55.974430: step 92, loss 1.83956, acc 0.617188, prec 0.0208934, recall 0.659091
2017-12-09T22:16:56.279534: step 93, loss 2.96614, acc 0.414062, prec 0.0206149, recall 0.659091
2017-12-09T22:16:56.577752: step 94, loss 2.91196, acc 0.523438, prec 0.0214261, recall 0.67033
2017-12-09T22:16:56.876337: step 95, loss 2.42127, acc 0.523438, prec 0.0217089, recall 0.675676
2017-12-09T22:16:57.174202: step 96, loss 3.12817, acc 0.453125, prec 0.0214482, recall 0.675676
2017-12-09T22:16:57.472190: step 97, loss 3.03501, acc 0.4375, prec 0.0220152, recall 0.684211
2017-12-09T22:16:57.772995: step 98, loss 2.49023, acc 0.46875, prec 0.0219284, recall 0.685864
2017-12-09T22:16:58.067890: step 99, loss 3.6087, acc 0.40625, prec 0.0216565, recall 0.682292
2017-12-09T22:16:58.362749: step 100, loss 2.41259, acc 0.429688, prec 0.0213982, recall 0.682292
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-100

2017-12-09T22:16:59.683323: step 101, loss 1.90937, acc 0.632812, prec 0.0215524, recall 0.685567
2017-12-09T22:16:59.992720: step 102, loss 4.24571, acc 0.609375, prec 0.0215399, recall 0.683673
2017-12-09T22:17:00.299366: step 103, loss 1.2921, acc 0.664062, prec 0.0213921, recall 0.683673
2017-12-09T22:17:00.600831: step 104, loss 5.36885, acc 0.648438, prec 0.0218631, recall 0.686567
2017-12-09T22:17:00.901158: step 105, loss 3.55198, acc 0.6875, prec 0.0220368, recall 0.686275
2017-12-09T22:17:01.200585: step 106, loss 6.78307, acc 0.71875, prec 0.0220692, recall 0.684466
2017-12-09T22:17:01.501590: step 107, loss 2.02383, acc 0.695312, prec 0.0219387, recall 0.681159
2017-12-09T22:17:01.804558: step 108, loss 4.07132, acc 0.671875, prec 0.0224042, recall 0.683962
2017-12-09T22:17:02.103190: step 109, loss 1.86953, acc 0.632812, prec 0.0222427, recall 0.683962
2017-12-09T22:17:02.396825: step 110, loss 5.22823, acc 0.640625, prec 0.0223915, recall 0.680556
2017-12-09T22:17:02.705498: step 111, loss 2.15312, acc 0.617188, prec 0.0228166, recall 0.686364
2017-12-09T22:17:03.006745: step 112, loss 1.53702, acc 0.585938, prec 0.0230746, recall 0.690583
2017-12-09T22:17:03.303916: step 113, loss 2.32291, acc 0.484375, prec 0.0228487, recall 0.690583
2017-12-09T22:17:03.601308: step 114, loss 2.49788, acc 0.507812, prec 0.0227807, recall 0.691964
2017-12-09T22:17:03.899892: step 115, loss 2.42864, acc 0.617188, prec 0.0230455, recall 0.696035
2017-12-09T22:17:04.197345: step 116, loss 2.43824, acc 0.453125, prec 0.0230947, recall 0.69869
2017-12-09T22:17:04.492636: step 117, loss 2.14172, acc 0.546875, prec 0.0234621, recall 0.703863
2017-12-09T22:17:04.793623: step 118, loss 2.2546, acc 0.5625, prec 0.0234142, recall 0.705128
2017-12-09T22:17:05.095644: step 119, loss 1.50423, acc 0.664062, prec 0.0238229, recall 0.710084
2017-12-09T22:17:05.403934: step 120, loss 1.96157, acc 0.671875, prec 0.0238229, recall 0.708333
2017-12-09T22:17:05.710636: step 121, loss 4.9284, acc 0.65625, prec 0.0238162, recall 0.706612
2017-12-09T22:17:06.013118: step 122, loss 5.76646, acc 0.742188, prec 0.0237105, recall 0.703704
2017-12-09T22:17:06.314337: step 123, loss 0.743348, acc 0.78125, prec 0.0236188, recall 0.703704
2017-12-09T22:17:06.611592: step 124, loss 5.62278, acc 0.679688, prec 0.0237572, recall 0.703252
2017-12-09T22:17:06.910624: step 125, loss 1.81701, acc 0.648438, prec 0.0238777, recall 0.705645
2017-12-09T22:17:07.207716: step 126, loss 1.06873, acc 0.757812, prec 0.0241749, recall 0.709163
2017-12-09T22:17:07.507058: step 127, loss 6.28267, acc 0.765625, prec 0.024476, recall 0.709804
2017-12-09T22:17:07.810162: step 128, loss 3.48149, acc 0.742188, prec 0.0246332, recall 0.709302
2017-12-09T22:17:08.109579: step 129, loss 1.79839, acc 0.632812, prec 0.0246088, recall 0.710425
2017-12-09T22:17:08.406288: step 130, loss 1.68022, acc 0.609375, prec 0.024834, recall 0.71374
2017-12-09T22:17:08.708535: step 131, loss 2.38974, acc 0.578125, prec 0.0249143, recall 0.715909
2017-12-09T22:17:09.005117: step 132, loss 1.60852, acc 0.570312, prec 0.024735, recall 0.715909
2017-12-09T22:17:09.309942: step 133, loss 1.69884, acc 0.625, prec 0.0245806, recall 0.715909
2017-12-09T22:17:09.606104: step 134, loss 3.73785, acc 0.65625, prec 0.0244439, recall 0.713208
2017-12-09T22:17:09.902357: step 135, loss 2.59681, acc 0.648438, prec 0.024431, recall 0.71161
2017-12-09T22:17:10.203446: step 136, loss 2.08535, acc 0.742188, prec 0.0245807, recall 0.711111
2017-12-09T22:17:10.500253: step 137, loss 1.62919, acc 0.671875, prec 0.0245735, recall 0.712177
2017-12-09T22:17:10.797242: step 138, loss 3.49696, acc 0.695312, prec 0.0244551, recall 0.709559
2017-12-09T22:17:11.095643: step 139, loss 4.14428, acc 0.671875, prec 0.0243288, recall 0.70696
2017-12-09T22:17:11.403678: step 140, loss 4.16518, acc 0.742188, prec 0.024476, recall 0.706522
2017-12-09T22:17:11.706795: step 141, loss 1.53044, acc 0.695312, prec 0.0244786, recall 0.707581
2017-12-09T22:17:12.008115: step 142, loss 1.62194, acc 0.679688, prec 0.0245963, recall 0.709677
2017-12-09T22:17:12.302440: step 143, loss 5.1441, acc 0.570312, prec 0.0246761, recall 0.706714
2017-12-09T22:17:12.608569: step 144, loss 2.1325, acc 0.554688, prec 0.0246233, recall 0.707747
2017-12-09T22:17:12.909747: step 145, loss 1.4217, acc 0.625, prec 0.0247169, recall 0.70979
2017-12-09T22:17:13.204255: step 146, loss 2.72324, acc 0.539062, prec 0.0250121, recall 0.713793
2017-12-09T22:17:13.501254: step 147, loss 1.84492, acc 0.648438, prec 0.0248768, recall 0.713793
2017-12-09T22:17:13.800949: step 148, loss 2.97239, acc 0.5, prec 0.0246899, recall 0.71134
2017-12-09T22:17:14.101560: step 149, loss 2.44397, acc 0.539062, prec 0.0247484, recall 0.713311
2017-12-09T22:17:14.400921: step 150, loss 1.63047, acc 0.5625, prec 0.0245853, recall 0.713311
2017-12-09T22:17:14.696859: step 151, loss 1.84213, acc 0.617188, prec 0.0245585, recall 0.714286
2017-12-09T22:17:15.000939: step 152, loss 4.11977, acc 0.640625, prec 0.0246569, recall 0.713805
2017-12-09T22:17:15.302686: step 153, loss 1.77408, acc 0.671875, prec 0.0247628, recall 0.715719
2017-12-09T22:17:15.601469: step 154, loss 1.54704, acc 0.703125, prec 0.0247667, recall 0.716667
2017-12-09T22:17:15.898360: step 155, loss 1.72772, acc 0.609375, prec 0.0246249, recall 0.716667
2017-12-09T22:17:16.194829: step 156, loss 3.0437, acc 0.773438, prec 0.0247689, recall 0.716172
2017-12-09T22:17:16.497291: step 157, loss 1.01233, acc 0.742188, prec 0.0248977, recall 0.718033
2017-12-09T22:17:16.798782: step 158, loss 1.30329, acc 0.703125, prec 0.024901, recall 0.718954
2017-12-09T22:17:17.101959: step 159, loss 0.799199, acc 0.765625, prec 0.0250367, recall 0.720779
2017-12-09T22:17:17.402191: step 160, loss 3.24044, acc 0.710938, prec 0.0249354, recall 0.718447
2017-12-09T22:17:17.708148: step 161, loss 0.734356, acc 0.835938, prec 0.0248767, recall 0.718447
2017-12-09T22:17:18.010797: step 162, loss 7.07686, acc 0.742188, prec 0.0248967, recall 0.717042
2017-12-09T22:17:18.313967: step 163, loss 1.36633, acc 0.773438, prec 0.0251418, recall 0.719745
2017-12-09T22:17:18.614769: step 164, loss 3.00714, acc 0.71875, prec 0.0250443, recall 0.71746
2017-12-09T22:17:18.916518: step 165, loss 0.978844, acc 0.773438, prec 0.0252871, recall 0.720126
2017-12-09T22:17:19.217256: step 166, loss 3.62024, acc 0.765625, prec 0.0253165, recall 0.716511
2017-12-09T22:17:19.522911: step 167, loss 4.87151, acc 0.726562, prec 0.0253289, recall 0.71517
2017-12-09T22:17:19.826845: step 168, loss 1.26736, acc 0.679688, prec 0.025322, recall 0.716049
2017-12-09T22:17:20.123437: step 169, loss 1.63603, acc 0.65625, prec 0.025201, recall 0.716049
2017-12-09T22:17:20.432784: step 170, loss 1.59588, acc 0.578125, prec 0.025054, recall 0.716049
2017-12-09T22:17:20.732704: step 171, loss 1.74424, acc 0.617188, prec 0.0252363, recall 0.718654
2017-12-09T22:17:21.030396: step 172, loss 2.13694, acc 0.507812, prec 0.0252746, recall 0.720365
2017-12-09T22:17:21.331928: step 173, loss 8.99608, acc 0.648438, prec 0.0252654, recall 0.714715
2017-12-09T22:17:21.628743: step 174, loss 2.23022, acc 0.53125, prec 0.0251055, recall 0.714715
2017-12-09T22:17:21.928060: step 175, loss 2.58368, acc 0.515625, prec 0.0250445, recall 0.715569
2017-12-09T22:17:22.225460: step 176, loss 6.65307, acc 0.46875, prec 0.0249714, recall 0.714286
2017-12-09T22:17:22.526228: step 177, loss 2.76991, acc 0.53125, prec 0.0248165, recall 0.714286
2017-12-09T22:17:22.823466: step 178, loss 2.01515, acc 0.53125, prec 0.0247637, recall 0.715134
2017-12-09T22:17:23.117852: step 179, loss 2.69463, acc 0.476562, prec 0.0248929, recall 0.717647
2017-12-09T22:17:23.414941: step 180, loss 2.51349, acc 0.539062, prec 0.0248454, recall 0.716374
2017-12-09T22:17:23.713381: step 181, loss 2.00004, acc 0.539062, prec 0.0248942, recall 0.718023
2017-12-09T22:17:24.016314: step 182, loss 1.57269, acc 0.640625, prec 0.0250727, recall 0.720461
2017-12-09T22:17:24.314589: step 183, loss 2.1747, acc 0.570312, prec 0.0250324, recall 0.721264
2017-12-09T22:17:24.617425: step 184, loss 1.82761, acc 0.59375, prec 0.0249033, recall 0.721264
2017-12-09T22:17:24.918630: step 185, loss 2.5263, acc 0.6875, prec 0.0249036, recall 0.72
2017-12-09T22:17:25.219095: step 186, loss 2.13226, acc 0.734375, prec 0.0248227, recall 0.717949
2017-12-09T22:17:25.518103: step 187, loss 1.55208, acc 0.726562, prec 0.0247398, recall 0.715909
2017-12-09T22:17:25.819402: step 188, loss 1.21322, acc 0.75, prec 0.0247578, recall 0.716714
2017-12-09T22:17:26.122213: step 189, loss 1.509, acc 0.703125, prec 0.0250463, recall 0.719888
2017-12-09T22:17:26.423162: step 190, loss 1.24514, acc 0.710938, prec 0.025051, recall 0.72067
2017-12-09T22:17:26.726494: step 191, loss 1.00314, acc 0.71875, prec 0.025058, recall 0.721448
2017-12-09T22:17:27.023050: step 192, loss 0.712171, acc 0.804688, prec 0.0251858, recall 0.722992
2017-12-09T22:17:27.323901: step 193, loss 6.38552, acc 0.773438, prec 0.0252117, recall 0.721763
2017-12-09T22:17:27.630105: step 194, loss 0.942593, acc 0.796875, prec 0.0252423, recall 0.722527
2017-12-09T22:17:27.928523: step 195, loss 1.27009, acc 0.78125, prec 0.0253637, recall 0.722071
2017-12-09T22:17:28.231330: step 196, loss 3.40202, acc 0.8125, prec 0.0254941, recall 0.721622
2017-12-09T22:17:28.527372: step 197, loss 2.74206, acc 0.757812, prec 0.0255141, recall 0.72043
2017-12-09T22:17:28.831935: step 198, loss 1.21117, acc 0.726562, prec 0.0255218, recall 0.72118
2017-12-09T22:17:29.130325: step 199, loss 1.01337, acc 0.734375, prec 0.0255319, recall 0.721925
2017-12-09T22:17:29.428573: step 200, loss 1.04574, acc 0.75, prec 0.0255468, recall 0.722667
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-200

2017-12-09T22:17:30.909804: step 201, loss 0.988747, acc 0.710938, prec 0.0255495, recall 0.723404
2017-12-09T22:17:31.209881: step 202, loss 1.85858, acc 0.726562, prec 0.0256506, recall 0.722955
2017-12-09T22:17:31.514504: step 203, loss 1.23838, acc 0.6875, prec 0.0258275, recall 0.725131
2017-12-09T22:17:31.815762: step 204, loss 1.02925, acc 0.734375, prec 0.0257459, recall 0.725131
2017-12-09T22:17:32.115500: step 205, loss 0.898008, acc 0.734375, prec 0.0256648, recall 0.725131
2017-12-09T22:17:32.417350: step 206, loss 1.11227, acc 0.71875, prec 0.0256694, recall 0.725849
2017-12-09T22:17:32.717383: step 207, loss 7.17868, acc 0.75, prec 0.0257756, recall 0.725389
2017-12-09T22:17:33.021036: step 208, loss 1.09493, acc 0.726562, prec 0.0256928, recall 0.725389
2017-12-09T22:17:33.318529: step 209, loss 0.759449, acc 0.773438, prec 0.0257138, recall 0.726098
2017-12-09T22:17:33.617645: step 210, loss 0.696393, acc 0.8125, prec 0.0258353, recall 0.727506
2017-12-09T22:17:33.916342: step 211, loss 6.30816, acc 0.796875, prec 0.0258676, recall 0.72449
2017-12-09T22:17:34.219982: step 212, loss 6.44671, acc 0.75, prec 0.0258903, recall 0.717884
2017-12-09T22:17:34.523289: step 213, loss 0.898432, acc 0.757812, prec 0.0258176, recall 0.717884
2017-12-09T22:17:34.825698: step 214, loss 1.58071, acc 0.625, prec 0.0257937, recall 0.718593
2017-12-09T22:17:35.132628: step 215, loss 1.78797, acc 0.648438, prec 0.0258644, recall 0.72
2017-12-09T22:17:35.442103: step 216, loss 1.45659, acc 0.617188, prec 0.0259253, recall 0.721393
2017-12-09T22:17:35.736101: step 217, loss 3.74708, acc 0.578125, prec 0.0258897, recall 0.720297
2017-12-09T22:17:36.036721: step 218, loss 2.17806, acc 0.5625, prec 0.0258476, recall 0.720988
2017-12-09T22:17:36.333230: step 219, loss 2.27951, acc 0.53125, prec 0.0257968, recall 0.721675
2017-12-09T22:17:36.632204: step 220, loss 4.59874, acc 0.515625, prec 0.0258296, recall 0.721271
2017-12-09T22:17:36.930452: step 221, loss 2.13368, acc 0.585938, prec 0.0261347, recall 0.724638
2017-12-09T22:17:37.226906: step 222, loss 2.78083, acc 0.476562, prec 0.0261517, recall 0.725962
2017-12-09T22:17:37.523321: step 223, loss 2.42424, acc 0.453125, prec 0.0261618, recall 0.727273
2017-12-09T22:17:37.825435: step 224, loss 2.8967, acc 0.5, prec 0.0261018, recall 0.727924
2017-12-09T22:17:38.120658: step 225, loss 2.08155, acc 0.53125, prec 0.0259685, recall 0.727924
2017-12-09T22:17:38.423814: step 226, loss 1.95628, acc 0.632812, prec 0.0261128, recall 0.729858
2017-12-09T22:17:38.727748: step 227, loss 4.18215, acc 0.554688, prec 0.0260715, recall 0.728774
2017-12-09T22:17:39.026480: step 228, loss 1.45789, acc 0.632812, prec 0.0259686, recall 0.728774
2017-12-09T22:17:39.324281: step 229, loss 1.68035, acc 0.5625, prec 0.0260099, recall 0.730047
2017-12-09T22:17:39.620297: step 230, loss 1.2012, acc 0.679688, prec 0.025921, recall 0.730047
2017-12-09T22:17:39.918445: step 231, loss 0.878673, acc 0.757812, prec 0.0259352, recall 0.730679
2017-12-09T22:17:40.220391: step 232, loss 6.49293, acc 0.773438, prec 0.0258749, recall 0.728972
2017-12-09T22:17:40.521834: step 233, loss 4.57531, acc 0.8125, prec 0.0259085, recall 0.726218
2017-12-09T22:17:40.828545: step 234, loss 1.28375, acc 0.710938, prec 0.0259097, recall 0.726852
2017-12-09T22:17:41.127727: step 235, loss 1.10523, acc 0.75, prec 0.0261621, recall 0.729358
2017-12-09T22:17:41.430659: step 236, loss 0.939835, acc 0.773438, prec 0.0262596, recall 0.730594
2017-12-09T22:17:41.728609: step 237, loss 1.47351, acc 0.710938, prec 0.0263395, recall 0.731818
2017-12-09T22:17:42.025044: step 238, loss 1.15923, acc 0.75, prec 0.0264296, recall 0.733032
2017-12-09T22:17:42.326700: step 239, loss 0.987082, acc 0.734375, prec 0.0263565, recall 0.733032
2017-12-09T22:17:42.626292: step 240, loss 1.27454, acc 0.703125, prec 0.0264331, recall 0.734234
2017-12-09T22:17:42.924073: step 241, loss 0.647277, acc 0.78125, prec 0.026452, recall 0.734831
2017-12-09T22:17:43.222622: step 242, loss 0.612597, acc 0.835938, prec 0.0265644, recall 0.736018
2017-12-09T22:17:43.523787: step 243, loss 3.85611, acc 0.75, prec 0.0265765, recall 0.734967
2017-12-09T22:17:43.822844: step 244, loss 0.831101, acc 0.773438, prec 0.0265145, recall 0.734967
2017-12-09T22:17:44.121838: step 245, loss 1.69676, acc 0.75, prec 0.0265267, recall 0.733925
2017-12-09T22:17:44.425955: step 246, loss 0.507347, acc 0.8125, prec 0.0264758, recall 0.733925
2017-12-09T22:17:44.730293: step 247, loss 0.651011, acc 0.804688, prec 0.0264229, recall 0.733925
2017-12-09T22:17:45.027016: step 248, loss 6.85621, acc 0.765625, prec 0.0264394, recall 0.732892
2017-12-09T22:17:45.208406: step 249, loss 1.807, acc 0.823529, prec 0.026498, recall 0.73348
2017-12-09T22:17:45.519080: step 250, loss 12.3621, acc 0.859375, prec 0.0266963, recall 0.732026
2017-12-09T22:17:45.819335: step 251, loss 4.05648, acc 0.78125, prec 0.0267935, recall 0.731602
2017-12-09T22:17:46.124109: step 252, loss 2.27141, acc 0.6875, prec 0.0268647, recall 0.731183
2017-12-09T22:17:46.423632: step 253, loss 1.55755, acc 0.640625, prec 0.0271504, recall 0.734043
2017-12-09T22:17:46.723138: step 254, loss 1.87094, acc 0.617188, prec 0.0271224, recall 0.734607
2017-12-09T22:17:47.020434: step 255, loss 2.24339, acc 0.570312, prec 0.0272337, recall 0.736287
2017-12-09T22:17:47.321170: step 256, loss 2.59886, acc 0.46875, prec 0.0271655, recall 0.736842
2017-12-09T22:17:47.617932: step 257, loss 2.30119, acc 0.484375, prec 0.0271773, recall 0.737945
2017-12-09T22:17:47.917951: step 258, loss 2.43607, acc 0.46875, prec 0.0270353, recall 0.737945
2017-12-09T22:17:48.212378: step 259, loss 2.2153, acc 0.515625, prec 0.0269816, recall 0.738494
2017-12-09T22:17:48.508063: step 260, loss 2.38121, acc 0.453125, prec 0.026912, recall 0.73904
2017-12-09T22:17:48.803637: step 261, loss 2.22898, acc 0.460938, prec 0.0269187, recall 0.740125
2017-12-09T22:17:49.108911: step 262, loss 1.34359, acc 0.632812, prec 0.0268967, recall 0.740664
2017-12-09T22:17:49.403630: step 263, loss 1.6595, acc 0.640625, prec 0.0268769, recall 0.741201
2017-12-09T22:17:49.707331: step 264, loss 5.24753, acc 0.773438, prec 0.0269663, recall 0.740741
2017-12-09T22:17:50.008643: step 265, loss 1.82998, acc 0.804688, prec 0.0269907, recall 0.739754
2017-12-09T22:17:50.319252: step 266, loss 0.679184, acc 0.835938, prec 0.0270936, recall 0.740816
2017-12-09T22:17:50.617501: step 267, loss 0.78307, acc 0.796875, prec 0.0272585, recall 0.742393
2017-12-09T22:17:50.921173: step 268, loss 1.16687, acc 0.835938, prec 0.0272903, recall 0.741414
2017-12-09T22:17:51.227266: step 269, loss 1.30001, acc 0.820312, prec 0.0273901, recall 0.740964
2017-12-09T22:17:51.528718: step 270, loss 0.755197, acc 0.804688, prec 0.0274835, recall 0.742
2017-12-09T22:17:51.833172: step 271, loss 0.819692, acc 0.789062, prec 0.0275006, recall 0.742515
2017-12-09T22:17:52.132243: step 272, loss 0.685796, acc 0.8125, prec 0.0274518, recall 0.742515
2017-12-09T22:17:52.432682: step 273, loss 1.84901, acc 0.875, prec 0.0274932, recall 0.741551
2017-12-09T22:17:52.733956: step 274, loss 3.13647, acc 0.804688, prec 0.0274446, recall 0.740079
2017-12-09T22:17:53.034875: step 275, loss 0.61615, acc 0.8125, prec 0.0273963, recall 0.740079
2017-12-09T22:17:53.336269: step 276, loss 0.521613, acc 0.828125, prec 0.0273521, recall 0.740079
2017-12-09T22:17:53.634696: step 277, loss 0.524455, acc 0.851562, prec 0.0273852, recall 0.740594
2017-12-09T22:17:53.937695: step 278, loss 1.5808, acc 0.835938, prec 0.0275565, recall 0.742126
2017-12-09T22:17:54.240580: step 279, loss 1.44018, acc 0.875, prec 0.0275973, recall 0.741176
2017-12-09T22:17:54.541339: step 280, loss 0.958581, acc 0.765625, prec 0.027537, recall 0.741176
2017-12-09T22:17:54.841511: step 281, loss 0.618154, acc 0.828125, prec 0.0275636, recall 0.741683
2017-12-09T22:17:55.151833: step 282, loss 2.6895, acc 0.828125, prec 0.0276628, recall 0.741245
2017-12-09T22:17:55.453595: step 283, loss 3.99155, acc 0.8125, prec 0.0277576, recall 0.740812
2017-12-09T22:17:55.756555: step 284, loss 1.06077, acc 0.789062, prec 0.0279847, recall 0.742802
2017-12-09T22:17:56.055372: step 285, loss 1.10159, acc 0.679688, prec 0.0279019, recall 0.742802
2017-12-09T22:17:56.351460: step 286, loss 1.40305, acc 0.695312, prec 0.0278237, recall 0.742802
2017-12-09T22:17:56.647996: step 287, loss 1.38991, acc 0.609375, prec 0.027724, recall 0.742802
2017-12-09T22:17:56.944971: step 288, loss 1.31014, acc 0.671875, prec 0.0276409, recall 0.742802
2017-12-09T22:17:57.242370: step 289, loss 1.69156, acc 0.648438, prec 0.0276908, recall 0.743786
2017-12-09T22:17:57.538749: step 290, loss 1.24745, acc 0.710938, prec 0.0276871, recall 0.744275
2017-12-09T22:17:57.835920: step 291, loss 1.46834, acc 0.664062, prec 0.027878, recall 0.746212
2017-12-09T22:17:58.132540: step 292, loss 0.962486, acc 0.742188, prec 0.0279503, recall 0.74717
2017-12-09T22:17:58.435440: step 293, loss 1.24696, acc 0.6875, prec 0.0280084, recall 0.74812
2017-12-09T22:17:58.733385: step 294, loss 0.745327, acc 0.796875, prec 0.0279573, recall 0.74812
2017-12-09T22:17:59.029380: step 295, loss 0.8171, acc 0.75, prec 0.0278946, recall 0.74812
2017-12-09T22:17:59.330492: step 296, loss 1.02184, acc 0.820312, prec 0.0279857, recall 0.749064
2017-12-09T22:17:59.627079: step 297, loss 2.68029, acc 0.875, prec 0.0280922, recall 0.748603
2017-12-09T22:17:59.932523: step 298, loss 1.13234, acc 0.820312, prec 0.028115, recall 0.749071
2017-12-09T22:18:00.239904: step 299, loss 0.699353, acc 0.867188, prec 0.0282848, recall 0.750462
2017-12-09T22:18:00.543354: step 300, loss 0.516828, acc 0.84375, prec 0.0283806, recall 0.751381

Evaluation:
2017-12-09T22:18:05.239339: step 300, loss 1.88489, acc 0.860742, prec 0.0322094, recall 0.724187

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-300

2017-12-09T22:18:06.431973: step 301, loss 4.57447, acc 0.765625, prec 0.0322115, recall 0.723554
2017-12-09T22:18:06.735015: step 302, loss 4.99658, acc 0.757812, prec 0.0322742, recall 0.7223
2017-12-09T22:18:07.031834: step 303, loss 6.02773, acc 0.789062, prec 0.0324054, recall 0.721448
2017-12-09T22:18:07.337675: step 304, loss 3.29758, acc 0.710938, prec 0.0324533, recall 0.721221
2017-12-09T22:18:07.640530: step 305, loss 1.31737, acc 0.648438, prec 0.0325431, recall 0.722376
2017-12-09T22:18:07.940881: step 306, loss 1.62433, acc 0.625, prec 0.0325062, recall 0.722759
2017-12-09T22:18:08.236225: step 307, loss 1.48913, acc 0.648438, prec 0.0325354, recall 0.723521
2017-12-09T22:18:08.529339: step 308, loss 1.99647, acc 0.53125, prec 0.032594, recall 0.724658
2017-12-09T22:18:08.823938: step 309, loss 2.49223, acc 0.476562, prec 0.0325193, recall 0.725034
2017-12-09T22:18:09.122102: step 310, loss 2.14677, acc 0.523438, prec 0.0324572, recall 0.72541
2017-12-09T22:18:09.417563: step 311, loss 1.91454, acc 0.546875, prec 0.0324015, recall 0.725784
2017-12-09T22:18:09.719727: step 312, loss 2.16518, acc 0.484375, prec 0.0323305, recall 0.726158
2017-12-09T22:18:10.021587: step 313, loss 3.02444, acc 0.515625, prec 0.0322698, recall 0.725543
2017-12-09T22:18:10.322696: step 314, loss 3.32814, acc 0.546875, prec 0.0322173, recall 0.724932
2017-12-09T22:18:10.620554: step 315, loss 1.63656, acc 0.625, prec 0.0322406, recall 0.725676
2017-12-09T22:18:10.920238: step 316, loss 1.12758, acc 0.648438, prec 0.0322697, recall 0.726415
2017-12-09T22:18:11.219659: step 317, loss 1.21764, acc 0.710938, prec 0.0323139, recall 0.727151
2017-12-09T22:18:11.521011: step 318, loss 1.62368, acc 0.664062, prec 0.0322888, recall 0.727517
2017-12-09T22:18:11.822045: step 319, loss 0.925002, acc 0.75, prec 0.0324575, recall 0.728972
2017-12-09T22:18:12.121533: step 320, loss 0.913201, acc 0.726562, prec 0.0324475, recall 0.729333
2017-12-09T22:18:12.418492: step 321, loss 2.16566, acc 0.8125, prec 0.0325179, recall 0.729084
2017-12-09T22:18:12.721304: step 322, loss 0.602851, acc 0.78125, prec 0.0324641, recall 0.729084
2017-12-09T22:18:13.017374: step 323, loss 3.55679, acc 0.804688, prec 0.0324181, recall 0.728117
2017-12-09T22:18:13.329392: step 324, loss 2.69248, acc 0.789062, prec 0.0325965, recall 0.72859
2017-12-09T22:18:13.630116: step 325, loss 0.582335, acc 0.820312, prec 0.0326093, recall 0.728947
2017-12-09T22:18:13.927564: step 326, loss 0.753583, acc 0.765625, prec 0.0326087, recall 0.729304
2017-12-09T22:18:14.230813: step 327, loss 1.07054, acc 0.773438, prec 0.0326667, recall 0.730013
2017-12-09T22:18:14.527538: step 328, loss 0.748542, acc 0.851562, prec 0.032687, recall 0.730366
2017-12-09T22:18:14.830382: step 329, loss 0.578852, acc 0.851562, prec 0.0327073, recall 0.730719
2017-12-09T22:18:15.132449: step 330, loss 0.645178, acc 0.835938, prec 0.0327237, recall 0.731071
2017-12-09T22:18:15.434756: step 331, loss 0.64594, acc 0.789062, prec 0.032785, recall 0.731771
2017-12-09T22:18:15.738478: step 332, loss 0.665504, acc 0.84375, prec 0.0328031, recall 0.73212
2017-12-09T22:18:16.038597: step 333, loss 3.90377, acc 0.789062, prec 0.0328117, recall 0.73057
2017-12-09T22:18:16.341687: step 334, loss 0.599261, acc 0.773438, prec 0.0328688, recall 0.731266
2017-12-09T22:18:16.644157: step 335, loss 0.553135, acc 0.8125, prec 0.032823, recall 0.731266
2017-12-09T22:18:16.938057: step 336, loss 1.08089, acc 0.765625, prec 0.0329899, recall 0.732648
2017-12-09T22:18:17.232801: step 337, loss 0.790833, acc 0.75, prec 0.0329848, recall 0.732991
2017-12-09T22:18:17.529999: step 338, loss 0.669641, acc 0.8125, prec 0.0329391, recall 0.732991
2017-12-09T22:18:17.826559: step 339, loss 0.722204, acc 0.78125, prec 0.0330531, recall 0.734015
2017-12-09T22:18:18.129912: step 340, loss 0.411188, acc 0.898438, prec 0.0332509, recall 0.735369
2017-12-09T22:18:18.433083: step 341, loss 2.36086, acc 0.796875, prec 0.0332587, recall 0.734772
2017-12-09T22:18:18.734794: step 342, loss 1.15535, acc 0.765625, prec 0.0332569, recall 0.735108
2017-12-09T22:18:19.038166: step 343, loss 0.450568, acc 0.820312, prec 0.0332684, recall 0.735443
2017-12-09T22:18:19.342175: step 344, loss 0.752519, acc 0.78125, prec 0.0333257, recall 0.736111
2017-12-09T22:18:19.638762: step 345, loss 3.53441, acc 0.835938, prec 0.0332877, recall 0.735183
2017-12-09T22:18:19.939903: step 346, loss 0.56382, acc 0.820312, prec 0.0334093, recall 0.736181
2017-12-09T22:18:20.245805: step 347, loss 0.719299, acc 0.804688, prec 0.0333618, recall 0.736181
2017-12-09T22:18:20.544450: step 348, loss 0.675006, acc 0.804688, prec 0.0333693, recall 0.736512
2017-12-09T22:18:20.841792: step 349, loss 0.554596, acc 0.8125, prec 0.0333239, recall 0.736512
2017-12-09T22:18:21.146317: step 350, loss 0.676125, acc 0.789062, prec 0.0333277, recall 0.736842
2017-12-09T22:18:21.445685: step 351, loss 0.623868, acc 0.804688, prec 0.0334446, recall 0.737828
2017-12-09T22:18:21.743701: step 352, loss 1.32435, acc 0.8125, prec 0.0335085, recall 0.738481
2017-12-09T22:18:22.041555: step 353, loss 0.478085, acc 0.828125, prec 0.0334669, recall 0.738481
2017-12-09T22:18:22.340429: step 354, loss 4.10353, acc 0.828125, prec 0.0334311, recall 0.735732
2017-12-09T22:18:22.645823: step 355, loss 0.788409, acc 0.8125, prec 0.0334403, recall 0.736059
2017-12-09T22:18:22.953343: step 356, loss 4.66651, acc 0.828125, prec 0.0335114, recall 0.734895
2017-12-09T22:18:23.256795: step 357, loss 0.746088, acc 0.773438, prec 0.0334568, recall 0.734895
2017-12-09T22:18:23.554595: step 358, loss 1.49395, acc 0.804688, prec 0.033466, recall 0.734317
2017-12-09T22:18:23.856546: step 359, loss 1.78372, acc 0.640625, prec 0.033542, recall 0.735294
2017-12-09T22:18:24.155088: step 360, loss 1.78844, acc 0.625, prec 0.0336139, recall 0.736264
2017-12-09T22:18:24.454577: step 361, loss 1.45067, acc 0.664062, prec 0.033641, recall 0.736906
2017-12-09T22:18:24.752447: step 362, loss 1.4092, acc 0.65625, prec 0.0336125, recall 0.737226
2017-12-09T22:18:25.049146: step 363, loss 1.62075, acc 0.585938, prec 0.0335674, recall 0.737546
2017-12-09T22:18:25.348757: step 364, loss 1.96594, acc 0.585938, prec 0.0336292, recall 0.738499
2017-12-09T22:18:25.644996: step 365, loss 4.75472, acc 0.578125, prec 0.0337437, recall 0.738869
2017-12-09T22:18:25.945484: step 366, loss 1.50408, acc 0.570312, prec 0.0336949, recall 0.739183
2017-12-09T22:18:26.244298: step 367, loss 1.28881, acc 0.679688, prec 0.0337251, recall 0.739808
2017-12-09T22:18:26.543121: step 368, loss 1.43307, acc 0.609375, prec 0.0338438, recall 0.74105
2017-12-09T22:18:26.835959: step 369, loss 1.6543, acc 0.523438, prec 0.0338366, recall 0.741667
2017-12-09T22:18:27.135594: step 370, loss 1.5352, acc 0.640625, prec 0.0338046, recall 0.741974
2017-12-09T22:18:27.436355: step 371, loss 1.33617, acc 0.695312, prec 0.0337334, recall 0.741974
2017-12-09T22:18:27.741614: step 372, loss 1.10783, acc 0.734375, prec 0.0337758, recall 0.742586
2017-12-09T22:18:28.040830: step 373, loss 1.63204, acc 0.679688, prec 0.0339093, recall 0.743802
2017-12-09T22:18:28.342746: step 374, loss 1.18503, acc 0.742188, prec 0.033901, recall 0.744104
2017-12-09T22:18:28.638020: step 375, loss 0.759135, acc 0.820312, prec 0.0339629, recall 0.744706
2017-12-09T22:18:28.932846: step 376, loss 0.523266, acc 0.859375, prec 0.0339301, recall 0.744706
2017-12-09T22:18:29.227778: step 377, loss 1.93955, acc 0.828125, prec 0.033892, recall 0.743831
2017-12-09T22:18:29.527537: step 378, loss 0.499529, acc 0.84375, prec 0.033959, recall 0.744431
2017-12-09T22:18:29.828817: step 379, loss 1.78524, acc 0.828125, prec 0.0339725, recall 0.74386
2017-12-09T22:18:30.141162: step 380, loss 2.10806, acc 0.828125, prec 0.033986, recall 0.743291
2017-12-09T22:18:30.445130: step 381, loss 1.2957, acc 0.835938, prec 0.0340528, recall 0.743023
2017-12-09T22:18:30.746327: step 382, loss 2.23562, acc 0.84375, prec 0.0341212, recall 0.742758
2017-12-09T22:18:31.045904: step 383, loss 0.693558, acc 0.796875, prec 0.0341253, recall 0.743056
2017-12-09T22:18:31.345049: step 384, loss 1.14568, acc 0.8125, prec 0.0341349, recall 0.742494
2017-12-09T22:18:31.641469: step 385, loss 0.773173, acc 0.734375, prec 0.0341246, recall 0.742791
2017-12-09T22:18:31.935695: step 386, loss 0.984457, acc 0.71875, prec 0.0341107, recall 0.743088
2017-12-09T22:18:32.232629: step 387, loss 1.03023, acc 0.695312, prec 0.0341425, recall 0.743678
2017-12-09T22:18:32.534576: step 388, loss 1.99838, acc 0.648438, prec 0.0341142, recall 0.743119
2017-12-09T22:18:32.834197: step 389, loss 1.03299, acc 0.671875, prec 0.0340897, recall 0.743414
2017-12-09T22:18:33.134682: step 390, loss 1.00024, acc 0.757812, prec 0.0340849, recall 0.743707
2017-12-09T22:18:33.432925: step 391, loss 0.843903, acc 0.71875, prec 0.0340207, recall 0.743707
2017-12-09T22:18:33.727039: step 392, loss 1.11628, acc 0.679688, prec 0.0339983, recall 0.744
2017-12-09T22:18:34.022965: step 393, loss 2.73301, acc 0.671875, prec 0.033976, recall 0.743444
2017-12-09T22:18:34.325082: step 394, loss 1.96143, acc 0.796875, prec 0.0340826, recall 0.743473
2017-12-09T22:18:34.623267: step 395, loss 1.80907, acc 0.75, prec 0.0341281, recall 0.743213
2017-12-09T22:18:34.924464: step 396, loss 2.73538, acc 0.703125, prec 0.0341628, recall 0.742954
2017-12-09T22:18:35.234791: step 397, loss 1.04703, acc 0.695312, prec 0.0341438, recall 0.743243
2017-12-09T22:18:35.531939: step 398, loss 0.986425, acc 0.734375, prec 0.0341836, recall 0.74382
2017-12-09T22:18:35.828993: step 399, loss 1.4405, acc 0.648438, prec 0.0342039, recall 0.744395
2017-12-09T22:18:36.129602: step 400, loss 0.779394, acc 0.75, prec 0.0341476, recall 0.744395
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-400

2017-12-09T22:18:37.440476: step 401, loss 1.32151, acc 0.640625, prec 0.0341661, recall 0.744966
2017-12-09T22:18:37.735510: step 402, loss 0.909523, acc 0.742188, prec 0.0341084, recall 0.744966
2017-12-09T22:18:38.035622: step 403, loss 0.64489, acc 0.78125, prec 0.0342077, recall 0.745819
2017-12-09T22:18:38.338034: step 404, loss 0.96083, acc 0.773438, prec 0.0342557, recall 0.746385
2017-12-09T22:18:38.634564: step 405, loss 3.17822, acc 0.789062, prec 0.0342103, recall 0.745556
2017-12-09T22:18:38.930876: step 406, loss 0.921062, acc 0.765625, prec 0.0342072, recall 0.745838
2017-12-09T22:18:39.229142: step 407, loss 1.0199, acc 0.796875, prec 0.0343092, recall 0.746681
2017-12-09T22:18:39.535240: step 408, loss 0.670137, acc 0.789062, prec 0.0343602, recall 0.747241
2017-12-09T22:18:39.835037: step 409, loss 1.4638, acc 0.828125, prec 0.0344216, recall 0.746975
2017-12-09T22:18:40.141802: step 410, loss 0.708564, acc 0.789062, prec 0.0344723, recall 0.74753
2017-12-09T22:18:40.440443: step 411, loss 2.43658, acc 0.78125, prec 0.034474, recall 0.746988
2017-12-09T22:18:40.740304: step 412, loss 1.85276, acc 0.851562, prec 0.0344427, recall 0.746171
2017-12-09T22:18:41.041547: step 413, loss 1.09456, acc 0.8125, prec 0.0345471, recall 0.747001
2017-12-09T22:18:41.339299: step 414, loss 5.50864, acc 0.71875, prec 0.0345366, recall 0.745652
2017-12-09T22:18:41.636929: step 415, loss 1.04197, acc 0.734375, prec 0.0345261, recall 0.745928
2017-12-09T22:18:41.930687: step 416, loss 0.887452, acc 0.75, prec 0.0345675, recall 0.746479
2017-12-09T22:18:42.225238: step 417, loss 2.23464, acc 0.742188, prec 0.0345138, recall 0.744865
2017-12-09T22:18:42.526262: step 418, loss 1.03616, acc 0.679688, prec 0.0344914, recall 0.74514
2017-12-09T22:18:42.828317: step 419, loss 1.05357, acc 0.664062, prec 0.0344656, recall 0.745415
2017-12-09T22:18:43.126323: step 420, loss 1.50171, acc 0.617188, prec 0.0344296, recall 0.74569
2017-12-09T22:18:43.429512: step 421, loss 1.22684, acc 0.695312, prec 0.0344588, recall 0.746237
2017-12-09T22:18:43.725857: step 422, loss 1.08446, acc 0.664062, prec 0.034481, recall 0.746781
2017-12-09T22:18:44.024958: step 423, loss 1.00804, acc 0.726562, prec 0.0344214, recall 0.746781
2017-12-09T22:18:44.323215: step 424, loss 1.01285, acc 0.632812, prec 0.0343415, recall 0.746781
2017-12-09T22:18:44.618900: step 425, loss 1.38222, acc 0.65625, prec 0.0343147, recall 0.747052
2017-12-09T22:18:44.913649: step 426, loss 5.43122, acc 0.734375, prec 0.0344489, recall 0.747335
2017-12-09T22:18:45.214233: step 427, loss 0.702768, acc 0.765625, prec 0.0344455, recall 0.747604
2017-12-09T22:18:45.513661: step 428, loss 0.781208, acc 0.742188, prec 0.0344844, recall 0.74814
2017-12-09T22:18:45.814059: step 429, loss 0.59494, acc 0.78125, prec 0.0344844, recall 0.748408
2017-12-09T22:18:46.115813: step 430, loss 0.52559, acc 0.78125, prec 0.0344844, recall 0.748674
2017-12-09T22:18:46.410995: step 431, loss 0.68877, acc 0.757812, prec 0.0344794, recall 0.748941
2017-12-09T22:18:46.707978: step 432, loss 0.830196, acc 0.796875, prec 0.0344357, recall 0.748941
2017-12-09T22:18:47.005814: step 433, loss 6.54123, acc 0.851562, prec 0.0345465, recall 0.748945
2017-12-09T22:18:47.308263: step 434, loss 0.909987, acc 0.820312, prec 0.0346017, recall 0.749474
2017-12-09T22:18:47.612091: step 435, loss 1.26676, acc 0.796875, prec 0.0346535, recall 0.749213
2017-12-09T22:18:47.911001: step 436, loss 0.644695, acc 0.796875, prec 0.0347502, recall 0.75
2017-12-09T22:18:48.212196: step 437, loss 0.521202, acc 0.835938, prec 0.0347616, recall 0.750261
2017-12-09T22:18:48.509662: step 438, loss 0.673499, acc 0.773438, prec 0.0348062, recall 0.750782
2017-12-09T22:18:48.807137: step 439, loss 0.724008, acc 0.851562, prec 0.0348674, recall 0.751301
2017-12-09T22:18:49.107473: step 440, loss 1.87859, acc 0.78125, prec 0.0349151, recall 0.751037
2017-12-09T22:18:49.408229: step 441, loss 0.596919, acc 0.8125, prec 0.0349677, recall 0.751553
2017-12-09T22:18:49.708982: step 442, loss 0.746912, acc 0.804688, prec 0.0349257, recall 0.751553
2017-12-09T22:18:50.012905: step 443, loss 0.496977, acc 0.835938, prec 0.0349832, recall 0.752066
2017-12-09T22:18:50.325346: step 444, loss 0.685172, acc 0.765625, prec 0.0350254, recall 0.752577
2017-12-09T22:18:50.620679: step 445, loss 3.11115, acc 0.789062, prec 0.0350743, recall 0.752312
2017-12-09T22:18:50.925708: step 446, loss 4.50438, acc 0.734375, prec 0.0351129, recall 0.751279
2017-12-09T22:18:51.222681: step 447, loss 3.71065, acc 0.65625, prec 0.0350425, recall 0.749745
2017-12-09T22:18:51.518267: step 448, loss 1.1005, acc 0.617188, prec 0.0350526, recall 0.750255
2017-12-09T22:18:51.814426: step 449, loss 1.19346, acc 0.664062, prec 0.0350269, recall 0.750509
2017-12-09T22:18:52.113182: step 450, loss 1.64587, acc 0.515625, prec 0.0350154, recall 0.751016
2017-12-09T22:18:52.411715: step 451, loss 5.56683, acc 0.4375, prec 0.0349891, recall 0.75076
2017-12-09T22:18:52.712947: step 452, loss 1.58418, acc 0.53125, prec 0.0350266, recall 0.751515
2017-12-09T22:18:53.012241: step 453, loss 1.86077, acc 0.5, prec 0.0352384, recall 0.75326
2017-12-09T22:18:53.311881: step 454, loss 2.45369, acc 0.414062, prec 0.035205, recall 0.753754
2017-12-09T22:18:53.612900: step 455, loss 1.70712, acc 0.5, prec 0.0351899, recall 0.754246
2017-12-09T22:18:53.907690: step 456, loss 1.56472, acc 0.546875, prec 0.0351399, recall 0.754491
2017-12-09T22:18:54.205494: step 457, loss 1.57565, acc 0.5625, prec 0.0350934, recall 0.754736
2017-12-09T22:18:54.500821: step 458, loss 1.17676, acc 0.625, prec 0.0350155, recall 0.754736
2017-12-09T22:18:54.797529: step 459, loss 0.845786, acc 0.710938, prec 0.0350002, recall 0.75498
2017-12-09T22:18:55.093363: step 460, loss 7.91812, acc 0.6875, prec 0.0349373, recall 0.754229
2017-12-09T22:18:55.394405: step 461, loss 0.847634, acc 0.773438, prec 0.0349795, recall 0.754717
2017-12-09T22:18:55.691873: step 462, loss 3.20405, acc 0.734375, prec 0.0349708, recall 0.754212
2017-12-09T22:18:55.991187: step 463, loss 0.517307, acc 0.835938, prec 0.0349814, recall 0.754455
2017-12-09T22:18:56.286765: step 464, loss 0.540853, acc 0.796875, prec 0.034984, recall 0.754698
2017-12-09T22:18:56.589080: step 465, loss 1.19267, acc 0.851562, prec 0.0349551, recall 0.753953
2017-12-09T22:18:56.892811: step 466, loss 0.377637, acc 0.867188, prec 0.0349279, recall 0.753953
2017-12-09T22:18:57.190351: step 467, loss 0.357119, acc 0.851562, prec 0.0348975, recall 0.753953
2017-12-09T22:18:57.495093: step 468, loss 2.01476, acc 0.867188, prec 0.0349602, recall 0.753695
2017-12-09T22:18:57.795077: step 469, loss 0.603626, acc 0.875, prec 0.035111, recall 0.754661
2017-12-09T22:18:58.093310: step 470, loss 0.715126, acc 0.765625, prec 0.0351069, recall 0.754902
2017-12-09T22:18:58.394028: step 471, loss 2.6325, acc 0.835938, prec 0.0350749, recall 0.754163
2017-12-09T22:18:58.694926: step 472, loss 1.05131, acc 0.851562, prec 0.0350462, recall 0.753425
2017-12-09T22:18:58.995451: step 473, loss 0.569007, acc 0.835938, prec 0.0350127, recall 0.753425
2017-12-09T22:18:59.292487: step 474, loss 0.530981, acc 0.804688, prec 0.0350168, recall 0.753666
2017-12-09T22:18:59.596497: step 475, loss 0.548549, acc 0.828125, prec 0.0349819, recall 0.753666
2017-12-09T22:18:59.898546: step 476, loss 1.05897, acc 0.796875, prec 0.0349844, recall 0.753906
2017-12-09T22:19:00.204914: step 477, loss 0.635131, acc 0.75, prec 0.0349774, recall 0.754146
2017-12-09T22:19:00.503025: step 478, loss 1.54155, acc 0.789062, prec 0.0350671, recall 0.75413
2017-12-09T22:19:00.806199: step 479, loss 1.45142, acc 0.773438, prec 0.0350228, recall 0.753398
2017-12-09T22:19:01.104764: step 480, loss 0.923934, acc 0.734375, prec 0.0350561, recall 0.753876
2017-12-09T22:19:01.403461: step 481, loss 0.50819, acc 0.835938, prec 0.035023, recall 0.753876
2017-12-09T22:19:01.700498: step 482, loss 3.19112, acc 0.804688, prec 0.0350286, recall 0.753385
2017-12-09T22:19:01.996592: step 483, loss 0.743888, acc 0.726562, prec 0.0350602, recall 0.753861
2017-12-09T22:19:02.296085: step 484, loss 1.41769, acc 0.65625, prec 0.0351207, recall 0.754572
2017-12-09T22:19:02.594135: step 485, loss 2.9562, acc 0.671875, prec 0.0350995, recall 0.754083
2017-12-09T22:19:02.888079: step 486, loss 0.771467, acc 0.734375, prec 0.0350893, recall 0.754319
2017-12-09T22:19:03.189727: step 487, loss 1.08948, acc 0.640625, prec 0.0351034, recall 0.754789
2017-12-09T22:19:03.487697: step 488, loss 2.25983, acc 0.6875, prec 0.0351712, recall 0.754771
2017-12-09T22:19:03.790503: step 489, loss 1.41599, acc 0.585938, prec 0.0351741, recall 0.755238
2017-12-09T22:19:04.085037: step 490, loss 1.28566, acc 0.625, prec 0.0350994, recall 0.755238
2017-12-09T22:19:04.383936: step 491, loss 1.31306, acc 0.617188, prec 0.0351512, recall 0.755935
2017-12-09T22:19:04.688687: step 492, loss 4.12738, acc 0.609375, prec 0.0351604, recall 0.755682
2017-12-09T22:19:04.990993: step 493, loss 1.31894, acc 0.601562, prec 0.0352088, recall 0.756374
2017-12-09T22:19:05.297147: step 494, loss 2.42815, acc 0.601562, prec 0.0352162, recall 0.756121
2017-12-09T22:19:05.595000: step 495, loss 1.88066, acc 0.695312, prec 0.0352421, recall 0.755869
2017-12-09T22:19:05.892734: step 496, loss 0.960193, acc 0.671875, prec 0.0351774, recall 0.755869
2017-12-09T22:19:06.190073: step 497, loss 1.25588, acc 0.695312, prec 0.0352859, recall 0.756782
2017-12-09T22:19:06.371161: step 498, loss 4.05087, acc 0.509804, prec 0.0353331, recall 0.75653
2017-12-09T22:19:06.677586: step 499, loss 1.31827, acc 0.585938, prec 0.0353775, recall 0.757209
2017-12-09T22:19:06.981879: step 500, loss 1.27427, acc 0.609375, prec 0.0353426, recall 0.757435
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-500

2017-12-09T22:19:08.309460: step 501, loss 1.09087, acc 0.6875, prec 0.0354067, recall 0.758109
2017-12-09T22:19:08.607512: step 502, loss 1.01803, acc 0.734375, prec 0.0354797, recall 0.75878
2017-12-09T22:19:08.904281: step 503, loss 1.27257, acc 0.625, prec 0.0354478, recall 0.759003
2017-12-09T22:19:09.200989: step 504, loss 1.1198, acc 0.726562, prec 0.0354775, recall 0.759447
2017-12-09T22:19:09.499614: step 505, loss 1.07958, acc 0.734375, prec 0.0355086, recall 0.75989
2017-12-09T22:19:09.796458: step 506, loss 1.10513, acc 0.71875, prec 0.0354537, recall 0.75989
2017-12-09T22:19:10.093655: step 507, loss 0.629447, acc 0.867188, prec 0.0355106, recall 0.760331
2017-12-09T22:19:10.391248: step 508, loss 0.657163, acc 0.742188, prec 0.0355017, recall 0.76055
2017-12-09T22:19:10.694529: step 509, loss 0.604188, acc 0.8125, prec 0.0356303, recall 0.761426
2017-12-09T22:19:10.999045: step 510, loss 0.689155, acc 0.789062, prec 0.0356716, recall 0.761861
2017-12-09T22:19:11.301159: step 511, loss 0.462129, acc 0.851562, prec 0.035725, recall 0.762295
2017-12-09T22:19:11.597247: step 512, loss 0.497006, acc 0.835938, prec 0.035693, recall 0.762295
2017-12-09T22:19:11.898636: step 513, loss 3.46328, acc 0.875, prec 0.0357949, recall 0.761559
2017-12-09T22:19:12.196505: step 514, loss 0.402416, acc 0.882812, prec 0.0358131, recall 0.761775
2017-12-09T22:19:12.491578: step 515, loss 0.367412, acc 0.851562, prec 0.0358252, recall 0.761991
2017-12-09T22:19:12.792176: step 516, loss 0.391499, acc 0.882812, prec 0.0358434, recall 0.762206
2017-12-09T22:19:13.092495: step 517, loss 0.324896, acc 0.890625, prec 0.035822, recall 0.762206
2017-12-09T22:19:13.393733: step 518, loss 2.40913, acc 0.90625, prec 0.0359281, recall 0.762162
2017-12-09T22:19:13.692956: step 519, loss 0.373145, acc 0.867188, prec 0.0359022, recall 0.762162
2017-12-09T22:19:13.990386: step 520, loss 0.455701, acc 0.84375, prec 0.0359535, recall 0.76259
2017-12-09T22:19:14.285260: step 521, loss 0.471419, acc 0.921875, prec 0.0361017, recall 0.763441
2017-12-09T22:19:14.589132: step 522, loss 0.550882, acc 0.84375, prec 0.0361935, recall 0.764075
2017-12-09T22:19:14.889515: step 523, loss 1.34398, acc 0.875, prec 0.0362113, recall 0.763604
2017-12-09T22:19:15.192652: step 524, loss 0.309514, acc 0.90625, prec 0.036193, recall 0.763604
2017-12-09T22:19:15.490076: step 525, loss 0.481368, acc 0.921875, prec 0.0362591, recall 0.764025
2017-12-09T22:19:15.787328: step 526, loss 0.56541, acc 0.820312, prec 0.0363053, recall 0.764444
2017-12-09T22:19:16.086610: step 527, loss 0.399289, acc 0.867188, prec 0.0362793, recall 0.764444
2017-12-09T22:19:16.383247: step 528, loss 0.514508, acc 0.828125, prec 0.0363269, recall 0.764862
2017-12-09T22:19:16.686524: step 529, loss 0.746936, acc 0.84375, prec 0.036418, recall 0.765487
2017-12-09T22:19:16.985105: step 530, loss 3.36888, acc 0.828125, prec 0.0364279, recall 0.764342
2017-12-09T22:19:17.289961: step 531, loss 0.556577, acc 0.84375, prec 0.0364783, recall 0.764758
2017-12-09T22:19:17.590483: step 532, loss 0.524813, acc 0.851562, prec 0.0364896, recall 0.764965
2017-12-09T22:19:17.893713: step 533, loss 0.4612, acc 0.859375, prec 0.0365025, recall 0.765172
2017-12-09T22:19:18.201381: step 534, loss 0.980397, acc 0.820312, prec 0.0365884, recall 0.765789
2017-12-09T22:19:18.499210: step 535, loss 0.645796, acc 0.765625, prec 0.0365425, recall 0.765789
2017-12-09T22:19:18.795953: step 536, loss 0.597272, acc 0.820312, prec 0.0365476, recall 0.765995
2017-12-09T22:19:19.091646: step 537, loss 0.838982, acc 0.742188, prec 0.0366984, recall 0.767016
2017-12-09T22:19:19.393126: step 538, loss 0.665036, acc 0.757812, prec 0.0367313, recall 0.767422
2017-12-09T22:19:19.691257: step 539, loss 2.939, acc 0.8125, prec 0.0366961, recall 0.766754
2017-12-09T22:19:19.991928: step 540, loss 2.82854, acc 0.789062, prec 0.0366564, recall 0.766087
2017-12-09T22:19:20.303441: step 541, loss 0.573502, acc 0.789062, prec 0.0366153, recall 0.766087
2017-12-09T22:19:20.602697: step 542, loss 0.456633, acc 0.851562, prec 0.0366264, recall 0.76629
2017-12-09T22:19:20.903083: step 543, loss 0.603727, acc 0.804688, prec 0.0367482, recall 0.7671
2017-12-09T22:19:21.203614: step 544, loss 1.90481, acc 0.828125, prec 0.0367961, recall 0.766839
2017-12-09T22:19:21.501787: step 545, loss 0.564651, acc 0.789062, prec 0.0367948, recall 0.767041
2017-12-09T22:19:21.805050: step 546, loss 0.646125, acc 0.726562, prec 0.0368212, recall 0.767442
2017-12-09T22:19:22.102655: step 547, loss 1.24046, acc 0.765625, prec 0.0368169, recall 0.766982
2017-12-09T22:19:22.407624: step 548, loss 1.40859, acc 0.726562, prec 0.0367653, recall 0.766323
2017-12-09T22:19:22.704402: step 549, loss 0.605592, acc 0.867188, prec 0.0367792, recall 0.766524
2017-12-09T22:19:23.001699: step 550, loss 1.09504, acc 0.734375, prec 0.036807, recall 0.766924
2017-12-09T22:19:23.299659: step 551, loss 0.621258, acc 0.78125, prec 0.0368043, recall 0.767123
2017-12-09T22:19:23.598922: step 552, loss 0.491306, acc 0.765625, prec 0.0367985, recall 0.767322
2017-12-09T22:19:23.894298: step 553, loss 0.690677, acc 0.734375, prec 0.0367472, recall 0.767322
2017-12-09T22:19:24.192121: step 554, loss 1.12, acc 0.765625, prec 0.0368204, recall 0.767918
2017-12-09T22:19:24.491768: step 555, loss 1.09233, acc 0.765625, prec 0.0370113, recall 0.7691
2017-12-09T22:19:24.789184: step 556, loss 0.718637, acc 0.765625, prec 0.0370446, recall 0.769492
2017-12-09T22:19:25.090205: step 557, loss 0.509606, acc 0.828125, prec 0.0370114, recall 0.769492
2017-12-09T22:19:25.389872: step 558, loss 0.623406, acc 0.773438, prec 0.0370069, recall 0.769687
2017-12-09T22:19:25.688180: step 559, loss 0.438431, acc 0.867188, prec 0.0370988, recall 0.77027
2017-12-09T22:19:25.984290: step 560, loss 0.635277, acc 0.796875, prec 0.037177, recall 0.770851
2017-12-09T22:19:26.285525: step 561, loss 1.6941, acc 0.875, prec 0.0371934, recall 0.770395
2017-12-09T22:19:26.587091: step 562, loss 1.9411, acc 0.851562, prec 0.0371663, recall 0.769748
2017-12-09T22:19:26.890993: step 563, loss 0.381427, acc 0.828125, prec 0.0371331, recall 0.769748
2017-12-09T22:19:27.193296: step 564, loss 0.37105, acc 0.867188, prec 0.0372246, recall 0.770327
2017-12-09T22:19:27.492093: step 565, loss 0.317579, acc 0.867188, prec 0.0372379, recall 0.770519
2017-12-09T22:19:27.791372: step 566, loss 0.4393, acc 0.84375, prec 0.0372467, recall 0.770711
2017-12-09T22:19:28.092632: step 567, loss 0.48311, acc 0.90625, prec 0.0373065, recall 0.771094
2017-12-09T22:19:28.388974: step 568, loss 0.417937, acc 0.898438, prec 0.0374813, recall 0.772047
2017-12-09T22:19:28.684785: step 569, loss 5.27002, acc 0.867188, prec 0.0374586, recall 0.770764
2017-12-09T22:19:28.989383: step 570, loss 3.82097, acc 0.828125, prec 0.0374672, recall 0.769677
2017-12-09T22:19:29.291467: step 571, loss 0.708492, acc 0.8125, prec 0.0375086, recall 0.770058
2017-12-09T22:19:29.588175: step 572, loss 0.563685, acc 0.804688, prec 0.0375483, recall 0.770438
2017-12-09T22:19:29.892148: step 573, loss 0.590756, acc 0.757812, prec 0.0375402, recall 0.770627
2017-12-09T22:19:30.199211: step 574, loss 0.766907, acc 0.773438, prec 0.0376124, recall 0.771193
2017-12-09T22:19:30.498256: step 575, loss 1.71459, acc 0.664062, prec 0.0375491, recall 0.770559
2017-12-09T22:19:30.797539: step 576, loss 1.12034, acc 0.75, prec 0.0376166, recall 0.771124
2017-12-09T22:19:31.096193: step 577, loss 1.02244, acc 0.695312, prec 0.0376733, recall 0.771686
2017-12-09T22:19:31.393815: step 578, loss 0.823317, acc 0.710938, prec 0.0376561, recall 0.771872
2017-12-09T22:19:31.692995: step 579, loss 1.06442, acc 0.6875, prec 0.0376344, recall 0.772059
2017-12-09T22:19:31.993868: step 580, loss 0.873205, acc 0.664062, prec 0.0376466, recall 0.772431
2017-12-09T22:19:32.292292: step 581, loss 0.957086, acc 0.679688, prec 0.0375853, recall 0.772431
2017-12-09T22:19:32.589444: step 582, loss 1.34382, acc 0.726562, prec 0.0376095, recall 0.772801
2017-12-09T22:19:32.890227: step 583, loss 1.15864, acc 0.703125, prec 0.0376672, recall 0.773355
2017-12-09T22:19:33.188367: step 584, loss 1.01353, acc 0.695312, prec 0.0376472, recall 0.773539
2017-12-09T22:19:33.491901: step 585, loss 1.06137, acc 0.6875, prec 0.0376637, recall 0.773906
2017-12-09T22:19:33.790785: step 586, loss 0.652943, acc 0.789062, prec 0.0376994, recall 0.774272
2017-12-09T22:19:34.088651: step 587, loss 1.2853, acc 0.75, prec 0.0377292, recall 0.774011
2017-12-09T22:19:34.390226: step 588, loss 0.353344, acc 0.851562, prec 0.0377388, recall 0.774194
2017-12-09T22:19:34.696651: step 589, loss 0.50288, acc 0.796875, prec 0.0377003, recall 0.774194
2017-12-09T22:19:34.999882: step 590, loss 1.22807, acc 0.820312, prec 0.0377055, recall 0.773752
2017-12-09T22:19:35.309185: step 591, loss 0.73228, acc 0.859375, prec 0.0377921, recall 0.774297
2017-12-09T22:19:35.612000: step 592, loss 1.7244, acc 0.882812, prec 0.0378467, recall 0.774038
2017-12-09T22:19:35.912946: step 593, loss 2.20696, acc 0.84375, prec 0.0378939, recall 0.773781
2017-12-09T22:19:36.216211: step 594, loss 1.02925, acc 0.835938, prec 0.0379757, recall 0.774322
2017-12-09T22:19:36.518456: step 595, loss 0.808598, acc 0.703125, prec 0.0379569, recall 0.774502
2017-12-09T22:19:36.814873: step 596, loss 0.602299, acc 0.757812, prec 0.0380236, recall 0.77504
2017-12-09T22:19:37.114012: step 597, loss 0.700232, acc 0.789062, prec 0.0379836, recall 0.77504
2017-12-09T22:19:37.412209: step 598, loss 0.66897, acc 0.804688, prec 0.0379466, recall 0.77504
2017-12-09T22:19:37.709668: step 599, loss 0.646944, acc 0.75, prec 0.0379742, recall 0.775397
2017-12-09T22:19:38.016590: step 600, loss 0.53943, acc 0.765625, prec 0.038042, recall 0.77593

Evaluation:
2017-12-09T22:19:42.668467: step 600, loss 1.23983, acc 0.781678, prec 0.0395657, recall 0.781359

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-600

2017-12-09T22:19:43.968363: step 601, loss 5.26, acc 0.78125, prec 0.0395618, recall 0.780966
2017-12-09T22:19:44.265331: step 602, loss 0.527085, acc 0.789062, prec 0.039524, recall 0.780966
2017-12-09T22:19:44.559735: step 603, loss 0.798327, acc 0.773438, prec 0.0395854, recall 0.781425
2017-12-09T22:19:44.856372: step 604, loss 0.66249, acc 0.804688, prec 0.0396862, recall 0.782033
2017-12-09T22:19:45.164034: step 605, loss 1.22414, acc 0.757812, prec 0.0398122, recall 0.78279
2017-12-09T22:19:45.461061: step 606, loss 0.99853, acc 0.695312, prec 0.0397913, recall 0.78294
2017-12-09T22:19:45.758794: step 607, loss 0.619847, acc 0.773438, prec 0.0398183, recall 0.783241
2017-12-09T22:19:46.058755: step 608, loss 1.38426, acc 0.679688, prec 0.0398285, recall 0.783541
2017-12-09T22:19:46.353603: step 609, loss 0.488295, acc 0.796875, prec 0.0398595, recall 0.78384
2017-12-09T22:19:46.648785: step 610, loss 0.425878, acc 0.835938, prec 0.0398975, recall 0.784138
2017-12-09T22:19:46.947102: step 611, loss 0.575661, acc 0.84375, prec 0.0399705, recall 0.784584
2017-12-09T22:19:47.253005: step 612, loss 0.561008, acc 0.796875, prec 0.0400686, recall 0.785175
2017-12-09T22:19:47.552039: step 613, loss 0.491228, acc 0.804688, prec 0.0401344, recall 0.785616
2017-12-09T22:19:47.856164: step 614, loss 0.597404, acc 0.84375, prec 0.040207, recall 0.786056
2017-12-09T22:19:48.154337: step 615, loss 0.374644, acc 0.875, prec 0.0401845, recall 0.786056
2017-12-09T22:19:48.456119: step 616, loss 2.60131, acc 0.835938, prec 0.04019, recall 0.785666
2017-12-09T22:19:48.758804: step 617, loss 0.267706, acc 0.898438, prec 0.0402052, recall 0.785812
2017-12-09T22:19:49.050886: step 618, loss 0.284348, acc 0.890625, prec 0.0401856, recall 0.785812
2017-12-09T22:19:49.351053: step 619, loss 0.528791, acc 0.898438, prec 0.0402343, recall 0.786104
2017-12-09T22:19:49.650791: step 620, loss 0.210151, acc 0.890625, prec 0.0402147, recall 0.786104
2017-12-09T22:19:49.951790: step 621, loss 0.293236, acc 0.875, prec 0.0402257, recall 0.786249
2017-12-09T22:19:50.263484: step 622, loss 3.06179, acc 0.90625, prec 0.0402451, recall 0.785326
2017-12-09T22:19:50.564926: step 623, loss 0.257684, acc 0.898438, prec 0.0402269, recall 0.785326
2017-12-09T22:19:50.857628: step 624, loss 0.237149, acc 0.914062, prec 0.0402449, recall 0.785472
2017-12-09T22:19:51.157371: step 625, loss 0.422852, acc 0.851562, prec 0.0402517, recall 0.785617
2017-12-09T22:19:51.459020: step 626, loss 3.75325, acc 0.828125, prec 0.040289, recall 0.785376
2017-12-09T22:19:51.759513: step 627, loss 0.544094, acc 0.789062, prec 0.0403178, recall 0.785666
2017-12-09T22:19:52.059760: step 628, loss 0.5089, acc 0.84375, prec 0.0403231, recall 0.785811
2017-12-09T22:19:52.372588: step 629, loss 0.497302, acc 0.859375, prec 0.0403312, recall 0.785955
2017-12-09T22:19:52.670157: step 630, loss 0.46119, acc 0.835938, prec 0.0403019, recall 0.785955
2017-12-09T22:19:52.971086: step 631, loss 0.80453, acc 0.773438, prec 0.0403279, recall 0.786244
2017-12-09T22:19:53.270553: step 632, loss 0.875202, acc 0.773438, prec 0.0403206, recall 0.786388
2017-12-09T22:19:53.578051: step 633, loss 0.566692, acc 0.867188, prec 0.0404295, recall 0.786962
2017-12-09T22:19:53.878852: step 634, loss 0.935839, acc 0.859375, prec 0.0404706, recall 0.787248
2017-12-09T22:19:54.179063: step 635, loss 1.94006, acc 0.851562, prec 0.040513, recall 0.786479
2017-12-09T22:19:54.484108: step 636, loss 0.555499, acc 0.804688, prec 0.0404782, recall 0.786479
2017-12-09T22:19:54.784779: step 637, loss 0.405743, acc 0.890625, prec 0.0405578, recall 0.786907
2017-12-09T22:19:55.084125: step 638, loss 0.681506, acc 0.765625, prec 0.0406479, recall 0.787475
2017-12-09T22:19:55.380021: step 639, loss 0.431312, acc 0.859375, prec 0.0406887, recall 0.787758
2017-12-09T22:19:55.675616: step 640, loss 0.770571, acc 0.742188, prec 0.0406755, recall 0.787899
2017-12-09T22:19:55.985068: step 641, loss 0.997627, acc 0.78125, prec 0.040768, recall 0.788462
2017-12-09T22:19:56.280436: step 642, loss 0.57675, acc 0.789062, prec 0.0407303, recall 0.788462
2017-12-09T22:19:56.581315: step 643, loss 0.775971, acc 0.867188, prec 0.0407723, recall 0.788742
2017-12-09T22:19:56.881924: step 644, loss 0.54105, acc 0.796875, prec 0.0407361, recall 0.788742
2017-12-09T22:19:57.182545: step 645, loss 0.53346, acc 0.859375, prec 0.0407438, recall 0.788882
2017-12-09T22:19:57.482887: step 646, loss 0.473565, acc 0.835938, prec 0.0407473, recall 0.789021
2017-12-09T22:19:57.780571: step 647, loss 0.331188, acc 0.867188, prec 0.0407892, recall 0.7893
2017-12-09T22:19:58.083253: step 648, loss 5.08607, acc 0.875, prec 0.0408024, recall 0.788398
2017-12-09T22:19:58.384634: step 649, loss 0.368649, acc 0.867188, prec 0.0407788, recall 0.788398
2017-12-09T22:19:58.678614: step 650, loss 0.583656, acc 0.859375, prec 0.0408191, recall 0.788677
2017-12-09T22:19:58.976193: step 651, loss 0.689074, acc 0.851562, prec 0.040858, recall 0.788955
2017-12-09T22:19:59.271970: step 652, loss 0.513455, acc 0.8125, prec 0.0408573, recall 0.789093
2017-12-09T22:19:59.568128: step 653, loss 0.700784, acc 0.828125, prec 0.0408919, recall 0.78937
2017-12-09T22:19:59.873101: step 654, loss 0.583104, acc 0.867188, prec 0.0409661, recall 0.789784
2017-12-09T22:20:00.177821: step 655, loss 0.481282, acc 0.867188, prec 0.0410075, recall 0.790059
2017-12-09T22:20:00.487529: step 656, loss 0.379359, acc 0.882812, prec 0.0410517, recall 0.790333
2017-12-09T22:20:00.780772: step 657, loss 0.240576, acc 0.914062, prec 0.0410364, recall 0.790333
2017-12-09T22:20:01.081274: step 658, loss 2.27682, acc 0.835938, prec 0.04101, recall 0.789302
2017-12-09T22:20:01.386820: step 659, loss 0.511503, acc 0.820312, prec 0.0409781, recall 0.789302
2017-12-09T22:20:01.681454: step 660, loss 0.833922, acc 0.835938, prec 0.0410138, recall 0.789577
2017-12-09T22:20:01.977719: step 661, loss 0.410727, acc 0.84375, prec 0.041051, recall 0.78985
2017-12-09T22:20:02.272391: step 662, loss 0.455599, acc 0.789062, prec 0.0410783, recall 0.790123
2017-12-09T22:20:02.572912: step 663, loss 0.630842, acc 0.859375, prec 0.0411505, recall 0.790532
2017-12-09T22:20:02.877761: step 664, loss 2.72699, acc 0.804688, prec 0.0411832, recall 0.78978
2017-12-09T22:20:03.173847: step 665, loss 0.495421, acc 0.867188, prec 0.0412889, recall 0.790323
2017-12-09T22:20:03.471241: step 666, loss 0.576154, acc 0.804688, prec 0.0412864, recall 0.790458
2017-12-09T22:20:03.770224: step 667, loss 0.769058, acc 0.765625, prec 0.0412447, recall 0.790458
2017-12-09T22:20:04.066532: step 668, loss 0.664745, acc 0.75, prec 0.0412648, recall 0.790728
2017-12-09T22:20:04.368531: step 669, loss 0.421563, acc 0.773438, prec 0.0413212, recall 0.791131
2017-12-09T22:20:04.664829: step 670, loss 0.394979, acc 0.84375, prec 0.0413578, recall 0.791399
2017-12-09T22:20:04.964736: step 671, loss 0.537169, acc 0.796875, prec 0.0414181, recall 0.7918
2017-12-09T22:20:05.267686: step 672, loss 0.38123, acc 0.875, prec 0.0414601, recall 0.792067
2017-12-09T22:20:05.570433: step 673, loss 0.596094, acc 0.820312, prec 0.0414603, recall 0.792199
2017-12-09T22:20:05.869606: step 674, loss 0.799738, acc 0.859375, prec 0.0415956, recall 0.792862
2017-12-09T22:20:06.172015: step 675, loss 2.37057, acc 0.835938, prec 0.0417599, recall 0.793147
2017-12-09T22:20:06.473933: step 676, loss 0.765805, acc 0.921875, prec 0.04181, recall 0.793409
2017-12-09T22:20:06.772938: step 677, loss 0.390879, acc 0.84375, prec 0.0418141, recall 0.79354
2017-12-09T22:20:07.073249: step 678, loss 0.593613, acc 0.84375, prec 0.0418501, recall 0.793801
2017-12-09T22:20:07.372331: step 679, loss 0.490503, acc 0.8125, prec 0.0418805, recall 0.794062
2017-12-09T22:20:07.673639: step 680, loss 0.40043, acc 0.859375, prec 0.0418554, recall 0.794062
2017-12-09T22:20:07.972099: step 681, loss 0.398134, acc 0.867188, prec 0.0418636, recall 0.794192
2017-12-09T22:20:08.274202: step 682, loss 0.450876, acc 0.859375, prec 0.0418385, recall 0.794192
2017-12-09T22:20:08.576706: step 683, loss 0.499524, acc 0.820312, prec 0.0418065, recall 0.794192
2017-12-09T22:20:08.875070: step 684, loss 1.09987, acc 0.84375, prec 0.0418438, recall 0.793951
2017-12-09T22:20:09.174988: step 685, loss 0.358514, acc 0.875, prec 0.0418852, recall 0.79421
2017-12-09T22:20:09.473967: step 686, loss 0.272501, acc 0.914062, prec 0.0418699, recall 0.79421
2017-12-09T22:20:09.776848: step 687, loss 0.179394, acc 0.9375, prec 0.0419223, recall 0.794469
2017-12-09T22:20:10.072426: step 688, loss 0.306983, acc 0.90625, prec 0.0419692, recall 0.794727
2017-12-09T22:20:10.371541: step 689, loss 1.95751, acc 0.859375, prec 0.042009, recall 0.794486
2017-12-09T22:20:10.670740: step 690, loss 0.268474, acc 0.898438, prec 0.0419909, recall 0.794486
2017-12-09T22:20:10.969971: step 691, loss 1.62967, acc 0.859375, prec 0.0419673, recall 0.793989
2017-12-09T22:20:11.265881: step 692, loss 0.686819, acc 0.875, prec 0.0421035, recall 0.794632
2017-12-09T22:20:11.573293: step 693, loss 0.347144, acc 0.851562, prec 0.0421087, recall 0.79476
2017-12-09T22:20:11.873520: step 694, loss 0.366157, acc 0.898438, prec 0.0421539, recall 0.795016
2017-12-09T22:20:12.172697: step 695, loss 0.754288, acc 0.890625, prec 0.0421977, recall 0.795271
2017-12-09T22:20:12.473727: step 696, loss 1.10694, acc 0.851562, prec 0.0422675, recall 0.795158
2017-12-09T22:20:12.772633: step 697, loss 0.366298, acc 0.890625, prec 0.0422795, recall 0.795285
2017-12-09T22:20:13.072832: step 698, loss 0.578238, acc 0.78125, prec 0.0422405, recall 0.795285
2017-12-09T22:20:13.369521: step 699, loss 0.469804, acc 0.8125, prec 0.0422387, recall 0.795412
2017-12-09T22:20:13.670932: step 700, loss 0.656766, acc 0.8125, prec 0.0422053, recall 0.795412
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-700

2017-12-09T22:20:15.020851: step 701, loss 0.4098, acc 0.820312, prec 0.0421734, recall 0.795412
2017-12-09T22:20:15.317857: step 702, loss 0.664613, acc 0.828125, prec 0.0422059, recall 0.795666
2017-12-09T22:20:15.619793: step 703, loss 0.642309, acc 0.851562, prec 0.0423053, recall 0.79617
2017-12-09T22:20:15.922367: step 704, loss 0.550114, acc 0.84375, prec 0.0423404, recall 0.796422
2017-12-09T22:20:16.220344: step 705, loss 0.790663, acc 0.84375, prec 0.042344, recall 0.796547
2017-12-09T22:20:16.518928: step 706, loss 0.353398, acc 0.875, prec 0.0423218, recall 0.796547
2017-12-09T22:20:16.820150: step 707, loss 0.649141, acc 0.804688, prec 0.0423499, recall 0.796798
2017-12-09T22:20:17.124947: step 708, loss 0.244356, acc 0.90625, prec 0.0423959, recall 0.797048
2017-12-09T22:20:17.423358: step 709, loss 0.47988, acc 0.859375, prec 0.0424649, recall 0.797422
2017-12-09T22:20:17.722017: step 710, loss 0.55865, acc 0.84375, prec 0.042531, recall 0.797794
2017-12-09T22:20:18.022595: step 711, loss 0.373373, acc 0.859375, prec 0.042506, recall 0.797794
2017-12-09T22:20:18.324591: step 712, loss 0.306408, acc 0.867188, prec 0.0425136, recall 0.797918
2017-12-09T22:20:18.622111: step 713, loss 0.268001, acc 0.898438, prec 0.0424956, recall 0.797918
2017-12-09T22:20:18.921988: step 714, loss 1.21967, acc 0.875, prec 0.0424748, recall 0.79743
2017-12-09T22:20:19.224537: step 715, loss 1.60793, acc 0.945312, prec 0.0424665, recall 0.796942
2017-12-09T22:20:19.525456: step 716, loss 0.53004, acc 0.929688, prec 0.0424853, recall 0.797066
2017-12-09T22:20:19.823990: step 717, loss 2.21934, acc 0.90625, prec 0.0425636, recall 0.796951
2017-12-09T22:20:20.135592: step 718, loss 0.319891, acc 0.867188, prec 0.0426024, recall 0.797199
2017-12-09T22:20:20.444980: step 719, loss 0.279651, acc 0.882812, prec 0.0425816, recall 0.797199
2017-12-09T22:20:20.746591: step 720, loss 1.44884, acc 0.835938, prec 0.0426472, recall 0.797084
2017-12-09T22:20:21.050487: step 721, loss 0.41348, acc 0.84375, prec 0.0427128, recall 0.797453
2017-12-09T22:20:21.348071: step 722, loss 2.71683, acc 0.835938, prec 0.0426851, recall 0.79697
2017-12-09T22:20:21.647936: step 723, loss 0.879749, acc 0.8125, prec 0.0427761, recall 0.797461
2017-12-09T22:20:21.947027: step 724, loss 0.81834, acc 0.742188, prec 0.0427303, recall 0.797461
2017-12-09T22:20:22.247107: step 725, loss 0.772396, acc 0.710938, prec 0.0427411, recall 0.797705
2017-12-09T22:20:22.543224: step 726, loss 1.15401, acc 0.617188, prec 0.0427044, recall 0.797827
2017-12-09T22:20:22.841232: step 727, loss 0.930344, acc 0.695312, prec 0.0426815, recall 0.797949
2017-12-09T22:20:23.146399: step 728, loss 0.857279, acc 0.679688, prec 0.0427177, recall 0.798314
2017-12-09T22:20:23.446870: step 729, loss 1.00494, acc 0.640625, prec 0.0426853, recall 0.798436
2017-12-09T22:20:23.743816: step 730, loss 1.11673, acc 0.695312, prec 0.0426626, recall 0.798557
2017-12-09T22:20:24.045872: step 731, loss 0.631427, acc 0.757812, prec 0.0426508, recall 0.798678
2017-12-09T22:20:24.346388: step 732, loss 0.818228, acc 0.710938, prec 0.0426309, recall 0.798799
2017-12-09T22:20:24.646282: step 733, loss 0.559342, acc 0.796875, prec 0.0425954, recall 0.798799
2017-12-09T22:20:24.949250: step 734, loss 1.87363, acc 0.851562, prec 0.0426322, recall 0.798561
2017-12-09T22:20:25.251594: step 735, loss 0.683542, acc 0.828125, prec 0.0426328, recall 0.798682
2017-12-09T22:20:25.551989: step 736, loss 1.64673, acc 0.84375, prec 0.0426375, recall 0.798324
2017-12-09T22:20:25.855275: step 737, loss 0.464154, acc 0.828125, prec 0.0426687, recall 0.798565
2017-12-09T22:20:26.159184: step 738, loss 0.403303, acc 0.875, prec 0.042708, recall 0.798806
2017-12-09T22:20:26.460762: step 739, loss 0.449254, acc 0.84375, prec 0.0427724, recall 0.799166
2017-12-09T22:20:26.760898: step 740, loss 0.348145, acc 0.929688, prec 0.0427906, recall 0.799285
2017-12-09T22:20:27.061937: step 741, loss 0.314149, acc 0.898438, prec 0.0427729, recall 0.799285
2017-12-09T22:20:27.364302: step 742, loss 0.642768, acc 0.898438, prec 0.0428162, recall 0.799524
2017-12-09T22:20:27.658925: step 743, loss 1.06607, acc 0.914062, prec 0.042833, recall 0.799168
2017-12-09T22:20:27.957093: step 744, loss 1.24313, acc 0.84375, prec 0.0428985, recall 0.799052
2017-12-09T22:20:28.261713: step 745, loss 1.58945, acc 0.90625, prec 0.042914, recall 0.798697
2017-12-09T22:20:28.563921: step 746, loss 0.384163, acc 0.898438, prec 0.0429266, recall 0.798817
2017-12-09T22:20:28.747994: step 747, loss 0.487851, acc 0.862745, prec 0.0429171, recall 0.798817
2017-12-09T22:20:29.058338: step 748, loss 0.411345, acc 0.859375, prec 0.0429534, recall 0.799054
2017-12-09T22:20:29.357911: step 749, loss 0.332764, acc 0.851562, prec 0.0429274, recall 0.799054
2017-12-09T22:20:29.658312: step 750, loss 0.379873, acc 0.882812, prec 0.0429981, recall 0.79941
2017-12-09T22:20:29.963485: step 751, loss 0.287632, acc 0.90625, prec 0.0429818, recall 0.79941
2017-12-09T22:20:30.268457: step 752, loss 0.393737, acc 0.875, prec 0.04296, recall 0.79941
2017-12-09T22:20:30.565427: step 753, loss 0.419304, acc 0.84375, prec 0.0429934, recall 0.799646
2017-12-09T22:20:30.865339: step 754, loss 0.220405, acc 0.898438, prec 0.0429757, recall 0.799646
2017-12-09T22:20:31.160099: step 755, loss 0.760677, acc 0.84375, prec 0.0430091, recall 0.799882
2017-12-09T22:20:31.459022: step 756, loss 0.45932, acc 0.90625, prec 0.0431138, recall 0.800352
2017-12-09T22:20:31.759630: step 757, loss 0.401593, acc 0.890625, prec 0.043125, recall 0.800469
2017-12-09T22:20:32.057894: step 758, loss 0.345995, acc 0.867188, prec 0.0431321, recall 0.800587
2017-12-09T22:20:32.357194: step 759, loss 0.267157, acc 0.90625, prec 0.0431459, recall 0.800703
2017-12-09T22:20:32.658837: step 760, loss 0.649259, acc 0.9375, prec 0.0432257, recall 0.801053
2017-12-09T22:20:32.956923: step 761, loss 0.297529, acc 0.90625, prec 0.0432395, recall 0.80117
2017-12-09T22:20:33.254666: step 762, loss 0.225952, acc 0.90625, prec 0.0432533, recall 0.801286
2017-12-09T22:20:33.550986: step 763, loss 0.254273, acc 0.898438, prec 0.0432657, recall 0.801402
2017-12-09T22:20:33.848847: step 764, loss 0.333208, acc 0.914062, prec 0.0432809, recall 0.801518
2017-12-09T22:20:34.141581: step 765, loss 2.21455, acc 0.898438, prec 0.0432947, recall 0.801166
2017-12-09T22:20:34.443452: step 766, loss 0.432042, acc 0.914062, prec 0.0433098, recall 0.801282
2017-12-09T22:20:34.741331: step 767, loss 1.86029, acc 0.914062, prec 0.0433263, recall 0.800931
2017-12-09T22:20:35.039773: step 768, loss 2.45932, acc 0.84375, prec 0.0433907, recall 0.800813
2017-12-09T22:20:35.346628: step 769, loss 0.38322, acc 0.851562, prec 0.0434249, recall 0.801044
2017-12-09T22:20:35.648961: step 770, loss 0.542593, acc 0.804688, prec 0.0434509, recall 0.801275
2017-12-09T22:20:35.949677: step 771, loss 0.569682, acc 0.820312, prec 0.0434496, recall 0.80139
2017-12-09T22:20:36.246822: step 772, loss 0.836707, acc 0.726562, prec 0.0434619, recall 0.801619
2017-12-09T22:20:36.544138: step 773, loss 0.784436, acc 0.734375, prec 0.0434755, recall 0.801849
2017-12-09T22:20:36.839925: step 774, loss 0.626197, acc 0.710938, prec 0.0434851, recall 0.802077
2017-12-09T22:20:37.138284: step 775, loss 0.728162, acc 0.734375, prec 0.0434688, recall 0.802191
2017-12-09T22:20:37.435841: step 776, loss 0.629061, acc 0.835938, prec 0.0434701, recall 0.802305
2017-12-09T22:20:37.734404: step 777, loss 0.650625, acc 0.789062, prec 0.0435529, recall 0.80276
2017-12-09T22:20:38.032190: step 778, loss 0.45306, acc 0.859375, prec 0.0435881, recall 0.802987
2017-12-09T22:20:38.332268: step 779, loss 0.522503, acc 0.78125, prec 0.0435798, recall 0.8031
2017-12-09T22:20:38.630088: step 780, loss 0.785514, acc 0.820312, prec 0.0436082, recall 0.803326
2017-12-09T22:20:38.933023: step 781, loss 1.01967, acc 0.914062, prec 0.0436825, recall 0.803663
2017-12-09T22:20:39.228750: step 782, loss 0.645394, acc 0.828125, prec 0.0436824, recall 0.803776
2017-12-09T22:20:39.529915: step 783, loss 0.454828, acc 0.882812, prec 0.0436917, recall 0.803888
2017-12-09T22:20:39.832945: step 784, loss 0.632725, acc 0.835938, prec 0.0437523, recall 0.804224
2017-12-09T22:20:40.131355: step 785, loss 0.425829, acc 0.84375, prec 0.0437252, recall 0.804224
2017-12-09T22:20:40.428961: step 786, loss 0.374697, acc 0.882812, prec 0.0437642, recall 0.804447
2017-12-09T22:20:40.723694: step 787, loss 0.317271, acc 0.90625, prec 0.0437775, recall 0.804558
2017-12-09T22:20:41.022460: step 788, loss 1.94553, acc 0.898438, prec 0.0437909, recall 0.804212
2017-12-09T22:20:41.323632: step 789, loss 0.315196, acc 0.898438, prec 0.0438325, recall 0.804434
2017-12-09T22:20:41.620240: step 790, loss 0.34256, acc 0.890625, prec 0.0438135, recall 0.804434
2017-12-09T22:20:41.917817: step 791, loss 0.355925, acc 0.890625, prec 0.0438537, recall 0.804656
2017-12-09T22:20:42.213099: step 792, loss 0.537125, acc 0.890625, prec 0.0438938, recall 0.804878
2017-12-09T22:20:42.515102: step 793, loss 0.43217, acc 0.890625, prec 0.0439931, recall 0.80532
2017-12-09T22:20:42.811897: step 794, loss 0.343552, acc 0.898438, prec 0.0440049, recall 0.80543
2017-12-09T22:20:43.109873: step 795, loss 0.349892, acc 0.914062, prec 0.04399, recall 0.80543
2017-12-09T22:20:43.410883: step 796, loss 0.41064, acc 0.890625, prec 0.0440005, recall 0.80554
2017-12-09T22:20:43.710214: step 797, loss 0.122908, acc 0.945312, prec 0.0440205, recall 0.80565
2017-12-09T22:20:44.006878: step 798, loss 0.217323, acc 0.945312, prec 0.044011, recall 0.80565
2017-12-09T22:20:44.303446: step 799, loss 1.12236, acc 0.921875, prec 0.0439988, recall 0.805195
2017-12-09T22:20:44.600209: step 800, loss 0.1444, acc 0.945312, prec 0.0440188, recall 0.805305
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-800

2017-12-09T22:20:46.078087: step 801, loss 0.516844, acc 0.921875, prec 0.0440347, recall 0.805415
2017-12-09T22:20:46.378208: step 802, loss 1.72174, acc 0.945312, prec 0.044056, recall 0.80507
2017-12-09T22:20:46.676706: step 803, loss 0.13338, acc 0.953125, prec 0.0440773, recall 0.80518
2017-12-09T22:20:46.977322: step 804, loss 0.136261, acc 0.929688, prec 0.0440651, recall 0.80518
2017-12-09T22:20:47.281272: step 805, loss 0.50248, acc 0.914062, prec 0.044109, recall 0.805399
2017-12-09T22:20:47.578801: step 806, loss 0.197853, acc 0.9375, prec 0.0441276, recall 0.805509
2017-12-09T22:20:47.872490: step 807, loss 0.587418, acc 0.882812, prec 0.0441955, recall 0.805836
2017-12-09T22:20:48.175495: step 808, loss 0.321281, acc 0.882812, prec 0.0442045, recall 0.805945
2017-12-09T22:20:48.472518: step 809, loss 0.381033, acc 0.84375, prec 0.0442067, recall 0.806054
2017-12-09T22:20:48.775782: step 810, loss 0.589776, acc 0.898438, prec 0.0442772, recall 0.806379
2017-12-09T22:20:49.074535: step 811, loss 0.359317, acc 0.859375, prec 0.044282, recall 0.806488
2017-12-09T22:20:49.376032: step 812, loss 0.270617, acc 0.90625, prec 0.0443244, recall 0.806704
2017-12-09T22:20:49.676874: step 813, loss 0.197956, acc 0.9375, prec 0.0443135, recall 0.806704
2017-12-09T22:20:49.974668: step 814, loss 0.841424, acc 0.90625, prec 0.0444438, recall 0.807242
2017-12-09T22:20:50.283678: step 815, loss 0.426465, acc 0.859375, prec 0.0444485, recall 0.80735
2017-12-09T22:20:50.578736: step 816, loss 0.288726, acc 0.875, prec 0.0444267, recall 0.80735
2017-12-09T22:20:50.882069: step 817, loss 0.40383, acc 0.882812, prec 0.0444941, recall 0.807671
2017-12-09T22:20:51.181320: step 818, loss 0.545165, acc 0.875, prec 0.0445016, recall 0.807778
2017-12-09T22:20:51.478872: step 819, loss 0.8538, acc 0.914062, prec 0.044488, recall 0.807329
2017-12-09T22:20:51.782124: step 820, loss 0.517704, acc 0.820312, prec 0.0445151, recall 0.807543
2017-12-09T22:20:52.084371: step 821, loss 0.499908, acc 0.921875, prec 0.0445599, recall 0.807756
2017-12-09T22:20:52.386003: step 822, loss 0.358191, acc 0.875, prec 0.0445381, recall 0.807756
2017-12-09T22:20:52.683855: step 823, loss 0.500882, acc 0.890625, prec 0.0446066, recall 0.808075
2017-12-09T22:20:52.979815: step 824, loss 0.959414, acc 0.875, prec 0.0446723, recall 0.808393
2017-12-09T22:20:53.279047: step 825, loss 0.865926, acc 0.90625, prec 0.0448016, recall 0.808921
2017-12-09T22:20:53.582652: step 826, loss 0.289389, acc 0.867188, prec 0.0448657, recall 0.809236
2017-12-09T22:20:53.883954: step 827, loss 0.52752, acc 0.90625, prec 0.0449075, recall 0.809445
2017-12-09T22:20:54.181424: step 828, loss 0.239016, acc 0.867188, prec 0.0448843, recall 0.809445
2017-12-09T22:20:54.486092: step 829, loss 0.488697, acc 0.875, prec 0.0449206, recall 0.809654
2017-12-09T22:20:54.783939: step 830, loss 0.426438, acc 0.859375, prec 0.044925, recall 0.809759
2017-12-09T22:20:55.089830: step 831, loss 0.348423, acc 0.875, prec 0.0449612, recall 0.809967
2017-12-09T22:20:55.387183: step 832, loss 0.289482, acc 0.882812, prec 0.0449698, recall 0.810071
2017-12-09T22:20:55.684820: step 833, loss 0.374438, acc 0.882812, prec 0.0449783, recall 0.810175
2017-12-09T22:20:55.983758: step 834, loss 0.274315, acc 0.890625, prec 0.0450171, recall 0.810382
2017-12-09T22:20:56.286852: step 835, loss 0.779285, acc 0.859375, prec 0.0450215, recall 0.810486
2017-12-09T22:20:56.589587: step 836, loss 0.269921, acc 0.882812, prec 0.0450011, recall 0.810486
2017-12-09T22:20:56.891907: step 837, loss 0.396093, acc 0.898438, prec 0.0450123, recall 0.810589
2017-12-09T22:20:57.186611: step 838, loss 0.445771, acc 0.859375, prec 0.0450456, recall 0.810796
2017-12-09T22:20:57.482651: step 839, loss 0.632019, acc 0.921875, prec 0.0451187, recall 0.811105
2017-12-09T22:20:57.785379: step 840, loss 0.366545, acc 0.953125, prec 0.0452261, recall 0.811516
2017-12-09T22:20:58.086488: step 841, loss 1.25873, acc 0.875, prec 0.0453212, recall 0.811484
2017-12-09T22:20:58.384152: step 842, loss 0.291257, acc 0.914062, prec 0.0453927, recall 0.81179
2017-12-09T22:20:58.683931: step 843, loss 0.627223, acc 0.84375, prec 0.0454518, recall 0.812095
2017-12-09T22:20:58.986414: step 844, loss 0.340086, acc 0.90625, prec 0.0454353, recall 0.812095
2017-12-09T22:20:59.282899: step 845, loss 0.467549, acc 0.828125, prec 0.0454051, recall 0.812095
2017-12-09T22:20:59.583866: step 846, loss 1.26203, acc 0.898438, prec 0.0454175, recall 0.811758
2017-12-09T22:20:59.891125: step 847, loss 0.574014, acc 0.796875, prec 0.045497, recall 0.812164
2017-12-09T22:21:00.198627: step 848, loss 1.99732, acc 0.890625, prec 0.0455655, recall 0.81203
2017-12-09T22:21:00.499274: step 849, loss 0.550772, acc 0.859375, prec 0.0455983, recall 0.812232
2017-12-09T22:21:00.795124: step 850, loss 0.520465, acc 0.84375, prec 0.0456857, recall 0.812634
2017-12-09T22:21:01.099180: step 851, loss 0.425315, acc 0.835938, prec 0.0456856, recall 0.812734
2017-12-09T22:21:01.392902: step 852, loss 0.620702, acc 0.859375, prec 0.0457469, recall 0.813034
2017-12-09T22:21:01.687742: step 853, loss 0.869549, acc 0.84375, prec 0.0458341, recall 0.813433
2017-12-09T22:21:01.988584: step 854, loss 0.4845, acc 0.8125, prec 0.0458297, recall 0.813532
2017-12-09T22:21:02.285038: step 855, loss 0.624101, acc 0.804688, prec 0.0457953, recall 0.813532
2017-12-09T22:21:02.583059: step 856, loss 0.615526, acc 0.804688, prec 0.045761, recall 0.813532
2017-12-09T22:21:02.875448: step 857, loss 0.43122, acc 0.835938, prec 0.0457894, recall 0.813731
2017-12-09T22:21:03.171137: step 858, loss 0.414842, acc 0.835938, prec 0.0458463, recall 0.814028
2017-12-09T22:21:03.470656: step 859, loss 0.446253, acc 0.867188, prec 0.0458515, recall 0.814126
2017-12-09T22:21:03.764601: step 860, loss 0.518718, acc 0.851562, prec 0.0458825, recall 0.814324
2017-12-09T22:21:04.059071: step 861, loss 0.317646, acc 0.875, prec 0.0459746, recall 0.814717
2017-12-09T22:21:04.355577: step 862, loss 1.50169, acc 0.890625, prec 0.0459852, recall 0.814384
2017-12-09T22:21:04.657395: step 863, loss 0.561234, acc 0.898438, prec 0.0460528, recall 0.814678
2017-12-09T22:21:04.954664: step 864, loss 0.22781, acc 0.914062, prec 0.0460377, recall 0.814678
2017-12-09T22:21:05.266970: step 865, loss 0.246845, acc 0.90625, prec 0.0460781, recall 0.814873
2017-12-09T22:21:05.569580: step 866, loss 0.28775, acc 0.914062, prec 0.0460915, recall 0.814971
2017-12-09T22:21:05.866777: step 867, loss 0.653397, acc 0.898438, prec 0.0461873, recall 0.81536
2017-12-09T22:21:06.168922: step 868, loss 0.729097, acc 0.875, prec 0.0462221, recall 0.815554
2017-12-09T22:21:06.480809: step 869, loss 1.08754, acc 0.828125, prec 0.0462216, recall 0.815223
2017-12-09T22:21:06.780756: step 870, loss 0.370045, acc 0.875, prec 0.0462847, recall 0.815514
2017-12-09T22:21:07.077603: step 871, loss 0.348145, acc 0.859375, prec 0.0462883, recall 0.81561
2017-12-09T22:21:07.376491: step 872, loss 0.524583, acc 0.84375, prec 0.0462891, recall 0.815707
2017-12-09T22:21:07.679459: step 873, loss 1.39453, acc 0.898438, prec 0.0462726, recall 0.81528
2017-12-09T22:21:07.990172: step 874, loss 0.441413, acc 0.882812, prec 0.046337, recall 0.815569
2017-12-09T22:21:08.289595: step 875, loss 0.449506, acc 0.875, prec 0.0463716, recall 0.815762
2017-12-09T22:21:08.591227: step 876, loss 0.543211, acc 0.828125, prec 0.0463413, recall 0.815762
2017-12-09T22:21:08.889414: step 877, loss 0.40636, acc 0.84375, prec 0.0464269, recall 0.816146
2017-12-09T22:21:09.186331: step 878, loss 0.546351, acc 0.820312, prec 0.04648, recall 0.816433
2017-12-09T22:21:09.493740: step 879, loss 0.499856, acc 0.835938, prec 0.0465921, recall 0.816909
2017-12-09T22:21:09.800430: step 880, loss 0.559673, acc 0.851562, prec 0.0466787, recall 0.817288
2017-12-09T22:21:10.102397: step 881, loss 0.450912, acc 0.84375, prec 0.0466793, recall 0.817382
2017-12-09T22:21:10.398407: step 882, loss 0.547081, acc 0.828125, prec 0.0467615, recall 0.817759
2017-12-09T22:21:10.698446: step 883, loss 0.438035, acc 0.851562, prec 0.0467634, recall 0.817853
2017-12-09T22:21:11.001632: step 884, loss 0.35066, acc 0.875, prec 0.0467976, recall 0.818041
2017-12-09T22:21:11.299047: step 885, loss 0.217372, acc 0.898438, prec 0.0467797, recall 0.818041
2017-12-09T22:21:11.595428: step 886, loss 0.191667, acc 0.9375, prec 0.0467686, recall 0.818041
2017-12-09T22:21:11.898300: step 887, loss 0.235928, acc 0.914062, prec 0.0468658, recall 0.818416
2017-12-09T22:21:12.195765: step 888, loss 1.46047, acc 0.90625, prec 0.0469348, recall 0.818275
2017-12-09T22:21:12.492632: step 889, loss 0.789073, acc 0.882812, prec 0.0469716, recall 0.818042
2017-12-09T22:21:12.793629: step 890, loss 0.397222, acc 0.890625, prec 0.0470083, recall 0.818228
2017-12-09T22:21:13.094366: step 891, loss 0.415305, acc 0.90625, prec 0.0470197, recall 0.818321
2017-12-09T22:21:13.399311: step 892, loss 0.21197, acc 0.921875, prec 0.0470059, recall 0.818321
2017-12-09T22:21:13.693513: step 893, loss 0.353515, acc 0.921875, prec 0.0470761, recall 0.8186
2017-12-09T22:21:13.993690: step 894, loss 3.25443, acc 0.875, prec 0.0470834, recall 0.818275
2017-12-09T22:21:14.293448: step 895, loss 3.34615, acc 0.882812, prec 0.0470934, recall 0.817533
2017-12-09T22:21:14.593940: step 896, loss 0.424415, acc 0.828125, prec 0.0471189, recall 0.817719
2017-12-09T22:21:14.892668: step 897, loss 0.2013, acc 0.914062, prec 0.0471316, recall 0.817812
2017-12-09T22:21:15.188092: step 898, loss 0.533978, acc 0.84375, prec 0.0471878, recall 0.818089
2017-12-09T22:21:15.486079: step 899, loss 0.508162, acc 0.804688, prec 0.0471811, recall 0.818182
2017-12-09T22:21:15.778409: step 900, loss 0.564104, acc 0.835938, prec 0.0472079, recall 0.818366

Evaluation:
2017-12-09T22:21:20.459830: step 900, loss 1.21807, acc 0.772809, prec 0.0476775, recall 0.819204

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-900

2017-12-09T22:21:21.774069: step 901, loss 1.11066, acc 0.726562, prec 0.0476839, recall 0.819373
2017-12-09T22:21:22.073055: step 902, loss 0.542276, acc 0.757812, prec 0.0477214, recall 0.819626
2017-12-09T22:21:22.371485: step 903, loss 0.807717, acc 0.734375, prec 0.0476773, recall 0.819626
2017-12-09T22:21:22.670218: step 904, loss 0.673844, acc 0.8125, prec 0.0477238, recall 0.819879
2017-12-09T22:21:22.976520: step 905, loss 0.516073, acc 0.835938, prec 0.0477742, recall 0.82013
2017-12-09T22:21:23.272470: step 906, loss 0.709311, acc 0.8125, prec 0.047898, recall 0.820632
2017-12-09T22:21:23.577985: step 907, loss 0.478054, acc 0.835938, prec 0.0478966, recall 0.820715
2017-12-09T22:21:23.879847: step 908, loss 0.43458, acc 0.796875, prec 0.0478886, recall 0.820799
2017-12-09T22:21:24.176969: step 909, loss 0.357098, acc 0.875, prec 0.0479194, recall 0.820965
2017-12-09T22:21:24.472023: step 910, loss 0.263534, acc 0.898438, prec 0.0479026, recall 0.820965
2017-12-09T22:21:24.772044: step 911, loss 0.345068, acc 0.851562, prec 0.047878, recall 0.820965
2017-12-09T22:21:25.075056: step 912, loss 1.87408, acc 0.921875, prec 0.0479435, recall 0.820833
2017-12-09T22:21:25.383241: step 913, loss 0.444784, acc 0.875, prec 0.0479485, recall 0.820916
2017-12-09T22:21:25.679731: step 914, loss 0.42517, acc 0.859375, prec 0.0479509, recall 0.820999
2017-12-09T22:21:25.978028: step 915, loss 0.661045, acc 0.84375, prec 0.0479508, recall 0.821082
2017-12-09T22:21:26.278889: step 916, loss 0.258329, acc 0.890625, prec 0.047984, recall 0.821247
2017-12-09T22:21:26.582305: step 917, loss 0.334428, acc 0.921875, prec 0.0479968, recall 0.82133
2017-12-09T22:21:26.885536: step 918, loss 1.23761, acc 0.90625, prec 0.0480595, recall 0.821198
2017-12-09T22:21:27.184584: step 919, loss 0.119778, acc 0.953125, prec 0.0480774, recall 0.821281
2017-12-09T22:21:27.483317: step 920, loss 0.445619, acc 0.914062, prec 0.0481145, recall 0.821445
2017-12-09T22:21:27.781409: step 921, loss 0.38792, acc 0.882812, prec 0.0481976, recall 0.821773
2017-12-09T22:21:28.084092: step 922, loss 0.666704, acc 0.859375, prec 0.0482255, recall 0.821937
2017-12-09T22:21:28.383576: step 923, loss 0.284897, acc 0.898438, prec 0.0482087, recall 0.821937
2017-12-09T22:21:28.681348: step 924, loss 1.12887, acc 0.890625, prec 0.0482686, recall 0.821805
2017-12-09T22:21:28.982936: step 925, loss 0.280061, acc 0.882812, prec 0.0482492, recall 0.821805
2017-12-09T22:21:29.280437: step 926, loss 0.262184, acc 0.914062, prec 0.0482349, recall 0.821805
2017-12-09T22:21:29.579629: step 927, loss 1.25182, acc 0.898438, prec 0.0482449, recall 0.82151
2017-12-09T22:21:29.884714: step 928, loss 0.422529, acc 0.875, prec 0.0482242, recall 0.82151
2017-12-09T22:21:30.191936: step 929, loss 0.491807, acc 0.859375, prec 0.048252, recall 0.821674
2017-12-09T22:21:31.198697: step 930, loss 0.479978, acc 0.8125, prec 0.0482464, recall 0.821755
2017-12-09T22:21:31.918073: step 931, loss 0.43669, acc 0.882812, prec 0.0483036, recall 0.821999
2017-12-09T22:21:32.509414: step 932, loss 0.495567, acc 0.828125, prec 0.0483261, recall 0.822161
2017-12-09T22:21:32.830492: step 933, loss 0.495809, acc 0.828125, prec 0.0483232, recall 0.822242
2017-12-09T22:21:33.169768: step 934, loss 0.55123, acc 0.789062, prec 0.0484156, recall 0.822647
2017-12-09T22:21:33.501332: step 935, loss 0.423768, acc 0.867188, prec 0.048419, recall 0.822727
2017-12-09T22:21:33.803413: step 936, loss 0.307553, acc 0.921875, prec 0.0484824, recall 0.822969
2017-12-09T22:21:34.110751: step 937, loss 0.430113, acc 0.875, prec 0.048538, recall 0.823209
2017-12-09T22:21:34.413037: step 938, loss 0.291781, acc 0.890625, prec 0.0485452, recall 0.82329
2017-12-09T22:21:34.711694: step 939, loss 0.292541, acc 0.90625, prec 0.0485551, recall 0.82337
2017-12-09T22:21:35.009063: step 940, loss 0.448831, acc 0.9375, prec 0.0485701, recall 0.82345
2017-12-09T22:21:35.322270: step 941, loss 0.224854, acc 0.90625, prec 0.04858, recall 0.823529
2017-12-09T22:21:35.620831: step 942, loss 0.231866, acc 0.914062, prec 0.0485657, recall 0.823529
2017-12-09T22:21:35.922131: step 943, loss 0.906288, acc 0.945312, prec 0.0485833, recall 0.823237
2017-12-09T22:21:36.222699: step 944, loss 2.9264, acc 0.914062, prec 0.0486465, recall 0.823105
2017-12-09T22:21:36.525629: step 945, loss 0.142172, acc 0.945312, prec 0.0487389, recall 0.823423
2017-12-09T22:21:36.829906: step 946, loss 0.286086, acc 0.875, prec 0.0487181, recall 0.823423
2017-12-09T22:21:37.129839: step 947, loss 0.241038, acc 0.898438, prec 0.0487266, recall 0.823503
2017-12-09T22:21:37.431128: step 948, loss 0.125517, acc 0.960938, prec 0.0487707, recall 0.823662
2017-12-09T22:21:37.732706: step 949, loss 0.304568, acc 0.882812, prec 0.0487513, recall 0.823662
2017-12-09T22:21:38.034004: step 950, loss 0.290841, acc 0.914062, prec 0.0487623, recall 0.823741
2017-12-09T22:21:38.330881: step 951, loss 0.651359, acc 0.898438, prec 0.0488467, recall 0.824057
2017-12-09T22:21:38.629011: step 952, loss 0.238976, acc 0.921875, prec 0.0488843, recall 0.824215
2017-12-09T22:21:38.928643: step 953, loss 0.439293, acc 0.875, prec 0.0489646, recall 0.82453
2017-12-09T22:21:39.232633: step 954, loss 0.423167, acc 0.890625, prec 0.0489969, recall 0.824687
2017-12-09T22:21:39.535808: step 955, loss 0.620007, acc 0.859375, prec 0.0490493, recall 0.824922
2017-12-09T22:21:39.839280: step 956, loss 0.266668, acc 0.890625, prec 0.0490815, recall 0.825078
2017-12-09T22:21:40.142890: step 957, loss 0.884189, acc 0.851562, prec 0.0491325, recall 0.825312
2017-12-09T22:21:40.447016: step 958, loss 0.395001, acc 0.859375, prec 0.0491847, recall 0.825545
2017-12-09T22:21:40.742266: step 959, loss 0.789135, acc 0.90625, prec 0.0492194, recall 0.8257
2017-12-09T22:21:41.047053: step 960, loss 0.326269, acc 0.90625, prec 0.0492794, recall 0.825933
2017-12-09T22:21:41.341773: step 961, loss 0.698786, acc 0.859375, prec 0.0493314, recall 0.826164
2017-12-09T22:21:41.644625: step 962, loss 0.420013, acc 0.898438, prec 0.0493396, recall 0.826241
2017-12-09T22:21:41.945635: step 963, loss 0.643073, acc 0.796875, prec 0.0493308, recall 0.826318
2017-12-09T22:21:42.239985: step 964, loss 0.439251, acc 0.84375, prec 0.0493298, recall 0.826395
2017-12-09T22:21:42.544389: step 965, loss 0.399762, acc 0.914062, prec 0.0493658, recall 0.826549
2017-12-09T22:21:42.841735: step 966, loss 0.431732, acc 0.84375, prec 0.0493648, recall 0.826625
2017-12-09T22:21:43.138627: step 967, loss 0.449818, acc 0.882812, prec 0.0493954, recall 0.826779
2017-12-09T22:21:43.438073: step 968, loss 0.291723, acc 0.914062, prec 0.0493811, recall 0.826779
2017-12-09T22:21:43.739997: step 969, loss 0.33648, acc 0.882812, prec 0.0494117, recall 0.826932
2017-12-09T22:21:44.040268: step 970, loss 0.247736, acc 0.898438, prec 0.0494449, recall 0.827084
2017-12-09T22:21:44.336262: step 971, loss 0.316917, acc 0.859375, prec 0.0494465, recall 0.82716
2017-12-09T22:21:44.638069: step 972, loss 0.841591, acc 0.929688, prec 0.0494611, recall 0.826872
2017-12-09T22:21:44.938854: step 973, loss 0.554763, acc 0.90625, prec 0.0494705, recall 0.826948
2017-12-09T22:21:45.237452: step 974, loss 0.314114, acc 0.921875, prec 0.0494575, recall 0.826948
2017-12-09T22:21:45.544209: step 975, loss 0.396516, acc 0.9375, prec 0.0494971, recall 0.827101
2017-12-09T22:21:45.842438: step 976, loss 0.662041, acc 0.90625, prec 0.0495065, recall 0.827177
2017-12-09T22:21:46.143076: step 977, loss 1.0995, acc 0.890625, prec 0.0495146, recall 0.826889
2017-12-09T22:21:46.443869: step 978, loss 0.435383, acc 0.90625, prec 0.049549, recall 0.827041
2017-12-09T22:21:46.740535: step 979, loss 0.769963, acc 0.9375, prec 0.0495398, recall 0.826678
2017-12-09T22:21:47.048103: step 980, loss 0.459603, acc 0.890625, prec 0.0495466, recall 0.826754
2017-12-09T22:21:47.348727: step 981, loss 2.34446, acc 0.898438, prec 0.0496059, recall 0.82662
2017-12-09T22:21:47.648311: step 982, loss 0.854029, acc 0.898438, prec 0.0496402, recall 0.82641
2017-12-09T22:21:47.946981: step 983, loss 0.563424, acc 0.804688, prec 0.0496076, recall 0.82641
2017-12-09T22:21:48.247679: step 984, loss 0.555406, acc 0.835938, prec 0.0496551, recall 0.826638
2017-12-09T22:21:48.547955: step 985, loss 0.617092, acc 0.789062, prec 0.0496448, recall 0.826713
2017-12-09T22:21:48.846225: step 986, loss 0.655975, acc 0.765625, prec 0.0496058, recall 0.826713
2017-12-09T22:21:49.142679: step 987, loss 0.627107, acc 0.765625, prec 0.0496166, recall 0.826864
2017-12-09T22:21:49.442835: step 988, loss 0.69417, acc 0.828125, prec 0.0496626, recall 0.827091
2017-12-09T22:21:49.739059: step 989, loss 0.556813, acc 0.828125, prec 0.0496838, recall 0.827241
2017-12-09T22:21:50.035947: step 990, loss 0.586003, acc 0.796875, prec 0.0496997, recall 0.827391
2017-12-09T22:21:50.347137: step 991, loss 0.869833, acc 0.78125, prec 0.0497129, recall 0.827541
2017-12-09T22:21:50.648774: step 992, loss 0.410245, acc 0.835938, prec 0.0497105, recall 0.827616
2017-12-09T22:21:50.946782: step 993, loss 0.508603, acc 0.789062, prec 0.0496755, recall 0.827616
2017-12-09T22:21:51.245923: step 994, loss 1.89347, acc 0.84375, prec 0.0496509, recall 0.827257
2017-12-09T22:21:51.550399: step 995, loss 0.400457, acc 0.859375, prec 0.0496772, recall 0.827407
2017-12-09T22:21:51.729240: step 996, loss 0.624104, acc 0.882353, prec 0.0497189, recall 0.827556
2017-12-09T22:21:52.036351: step 997, loss 0.418444, acc 0.796875, prec 0.04971, recall 0.827631
2017-12-09T22:21:52.337192: step 998, loss 0.481399, acc 0.875, prec 0.049714, recall 0.827706
2017-12-09T22:21:52.638005: step 999, loss 0.442347, acc 0.859375, prec 0.0497401, recall 0.827855
2017-12-09T22:21:52.936364: step 1000, loss 0.519171, acc 0.859375, prec 0.0497169, recall 0.827855
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1000

2017-12-09T22:21:54.454737: step 1001, loss 0.428611, acc 0.875, prec 0.0497209, recall 0.827929
2017-12-09T22:21:54.749625: step 1002, loss 0.238374, acc 0.921875, prec 0.049708, recall 0.827929
2017-12-09T22:21:55.048441: step 1003, loss 0.727909, acc 0.882812, prec 0.0497379, recall 0.828078
2017-12-09T22:21:55.350004: step 1004, loss 0.366743, acc 0.921875, prec 0.0497743, recall 0.828226
2017-12-09T22:21:55.651242: step 1005, loss 0.204994, acc 0.921875, prec 0.0497614, recall 0.828226
2017-12-09T22:21:55.952628: step 1006, loss 0.299158, acc 0.953125, prec 0.0497783, recall 0.8283
2017-12-09T22:21:56.251249: step 1007, loss 0.384373, acc 0.90625, prec 0.0498614, recall 0.828596
2017-12-09T22:21:56.546152: step 1008, loss 0.216078, acc 0.9375, prec 0.0498756, recall 0.82867
2017-12-09T22:21:56.845745: step 1009, loss 0.197608, acc 0.914062, prec 0.049886, recall 0.828744
2017-12-09T22:21:57.141392: step 1010, loss 0.344023, acc 0.976562, prec 0.0499314, recall 0.828891
2017-12-09T22:21:57.438798: step 1011, loss 0.205623, acc 0.929688, prec 0.0499443, recall 0.828964
2017-12-09T22:21:57.738248: step 1012, loss 0.540131, acc 0.953125, prec 0.0499612, recall 0.829038
2017-12-09T22:21:58.039883: step 1013, loss 1.35407, acc 0.9375, prec 0.0499767, recall 0.828755
2017-12-09T22:21:58.341100: step 1014, loss 0.209925, acc 0.921875, prec 0.0500375, recall 0.828976
2017-12-09T22:21:58.643080: step 1015, loss 0.146362, acc 0.9375, prec 0.0500517, recall 0.829049
2017-12-09T22:21:58.941549: step 1016, loss 0.266099, acc 0.945312, prec 0.0500918, recall 0.829195
2017-12-09T22:21:59.240402: step 1017, loss 0.254396, acc 0.953125, prec 0.0501332, recall 0.829341
2017-12-09T22:21:59.539079: step 1018, loss 0.40671, acc 0.96875, prec 0.0502016, recall 0.82956
2017-12-09T22:21:59.841192: step 1019, loss 0.551301, acc 0.890625, prec 0.050208, recall 0.829633
2017-12-09T22:22:00.166323: step 1020, loss 0.157143, acc 0.945312, prec 0.0502235, recall 0.829705
2017-12-09T22:22:00.466733: step 1021, loss 0.152266, acc 0.929688, prec 0.0502118, recall 0.829705
2017-12-09T22:22:00.765952: step 1022, loss 0.2852, acc 0.921875, prec 0.0502724, recall 0.829923
2017-12-09T22:22:01.069808: step 1023, loss 0.192665, acc 0.945312, prec 0.0502878, recall 0.829996
2017-12-09T22:22:01.367514: step 1024, loss 0.314662, acc 0.898438, prec 0.0502955, recall 0.830068
2017-12-09T22:22:01.667848: step 1025, loss 0.547652, acc 0.929688, prec 0.0503083, recall 0.83014
2017-12-09T22:22:01.967350: step 1026, loss 0.559515, acc 0.890625, prec 0.0503391, recall 0.830285
2017-12-09T22:22:02.267849: step 1027, loss 0.413865, acc 0.890625, prec 0.0503209, recall 0.830285
2017-12-09T22:22:02.570558: step 1028, loss 1.08085, acc 0.898438, prec 0.0503054, recall 0.829932
2017-12-09T22:22:02.882664: step 1029, loss 0.291595, acc 0.875, prec 0.050358, recall 0.830149
2017-12-09T22:22:03.179078: step 1030, loss 0.367012, acc 0.90625, prec 0.0503914, recall 0.830293
2017-12-09T22:22:03.479018: step 1031, loss 0.245494, acc 0.921875, prec 0.0503784, recall 0.830293
2017-12-09T22:22:03.782705: step 1032, loss 0.545825, acc 0.875, prec 0.0504065, recall 0.830437
2017-12-09T22:22:04.081158: step 1033, loss 0.301745, acc 0.890625, prec 0.0504372, recall 0.83058
2017-12-09T22:22:04.378977: step 1034, loss 0.709119, acc 0.898438, prec 0.0504936, recall 0.830795
2017-12-09T22:22:04.681870: step 1035, loss 0.242239, acc 0.890625, prec 0.0504755, recall 0.830795
2017-12-09T22:22:04.980746: step 1036, loss 0.227262, acc 0.914062, prec 0.0504612, recall 0.830795
2017-12-09T22:22:05.299652: step 1037, loss 0.384245, acc 0.867188, prec 0.0505123, recall 0.83101
2017-12-09T22:22:05.598064: step 1038, loss 0.673688, acc 0.929688, prec 0.050525, recall 0.831081
2017-12-09T22:22:05.897370: step 1039, loss 0.499032, acc 0.898438, prec 0.0505325, recall 0.831152
2017-12-09T22:22:06.198979: step 1040, loss 0.322945, acc 0.890625, prec 0.0505387, recall 0.831224
2017-12-09T22:22:07.209442: step 1041, loss 0.268184, acc 0.921875, prec 0.0505745, recall 0.831366
2017-12-09T22:22:07.611172: step 1042, loss 0.444602, acc 0.898438, prec 0.0506063, recall 0.831508
2017-12-09T22:22:07.911190: step 1043, loss 0.535098, acc 0.921875, prec 0.0506663, recall 0.831721
2017-12-09T22:22:08.822338: step 1044, loss 0.259575, acc 0.890625, prec 0.0506725, recall 0.831791
2017-12-09T22:22:09.220068: step 1045, loss 0.384309, acc 0.859375, prec 0.050722, recall 0.832003
2017-12-09T22:22:09.522563: step 1046, loss 0.622413, acc 0.9375, prec 0.050906, recall 0.832566
2017-12-09T22:22:09.819189: step 1047, loss 0.3665, acc 0.882812, prec 0.0509593, recall 0.832776
2017-12-09T22:22:10.119517: step 1048, loss 0.319439, acc 0.859375, prec 0.0509601, recall 0.832846
2017-12-09T22:22:10.423020: step 1049, loss 0.299245, acc 0.890625, prec 0.0509419, recall 0.832846
2017-12-09T22:22:10.731592: step 1050, loss 0.32787, acc 0.882812, prec 0.0509951, recall 0.833055
2017-12-09T22:22:11.034545: step 1051, loss 0.883335, acc 0.953125, prec 0.05106, recall 0.833264
2017-12-09T22:22:11.335668: step 1052, loss 0.352391, acc 0.882812, prec 0.0510889, recall 0.833403
2017-12-09T22:22:11.634177: step 1053, loss 0.256978, acc 0.921875, prec 0.0511243, recall 0.833541
2017-12-09T22:22:11.928476: step 1054, loss 0.545284, acc 0.90625, prec 0.0511571, recall 0.83368
2017-12-09T22:22:12.227692: step 1055, loss 0.200917, acc 0.90625, prec 0.0511656, recall 0.833749
2017-12-09T22:22:12.528740: step 1056, loss 0.175599, acc 0.953125, prec 0.0512788, recall 0.834094
2017-12-09T22:22:12.828040: step 1057, loss 0.46009, acc 0.867188, prec 0.0512807, recall 0.834163
2017-12-09T22:22:13.125377: step 1058, loss 0.184496, acc 0.960938, prec 0.0512984, recall 0.834231
2017-12-09T22:22:13.423679: step 1059, loss 0.266102, acc 0.90625, prec 0.0513069, recall 0.8343
2017-12-09T22:22:13.722891: step 1060, loss 0.318472, acc 0.898438, prec 0.0513624, recall 0.834506
2017-12-09T22:22:14.023547: step 1061, loss 0.816452, acc 0.898438, prec 0.0514178, recall 0.834711
2017-12-09T22:22:14.319789: step 1062, loss 0.760962, acc 0.867188, prec 0.0514921, recall 0.834984
2017-12-09T22:22:14.626583: step 1063, loss 0.241613, acc 0.890625, prec 0.0514979, recall 0.835052
2017-12-09T22:22:14.924791: step 1064, loss 0.360882, acc 0.890625, prec 0.0515037, recall 0.83512
2017-12-09T22:22:15.223126: step 1065, loss 0.173827, acc 0.914062, prec 0.0514893, recall 0.83512
2017-12-09T22:22:15.523030: step 1066, loss 0.259408, acc 0.890625, prec 0.0514951, recall 0.835187
2017-12-09T22:22:15.823329: step 1067, loss 0.235089, acc 0.945312, prec 0.0515341, recall 0.835323
2017-12-09T22:22:16.124247: step 1068, loss 0.319566, acc 0.921875, prec 0.0515932, recall 0.835526
2017-12-09T22:22:16.422312: step 1069, loss 0.238217, acc 0.90625, prec 0.0515775, recall 0.835526
2017-12-09T22:22:16.719229: step 1070, loss 0.251609, acc 0.929688, prec 0.0516139, recall 0.835661
2017-12-09T22:22:17.020456: step 1071, loss 0.343012, acc 0.921875, prec 0.0516248, recall 0.835729
2017-12-09T22:22:17.314786: step 1072, loss 0.263437, acc 0.914062, prec 0.0516104, recall 0.835729
2017-12-09T22:22:17.619876: step 1073, loss 0.131379, acc 0.960938, prec 0.0516039, recall 0.835729
2017-12-09T22:22:17.921666: step 1074, loss 0.24113, acc 0.914062, prec 0.0516136, recall 0.835796
2017-12-09T22:22:18.219464: step 1075, loss 0.513713, acc 0.945312, prec 0.0516284, recall 0.835864
2017-12-09T22:22:18.520129: step 1076, loss 0.692206, acc 0.945312, prec 0.0516673, recall 0.835998
2017-12-09T22:22:18.820081: step 1077, loss 0.216086, acc 0.953125, prec 0.0516835, recall 0.836066
2017-12-09T22:22:19.123802: step 1078, loss 0.369957, acc 0.96875, prec 0.0517263, recall 0.8362
2017-12-09T22:22:19.423234: step 1079, loss 1.56229, acc 0.929688, prec 0.0517652, recall 0.83565
2017-12-09T22:22:19.726060: step 1080, loss 0.206953, acc 0.953125, prec 0.0517813, recall 0.835717
2017-12-09T22:22:20.032460: step 1081, loss 0.270837, acc 0.90625, prec 0.0517896, recall 0.835784
2017-12-09T22:22:20.341517: step 1082, loss 0.181003, acc 0.9375, prec 0.0518031, recall 0.835851
2017-12-09T22:22:20.640603: step 1083, loss 0.217985, acc 0.914062, prec 0.0518607, recall 0.836052
2017-12-09T22:22:20.938736: step 1084, loss 0.366895, acc 0.890625, prec 0.0519142, recall 0.836253
2017-12-09T22:22:21.233044: step 1085, loss 0.422123, acc 0.867188, prec 0.0519878, recall 0.836519
2017-12-09T22:22:21.535133: step 1086, loss 0.246446, acc 0.90625, prec 0.051996, recall 0.836585
2017-12-09T22:22:21.832921: step 1087, loss 0.247816, acc 0.890625, prec 0.0519776, recall 0.836585
2017-12-09T22:22:22.135619: step 1088, loss 0.215443, acc 0.890625, prec 0.0520071, recall 0.836718
2017-12-09T22:22:22.432616: step 1089, loss 1.40362, acc 0.875, prec 0.0520352, recall 0.836511
2017-12-09T22:22:22.735294: step 1090, loss 0.374554, acc 0.890625, prec 0.0521125, recall 0.836776
2017-12-09T22:22:23.032532: step 1091, loss 0.330252, acc 0.890625, prec 0.052118, recall 0.836842
2017-12-09T22:22:23.336048: step 1092, loss 0.877915, acc 0.882812, prec 0.05217, recall 0.83704
2017-12-09T22:22:23.635799: step 1093, loss 0.496959, acc 0.867188, prec 0.0521715, recall 0.837106
2017-12-09T22:22:23.934595: step 1094, loss 0.482873, acc 0.835938, prec 0.0521916, recall 0.837237
2017-12-09T22:22:24.232212: step 1095, loss 0.31342, acc 0.867188, prec 0.0521932, recall 0.837303
2017-12-09T22:22:24.532259: step 1096, loss 0.567123, acc 0.875, prec 0.0522198, recall 0.837434
2017-12-09T22:22:24.831595: step 1097, loss 0.611066, acc 0.914062, prec 0.0522292, recall 0.8375
2017-12-09T22:22:25.131270: step 1098, loss 0.276623, acc 0.898438, prec 0.0522598, recall 0.837631
2017-12-09T22:22:25.429796: step 1099, loss 0.502649, acc 0.851562, prec 0.0522825, recall 0.837762
2017-12-09T22:22:25.733551: step 1100, loss 0.373099, acc 0.84375, prec 0.0522562, recall 0.837762
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1100

2017-12-09T22:22:27.177123: step 1101, loss 0.624008, acc 0.8125, prec 0.0522961, recall 0.837957
2017-12-09T22:22:27.477201: step 1102, loss 0.363244, acc 0.882812, prec 0.0523002, recall 0.838023
2017-12-09T22:22:27.773934: step 1103, loss 0.486677, acc 0.875, prec 0.0523267, recall 0.838153
2017-12-09T22:22:28.078948: step 1104, loss 0.276068, acc 0.898438, prec 0.0523335, recall 0.838218
2017-12-09T22:22:28.383806: step 1105, loss 0.49183, acc 0.867188, prec 0.0523824, recall 0.838412
2017-12-09T22:22:28.685452: step 1106, loss 0.319266, acc 0.921875, prec 0.0524167, recall 0.838542
2017-12-09T22:22:28.984279: step 1107, loss 0.272144, acc 0.890625, prec 0.0524695, recall 0.838736
2017-12-09T22:22:29.287931: step 1108, loss 0.214162, acc 0.929688, prec 0.0525051, recall 0.838864
2017-12-09T22:22:29.587795: step 1109, loss 0.138053, acc 0.960938, prec 0.0525223, recall 0.838929
2017-12-09T22:22:29.889243: step 1110, loss 0.497238, acc 0.976562, prec 0.0526368, recall 0.83925
2017-12-09T22:22:30.195916: step 1111, loss 0.501838, acc 0.945312, prec 0.0526987, recall 0.839442
2017-12-09T22:22:30.500027: step 1112, loss 1.37006, acc 0.929688, prec 0.0527592, recall 0.8393
2017-12-09T22:22:30.799249: step 1113, loss 0.138725, acc 0.953125, prec 0.0527987, recall 0.839428
2017-12-09T22:22:31.099636: step 1114, loss 0.855227, acc 0.9375, prec 0.0528368, recall 0.839222
2017-12-09T22:22:31.394462: step 1115, loss 0.318468, acc 0.921875, prec 0.0528709, recall 0.839349
2017-12-09T22:22:31.692904: step 1116, loss 0.474416, acc 0.882812, prec 0.0528984, recall 0.839477
2017-12-09T22:22:31.991806: step 1117, loss 0.347428, acc 0.890625, prec 0.0529272, recall 0.839604
2017-12-09T22:22:32.288654: step 1118, loss 0.309871, acc 0.890625, prec 0.0529324, recall 0.839667
2017-12-09T22:22:32.588005: step 1119, loss 0.691723, acc 0.90625, prec 0.0529638, recall 0.839794
2017-12-09T22:22:32.888019: step 1120, loss 0.358259, acc 0.867188, prec 0.0530358, recall 0.840047
2017-12-09T22:22:33.190641: step 1121, loss 0.510806, acc 0.804688, prec 0.0530263, recall 0.840111
2017-12-09T22:22:33.491221: step 1122, loss 0.381248, acc 0.867188, prec 0.0530039, recall 0.840111
2017-12-09T22:22:33.792898: step 1123, loss 0.317002, acc 0.867188, prec 0.053005, recall 0.840174
2017-12-09T22:22:34.091145: step 1124, loss 0.602556, acc 0.8125, prec 0.0530912, recall 0.840488
2017-12-09T22:22:34.389483: step 1125, loss 0.405063, acc 0.898438, prec 0.0530976, recall 0.840551
2017-12-09T22:22:34.690454: step 1126, loss 0.456834, acc 0.851562, prec 0.0531196, recall 0.840677
2017-12-09T22:22:34.989142: step 1127, loss 0.42888, acc 0.867188, prec 0.0531207, recall 0.840739
2017-12-09T22:22:35.298058: step 1128, loss 1.07675, acc 0.898438, prec 0.0532224, recall 0.840722
2017-12-09T22:22:35.596947: step 1129, loss 0.184013, acc 0.914062, prec 0.0532314, recall 0.840784
2017-12-09T22:22:35.897030: step 1130, loss 0.533397, acc 0.882812, prec 0.0532585, recall 0.840909
2017-12-09T22:22:36.202380: step 1131, loss 0.140745, acc 0.945312, prec 0.0532493, recall 0.840909
2017-12-09T22:22:36.498266: step 1132, loss 0.237747, acc 0.90625, prec 0.0532804, recall 0.841034
2017-12-09T22:22:36.801407: step 1133, loss 0.136615, acc 0.953125, prec 0.0533194, recall 0.841158
2017-12-09T22:22:37.100486: step 1134, loss 0.187724, acc 0.9375, prec 0.0533089, recall 0.841158
2017-12-09T22:22:37.397250: step 1135, loss 0.19708, acc 0.929688, prec 0.0533439, recall 0.841282
2017-12-09T22:22:37.693760: step 1136, loss 0.188057, acc 0.914062, prec 0.0533294, recall 0.841282
2017-12-09T22:22:37.998921: step 1137, loss 0.144143, acc 0.945312, prec 0.053367, recall 0.841406
2017-12-09T22:22:38.298675: step 1138, loss 0.476283, acc 0.921875, prec 0.053471, recall 0.841715
2017-12-09T22:22:38.605174: step 1139, loss 0.628033, acc 0.960938, prec 0.0534657, recall 0.841387
2017-12-09T22:22:38.906480: step 1140, loss 0.13859, acc 0.921875, prec 0.0534525, recall 0.841387
2017-12-09T22:22:39.206460: step 1141, loss 0.270391, acc 0.921875, prec 0.0534861, recall 0.841511
2017-12-09T22:22:39.509907: step 1142, loss 0.200417, acc 0.953125, prec 0.0535016, recall 0.841573
2017-12-09T22:22:39.809347: step 1143, loss 0.176353, acc 0.921875, prec 0.0534884, recall 0.841573
2017-12-09T22:22:40.108340: step 1144, loss 0.171518, acc 0.953125, prec 0.0535038, recall 0.841634
2017-12-09T22:22:40.405397: step 1145, loss 0.430126, acc 0.96875, prec 0.0536156, recall 0.841942
2017-12-09T22:22:40.711200: step 1146, loss 0.756309, acc 0.921875, prec 0.0537895, recall 0.842431
2017-12-09T22:22:41.019229: step 1147, loss 0.161433, acc 0.96875, prec 0.0538309, recall 0.842553
2017-12-09T22:22:41.318258: step 1148, loss 2.4162, acc 0.898438, prec 0.053815, recall 0.842227
2017-12-09T22:22:41.625994: step 1149, loss 0.415726, acc 0.890625, prec 0.0538431, recall 0.842349
2017-12-09T22:22:41.932375: step 1150, loss 0.309516, acc 0.859375, prec 0.0538893, recall 0.842532
2017-12-09T22:22:42.229777: step 1151, loss 0.245808, acc 0.882812, prec 0.0538927, recall 0.842593
2017-12-09T22:22:42.530303: step 1152, loss 0.236282, acc 0.921875, prec 0.0539027, recall 0.842653
2017-12-09T22:22:42.823800: step 1153, loss 0.364323, acc 0.898438, prec 0.0539088, recall 0.842714
2017-12-09T22:22:43.121529: step 1154, loss 0.599328, acc 0.898438, prec 0.0539381, recall 0.842835
2017-12-09T22:22:43.431921: step 1155, loss 0.313304, acc 0.875, prec 0.0539635, recall 0.842956
2017-12-09T22:22:43.729831: step 1156, loss 1.09646, acc 0.882812, prec 0.0540148, recall 0.842813
2017-12-09T22:22:44.027349: step 1157, loss 0.582024, acc 0.773438, prec 0.0539995, recall 0.842874
2017-12-09T22:22:44.327709: step 1158, loss 0.40665, acc 0.859375, prec 0.0540454, recall 0.843054
2017-12-09T22:22:44.634311: step 1159, loss 0.575313, acc 0.84375, prec 0.0540654, recall 0.843175
2017-12-09T22:22:44.931975: step 1160, loss 0.452934, acc 0.859375, prec 0.0541112, recall 0.843355
2017-12-09T22:22:45.228230: step 1161, loss 0.56827, acc 0.789062, prec 0.0541682, recall 0.843595
2017-12-09T22:22:45.530371: step 1162, loss 0.358281, acc 0.835938, prec 0.0541635, recall 0.843654
2017-12-09T22:22:45.829290: step 1163, loss 0.346911, acc 0.875, prec 0.0542118, recall 0.843834
2017-12-09T22:22:46.132177: step 1164, loss 0.313174, acc 0.867188, prec 0.0542588, recall 0.844012
2017-12-09T22:22:46.429024: step 1165, loss 0.277842, acc 0.898438, prec 0.0542415, recall 0.844012
2017-12-09T22:22:46.726765: step 1166, loss 0.909631, acc 0.875, prec 0.0542434, recall 0.844072
2017-12-09T22:22:47.021359: step 1167, loss 0.266037, acc 0.890625, prec 0.0542943, recall 0.84425
2017-12-09T22:22:47.319464: step 1168, loss 0.462462, acc 0.90625, prec 0.054371, recall 0.844487
2017-12-09T22:22:47.617604: step 1169, loss 0.39611, acc 0.898438, prec 0.0544231, recall 0.844664
2017-12-09T22:22:47.920157: step 1170, loss 0.341121, acc 0.898438, prec 0.0544521, recall 0.844782
2017-12-09T22:22:48.219459: step 1171, loss 0.386893, acc 0.929688, prec 0.0545094, recall 0.844958
2017-12-09T22:22:48.517344: step 1172, loss 0.307754, acc 0.898438, prec 0.0545152, recall 0.845017
2017-12-09T22:22:48.815240: step 1173, loss 0.298391, acc 0.929688, prec 0.0545263, recall 0.845076
2017-12-09T22:22:49.113029: step 1174, loss 0.437899, acc 0.945312, prec 0.0546094, recall 0.84531
2017-12-09T22:22:49.411350: step 1175, loss 0.215146, acc 0.921875, prec 0.0546192, recall 0.845369
2017-12-09T22:22:49.713034: step 1176, loss 0.296323, acc 0.890625, prec 0.0546005, recall 0.845369
2017-12-09T22:22:50.027803: step 1177, loss 0.196229, acc 0.945312, prec 0.0546373, recall 0.845485
2017-12-09T22:22:50.339744: step 1178, loss 0.268096, acc 0.921875, prec 0.0546471, recall 0.845544
2017-12-09T22:22:50.640315: step 1179, loss 0.264518, acc 0.945312, prec 0.05473, recall 0.845777
2017-12-09T22:22:50.940797: step 1180, loss 0.343926, acc 0.9375, prec 0.0548116, recall 0.846009
2017-12-09T22:22:51.239522: step 1181, loss 3.2616, acc 0.945312, prec 0.054851, recall 0.845489
2017-12-09T22:22:51.537635: step 1182, loss 0.168522, acc 0.929688, prec 0.0548389, recall 0.845489
2017-12-09T22:22:51.837113: step 1183, loss 0.720683, acc 0.898438, prec 0.0548907, recall 0.845663
2017-12-09T22:22:52.139997: step 1184, loss 0.314675, acc 0.875, prec 0.0548693, recall 0.845663
2017-12-09T22:22:52.448728: step 1185, loss 0.308064, acc 0.890625, prec 0.0548736, recall 0.845721
2017-12-09T22:22:52.745745: step 1186, loss 0.635493, acc 0.875, prec 0.0548752, recall 0.845779
2017-12-09T22:22:53.042130: step 1187, loss 0.351299, acc 0.859375, prec 0.0548742, recall 0.845836
2017-12-09T22:22:53.341092: step 1188, loss 0.485679, acc 0.890625, prec 0.0549015, recall 0.845952
2017-12-09T22:22:53.635909: step 1189, loss 0.736892, acc 0.796875, prec 0.0549357, recall 0.846125
2017-12-09T22:22:53.935049: step 1190, loss 0.390248, acc 0.867188, prec 0.0549589, recall 0.84624
2017-12-09T22:22:54.234580: step 1191, loss 0.522172, acc 0.828125, prec 0.0549755, recall 0.846355
2017-12-09T22:22:54.538029: step 1192, loss 0.800804, acc 0.773438, prec 0.0550056, recall 0.846527
2017-12-09T22:22:54.838572: step 1193, loss 0.462015, acc 0.835938, prec 0.0550921, recall 0.846813
2017-12-09T22:22:55.141548: step 1194, loss 0.378965, acc 0.859375, prec 0.055091, recall 0.84687
2017-12-09T22:22:55.437675: step 1195, loss 0.512426, acc 0.835938, prec 0.0550859, recall 0.846927
2017-12-09T22:22:55.742161: step 1196, loss 0.40802, acc 0.84375, prec 0.055105, recall 0.847041
2017-12-09T22:22:56.038075: step 1197, loss 0.272039, acc 0.914062, prec 0.0551132, recall 0.847098
2017-12-09T22:22:56.337936: step 1198, loss 0.333493, acc 0.875, prec 0.0551147, recall 0.847155
2017-12-09T22:22:56.636368: step 1199, loss 0.210258, acc 0.921875, prec 0.0551013, recall 0.847155
2017-12-09T22:22:56.937101: step 1200, loss 0.577616, acc 0.859375, prec 0.0551459, recall 0.847325

Evaluation:
2017-12-09T22:23:01.713147: step 1200, loss 2.13729, acc 0.938673, prec 0.0561177, recall 0.82528

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1200

2017-12-09T22:23:03.174144: step 1201, loss 0.142791, acc 0.960938, prec 0.0561335, recall 0.825341
2017-12-09T22:23:03.475740: step 1202, loss 0.074306, acc 0.976562, prec 0.0561295, recall 0.825341
2017-12-09T22:23:03.779480: step 1203, loss 0.154709, acc 0.953125, prec 0.0561439, recall 0.825402
2017-12-09T22:23:04.079429: step 1204, loss 0.150856, acc 0.945312, prec 0.056157, recall 0.825463
2017-12-09T22:23:04.375550: step 1205, loss 0.576521, acc 0.953125, prec 0.0562388, recall 0.825707
2017-12-09T22:23:04.679553: step 1206, loss 0.474013, acc 0.992188, prec 0.0563049, recall 0.82589
2017-12-09T22:23:04.980150: step 1207, loss 0.0680038, acc 0.976562, prec 0.0563233, recall 0.82595
2017-12-09T22:23:05.283803: step 1208, loss 0.581885, acc 0.96875, prec 0.0564077, recall 0.826193
2017-12-09T22:23:05.587952: step 1209, loss 0.361157, acc 0.945312, prec 0.0564432, recall 0.826314
2017-12-09T22:23:05.888534: step 1210, loss 0.487788, acc 0.960938, prec 0.0565262, recall 0.826555
2017-12-09T22:23:06.188602: step 1211, loss 0.144874, acc 0.9375, prec 0.0565154, recall 0.826555
2017-12-09T22:23:06.488035: step 1212, loss 0.151877, acc 0.960938, prec 0.0565536, recall 0.826676
2017-12-09T22:23:06.783320: step 1213, loss 0.252752, acc 0.953125, prec 0.0566127, recall 0.826856
2017-12-09T22:23:07.081595: step 1214, loss 0.256741, acc 0.898438, prec 0.0565953, recall 0.826856
2017-12-09T22:23:07.386280: step 1215, loss 0.158525, acc 0.945312, prec 0.0565859, recall 0.826856
2017-12-09T22:23:07.687931: step 1216, loss 2.55846, acc 0.921875, prec 0.0565962, recall 0.82663
2017-12-09T22:23:07.990305: step 1217, loss 0.468456, acc 0.945312, prec 0.0566091, recall 0.82669
2017-12-09T22:23:08.292347: step 1218, loss 0.391492, acc 0.914062, prec 0.0566391, recall 0.82681
2017-12-09T22:23:08.597943: step 1219, loss 0.299023, acc 0.90625, prec 0.0566678, recall 0.82693
2017-12-09T22:23:08.896392: step 1220, loss 0.349509, acc 0.851562, prec 0.0566422, recall 0.82693
2017-12-09T22:23:09.193879: step 1221, loss 0.496735, acc 0.804688, prec 0.056631, recall 0.82699
2017-12-09T22:23:09.493042: step 1222, loss 0.543538, acc 0.835938, prec 0.0566476, recall 0.827109
2017-12-09T22:23:09.793606: step 1223, loss 0.496542, acc 0.851562, prec 0.0566891, recall 0.827288
2017-12-09T22:23:10.093462: step 1224, loss 0.39462, acc 0.867188, prec 0.0566886, recall 0.827348
2017-12-09T22:23:10.389638: step 1225, loss 0.552109, acc 0.8125, prec 0.0567233, recall 0.827527
2017-12-09T22:23:10.689162: step 1226, loss 0.421409, acc 0.828125, prec 0.0566938, recall 0.827527
2017-12-09T22:23:10.988140: step 1227, loss 0.360085, acc 0.882812, prec 0.056696, recall 0.827586
2017-12-09T22:23:11.292593: step 1228, loss 0.330544, acc 0.914062, prec 0.0567036, recall 0.827646
2017-12-09T22:23:11.593502: step 1229, loss 0.226471, acc 0.914062, prec 0.0567111, recall 0.827705
2017-12-09T22:23:11.890831: step 1230, loss 0.39381, acc 0.898438, prec 0.0567605, recall 0.827883
2017-12-09T22:23:12.199850: step 1231, loss 0.128458, acc 0.96875, prec 0.0567774, recall 0.827942
2017-12-09T22:23:12.500888: step 1232, loss 0.750698, acc 0.84375, prec 0.0567729, recall 0.828001
2017-12-09T22:23:12.806054: step 1233, loss 0.266775, acc 0.882812, prec 0.0567528, recall 0.828001
2017-12-09T22:23:13.105396: step 1234, loss 0.239579, acc 0.90625, prec 0.0567812, recall 0.82812
2017-12-09T22:23:13.407021: step 1235, loss 0.162068, acc 0.9375, prec 0.056815, recall 0.828238
2017-12-09T22:23:13.703889: step 1236, loss 0.142266, acc 0.960938, prec 0.0568083, recall 0.828238
2017-12-09T22:23:14.002114: step 1237, loss 0.378399, acc 0.96875, prec 0.0568251, recall 0.828297
2017-12-09T22:23:14.304769: step 1238, loss 0.479357, acc 0.96875, prec 0.0569087, recall 0.828532
2017-12-09T22:23:14.608319: step 1239, loss 2.14878, acc 0.992188, prec 0.0569309, recall 0.828307
2017-12-09T22:23:14.913307: step 1240, loss 0.102421, acc 0.960938, prec 0.0569242, recall 0.828307
2017-12-09T22:23:15.213127: step 1241, loss 0.60699, acc 0.945312, prec 0.0569814, recall 0.828483
2017-12-09T22:23:15.516135: step 1242, loss 0.350781, acc 0.921875, prec 0.0570124, recall 0.828601
2017-12-09T22:23:15.817162: step 1243, loss 0.273151, acc 0.9375, prec 0.0570016, recall 0.828601
2017-12-09T22:23:16.114625: step 1244, loss 0.322433, acc 0.867188, prec 0.0570232, recall 0.828718
2017-12-09T22:23:16.290088: step 1245, loss 0.294538, acc 0.882353, prec 0.0570373, recall 0.828776
2017-12-09T22:23:16.598940: step 1246, loss 0.227011, acc 0.90625, prec 0.0570434, recall 0.828835
2017-12-09T22:23:16.900481: step 1247, loss 0.384414, acc 0.914062, prec 0.0570508, recall 0.828893
2017-12-09T22:23:17.200434: step 1248, loss 0.309844, acc 0.929688, prec 0.0570388, recall 0.828893
2017-12-09T22:23:17.498786: step 1249, loss 2.29948, acc 0.882812, prec 0.0570422, recall 0.828669
2017-12-09T22:23:17.801167: step 1250, loss 0.486109, acc 0.90625, prec 0.0570482, recall 0.828727
2017-12-09T22:23:18.107846: step 1251, loss 0.224494, acc 0.914062, prec 0.0570335, recall 0.828727
2017-12-09T22:23:18.407376: step 1252, loss 0.300057, acc 0.875, prec 0.0571006, recall 0.828961
2017-12-09T22:23:18.713072: step 1253, loss 0.315628, acc 0.890625, prec 0.0571261, recall 0.829077
2017-12-09T22:23:19.012435: step 1254, loss 0.314554, acc 0.890625, prec 0.0571073, recall 0.829077
2017-12-09T22:23:19.313182: step 1255, loss 0.315413, acc 0.882812, prec 0.0571315, recall 0.829194
2017-12-09T22:23:19.613107: step 1256, loss 0.199309, acc 0.929688, prec 0.0571636, recall 0.82931
2017-12-09T22:23:19.913495: step 1257, loss 0.353147, acc 0.875, prec 0.0571643, recall 0.829368
2017-12-09T22:23:20.216159: step 1258, loss 0.254127, acc 0.914062, prec 0.0571937, recall 0.829484
2017-12-09T22:23:20.518356: step 1259, loss 0.291963, acc 0.882812, prec 0.0571736, recall 0.829484
2017-12-09T22:23:20.815731: step 1260, loss 0.184579, acc 0.9375, prec 0.057185, recall 0.829542
2017-12-09T22:23:21.111613: step 1261, loss 0.324689, acc 0.929688, prec 0.0572612, recall 0.829773
2017-12-09T22:23:21.412121: step 1262, loss 0.345328, acc 0.9375, prec 0.0572946, recall 0.829888
2017-12-09T22:23:21.717080: step 1263, loss 0.357785, acc 0.9375, prec 0.05735, recall 0.830061
2017-12-09T22:23:22.016860: step 1264, loss 0.31708, acc 0.921875, prec 0.0573586, recall 0.830118
2017-12-09T22:23:22.318619: step 1265, loss 0.303117, acc 0.898438, prec 0.0573853, recall 0.830233
2017-12-09T22:23:22.612160: step 1266, loss 0.143976, acc 0.960938, prec 0.0574226, recall 0.830348
2017-12-09T22:23:22.904750: step 1267, loss 0.164169, acc 0.953125, prec 0.0574586, recall 0.830463
2017-12-09T22:23:23.202822: step 1268, loss 0.133094, acc 0.945312, prec 0.0574492, recall 0.830463
2017-12-09T22:23:23.507837: step 1269, loss 0.115583, acc 0.960938, prec 0.0574866, recall 0.830577
2017-12-09T22:23:23.806117: step 1270, loss 0.133493, acc 0.945312, prec 0.0574992, recall 0.830634
2017-12-09T22:23:24.099765: step 1271, loss 0.17369, acc 0.96875, prec 0.0575158, recall 0.830691
2017-12-09T22:23:24.396736: step 1272, loss 0.106821, acc 0.953125, prec 0.0575298, recall 0.830748
2017-12-09T22:23:24.695509: step 1273, loss 0.152087, acc 0.929688, prec 0.0575177, recall 0.830748
2017-12-09T22:23:24.993804: step 1274, loss 0.20149, acc 0.96875, prec 0.0576003, recall 0.830976
2017-12-09T22:23:25.294082: step 1275, loss 0.559004, acc 0.976562, prec 0.0576183, recall 0.831033
2017-12-09T22:23:25.601335: step 1276, loss 0.762905, acc 0.976562, prec 0.0576596, recall 0.830868
2017-12-09T22:23:25.902322: step 1277, loss 0.618499, acc 0.984375, prec 0.0576582, recall 0.830588
2017-12-09T22:23:26.202687: step 1278, loss 2.43461, acc 0.976562, prec 0.0577215, recall 0.83048
2017-12-09T22:23:26.508718: step 1279, loss 0.172537, acc 0.960938, prec 0.0577367, recall 0.830537
2017-12-09T22:23:26.807291: step 1280, loss 0.21964, acc 0.921875, prec 0.0577892, recall 0.830707
2017-12-09T22:23:27.109754: step 1281, loss 0.231703, acc 0.929688, prec 0.057843, recall 0.830877
2017-12-09T22:23:27.405853: step 1282, loss 0.210298, acc 0.929688, prec 0.0578747, recall 0.830991
2017-12-09T22:23:27.706938: step 1283, loss 0.195478, acc 0.929688, prec 0.0579285, recall 0.83116
2017-12-09T22:23:28.005838: step 1284, loss 0.291802, acc 0.9375, prec 0.0579835, recall 0.831329
2017-12-09T22:23:28.305619: step 1285, loss 0.31966, acc 0.914062, prec 0.0580125, recall 0.831442
2017-12-09T22:23:28.606245: step 1286, loss 0.370537, acc 0.875, prec 0.0580129, recall 0.831498
2017-12-09T22:23:28.902853: step 1287, loss 0.450032, acc 0.875, prec 0.0580132, recall 0.831554
2017-12-09T22:23:29.204087: step 1288, loss 0.341075, acc 0.898438, prec 0.0580175, recall 0.831611
2017-12-09T22:23:29.499020: step 1289, loss 0.442257, acc 0.820312, prec 0.0579865, recall 0.831611
2017-12-09T22:23:29.804519: step 1290, loss 0.425561, acc 0.890625, prec 0.0579676, recall 0.831611
2017-12-09T22:23:30.108725: step 1291, loss 0.385594, acc 0.875, prec 0.057968, recall 0.831667
2017-12-09T22:23:30.407036: step 1292, loss 0.28139, acc 0.914062, prec 0.0579969, recall 0.831779
2017-12-09T22:23:30.708231: step 1293, loss 0.508991, acc 0.859375, prec 0.0579727, recall 0.831779
2017-12-09T22:23:31.003112: step 1294, loss 0.204158, acc 0.945312, prec 0.058007, recall 0.831891
2017-12-09T22:23:31.301275: step 1295, loss 0.301214, acc 0.914062, prec 0.0579922, recall 0.831891
2017-12-09T22:23:31.598859: step 1296, loss 2.36526, acc 0.960938, prec 0.0580961, recall 0.831894
2017-12-09T22:23:31.903965: step 1297, loss 0.252839, acc 0.898438, prec 0.058166, recall 0.832117
2017-12-09T22:23:32.204809: step 1298, loss 0.210964, acc 0.9375, prec 0.0581989, recall 0.832228
2017-12-09T22:23:32.504191: step 1299, loss 0.223255, acc 0.90625, prec 0.0581827, recall 0.832228
2017-12-09T22:23:32.804997: step 1300, loss 0.289105, acc 0.90625, prec 0.0582101, recall 0.832339
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1300

2017-12-09T22:23:34.064793: step 1301, loss 0.329301, acc 0.882812, prec 0.0582335, recall 0.83245
2017-12-09T22:23:34.359541: step 1302, loss 0.294756, acc 0.898438, prec 0.0582814, recall 0.832617
2017-12-09T22:23:34.656051: step 1303, loss 0.643235, acc 0.882812, prec 0.0583048, recall 0.832727
2017-12-09T22:23:34.956476: step 1304, loss 0.413275, acc 0.9375, prec 0.0583376, recall 0.832838
2017-12-09T22:23:35.263198: step 1305, loss 0.446613, acc 0.914062, prec 0.0584099, recall 0.833058
2017-12-09T22:23:35.563279: step 1306, loss 0.352062, acc 0.921875, prec 0.0584181, recall 0.833113
2017-12-09T22:23:35.861380: step 1307, loss 0.716705, acc 0.90625, prec 0.0585325, recall 0.833443
2017-12-09T22:23:36.161116: step 1308, loss 0.23628, acc 0.945312, prec 0.0585448, recall 0.833498
2017-12-09T22:23:36.458569: step 1309, loss 0.186719, acc 0.890625, prec 0.0585476, recall 0.833553
2017-12-09T22:23:36.752502: step 1310, loss 0.164616, acc 0.929688, prec 0.058579, recall 0.833662
2017-12-09T22:23:37.049644: step 1311, loss 0.290139, acc 0.921875, prec 0.0586089, recall 0.833771
2017-12-09T22:23:37.350997: step 1312, loss 0.223275, acc 0.914062, prec 0.0586158, recall 0.833826
2017-12-09T22:23:37.646423: step 1313, loss 0.387281, acc 0.84375, prec 0.0586104, recall 0.83388
2017-12-09T22:23:37.951020: step 1314, loss 0.26394, acc 0.929688, prec 0.0586417, recall 0.83399
2017-12-09T22:23:38.261211: step 1315, loss 0.182186, acc 0.921875, prec 0.0586933, recall 0.834153
2017-12-09T22:23:38.558714: step 1316, loss 0.16981, acc 0.929688, prec 0.0587245, recall 0.834261
2017-12-09T22:23:38.857454: step 1317, loss 0.238894, acc 0.921875, prec 0.0587327, recall 0.834316
2017-12-09T22:23:39.155084: step 1318, loss 0.364861, acc 0.914062, prec 0.0587612, recall 0.834424
2017-12-09T22:23:39.458453: step 1319, loss 0.521564, acc 0.929688, prec 0.0587707, recall 0.834478
2017-12-09T22:23:39.758638: step 1320, loss 0.418313, acc 0.953125, prec 0.0588276, recall 0.834641
2017-12-09T22:23:40.060073: step 1321, loss 0.18107, acc 0.953125, prec 0.0589062, recall 0.834856
2017-12-09T22:23:40.360734: step 1322, loss 0.24736, acc 0.945312, prec 0.0589183, recall 0.83491
2017-12-09T22:23:40.659554: step 1323, loss 0.140645, acc 0.953125, prec 0.0589319, recall 0.834964
2017-12-09T22:23:40.966541: step 1324, loss 0.1028, acc 0.96875, prec 0.0589481, recall 0.835018
2017-12-09T22:23:41.269203: step 1325, loss 0.143283, acc 0.953125, prec 0.0589616, recall 0.835072
2017-12-09T22:23:41.571012: step 1326, loss 0.876871, acc 0.953125, prec 0.0589548, recall 0.8348
2017-12-09T22:23:41.877624: step 1327, loss 0.190046, acc 0.9375, prec 0.0589656, recall 0.834853
2017-12-09T22:23:42.178440: step 1328, loss 0.179438, acc 0.96875, prec 0.0590251, recall 0.835015
2017-12-09T22:23:42.476508: step 1329, loss 0.169274, acc 0.929688, prec 0.0590129, recall 0.835015
2017-12-09T22:23:42.780777: step 1330, loss 0.217929, acc 0.945312, prec 0.0590251, recall 0.835068
2017-12-09T22:23:43.080265: step 1331, loss 0.804362, acc 0.976562, prec 0.059044, recall 0.83485
2017-12-09T22:23:43.386506: step 1332, loss 0.142081, acc 0.96875, prec 0.0590602, recall 0.834904
2017-12-09T22:23:43.688697: step 1333, loss 0.157484, acc 0.945312, prec 0.0590723, recall 0.834958
2017-12-09T22:23:43.987994: step 1334, loss 0.184413, acc 0.90625, prec 0.0590993, recall 0.835065
2017-12-09T22:23:44.286364: step 1335, loss 0.231126, acc 0.898438, prec 0.0591248, recall 0.835172
2017-12-09T22:23:44.595657: step 1336, loss 0.454754, acc 0.90625, prec 0.0591518, recall 0.835279
2017-12-09T22:23:44.897782: step 1337, loss 0.318947, acc 0.914062, prec 0.0592232, recall 0.835492
2017-12-09T22:23:45.198327: step 1338, loss 0.261939, acc 0.914062, prec 0.0592514, recall 0.835599
2017-12-09T22:23:45.500888: step 1339, loss 0.815347, acc 0.914062, prec 0.0592581, recall 0.835652
2017-12-09T22:23:45.804682: step 1340, loss 0.368446, acc 0.90625, prec 0.0593065, recall 0.835811
2017-12-09T22:23:46.117847: step 1341, loss 0.238748, acc 0.90625, prec 0.0593549, recall 0.83597
2017-12-09T22:23:46.423918: step 1342, loss 0.836279, acc 0.9375, prec 0.0593453, recall 0.8357
2017-12-09T22:23:46.724761: step 1343, loss 0.266563, acc 0.90625, prec 0.0593721, recall 0.835806
2017-12-09T22:23:47.026697: step 1344, loss 0.284219, acc 0.914062, prec 0.0594003, recall 0.835912
2017-12-09T22:23:47.327314: step 1345, loss 0.38868, acc 0.851562, prec 0.059439, recall 0.836071
2017-12-09T22:23:47.633139: step 1346, loss 0.328147, acc 0.890625, prec 0.059463, recall 0.836176
2017-12-09T22:23:47.936863: step 1347, loss 0.211712, acc 0.929688, prec 0.0595369, recall 0.836387
2017-12-09T22:23:48.232633: step 1348, loss 0.248223, acc 0.945312, prec 0.0595704, recall 0.836492
2017-12-09T22:23:48.528966: step 1349, loss 0.319343, acc 0.898438, prec 0.0595742, recall 0.836545
2017-12-09T22:23:48.827131: step 1350, loss 0.159876, acc 0.945312, prec 0.0596076, recall 0.83665
2017-12-09T22:23:49.128130: step 1351, loss 0.282271, acc 0.960938, prec 0.0596868, recall 0.836859
2017-12-09T22:23:49.431688: step 1352, loss 0.169191, acc 0.953125, prec 0.0596786, recall 0.836859
2017-12-09T22:23:49.733569: step 1353, loss 0.233458, acc 0.929688, prec 0.0597093, recall 0.836963
2017-12-09T22:23:50.036546: step 1354, loss 0.152416, acc 0.9375, prec 0.0597629, recall 0.83712
2017-12-09T22:23:50.352075: step 1355, loss 0.130483, acc 0.976562, prec 0.0598447, recall 0.837328
2017-12-09T22:23:50.652883: step 1356, loss 0.168118, acc 0.953125, prec 0.0598794, recall 0.837432
2017-12-09T22:23:50.952526: step 1357, loss 0.193646, acc 0.945312, prec 0.0598913, recall 0.837484
2017-12-09T22:23:51.255394: step 1358, loss 0.174339, acc 0.945312, prec 0.0600105, recall 0.837795
2017-12-09T22:23:51.559281: step 1359, loss 0.383406, acc 0.945312, prec 0.0600653, recall 0.83795
2017-12-09T22:23:51.859126: step 1360, loss 0.483471, acc 0.914062, prec 0.0600716, recall 0.838001
2017-12-09T22:23:52.159151: step 1361, loss 0.237977, acc 0.96875, prec 0.060109, recall 0.838104
2017-12-09T22:23:52.460405: step 1362, loss 0.166128, acc 0.9375, prec 0.0600981, recall 0.838104
2017-12-09T22:23:52.755127: step 1363, loss 0.103977, acc 0.960938, prec 0.0600912, recall 0.838104
2017-12-09T22:23:53.053127: step 1364, loss 1.23427, acc 0.96875, prec 0.0601715, recall 0.83831
2017-12-09T22:23:53.358496: step 1365, loss 0.0651244, acc 0.984375, prec 0.0601901, recall 0.838361
2017-12-09T22:23:53.655018: step 1366, loss 0.305018, acc 0.953125, prec 0.0602462, recall 0.838515
2017-12-09T22:23:53.955875: step 1367, loss 0.263356, acc 0.929688, prec 0.0602767, recall 0.838618
2017-12-09T22:23:54.257660: step 1368, loss 0.308006, acc 0.921875, prec 0.0603057, recall 0.83872
2017-12-09T22:23:54.557720: step 1369, loss 0.234137, acc 0.890625, prec 0.0602865, recall 0.83872
2017-12-09T22:23:54.854541: step 1370, loss 0.203962, acc 0.921875, prec 0.0602942, recall 0.838771
2017-12-09T22:23:55.149570: step 1371, loss 0.390253, acc 0.921875, prec 0.0603446, recall 0.838924
2017-12-09T22:23:55.448513: step 1372, loss 0.409841, acc 0.914062, prec 0.0604151, recall 0.839128
2017-12-09T22:23:55.745093: step 1373, loss 0.252857, acc 0.9375, prec 0.0604468, recall 0.839229
2017-12-09T22:23:56.044889: step 1374, loss 1.96254, acc 0.953125, prec 0.0605254, recall 0.839167
2017-12-09T22:23:56.342317: step 1375, loss 0.54114, acc 0.921875, prec 0.0605544, recall 0.839269
2017-12-09T22:23:56.639398: step 1376, loss 0.305198, acc 0.882812, prec 0.0605337, recall 0.839269
2017-12-09T22:23:56.934876: step 1377, loss 2.22831, acc 0.859375, prec 0.0605103, recall 0.839004
2017-12-09T22:23:57.233641: step 1378, loss 0.381148, acc 0.835938, prec 0.0605028, recall 0.839055
2017-12-09T22:23:57.535561: step 1379, loss 0.400381, acc 0.875, prec 0.0605448, recall 0.839207
2017-12-09T22:23:57.835805: step 1380, loss 0.236592, acc 0.945312, prec 0.0606205, recall 0.839409
2017-12-09T22:23:58.132885: step 1381, loss 0.283972, acc 0.90625, prec 0.060604, recall 0.839409
2017-12-09T22:23:58.430836: step 1382, loss 0.487651, acc 0.828125, prec 0.0605951, recall 0.83946
2017-12-09T22:23:58.731518: step 1383, loss 0.402805, acc 0.84375, prec 0.0605889, recall 0.83951
2017-12-09T22:23:59.034874: step 1384, loss 0.817289, acc 0.8125, prec 0.0606411, recall 0.839711
2017-12-09T22:23:59.332174: step 1385, loss 0.265688, acc 0.890625, prec 0.0606218, recall 0.839711
2017-12-09T22:23:59.632576: step 1386, loss 0.299419, acc 0.90625, prec 0.0606904, recall 0.839912
2017-12-09T22:23:59.941383: step 1387, loss 0.508661, acc 0.859375, prec 0.060687, recall 0.839962
2017-12-09T22:24:00.250592: step 1388, loss 0.543433, acc 0.867188, prec 0.0606849, recall 0.840013
2017-12-09T22:24:00.547170: step 1389, loss 0.364476, acc 0.914062, prec 0.0607547, recall 0.840213
2017-12-09T22:24:00.848593: step 1390, loss 0.384185, acc 0.867188, prec 0.0607951, recall 0.840362
2017-12-09T22:24:01.147588: step 1391, loss 0.243654, acc 0.921875, prec 0.0608238, recall 0.840462
2017-12-09T22:24:01.449213: step 1392, loss 0.205494, acc 0.929688, prec 0.0608539, recall 0.840562
2017-12-09T22:24:01.747400: step 1393, loss 0.461726, acc 0.921875, prec 0.0608613, recall 0.840611
2017-12-09T22:24:02.046226: step 1394, loss 0.234302, acc 0.921875, prec 0.0609112, recall 0.84076
2017-12-09T22:24:02.347366: step 1395, loss 0.166782, acc 0.9375, prec 0.0609214, recall 0.84081
2017-12-09T22:24:02.644089: step 1396, loss 0.253251, acc 0.90625, prec 0.0609473, recall 0.840909
2017-12-09T22:24:02.942562: step 1397, loss 0.348368, acc 0.945312, prec 0.0610436, recall 0.841156
2017-12-09T22:24:03.245014: step 1398, loss 0.332615, acc 0.953125, prec 0.0610988, recall 0.841304
2017-12-09T22:24:03.547305: step 1399, loss 0.136613, acc 0.984375, prec 0.0611384, recall 0.841403
2017-12-09T22:24:03.851288: step 1400, loss 0.137691, acc 0.953125, prec 0.0611725, recall 0.841501
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1400

2017-12-09T22:24:05.195329: step 1401, loss 0.395846, acc 0.960938, prec 0.0611868, recall 0.84155
2017-12-09T22:24:05.511871: step 1402, loss 0.171815, acc 0.945312, prec 0.0611771, recall 0.84155
2017-12-09T22:24:05.824539: step 1403, loss 0.144818, acc 0.9375, prec 0.0611661, recall 0.84155
2017-12-09T22:24:06.128615: step 1404, loss 0.460669, acc 0.945312, prec 0.0611776, recall 0.8416
2017-12-09T22:24:06.427991: step 1405, loss 0.646764, acc 0.953125, prec 0.0611707, recall 0.841339
2017-12-09T22:24:06.730034: step 1406, loss 0.274535, acc 0.945312, prec 0.0612033, recall 0.841437
2017-12-09T22:24:07.036339: step 1407, loss 2.14409, acc 0.929688, prec 0.0612135, recall 0.841226
2017-12-09T22:24:07.336435: step 1408, loss 0.141098, acc 0.945312, prec 0.0612249, recall 0.841275
2017-12-09T22:24:07.636886: step 1409, loss 1.03536, acc 0.914062, prec 0.0612323, recall 0.841064
2017-12-09T22:24:07.944457: step 1410, loss 0.429125, acc 0.859375, prec 0.0612709, recall 0.841211
2017-12-09T22:24:08.246238: step 1411, loss 0.224021, acc 0.914062, prec 0.0613402, recall 0.841407
2017-12-09T22:24:08.548738: step 1412, loss 0.419507, acc 0.851562, prec 0.061314, recall 0.841407
2017-12-09T22:24:08.846359: step 1413, loss 0.451824, acc 0.859375, prec 0.0613314, recall 0.841505
2017-12-09T22:24:09.143409: step 1414, loss 0.232403, acc 0.914062, prec 0.0613373, recall 0.841554
2017-12-09T22:24:09.444824: step 1415, loss 0.498651, acc 0.828125, prec 0.0613491, recall 0.841651
2017-12-09T22:24:09.744859: step 1416, loss 0.315762, acc 0.859375, prec 0.0613244, recall 0.841651
2017-12-09T22:24:10.042917: step 1417, loss 0.386527, acc 0.867188, prec 0.061322, recall 0.8417
2017-12-09T22:24:10.339172: step 1418, loss 0.432701, acc 0.84375, prec 0.0613156, recall 0.841749
2017-12-09T22:24:10.634843: step 1419, loss 0.26915, acc 0.898438, prec 0.0612977, recall 0.841749
2017-12-09T22:24:10.926903: step 1420, loss 0.654167, acc 0.890625, prec 0.0613626, recall 0.841943
2017-12-09T22:24:11.223466: step 1421, loss 0.211552, acc 0.90625, prec 0.0613672, recall 0.841992
2017-12-09T22:24:11.519627: step 1422, loss 0.82671, acc 0.875, prec 0.0614082, recall 0.842138
2017-12-09T22:24:11.820436: step 1423, loss 0.472472, acc 0.890625, prec 0.06141, recall 0.842186
2017-12-09T22:24:12.121048: step 1424, loss 0.546293, acc 0.875, prec 0.061472, recall 0.84238
2017-12-09T22:24:12.419075: step 1425, loss 0.473871, acc 0.898438, prec 0.0614751, recall 0.842428
2017-12-09T22:24:12.714627: step 1426, loss 0.47402, acc 0.859375, prec 0.0615133, recall 0.842573
2017-12-09T22:24:13.010495: step 1427, loss 0.473647, acc 0.867188, prec 0.0615319, recall 0.842669
2017-12-09T22:24:13.308970: step 1428, loss 0.212208, acc 0.914062, prec 0.0615168, recall 0.842669
2017-12-09T22:24:13.610086: step 1429, loss 0.482866, acc 0.84375, prec 0.0615312, recall 0.842765
2017-12-09T22:24:13.907163: step 1430, loss 0.843785, acc 0.929688, prec 0.0616236, recall 0.843005
2017-12-09T22:24:14.204808: step 1431, loss 1.20395, acc 0.914062, prec 0.0616308, recall 0.842796
2017-12-09T22:24:14.501793: step 1432, loss 0.363345, acc 0.898438, prec 0.0616339, recall 0.842844
2017-12-09T22:24:14.805090: step 1433, loss 0.35313, acc 0.914062, prec 0.0616397, recall 0.842892
2017-12-09T22:24:15.101059: step 1434, loss 0.408474, acc 0.84375, prec 0.061675, recall 0.843036
2017-12-09T22:24:15.400632: step 1435, loss 0.231813, acc 0.914062, prec 0.0616808, recall 0.843084
2017-12-09T22:24:15.700555: step 1436, loss 0.277119, acc 0.882812, prec 0.0616602, recall 0.843084
2017-12-09T22:24:15.996408: step 1437, loss 0.432373, acc 0.890625, prec 0.0617036, recall 0.843227
2017-12-09T22:24:16.297697: step 1438, loss 0.4519, acc 0.882812, prec 0.0617248, recall 0.843322
2017-12-09T22:24:16.594259: step 1439, loss 0.461849, acc 0.859375, prec 0.0617419, recall 0.843417
2017-12-09T22:24:17.594444: step 1440, loss 0.789258, acc 0.835938, prec 0.0617548, recall 0.843513
2017-12-09T22:24:18.489123: step 1441, loss 0.394993, acc 0.859375, prec 0.0617509, recall 0.84356
2017-12-09T22:24:19.051270: step 1442, loss 0.473653, acc 0.84375, prec 0.0617443, recall 0.843608
2017-12-09T22:24:19.376583: step 1443, loss 0.322662, acc 0.914062, prec 0.0617709, recall 0.843703
2017-12-09T22:24:19.680276: step 1444, loss 0.413704, acc 0.921875, prec 0.0617989, recall 0.843797
2017-12-09T22:24:19.984530: step 1445, loss 0.195095, acc 0.921875, prec 0.0618268, recall 0.843892
2017-12-09T22:24:20.294579: step 1446, loss 0.118261, acc 0.945312, prec 0.0618172, recall 0.843892
2017-12-09T22:24:20.596580: step 1447, loss 0.562836, acc 0.914062, prec 0.0619895, recall 0.844317
2017-12-09T22:24:20.897677: step 1448, loss 0.311519, acc 0.929688, prec 0.0620188, recall 0.844411
2017-12-09T22:24:21.200047: step 1449, loss 0.19752, acc 0.945312, prec 0.0620508, recall 0.844505
2017-12-09T22:24:21.498737: step 1450, loss 0.761481, acc 0.9375, prec 0.0620814, recall 0.844599
2017-12-09T22:24:21.800780: step 1451, loss 1.34635, acc 0.90625, prec 0.0621078, recall 0.844438
2017-12-09T22:24:22.100739: step 1452, loss 0.265217, acc 0.90625, prec 0.0620913, recall 0.844438
2017-12-09T22:24:22.401296: step 1453, loss 0.320457, acc 0.96875, prec 0.0621066, recall 0.844485
2017-12-09T22:24:22.708247: step 1454, loss 0.129695, acc 0.960938, prec 0.0620997, recall 0.844485
2017-12-09T22:24:23.004636: step 1455, loss 0.267564, acc 0.914062, prec 0.0621053, recall 0.844531
2017-12-09T22:24:23.307828: step 1456, loss 0.178089, acc 0.945312, prec 0.0621373, recall 0.844625
2017-12-09T22:24:23.610529: step 1457, loss 0.599539, acc 0.90625, prec 0.062183, recall 0.844765
2017-12-09T22:24:23.910107: step 1458, loss 0.400911, acc 0.851562, prec 0.0621777, recall 0.844812
2017-12-09T22:24:24.208636: step 1459, loss 0.394289, acc 0.898438, prec 0.0621805, recall 0.844859
2017-12-09T22:24:24.504058: step 1460, loss 0.291731, acc 0.890625, prec 0.062182, recall 0.844905
2017-12-09T22:24:24.805848: step 1461, loss 0.336828, acc 0.898438, prec 0.0621641, recall 0.844905
2017-12-09T22:24:25.110408: step 1462, loss 0.372952, acc 0.890625, prec 0.0621656, recall 0.844952
2017-12-09T22:24:25.408565: step 1463, loss 0.2027, acc 0.929688, prec 0.062174, recall 0.844998
2017-12-09T22:24:25.716889: step 1464, loss 0.183692, acc 0.929688, prec 0.0621823, recall 0.845045
2017-12-09T22:24:26.020565: step 1465, loss 0.188976, acc 0.9375, prec 0.0622128, recall 0.845138
2017-12-09T22:24:26.327223: step 1466, loss 1.14001, acc 0.890625, prec 0.0621949, recall 0.844885
2017-12-09T22:24:26.623129: step 1467, loss 0.160414, acc 0.960938, prec 0.0622709, recall 0.84507
2017-12-09T22:24:26.921084: step 1468, loss 0.179566, acc 0.945312, prec 0.062282, recall 0.845117
2017-12-09T22:24:27.217591: step 1469, loss 0.361483, acc 0.945312, prec 0.0623345, recall 0.845256
2017-12-09T22:24:27.514137: step 1470, loss 0.150925, acc 0.953125, prec 0.0623469, recall 0.845302
2017-12-09T22:24:27.809472: step 1471, loss 1.66156, acc 0.953125, prec 0.0623607, recall 0.845096
2017-12-09T22:24:28.109270: step 1472, loss 2.82006, acc 0.9375, prec 0.0623718, recall 0.844889
2017-12-09T22:24:28.414033: step 1473, loss 0.400332, acc 0.875, prec 0.0623704, recall 0.844936
2017-12-09T22:24:28.713515: step 1474, loss 0.766162, acc 0.90625, prec 0.0624159, recall 0.845075
2017-12-09T22:24:29.017142: step 1475, loss 0.345387, acc 0.828125, prec 0.062427, recall 0.845167
2017-12-09T22:24:29.319431: step 1476, loss 0.692723, acc 0.835938, prec 0.0624394, recall 0.845259
2017-12-09T22:24:29.620970: step 1477, loss 0.511309, acc 0.820312, prec 0.0624285, recall 0.845306
2017-12-09T22:24:29.928050: step 1478, loss 0.509008, acc 0.859375, prec 0.062445, recall 0.845398
2017-12-09T22:24:30.236938: step 1479, loss 0.484452, acc 0.828125, prec 0.062456, recall 0.84549
2017-12-09T22:24:30.530524: step 1480, loss 0.637791, acc 0.804688, prec 0.0624629, recall 0.845582
2017-12-09T22:24:30.826331: step 1481, loss 0.526436, acc 0.835938, prec 0.0624753, recall 0.845674
2017-12-09T22:24:31.120672: step 1482, loss 0.652878, acc 0.773438, prec 0.0624561, recall 0.845719
2017-12-09T22:24:31.422342: step 1483, loss 0.564592, acc 0.78125, prec 0.0624383, recall 0.845765
2017-12-09T22:24:31.723889: step 1484, loss 0.434295, acc 0.835938, prec 0.0624095, recall 0.845765
2017-12-09T22:24:32.017908: step 1485, loss 0.342207, acc 0.890625, prec 0.0624109, recall 0.845811
2017-12-09T22:24:32.315234: step 1486, loss 0.33788, acc 0.890625, prec 0.0624123, recall 0.845857
2017-12-09T22:24:32.617205: step 1487, loss 0.423493, acc 0.859375, prec 0.0624288, recall 0.845948
2017-12-09T22:24:32.917021: step 1488, loss 0.387231, acc 0.875, prec 0.0624275, recall 0.845994
2017-12-09T22:24:33.215743: step 1489, loss 0.137765, acc 0.953125, prec 0.0625014, recall 0.846177
2017-12-09T22:24:33.509406: step 1490, loss 0.346426, acc 0.898438, prec 0.0624836, recall 0.846177
2017-12-09T22:24:33.803808: step 1491, loss 0.238314, acc 0.929688, prec 0.0624713, recall 0.846177
2017-12-09T22:24:34.106271: step 1492, loss 0.122782, acc 0.960938, prec 0.0625055, recall 0.846268
2017-12-09T22:24:34.407981: step 1493, loss 0.281186, acc 0.953125, prec 0.0625793, recall 0.84645
2017-12-09T22:24:34.586189: step 1494, loss 0.101105, acc 0.960784, prec 0.0625766, recall 0.84645
2017-12-09T22:24:34.888719: step 1495, loss 0.221494, acc 0.96875, prec 0.0626326, recall 0.846586
2017-12-09T22:24:35.202788: step 1496, loss 0.0633432, acc 0.976562, prec 0.0626285, recall 0.846586
2017-12-09T22:24:35.508759: step 1497, loss 0.0940723, acc 0.984375, prec 0.0626667, recall 0.846677
2017-12-09T22:24:35.807988: step 1498, loss 0.11081, acc 0.96875, prec 0.0627022, recall 0.846767
2017-12-09T22:24:36.110925: step 1499, loss 0.662698, acc 0.984375, prec 0.0627418, recall 0.846608
2017-12-09T22:24:36.412832: step 1500, loss 0.0563318, acc 0.984375, prec 0.0627596, recall 0.846653

Evaluation:
2017-12-09T22:24:41.165897: step 1500, loss 3.71141, acc 0.968676, prec 0.0633288, recall 0.819409

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1500

2017-12-09T22:24:42.534262: step 1501, loss 0.126975, acc 0.984375, prec 0.0634075, recall 0.819612
2017-12-09T22:24:42.834658: step 1502, loss 0.0319688, acc 1, prec 0.0634279, recall 0.819663
2017-12-09T22:24:43.133813: step 1503, loss 0.288589, acc 0.984375, prec 0.0634862, recall 0.819815
2017-12-09T22:24:43.431630: step 1504, loss 0.212967, acc 0.984375, prec 0.0635038, recall 0.819865
2017-12-09T22:24:43.739635: step 1505, loss 0.268734, acc 0.984375, prec 0.0635621, recall 0.820017
2017-12-09T22:24:44.040723: step 1506, loss 1.5639, acc 0.976562, prec 0.0636204, recall 0.819938
2017-12-09T22:24:44.340158: step 1507, loss 0.131481, acc 0.945312, prec 0.0636107, recall 0.819938
2017-12-09T22:24:44.641517: step 1508, loss 0.262478, acc 0.953125, prec 0.0636431, recall 0.820039
2017-12-09T22:24:44.939808: step 1509, loss 0.137736, acc 0.96875, prec 0.0636986, recall 0.82019
2017-12-09T22:24:45.242039: step 1510, loss 0.146725, acc 0.960938, prec 0.0637323, recall 0.820291
2017-12-09T22:24:45.544851: step 1511, loss 0.387502, acc 0.882812, prec 0.0637725, recall 0.820441
2017-12-09T22:24:45.851864: step 1512, loss 0.283692, acc 0.890625, prec 0.0638141, recall 0.820592
2017-12-09T22:24:46.152481: step 1513, loss 0.356987, acc 0.929688, prec 0.0638829, recall 0.820792
2017-12-09T22:24:46.449213: step 1514, loss 0.348357, acc 0.898438, prec 0.0638852, recall 0.820841
2017-12-09T22:24:46.746959: step 1515, loss 0.308235, acc 0.867188, prec 0.0638616, recall 0.820841
2017-12-09T22:24:47.049145: step 1516, loss 0.233586, acc 0.945312, prec 0.0638925, recall 0.820941
2017-12-09T22:24:47.345817: step 1517, loss 0.18766, acc 0.929688, prec 0.0639003, recall 0.820991
2017-12-09T22:24:47.643126: step 1518, loss 0.260935, acc 0.953125, prec 0.0639529, recall 0.82114
2017-12-09T22:24:47.943907: step 1519, loss 0.290602, acc 0.914062, prec 0.0639579, recall 0.82119
2017-12-09T22:24:48.242161: step 1520, loss 0.479305, acc 0.84375, prec 0.0639302, recall 0.82119
2017-12-09T22:24:48.543210: step 1521, loss 0.302328, acc 0.914062, prec 0.0639758, recall 0.821339
2017-12-09T22:24:48.847445: step 1522, loss 0.283524, acc 0.898438, prec 0.0639983, recall 0.821438
2017-12-09T22:24:49.151021: step 1523, loss 0.242657, acc 0.929688, prec 0.0640668, recall 0.821637
2017-12-09T22:24:49.454390: step 1524, loss 0.131759, acc 0.945312, prec 0.0640571, recall 0.821637
2017-12-09T22:24:49.751303: step 1525, loss 0.604901, acc 0.945312, prec 0.0641081, recall 0.821785
2017-12-09T22:24:50.065326: step 1526, loss 0.194697, acc 0.929688, prec 0.0640956, recall 0.821785
2017-12-09T22:24:50.385105: step 1527, loss 1.04981, acc 0.914062, prec 0.0641222, recall 0.821656
2017-12-09T22:24:50.685981: step 1528, loss 0.195492, acc 0.921875, prec 0.0641286, recall 0.821705
2017-12-09T22:24:50.983721: step 1529, loss 0.179944, acc 0.914062, prec 0.0641538, recall 0.821804
2017-12-09T22:24:51.282701: step 1530, loss 0.18695, acc 0.914062, prec 0.0641588, recall 0.821853
2017-12-09T22:24:51.585041: step 1531, loss 0.355612, acc 0.945312, prec 0.0642097, recall 0.822001
2017-12-09T22:24:51.890051: step 1532, loss 0.229246, acc 0.921875, prec 0.0642362, recall 0.822099
2017-12-09T22:24:52.191055: step 1533, loss 0.24655, acc 0.914062, prec 0.0643219, recall 0.822345
2017-12-09T22:24:52.489155: step 1534, loss 0.213618, acc 0.898438, prec 0.0643039, recall 0.822345
2017-12-09T22:24:52.786703: step 1535, loss 0.143286, acc 0.960938, prec 0.064297, recall 0.822345
2017-12-09T22:24:53.087249: step 1536, loss 0.955439, acc 0.945312, prec 0.064329, recall 0.822216
2017-12-09T22:24:53.389759: step 1537, loss 0.188743, acc 0.96875, prec 0.0643436, recall 0.822265
2017-12-09T22:24:53.687830: step 1538, loss 0.285132, acc 0.890625, prec 0.0643444, recall 0.822314
2017-12-09T22:24:53.990815: step 1539, loss 0.276363, acc 0.96875, prec 0.064359, recall 0.822363
2017-12-09T22:24:54.289522: step 1540, loss 0.157814, acc 0.945312, prec 0.0644098, recall 0.82251
2017-12-09T22:24:54.584714: step 1541, loss 0.205344, acc 0.96875, prec 0.0644445, recall 0.822607
2017-12-09T22:24:54.879792: step 1542, loss 0.448532, acc 0.929688, prec 0.0644522, recall 0.822656
2017-12-09T22:24:55.180841: step 1543, loss 0.153974, acc 0.953125, prec 0.064464, recall 0.822705
2017-12-09T22:24:55.481743: step 1544, loss 0.290063, acc 0.960938, prec 0.0644974, recall 0.822802
2017-12-09T22:24:55.780977: step 1545, loss 0.173326, acc 0.960938, prec 0.0645307, recall 0.8229
2017-12-09T22:24:56.080792: step 1546, loss 0.224981, acc 0.929688, prec 0.0645384, recall 0.822948
2017-12-09T22:24:56.380528: step 1547, loss 0.126788, acc 0.9375, prec 0.0645272, recall 0.822948
2017-12-09T22:24:56.679230: step 1548, loss 0.172914, acc 0.9375, prec 0.0645564, recall 0.823045
2017-12-09T22:24:56.982179: step 1549, loss 0.183263, acc 0.96875, prec 0.0646313, recall 0.823239
2017-12-09T22:24:57.288459: step 1550, loss 0.159236, acc 0.9375, prec 0.0646202, recall 0.823239
2017-12-09T22:24:57.583990: step 1551, loss 0.0953061, acc 0.96875, prec 0.0646348, recall 0.823288
2017-12-09T22:24:57.883589: step 1552, loss 3.33643, acc 0.945312, prec 0.0647069, recall 0.823256
2017-12-09T22:24:58.185494: step 1553, loss 0.122142, acc 0.960938, prec 0.0646999, recall 0.823256
2017-12-09T22:24:58.485597: step 1554, loss 0.261384, acc 0.9375, prec 0.064729, recall 0.823352
2017-12-09T22:24:58.788898: step 1555, loss 0.245411, acc 0.921875, prec 0.0647151, recall 0.823352
2017-12-09T22:24:59.088404: step 1556, loss 0.431113, acc 0.914062, prec 0.0647199, recall 0.823401
2017-12-09T22:24:59.388884: step 1557, loss 0.220773, acc 0.914062, prec 0.0647046, recall 0.823401
2017-12-09T22:24:59.687714: step 1558, loss 0.368659, acc 0.90625, prec 0.064708, recall 0.823449
2017-12-09T22:24:59.995599: step 1559, loss 0.277992, acc 0.890625, prec 0.0647087, recall 0.823497
2017-12-09T22:25:00.299682: step 1560, loss 0.179213, acc 0.9375, prec 0.0646975, recall 0.823497
2017-12-09T22:25:00.601317: step 1561, loss 0.302726, acc 0.890625, prec 0.0646781, recall 0.823497
2017-12-09T22:25:00.897908: step 1562, loss 0.177316, acc 0.945312, prec 0.0647085, recall 0.823594
2017-12-09T22:25:01.193211: step 1563, loss 0.328988, acc 0.90625, prec 0.0647119, recall 0.823642
2017-12-09T22:25:01.496231: step 1564, loss 0.504033, acc 0.914062, prec 0.0647167, recall 0.82369
2017-12-09T22:25:01.790490: step 1565, loss 0.424417, acc 0.820312, prec 0.0647049, recall 0.823738
2017-12-09T22:25:02.088747: step 1566, loss 0.389994, acc 0.882812, prec 0.0647041, recall 0.823786
2017-12-09T22:25:02.386055: step 1567, loss 0.313805, acc 0.882812, prec 0.0647034, recall 0.823834
2017-12-09T22:25:02.680272: step 1568, loss 0.216494, acc 0.921875, prec 0.0647496, recall 0.823978
2017-12-09T22:25:02.980138: step 1569, loss 0.181998, acc 0.929688, prec 0.0647772, recall 0.824074
2017-12-09T22:25:03.281238: step 1570, loss 0.207158, acc 0.945312, prec 0.0648075, recall 0.82417
2017-12-09T22:25:03.584882: step 1571, loss 0.16905, acc 0.953125, prec 0.0648392, recall 0.824265
2017-12-09T22:25:03.888400: step 1572, loss 0.157893, acc 0.953125, prec 0.0648709, recall 0.824361
2017-12-09T22:25:04.195628: step 1573, loss 0.192115, acc 0.921875, prec 0.064897, recall 0.824457
2017-12-09T22:25:04.495050: step 1574, loss 0.0906411, acc 0.960938, prec 0.0649101, recall 0.824504
2017-12-09T22:25:04.794063: step 1575, loss 0.057888, acc 0.992188, prec 0.0649287, recall 0.824552
2017-12-09T22:25:05.092291: step 1576, loss 0.127039, acc 0.976562, prec 0.0649645, recall 0.824647
2017-12-09T22:25:05.401221: step 1577, loss 0.702107, acc 0.96875, prec 0.0650189, recall 0.82479
2017-12-09T22:25:05.698237: step 1578, loss 0.0928924, acc 0.976562, prec 0.0650347, recall 0.824837
2017-12-09T22:25:05.997225: step 1579, loss 0.259152, acc 0.976562, prec 0.0650705, recall 0.824932
2017-12-09T22:25:06.299138: step 1580, loss 0.168925, acc 0.992188, prec 0.0651291, recall 0.825074
2017-12-09T22:25:06.600621: step 1581, loss 0.0855886, acc 0.984375, prec 0.0651663, recall 0.825169
2017-12-09T22:25:06.898802: step 1582, loss 0.136959, acc 0.953125, prec 0.0651979, recall 0.825264
2017-12-09T22:25:07.199899: step 1583, loss 0.178942, acc 0.921875, prec 0.065184, recall 0.825264
2017-12-09T22:25:07.503529: step 1584, loss 0.69953, acc 0.96875, prec 0.0651997, recall 0.825088
2017-12-09T22:25:07.807640: step 1585, loss 0.140244, acc 0.976562, prec 0.0652555, recall 0.82523
2017-12-09T22:25:08.106870: step 1586, loss 0.412208, acc 0.945312, prec 0.0652856, recall 0.825324
2017-12-09T22:25:08.405864: step 1587, loss 0.210935, acc 0.96875, prec 0.0653399, recall 0.825465
2017-12-09T22:25:08.704714: step 1588, loss 0.248621, acc 0.945312, prec 0.0653501, recall 0.825512
2017-12-09T22:25:09.008845: step 1589, loss 0.512779, acc 0.9375, prec 0.0653789, recall 0.825606
2017-12-09T22:25:09.307605: step 1590, loss 0.199905, acc 0.898438, prec 0.0654006, recall 0.8257
2017-12-09T22:25:09.609976: step 1591, loss 0.209674, acc 0.953125, prec 0.0654521, recall 0.825841
2017-12-09T22:25:09.906802: step 1592, loss 0.182351, acc 0.929688, prec 0.0654993, recall 0.825982
2017-12-09T22:25:10.214186: step 1593, loss 0.340063, acc 0.835938, prec 0.0655497, recall 0.826169
2017-12-09T22:25:10.512732: step 1594, loss 0.390697, acc 0.90625, prec 0.0655926, recall 0.826309
2017-12-09T22:25:10.810969: step 1595, loss 0.202975, acc 0.929688, prec 0.0656199, recall 0.826402
2017-12-09T22:25:11.108131: step 1596, loss 0.357989, acc 0.90625, prec 0.065623, recall 0.826449
2017-12-09T22:25:11.401026: step 1597, loss 0.210456, acc 0.929688, prec 0.0656502, recall 0.826542
2017-12-09T22:25:11.701790: step 1598, loss 0.286818, acc 0.921875, prec 0.0656561, recall 0.826588
2017-12-09T22:25:11.997399: step 1599, loss 0.332734, acc 0.914062, prec 0.0657004, recall 0.826727
2017-12-09T22:25:12.295297: step 1600, loss 0.128447, acc 0.960938, prec 0.0656934, recall 0.826727
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1600

2017-12-09T22:25:13.683682: step 1601, loss 0.714629, acc 0.9375, prec 0.0657816, recall 0.826959
2017-12-09T22:25:13.983878: step 1602, loss 0.547838, acc 0.90625, prec 0.0658046, recall 0.827052
2017-12-09T22:25:14.280877: step 1603, loss 0.191152, acc 0.945312, prec 0.0657948, recall 0.827052
2017-12-09T22:25:14.581187: step 1604, loss 0.472293, acc 0.929688, prec 0.0659014, recall 0.827329
2017-12-09T22:25:14.886016: step 1605, loss 0.302965, acc 0.90625, prec 0.0659044, recall 0.827375
2017-12-09T22:25:15.185449: step 1606, loss 0.303957, acc 0.9375, prec 0.0659131, recall 0.827421
2017-12-09T22:25:15.482621: step 1607, loss 0.258492, acc 0.90625, prec 0.0659161, recall 0.827467
2017-12-09T22:25:15.782283: step 1608, loss 0.33177, acc 0.921875, prec 0.0659418, recall 0.827559
2017-12-09T22:25:16.080929: step 1609, loss 0.145471, acc 0.9375, prec 0.0659504, recall 0.827605
2017-12-09T22:25:16.377593: step 1610, loss 0.271776, acc 0.921875, prec 0.0659364, recall 0.827605
2017-12-09T22:25:16.675884: step 1611, loss 0.246885, acc 0.890625, prec 0.0659366, recall 0.82765
2017-12-09T22:25:16.973510: step 1612, loss 0.184732, acc 0.9375, prec 0.0659254, recall 0.82765
2017-12-09T22:25:17.270429: step 1613, loss 0.279349, acc 0.890625, prec 0.0659455, recall 0.827742
2017-12-09T22:25:17.568455: step 1614, loss 0.153813, acc 0.9375, prec 0.0659343, recall 0.827742
2017-12-09T22:25:17.867755: step 1615, loss 0.373053, acc 0.953125, prec 0.0659457, recall 0.827788
2017-12-09T22:25:18.164486: step 1616, loss 0.184689, acc 0.945312, prec 0.0659953, recall 0.827926
2017-12-09T22:25:18.466372: step 1617, loss 0.0985484, acc 0.945312, prec 0.0659855, recall 0.827926
2017-12-09T22:25:18.763277: step 1618, loss 0.166543, acc 0.96875, prec 0.0659997, recall 0.827971
2017-12-09T22:25:19.063937: step 1619, loss 0.203748, acc 0.9375, prec 0.0660281, recall 0.828063
2017-12-09T22:25:19.362060: step 1620, loss 0.0410653, acc 0.984375, prec 0.0660253, recall 0.828063
2017-12-09T22:25:19.658227: step 1621, loss 0.0504478, acc 1, prec 0.0660451, recall 0.828108
2017-12-09T22:25:19.958083: step 1622, loss 0.0995376, acc 0.984375, prec 0.0660819, recall 0.8282
2017-12-09T22:25:20.269739: step 1623, loss 0.0642993, acc 0.984375, prec 0.0661187, recall 0.828291
2017-12-09T22:25:20.570907: step 1624, loss 1.94826, acc 0.960938, prec 0.0661526, recall 0.828162
2017-12-09T22:25:20.874672: step 1625, loss 1.37703, acc 0.960938, prec 0.0661668, recall 0.827988
2017-12-09T22:25:21.175499: step 1626, loss 0.249391, acc 0.953125, prec 0.0662177, recall 0.828125
2017-12-09T22:25:21.478025: step 1627, loss 0.190978, acc 0.945312, prec 0.0662475, recall 0.828216
2017-12-09T22:25:21.782869: step 1628, loss 0.208765, acc 0.9375, prec 0.0662758, recall 0.828307
2017-12-09T22:25:22.085555: step 1629, loss 0.161454, acc 0.945312, prec 0.066266, recall 0.828307
2017-12-09T22:25:22.387119: step 1630, loss 0.283011, acc 0.90625, prec 0.0662689, recall 0.828352
2017-12-09T22:25:22.686860: step 1631, loss 0.234355, acc 0.914062, prec 0.0663325, recall 0.828534
2017-12-09T22:25:22.985210: step 1632, loss 0.837827, acc 0.875, prec 0.0663693, recall 0.828669
2017-12-09T22:25:23.284104: step 1633, loss 0.393317, acc 0.890625, prec 0.0663891, recall 0.82876
2017-12-09T22:25:23.583548: step 1634, loss 0.339246, acc 0.898438, prec 0.0664103, recall 0.82885
2017-12-09T22:25:23.884226: step 1635, loss 0.312942, acc 0.914062, prec 0.066454, recall 0.828986
2017-12-09T22:25:24.182755: step 1636, loss 0.325184, acc 0.921875, prec 0.0664794, recall 0.829076
2017-12-09T22:25:24.481865: step 1637, loss 0.326523, acc 0.882812, prec 0.0664584, recall 0.829076
2017-12-09T22:25:24.774650: step 1638, loss 0.863561, acc 0.929688, prec 0.066564, recall 0.829345
2017-12-09T22:25:25.079461: step 1639, loss 0.27888, acc 0.914062, prec 0.0666076, recall 0.82948
2017-12-09T22:25:25.381151: step 1640, loss 0.431324, acc 0.851562, prec 0.0666006, recall 0.829525
2017-12-09T22:25:25.679567: step 1641, loss 0.291382, acc 0.914062, prec 0.0666245, recall 0.829614
2017-12-09T22:25:25.976851: step 1642, loss 0.628682, acc 0.914062, prec 0.0667074, recall 0.829837
2017-12-09T22:25:26.277573: step 1643, loss 0.321526, acc 0.914062, prec 0.0667706, recall 0.830016
2017-12-09T22:25:26.577884: step 1644, loss 0.407418, acc 0.882812, prec 0.0668281, recall 0.830194
2017-12-09T22:25:26.881653: step 1645, loss 0.312158, acc 0.914062, prec 0.066852, recall 0.830282
2017-12-09T22:25:27.177618: step 1646, loss 0.313594, acc 0.90625, prec 0.0669136, recall 0.83046
2017-12-09T22:25:27.474250: step 1647, loss 0.262654, acc 0.914062, prec 0.0669374, recall 0.830548
2017-12-09T22:25:27.772438: step 1648, loss 0.105662, acc 0.953125, prec 0.066929, recall 0.830548
2017-12-09T22:25:28.078770: step 1649, loss 0.717586, acc 0.96875, prec 0.0669626, recall 0.830637
2017-12-09T22:25:28.380409: step 1650, loss 0.13372, acc 0.96875, prec 0.0669766, recall 0.830681
2017-12-09T22:25:28.679231: step 1651, loss 0.277364, acc 0.90625, prec 0.0670185, recall 0.830813
2017-12-09T22:25:28.980712: step 1652, loss 0.112643, acc 0.960938, prec 0.0670311, recall 0.830857
2017-12-09T22:25:29.283030: step 1653, loss 0.386238, acc 0.953125, prec 0.0671207, recall 0.831078
2017-12-09T22:25:29.584634: step 1654, loss 0.239568, acc 0.9375, prec 0.067129, recall 0.831122
2017-12-09T22:25:29.885600: step 1655, loss 0.476777, acc 0.921875, prec 0.0671737, recall 0.831253
2017-12-09T22:25:30.192522: step 1656, loss 0.185225, acc 0.9375, prec 0.0672016, recall 0.831341
2017-12-09T22:25:30.496278: step 1657, loss 0.618061, acc 0.960938, prec 0.0672548, recall 0.831257
2017-12-09T22:25:30.794199: step 1658, loss 0.137739, acc 0.945312, prec 0.0672645, recall 0.8313
2017-12-09T22:25:31.094805: step 1659, loss 0.244535, acc 0.953125, prec 0.0672756, recall 0.831344
2017-12-09T22:25:31.392587: step 1660, loss 0.251179, acc 0.898438, prec 0.0672768, recall 0.831388
2017-12-09T22:25:31.688605: step 1661, loss 0.165807, acc 0.945312, prec 0.0672669, recall 0.831388
2017-12-09T22:25:31.992949: step 1662, loss 0.186456, acc 0.9375, prec 0.0672556, recall 0.831388
2017-12-09T22:25:32.292700: step 1663, loss 0.17103, acc 0.914062, prec 0.0672401, recall 0.831388
2017-12-09T22:25:32.592654: step 1664, loss 0.185853, acc 0.945312, prec 0.0672498, recall 0.831432
2017-12-09T22:25:32.898977: step 1665, loss 0.293067, acc 0.945312, prec 0.0672986, recall 0.831563
2017-12-09T22:25:33.197382: step 1666, loss 0.265025, acc 0.921875, prec 0.0673236, recall 0.83165
2017-12-09T22:25:33.492808: step 1667, loss 0.352719, acc 0.914062, prec 0.0673276, recall 0.831693
2017-12-09T22:25:33.791295: step 1668, loss 0.298005, acc 0.898438, prec 0.0673484, recall 0.831781
2017-12-09T22:25:34.091686: step 1669, loss 0.0507408, acc 0.992188, prec 0.0673861, recall 0.831868
2017-12-09T22:25:34.393264: step 1670, loss 0.198845, acc 0.945312, prec 0.0674348, recall 0.831998
2017-12-09T22:25:34.690539: step 1671, loss 0.209762, acc 0.960938, prec 0.0674473, recall 0.832041
2017-12-09T22:25:34.990378: step 1672, loss 1.65983, acc 0.953125, prec 0.0674402, recall 0.831826
2017-12-09T22:25:35.297203: step 1673, loss 0.146488, acc 0.960938, prec 0.0674917, recall 0.831957
2017-12-09T22:25:35.598548: step 1674, loss 0.275578, acc 0.960938, prec 0.0675237, recall 0.832043
2017-12-09T22:25:35.901079: step 1675, loss 0.0758782, acc 0.96875, prec 0.0675376, recall 0.832087
2017-12-09T22:25:36.200581: step 1676, loss 0.354209, acc 0.945312, prec 0.0676057, recall 0.83226
2017-12-09T22:25:36.499320: step 1677, loss 0.251465, acc 0.914062, prec 0.0676097, recall 0.832303
2017-12-09T22:25:36.794154: step 1678, loss 0.305638, acc 0.898438, prec 0.0675913, recall 0.832303
2017-12-09T22:25:37.089956: step 1679, loss 0.168173, acc 0.929688, prec 0.0675981, recall 0.832346
2017-12-09T22:25:37.384312: step 1680, loss 0.218059, acc 0.945312, prec 0.0676272, recall 0.832432
2017-12-09T22:25:37.682508: step 1681, loss 0.2162, acc 0.929688, prec 0.0676729, recall 0.832562
2017-12-09T22:25:37.980516: step 1682, loss 0.340386, acc 0.953125, prec 0.0677229, recall 0.832691
2017-12-09T22:25:38.283889: step 1683, loss 0.302928, acc 0.976562, prec 0.0677576, recall 0.832777
2017-12-09T22:25:38.589532: step 1684, loss 0.163457, acc 0.9375, prec 0.0677658, recall 0.83282
2017-12-09T22:25:38.891289: step 1685, loss 2.065, acc 0.945312, prec 0.0677963, recall 0.832692
2017-12-09T22:25:39.198710: step 1686, loss 0.223669, acc 0.9375, prec 0.0677849, recall 0.832692
2017-12-09T22:25:39.501000: step 1687, loss 0.339512, acc 0.90625, prec 0.0678069, recall 0.832778
2017-12-09T22:25:39.800134: step 1688, loss 0.18485, acc 0.921875, prec 0.0678122, recall 0.832821
2017-12-09T22:25:40.097229: step 1689, loss 0.436964, acc 0.898438, prec 0.0678716, recall 0.832992
2017-12-09T22:25:40.400340: step 1690, loss 0.262247, acc 0.898438, prec 0.0678726, recall 0.833035
2017-12-09T22:25:40.697770: step 1691, loss 0.260805, acc 0.90625, prec 0.0678557, recall 0.833035
2017-12-09T22:25:40.999701: step 1692, loss 0.378651, acc 0.882812, prec 0.0679122, recall 0.833205
2017-12-09T22:25:41.295156: step 1693, loss 0.283029, acc 0.882812, prec 0.0678909, recall 0.833205
2017-12-09T22:25:41.595743: step 1694, loss 0.444998, acc 0.859375, prec 0.0678655, recall 0.833205
2017-12-09T22:25:41.896407: step 1695, loss 0.17517, acc 0.9375, prec 0.067893, recall 0.833291
2017-12-09T22:25:42.191430: step 1696, loss 0.201646, acc 0.90625, prec 0.0679149, recall 0.833376
2017-12-09T22:25:42.495241: step 1697, loss 0.259639, acc 0.898438, prec 0.0679353, recall 0.833461
2017-12-09T22:25:42.796094: step 1698, loss 0.326838, acc 0.914062, prec 0.0679779, recall 0.833589
2017-12-09T22:25:43.093753: step 1699, loss 0.0854901, acc 0.96875, prec 0.0679917, recall 0.833631
2017-12-09T22:25:43.390055: step 1700, loss 0.249653, acc 0.914062, prec 0.0680537, recall 0.833801
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1700

2017-12-09T22:25:44.693444: step 1701, loss 0.164615, acc 0.9375, prec 0.0680617, recall 0.833843
2017-12-09T22:25:44.995820: step 1702, loss 0.604824, acc 0.953125, prec 0.0681114, recall 0.83397
2017-12-09T22:25:45.298568: step 1703, loss 3.08693, acc 0.953125, prec 0.0681043, recall 0.833758
2017-12-09T22:25:45.604006: step 1704, loss 0.601073, acc 0.945312, prec 0.0681719, recall 0.833927
2017-12-09T22:25:45.905096: step 1705, loss 0.636796, acc 0.945312, prec 0.0682007, recall 0.834011
2017-12-09T22:25:46.207446: step 1706, loss 0.310939, acc 0.90625, prec 0.0682031, recall 0.834053
2017-12-09T22:25:46.504853: step 1707, loss 0.381699, acc 0.921875, prec 0.0682276, recall 0.834138
2017-12-09T22:25:46.802601: step 1708, loss 0.138659, acc 0.945312, prec 0.0682371, recall 0.83418
2017-12-09T22:25:47.102031: step 1709, loss 0.402789, acc 0.890625, prec 0.0682366, recall 0.834222
2017-12-09T22:25:47.399192: step 1710, loss 0.203693, acc 0.90625, prec 0.0682196, recall 0.834222
2017-12-09T22:25:47.694520: step 1711, loss 0.157374, acc 0.953125, prec 0.0682304, recall 0.834264
2017-12-09T22:25:47.989880: step 1712, loss 0.358533, acc 0.90625, prec 0.0682714, recall 0.83439
2017-12-09T22:25:48.292446: step 1713, loss 0.251421, acc 0.90625, prec 0.0682931, recall 0.834474
2017-12-09T22:25:48.591058: step 1714, loss 0.225996, acc 0.921875, prec 0.0682789, recall 0.834474
2017-12-09T22:25:48.888198: step 1715, loss 0.322186, acc 0.898438, prec 0.0683378, recall 0.834642
2017-12-09T22:25:49.182056: step 1716, loss 0.306582, acc 0.882812, prec 0.0683165, recall 0.834642
2017-12-09T22:25:49.480935: step 1717, loss 0.179252, acc 0.921875, prec 0.0683217, recall 0.834684
2017-12-09T22:25:49.781802: step 1718, loss 0.229661, acc 0.9375, prec 0.0683683, recall 0.834809
2017-12-09T22:25:50.081332: step 1719, loss 0.652124, acc 0.9375, prec 0.0683762, recall 0.834851
2017-12-09T22:25:50.394328: step 1720, loss 0.201352, acc 0.929688, prec 0.0684021, recall 0.834934
2017-12-09T22:25:50.693592: step 1721, loss 0.276667, acc 0.921875, prec 0.0684072, recall 0.834976
2017-12-09T22:25:50.995997: step 1722, loss 0.19964, acc 0.9375, prec 0.0684345, recall 0.835059
2017-12-09T22:25:51.293961: step 1723, loss 0.0934364, acc 0.960938, prec 0.0684467, recall 0.835101
2017-12-09T22:25:51.589518: step 1724, loss 0.338032, acc 0.929688, prec 0.0684725, recall 0.835184
2017-12-09T22:25:51.887360: step 1725, loss 0.270435, acc 0.9375, prec 0.0684804, recall 0.835226
2017-12-09T22:25:52.189830: step 1726, loss 0.634624, acc 0.929688, prec 0.0685062, recall 0.835309
2017-12-09T22:25:52.488928: step 1727, loss 0.550753, acc 0.9375, prec 0.0685719, recall 0.835475
2017-12-09T22:25:52.791171: step 1728, loss 0.241827, acc 0.953125, prec 0.0686212, recall 0.835599
2017-12-09T22:25:53.092778: step 1729, loss 0.190825, acc 0.9375, prec 0.0686291, recall 0.835641
2017-12-09T22:25:53.395353: step 1730, loss 0.508057, acc 0.921875, prec 0.0686919, recall 0.835806
2017-12-09T22:25:53.695540: step 1731, loss 0.368985, acc 0.921875, prec 0.0686969, recall 0.835847
2017-12-09T22:25:53.992832: step 1732, loss 0.232318, acc 0.890625, prec 0.0687155, recall 0.83593
2017-12-09T22:25:54.287298: step 1733, loss 0.19029, acc 0.953125, prec 0.0687647, recall 0.836053
2017-12-09T22:25:54.585928: step 1734, loss 0.18163, acc 0.9375, prec 0.0687918, recall 0.836136
2017-12-09T22:25:54.881468: step 1735, loss 0.143757, acc 0.9375, prec 0.0687804, recall 0.836136
2017-12-09T22:25:55.181330: step 1736, loss 0.2115, acc 0.929688, prec 0.0687869, recall 0.836177
2017-12-09T22:25:55.484995: step 1737, loss 0.196376, acc 0.929688, prec 0.0688125, recall 0.836259
2017-12-09T22:25:55.786797: step 1738, loss 0.168933, acc 0.953125, prec 0.0688232, recall 0.8363
2017-12-09T22:25:56.085665: step 1739, loss 0.21122, acc 0.921875, prec 0.0688282, recall 0.836341
2017-12-09T22:25:56.383186: step 1740, loss 0.406979, acc 0.9375, prec 0.0689129, recall 0.836546
2017-12-09T22:25:56.683611: step 1741, loss 0.213281, acc 0.914062, prec 0.0688973, recall 0.836546
2017-12-09T22:25:56.982623: step 1742, loss 1.17724, acc 0.921875, prec 0.0689229, recall 0.836418
2017-12-09T22:25:57.159467: step 1743, loss 0.0613659, acc 1, prec 0.0689229, recall 0.836418
2017-12-09T22:25:57.461535: step 1744, loss 0.2572, acc 0.914062, prec 0.0689264, recall 0.836459
2017-12-09T22:25:57.760441: step 1745, loss 0.156493, acc 0.945312, prec 0.0689549, recall 0.836541
2017-12-09T22:25:58.064633: step 1746, loss 0.155198, acc 0.960938, prec 0.0689669, recall 0.836582
2017-12-09T22:25:58.361640: step 1747, loss 0.127871, acc 0.9375, prec 0.0689747, recall 0.836623
2017-12-09T22:25:58.664590: step 1748, loss 0.159866, acc 0.945312, prec 0.0690415, recall 0.836786
2017-12-09T22:25:58.962376: step 1749, loss 0.149991, acc 0.945312, prec 0.0690507, recall 0.836826
2017-12-09T22:25:59.257550: step 1750, loss 0.159317, acc 0.960938, prec 0.0691011, recall 0.836948
2017-12-09T22:25:59.556548: step 1751, loss 0.131815, acc 0.953125, prec 0.0690926, recall 0.836948
2017-12-09T22:25:59.862340: step 1752, loss 0.187699, acc 0.9375, prec 0.0691386, recall 0.83707
2017-12-09T22:26:00.173197: step 1753, loss 0.225979, acc 0.96875, prec 0.0691521, recall 0.837111
2017-12-09T22:26:00.474035: step 1754, loss 0.109218, acc 0.992188, prec 0.069189, recall 0.837192
2017-12-09T22:26:00.770381: step 1755, loss 2.36695, acc 0.90625, prec 0.0692308, recall 0.837105
2017-12-09T22:26:01.078924: step 1756, loss 1.31008, acc 0.945312, prec 0.0692988, recall 0.837059
2017-12-09T22:26:01.380497: step 1757, loss 0.577918, acc 0.921875, prec 0.0693228, recall 0.83714
2017-12-09T22:26:01.678822: step 1758, loss 0.270113, acc 0.898438, prec 0.0693808, recall 0.837302
2017-12-09T22:26:01.977881: step 1759, loss 0.335657, acc 0.875, prec 0.0693771, recall 0.837342
2017-12-09T22:26:02.269726: step 1760, loss 0.339126, acc 0.890625, prec 0.0694145, recall 0.837463
2017-12-09T22:26:02.567741: step 1761, loss 0.310676, acc 0.929688, prec 0.0694399, recall 0.837543
2017-12-09T22:26:02.869267: step 1762, loss 0.348859, acc 0.890625, prec 0.069439, recall 0.837584
2017-12-09T22:26:03.162119: step 1763, loss 0.550113, acc 0.828125, prec 0.0694077, recall 0.837584
2017-12-09T22:26:03.469659: step 1764, loss 0.334144, acc 0.875, prec 0.069404, recall 0.837624
2017-12-09T22:26:03.769476: step 1765, loss 0.447986, acc 0.851562, prec 0.069377, recall 0.837624
2017-12-09T22:26:04.071258: step 1766, loss 0.43505, acc 0.859375, prec 0.0694086, recall 0.837744
2017-12-09T22:26:04.381030: step 1767, loss 0.271289, acc 0.882812, prec 0.0693873, recall 0.837744
2017-12-09T22:26:04.676059: step 1768, loss 0.197148, acc 0.921875, prec 0.0694112, recall 0.837824
2017-12-09T22:26:04.974985: step 1769, loss 0.353733, acc 0.898438, prec 0.0694308, recall 0.837905
2017-12-09T22:26:05.285943: step 1770, loss 0.233212, acc 0.9375, prec 0.0694575, recall 0.837985
2017-12-09T22:26:05.582831: step 1771, loss 0.189927, acc 0.953125, prec 0.0695061, recall 0.838105
2017-12-09T22:26:05.879428: step 1772, loss 0.178733, acc 0.945312, prec 0.0695343, recall 0.838185
2017-12-09T22:26:06.179036: step 1773, loss 0.121165, acc 0.96875, prec 0.0695476, recall 0.838224
2017-12-09T22:26:06.478196: step 1774, loss 0.172371, acc 0.960938, prec 0.0696166, recall 0.838384
2017-12-09T22:26:06.786413: step 1775, loss 0.345857, acc 0.953125, prec 0.0696271, recall 0.838424
2017-12-09T22:26:07.084044: step 1776, loss 0.640326, acc 0.9375, prec 0.0696348, recall 0.838463
2017-12-09T22:26:07.386049: step 1777, loss 0.0833735, acc 0.976562, prec 0.0696495, recall 0.838503
2017-12-09T22:26:07.686259: step 1778, loss 0.167716, acc 0.945312, prec 0.0696586, recall 0.838543
2017-12-09T22:26:07.988368: step 1779, loss 0.150663, acc 0.976562, prec 0.0696733, recall 0.838583
2017-12-09T22:26:08.289301: step 1780, loss 0.12571, acc 0.96875, prec 0.0696676, recall 0.838583
2017-12-09T22:26:08.586312: step 1781, loss 0.641957, acc 0.945312, prec 0.0696591, recall 0.838376
2017-12-09T22:26:08.890583: step 1782, loss 0.155351, acc 0.960938, prec 0.06969, recall 0.838456
2017-12-09T22:26:09.192534: step 1783, loss 0.23685, acc 0.992188, prec 0.0697076, recall 0.838496
2017-12-09T22:26:09.490357: step 1784, loss 0.0782167, acc 0.976562, prec 0.0697223, recall 0.838535
2017-12-09T22:26:09.789643: step 1785, loss 0.0615694, acc 0.984375, prec 0.0697765, recall 0.838654
2017-12-09T22:26:10.092203: step 1786, loss 0.0796277, acc 0.984375, prec 0.0697926, recall 0.838694
2017-12-09T22:26:10.388816: step 1787, loss 0.137285, acc 0.984375, prec 0.0698088, recall 0.838733
2017-12-09T22:26:10.687465: step 1788, loss 0.162813, acc 0.976562, prec 0.0698805, recall 0.838892
2017-12-09T22:26:10.985653: step 1789, loss 0.562154, acc 0.921875, prec 0.0699042, recall 0.838971
2017-12-09T22:26:11.288227: step 1790, loss 0.147591, acc 0.96875, prec 0.0699365, recall 0.839049
2017-12-09T22:26:11.586563: step 1791, loss 0.132806, acc 0.96875, prec 0.0699498, recall 0.839089
2017-12-09T22:26:11.887708: step 1792, loss 0.173861, acc 0.914062, prec 0.0699531, recall 0.839128
2017-12-09T22:26:12.186036: step 1793, loss 0.22499, acc 0.96875, prec 0.0700043, recall 0.839246
2017-12-09T22:26:12.486135: step 1794, loss 0.345091, acc 0.945312, prec 0.0700702, recall 0.839404
2017-12-09T22:26:12.784693: step 1795, loss 0.299042, acc 0.960938, prec 0.0701579, recall 0.8396
2017-12-09T22:26:13.083046: step 1796, loss 0.534552, acc 0.9375, prec 0.0702033, recall 0.839717
2017-12-09T22:26:13.383575: step 1797, loss 0.24131, acc 0.890625, prec 0.0702402, recall 0.839834
2017-12-09T22:26:13.683657: step 1798, loss 0.145907, acc 0.960938, prec 0.070233, recall 0.839834
2017-12-09T22:26:13.981529: step 1799, loss 0.248555, acc 0.914062, prec 0.0702173, recall 0.839834
2017-12-09T22:26:14.279906: step 1800, loss 0.255754, acc 0.90625, prec 0.0702569, recall 0.839951

Evaluation:
2017-12-09T22:26:19.020817: step 1800, loss 1.87671, acc 0.920276, prec 0.0710197, recall 0.831108

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1800

2017-12-09T22:26:20.260584: step 1801, loss 0.434021, acc 0.867188, prec 0.071107, recall 0.831345
2017-12-09T22:26:20.563429: step 1802, loss 0.137512, acc 0.945312, prec 0.0711156, recall 0.831384
2017-12-09T22:26:20.860913: step 1803, loss 0.21157, acc 0.929688, prec 0.0711772, recall 0.831542
2017-12-09T22:26:21.163805: step 1804, loss 0.233079, acc 0.9375, prec 0.0711843, recall 0.831581
2017-12-09T22:26:21.466043: step 1805, loss 0.149156, acc 0.960938, prec 0.0712144, recall 0.83166
2017-12-09T22:26:21.774360: step 1806, loss 0.0863756, acc 0.96875, prec 0.0712272, recall 0.831699
2017-12-09T22:26:22.076002: step 1807, loss 0.0662241, acc 0.976562, prec 0.0712415, recall 0.831739
2017-12-09T22:26:22.383237: step 1808, loss 0.241081, acc 0.960938, prec 0.0712901, recall 0.831856
2017-12-09T22:26:22.682545: step 1809, loss 0.161954, acc 0.976562, prec 0.0713415, recall 0.831974
2017-12-09T22:26:22.984669: step 1810, loss 0.225244, acc 0.9375, prec 0.0713301, recall 0.831974
2017-12-09T22:26:23.281766: step 1811, loss 0.159483, acc 0.953125, prec 0.0713401, recall 0.832013
2017-12-09T22:26:23.581391: step 1812, loss 0.101044, acc 0.976562, prec 0.0713544, recall 0.832052
2017-12-09T22:26:23.879150: step 1813, loss 0.183848, acc 0.976562, prec 0.0713872, recall 0.83213
2017-12-09T22:26:24.181063: step 1814, loss 0.0646433, acc 0.984375, prec 0.0713843, recall 0.83213
2017-12-09T22:26:24.476932: step 1815, loss 0.0339727, acc 0.992188, prec 0.0713829, recall 0.83213
2017-12-09T22:26:24.776701: step 1816, loss 0.0385139, acc 0.992188, prec 0.0714, recall 0.832169
2017-12-09T22:26:25.076839: step 1817, loss 0.18262, acc 0.953125, prec 0.0714286, recall 0.832248
2017-12-09T22:26:25.380738: step 1818, loss 0.0509473, acc 0.984375, prec 0.0714257, recall 0.832248
2017-12-09T22:26:25.679153: step 1819, loss 0.261525, acc 0.960938, prec 0.0714927, recall 0.832404
2017-12-09T22:26:25.978975: step 1820, loss 0.192151, acc 0.953125, prec 0.0715583, recall 0.832559
2017-12-09T22:26:26.283799: step 1821, loss 0.213071, acc 0.984375, prec 0.0715925, recall 0.832637
2017-12-09T22:26:26.582698: step 1822, loss 0.0475827, acc 0.976562, prec 0.0715882, recall 0.832637
2017-12-09T22:26:26.877151: step 1823, loss 0.0918908, acc 0.976562, prec 0.0716395, recall 0.832753
2017-12-09T22:26:27.185512: step 1824, loss 0.648987, acc 0.960938, prec 0.0716523, recall 0.832599
2017-12-09T22:26:27.489163: step 1825, loss 0.143, acc 0.960938, prec 0.0716822, recall 0.832677
2017-12-09T22:26:27.790524: step 1826, loss 0.283207, acc 0.960938, prec 0.0716936, recall 0.832715
2017-12-09T22:26:28.090744: step 1827, loss 0.158622, acc 0.960938, prec 0.071705, recall 0.832754
2017-12-09T22:26:28.387283: step 1828, loss 0.184985, acc 0.992188, prec 0.071722, recall 0.832793
2017-12-09T22:26:28.683827: step 1829, loss 0.217994, acc 0.984375, prec 0.0717562, recall 0.83287
2017-12-09T22:26:28.987170: step 1830, loss 0.181457, acc 0.976562, prec 0.071826, recall 0.833025
2017-12-09T22:26:29.290118: step 1831, loss 0.2359, acc 0.953125, prec 0.0718729, recall 0.833141
2017-12-09T22:26:29.591198: step 1832, loss 0.077948, acc 0.984375, prec 0.0718885, recall 0.833179
2017-12-09T22:26:29.899251: step 1833, loss 0.153426, acc 0.960938, prec 0.0719184, recall 0.833256
2017-12-09T22:26:30.207187: step 1834, loss 0.253735, acc 0.882812, prec 0.0719338, recall 0.833333
2017-12-09T22:26:30.505036: step 1835, loss 0.226783, acc 0.921875, prec 0.071938, recall 0.833372
2017-12-09T22:26:30.800877: step 1836, loss 0.223004, acc 0.921875, prec 0.0719237, recall 0.833372
2017-12-09T22:26:31.096655: step 1837, loss 0.192638, acc 0.945312, prec 0.0719136, recall 0.833372
2017-12-09T22:26:31.397987: step 1838, loss 0.155661, acc 0.9375, prec 0.0719391, recall 0.833449
2017-12-09T22:26:31.690754: step 1839, loss 0.14056, acc 0.945312, prec 0.0719291, recall 0.833449
2017-12-09T22:26:31.990716: step 1840, loss 0.120763, acc 0.976562, prec 0.0719803, recall 0.833564
2017-12-09T22:26:32.287680: step 1841, loss 0.0880564, acc 0.976562, prec 0.0719944, recall 0.833602
2017-12-09T22:26:32.585294: step 1842, loss 0.134047, acc 0.960938, prec 0.0720057, recall 0.833641
2017-12-09T22:26:32.884946: step 1843, loss 0.194057, acc 0.945312, prec 0.0720142, recall 0.833679
2017-12-09T22:26:33.185758: step 1844, loss 0.789795, acc 0.960938, prec 0.0720269, recall 0.833525
2017-12-09T22:26:33.485188: step 1845, loss 0.0931107, acc 0.96875, prec 0.0720581, recall 0.833602
2017-12-09T22:26:33.784349: step 1846, loss 0.133324, acc 0.945312, prec 0.0720665, recall 0.83364
2017-12-09T22:26:34.079376: step 1847, loss 0.125465, acc 0.976562, prec 0.0721176, recall 0.833755
2017-12-09T22:26:34.385914: step 1848, loss 0.0748946, acc 0.96875, prec 0.0721118, recall 0.833755
2017-12-09T22:26:34.687908: step 1849, loss 0.528003, acc 0.9375, prec 0.0721557, recall 0.833869
2017-12-09T22:26:34.987246: step 1850, loss 0.100174, acc 0.960938, prec 0.072167, recall 0.833908
2017-12-09T22:26:35.294743: step 1851, loss 0.099123, acc 0.96875, prec 0.0721797, recall 0.833946
2017-12-09T22:26:35.590979: step 1852, loss 0.241423, acc 0.96875, prec 0.0722108, recall 0.834022
2017-12-09T22:26:35.896258: step 1853, loss 0.266969, acc 0.921875, prec 0.0722149, recall 0.83406
2017-12-09T22:26:36.199452: step 1854, loss 0.0791871, acc 0.984375, prec 0.0722489, recall 0.834136
2017-12-09T22:26:36.498228: step 1855, loss 0.179588, acc 0.929688, prec 0.0722544, recall 0.834174
2017-12-09T22:26:36.792626: step 1856, loss 0.506866, acc 0.914062, prec 0.0722939, recall 0.834288
2017-12-09T22:26:37.095045: step 1857, loss 0.243585, acc 0.96875, prec 0.072325, recall 0.834364
2017-12-09T22:26:37.396335: step 1858, loss 0.381653, acc 0.921875, prec 0.0723107, recall 0.834364
2017-12-09T22:26:37.694969: step 1859, loss 0.131499, acc 0.960938, prec 0.0723035, recall 0.834364
2017-12-09T22:26:37.990928: step 1860, loss 0.271896, acc 0.96875, prec 0.0723346, recall 0.83444
2017-12-09T22:26:38.286257: step 1861, loss 0.15515, acc 0.960938, prec 0.0723827, recall 0.834554
2017-12-09T22:26:38.589786: step 1862, loss 0.420757, acc 0.9375, prec 0.072408, recall 0.834629
2017-12-09T22:26:38.889809: step 1863, loss 0.280112, acc 0.953125, prec 0.0724362, recall 0.834705
2017-12-09T22:26:39.189970: step 1864, loss 0.0833551, acc 0.984375, prec 0.0724517, recall 0.834743
2017-12-09T22:26:39.490660: step 1865, loss 0.138721, acc 0.960938, prec 0.0724445, recall 0.834743
2017-12-09T22:26:39.792277: step 1866, loss 0.192294, acc 0.953125, prec 0.0724911, recall 0.834856
2017-12-09T22:26:40.087254: step 1867, loss 0.162988, acc 0.929688, prec 0.0725149, recall 0.834931
2017-12-09T22:26:40.380960: step 1868, loss 0.133408, acc 0.953125, prec 0.0725063, recall 0.834931
2017-12-09T22:26:40.676309: step 1869, loss 0.196122, acc 0.9375, prec 0.0725132, recall 0.834969
2017-12-09T22:26:40.976999: step 1870, loss 0.613211, acc 0.96875, prec 0.0725626, recall 0.835082
2017-12-09T22:26:41.278774: step 1871, loss 0.359456, acc 0.945312, prec 0.0725893, recall 0.835157
2017-12-09T22:26:41.577905: step 1872, loss 0.19786, acc 0.9375, prec 0.0725961, recall 0.835195
2017-12-09T22:26:41.880217: step 1873, loss 0.170222, acc 0.953125, prec 0.0726243, recall 0.83527
2017-12-09T22:26:42.176886: step 1874, loss 0.214183, acc 0.945312, prec 0.0726509, recall 0.835345
2017-12-09T22:26:42.473576: step 1875, loss 0.28518, acc 0.914062, prec 0.0726535, recall 0.835383
2017-12-09T22:26:42.775785: step 1876, loss 0.168586, acc 0.9375, prec 0.072697, recall 0.835495
2017-12-09T22:26:43.075596: step 1877, loss 0.0935753, acc 0.960938, prec 0.0727082, recall 0.835532
2017-12-09T22:26:43.375798: step 1878, loss 0.194221, acc 0.921875, prec 0.0727122, recall 0.83557
2017-12-09T22:26:43.681465: step 1879, loss 0.187672, acc 0.960938, prec 0.0727967, recall 0.835756
2017-12-09T22:26:43.980080: step 1880, loss 0.114414, acc 0.953125, prec 0.0728248, recall 0.835831
2017-12-09T22:26:44.280870: step 1881, loss 0.613141, acc 0.984375, prec 0.0728952, recall 0.83598
2017-12-09T22:26:44.582682: step 1882, loss 0.188246, acc 0.929688, prec 0.0729006, recall 0.836017
2017-12-09T22:26:44.878623: step 1883, loss 0.261718, acc 0.929688, prec 0.0729243, recall 0.836092
2017-12-09T22:26:45.177590: step 1884, loss 0.133793, acc 0.953125, prec 0.0729706, recall 0.836203
2017-12-09T22:26:45.475128: step 1885, loss 0.435513, acc 0.960938, prec 0.0730367, recall 0.836351
2017-12-09T22:26:45.774612: step 1886, loss 0.469266, acc 0.914062, prec 0.0730391, recall 0.836388
2017-12-09T22:26:46.075432: step 1887, loss 0.268982, acc 0.96875, prec 0.07307, recall 0.836462
2017-12-09T22:26:46.374257: step 1888, loss 0.159206, acc 0.929688, prec 0.0730753, recall 0.836499
2017-12-09T22:26:46.673520: step 1889, loss 0.137785, acc 0.953125, prec 0.0731033, recall 0.836573
2017-12-09T22:26:46.976037: step 1890, loss 0.165714, acc 0.953125, prec 0.0731495, recall 0.836684
2017-12-09T22:26:47.271794: step 1891, loss 0.266963, acc 0.898438, prec 0.0731857, recall 0.836795
2017-12-09T22:26:47.572180: step 1892, loss 0.207464, acc 0.953125, prec 0.073177, recall 0.836795
2017-12-09T22:26:47.865506: step 1893, loss 0.261037, acc 0.929688, prec 0.0731823, recall 0.836831
2017-12-09T22:26:48.163495: step 1894, loss 0.169135, acc 0.96875, prec 0.0731948, recall 0.836868
2017-12-09T22:26:48.460814: step 1895, loss 0.161269, acc 0.953125, prec 0.0732227, recall 0.836942
2017-12-09T22:26:48.757542: step 1896, loss 0.179413, acc 0.960938, prec 0.0732521, recall 0.837015
2017-12-09T22:26:49.058551: step 1897, loss 0.184191, acc 0.929688, prec 0.0732939, recall 0.837125
2017-12-09T22:26:49.357269: step 1898, loss 0.135568, acc 0.953125, prec 0.0733218, recall 0.837199
2017-12-09T22:26:49.655354: step 1899, loss 0.0558373, acc 0.96875, prec 0.073316, recall 0.837199
2017-12-09T22:26:49.955051: step 1900, loss 0.0795719, acc 0.976562, prec 0.0733665, recall 0.837309
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-1900

2017-12-09T22:26:51.217277: step 1901, loss 0.0802917, acc 0.984375, prec 0.0734184, recall 0.837418
2017-12-09T22:26:51.518064: step 1902, loss 0.440845, acc 0.945312, prec 0.0734265, recall 0.837455
2017-12-09T22:26:51.820460: step 1903, loss 0.2197, acc 0.9375, prec 0.0734149, recall 0.837455
2017-12-09T22:26:52.119891: step 1904, loss 0.116825, acc 0.984375, prec 0.0734486, recall 0.837528
2017-12-09T22:26:52.418186: step 1905, loss 0.898703, acc 0.914062, prec 0.0734706, recall 0.837413
2017-12-09T22:26:52.720462: step 1906, loss 0.144359, acc 0.984375, prec 0.0735042, recall 0.837486
2017-12-09T22:26:53.021484: step 1907, loss 0.121922, acc 0.984375, prec 0.0735378, recall 0.837559
2017-12-09T22:26:53.317870: step 1908, loss 0.0545417, acc 0.984375, prec 0.0735532, recall 0.837595
2017-12-09T22:26:53.616551: step 1909, loss 0.189846, acc 0.96875, prec 0.0736204, recall 0.837741
2017-12-09T22:26:53.917397: step 1910, loss 0.359241, acc 0.976562, prec 0.073689, recall 0.837886
2017-12-09T22:26:54.216310: step 1911, loss 0.158804, acc 0.945312, prec 0.0736971, recall 0.837923
2017-12-09T22:26:54.512627: step 1912, loss 0.177679, acc 0.929688, prec 0.0737022, recall 0.837959
2017-12-09T22:26:54.810846: step 1913, loss 0.0816978, acc 0.953125, prec 0.0736935, recall 0.837959
2017-12-09T22:26:55.105378: step 1914, loss 0.14807, acc 0.960938, prec 0.0737045, recall 0.837995
2017-12-09T22:26:55.403816: step 1915, loss 0.17157, acc 0.945312, prec 0.0736944, recall 0.837995
2017-12-09T22:26:55.700255: step 1916, loss 0.149165, acc 0.953125, prec 0.073795, recall 0.838212
2017-12-09T22:26:55.998204: step 1917, loss 0.25555, acc 0.929688, prec 0.0738366, recall 0.838321
2017-12-09T22:26:56.297654: step 1918, loss 0.49712, acc 0.984375, prec 0.0739248, recall 0.838501
2017-12-09T22:26:56.596181: step 1919, loss 0.310233, acc 0.96875, prec 0.0739372, recall 0.838537
2017-12-09T22:26:56.897605: step 1920, loss 0.207673, acc 0.945312, prec 0.0739634, recall 0.838609
2017-12-09T22:26:57.191128: step 1921, loss 1.0692, acc 0.929688, prec 0.0740246, recall 0.838566
2017-12-09T22:26:57.490615: step 1922, loss 0.298703, acc 0.90625, prec 0.0740799, recall 0.83871
2017-12-09T22:26:57.787122: step 1923, loss 0.155502, acc 0.9375, prec 0.0740864, recall 0.838746
2017-12-09T22:26:58.090688: step 1924, loss 0.26268, acc 0.914062, prec 0.0741068, recall 0.838817
2017-12-09T22:26:58.389443: step 1925, loss 0.528181, acc 0.945312, prec 0.0741512, recall 0.838925
2017-12-09T22:26:58.686871: step 1926, loss 0.20418, acc 0.9375, prec 0.0741759, recall 0.838996
2017-12-09T22:26:58.987417: step 1927, loss 0.533072, acc 0.921875, prec 0.0742158, recall 0.839103
2017-12-09T22:26:59.284926: step 1928, loss 0.386709, acc 0.875, prec 0.0742289, recall 0.839175
2017-12-09T22:26:59.585480: step 1929, loss 0.329124, acc 0.867188, prec 0.0742767, recall 0.839317
2017-12-09T22:26:59.892526: step 1930, loss 0.372099, acc 0.882812, prec 0.0742549, recall 0.839317
2017-12-09T22:27:00.198033: step 1931, loss 0.384897, acc 0.921875, prec 0.0742766, recall 0.839389
2017-12-09T22:27:00.494376: step 1932, loss 0.174944, acc 0.9375, prec 0.0742831, recall 0.839424
2017-12-09T22:27:00.793414: step 1933, loss 0.22328, acc 0.9375, prec 0.0743078, recall 0.839495
2017-12-09T22:27:01.089792: step 1934, loss 0.206886, acc 0.914062, prec 0.0742918, recall 0.839495
2017-12-09T22:27:01.401239: step 1935, loss 0.41965, acc 0.851562, prec 0.0742641, recall 0.839495
2017-12-09T22:27:01.699173: step 1936, loss 0.280553, acc 0.890625, prec 0.0742438, recall 0.839495
2017-12-09T22:27:01.998120: step 1937, loss 0.217484, acc 0.929688, prec 0.074285, recall 0.839602
2017-12-09T22:27:02.294578: step 1938, loss 0.134023, acc 0.960938, prec 0.074314, recall 0.839673
2017-12-09T22:27:02.596115: step 1939, loss 0.127342, acc 0.96875, prec 0.0743263, recall 0.839708
2017-12-09T22:27:02.893461: step 1940, loss 0.105925, acc 0.96875, prec 0.074411, recall 0.839885
2017-12-09T22:27:03.190243: step 1941, loss 0.464991, acc 0.953125, prec 0.0744928, recall 0.840062
2017-12-09T22:27:03.492607: step 1942, loss 0.223616, acc 0.960938, prec 0.0745037, recall 0.840097
2017-12-09T22:27:03.788356: step 1943, loss 0.0331771, acc 1, prec 0.0745399, recall 0.840168
2017-12-09T22:27:04.086917: step 1944, loss 0.206106, acc 0.945312, prec 0.0745659, recall 0.840238
2017-12-09T22:27:04.384293: step 1945, loss 0.0483572, acc 0.984375, prec 0.0745991, recall 0.840308
2017-12-09T22:27:04.679693: step 1946, loss 0.346114, acc 0.921875, prec 0.0746207, recall 0.840379
2017-12-09T22:27:04.980218: step 1947, loss 0.162836, acc 0.960938, prec 0.0746315, recall 0.840414
2017-12-09T22:27:05.290508: step 1948, loss 0.0476177, acc 0.984375, prec 0.0746467, recall 0.840449
2017-12-09T22:27:05.591070: step 1949, loss 1.13489, acc 0.960938, prec 0.074659, recall 0.840299
2017-12-09T22:27:05.898299: step 1950, loss 0.131065, acc 0.9375, prec 0.0747015, recall 0.840405
2017-12-09T22:27:06.199095: step 1951, loss 0.103356, acc 0.976562, prec 0.0747333, recall 0.840475
2017-12-09T22:27:06.499534: step 1952, loss 0.312688, acc 0.976562, prec 0.074747, recall 0.84051
2017-12-09T22:27:06.800653: step 1953, loss 0.0864699, acc 0.976562, prec 0.0747788, recall 0.84058
2017-12-09T22:27:07.098264: step 1954, loss 0.174817, acc 0.945312, prec 0.0747685, recall 0.84058
2017-12-09T22:27:07.398599: step 1955, loss 0.258593, acc 0.945312, prec 0.0748487, recall 0.840755
2017-12-09T22:27:07.698117: step 1956, loss 0.173381, acc 0.945312, prec 0.0748565, recall 0.840789
2017-12-09T22:27:07.996933: step 1957, loss 0.297196, acc 0.960938, prec 0.0749034, recall 0.840894
2017-12-09T22:27:08.299123: step 1958, loss 0.322239, acc 0.90625, prec 0.0749219, recall 0.840964
2017-12-09T22:27:08.596686: step 1959, loss 0.124943, acc 0.953125, prec 0.0749312, recall 0.840999
2017-12-09T22:27:08.893631: step 1960, loss 0.116439, acc 0.96875, prec 0.0749434, recall 0.841034
2017-12-09T22:27:09.190709: step 1961, loss 0.184643, acc 0.929688, prec 0.0749483, recall 0.841068
2017-12-09T22:27:09.489143: step 1962, loss 0.14264, acc 0.960938, prec 0.074959, recall 0.841103
2017-12-09T22:27:09.790805: step 1963, loss 0.208407, acc 0.929688, prec 0.0749459, recall 0.841103
2017-12-09T22:27:10.093571: step 1964, loss 0.181236, acc 0.945312, prec 0.0749357, recall 0.841103
2017-12-09T22:27:10.391815: step 1965, loss 0.200899, acc 0.929688, prec 0.0749586, recall 0.841173
2017-12-09T22:27:10.690471: step 1966, loss 0.130786, acc 0.976562, prec 0.0749722, recall 0.841207
2017-12-09T22:27:10.987167: step 1967, loss 0.166198, acc 0.960938, prec 0.075001, recall 0.841277
2017-12-09T22:27:11.290667: step 1968, loss 0.0918084, acc 0.984375, prec 0.0750341, recall 0.841346
2017-12-09T22:27:11.590688: step 1969, loss 0.133171, acc 0.976562, prec 0.0750297, recall 0.841346
2017-12-09T22:27:11.888756: step 1970, loss 0.46439, acc 0.960938, prec 0.0750765, recall 0.84145
2017-12-09T22:27:12.191638: step 1971, loss 0.219661, acc 0.953125, prec 0.0751037, recall 0.841519
2017-12-09T22:27:12.490196: step 1972, loss 0.15135, acc 0.953125, prec 0.075131, recall 0.841588
2017-12-09T22:27:12.786696: step 1973, loss 0.0460801, acc 0.976562, prec 0.0751446, recall 0.841623
2017-12-09T22:27:13.084441: step 1974, loss 0.0646037, acc 0.976562, prec 0.0751402, recall 0.841623
2017-12-09T22:27:13.385339: step 1975, loss 0.769981, acc 0.976562, prec 0.0751733, recall 0.841509
2017-12-09T22:27:13.687685: step 1976, loss 0.357461, acc 0.976562, prec 0.075259, recall 0.841681
2017-12-09T22:27:13.993862: step 1977, loss 0.131129, acc 0.953125, prec 0.0752862, recall 0.84175
2017-12-09T22:27:14.293342: step 1978, loss 0.156402, acc 0.9375, prec 0.0753105, recall 0.841819
2017-12-09T22:27:14.588909: step 1979, loss 0.18791, acc 0.953125, prec 0.0753557, recall 0.841922
2017-12-09T22:27:14.886905: step 1980, loss 0.135574, acc 0.953125, prec 0.0753649, recall 0.841956
2017-12-09T22:27:15.184142: step 1981, loss 0.0921917, acc 0.96875, prec 0.075359, recall 0.841956
2017-12-09T22:27:15.484739: step 1982, loss 0.193414, acc 0.921875, prec 0.0753623, recall 0.841991
2017-12-09T22:27:15.784331: step 1983, loss 0.457298, acc 0.96875, prec 0.0753744, recall 0.842025
2017-12-09T22:27:16.083560: step 1984, loss 0.304777, acc 0.890625, prec 0.0753719, recall 0.84206
2017-12-09T22:27:16.380678: step 1985, loss 0.245427, acc 0.960938, prec 0.0754365, recall 0.842197
2017-12-09T22:27:16.682911: step 1986, loss 0.232151, acc 0.914062, prec 0.0754563, recall 0.842265
2017-12-09T22:27:16.982783: step 1987, loss 0.204417, acc 0.921875, prec 0.0754776, recall 0.842334
2017-12-09T22:27:17.277156: step 1988, loss 0.120863, acc 0.953125, prec 0.0754867, recall 0.842368
2017-12-09T22:27:17.574348: step 1989, loss 0.198444, acc 0.960938, prec 0.0754974, recall 0.842402
2017-12-09T22:27:17.868929: step 1990, loss 0.107764, acc 0.976562, prec 0.0755109, recall 0.842436
2017-12-09T22:27:18.167036: step 1991, loss 0.146126, acc 0.945312, prec 0.0755366, recall 0.842504
2017-12-09T22:27:18.347548: step 1992, loss 0.172256, acc 0.941176, prec 0.0755322, recall 0.842504
2017-12-09T22:27:18.659639: step 1993, loss 0.101274, acc 0.984375, prec 0.0755472, recall 0.842538
2017-12-09T22:27:18.959809: step 1994, loss 0.14111, acc 0.96875, prec 0.0755593, recall 0.842573
2017-12-09T22:27:19.257200: step 1995, loss 0.144563, acc 0.96875, prec 0.0755714, recall 0.842607
2017-12-09T22:27:19.557252: step 1996, loss 0.492133, acc 0.960938, prec 0.0756358, recall 0.842743
2017-12-09T22:27:19.857444: step 1997, loss 0.0400513, acc 0.984375, prec 0.0756329, recall 0.842743
2017-12-09T22:27:20.157984: step 1998, loss 0.100283, acc 0.976562, prec 0.0756464, recall 0.842777
2017-12-09T22:27:20.463471: step 1999, loss 0.120485, acc 0.96875, prec 0.0756764, recall 0.842845
2017-12-09T22:27:20.761950: step 2000, loss 0.144054, acc 0.953125, prec 0.0756855, recall 0.842879
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-2000

2017-12-09T22:27:22.197085: step 2001, loss 0.497497, acc 0.953125, prec 0.0757126, recall 0.842947
2017-12-09T22:27:22.500067: step 2002, loss 0.149221, acc 0.953125, prec 0.0757396, recall 0.843014
2017-12-09T22:27:22.793764: step 2003, loss 0.19636, acc 0.945312, prec 0.0757831, recall 0.843116
2017-12-09T22:27:23.095574: step 2004, loss 0.175046, acc 0.953125, prec 0.0757923, recall 0.84315
2017-12-09T22:27:23.397728: step 2005, loss 0.126181, acc 0.960938, prec 0.0758028, recall 0.843184
2017-12-09T22:27:23.693739: step 2006, loss 0.162292, acc 0.953125, prec 0.0758478, recall 0.843285
2017-12-09T22:27:23.995057: step 2007, loss 0.0953053, acc 0.953125, prec 0.0758389, recall 0.843285
2017-12-09T22:27:24.287186: step 2008, loss 0.248031, acc 0.960938, prec 0.0759032, recall 0.84342
2017-12-09T22:27:24.587493: step 2009, loss 0.118568, acc 0.953125, prec 0.0758944, recall 0.84342
2017-12-09T22:27:24.886355: step 2010, loss 0.631659, acc 0.96875, prec 0.0759781, recall 0.843589
2017-12-09T22:27:25.186924: step 2011, loss 0.079277, acc 0.976562, prec 0.0759736, recall 0.843589
2017-12-09T22:27:25.486710: step 2012, loss 0.394713, acc 0.976562, prec 0.0760587, recall 0.843757
2017-12-09T22:27:25.792639: step 2013, loss 0.0749618, acc 0.96875, prec 0.0760528, recall 0.843757
2017-12-09T22:27:26.090234: step 2014, loss 2.02251, acc 0.945312, prec 0.0760977, recall 0.843676
2017-12-09T22:27:26.389426: step 2015, loss 0.176184, acc 0.945312, prec 0.0761589, recall 0.84381
2017-12-09T22:27:26.687550: step 2016, loss 0.124802, acc 0.960938, prec 0.0761516, recall 0.84381
2017-12-09T22:27:26.983363: step 2017, loss 0.258554, acc 0.914062, prec 0.0761354, recall 0.84381
2017-12-09T22:27:27.288376: step 2018, loss 0.334369, acc 0.890625, prec 0.0761684, recall 0.843911
2017-12-09T22:27:27.592136: step 2019, loss 0.216249, acc 0.914062, prec 0.0761521, recall 0.843911
2017-12-09T22:27:27.893371: step 2020, loss 0.313144, acc 0.882812, prec 0.0761479, recall 0.843944
2017-12-09T22:27:28.191733: step 2021, loss 0.376198, acc 0.867188, prec 0.0761408, recall 0.843978
2017-12-09T22:27:28.486476: step 2022, loss 0.294527, acc 0.882812, prec 0.0761365, recall 0.844011
2017-12-09T22:27:28.784797: step 2023, loss 0.31893, acc 0.914062, prec 0.0761739, recall 0.844111
2017-12-09T22:27:29.087180: step 2024, loss 0.530846, acc 0.867188, prec 0.0761846, recall 0.844178
2017-12-09T22:27:29.385668: step 2025, loss 0.558181, acc 0.8125, prec 0.0762028, recall 0.844278
2017-12-09T22:27:29.686729: step 2026, loss 0.277983, acc 0.921875, prec 0.0761881, recall 0.844278
2017-12-09T22:27:29.990661: step 2027, loss 0.246871, acc 0.953125, prec 0.0762149, recall 0.844345
2017-12-09T22:27:30.300019: step 2028, loss 0.272718, acc 0.90625, prec 0.0761973, recall 0.844345
2017-12-09T22:27:30.595404: step 2029, loss 0.15048, acc 0.945312, prec 0.0762405, recall 0.844444
2017-12-09T22:27:30.897823: step 2030, loss 0.252355, acc 0.929688, prec 0.0762628, recall 0.844511
2017-12-09T22:27:31.191930: step 2031, loss 0.168148, acc 0.96875, prec 0.0762748, recall 0.844544
2017-12-09T22:27:31.493405: step 2032, loss 0.150349, acc 0.953125, prec 0.0762838, recall 0.844577
2017-12-09T22:27:31.792281: step 2033, loss 0.105562, acc 0.984375, prec 0.0763521, recall 0.84471
2017-12-09T22:27:32.099573: step 2034, loss 0.257109, acc 0.921875, prec 0.0763374, recall 0.84471
2017-12-09T22:27:32.399309: step 2035, loss 0.619157, acc 0.984375, prec 0.0763878, recall 0.844809
2017-12-09T22:27:32.703480: step 2036, loss 0.100774, acc 0.976562, prec 0.076419, recall 0.844875
2017-12-09T22:27:33.001717: step 2037, loss 0.537318, acc 0.96875, prec 0.0764843, recall 0.845007
2017-12-09T22:27:33.303866: step 2038, loss 0.0972086, acc 0.976562, prec 0.0764977, recall 0.84504
2017-12-09T22:27:33.602289: step 2039, loss 0.0791559, acc 0.960938, prec 0.0764903, recall 0.84504
2017-12-09T22:27:33.898996: step 2040, loss 0.224761, acc 0.976562, prec 0.0765215, recall 0.845106
2017-12-09T22:27:34.198734: step 2041, loss 0.11068, acc 0.96875, prec 0.0765334, recall 0.845139
2017-12-09T22:27:34.499985: step 2042, loss 0.194395, acc 0.992188, prec 0.076603, recall 0.845271
2017-12-09T22:27:34.802729: step 2043, loss 0.0767496, acc 0.984375, prec 0.0766534, recall 0.84537
2017-12-09T22:27:35.102261: step 2044, loss 0.172398, acc 0.96875, prec 0.0767187, recall 0.845501
2017-12-09T22:27:35.410852: step 2045, loss 0.289144, acc 0.984375, prec 0.0767868, recall 0.845632
2017-12-09T22:27:35.708792: step 2046, loss 0.112052, acc 0.976562, prec 0.0767824, recall 0.845632
2017-12-09T22:27:36.002297: step 2047, loss 0.305528, acc 0.945312, prec 0.0768254, recall 0.84573
2017-12-09T22:27:36.305938: step 2048, loss 0.152516, acc 0.984375, prec 0.0768579, recall 0.845795
2017-12-09T22:27:36.603639: step 2049, loss 0.0866853, acc 0.976562, prec 0.0768713, recall 0.845828
2017-12-09T22:27:36.901229: step 2050, loss 0.223664, acc 0.9375, prec 0.076895, recall 0.845893
2017-12-09T22:27:37.201962: step 2051, loss 0.117789, acc 0.96875, prec 0.0769068, recall 0.845926
2017-12-09T22:27:37.501815: step 2052, loss 0.237942, acc 0.9375, prec 0.0769127, recall 0.845959
2017-12-09T22:27:37.803704: step 2053, loss 0.216477, acc 0.96875, prec 0.0769423, recall 0.846024
2017-12-09T22:27:38.104144: step 2054, loss 0.196631, acc 0.945312, prec 0.0769675, recall 0.846089
2017-12-09T22:27:38.400485: step 2055, loss 0.208947, acc 0.953125, prec 0.0769941, recall 0.846154
2017-12-09T22:27:38.707416: step 2056, loss 0.116475, acc 0.960938, prec 0.0770044, recall 0.846186
2017-12-09T22:27:39.006835: step 2057, loss 0.224512, acc 0.9375, prec 0.0770281, recall 0.846251
2017-12-09T22:27:39.307363: step 2058, loss 0.089011, acc 0.960938, prec 0.0770207, recall 0.846251
2017-12-09T22:27:39.615791: step 2059, loss 0.201511, acc 0.898438, prec 0.0770369, recall 0.846316
2017-12-09T22:27:39.920228: step 2060, loss 0.188798, acc 0.9375, prec 0.0770605, recall 0.846381
2017-12-09T22:27:40.229996: step 2061, loss 0.173846, acc 0.9375, prec 0.0770487, recall 0.846381
2017-12-09T22:27:40.528368: step 2062, loss 0.152915, acc 0.9375, prec 0.0770546, recall 0.846413
2017-12-09T22:27:40.828289: step 2063, loss 0.16542, acc 0.976562, prec 0.0770679, recall 0.846446
2017-12-09T22:27:41.126188: step 2064, loss 0.0681022, acc 0.984375, prec 0.0770826, recall 0.846478
2017-12-09T22:27:41.428884: step 2065, loss 0.590605, acc 0.96875, prec 0.0771476, recall 0.846608
2017-12-09T22:27:41.732973: step 2066, loss 0.413868, acc 0.984375, prec 0.0771801, recall 0.846672
2017-12-09T22:27:42.036620: step 2067, loss 0.177553, acc 0.960938, prec 0.0771904, recall 0.846705
2017-12-09T22:27:42.341026: step 2068, loss 0.266005, acc 0.976562, prec 0.0772391, recall 0.846801
2017-12-09T22:27:42.641909: step 2069, loss 0.0982749, acc 0.960938, prec 0.0772493, recall 0.846834
2017-12-09T22:27:42.939035: step 2070, loss 0.213089, acc 0.96875, prec 0.0772611, recall 0.846866
2017-12-09T22:27:43.240456: step 2071, loss 0.0572702, acc 0.976562, prec 0.0773275, recall 0.846995
2017-12-09T22:27:43.539528: step 2072, loss 0.119101, acc 0.976562, prec 0.077323, recall 0.846995
2017-12-09T22:27:43.839860: step 2073, loss 2.6696, acc 0.953125, prec 0.077351, recall 0.846881
2017-12-09T22:27:44.141988: step 2074, loss 0.227503, acc 0.945312, prec 0.0773583, recall 0.846913
2017-12-09T22:27:44.436621: step 2075, loss 0.193539, acc 0.960938, prec 0.0773686, recall 0.846945
2017-12-09T22:27:44.736387: step 2076, loss 0.159083, acc 0.9375, prec 0.0773744, recall 0.846977
2017-12-09T22:27:45.037547: step 2077, loss 0.206992, acc 0.90625, prec 0.0774274, recall 0.847106
2017-12-09T22:27:45.337799: step 2078, loss 0.230238, acc 0.914062, prec 0.0774464, recall 0.84717
2017-12-09T22:27:45.642441: step 2079, loss 0.157658, acc 0.914062, prec 0.0774655, recall 0.847234
2017-12-09T22:27:45.945861: step 2080, loss 0.278048, acc 0.929688, prec 0.0775228, recall 0.847362
2017-12-09T22:27:46.248739: step 2081, loss 0.267968, acc 0.882812, prec 0.0775535, recall 0.847458
2017-12-09T22:27:46.548853: step 2082, loss 0.289958, acc 0.90625, prec 0.0775534, recall 0.84749
2017-12-09T22:27:46.852601: step 2083, loss 0.217076, acc 0.9375, prec 0.0775945, recall 0.847585
2017-12-09T22:27:47.149811: step 2084, loss 0.185495, acc 0.9375, prec 0.0776002, recall 0.847617
2017-12-09T22:27:47.448523: step 2085, loss 0.16371, acc 0.945312, prec 0.0776251, recall 0.847681
2017-12-09T22:27:47.754169: step 2086, loss 0.177156, acc 0.929688, prec 0.0776118, recall 0.847681
2017-12-09T22:27:48.058757: step 2087, loss 0.195201, acc 0.953125, prec 0.0776381, recall 0.847744
2017-12-09T22:27:48.360643: step 2088, loss 0.172418, acc 0.929688, prec 0.0776424, recall 0.847776
2017-12-09T22:27:48.668874: step 2089, loss 0.160517, acc 0.9375, prec 0.0776482, recall 0.847808
2017-12-09T22:27:48.965608: step 2090, loss 0.207581, acc 0.9375, prec 0.0776539, recall 0.84784
2017-12-09T22:27:49.266151: step 2091, loss 0.0795932, acc 0.976562, prec 0.0776671, recall 0.847871
2017-12-09T22:27:49.568504: step 2092, loss 0.144279, acc 0.976562, prec 0.0776803, recall 0.847903
2017-12-09T22:27:49.865313: step 2093, loss 0.0289632, acc 1, prec 0.0776979, recall 0.847935
2017-12-09T22:27:50.169025: step 2094, loss 0.142197, acc 0.96875, prec 0.0777272, recall 0.847998
2017-12-09T22:27:50.475484: step 2095, loss 0.111991, acc 0.96875, prec 0.0777565, recall 0.848062
2017-12-09T22:27:50.773210: step 2096, loss 0.156715, acc 0.960938, prec 0.0777844, recall 0.848125
2017-12-09T22:27:51.072722: step 2097, loss 0.102872, acc 0.96875, prec 0.077796, recall 0.848157
2017-12-09T22:27:51.369399: step 2098, loss 0.0225124, acc 1, prec 0.0778665, recall 0.848283
2017-12-09T22:27:51.667795: step 2099, loss 0.505194, acc 0.976562, prec 0.0779325, recall 0.848409
2017-12-09T22:27:51.975167: step 2100, loss 0.110863, acc 0.96875, prec 0.0779442, recall 0.848441

Evaluation:
2017-12-09T22:27:56.692237: step 2100, loss 3.90401, acc 0.966978, prec 0.0783263, recall 0.828709

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-2100

2017-12-09T22:27:57.998675: step 2101, loss 0.0875621, acc 0.976562, prec 0.0783218, recall 0.828709
2017-12-09T22:27:58.298399: step 2102, loss 0.115734, acc 0.992188, prec 0.0783379, recall 0.828744
2017-12-09T22:27:58.599740: step 2103, loss 0.0314858, acc 0.992188, prec 0.0783889, recall 0.828847
2017-12-09T22:27:58.903288: step 2104, loss 0.140967, acc 0.976562, prec 0.0784019, recall 0.828881
2017-12-09T22:27:59.204899: step 2105, loss 0.533732, acc 0.992188, prec 0.0784705, recall 0.829019
2017-12-09T22:27:59.503881: step 2106, loss 0.12586, acc 0.96875, prec 0.0784645, recall 0.829019
2017-12-09T22:27:59.804670: step 2107, loss 0.369521, acc 0.984375, prec 0.078514, recall 0.829122
2017-12-09T22:28:00.123753: step 2108, loss 0.56688, acc 0.976562, prec 0.0785971, recall 0.829293
2017-12-09T22:28:01.133140: step 2109, loss 0.121074, acc 0.96875, prec 0.0786086, recall 0.829327
2017-12-09T22:28:01.529649: step 2110, loss 0.089546, acc 0.96875, prec 0.0786201, recall 0.829361
2017-12-09T22:28:01.828358: step 2111, loss 0.240498, acc 0.9375, prec 0.0786257, recall 0.829395
2017-12-09T22:28:02.127921: step 2112, loss 0.182568, acc 0.9375, prec 0.0787186, recall 0.8296
2017-12-09T22:28:02.424877: step 2113, loss 0.173789, acc 0.929688, prec 0.0787227, recall 0.829634
2017-12-09T22:28:02.723096: step 2114, loss 0.237689, acc 0.953125, prec 0.0787487, recall 0.829702
2017-12-09T22:28:03.020130: step 2115, loss 0.15943, acc 0.953125, prec 0.0787747, recall 0.82977
2017-12-09T22:28:03.316446: step 2116, loss 0.141304, acc 0.953125, prec 0.078853, recall 0.82994
2017-12-09T22:28:03.615610: step 2117, loss 0.220637, acc 0.945312, prec 0.078895, recall 0.830042
2017-12-09T22:28:03.913046: step 2118, loss 0.536377, acc 0.960938, prec 0.0789399, recall 0.830144
2017-12-09T22:28:04.213530: step 2119, loss 0.146247, acc 0.953125, prec 0.0789484, recall 0.830177
2017-12-09T22:28:04.517010: step 2120, loss 0.17986, acc 0.953125, prec 0.0789394, recall 0.830177
2017-12-09T22:28:04.818639: step 2121, loss 0.135786, acc 0.929688, prec 0.0789783, recall 0.830279
2017-12-09T22:28:05.119651: step 2122, loss 0.175118, acc 0.9375, prec 0.0790012, recall 0.830346
2017-12-09T22:28:05.433205: step 2123, loss 0.13161, acc 0.96875, prec 0.0790301, recall 0.830414
2017-12-09T22:28:05.730654: step 2124, loss 0.229946, acc 0.9375, prec 0.0790879, recall 0.830549
2017-12-09T22:28:06.031143: step 2125, loss 0.184052, acc 0.9375, prec 0.0790934, recall 0.830583
2017-12-09T22:28:06.329306: step 2126, loss 0.159188, acc 0.9375, prec 0.0791163, recall 0.83065
2017-12-09T22:28:06.628151: step 2127, loss 0.151169, acc 0.976562, prec 0.0791815, recall 0.830784
2017-12-09T22:28:06.927450: step 2128, loss 0.16821, acc 0.953125, prec 0.0792248, recall 0.830885
2017-12-09T22:28:07.226082: step 2129, loss 0.147927, acc 0.960938, prec 0.0792347, recall 0.830919
2017-12-09T22:28:07.525934: step 2130, loss 0.200864, acc 0.9375, prec 0.0792227, recall 0.830919
2017-12-09T22:28:07.820630: step 2131, loss 1.2027, acc 0.96875, prec 0.0792182, recall 0.830754
2017-12-09T22:28:08.123542: step 2132, loss 0.190516, acc 0.992188, prec 0.0792516, recall 0.830821
2017-12-09T22:28:08.421126: step 2133, loss 0.255393, acc 0.929688, prec 0.0793077, recall 0.830955
2017-12-09T22:28:08.722360: step 2134, loss 0.263012, acc 0.9375, prec 0.0793306, recall 0.831022
2017-12-09T22:28:09.026242: step 2135, loss 0.276129, acc 0.945312, prec 0.0793723, recall 0.831123
2017-12-09T22:28:09.323718: step 2136, loss 0.101177, acc 0.945312, prec 0.0793792, recall 0.831156
2017-12-09T22:28:09.619651: step 2137, loss 0.195703, acc 0.953125, prec 0.079405, recall 0.831223
2017-12-09T22:28:09.922622: step 2138, loss 0.28225, acc 0.976562, prec 0.0794353, recall 0.83129
2017-12-09T22:28:10.231095: step 2139, loss 0.0637515, acc 0.984375, prec 0.0794671, recall 0.831356
2017-12-09T22:28:10.533201: step 2140, loss 0.0479103, acc 0.984375, prec 0.0794815, recall 0.83139
2017-12-09T22:28:10.830750: step 2141, loss 0.172536, acc 0.921875, prec 0.0794664, recall 0.83139
2017-12-09T22:28:11.131448: step 2142, loss 0.277528, acc 0.9375, prec 0.0794892, recall 0.831456
2017-12-09T22:28:11.433701: step 2143, loss 0.380567, acc 0.929688, prec 0.0795279, recall 0.831556
2017-12-09T22:28:11.731180: step 2144, loss 0.0580283, acc 0.976562, prec 0.0795581, recall 0.831623
2017-12-09T22:28:12.030710: step 2145, loss 0.167909, acc 0.96875, prec 0.0795869, recall 0.831689
2017-12-09T22:28:12.333460: step 2146, loss 0.13441, acc 0.945312, prec 0.0795764, recall 0.831689
2017-12-09T22:28:12.633768: step 2147, loss 0.205024, acc 0.929688, prec 0.0795802, recall 0.831722
2017-12-09T22:28:12.932400: step 2148, loss 0.314758, acc 0.953125, prec 0.0796233, recall 0.831822
2017-12-09T22:28:13.228646: step 2149, loss 0.209942, acc 0.96875, prec 0.079652, recall 0.831888
2017-12-09T22:28:13.529855: step 2150, loss 0.30645, acc 0.960938, prec 0.079714, recall 0.83202
2017-12-09T22:28:13.828547: step 2151, loss 0.108633, acc 0.9375, prec 0.0797019, recall 0.83202
2017-12-09T22:28:14.134518: step 2152, loss 0.149205, acc 0.976562, prec 0.0797321, recall 0.832087
2017-12-09T22:28:14.434080: step 2153, loss 0.220434, acc 0.953125, prec 0.0797578, recall 0.832153
2017-12-09T22:28:14.732788: step 2154, loss 0.260244, acc 0.9375, prec 0.0797805, recall 0.832219
2017-12-09T22:28:15.031097: step 2155, loss 0.228551, acc 0.953125, prec 0.0798062, recall 0.832285
2017-12-09T22:28:15.334269: step 2156, loss 0.145844, acc 0.960938, prec 0.0798507, recall 0.832384
2017-12-09T22:28:15.638150: step 2157, loss 0.0319981, acc 0.992188, prec 0.0798492, recall 0.832384
2017-12-09T22:28:15.934984: step 2158, loss 0.0736578, acc 0.976562, prec 0.079862, recall 0.832416
2017-12-09T22:28:16.234489: step 2159, loss 0.190091, acc 0.953125, prec 0.079905, recall 0.832515
2017-12-09T22:28:16.531145: step 2160, loss 0.211931, acc 0.921875, prec 0.07989, recall 0.832515
2017-12-09T22:28:16.827873: step 2161, loss 0.312038, acc 0.929688, prec 0.0799111, recall 0.832581
2017-12-09T22:28:17.127236: step 2162, loss 1.16788, acc 0.960938, prec 0.0799397, recall 0.832483
2017-12-09T22:28:17.430871: step 2163, loss 0.100518, acc 0.976562, prec 0.0799525, recall 0.832516
2017-12-09T22:28:17.735268: step 2164, loss 0.121785, acc 0.976562, prec 0.0799827, recall 0.832582
2017-12-09T22:28:18.044632: step 2165, loss 0.298531, acc 0.9375, prec 0.0800053, recall 0.832647
2017-12-09T22:28:18.344551: step 2166, loss 0.268383, acc 0.929688, prec 0.0800264, recall 0.832713
2017-12-09T22:28:18.643010: step 2167, loss 1.31928, acc 0.953125, prec 0.0800188, recall 0.83255
2017-12-09T22:28:18.940490: step 2168, loss 0.145672, acc 0.945312, prec 0.0800429, recall 0.832615
2017-12-09T22:28:19.237834: step 2169, loss 0.211991, acc 0.9375, prec 0.0800482, recall 0.832648
2017-12-09T22:28:19.532791: step 2170, loss 0.201368, acc 0.929688, prec 0.0800692, recall 0.832714
2017-12-09T22:28:19.835647: step 2171, loss 0.121183, acc 0.9375, prec 0.0800572, recall 0.832714
2017-12-09T22:28:20.134767: step 2172, loss 0.100944, acc 0.976562, prec 0.0800527, recall 0.832714
2017-12-09T22:28:20.440716: step 2173, loss 0.395099, acc 0.890625, prec 0.0800662, recall 0.832779
2017-12-09T22:28:20.737847: step 2174, loss 0.218168, acc 0.929688, prec 0.0800699, recall 0.832812
2017-12-09T22:28:21.036880: step 2175, loss 0.304659, acc 0.90625, prec 0.0801037, recall 0.83291
2017-12-09T22:28:21.337086: step 2176, loss 0.228359, acc 0.9375, prec 0.0801436, recall 0.833008
2017-12-09T22:28:21.635081: step 2177, loss 0.226827, acc 0.898438, prec 0.080124, recall 0.833008
2017-12-09T22:28:21.933595: step 2178, loss 0.158337, acc 0.9375, prec 0.0801292, recall 0.83304
2017-12-09T22:28:22.233440: step 2179, loss 0.156435, acc 0.953125, prec 0.0801548, recall 0.833106
2017-12-09T22:28:22.535095: step 2180, loss 0.204105, acc 0.9375, prec 0.08016, recall 0.833138
2017-12-09T22:28:22.831296: step 2181, loss 0.11399, acc 0.96875, prec 0.0801885, recall 0.833203
2017-12-09T22:28:23.128830: step 2182, loss 0.127344, acc 0.976562, prec 0.080184, recall 0.833203
2017-12-09T22:28:23.424216: step 2183, loss 0.226121, acc 0.9375, prec 0.0802065, recall 0.833268
2017-12-09T22:28:23.724367: step 2184, loss 0.275032, acc 0.945312, prec 0.0802305, recall 0.833333
2017-12-09T22:28:24.027985: step 2185, loss 0.21944, acc 0.984375, prec 0.080262, recall 0.833398
2017-12-09T22:28:24.326141: step 2186, loss 0.709217, acc 0.960938, prec 0.0802732, recall 0.833268
2017-12-09T22:28:24.628470: step 2187, loss 0.116747, acc 0.992188, prec 0.0803062, recall 0.833333
2017-12-09T22:28:24.925607: step 2188, loss 0.0787968, acc 0.976562, prec 0.0803017, recall 0.833333
2017-12-09T22:28:25.225782: step 2189, loss 0.0810234, acc 0.96875, prec 0.0803474, recall 0.833431
2017-12-09T22:28:25.526147: step 2190, loss 0.299209, acc 0.9375, prec 0.0803871, recall 0.833528
2017-12-09T22:28:25.825671: step 2191, loss 0.196983, acc 0.953125, prec 0.0803953, recall 0.83356
2017-12-09T22:28:26.126569: step 2192, loss 0.237612, acc 0.976562, prec 0.0804253, recall 0.833625
2017-12-09T22:28:26.426347: step 2193, loss 0.0663915, acc 0.984375, prec 0.0804568, recall 0.83369
2017-12-09T22:28:26.734086: step 2194, loss 0.173558, acc 0.960938, prec 0.0804492, recall 0.83369
2017-12-09T22:28:27.044614: step 2195, loss 0.534769, acc 0.96875, prec 0.0805466, recall 0.833883
2017-12-09T22:28:27.343516: step 2196, loss 0.148479, acc 0.976562, prec 0.0805938, recall 0.83398
2017-12-09T22:28:27.642496: step 2197, loss 0.338503, acc 0.976562, prec 0.0806237, recall 0.834044
2017-12-09T22:28:27.947200: step 2198, loss 0.174117, acc 0.96875, prec 0.0806521, recall 0.834109
2017-12-09T22:28:28.251365: step 2199, loss 0.582036, acc 0.953125, prec 0.0806947, recall 0.834205
2017-12-09T22:28:28.553673: step 2200, loss 0.887124, acc 0.945312, prec 0.0807201, recall 0.834108
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-2200

2017-12-09T22:28:29.847173: step 2201, loss 0.210001, acc 0.945312, prec 0.0807439, recall 0.834172
2017-12-09T22:28:30.154194: step 2202, loss 0.205474, acc 0.945312, prec 0.0807678, recall 0.834236
2017-12-09T22:28:30.461152: step 2203, loss 0.247248, acc 0.882812, prec 0.0807451, recall 0.834236
2017-12-09T22:28:30.761089: step 2204, loss 0.165438, acc 0.914062, prec 0.0807457, recall 0.834268
2017-12-09T22:28:31.061071: step 2205, loss 0.37866, acc 0.867188, prec 0.08072, recall 0.834268
2017-12-09T22:28:31.360723: step 2206, loss 0.380968, acc 0.867188, prec 0.0807287, recall 0.834332
2017-12-09T22:28:31.656230: step 2207, loss 0.367347, acc 0.851562, prec 0.0807344, recall 0.834396
2017-12-09T22:28:31.958146: step 2208, loss 0.376898, acc 0.875, prec 0.0807446, recall 0.83446
2017-12-09T22:28:32.259986: step 2209, loss 0.378645, acc 0.898438, prec 0.080725, recall 0.83446
2017-12-09T22:28:32.558918: step 2210, loss 0.31139, acc 0.90625, prec 0.0807928, recall 0.83462
2017-12-09T22:28:32.858022: step 2211, loss 0.232976, acc 0.921875, prec 0.0807949, recall 0.834652
2017-12-09T22:28:33.157675: step 2212, loss 0.427756, acc 0.914062, prec 0.0808641, recall 0.834811
2017-12-09T22:28:33.457639: step 2213, loss 0.180863, acc 0.945312, prec 0.0808878, recall 0.834875
2017-12-09T22:28:33.754117: step 2214, loss 0.177522, acc 0.929688, prec 0.0809429, recall 0.835002
2017-12-09T22:28:34.053405: step 2215, loss 0.202681, acc 0.929688, prec 0.0809293, recall 0.835002
2017-12-09T22:28:34.352997: step 2216, loss 0.587028, acc 0.960938, prec 0.0809903, recall 0.835129
2017-12-09T22:28:34.650984: step 2217, loss 0.0864176, acc 0.96875, prec 0.0810014, recall 0.835161
2017-12-09T22:28:34.947926: step 2218, loss 0.337076, acc 0.960938, prec 0.0810453, recall 0.835256
2017-12-09T22:28:35.255665: step 2219, loss 0.282796, acc 0.96875, prec 0.0810735, recall 0.835319
2017-12-09T22:28:35.560437: step 2220, loss 0.123451, acc 0.9375, prec 0.0810614, recall 0.835319
2017-12-09T22:28:35.861246: step 2221, loss 0.0457079, acc 0.984375, prec 0.0810755, recall 0.835351
2017-12-09T22:28:36.156359: step 2222, loss 0.0657977, acc 0.976562, prec 0.081071, recall 0.835351
2017-12-09T22:28:36.451548: step 2223, loss 0.18962, acc 0.929688, prec 0.0810745, recall 0.835382
2017-12-09T22:28:36.750086: step 2224, loss 0.0987175, acc 0.960938, prec 0.0811184, recall 0.835477
2017-12-09T22:28:37.051968: step 2225, loss 0.0771309, acc 0.976562, prec 0.0811138, recall 0.835477
2017-12-09T22:28:37.350939: step 2226, loss 0.215616, acc 0.960938, prec 0.0811234, recall 0.835509
2017-12-09T22:28:37.660283: step 2227, loss 0.0689398, acc 0.984375, prec 0.0811889, recall 0.835635
2017-12-09T22:28:37.957157: step 2228, loss 0.243666, acc 0.984375, prec 0.0812372, recall 0.835729
2017-12-09T22:28:38.256793: step 2229, loss 0.104761, acc 0.960938, prec 0.0812467, recall 0.835761
2017-12-09T22:28:38.557509: step 2230, loss 0.127336, acc 0.960938, prec 0.0812734, recall 0.835824
2017-12-09T22:28:38.853567: step 2231, loss 0.129411, acc 0.96875, prec 0.0813187, recall 0.835918
2017-12-09T22:28:39.153270: step 2232, loss 0.214429, acc 0.960938, prec 0.0813282, recall 0.835949
2017-12-09T22:28:39.450606: step 2233, loss 0.072817, acc 0.984375, prec 0.0813594, recall 0.836012
2017-12-09T22:28:39.749038: step 2234, loss 0.153638, acc 0.953125, prec 0.0813503, recall 0.836012
2017-12-09T22:28:40.053657: step 2235, loss 0.0958539, acc 0.96875, prec 0.0813614, recall 0.836044
2017-12-09T22:28:40.349612: step 2236, loss 0.0771874, acc 0.976562, prec 0.0813568, recall 0.836044
2017-12-09T22:28:40.646746: step 2237, loss 0.0259952, acc 1, prec 0.081391, recall 0.836106
2017-12-09T22:28:40.948462: step 2238, loss 0.0727464, acc 0.992188, prec 0.0814237, recall 0.836169
2017-12-09T22:28:41.252363: step 2239, loss 2.59848, acc 0.976562, prec 0.0814207, recall 0.836009
2017-12-09T22:28:41.557851: step 2240, loss 0.887886, acc 0.96875, prec 0.0814332, recall 0.835881
2017-12-09T22:28:41.737482: step 2241, loss 0.0549344, acc 0.980392, prec 0.0814317, recall 0.835881
2017-12-09T22:28:42.043104: step 2242, loss 0.077041, acc 0.96875, prec 0.0814256, recall 0.835881
2017-12-09T22:28:42.342023: step 2243, loss 0.130361, acc 0.984375, prec 0.0814568, recall 0.835943
2017-12-09T22:28:42.646982: step 2244, loss 0.0518153, acc 0.984375, prec 0.0814538, recall 0.835943
2017-12-09T22:28:42.941983: step 2245, loss 0.221777, acc 0.9375, prec 0.08151, recall 0.836069
2017-12-09T22:28:43.244159: step 2246, loss 0.199606, acc 0.945312, prec 0.0815336, recall 0.836131
2017-12-09T22:28:43.544911: step 2247, loss 0.252903, acc 0.960938, prec 0.0815943, recall 0.836256
2017-12-09T22:28:43.844431: step 2248, loss 0.406309, acc 0.890625, prec 0.0815902, recall 0.836287
2017-12-09T22:28:44.144861: step 2249, loss 0.148249, acc 0.945312, prec 0.0816137, recall 0.83635
2017-12-09T22:28:44.444618: step 2250, loss 0.345859, acc 0.890625, prec 0.0816095, recall 0.836381
2017-12-09T22:28:44.739973: step 2251, loss 0.311322, acc 0.898438, prec 0.0816239, recall 0.836443
2017-12-09T22:28:45.037637: step 2252, loss 0.294712, acc 0.929688, prec 0.0816444, recall 0.836506
2017-12-09T22:28:45.338332: step 2253, loss 0.239462, acc 0.9375, prec 0.0816323, recall 0.836506
2017-12-09T22:28:45.636451: step 2254, loss 0.208954, acc 0.921875, prec 0.0816853, recall 0.83663
2017-12-09T22:28:45.933152: step 2255, loss 0.128227, acc 0.953125, prec 0.0816762, recall 0.83663
2017-12-09T22:28:46.226511: step 2256, loss 0.0541611, acc 0.992188, prec 0.0816747, recall 0.83663
2017-12-09T22:28:46.523719: step 2257, loss 0.245093, acc 0.953125, prec 0.0817338, recall 0.836754
2017-12-09T22:28:46.827545: step 2258, loss 0.21747, acc 0.921875, prec 0.0817357, recall 0.836785
2017-12-09T22:28:47.127482: step 2259, loss 0.19766, acc 0.914062, prec 0.081736, recall 0.836816
2017-12-09T22:28:47.427668: step 2260, loss 0.139686, acc 0.976562, prec 0.0817656, recall 0.836878
2017-12-09T22:28:47.729038: step 2261, loss 0.135818, acc 0.960938, prec 0.081792, recall 0.83694
2017-12-09T22:28:48.026617: step 2262, loss 0.0411492, acc 0.992188, prec 0.0818076, recall 0.836971
2017-12-09T22:28:48.328658: step 2263, loss 0.128891, acc 0.96875, prec 0.0818185, recall 0.837002
2017-12-09T22:28:48.625564: step 2264, loss 0.0556121, acc 0.96875, prec 0.0818295, recall 0.837033
2017-12-09T22:28:48.924167: step 2265, loss 0.0346961, acc 0.992188, prec 0.081845, recall 0.837064
2017-12-09T22:28:49.223873: step 2266, loss 0.535781, acc 0.984375, prec 0.081876, recall 0.837126
2017-12-09T22:28:49.528558: step 2267, loss 0.0131254, acc 1, prec 0.081876, recall 0.837126
2017-12-09T22:28:49.827375: step 2268, loss 0.386209, acc 0.960938, prec 0.0819025, recall 0.837187
2017-12-09T22:28:50.136930: step 2269, loss 0.033812, acc 0.992188, prec 0.081918, recall 0.837218
2017-12-09T22:28:50.446648: step 2270, loss 0.184175, acc 0.96875, prec 0.081963, recall 0.837311
2017-12-09T22:28:50.750237: step 2271, loss 0.133268, acc 0.992188, prec 0.0819955, recall 0.837372
2017-12-09T22:28:51.050551: step 2272, loss 0.130012, acc 0.96875, prec 0.0820234, recall 0.837434
2017-12-09T22:28:51.355500: step 2273, loss 0.0442094, acc 0.984375, prec 0.0820714, recall 0.837526
2017-12-09T22:28:51.652195: step 2274, loss 0.0370014, acc 0.984375, prec 0.0820684, recall 0.837526
2017-12-09T22:28:51.947119: step 2275, loss 0.23004, acc 0.984375, prec 0.0821164, recall 0.837618
2017-12-09T22:28:52.248156: step 2276, loss 0.175269, acc 0.945312, prec 0.0821227, recall 0.837649
2017-12-09T22:28:52.544923: step 2277, loss 0.205579, acc 0.96875, prec 0.0821847, recall 0.837771
2017-12-09T22:28:52.851218: step 2278, loss 0.642621, acc 0.976562, prec 0.0822156, recall 0.837675
2017-12-09T22:28:53.150407: step 2279, loss 0.24397, acc 0.96875, prec 0.0822605, recall 0.837766
2017-12-09T22:28:53.455618: step 2280, loss 0.0850215, acc 0.976562, prec 0.082273, recall 0.837797
2017-12-09T22:28:53.755105: step 2281, loss 0.050694, acc 0.992188, prec 0.0822884, recall 0.837828
2017-12-09T22:28:54.065385: step 2282, loss 0.132478, acc 0.953125, prec 0.0823133, recall 0.837889
2017-12-09T22:28:54.369973: step 2283, loss 0.0949471, acc 0.976562, prec 0.0823257, recall 0.837919
2017-12-09T22:28:54.681263: step 2284, loss 0.0549637, acc 0.976562, prec 0.0823211, recall 0.837919
2017-12-09T22:28:54.980861: step 2285, loss 0.141931, acc 0.953125, prec 0.082346, recall 0.83798
2017-12-09T22:28:55.278720: step 2286, loss 0.340054, acc 0.984375, prec 0.0823769, recall 0.838041
2017-12-09T22:28:55.583547: step 2287, loss 0.119154, acc 0.96875, prec 0.0824557, recall 0.838194
2017-12-09T22:28:55.887791: step 2288, loss 0.0449746, acc 0.984375, prec 0.0824696, recall 0.838224
2017-12-09T22:28:56.186141: step 2289, loss 0.119778, acc 0.960938, prec 0.082479, recall 0.838255
2017-12-09T22:28:56.484988: step 2290, loss 0.203711, acc 0.96875, prec 0.0825068, recall 0.838315
2017-12-09T22:28:56.784647: step 2291, loss 0.14402, acc 0.960938, prec 0.0825501, recall 0.838407
2017-12-09T22:28:57.084898: step 2292, loss 0.108358, acc 0.992188, prec 0.0825656, recall 0.838437
2017-12-09T22:28:57.381326: step 2293, loss 0.192216, acc 0.96875, prec 0.0826104, recall 0.838528
2017-12-09T22:28:57.683099: step 2294, loss 0.160366, acc 0.960938, prec 0.0826027, recall 0.838528
2017-12-09T22:28:57.985899: step 2295, loss 0.204946, acc 0.929688, prec 0.0826229, recall 0.838589
2017-12-09T22:28:58.297908: step 2296, loss 0.0603506, acc 0.976562, prec 0.0826353, recall 0.838619
2017-12-09T22:28:58.599981: step 2297, loss 0.139131, acc 0.953125, prec 0.0826431, recall 0.838649
2017-12-09T22:28:58.898521: step 2298, loss 0.103755, acc 0.976562, prec 0.0827063, recall 0.83877
2017-12-09T22:28:59.205575: step 2299, loss 0.0907575, acc 0.960938, prec 0.0827157, recall 0.8388
2017-12-09T22:28:59.504177: step 2300, loss 0.147288, acc 0.96875, prec 0.0827265, recall 0.838831
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-2300

2017-12-09T22:29:01.534892: step 2301, loss 0.0536221, acc 0.992188, prec 0.0827419, recall 0.838861
2017-12-09T22:29:01.937030: step 2302, loss 0.254025, acc 0.960938, prec 0.0827682, recall 0.838921
2017-12-09T22:29:02.235121: step 2303, loss 0.0871466, acc 0.953125, prec 0.082776, recall 0.838951
2017-12-09T22:29:03.255147: step 2304, loss 0.143791, acc 0.96875, prec 0.0828037, recall 0.839012
2017-12-09T22:29:04.470146: step 2305, loss 0.358718, acc 0.96875, prec 0.0828484, recall 0.839102
2017-12-09T22:29:04.839661: step 2306, loss 0.0799365, acc 0.96875, prec 0.0828593, recall 0.839132
2017-12-09T22:29:05.158922: step 2307, loss 0.125586, acc 0.960938, prec 0.0828685, recall 0.839162
2017-12-09T22:29:05.500132: step 2308, loss 0.111427, acc 0.953125, prec 0.0828763, recall 0.839192
2017-12-09T22:29:05.821378: step 2309, loss 0.2171, acc 0.960938, prec 0.0829025, recall 0.839252
2017-12-09T22:29:06.125728: step 2310, loss 0.0671203, acc 1, prec 0.0829194, recall 0.839282
2017-12-09T22:29:06.431569: step 2311, loss 0.128963, acc 0.984375, prec 0.0829502, recall 0.839342
2017-12-09T22:29:06.732127: step 2312, loss 0.118647, acc 0.96875, prec 0.0830118, recall 0.839462
2017-12-09T22:29:07.028956: step 2313, loss 0.0566638, acc 0.992188, prec 0.0830441, recall 0.839522
2017-12-09T22:29:07.329085: step 2314, loss 0.369173, acc 0.96875, prec 0.0830888, recall 0.839612
2017-12-09T22:29:07.631930: step 2315, loss 1.53826, acc 0.929688, prec 0.0831273, recall 0.839545
2017-12-09T22:29:07.932794: step 2316, loss 0.174114, acc 0.953125, prec 0.0831857, recall 0.839665
2017-12-09T22:29:08.235980: step 2317, loss 0.190359, acc 0.953125, prec 0.0831935, recall 0.839695
2017-12-09T22:29:08.535611: step 2318, loss 0.148016, acc 0.96875, prec 0.0832549, recall 0.839814
2017-12-09T22:29:08.832781: step 2319, loss 0.141213, acc 0.96875, prec 0.0832488, recall 0.839814
2017-12-09T22:29:09.131327: step 2320, loss 0.205464, acc 0.953125, prec 0.0832734, recall 0.839874
2017-12-09T22:29:09.430882: step 2321, loss 0.123218, acc 0.953125, prec 0.0832811, recall 0.839903
2017-12-09T22:29:09.732727: step 2322, loss 0.147341, acc 0.960938, prec 0.0833072, recall 0.839963
2017-12-09T22:29:10.030844: step 2323, loss 0.165409, acc 0.953125, prec 0.0833487, recall 0.840052
2017-12-09T22:29:10.335687: step 2324, loss 0.182039, acc 0.945312, prec 0.0833717, recall 0.840111
2017-12-09T22:29:10.637231: step 2325, loss 0.176409, acc 0.929688, prec 0.0834086, recall 0.8402
2017-12-09T22:29:10.930275: step 2326, loss 0.262162, acc 0.914062, prec 0.0833917, recall 0.8402
2017-12-09T22:29:11.226929: step 2327, loss 0.238839, acc 0.945312, prec 0.0834484, recall 0.840319
2017-12-09T22:29:11.525935: step 2328, loss 0.319758, acc 0.921875, prec 0.0835006, recall 0.840437
2017-12-09T22:29:11.825919: step 2329, loss 0.217454, acc 0.9375, prec 0.0834883, recall 0.840437
2017-12-09T22:29:12.122994: step 2330, loss 0.185106, acc 0.929688, prec 0.0834744, recall 0.840437
2017-12-09T22:29:12.423942: step 2331, loss 0.128778, acc 0.945312, prec 0.0834637, recall 0.840437
2017-12-09T22:29:12.724979: step 2332, loss 0.128973, acc 0.9375, prec 0.0834683, recall 0.840467
2017-12-09T22:29:13.025185: step 2333, loss 0.083306, acc 0.976562, prec 0.083548, recall 0.840615
2017-12-09T22:29:13.339160: step 2334, loss 0.22943, acc 0.921875, prec 0.0835663, recall 0.840674
2017-12-09T22:29:13.642197: step 2335, loss 0.257085, acc 0.929688, prec 0.0835694, recall 0.840703
2017-12-09T22:29:13.943079: step 2336, loss 0.186288, acc 0.960938, prec 0.0835785, recall 0.840733
2017-12-09T22:29:14.239563: step 2337, loss 0.0737808, acc 0.96875, prec 0.0836061, recall 0.840791
2017-12-09T22:29:14.539967: step 2338, loss 0.801228, acc 0.960938, prec 0.0836505, recall 0.840724
2017-12-09T22:29:14.849307: step 2339, loss 0.120065, acc 0.960938, prec 0.0836596, recall 0.840754
2017-12-09T22:29:15.142500: step 2340, loss 0.0961425, acc 0.960938, prec 0.0836519, recall 0.840754
2017-12-09T22:29:15.446566: step 2341, loss 0.118512, acc 0.96875, prec 0.0836626, recall 0.840783
2017-12-09T22:29:15.743986: step 2342, loss 0.148811, acc 0.960938, prec 0.0836718, recall 0.840813
2017-12-09T22:29:16.042874: step 2343, loss 0.0339788, acc 0.992188, prec 0.0836702, recall 0.840813
2017-12-09T22:29:16.339538: step 2344, loss 0.814242, acc 0.953125, prec 0.0837131, recall 0.840746
2017-12-09T22:29:16.642356: step 2345, loss 0.136744, acc 0.953125, prec 0.0837038, recall 0.840746
2017-12-09T22:29:16.942374: step 2346, loss 0.203981, acc 0.96875, prec 0.0837314, recall 0.840804
2017-12-09T22:29:17.244801: step 2347, loss 0.169709, acc 0.9375, prec 0.0837191, recall 0.840804
2017-12-09T22:29:17.547839: step 2348, loss 0.127336, acc 0.945312, prec 0.0837588, recall 0.840892
2017-12-09T22:29:17.845189: step 2349, loss 0.271811, acc 0.921875, prec 0.0837602, recall 0.840922
2017-12-09T22:29:18.146965: step 2350, loss 0.744597, acc 0.953125, prec 0.0837862, recall 0.840825
2017-12-09T22:29:18.450988: step 2351, loss 0.247396, acc 0.960938, prec 0.0838121, recall 0.840884
2017-12-09T22:29:18.752922: step 2352, loss 0.0868856, acc 0.976562, prec 0.0838075, recall 0.840884
2017-12-09T22:29:19.050918: step 2353, loss 0.073133, acc 0.96875, prec 0.0838013, recall 0.840884
2017-12-09T22:29:19.350173: step 2354, loss 0.218172, acc 0.9375, prec 0.0838059, recall 0.840913
2017-12-09T22:29:19.648685: step 2355, loss 0.201604, acc 0.960938, prec 0.0838486, recall 0.841001
2017-12-09T22:29:19.946485: step 2356, loss 0.112682, acc 0.960938, prec 0.0838745, recall 0.84106
2017-12-09T22:29:20.260489: step 2357, loss 0.229346, acc 0.960938, prec 0.0839172, recall 0.841147
2017-12-09T22:29:20.559651: step 2358, loss 0.0781466, acc 0.96875, prec 0.0839279, recall 0.841176
2017-12-09T22:29:20.856636: step 2359, loss 0.109766, acc 0.960938, prec 0.0839706, recall 0.841264
2017-12-09T22:29:21.153585: step 2360, loss 0.33207, acc 0.953125, prec 0.0840453, recall 0.84141
2017-12-09T22:29:21.454914: step 2361, loss 0.271143, acc 0.945312, prec 0.0840681, recall 0.841468
2017-12-09T22:29:21.752548: step 2362, loss 0.199031, acc 0.953125, prec 0.0840757, recall 0.841497
2017-12-09T22:29:22.048727: step 2363, loss 0.121131, acc 0.960938, prec 0.0840847, recall 0.841526
2017-12-09T22:29:22.345441: step 2364, loss 0.19464, acc 0.953125, prec 0.0841091, recall 0.841584
2017-12-09T22:29:22.642208: step 2365, loss 0.297804, acc 0.953125, prec 0.0841669, recall 0.8417
2017-12-09T22:29:22.942610: step 2366, loss 0.100241, acc 0.984375, prec 0.0841974, recall 0.841758
2017-12-09T22:29:23.239799: step 2367, loss 0.343476, acc 0.921875, prec 0.0841988, recall 0.841787
2017-12-09T22:29:23.537486: step 2368, loss 0.171104, acc 0.953125, prec 0.0841895, recall 0.841787
2017-12-09T22:29:23.839202: step 2369, loss 0.147486, acc 0.960938, prec 0.0842321, recall 0.841874
2017-12-09T22:29:24.137638: step 2370, loss 0.141464, acc 0.945312, prec 0.0842213, recall 0.841874
2017-12-09T22:29:24.437680: step 2371, loss 0.226573, acc 0.960938, prec 0.0842304, recall 0.841903
2017-12-09T22:29:24.739954: step 2372, loss 0.382368, acc 0.914062, prec 0.0842469, recall 0.841961
2017-12-09T22:29:25.037358: step 2373, loss 0.121851, acc 0.976562, prec 0.0842758, recall 0.842019
2017-12-09T22:29:25.342686: step 2374, loss 0.130101, acc 0.976562, prec 0.084288, recall 0.842048
2017-12-09T22:29:25.645179: step 2375, loss 0.309327, acc 0.945312, prec 0.0843442, recall 0.842163
2017-12-09T22:29:25.945341: step 2376, loss 0.467715, acc 0.96875, prec 0.084405, recall 0.842278
2017-12-09T22:29:26.248015: step 2377, loss 0.113334, acc 0.960938, prec 0.0843973, recall 0.842278
2017-12-09T22:29:26.543120: step 2378, loss 0.102767, acc 0.984375, prec 0.0844612, recall 0.842393
2017-12-09T22:29:26.848857: step 2379, loss 0.138672, acc 0.960938, prec 0.084487, recall 0.842451
2017-12-09T22:29:27.151797: step 2380, loss 0.172401, acc 0.953125, prec 0.0845614, recall 0.842594
2017-12-09T22:29:27.450157: step 2381, loss 0.136427, acc 0.9375, prec 0.0845992, recall 0.84268
2017-12-09T22:29:27.759654: step 2382, loss 0.307228, acc 1, prec 0.0846327, recall 0.842738
2017-12-09T22:29:28.061820: step 2383, loss 0.183081, acc 0.960938, prec 0.0846417, recall 0.842766
2017-12-09T22:29:28.366642: step 2384, loss 0.262846, acc 0.9375, prec 0.0846628, recall 0.842823
2017-12-09T22:29:28.667640: step 2385, loss 0.262267, acc 0.960938, prec 0.0846885, recall 0.842881
2017-12-09T22:29:28.970066: step 2386, loss 0.15967, acc 0.976562, prec 0.0846838, recall 0.842881
2017-12-09T22:29:29.269553: step 2387, loss 0.153247, acc 0.960938, prec 0.0847263, recall 0.842966
2017-12-09T22:29:29.568294: step 2388, loss 0.158999, acc 0.953125, prec 0.0847504, recall 0.843023
2017-12-09T22:29:29.868992: step 2389, loss 0.185713, acc 0.9375, prec 0.0847715, recall 0.84308
2017-12-09T22:29:30.181405: step 2390, loss 0.204711, acc 0.945312, prec 0.0847606, recall 0.84308
2017-12-09T22:29:30.481758: step 2391, loss 0.132692, acc 0.960938, prec 0.0847863, recall 0.843137
2017-12-09T22:29:30.782332: step 2392, loss 0.0759043, acc 0.984375, prec 0.0847999, recall 0.843166
2017-12-09T22:29:31.082125: step 2393, loss 0.161722, acc 0.953125, prec 0.0848073, recall 0.843194
2017-12-09T22:29:31.379778: step 2394, loss 0.0940964, acc 0.953125, prec 0.0848147, recall 0.843223
2017-12-09T22:29:31.686334: step 2395, loss 0.0986086, acc 0.953125, prec 0.0848389, recall 0.84328
2017-12-09T22:29:31.988218: step 2396, loss 0.184782, acc 0.953125, prec 0.084863, recall 0.843336
2017-12-09T22:29:32.284737: step 2397, loss 0.122892, acc 0.960938, prec 0.0848886, recall 0.843393
2017-12-09T22:29:32.582485: step 2398, loss 0.873292, acc 0.9375, prec 0.0849112, recall 0.843297
2017-12-09T22:29:32.885074: step 2399, loss 0.277722, acc 0.960938, prec 0.0849201, recall 0.843325
2017-12-09T22:29:33.183128: step 2400, loss 0.301923, acc 0.953125, prec 0.0849442, recall 0.843382

Evaluation:
2017-12-09T22:29:38.003882: step 2400, loss 2.90657, acc 0.951127, prec 0.0854359, recall 0.831194

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_0/1512875785/checkpoints/model-2400

2017-12-09T22:29:39.273094: step 2401, loss 0.179779, acc 0.976562, prec 0.0854478, recall 0.831224
2017-12-09T22:29:39.582186: step 2402, loss 0.0699862, acc 0.984375, prec 0.0854613, recall 0.831253
2017-12-09T22:29:39.894890: step 2403, loss 0.270454, acc 0.9375, prec 0.0854985, recall 0.831342
2017-12-09T22:29:40.196188: step 2404, loss 0.245701, acc 0.960938, prec 0.0855238, recall 0.831401
2017-12-09T22:29:40.504561: step 2405, loss 0.148095, acc 0.96875, prec 0.0855342, recall 0.831431
2017-12-09T22:29:40.807632: step 2406, loss 0.205521, acc 0.960938, prec 0.0855595, recall 0.83149
2017-12-09T22:29:41.105076: step 2407, loss 0.0513696, acc 0.984375, prec 0.0855894, recall 0.831549
2017-12-09T22:29:41.411580: step 2408, loss 0.168755, acc 0.953125, prec 0.0856297, recall 0.831638
2017-12-09T22:29:41.709905: step 2409, loss 0.14799, acc 0.960938, prec 0.085688, recall 0.831756
2017-12-09T22:29:42.010843: step 2410, loss 0.330904, acc 0.921875, prec 0.0857055, recall 0.831815
2017-12-09T22:29:42.312012: step 2411, loss 0.0743339, acc 0.96875, prec 0.0856993, recall 0.831815
2017-12-09T22:29:42.608294: step 2412, loss 0.450395, acc 0.960938, prec 0.0857741, recall 0.831962
2017-12-09T22:29:42.912035: step 2413, loss 0.260142, acc 0.945312, prec 0.0858292, recall 0.83208
2017-12-09T22:29:43.210797: step 2414, loss 0.492265, acc 0.984375, prec 0.0858921, recall 0.832197
2017-12-09T22:29:43.510374: step 2415, loss 0.154722, acc 0.953125, prec 0.0858993, recall 0.832227
2017-12-09T22:29:43.810120: step 2416, loss 0.301945, acc 0.921875, prec 0.0859168, recall 0.832285
2017-12-09T22:29:44.107883: step 2417, loss 0.121472, acc 0.953125, prec 0.0859075, recall 0.832285
2017-12-09T22:29:44.404084: step 2418, loss 0.235049, acc 0.953125, prec 0.0859806, recall 0.832431
2017-12-09T22:29:44.700485: step 2419, loss 0.11081, acc 0.953125, prec 0.0860043, recall 0.83249
2017-12-09T22:29:44.999251: step 2420, loss 0.188145, acc 0.992188, prec 0.0860192, recall 0.832519
2017-12-09T22:29:45.307844: step 2421, loss 0.241078, acc 0.960938, prec 0.0860773, recall 0.832636
2017-12-09T22:29:45.612692: step 2422, loss 0.117886, acc 0.976562, prec 0.0861385, recall 0.832753
2017-12-09T22:29:45.907441: step 2423, loss 0.290327, acc 0.921875, prec 0.086156, recall 0.832811
2017-12-09T22:29:46.213047: step 2424, loss 0.06111, acc 0.96875, prec 0.0861497, recall 0.832811
2017-12-09T22:29:46.519119: step 2425, loss 0.0512023, acc 0.984375, prec 0.0861466, recall 0.832811
2017-12-09T22:29:46.816030: step 2426, loss 0.102852, acc 0.96875, prec 0.0861404, recall 0.832811
2017-12-09T22:29:47.113185: step 2427, loss 0.170658, acc 0.9375, prec 0.0861445, recall 0.83284
2017-12-09T22:29:47.408302: step 2428, loss 0.165264, acc 0.960938, prec 0.0861861, recall 0.832927
2017-12-09T22:29:47.709499: step 2429, loss 0.361981, acc 0.914062, prec 0.0861855, recall 0.832956
2017-12-09T22:29:48.016260: step 2430, loss 0.158982, acc 0.96875, prec 0.0861957, recall 0.832985
2017-12-09T22:29:48.315351: step 2431, loss 0.261106, acc 0.976562, prec 0.0862075, recall 0.833014
2017-12-09T22:29:48.615765: step 2432, loss 0.104604, acc 0.96875, prec 0.0862342, recall 0.833072
2017-12-09T22:29:48.915207: step 2433, loss 0.115447, acc 0.945312, prec 0.0862398, recall 0.833102
2017-12-09T22:29:49.212404: step 2434, loss 0.110315, acc 0.992188, prec 0.0862876, recall 0.833189
2017-12-09T22:29:49.512926: step 2435, loss 0.231419, acc 0.960938, prec 0.0863291, recall 0.833275
2017-12-09T22:29:49.824978: step 2436, loss 0.0582846, acc 0.984375, prec 0.0863425, recall 0.833304
2017-12-09T22:29:50.128370: step 2437, loss 0.129784, acc 0.960938, prec 0.0863676, recall 0.833362
2017-12-09T22:29:50.441306: step 2438, loss 0.0902418, acc 0.96875, prec 0.0863778, recall 0.833391
2017-12-09T22:29:50.746576: step 2439, loss 1.24901, acc 0.984375, prec 0.0863927, recall 0.833275
2017-12-09T22:29:51.048691: step 2440, loss 0.0445201, acc 0.992188, prec 0.086424, recall 0.833333
2017-12-09T22:29:51.350218: step 2441, loss 0.0388754, acc 0.984375, prec 0.0864209, recall 0.833333
2017-12-09T22:29:51.644889: step 2442, loss 0.109444, acc 0.96875, prec 0.0864311, recall 0.833362
2017-12-09T22:29:51.942478: step 2443, loss 0.0881286, acc 0.984375, prec 0.0864608, recall 0.83342
2017-12-09T22:29:52.246146: step 2444, loss 0.102286, acc 0.945312, prec 0.0864499, recall 0.83342
2017-12-09T22:29:52.547772: step 2445, loss 0.174856, acc 0.984375, prec 0.0864797, recall 0.833478
2017-12-09T22:29:52.847904: step 2446, loss 0.525384, acc 0.96875, prec 0.0865391, recall 0.833593
2017-12-09T22:29:53.151957: step 2447, loss 0.102373, acc 0.984375, prec 0.0865525, recall 0.833622
2017-12-09T22:29:53.460521: step 2448, loss 0.139274, acc 0.960938, prec 0.0865775, recall 0.833679
2017-12-09T22:29:53.762927: step 2449, loss 0.290878, acc 0.953125, prec 0.0866174, recall 0.833766
2017-12-09T22:29:54.067266: step 2450, loss 0.187964, acc 0.9375, prec 0.0866542, recall 0.833852
2017-12-09T22:29:54.370434: step 2451, loss 0.152311, acc 0.960938, prec 0.0866464, recall 0.833852
2017-12-09T22:29:54.672939: step 2452, loss 0.230494, acc 0.90625, prec 0.0866278, recall 0.833852
2017-12-09T22:29:54.967322: step 2453, loss 0.113718, acc 0.960938, prec 0.08662, recall 0.833852
2017-12-09T22:29:55.264291: step 2454, loss 0.224643, acc 0.898438, prec 0.0865998, recall 0.833852
2017-12-09T22:29:55.562353: step 2455, loss 0.216184, acc 0.914062, prec 0.0865991, recall 0.833881
2017-12-09T22:29:55.864632: step 2456, loss 0.241313, acc 0.914062, prec 0.0865983, recall 0.833909
2017-12-09T22:29:56.169656: step 2457, loss 0.326144, acc 0.953125, prec 0.086671, recall 0.834053
2017-12-09T22:29:56.472324: step 2458, loss 0.152267, acc 0.96875, prec 0.0866975, recall 0.83411
2017-12-09T22:29:56.776650: step 2459, loss 0.184391, acc 0.953125, prec 0.0867046, recall 0.834139
2017-12-09T22:29:57.072510: step 2460, loss 0.116141, acc 0.96875, prec 0.0867147, recall 0.834167
2017-12-09T22:29:57.373344: step 2461, loss 0.0625785, acc 0.976562, prec 0.0867101, recall 0.834167
2017-12-09T22:29:57.669096: step 2462, loss 0.156834, acc 0.953125, prec 0.0867335, recall 0.834225
2017-12-09T22:29:57.969696: step 2463, loss 0.0535119, acc 0.976562, prec 0.0867288, recall 0.834225
2017-12-09T22:29:58.271183: step 2464, loss 0.0566207, acc 0.984375, prec 0.0867421, recall 0.834253
2017-12-09T22:29:58.571647: step 2465, loss 0.0903131, acc 0.96875, prec 0.0867686, recall 0.83431
2017-12-09T22:29:58.869927: step 2466, loss 0.188997, acc 0.953125, prec 0.0867757, recall 0.834339
2017-12-09T22:29:59.174846: step 2467, loss 0.276025, acc 0.976562, prec 0.0868529, recall 0.834482
2017-12-09T22:29:59.473174: step 2468, loss 1.00475, acc 0.984375, prec 0.0868677, recall 0.834366
2017-12-09T22:29:59.778191: step 2469, loss 0.10889, acc 0.984375, prec 0.0868809, recall 0.834395
2017-12-09T22:30:00.086290: step 2470, loss 0.0810421, acc 0.984375, prec 0.0869105, recall 0.834452
2017-12-09T22:30:00.388704: step 2471, loss 0.0569281, acc 0.984375, prec 0.0869238, recall 0.83448
2017-12-09T22:30:00.686999: step 2472, loss 0.0631696, acc 0.976562, prec 0.0869846, recall 0.834594
2017-12-09T22:30:00.990220: step 2473, loss 0.182355, acc 0.9375, prec 0.0869885, recall 0.834623
2017-12-09T22:30:01.290739: step 2474, loss 0.0423235, acc 0.984375, prec 0.0870181, recall 0.834679
2017-12-09T22:30:01.599587: step 2475, loss 0.119658, acc 0.96875, prec 0.0870118, recall 0.834679
2017-12-09T22:30:01.898654: step 2476, loss 0.659307, acc 0.976562, prec 0.0870578, recall 0.834621
2017-12-09T22:30:02.204005: step 2477, loss 0.155136, acc 0.953125, prec 0.0870811, recall 0.834678
2017-12-09T22:30:02.501879: step 2478, loss 0.0864182, acc 0.960938, prec 0.0870733, recall 0.834678
2017-12-09T22:30:02.804363: step 2479, loss 0.0622947, acc 0.992188, prec 0.0871045, recall 0.834735
2017-12-09T22:30:03.108405: step 2480, loss 0.371655, acc 0.960938, prec 0.0871457, recall 0.83482
2017-12-09T22:30:03.405848: step 2481, loss 0.236629, acc 0.945312, prec 0.0871675, recall 0.834877
2017-12-09T22:30:03.706672: step 2482, loss 0.268125, acc 0.921875, prec 0.0871845, recall 0.834933
2017-12-09T22:30:04.012135: step 2483, loss 0.114559, acc 0.960938, prec 0.0871931, recall 0.834961
2017-12-09T22:30:04.309786: step 2484, loss 0.101024, acc 0.953125, prec 0.0871837, recall 0.834961
2017-12-09T22:30:04.622862: step 2485, loss 0.109687, acc 0.976562, prec 0.0872444, recall 0.835074
2017-12-09T22:30:04.923895: step 2486, loss 0.232702, acc 0.9375, prec 0.0873135, recall 0.835216
2017-12-09T22:30:05.231518: step 2487, loss 0.304236, acc 0.945312, prec 0.0873352, recall 0.835272
2017-12-09T22:30:05.538582: step 2488, loss 0.197056, acc 0.945312, prec 0.0873896, recall 0.835385
2017-12-09T22:30:05.835992: step 2489, loss 0.140373, acc 0.960938, prec 0.0874144, recall 0.835441
2017-12-09T22:30:06.017110: step 2490, loss 0.0414257, acc 1, prec 0.0874144, recall 0.835441
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 128
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_128_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607

Start training
2017-12-09T22:30:09.945957: step 1, loss 3.64686, acc 0.25, prec 0.0103093, recall 1
2017-12-09T22:30:10.239588: step 2, loss 10.3588, acc 0.726562, prec 0.00769231, recall 0.333333
2017-12-09T22:30:10.539679: step 3, loss 1.70806, acc 0.78125, prec 0.0188679, recall 0.5
2017-12-09T22:30:10.841920: step 4, loss 17.4547, acc 0.679688, prec 0.0153846, recall 0.272727
2017-12-09T22:30:11.141063: step 5, loss 12.4574, acc 0.539062, prec 0.0235294, recall 0.375
2017-12-09T22:30:11.438017: step 6, loss 6.98602, acc 0.351562, prec 0.0264706, recall 0.45
2017-12-09T22:30:11.735934: step 7, loss 5.22607, acc 0.179688, prec 0.0224215, recall 0.47619
2017-12-09T22:30:12.040242: step 8, loss 6.56156, acc 0.078125, prec 0.0177305, recall 0.47619
2017-12-09T22:30:12.333802: step 9, loss 6.69993, acc 0.109375, prec 0.0176471, recall 0.521739
2017-12-09T22:30:12.627855: step 10, loss 7.19077, acc 0.0625, prec 0.0162297, recall 0.541667
2017-12-09T22:30:12.922133: step 11, loss 6.7682, acc 0.125, prec 0.0153173, recall 0.56
2017-12-09T22:30:13.217705: step 12, loss 6.32414, acc 0.15625, prec 0.015625, recall 0.592593
2017-12-09T22:30:13.517164: step 13, loss 9.58185, acc 0.242188, prec 0.016919, recall 0.612903
2017-12-09T22:30:13.815841: step 14, loss 4.80067, acc 0.179688, prec 0.0178716, recall 0.647059
2017-12-09T22:30:14.110474: step 15, loss 4.49541, acc 0.289062, prec 0.0181406, recall 0.648649
2017-12-09T22:30:14.408113: step 16, loss 3.20891, acc 0.382812, prec 0.0185317, recall 0.65
2017-12-09T22:30:14.704509: step 17, loss 3.03155, acc 0.351562, prec 0.0174966, recall 0.65
2017-12-09T22:30:15.002870: step 18, loss 1.91983, acc 0.601562, prec 0.0181936, recall 0.666667
2017-12-09T22:30:15.301625: step 19, loss 4.5806, acc 0.609375, prec 0.0176322, recall 0.651163
2017-12-09T22:30:15.602001: step 20, loss 2.08158, acc 0.734375, prec 0.0172733, recall 0.636364
2017-12-09T22:30:15.910221: step 21, loss 1.57358, acc 0.757812, prec 0.0181378, recall 0.652174
2017-12-09T22:30:16.212836: step 22, loss 11.4788, acc 0.78125, prec 0.0184414, recall 0.632653
2017-12-09T22:30:16.512986: step 23, loss 2.42348, acc 0.710938, prec 0.0191972, recall 0.634615
2017-12-09T22:30:16.814144: step 24, loss 5.38444, acc 0.632812, prec 0.0198188, recall 0.625
2017-12-09T22:30:17.109550: step 25, loss 13.423, acc 0.65625, prec 0.020442, recall 0.616667
2017-12-09T22:30:17.407361: step 26, loss 10.5898, acc 0.570312, prec 0.0198711, recall 0.587302
2017-12-09T22:30:17.713785: step 27, loss 2.56745, acc 0.546875, prec 0.0197814, recall 0.59375
2017-12-09T22:30:18.017367: step 28, loss 2.87202, acc 0.429688, prec 0.0190572, recall 0.59375
2017-12-09T22:30:18.313666: step 29, loss 2.90138, acc 0.359375, prec 0.0187771, recall 0.6
2017-12-09T22:30:18.614715: step 30, loss 4.0379, acc 0.296875, prec 0.0179972, recall 0.6
2017-12-09T22:30:18.906386: step 31, loss 5.29193, acc 0.304688, prec 0.0177305, recall 0.597015
2017-12-09T22:30:19.205937: step 32, loss 3.86218, acc 0.289062, prec 0.017043, recall 0.597015
2017-12-09T22:30:19.501075: step 33, loss 3.96702, acc 0.3125, prec 0.0176374, recall 0.614286
2017-12-09T22:30:19.798983: step 34, loss 4.09954, acc 0.296875, prec 0.0177866, recall 0.625
2017-12-09T22:30:20.099574: step 35, loss 3.63823, acc 0.351562, prec 0.0179732, recall 0.635135
2017-12-09T22:30:20.401746: step 36, loss 2.97502, acc 0.460938, prec 0.0182427, recall 0.644737
2017-12-09T22:30:20.696576: step 37, loss 2.45936, acc 0.5, prec 0.0188885, recall 0.658228
2017-12-09T22:30:20.993916: step 38, loss 2.1224, acc 0.585938, prec 0.0195799, recall 0.670732
2017-12-09T22:30:21.292755: step 39, loss 10.9439, acc 0.625, prec 0.019951, recall 0.662791
2017-12-09T22:30:21.590516: step 40, loss 1.35433, acc 0.703125, prec 0.0200276, recall 0.666667
2017-12-09T22:30:21.892650: step 41, loss 2.48029, acc 0.671875, prec 0.0210813, recall 0.673913
2017-12-09T22:30:22.193882: step 42, loss 6.75002, acc 0.65625, prec 0.0211055, recall 0.670213
2017-12-09T22:30:22.494271: step 43, loss 1.3747, acc 0.671875, prec 0.0208127, recall 0.670213
2017-12-09T22:30:22.795058: step 44, loss 7.22817, acc 0.601562, prec 0.0217532, recall 0.67
2017-12-09T22:30:23.095990: step 45, loss 1.77163, acc 0.695312, prec 0.0217949, recall 0.673267
2017-12-09T22:30:23.392815: step 46, loss 4.07403, acc 0.648438, prec 0.0214918, recall 0.666667
2017-12-09T22:30:23.694101: step 47, loss 2.1256, acc 0.648438, prec 0.021502, recall 0.663462
2017-12-09T22:30:23.990835: step 48, loss 2.76374, acc 0.609375, prec 0.021479, recall 0.660377
2017-12-09T22:30:24.285842: step 49, loss 1.68968, acc 0.601562, prec 0.021148, recall 0.660377
2017-12-09T22:30:24.591424: step 50, loss 5.46879, acc 0.585938, prec 0.0211121, recall 0.657407
2017-12-09T22:30:24.892068: step 51, loss 2.23589, acc 0.515625, prec 0.0210158, recall 0.66055
2017-12-09T22:30:25.195157: step 52, loss 2.10591, acc 0.484375, prec 0.0206186, recall 0.66055
2017-12-09T22:30:25.496781: step 53, loss 1.92412, acc 0.554688, prec 0.0211149, recall 0.669643
2017-12-09T22:30:25.795809: step 54, loss 1.9593, acc 0.617188, prec 0.0208275, recall 0.669643
2017-12-09T22:30:26.094178: step 55, loss 1.75793, acc 0.65625, prec 0.0208448, recall 0.672566
2017-12-09T22:30:26.391291: step 56, loss 2.16528, acc 0.609375, prec 0.0213571, recall 0.681035
2017-12-09T22:30:26.690244: step 57, loss 3.34889, acc 0.703125, prec 0.0214075, recall 0.677966
2017-12-09T22:30:26.996924: step 58, loss 1.40797, acc 0.632812, prec 0.0211416, recall 0.677966
2017-12-09T22:30:27.291767: step 59, loss 0.947482, acc 0.710938, prec 0.0209369, recall 0.677966
2017-12-09T22:30:27.595023: step 60, loss 1.14741, acc 0.679688, prec 0.0207147, recall 0.677966
2017-12-09T22:30:27.895821: step 61, loss 5.80814, acc 0.757812, prec 0.021058, recall 0.677686
2017-12-09T22:30:28.195800: step 62, loss 0.772132, acc 0.828125, prec 0.0209397, recall 0.677686
2017-12-09T22:30:28.495989: step 63, loss 1.22999, acc 0.734375, prec 0.0207595, recall 0.677686
2017-12-09T22:30:28.796712: step 64, loss 1.16244, acc 0.75, prec 0.0205927, recall 0.677686
2017-12-09T22:30:29.102974: step 65, loss 5.2469, acc 0.78125, prec 0.020454, recall 0.672131
2017-12-09T22:30:29.399566: step 66, loss 0.582129, acc 0.84375, prec 0.0203524, recall 0.672131
2017-12-09T22:30:29.695164: step 67, loss 15.5743, acc 0.828125, prec 0.0202519, recall 0.66129
2017-12-09T22:30:30.007852: step 68, loss 6.98251, acc 0.75, prec 0.020103, recall 0.650794
2017-12-09T22:30:30.319313: step 69, loss 12.8895, acc 0.835938, prec 0.0204828, recall 0.651163
2017-12-09T22:30:30.625066: step 70, loss 0.828373, acc 0.742188, prec 0.0203193, recall 0.651163
2017-12-09T22:30:30.925490: step 71, loss 2.01605, acc 0.648438, prec 0.0201053, recall 0.646154
2017-12-09T22:30:31.228605: step 72, loss 2.11583, acc 0.601562, prec 0.0200993, recall 0.643939
2017-12-09T22:30:31.531172: step 73, loss 2.14212, acc 0.625, prec 0.0203319, recall 0.649254
2017-12-09T22:30:31.829246: step 74, loss 2.18065, acc 0.492188, prec 0.0207039, recall 0.656934
2017-12-09T22:30:32.132643: step 75, loss 2.14793, acc 0.59375, prec 0.0204592, recall 0.656934
2017-12-09T22:30:32.426369: step 76, loss 4.80226, acc 0.398438, prec 0.0201117, recall 0.652174
2017-12-09T22:30:32.727814: step 77, loss 5.04636, acc 0.429688, prec 0.0202242, recall 0.652482
2017-12-09T22:30:33.029755: step 78, loss 2.41835, acc 0.539062, prec 0.0199653, recall 0.652482
2017-12-09T22:30:33.332149: step 79, loss 2.43958, acc 0.460938, prec 0.0198803, recall 0.65493
2017-12-09T22:30:33.629589: step 80, loss 3.64599, acc 0.429688, prec 0.0197811, recall 0.657343
2017-12-09T22:30:33.929630: step 81, loss 6.59241, acc 0.492188, prec 0.0195183, recall 0.652778
2017-12-09T22:30:34.231370: step 82, loss 2.00121, acc 0.609375, prec 0.0197206, recall 0.657534
2017-12-09T22:30:34.533106: step 83, loss 2.03109, acc 0.523438, prec 0.0198743, recall 0.662162
2017-12-09T22:30:34.830889: step 84, loss 2.21355, acc 0.632812, prec 0.0200844, recall 0.662252
2017-12-09T22:30:35.131357: step 85, loss 1.44444, acc 0.671875, prec 0.0203066, recall 0.666667
2017-12-09T22:30:35.444394: step 86, loss 5.00676, acc 0.601562, prec 0.0202996, recall 0.664516
2017-12-09T22:30:35.749630: step 87, loss 8.1466, acc 0.65625, prec 0.0201369, recall 0.651899
2017-12-09T22:30:36.055190: step 88, loss 1.62542, acc 0.625, prec 0.0199496, recall 0.651899
2017-12-09T22:30:36.358867: step 89, loss 1.688, acc 0.617188, prec 0.0197621, recall 0.651899
2017-12-09T22:30:36.661170: step 90, loss 1.24859, acc 0.6875, prec 0.0197982, recall 0.654088
2017-12-09T22:30:36.965848: step 91, loss 1.52643, acc 0.671875, prec 0.0198263, recall 0.65625
2017-12-09T22:30:37.270935: step 92, loss 10.5665, acc 0.546875, prec 0.0198057, recall 0.646341
2017-12-09T22:30:37.576825: step 93, loss 1.94311, acc 0.554688, prec 0.019597, recall 0.646341
2017-12-09T22:30:37.875915: step 94, loss 2.2989, acc 0.5625, prec 0.0193962, recall 0.646341
2017-12-09T22:30:38.177668: step 95, loss 1.33908, acc 0.640625, prec 0.0195901, recall 0.650602
2017-12-09T22:30:38.478971: step 96, loss 4.66882, acc 0.46875, prec 0.0197097, recall 0.647059
2017-12-09T22:30:38.782177: step 97, loss 3.22027, acc 0.59375, prec 0.0197053, recall 0.645349
2017-12-09T22:30:39.081164: step 98, loss 3.00616, acc 0.5, prec 0.0198315, recall 0.645714
2017-12-09T22:30:39.381796: step 99, loss 2.38761, acc 0.523438, prec 0.0196215, recall 0.645714
2017-12-09T22:30:39.678899: step 100, loss 2.82427, acc 0.515625, prec 0.0197493, recall 0.649718
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-100

2017-12-09T22:30:40.956836: step 101, loss 2.45322, acc 0.53125, prec 0.0198811, recall 0.653631
2017-12-09T22:30:41.257570: step 102, loss 4.86545, acc 0.554688, prec 0.0196937, recall 0.65
2017-12-09T22:30:41.559660: step 103, loss 2.56722, acc 0.46875, prec 0.0196339, recall 0.651934
2017-12-09T22:30:41.855466: step 104, loss 2.08887, acc 0.546875, prec 0.0196078, recall 0.653846
2017-12-09T22:30:42.155677: step 105, loss 1.62636, acc 0.578125, prec 0.0197551, recall 0.657609
2017-12-09T22:30:42.452909: step 106, loss 1.86435, acc 0.570312, prec 0.0197379, recall 0.659459
2017-12-09T22:30:42.752865: step 107, loss 4.27942, acc 0.679688, prec 0.0200835, recall 0.661376
2017-12-09T22:30:43.054400: step 108, loss 1.69999, acc 0.617188, prec 0.0203952, recall 0.666667
2017-12-09T22:30:43.358644: step 109, loss 3.64083, acc 0.710938, prec 0.020434, recall 0.664948
2017-12-09T22:30:43.657840: step 110, loss 1.15153, acc 0.75, prec 0.020331, recall 0.664948
2017-12-09T22:30:43.958896: step 111, loss 4.47574, acc 0.71875, prec 0.0205265, recall 0.664975
2017-12-09T22:30:44.259080: step 112, loss 1.02425, acc 0.796875, prec 0.020596, recall 0.666667
2017-12-09T22:30:44.555762: step 113, loss 0.932557, acc 0.742188, prec 0.0207945, recall 0.67
2017-12-09T22:30:44.852343: step 114, loss 1.17755, acc 0.71875, prec 0.020679, recall 0.67
2017-12-09T22:30:45.151382: step 115, loss 1.17696, acc 0.8125, prec 0.0207533, recall 0.671642
2017-12-09T22:30:45.452437: step 116, loss 3.33434, acc 0.742188, prec 0.0206548, recall 0.665025
2017-12-09T22:30:45.754924: step 117, loss 1.07578, acc 0.765625, prec 0.0210078, recall 0.669903
2017-12-09T22:30:46.055224: step 118, loss 1.67488, acc 0.671875, prec 0.0208775, recall 0.666667
2017-12-09T22:30:46.351462: step 119, loss 1.29143, acc 0.695312, prec 0.020755, recall 0.666667
2017-12-09T22:30:46.650357: step 120, loss 1.63748, acc 0.671875, prec 0.0207711, recall 0.668269
2017-12-09T22:30:46.954140: step 121, loss 3.41585, acc 0.703125, prec 0.0209479, recall 0.668246
2017-12-09T22:30:47.250956: step 122, loss 1.68863, acc 0.734375, prec 0.021132, recall 0.671362
2017-12-09T22:30:47.553873: step 123, loss 1.31656, acc 0.65625, prec 0.0211392, recall 0.672897
2017-12-09T22:30:47.860151: step 124, loss 1.83449, acc 0.609375, prec 0.0211278, recall 0.674419
2017-12-09T22:30:48.161481: step 125, loss 1.45589, acc 0.632812, prec 0.0211257, recall 0.675926
2017-12-09T22:30:48.465132: step 126, loss 1.36299, acc 0.6875, prec 0.021145, recall 0.677419
2017-12-09T22:30:48.763144: step 127, loss 4.18003, acc 0.6875, prec 0.021167, recall 0.675799
2017-12-09T22:30:49.062624: step 128, loss 1.22262, acc 0.648438, prec 0.0210317, recall 0.675799
2017-12-09T22:30:49.363044: step 129, loss 1.51866, acc 0.679688, prec 0.0213247, recall 0.68018
2017-12-09T22:30:49.665407: step 130, loss 1.5867, acc 0.757812, prec 0.0217819, recall 0.685841
2017-12-09T22:30:49.966730: step 131, loss 1.13661, acc 0.789062, prec 0.0221103, recall 0.689956
2017-12-09T22:30:50.275866: step 132, loss 6.63158, acc 0.789062, prec 0.0224422, recall 0.688034
2017-12-09T22:30:50.580642: step 133, loss 3.70724, acc 0.6875, prec 0.0224563, recall 0.686441
2017-12-09T22:30:50.883512: step 134, loss 1.48368, acc 0.6875, prec 0.0224673, recall 0.687764
2017-12-09T22:30:51.181903: step 135, loss 1.02587, acc 0.71875, prec 0.0226244, recall 0.690377
2017-12-09T22:30:51.484590: step 136, loss 1.68482, acc 0.554688, prec 0.022449, recall 0.690377
2017-12-09T22:30:51.787169: step 137, loss 1.47961, acc 0.617188, prec 0.0224324, recall 0.691667
2017-12-09T22:30:52.084145: step 138, loss 2.16751, acc 0.65625, prec 0.0223028, recall 0.688797
2017-12-09T22:30:52.388637: step 139, loss 1.31892, acc 0.679688, prec 0.0225725, recall 0.692623
2017-12-09T22:30:52.685138: step 140, loss 3.15731, acc 0.640625, prec 0.0225674, recall 0.691057
2017-12-09T22:30:52.984100: step 141, loss 1.55324, acc 0.648438, prec 0.0224334, recall 0.691057
2017-12-09T22:30:53.285450: step 142, loss 5.56344, acc 0.570312, prec 0.0224027, recall 0.689516
2017-12-09T22:30:53.580848: step 143, loss 4.37087, acc 0.609375, prec 0.0225143, recall 0.689243
2017-12-09T22:30:53.877175: step 144, loss 3.25573, acc 0.53125, prec 0.0225952, recall 0.688976
2017-12-09T22:30:54.182317: step 145, loss 2.3907, acc 0.546875, prec 0.0224273, recall 0.688976
2017-12-09T22:30:54.478602: step 146, loss 2.01237, acc 0.554688, prec 0.0226377, recall 0.692607
2017-12-09T22:30:54.779457: step 147, loss 3.71471, acc 0.5625, prec 0.0226039, recall 0.69112
2017-12-09T22:30:55.077415: step 148, loss 5.58966, acc 0.507812, prec 0.0224283, recall 0.688462
2017-12-09T22:30:55.372991: step 149, loss 3.05796, acc 0.4375, prec 0.0225919, recall 0.692015
2017-12-09T22:30:55.667586: step 150, loss 5.55498, acc 0.429688, prec 0.0223917, recall 0.689394
2017-12-09T22:30:55.968696: step 151, loss 3.04277, acc 0.398438, prec 0.0224199, recall 0.691729
2017-12-09T22:30:56.267134: step 152, loss 3.27825, acc 0.390625, prec 0.0223268, recall 0.692884
2017-12-09T22:30:56.565119: step 153, loss 2.3902, acc 0.5, prec 0.0223898, recall 0.695167
2017-12-09T22:30:56.860654: step 154, loss 2.76711, acc 0.5, prec 0.0223357, recall 0.696296
2017-12-09T22:30:57.160984: step 155, loss 2.02361, acc 0.578125, prec 0.0226549, recall 0.70073
2017-12-09T22:30:57.456051: step 156, loss 2.40713, acc 0.59375, prec 0.0228605, recall 0.703971
2017-12-09T22:30:57.759839: step 157, loss 1.55448, acc 0.65625, prec 0.023085, recall 0.707143
2017-12-09T22:30:58.064598: step 158, loss 1.79741, acc 0.59375, prec 0.0229459, recall 0.707143
2017-12-09T22:30:58.363718: step 159, loss 2.08083, acc 0.6875, prec 0.0228426, recall 0.704626
2017-12-09T22:30:58.668200: step 160, loss 2.59659, acc 0.71875, prec 0.0229753, recall 0.704225
2017-12-09T22:30:58.970749: step 161, loss 1.68596, acc 0.773438, prec 0.0231254, recall 0.703833
2017-12-09T22:30:59.272317: step 162, loss 1.26731, acc 0.703125, prec 0.0230252, recall 0.703833
2017-12-09T22:30:59.571901: step 163, loss 1.12392, acc 0.765625, prec 0.0231687, recall 0.705882
2017-12-09T22:30:59.881773: step 164, loss 8.6216, acc 0.742188, prec 0.0231979, recall 0.702055
2017-12-09T22:31:00.195669: step 165, loss 3.40844, acc 0.796875, prec 0.0233529, recall 0.701695
2017-12-09T22:31:00.500190: step 166, loss 3.48891, acc 0.648438, prec 0.0232401, recall 0.69697
2017-12-09T22:31:00.804565: step 167, loss 5.46231, acc 0.640625, prec 0.0231233, recall 0.694631
2017-12-09T22:31:01.106568: step 168, loss 1.72526, acc 0.570312, prec 0.0229821, recall 0.694631
2017-12-09T22:31:01.406511: step 169, loss 2.33744, acc 0.585938, prec 0.0229555, recall 0.695652
2017-12-09T22:31:01.707672: step 170, loss 1.82217, acc 0.5625, prec 0.0232431, recall 0.69967
2017-12-09T22:31:02.005754: step 171, loss 1.9403, acc 0.625, prec 0.0232279, recall 0.700658
2017-12-09T22:31:02.302713: step 172, loss 2.28392, acc 0.484375, prec 0.0230619, recall 0.700658
2017-12-09T22:31:02.604505: step 173, loss 2.94163, acc 0.515625, prec 0.0233308, recall 0.702265
2017-12-09T22:31:02.901003: step 174, loss 2.12247, acc 0.5625, prec 0.0231912, recall 0.702265
2017-12-09T22:31:03.204841: step 175, loss 3.77888, acc 0.546875, prec 0.0231545, recall 0.700965
2017-12-09T22:31:03.497829: step 176, loss 1.479, acc 0.609375, prec 0.0232386, recall 0.702875
2017-12-09T22:31:03.792229: step 177, loss 6.50564, acc 0.546875, prec 0.0234096, recall 0.701258
2017-12-09T22:31:04.092970: step 178, loss 2.14493, acc 0.585938, prec 0.023382, recall 0.702194
2017-12-09T22:31:04.390963: step 179, loss 1.53638, acc 0.609375, prec 0.0234635, recall 0.70405
2017-12-09T22:31:04.688235: step 180, loss 1.87383, acc 0.664062, prec 0.023561, recall 0.705882
2017-12-09T22:31:04.988509: step 181, loss 1.74285, acc 0.65625, prec 0.0234544, recall 0.705882
2017-12-09T22:31:05.302551: step 182, loss 5.23551, acc 0.671875, prec 0.0235559, recall 0.705521
2017-12-09T22:31:05.599749: step 183, loss 1.31534, acc 0.703125, prec 0.0235642, recall 0.706422
2017-12-09T22:31:05.900603: step 184, loss 6.95499, acc 0.71875, prec 0.0236837, recall 0.701807
2017-12-09T22:31:06.202332: step 185, loss 1.60951, acc 0.640625, prec 0.023771, recall 0.703593
2017-12-09T22:31:06.502912: step 186, loss 1.83571, acc 0.632812, prec 0.0240515, recall 0.707101
2017-12-09T22:31:06.802583: step 187, loss 2.22137, acc 0.570312, prec 0.0241145, recall 0.708824
2017-12-09T22:31:07.100556: step 188, loss 3.84317, acc 0.578125, prec 0.0244727, recall 0.710983
2017-12-09T22:31:07.399803: step 189, loss 2.73081, acc 0.632812, prec 0.0244579, recall 0.70977
2017-12-09T22:31:07.700916: step 190, loss 2.81989, acc 0.460938, prec 0.0242919, recall 0.70977
2017-12-09T22:31:08.000716: step 191, loss 2.23199, acc 0.492188, prec 0.0242329, recall 0.710602
2017-12-09T22:31:08.296693: step 192, loss 3.47764, acc 0.492188, prec 0.0241771, recall 0.709402
2017-12-09T22:31:08.596111: step 193, loss 2.52046, acc 0.4375, prec 0.0240093, recall 0.709402
2017-12-09T22:31:08.896193: step 194, loss 5.61105, acc 0.515625, prec 0.0241518, recall 0.707865
2017-12-09T22:31:09.200681: step 195, loss 3.80543, acc 0.53125, prec 0.024202, recall 0.707521
2017-12-09T22:31:09.500992: step 196, loss 2.96402, acc 0.421875, prec 0.0244018, recall 0.710744
2017-12-09T22:31:09.804506: step 197, loss 2.16234, acc 0.546875, prec 0.0245439, recall 0.713115
2017-12-09T22:31:10.106337: step 198, loss 2.59847, acc 0.484375, prec 0.0244837, recall 0.713896
2017-12-09T22:31:10.410815: step 199, loss 2.17109, acc 0.554688, prec 0.0244447, recall 0.714674
2017-12-09T22:31:10.710962: step 200, loss 2.83072, acc 0.40625, prec 0.0245433, recall 0.716981
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-200

2017-12-09T22:31:11.925403: step 201, loss 1.85725, acc 0.570312, prec 0.0245984, recall 0.718499
2017-12-09T22:31:12.225113: step 202, loss 5.19373, acc 0.664062, prec 0.0247715, recall 0.718833
2017-12-09T22:31:12.528759: step 203, loss 4.91213, acc 0.6875, prec 0.0247723, recall 0.717678
2017-12-09T22:31:12.828638: step 204, loss 1.87083, acc 0.585938, prec 0.0250068, recall 0.720627
2017-12-09T22:31:13.125565: step 205, loss 1.38449, acc 0.679688, prec 0.0250903, recall 0.722078
2017-12-09T22:31:13.423293: step 206, loss 3.54454, acc 0.625, prec 0.0250719, recall 0.72093
2017-12-09T22:31:13.717999: step 207, loss 1.60092, acc 0.648438, prec 0.0253199, recall 0.723785
2017-12-09T22:31:14.014435: step 208, loss 0.907554, acc 0.703125, prec 0.025321, recall 0.72449
2017-12-09T22:31:14.316273: step 209, loss 1.26412, acc 0.695312, prec 0.0252332, recall 0.72449
2017-12-09T22:31:14.619180: step 210, loss 0.677303, acc 0.804688, prec 0.0252637, recall 0.725191
2017-12-09T22:31:14.925149: step 211, loss 2.41471, acc 0.734375, prec 0.0253623, recall 0.724747
2017-12-09T22:31:15.233826: step 212, loss 0.634303, acc 0.90625, prec 0.0254215, recall 0.725441
2017-12-09T22:31:15.536856: step 213, loss 11.4696, acc 0.804688, prec 0.0254558, recall 0.7225
2017-12-09T22:31:15.837321: step 214, loss 3.16765, acc 0.789062, prec 0.0253977, recall 0.720698
2017-12-09T22:31:16.140769: step 215, loss 3.89183, acc 0.773438, prec 0.0255916, recall 0.720988
2017-12-09T22:31:16.441685: step 216, loss 13.1182, acc 0.703125, prec 0.0255111, recall 0.717445
2017-12-09T22:31:16.740846: step 217, loss 1.21932, acc 0.703125, prec 0.0254267, recall 0.717445
2017-12-09T22:31:17.045588: step 218, loss 2.62557, acc 0.585938, prec 0.0253966, recall 0.716381
2017-12-09T22:31:17.347009: step 219, loss 2.39406, acc 0.539062, prec 0.0254354, recall 0.717762
2017-12-09T22:31:17.646020: step 220, loss 2.4166, acc 0.515625, prec 0.0254673, recall 0.719128
2017-12-09T22:31:17.946598: step 221, loss 3.16288, acc 0.445312, prec 0.0256454, recall 0.721823
2017-12-09T22:31:18.247119: step 222, loss 2.70848, acc 0.453125, prec 0.0255759, recall 0.722488
2017-12-09T22:31:18.547731: step 223, loss 2.84669, acc 0.492188, prec 0.0255179, recall 0.72315
2017-12-09T22:31:18.844178: step 224, loss 3.11537, acc 0.4375, prec 0.0256088, recall 0.725118
2017-12-09T22:31:19.141166: step 225, loss 2.92668, acc 0.429688, prec 0.0255344, recall 0.725768
2017-12-09T22:31:19.446253: step 226, loss 2.94227, acc 0.40625, prec 0.0255351, recall 0.727059
2017-12-09T22:31:19.750197: step 227, loss 4.57277, acc 0.484375, prec 0.0255588, recall 0.726636
2017-12-09T22:31:20.053980: step 228, loss 2.32666, acc 0.507812, prec 0.0256662, recall 0.728538
2017-12-09T22:31:20.365450: step 229, loss 2.124, acc 0.523438, prec 0.0256973, recall 0.729792
2017-12-09T22:31:20.665517: step 230, loss 1.81199, acc 0.53125, prec 0.0256514, recall 0.730415
2017-12-09T22:31:20.964746: step 231, loss 1.37216, acc 0.632812, prec 0.0255542, recall 0.730415
2017-12-09T22:31:21.269595: step 232, loss 2.99045, acc 0.703125, prec 0.0257131, recall 0.730594
2017-12-09T22:31:21.571403: step 233, loss 1.15203, acc 0.742188, prec 0.0258013, recall 0.731818
2017-12-09T22:31:21.869623: step 234, loss 1.12502, acc 0.71875, prec 0.0259605, recall 0.733634
2017-12-09T22:31:22.168210: step 235, loss 0.805058, acc 0.828125, prec 0.0259927, recall 0.734234
2017-12-09T22:31:22.469743: step 236, loss 0.655085, acc 0.84375, prec 0.0260288, recall 0.734831
2017-12-09T22:31:22.776361: step 237, loss 4.83328, acc 0.804688, prec 0.026136, recall 0.732739
2017-12-09T22:31:23.081225: step 238, loss 12.4553, acc 0.867188, prec 0.0261822, recall 0.730088
2017-12-09T22:31:23.388696: step 239, loss 6.32363, acc 0.859375, prec 0.026151, recall 0.725275
2017-12-09T22:31:23.693102: step 240, loss 1.13284, acc 0.734375, prec 0.0261577, recall 0.725877
2017-12-09T22:31:23.993789: step 241, loss 1.08057, acc 0.625, prec 0.0261356, recall 0.726477
2017-12-09T22:31:24.295339: step 242, loss 1.65933, acc 0.625, prec 0.0260372, recall 0.726477
2017-12-09T22:31:24.597981: step 243, loss 2.05927, acc 0.5, prec 0.0260591, recall 0.727669
2017-12-09T22:31:24.898963: step 244, loss 1.82518, acc 0.570312, prec 0.0262504, recall 0.730022
2017-12-09T22:31:25.200673: step 245, loss 1.70382, acc 0.53125, prec 0.0262039, recall 0.730603
2017-12-09T22:31:25.504950: step 246, loss 4.18218, acc 0.53125, prec 0.0260849, recall 0.729032
2017-12-09T22:31:25.805331: step 247, loss 4.4, acc 0.460938, prec 0.0260237, recall 0.728051
2017-12-09T22:31:26.105843: step 248, loss 2.29839, acc 0.5, prec 0.0258969, recall 0.728051
2017-12-09T22:31:26.290740: step 249, loss 2.13801, acc 0.509804, prec 0.0258477, recall 0.728051
2017-12-09T22:31:26.592620: step 250, loss 1.94922, acc 0.476562, prec 0.0257903, recall 0.728632
2017-12-09T22:31:26.889690: step 251, loss 5.1017, acc 0.640625, prec 0.0257029, recall 0.727079
2017-12-09T22:31:27.187008: step 252, loss 1.70156, acc 0.632812, prec 0.0259048, recall 0.729387
2017-12-09T22:31:27.485196: step 253, loss 1.40631, acc 0.65625, prec 0.0259653, recall 0.730526
2017-12-09T22:31:27.786352: step 254, loss 1.21179, acc 0.71875, prec 0.0258955, recall 0.730526
2017-12-09T22:31:28.080499: step 255, loss 1.117, acc 0.65625, prec 0.0258832, recall 0.731092
2017-12-09T22:31:28.380510: step 256, loss 0.970997, acc 0.765625, prec 0.0260424, recall 0.732777
2017-12-09T22:31:28.682490: step 257, loss 0.661531, acc 0.757812, prec 0.0259827, recall 0.732777
2017-12-09T22:31:28.986125: step 258, loss 2.658, acc 0.765625, prec 0.025999, recall 0.731809
2017-12-09T22:31:29.293923: step 259, loss 0.7229, acc 0.804688, prec 0.025951, recall 0.731809
2017-12-09T22:31:29.592220: step 260, loss 0.967926, acc 0.820312, prec 0.0259788, recall 0.732365
2017-12-09T22:31:29.894547: step 261, loss 3.65993, acc 0.796875, prec 0.0262172, recall 0.73306
2017-12-09T22:31:30.206971: step 262, loss 5.53672, acc 0.8125, prec 0.0263158, recall 0.732653
2017-12-09T22:31:30.516525: step 263, loss 0.517557, acc 0.835938, prec 0.0263466, recall 0.733198
2017-12-09T22:31:30.816861: step 264, loss 1.22547, acc 0.757812, prec 0.0262889, recall 0.731707
2017-12-09T22:31:31.114734: step 265, loss 0.90332, acc 0.796875, prec 0.02631, recall 0.732252
2017-12-09T22:31:31.418815: step 266, loss 1.68083, acc 0.710938, prec 0.026312, recall 0.731313
2017-12-09T22:31:31.712572: step 267, loss 0.590151, acc 0.789062, prec 0.0263311, recall 0.731855
2017-12-09T22:31:32.013703: step 268, loss 1.50354, acc 0.757812, prec 0.0264129, recall 0.732932
2017-12-09T22:31:32.308411: step 269, loss 0.835617, acc 0.78125, prec 0.0267109, recall 0.735586
2017-12-09T22:31:32.611258: step 270, loss 1.43773, acc 0.6875, prec 0.0267041, recall 0.736111
2017-12-09T22:31:32.910386: step 271, loss 0.954474, acc 0.71875, prec 0.0268447, recall 0.737673
2017-12-09T22:31:33.212509: step 272, loss 3.37457, acc 0.734375, prec 0.0269206, recall 0.737255
2017-12-09T22:31:33.521781: step 273, loss 0.852767, acc 0.757812, prec 0.026861, recall 0.737255
2017-12-09T22:31:33.820606: step 274, loss 3.95312, acc 0.789062, prec 0.0268112, recall 0.735812
2017-12-09T22:31:34.120974: step 275, loss 3.30008, acc 0.773438, prec 0.0269655, recall 0.735922
2017-12-09T22:31:34.419323: step 276, loss 2.67753, acc 0.742188, prec 0.0269733, recall 0.73501
2017-12-09T22:31:34.720328: step 277, loss 0.840292, acc 0.75, prec 0.0269811, recall 0.735521
2017-12-09T22:31:35.025005: step 278, loss 4.99781, acc 0.664062, prec 0.0270385, recall 0.735125
2017-12-09T22:31:35.333460: step 279, loss 1.15853, acc 0.6875, prec 0.0270308, recall 0.735632
2017-12-09T22:31:35.633297: step 280, loss 3.43811, acc 0.742188, prec 0.0270384, recall 0.734733
2017-12-09T22:31:35.931170: step 281, loss 1.54297, acc 0.625, prec 0.0270838, recall 0.735741
2017-12-09T22:31:36.228187: step 282, loss 1.5497, acc 0.578125, prec 0.0270496, recall 0.736243
2017-12-09T22:31:36.524356: step 283, loss 1.65714, acc 0.539062, prec 0.0271415, recall 0.737736
2017-12-09T22:31:36.828463: step 284, loss 1.79335, acc 0.585938, prec 0.0271093, recall 0.73823
2017-12-09T22:31:37.129319: step 285, loss 1.98123, acc 0.617188, prec 0.0270196, recall 0.736842
2017-12-09T22:31:37.429273: step 286, loss 1.52501, acc 0.601562, prec 0.0269918, recall 0.737336
2017-12-09T22:31:37.726374: step 287, loss 1.62372, acc 0.65625, prec 0.0270437, recall 0.738318
2017-12-09T22:31:38.024935: step 288, loss 1.56681, acc 0.664062, prec 0.0269643, recall 0.738318
2017-12-09T22:31:38.327673: step 289, loss 1.19468, acc 0.632812, prec 0.0270767, recall 0.739777
2017-12-09T22:31:38.625345: step 290, loss 0.951306, acc 0.757812, prec 0.0270197, recall 0.739777
2017-12-09T22:31:38.924500: step 291, loss 2.03972, acc 0.773438, prec 0.0269684, recall 0.738404
2017-12-09T22:31:39.225782: step 292, loss 0.764129, acc 0.75, prec 0.0269101, recall 0.738404
2017-12-09T22:31:39.521139: step 293, loss 0.672662, acc 0.726562, prec 0.0269122, recall 0.738889
2017-12-09T22:31:39.822079: step 294, loss 0.679474, acc 0.773438, prec 0.0269906, recall 0.739852
2017-12-09T22:31:40.119704: step 295, loss 0.614992, acc 0.835938, prec 0.0270179, recall 0.740331
2017-12-09T22:31:40.422321: step 296, loss 5.35813, acc 0.828125, prec 0.0269817, recall 0.737615
2017-12-09T22:31:40.728352: step 297, loss 6.66117, acc 0.898438, prec 0.027027, recall 0.735401
2017-12-09T22:31:41.032394: step 298, loss 0.524255, acc 0.859375, prec 0.0270596, recall 0.735883
2017-12-09T22:31:41.333195: step 299, loss 0.479286, acc 0.875, prec 0.0270306, recall 0.735883
2017-12-09T22:31:41.636480: step 300, loss 2.00959, acc 0.828125, prec 0.0271227, recall 0.735507

Evaluation:
2017-12-09T22:31:46.364066: step 300, loss 1.41736, acc 0.857439, prec 0.0308399, recall 0.725462

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-300

2017-12-09T22:31:47.638873: step 301, loss 0.571921, acc 0.835938, prec 0.0309764, recall 0.726629
2017-12-09T22:31:47.939406: step 302, loss 2.89167, acc 0.765625, prec 0.0310975, recall 0.726761
2017-12-09T22:31:48.239291: step 303, loss 1.52465, acc 0.789062, prec 0.0311071, recall 0.726124
2017-12-09T22:31:48.543514: step 304, loss 2.27911, acc 0.710938, prec 0.0311562, recall 0.725874
2017-12-09T22:31:48.841436: step 305, loss 0.993616, acc 0.734375, prec 0.0312668, recall 0.727019
2017-12-09T22:31:49.144047: step 306, loss 1.153, acc 0.695312, prec 0.0313097, recall 0.727778
2017-12-09T22:31:49.443720: step 307, loss 2.21259, acc 0.625, prec 0.0312798, recall 0.727147
2017-12-09T22:31:49.749508: step 308, loss 1.52823, acc 0.59375, prec 0.0312983, recall 0.727901
2017-12-09T22:31:50.048003: step 309, loss 4.06884, acc 0.625, prec 0.0314406, recall 0.728395
2017-12-09T22:31:50.361628: step 310, loss 2.64167, acc 0.453125, prec 0.0314821, recall 0.729508
2017-12-09T22:31:50.662485: step 311, loss 1.64269, acc 0.617188, prec 0.0314484, recall 0.729877
2017-12-09T22:31:50.961942: step 312, loss 2.11192, acc 0.53125, prec 0.0315081, recall 0.730978
2017-12-09T22:31:51.259097: step 313, loss 1.70856, acc 0.546875, prec 0.031571, recall 0.73207
2017-12-09T22:31:51.559264: step 314, loss 1.41086, acc 0.632812, prec 0.0317663, recall 0.733871
2017-12-09T22:31:51.859031: step 315, loss 1.60095, acc 0.578125, prec 0.031723, recall 0.734228
2017-12-09T22:31:52.160801: step 316, loss 1.50833, acc 0.585938, prec 0.0317378, recall 0.73494
2017-12-09T22:31:52.464513: step 317, loss 1.57413, acc 0.59375, prec 0.0317543, recall 0.735648
2017-12-09T22:31:52.769294: step 318, loss 1.28581, acc 0.640625, prec 0.0318373, recall 0.736702
2017-12-09T22:31:53.070039: step 319, loss 1.02886, acc 0.703125, prec 0.0317679, recall 0.736702
2017-12-09T22:31:53.367799: step 320, loss 0.772825, acc 0.742188, prec 0.0317633, recall 0.737052
2017-12-09T22:31:53.667721: step 321, loss 0.760557, acc 0.742188, prec 0.0317587, recall 0.737401
2017-12-09T22:31:53.969024: step 322, loss 0.464352, acc 0.828125, prec 0.0317741, recall 0.737748
2017-12-09T22:31:54.269851: step 323, loss 0.516251, acc 0.820312, prec 0.0317876, recall 0.738095
2017-12-09T22:31:54.569203: step 324, loss 0.515283, acc 0.867188, prec 0.031812, recall 0.738441
2017-12-09T22:31:54.873016: step 325, loss 0.649354, acc 0.90625, prec 0.0318453, recall 0.738786
2017-12-09T22:31:55.178581: step 326, loss 0.307084, acc 0.898438, prec 0.0318768, recall 0.73913
2017-12-09T22:31:55.479250: step 327, loss 9.09861, acc 0.945312, prec 0.0319209, recall 0.738502
2017-12-09T22:31:55.781327: step 328, loss 5.54539, acc 0.921875, prec 0.0320163, recall 0.737255
2017-12-09T22:31:56.083795: step 329, loss 4.44268, acc 0.898438, prec 0.0319946, recall 0.736292
2017-12-09T22:31:56.387202: step 330, loss 0.302242, acc 0.9375, prec 0.0320349, recall 0.736636
2017-12-09T22:31:56.686557: step 331, loss 9.47098, acc 0.882812, prec 0.0320662, recall 0.735065
2017-12-09T22:31:56.990022: step 332, loss 0.805317, acc 0.890625, prec 0.0321503, recall 0.735751
2017-12-09T22:31:57.293954: step 333, loss 0.881346, acc 0.6875, prec 0.032187, recall 0.736434
2017-12-09T22:31:57.594642: step 334, loss 0.992702, acc 0.75, prec 0.0322381, recall 0.737113
2017-12-09T22:31:57.890983: step 335, loss 6.14744, acc 0.632812, prec 0.0323179, recall 0.737179
2017-12-09T22:31:58.194993: step 336, loss 1.26992, acc 0.671875, prec 0.0323503, recall 0.737852
2017-12-09T22:31:58.491128: step 337, loss 2.14878, acc 0.609375, prec 0.032424, recall 0.737913
2017-12-09T22:31:58.792006: step 338, loss 1.9091, acc 0.484375, prec 0.0323587, recall 0.738247
2017-12-09T22:31:59.096196: step 339, loss 2.29946, acc 0.46875, prec 0.032344, recall 0.73891
2017-12-09T22:31:59.396773: step 340, loss 2.11636, acc 0.492188, prec 0.0322812, recall 0.739241
2017-12-09T22:31:59.691366: step 341, loss 2.16935, acc 0.4375, prec 0.0323131, recall 0.740227
2017-12-09T22:31:59.992230: step 342, loss 1.66512, acc 0.554688, prec 0.0322651, recall 0.740554
2017-12-09T22:32:00.300807: step 343, loss 1.50932, acc 0.601562, prec 0.0323869, recall 0.741855
2017-12-09T22:32:00.600358: step 344, loss 3.26106, acc 0.648438, prec 0.0324147, recall 0.741573
2017-12-09T22:32:00.904283: step 345, loss 1.26818, acc 0.679688, prec 0.0324477, recall 0.742217
2017-12-09T22:32:01.197707: step 346, loss 1.48127, acc 0.609375, prec 0.0323596, recall 0.742217
2017-12-09T22:32:01.495043: step 347, loss 1.08788, acc 0.710938, prec 0.0322948, recall 0.742217
2017-12-09T22:32:01.791482: step 348, loss 1.31982, acc 0.703125, prec 0.0323331, recall 0.742857
2017-12-09T22:32:02.093729: step 349, loss 3.26234, acc 0.71875, prec 0.0324286, recall 0.742892
2017-12-09T22:32:02.395277: step 350, loss 0.808035, acc 0.828125, prec 0.0324423, recall 0.74321
2017-12-09T22:32:02.697904: step 351, loss 0.823628, acc 0.734375, prec 0.032383, recall 0.74321
2017-12-09T22:32:02.993970: step 352, loss 0.633741, acc 0.8125, prec 0.0323412, recall 0.74321
2017-12-09T22:32:03.292381: step 353, loss 3.05416, acc 0.820312, prec 0.0323048, recall 0.741379
2017-12-09T22:32:03.595204: step 354, loss 0.630791, acc 0.828125, prec 0.0323704, recall 0.742015
2017-12-09T22:32:03.899145: step 355, loss 0.811095, acc 0.804688, prec 0.0323789, recall 0.742331
2017-12-09T22:32:04.201271: step 356, loss 0.911651, acc 0.828125, prec 0.0323926, recall 0.742647
2017-12-09T22:32:04.502808: step 357, loss 2.79373, acc 0.859375, prec 0.0324665, recall 0.742369
2017-12-09T22:32:04.804748: step 358, loss 1.46535, acc 0.882812, prec 0.0325455, recall 0.742092
2017-12-09T22:32:05.111645: step 359, loss 0.754724, acc 0.78125, prec 0.0326, recall 0.742718
2017-12-09T22:32:05.428205: step 360, loss 0.527059, acc 0.804688, prec 0.0326596, recall 0.743341
2017-12-09T22:32:05.730006: step 361, loss 0.958494, acc 0.726562, prec 0.0327016, recall 0.743961
2017-12-09T22:32:06.024775: step 362, loss 0.72691, acc 0.773438, prec 0.0327539, recall 0.744578
2017-12-09T22:32:06.321839: step 363, loss 0.967054, acc 0.703125, prec 0.0328415, recall 0.745498
2017-12-09T22:32:06.620848: step 364, loss 0.739804, acc 0.789062, prec 0.0328458, recall 0.745803
2017-12-09T22:32:06.920087: step 365, loss 0.655411, acc 0.765625, prec 0.0327938, recall 0.745803
2017-12-09T22:32:07.222173: step 366, loss 0.437646, acc 0.875, prec 0.0328171, recall 0.746108
2017-12-09T22:32:07.524763: step 367, loss 0.544348, acc 0.78125, prec 0.0328705, recall 0.746714
2017-12-09T22:32:07.824501: step 368, loss 5.43457, acc 0.851562, prec 0.0328394, recall 0.745823
2017-12-09T22:32:08.118050: step 369, loss 4.95299, acc 0.867188, prec 0.0328118, recall 0.744934
2017-12-09T22:32:08.417718: step 370, loss 0.467319, acc 0.828125, prec 0.0328247, recall 0.745238
2017-12-09T22:32:08.713007: step 371, loss 0.599544, acc 0.765625, prec 0.0328238, recall 0.745541
2017-12-09T22:32:09.016787: step 372, loss 0.817272, acc 0.773438, prec 0.0328751, recall 0.746145
2017-12-09T22:32:09.318850: step 373, loss 2.81576, acc 0.796875, prec 0.0328827, recall 0.745562
2017-12-09T22:32:09.619036: step 374, loss 0.762561, acc 0.8125, prec 0.032892, recall 0.745863
2017-12-09T22:32:09.917310: step 375, loss 0.603731, acc 0.796875, prec 0.0328978, recall 0.746163
2017-12-09T22:32:10.213591: step 376, loss 3.50568, acc 0.78125, prec 0.0329019, recall 0.745583
2017-12-09T22:32:10.513955: step 377, loss 1.09591, acc 0.789062, prec 0.032906, recall 0.745882
2017-12-09T22:32:10.817107: step 378, loss 0.692666, acc 0.695312, prec 0.0328395, recall 0.745882
2017-12-09T22:32:11.816071: step 379, loss 1.81232, acc 0.734375, prec 0.0327835, recall 0.745006
2017-12-09T22:32:12.219899: step 380, loss 0.82695, acc 0.734375, prec 0.0328258, recall 0.745604
2017-12-09T22:32:12.528870: step 381, loss 1.31776, acc 0.632812, prec 0.032846, recall 0.746199
2017-12-09T22:32:12.834913: step 382, loss 1.27529, acc 0.648438, prec 0.03277, recall 0.746199
2017-12-09T22:32:13.135680: step 383, loss 3.38957, acc 0.703125, prec 0.0328071, recall 0.745921
2017-12-09T22:32:13.438883: step 384, loss 1.73334, acc 0.695312, prec 0.0327928, recall 0.745349
2017-12-09T22:32:13.738947: step 385, loss 1.14507, acc 0.664062, prec 0.0327701, recall 0.745645
2017-12-09T22:32:14.038783: step 386, loss 1.11085, acc 0.71875, prec 0.0328086, recall 0.746234
2017-12-09T22:32:14.338241: step 387, loss 1.04042, acc 0.742188, prec 0.0328519, recall 0.746821
2017-12-09T22:32:14.634749: step 388, loss 0.934084, acc 0.726562, prec 0.0328917, recall 0.747405
2017-12-09T22:32:14.933223: step 389, loss 6.68308, acc 0.765625, prec 0.0328434, recall 0.746544
2017-12-09T22:32:15.232409: step 390, loss 0.596807, acc 0.789062, prec 0.0327985, recall 0.746544
2017-12-09T22:32:15.526970: step 391, loss 0.901529, acc 0.71875, prec 0.0327877, recall 0.746835
2017-12-09T22:32:15.822953: step 392, loss 0.891252, acc 0.75, prec 0.0328324, recall 0.747417
2017-12-09T22:32:16.121675: step 393, loss 1.79689, acc 0.796875, prec 0.0328397, recall 0.74685
2017-12-09T22:32:16.431780: step 394, loss 0.764772, acc 0.796875, prec 0.0328941, recall 0.747429
2017-12-09T22:32:16.727757: step 395, loss 0.997478, acc 0.835938, prec 0.0330537, recall 0.748578
2017-12-09T22:32:17.030627: step 396, loss 2.01805, acc 0.820312, prec 0.0331628, recall 0.748584
2017-12-09T22:32:17.331249: step 397, loss 1.75961, acc 0.742188, prec 0.0332549, recall 0.748591
2017-12-09T22:32:17.631190: step 398, loss 0.716263, acc 0.773438, prec 0.0332066, recall 0.748591
2017-12-09T22:32:17.933195: step 399, loss 0.872857, acc 0.765625, prec 0.0331569, recall 0.748591
2017-12-09T22:32:18.230261: step 400, loss 0.991565, acc 0.8125, prec 0.0331654, recall 0.748874
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-400

2017-12-09T22:32:19.559796: step 401, loss 4.88029, acc 0.757812, prec 0.033262, recall 0.74804
2017-12-09T22:32:19.860888: step 402, loss 1.56413, acc 0.765625, prec 0.0332621, recall 0.747486
2017-12-09T22:32:20.162823: step 403, loss 1.03535, acc 0.703125, prec 0.0333912, recall 0.74861
2017-12-09T22:32:20.477829: step 404, loss 1.71219, acc 0.640625, prec 0.033363, recall 0.748889
2017-12-09T22:32:20.779658: step 405, loss 1.15296, acc 0.71875, prec 0.0333992, recall 0.749446
2017-12-09T22:32:21.079442: step 406, loss 1.21265, acc 0.703125, prec 0.0334796, recall 0.750276
2017-12-09T22:32:21.379300: step 407, loss 1.12461, acc 0.625, prec 0.0334481, recall 0.750552
2017-12-09T22:32:21.678349: step 408, loss 1.4465, acc 0.648438, prec 0.0335165, recall 0.751375
2017-12-09T22:32:21.978604: step 409, loss 1.19262, acc 0.640625, prec 0.0334884, recall 0.751648
2017-12-09T22:32:22.274819: step 410, loss 4.89065, acc 0.703125, prec 0.0334751, recall 0.751096
2017-12-09T22:32:22.575980: step 411, loss 1.45605, acc 0.75, prec 0.0335659, recall 0.751092
2017-12-09T22:32:22.877606: step 412, loss 1.73285, acc 0.757812, prec 0.0335639, recall 0.750545
2017-12-09T22:32:23.177410: step 413, loss 0.827923, acc 0.742188, prec 0.033604, recall 0.751087
2017-12-09T22:32:23.483226: step 414, loss 0.675054, acc 0.71875, prec 0.0335922, recall 0.751357
2017-12-09T22:32:23.788698: step 415, loss 1.18278, acc 0.757812, prec 0.0337291, recall 0.752432
2017-12-09T22:32:24.087453: step 416, loss 4.61705, acc 0.710938, prec 0.0337638, recall 0.752155
2017-12-09T22:32:24.394031: step 417, loss 3.75954, acc 0.71875, prec 0.0337534, recall 0.751613
2017-12-09T22:32:24.700503: step 418, loss 1.37354, acc 0.71875, prec 0.0337414, recall 0.75188
2017-12-09T22:32:24.996127: step 419, loss 3.76268, acc 0.6875, prec 0.0337711, recall 0.751606
2017-12-09T22:32:25.293607: step 420, loss 2.20695, acc 0.625, prec 0.0337877, recall 0.751334
2017-12-09T22:32:25.589576: step 421, loss 1.44011, acc 0.671875, prec 0.0338585, recall 0.752128
2017-12-09T22:32:25.884773: step 422, loss 1.43122, acc 0.632812, prec 0.0339671, recall 0.753178
2017-12-09T22:32:26.180805: step 423, loss 1.20291, acc 0.578125, prec 0.0339258, recall 0.753439
2017-12-09T22:32:26.477894: step 424, loss 1.61219, acc 0.617188, prec 0.0339845, recall 0.754219
2017-12-09T22:32:26.779603: step 425, loss 1.45824, acc 0.578125, prec 0.0339433, recall 0.754478
2017-12-09T22:32:27.076514: step 426, loss 1.13266, acc 0.664062, prec 0.0339657, recall 0.754995
2017-12-09T22:32:27.373614: step 427, loss 1.15361, acc 0.648438, prec 0.0340303, recall 0.755765
2017-12-09T22:32:27.672321: step 428, loss 1.16024, acc 0.703125, prec 0.0340149, recall 0.756021
2017-12-09T22:32:27.976795: step 429, loss 0.758019, acc 0.765625, prec 0.0340123, recall 0.756276
2017-12-09T22:32:28.277119: step 430, loss 1.44302, acc 0.726562, prec 0.033958, recall 0.755486
2017-12-09T22:32:28.577300: step 431, loss 0.822924, acc 0.765625, prec 0.0339555, recall 0.755741
2017-12-09T22:32:28.877165: step 432, loss 0.600562, acc 0.828125, prec 0.0339205, recall 0.755741
2017-12-09T22:32:29.176580: step 433, loss 0.605266, acc 0.820312, prec 0.0339292, recall 0.755996
2017-12-09T22:32:29.476548: step 434, loss 1.2272, acc 0.796875, prec 0.0339332, recall 0.75625
2017-12-09T22:32:29.779968: step 435, loss 8.59605, acc 0.835938, prec 0.0339482, recall 0.754933
2017-12-09T22:32:30.087439: step 436, loss 1.48469, acc 0.804688, prec 0.0341805, recall 0.75567
2017-12-09T22:32:30.395183: step 437, loss 4.22302, acc 0.773438, prec 0.0342258, recall 0.755396
2017-12-09T22:32:30.699237: step 438, loss 0.547617, acc 0.78125, prec 0.0341813, recall 0.755396
2017-12-09T22:32:30.995310: step 439, loss 1.024, acc 0.742188, prec 0.0342186, recall 0.755897
2017-12-09T22:32:31.293517: step 440, loss 0.875505, acc 0.726562, prec 0.0342526, recall 0.756397
2017-12-09T22:32:31.598042: step 441, loss 0.980612, acc 0.71875, prec 0.0342402, recall 0.756646
2017-12-09T22:32:31.894086: step 442, loss 0.986895, acc 0.75, prec 0.0342789, recall 0.757143
2017-12-09T22:32:32.187064: step 443, loss 1.73386, acc 0.75, prec 0.0342744, recall 0.756619
2017-12-09T22:32:32.490824: step 444, loss 0.65001, acc 0.796875, prec 0.0342778, recall 0.756867
2017-12-09T22:32:32.792307: step 445, loss 1.0364, acc 0.6875, prec 0.0343036, recall 0.75736
2017-12-09T22:32:33.087153: step 446, loss 0.909424, acc 0.765625, prec 0.034345, recall 0.757852
2017-12-09T22:32:33.382709: step 447, loss 1.92095, acc 0.734375, prec 0.0344259, recall 0.75782
2017-12-09T22:32:33.677828: step 448, loss 1.0687, acc 0.742188, prec 0.0345064, recall 0.758551
2017-12-09T22:32:33.975649: step 449, loss 0.878943, acc 0.765625, prec 0.0345915, recall 0.759278
2017-12-09T22:32:34.270624: step 450, loss 2.53109, acc 0.71875, prec 0.0345803, recall 0.758759
2017-12-09T22:32:34.574866: step 451, loss 0.793014, acc 0.765625, prec 0.034577, recall 0.759
2017-12-09T22:32:34.867598: step 452, loss 0.683294, acc 0.75, prec 0.0345267, recall 0.759
2017-12-09T22:32:35.167295: step 453, loss 0.912623, acc 0.726562, prec 0.0345595, recall 0.759481
2017-12-09T22:32:35.476104: step 454, loss 2.09937, acc 0.742188, prec 0.0345093, recall 0.758724
2017-12-09T22:32:35.769768: step 455, loss 0.632884, acc 0.820312, prec 0.0344734, recall 0.758724
2017-12-09T22:32:36.069101: step 456, loss 0.649462, acc 0.8125, prec 0.034567, recall 0.759443
2017-12-09T22:32:36.367911: step 457, loss 0.782841, acc 0.71875, prec 0.0345544, recall 0.759682
2017-12-09T22:32:36.668512: step 458, loss 0.412229, acc 0.859375, prec 0.0346135, recall 0.760159
2017-12-09T22:32:36.965016: step 459, loss 1.7173, acc 0.851562, prec 0.0346305, recall 0.758893
2017-12-09T22:32:37.270114: step 460, loss 0.43375, acc 0.859375, prec 0.0346894, recall 0.759369
2017-12-09T22:32:37.566954: step 461, loss 0.742577, acc 0.820312, prec 0.0347403, recall 0.759843
2017-12-09T22:32:37.869675: step 462, loss 0.666464, acc 0.835938, prec 0.0347943, recall 0.760314
2017-12-09T22:32:38.168947: step 463, loss 0.412065, acc 0.835938, prec 0.0347615, recall 0.760314
2017-12-09T22:32:38.469075: step 464, loss 2.49195, acc 0.820312, prec 0.0347705, recall 0.759804
2017-12-09T22:32:38.769569: step 465, loss 0.6698, acc 0.796875, prec 0.0348165, recall 0.760274
2017-12-09T22:32:39.068616: step 466, loss 0.612414, acc 0.78125, prec 0.0348593, recall 0.760742
2017-12-09T22:32:39.367605: step 467, loss 1.49505, acc 0.820312, prec 0.0348681, recall 0.760234
2017-12-09T22:32:39.665973: step 468, loss 0.70887, acc 0.757812, prec 0.0348199, recall 0.760234
2017-12-09T22:32:39.968708: step 469, loss 0.442946, acc 0.851562, prec 0.0348765, recall 0.7607
2017-12-09T22:32:40.267361: step 470, loss 0.548849, acc 0.835938, prec 0.0348868, recall 0.760933
2017-12-09T22:32:40.563109: step 471, loss 1.75837, acc 0.78125, prec 0.0349308, recall 0.760659
2017-12-09T22:32:40.867123: step 472, loss 0.737362, acc 0.835938, prec 0.034984, recall 0.761122
2017-12-09T22:32:41.162271: step 473, loss 0.915896, acc 0.765625, prec 0.0350231, recall 0.761583
2017-12-09T22:32:41.458822: step 474, loss 1.07072, acc 0.765625, prec 0.0352331, recall 0.762956
2017-12-09T22:32:41.761498: step 475, loss 0.571645, acc 0.820312, prec 0.0351972, recall 0.762956
2017-12-09T22:32:42.061635: step 476, loss 0.962936, acc 0.789062, prec 0.0352405, recall 0.76341
2017-12-09T22:32:42.366513: step 477, loss 0.484199, acc 0.828125, prec 0.0352489, recall 0.763636
2017-12-09T22:32:42.667690: step 478, loss 1.2939, acc 0.789062, prec 0.0352085, recall 0.762906
2017-12-09T22:32:42.970503: step 479, loss 0.505223, acc 0.835938, prec 0.0352609, recall 0.763359
2017-12-09T22:32:43.268858: step 480, loss 2.98596, acc 0.8125, prec 0.0353526, recall 0.763308
2017-12-09T22:32:43.565720: step 481, loss 0.776532, acc 0.796875, prec 0.0353546, recall 0.763533
2017-12-09T22:32:43.864814: step 482, loss 4.08826, acc 0.8125, prec 0.0353613, recall 0.763033
2017-12-09T22:32:44.164873: step 483, loss 0.64821, acc 0.804688, prec 0.0353225, recall 0.763033
2017-12-09T22:32:44.463140: step 484, loss 1.17989, acc 0.773438, prec 0.0352792, recall 0.762311
2017-12-09T22:32:44.759502: step 485, loss 1.53435, acc 0.664062, prec 0.0353816, recall 0.763208
2017-12-09T22:32:45.053486: step 486, loss 2.36079, acc 0.703125, prec 0.0353665, recall 0.762712
2017-12-09T22:32:45.356250: step 487, loss 1.03883, acc 0.734375, prec 0.0354403, recall 0.76338
2017-12-09T22:32:45.653038: step 488, loss 1.05742, acc 0.726562, prec 0.0354703, recall 0.763824
2017-12-09T22:32:45.952130: step 489, loss 1.09991, acc 0.679688, prec 0.035449, recall 0.764045
2017-12-09T22:32:46.257032: step 490, loss 1.16414, acc 0.664062, prec 0.0354665, recall 0.764486
2017-12-09T22:32:46.558593: step 491, loss 1.0007, acc 0.703125, prec 0.0354499, recall 0.764706
2017-12-09T22:32:46.856274: step 492, loss 0.837603, acc 0.726562, prec 0.0354797, recall 0.765144
2017-12-09T22:32:47.151703: step 493, loss 3.68465, acc 0.742188, prec 0.0355556, recall 0.765088
2017-12-09T22:32:47.450244: step 494, loss 0.813669, acc 0.71875, prec 0.035542, recall 0.765306
2017-12-09T22:32:47.750282: step 495, loss 0.63906, acc 0.742188, prec 0.035533, recall 0.765524
2017-12-09T22:32:48.050822: step 496, loss 1.27757, acc 0.6875, prec 0.0355548, recall 0.765957
2017-12-09T22:32:48.349442: step 497, loss 1.3867, acc 0.679688, prec 0.0356163, recall 0.766605
2017-12-09T22:32:48.527000: step 498, loss 0.655962, acc 0.666667, prec 0.0356317, recall 0.76682
2017-12-09T22:32:48.833197: step 499, loss 0.740802, acc 0.757812, prec 0.0356257, recall 0.767035
2017-12-09T22:32:49.132881: step 500, loss 1.91984, acc 0.828125, prec 0.0356365, recall 0.76584
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-500

2017-12-09T22:32:50.386286: step 501, loss 0.574818, acc 0.828125, prec 0.0357265, recall 0.766484
2017-12-09T22:32:50.687669: step 502, loss 0.95116, acc 0.773438, prec 0.0358056, recall 0.767123
2017-12-09T22:32:50.989380: step 503, loss 0.382939, acc 0.828125, prec 0.0358131, recall 0.767336
2017-12-09T22:32:51.287675: step 504, loss 0.686989, acc 0.820312, prec 0.0358601, recall 0.76776
2017-12-09T22:32:51.583662: step 505, loss 0.581781, acc 0.78125, prec 0.0359404, recall 0.768392
2017-12-09T22:32:51.882165: step 506, loss 0.384421, acc 0.851562, prec 0.0359114, recall 0.768392
2017-12-09T22:32:52.188524: step 507, loss 0.514861, acc 0.882812, prec 0.0359294, recall 0.768603
2017-12-09T22:32:52.490926: step 508, loss 0.479767, acc 0.875, prec 0.035905, recall 0.768603
2017-12-09T22:32:52.788517: step 509, loss 3.01982, acc 0.859375, prec 0.03592, recall 0.768116
2017-12-09T22:32:53.088203: step 510, loss 0.670812, acc 0.890625, prec 0.0359396, recall 0.768326
2017-12-09T22:32:53.390132: step 511, loss 0.46349, acc 0.867188, prec 0.0359545, recall 0.768535
2017-12-09T22:32:53.686387: step 512, loss 1.02808, acc 0.828125, prec 0.0359618, recall 0.768744
2017-12-09T22:32:53.990882: step 513, loss 4.28695, acc 0.8125, prec 0.0360505, recall 0.767986
2017-12-09T22:32:54.291153: step 514, loss 0.644836, acc 0.851562, prec 0.0360623, recall 0.768194
2017-12-09T22:32:54.596645: step 515, loss 1.38006, acc 0.78125, prec 0.0361025, recall 0.767921
2017-12-09T22:32:54.898063: step 516, loss 0.679874, acc 0.703125, prec 0.0360448, recall 0.767921
2017-12-09T22:32:55.194331: step 517, loss 0.526457, acc 0.796875, prec 0.0360054, recall 0.767921
2017-12-09T22:32:55.494254: step 518, loss 0.78036, acc 0.765625, prec 0.0360005, recall 0.768129
2017-12-09T22:32:55.793602: step 519, loss 0.752191, acc 0.734375, prec 0.03603, recall 0.768543
2017-12-09T22:32:56.087329: step 520, loss 0.966421, acc 0.679688, prec 0.0360488, recall 0.768956
2017-12-09T22:32:56.386612: step 521, loss 1.17189, acc 0.664062, prec 0.0360646, recall 0.769368
2017-12-09T22:32:56.687321: step 522, loss 0.900135, acc 0.75, prec 0.0361772, recall 0.770186
2017-12-09T22:32:56.987254: step 523, loss 1.02789, acc 0.671875, prec 0.0362744, recall 0.770999
2017-12-09T22:32:57.287860: step 524, loss 0.964441, acc 0.71875, prec 0.0363402, recall 0.771605
2017-12-09T22:32:57.585650: step 525, loss 1.00271, acc 0.796875, prec 0.0365409, recall 0.772807
2017-12-09T22:32:57.880998: step 526, loss 0.92175, acc 0.726562, prec 0.0366076, recall 0.773403
2017-12-09T22:32:58.177295: step 527, loss 0.730548, acc 0.789062, prec 0.0366066, recall 0.773601
2017-12-09T22:32:58.473685: step 528, loss 0.576378, acc 0.796875, prec 0.0366468, recall 0.773997
2017-12-09T22:32:58.768700: step 529, loss 0.607328, acc 0.78125, prec 0.0366442, recall 0.774194
2017-12-09T22:32:59.066123: step 530, loss 2.67936, acc 0.867188, prec 0.0366995, recall 0.773913
2017-12-09T22:32:59.364861: step 531, loss 1.03279, acc 0.804688, prec 0.0367411, recall 0.774306
2017-12-09T22:32:59.671460: step 532, loss 0.798648, acc 0.890625, prec 0.0367214, recall 0.773634
2017-12-09T22:32:59.981480: step 533, loss 0.496455, acc 0.882812, prec 0.036778, recall 0.774026
2017-12-09T22:33:00.287105: step 534, loss 3.5123, acc 0.875, prec 0.0367553, recall 0.773356
2017-12-09T22:33:00.590517: step 535, loss 0.557444, acc 0.804688, prec 0.0367572, recall 0.773552
2017-12-09T22:33:00.886696: step 536, loss 0.608172, acc 0.773438, prec 0.0367529, recall 0.773748
2017-12-09T22:33:01.183415: step 537, loss 0.565895, acc 0.835938, prec 0.0367608, recall 0.773943
2017-12-09T22:33:01.479410: step 538, loss 0.456208, acc 0.84375, prec 0.0367701, recall 0.774138
2017-12-09T22:33:01.780777: step 539, loss 0.623365, acc 0.78125, prec 0.0368462, recall 0.774721
2017-12-09T22:33:02.082151: step 540, loss 0.807455, acc 0.773438, prec 0.0368812, recall 0.775107
2017-12-09T22:33:02.376417: step 541, loss 0.42675, acc 0.867188, prec 0.0369343, recall 0.775493
2017-12-09T22:33:02.675768: step 542, loss 0.357695, acc 0.867188, prec 0.0369479, recall 0.775685
2017-12-09T22:33:02.975168: step 543, loss 0.900121, acc 0.898438, prec 0.0369691, recall 0.775214
2017-12-09T22:33:03.274197: step 544, loss 0.396559, acc 0.859375, prec 0.0370204, recall 0.775597
2017-12-09T22:33:03.571396: step 545, loss 3.4819, acc 0.835938, prec 0.037031, recall 0.774468
2017-12-09T22:33:03.871642: step 546, loss 0.30561, acc 0.875, prec 0.0370852, recall 0.774851
2017-12-09T22:33:04.174361: step 547, loss 0.801041, acc 0.914062, prec 0.0371469, recall 0.775233
2017-12-09T22:33:04.473250: step 548, loss 0.669751, acc 0.796875, prec 0.0371468, recall 0.775424
2017-12-09T22:33:04.778837: step 549, loss 0.867235, acc 0.796875, prec 0.0371076, recall 0.775424
2017-12-09T22:33:05.082652: step 550, loss 1.14686, acc 0.882812, prec 0.0370866, recall 0.774767
2017-12-09T22:33:05.395380: step 551, loss 0.860835, acc 0.828125, prec 0.0372485, recall 0.775717
2017-12-09T22:33:05.696082: step 552, loss 0.582515, acc 0.78125, prec 0.0372452, recall 0.775906
2017-12-09T22:33:05.992481: step 553, loss 0.705054, acc 0.773438, prec 0.0372405, recall 0.776094
2017-12-09T22:33:06.288135: step 554, loss 0.682829, acc 0.757812, prec 0.0371939, recall 0.776094
2017-12-09T22:33:06.581691: step 555, loss 1.37109, acc 0.71875, prec 0.0372563, recall 0.776658
2017-12-09T22:33:06.882522: step 556, loss 0.695838, acc 0.828125, prec 0.0373008, recall 0.777033
2017-12-09T22:33:07.185538: step 557, loss 0.489157, acc 0.8125, prec 0.0372648, recall 0.777033
2017-12-09T22:33:07.483483: step 558, loss 1.3463, acc 0.6875, prec 0.0372823, recall 0.777406
2017-12-09T22:33:07.782561: step 559, loss 0.729771, acc 0.765625, prec 0.0373146, recall 0.777778
2017-12-09T22:33:08.084737: step 560, loss 0.544609, acc 0.78125, prec 0.0372728, recall 0.777778
2017-12-09T22:33:08.385653: step 561, loss 0.725212, acc 0.8125, prec 0.037391, recall 0.778518
2017-12-09T22:33:08.685605: step 562, loss 0.432218, acc 0.882812, prec 0.0374071, recall 0.778702
2017-12-09T22:33:08.985043: step 563, loss 0.94051, acc 0.796875, prec 0.0374067, recall 0.778886
2017-12-09T22:33:09.286565: step 564, loss 0.376289, acc 0.867188, prec 0.0374197, recall 0.77907
2017-12-09T22:33:09.588120: step 565, loss 0.46492, acc 0.828125, prec 0.0373869, recall 0.77907
2017-12-09T22:33:09.891198: step 566, loss 0.242388, acc 0.9375, prec 0.037375, recall 0.77907
2017-12-09T22:33:10.188822: step 567, loss 0.884051, acc 0.921875, prec 0.0374383, recall 0.77879
2017-12-09T22:33:10.487258: step 568, loss 0.345196, acc 0.929688, prec 0.0374632, recall 0.778974
2017-12-09T22:33:10.787567: step 569, loss 4.06948, acc 0.882812, prec 0.0374438, recall 0.777686
2017-12-09T22:33:11.086593: step 570, loss 0.275128, acc 0.914062, prec 0.0374274, recall 0.777686
2017-12-09T22:33:11.390392: step 571, loss 0.668544, acc 0.90625, prec 0.0374861, recall 0.778053
2017-12-09T22:33:11.692436: step 572, loss 0.323534, acc 0.84375, prec 0.0374945, recall 0.778236
2017-12-09T22:33:11.988199: step 573, loss 0.351698, acc 0.890625, prec 0.0375501, recall 0.778601
2017-12-09T22:33:12.287462: step 574, loss 1.08223, acc 0.882812, prec 0.0375674, recall 0.778143
2017-12-09T22:33:12.586976: step 575, loss 0.3787, acc 0.851562, prec 0.0375391, recall 0.778143
2017-12-09T22:33:12.892095: step 576, loss 0.416535, acc 0.859375, prec 0.0375505, recall 0.778325
2017-12-09T22:33:13.201765: step 577, loss 0.405686, acc 0.890625, prec 0.0375678, recall 0.778507
2017-12-09T22:33:13.499016: step 578, loss 0.616025, acc 0.8125, prec 0.0375702, recall 0.778689
2017-12-09T22:33:13.793892: step 579, loss 0.461419, acc 0.828125, prec 0.0375756, recall 0.77887
2017-12-09T22:33:14.090646: step 580, loss 0.349024, acc 0.875, prec 0.0375898, recall 0.779051
2017-12-09T22:33:14.390808: step 581, loss 0.518371, acc 0.835938, prec 0.0375967, recall 0.779231
2017-12-09T22:33:14.686990: step 582, loss 1.32329, acc 0.84375, prec 0.0376064, recall 0.778776
2017-12-09T22:33:14.984625: step 583, loss 1.50804, acc 0.875, prec 0.0376236, recall 0.777687
2017-12-09T22:33:15.286148: step 584, loss 0.947803, acc 0.859375, prec 0.0375984, recall 0.777054
2017-12-09T22:33:15.586254: step 585, loss 0.365689, acc 0.84375, prec 0.0375688, recall 0.777054
2017-12-09T22:33:15.887447: step 586, loss 1.85672, acc 0.859375, prec 0.0376951, recall 0.777147
2017-12-09T22:33:16.198106: step 587, loss 0.303043, acc 0.875, prec 0.037747, recall 0.777508
2017-12-09T22:33:16.499988: step 588, loss 0.839014, acc 0.757812, prec 0.0377388, recall 0.777688
2017-12-09T22:33:16.798247: step 589, loss 0.946482, acc 0.742188, prec 0.0377654, recall 0.778047
2017-12-09T22:33:17.102062: step 590, loss 0.597744, acc 0.8125, prec 0.0378429, recall 0.778583
2017-12-09T22:33:17.398139: step 591, loss 4.07158, acc 0.765625, prec 0.0379128, recall 0.778491
2017-12-09T22:33:17.701460: step 592, loss 0.932404, acc 0.734375, prec 0.0379376, recall 0.778846
2017-12-09T22:33:18.001918: step 593, loss 0.85552, acc 0.734375, prec 0.0380373, recall 0.779553
2017-12-09T22:33:18.299370: step 594, loss 0.713352, acc 0.734375, prec 0.0380619, recall 0.779904
2017-12-09T22:33:18.600089: step 595, loss 0.447361, acc 0.867188, prec 0.0380741, recall 0.78008
2017-12-09T22:33:18.901837: step 596, loss 0.856605, acc 0.710938, prec 0.0381315, recall 0.780604
2017-12-09T22:33:19.199659: step 597, loss 1.78991, acc 0.742188, prec 0.0381588, recall 0.780333
2017-12-09T22:33:19.502152: step 598, loss 0.855288, acc 0.6875, prec 0.0381741, recall 0.780681
2017-12-09T22:33:19.803752: step 599, loss 0.647981, acc 0.75, prec 0.0381641, recall 0.780854
2017-12-09T22:33:20.098392: step 600, loss 0.578753, acc 0.851562, prec 0.0381361, recall 0.780854

Evaluation:
2017-12-09T22:33:24.821064: step 600, loss 1.20019, acc 0.818285, prec 0.0397161, recall 0.783039

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-600

2017-12-09T22:33:26.019942: step 601, loss 0.705822, acc 0.75, prec 0.039705, recall 0.783192
2017-12-09T22:33:26.323335: step 602, loss 0.548671, acc 0.820312, prec 0.0397754, recall 0.78365
2017-12-09T22:33:26.627132: step 603, loss 4.56619, acc 0.820312, prec 0.0398828, recall 0.783158
2017-12-09T22:33:26.928509: step 604, loss 0.358862, acc 0.859375, prec 0.0399257, recall 0.783462
2017-12-09T22:33:27.227879: step 605, loss 0.41985, acc 0.867188, prec 0.03997, recall 0.783765
2017-12-09T22:33:27.531277: step 606, loss 0.994707, acc 0.859375, prec 0.0400813, recall 0.784368
2017-12-09T22:33:27.837869: step 607, loss 0.675239, acc 0.8125, prec 0.0401496, recall 0.784819
2017-12-09T22:33:28.137799: step 608, loss 0.378574, acc 0.875, prec 0.0402293, recall 0.785268
2017-12-09T22:33:28.435878: step 609, loss 0.673365, acc 0.796875, prec 0.0402262, recall 0.785417
2017-12-09T22:33:28.734614: step 610, loss 0.95635, acc 0.820312, prec 0.0402615, recall 0.785714
2017-12-09T22:33:29.034994: step 611, loss 1.75289, acc 0.820312, prec 0.0403323, recall 0.785616
2017-12-09T22:33:29.334622: step 612, loss 0.943814, acc 0.796875, prec 0.0403972, recall 0.786059
2017-12-09T22:33:29.638095: step 613, loss 1.84584, acc 0.851562, prec 0.0404055, recall 0.785665
2017-12-09T22:33:29.945093: step 614, loss 1.08222, acc 0.835938, prec 0.0404434, recall 0.78596
2017-12-09T22:33:30.253296: step 615, loss 1.26626, acc 0.757812, prec 0.040433, recall 0.786107
2017-12-09T22:33:30.552086: step 616, loss 0.920038, acc 0.6875, prec 0.0404097, recall 0.786254
2017-12-09T22:33:30.854254: step 617, loss 0.819545, acc 0.71875, prec 0.04046, recall 0.786694
2017-12-09T22:33:31.149769: step 618, loss 0.864311, acc 0.757812, prec 0.0404834, recall 0.786986
2017-12-09T22:33:31.448270: step 619, loss 1.75317, acc 0.734375, prec 0.0404364, recall 0.786448
2017-12-09T22:33:31.745446: step 620, loss 0.691121, acc 0.820312, prec 0.0404374, recall 0.786594
2017-12-09T22:33:32.044645: step 621, loss 0.655279, acc 0.757812, prec 0.0404271, recall 0.78674
2017-12-09T22:33:32.341801: step 622, loss 0.652996, acc 0.765625, prec 0.0403845, recall 0.78674
2017-12-09T22:33:32.639162: step 623, loss 0.72204, acc 0.78125, prec 0.0405131, recall 0.787466
2017-12-09T22:33:32.943249: step 624, loss 2.97889, acc 0.835938, prec 0.0405519, recall 0.78722
2017-12-09T22:33:33.247582: step 625, loss 0.496154, acc 0.835938, prec 0.0405221, recall 0.78722
2017-12-09T22:33:33.543686: step 626, loss 0.856704, acc 0.8125, prec 0.0405552, recall 0.787508
2017-12-09T22:33:33.841725: step 627, loss 0.627399, acc 0.796875, prec 0.0405184, recall 0.787508
2017-12-09T22:33:34.137691: step 628, loss 1.40866, acc 0.8125, prec 0.0405193, recall 0.787119
2017-12-09T22:33:34.442610: step 629, loss 0.667524, acc 0.789062, prec 0.0405481, recall 0.787407
2017-12-09T22:33:34.742645: step 630, loss 1.5853, acc 0.734375, prec 0.0405683, recall 0.787162
2017-12-09T22:33:35.039941: step 631, loss 0.543986, acc 0.804688, prec 0.040533, recall 0.787162
2017-12-09T22:33:35.341523: step 632, loss 0.931588, acc 0.8125, prec 0.0405659, recall 0.787449
2017-12-09T22:33:35.636442: step 633, loss 0.511894, acc 0.820312, prec 0.0406335, recall 0.787879
2017-12-09T22:33:35.940498: step 634, loss 0.899971, acc 0.820312, prec 0.0407342, recall 0.788449
2017-12-09T22:33:36.240880: step 635, loss 0.577317, acc 0.796875, prec 0.0407307, recall 0.788591
2017-12-09T22:33:36.547056: step 636, loss 0.414842, acc 0.820312, prec 0.0407315, recall 0.788732
2017-12-09T22:33:36.850653: step 637, loss 2.22737, acc 0.820312, prec 0.0407337, recall 0.788346
2017-12-09T22:33:37.152569: step 638, loss 0.512655, acc 0.875, prec 0.0407775, recall 0.788629
2017-12-09T22:33:37.452733: step 639, loss 1.36637, acc 0.8125, prec 0.0407782, recall 0.788243
2017-12-09T22:33:37.753790: step 640, loss 0.583553, acc 0.835938, prec 0.0407818, recall 0.788384
2017-12-09T22:33:38.052730: step 641, loss 0.568407, acc 0.8125, prec 0.0407811, recall 0.788526
2017-12-09T22:33:38.353368: step 642, loss 0.562122, acc 0.828125, prec 0.0407833, recall 0.788667
2017-12-09T22:33:38.657763: step 643, loss 0.609206, acc 0.78125, prec 0.040777, recall 0.788807
2017-12-09T22:33:38.953872: step 644, loss 0.564741, acc 0.8125, prec 0.0408093, recall 0.789088
2017-12-09T22:33:39.254589: step 645, loss 0.600612, acc 0.804688, prec 0.0408072, recall 0.789229
2017-12-09T22:33:39.558927: step 646, loss 0.759987, acc 0.8125, prec 0.0408395, recall 0.789509
2017-12-09T22:33:39.864488: step 647, loss 1.30072, acc 0.835938, prec 0.0408114, recall 0.788985
2017-12-09T22:33:40.168564: step 648, loss 0.463797, acc 0.84375, prec 0.0408163, recall 0.789125
2017-12-09T22:33:40.469153: step 649, loss 0.505008, acc 0.820312, prec 0.040817, recall 0.789264
2017-12-09T22:33:40.767790: step 650, loss 0.282931, acc 0.882812, prec 0.0407961, recall 0.789264
2017-12-09T22:33:41.068302: step 651, loss 0.433401, acc 0.882812, prec 0.0407751, recall 0.789264
2017-12-09T22:33:41.365233: step 652, loss 0.407084, acc 0.890625, prec 0.0408212, recall 0.789543
2017-12-09T22:33:41.666160: step 653, loss 0.429814, acc 0.875, prec 0.0408645, recall 0.789822
2017-12-09T22:33:41.963994: step 654, loss 0.214269, acc 0.914062, prec 0.0408491, recall 0.789822
2017-12-09T22:33:42.264377: step 655, loss 0.315961, acc 0.9375, prec 0.0409035, recall 0.790099
2017-12-09T22:33:42.564771: step 656, loss 0.279971, acc 0.914062, prec 0.0409209, recall 0.790237
2017-12-09T22:33:42.865416: step 657, loss 0.496931, acc 0.882812, prec 0.0409654, recall 0.790514
2017-12-09T22:33:43.169596: step 658, loss 1.83086, acc 0.890625, prec 0.0409486, recall 0.789474
2017-12-09T22:33:44.167938: step 659, loss 3.11257, acc 0.929688, prec 0.0409716, recall 0.788575
2017-12-09T22:33:44.561172: step 660, loss 3.50082, acc 0.84375, prec 0.0409791, recall 0.78768
2017-12-09T22:33:44.857328: step 661, loss 0.289416, acc 0.882812, prec 0.0409909, recall 0.787819
2017-12-09T22:33:45.442198: step 662, loss 2.88328, acc 0.804688, prec 0.04099, recall 0.787443
2017-12-09T22:33:46.457787: step 663, loss 0.742756, acc 0.734375, prec 0.0409426, recall 0.787443
2017-12-09T22:33:47.409805: step 664, loss 0.861327, acc 0.710938, prec 0.0409238, recall 0.787582
2017-12-09T22:33:48.657156: step 665, loss 0.861802, acc 0.78125, prec 0.04095, recall 0.787859
2017-12-09T22:33:49.086195: step 666, loss 1.81924, acc 0.640625, prec 0.0409525, recall 0.787622
2017-12-09T22:33:49.401254: step 667, loss 1.69472, acc 0.609375, prec 0.0409806, recall 0.788036
2017-12-09T22:33:49.726548: step 668, loss 1.65433, acc 0.515625, prec 0.0409272, recall 0.788174
2017-12-09T22:33:50.056779: step 669, loss 1.50443, acc 0.523438, prec 0.0410369, recall 0.788997
2017-12-09T22:33:50.366609: step 670, loss 1.97797, acc 0.507812, prec 0.0409822, recall 0.789133
2017-12-09T22:33:50.667486: step 671, loss 1.72026, acc 0.492188, prec 0.0409251, recall 0.78927
2017-12-09T22:33:50.968075: step 672, loss 1.30662, acc 0.617188, prec 0.040858, recall 0.78927
2017-12-09T22:33:51.267465: step 673, loss 1.35281, acc 0.640625, prec 0.0408913, recall 0.789677
2017-12-09T22:33:51.565014: step 674, loss 1.48503, acc 0.648438, prec 0.0408939, recall 0.789948
2017-12-09T22:33:51.866171: step 675, loss 1.24861, acc 0.703125, prec 0.0408422, recall 0.789948
2017-12-09T22:33:52.162163: step 676, loss 0.919114, acc 0.71875, prec 0.0408252, recall 0.790084
2017-12-09T22:33:52.456984: step 677, loss 0.571724, acc 0.84375, prec 0.0409574, recall 0.790757
2017-12-09T22:33:52.753747: step 678, loss 0.587231, acc 0.8125, prec 0.0409248, recall 0.790757
2017-12-09T22:33:53.052861: step 679, loss 0.859231, acc 0.78125, prec 0.0409504, recall 0.791026
2017-12-09T22:33:53.353534: step 680, loss 0.366321, acc 0.84375, prec 0.0409551, recall 0.79116
2017-12-09T22:33:53.647262: step 681, loss 2.74087, acc 0.875, prec 0.0409996, recall 0.790415
2017-12-09T22:33:53.949177: step 682, loss 0.620825, acc 0.90625, prec 0.0410469, recall 0.790683
2017-12-09T22:33:54.248088: step 683, loss 0.339305, acc 0.84375, prec 0.0410514, recall 0.790816
2017-12-09T22:33:54.546195: step 684, loss 0.398334, acc 0.90625, prec 0.0410986, recall 0.791083
2017-12-09T22:33:54.850728: step 685, loss 0.517228, acc 0.875, prec 0.0411086, recall 0.791216
2017-12-09T22:33:55.149722: step 686, loss 0.356603, acc 0.898438, prec 0.0410909, recall 0.791216
2017-12-09T22:33:55.455508: step 687, loss 0.716023, acc 0.921875, prec 0.0411407, recall 0.791481
2017-12-09T22:33:55.759597: step 688, loss 0.753552, acc 0.890625, prec 0.0412483, recall 0.79201
2017-12-09T22:33:56.060872: step 689, loss 0.265688, acc 0.898438, prec 0.0412623, recall 0.792142
2017-12-09T22:33:56.366082: step 690, loss 0.668157, acc 0.882812, prec 0.0413368, recall 0.792536
2017-12-09T22:33:56.666355: step 691, loss 2.29384, acc 0.851562, prec 0.041407, recall 0.792429
2017-12-09T22:33:56.964528: step 692, loss 0.473021, acc 0.859375, prec 0.0414457, recall 0.792691
2017-12-09T22:33:57.266379: step 693, loss 0.470064, acc 0.859375, prec 0.0415158, recall 0.793082
2017-12-09T22:33:57.561433: step 694, loss 0.526862, acc 0.851562, prec 0.0415844, recall 0.793471
2017-12-09T22:33:57.858349: step 695, loss 0.350957, acc 0.898438, prec 0.0415982, recall 0.793601
2017-12-09T22:33:58.150827: step 696, loss 0.286289, acc 0.875, prec 0.0416078, recall 0.79373
2017-12-09T22:33:58.447133: step 697, loss 2.84103, acc 0.882812, prec 0.0416201, recall 0.793363
2017-12-09T22:33:58.751848: step 698, loss 0.540128, acc 0.8125, prec 0.0416188, recall 0.793492
2017-12-09T22:33:59.052695: step 699, loss 0.73021, acc 0.796875, prec 0.0416147, recall 0.793621
2017-12-09T22:33:59.358142: step 700, loss 0.559018, acc 0.84375, prec 0.0416817, recall 0.794007
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-700

2017-12-09T22:34:00.677354: step 701, loss 3.45498, acc 0.8125, prec 0.0417144, recall 0.793275
2017-12-09T22:34:00.980067: step 702, loss 1.13958, acc 0.820312, prec 0.0417157, recall 0.79291
2017-12-09T22:34:01.280280: step 703, loss 0.799903, acc 0.773438, prec 0.0416762, recall 0.79291
2017-12-09T22:34:01.581652: step 704, loss 0.652992, acc 0.773438, prec 0.0417306, recall 0.793296
2017-12-09T22:34:01.877486: step 705, loss 0.899307, acc 0.726562, prec 0.041808, recall 0.793808
2017-12-09T22:34:02.175103: step 706, loss 1.59235, acc 0.6875, prec 0.0417861, recall 0.793445
2017-12-09T22:34:02.479732: step 707, loss 1.07972, acc 0.703125, prec 0.0418279, recall 0.793827
2017-12-09T22:34:02.776986: step 708, loss 1.34933, acc 0.53125, prec 0.0417465, recall 0.793827
2017-12-09T22:34:03.069315: step 709, loss 0.690337, acc 0.796875, prec 0.0418977, recall 0.794588
2017-12-09T22:34:03.369449: step 710, loss 2.377, acc 0.703125, prec 0.0418785, recall 0.794226
2017-12-09T22:34:03.671480: step 711, loss 1.27471, acc 0.671875, prec 0.0419766, recall 0.794856
2017-12-09T22:34:03.967906: step 712, loss 0.839177, acc 0.703125, prec 0.041987, recall 0.795107
2017-12-09T22:34:04.267510: step 713, loss 0.677076, acc 0.726562, prec 0.0419705, recall 0.795232
2017-12-09T22:34:04.566579: step 714, loss 0.798456, acc 0.75, prec 0.041958, recall 0.795357
2017-12-09T22:34:04.868042: step 715, loss 1.34164, acc 0.703125, prec 0.0419992, recall 0.795732
2017-12-09T22:34:05.172228: step 716, loss 0.75959, acc 0.804688, prec 0.0420579, recall 0.796105
2017-12-09T22:34:05.486511: step 717, loss 1.76981, acc 0.789062, prec 0.0421151, recall 0.795993
2017-12-09T22:34:05.787021: step 718, loss 0.753381, acc 0.789062, prec 0.0421401, recall 0.79624
2017-12-09T22:34:06.087722: step 719, loss 2.15923, acc 0.796875, prec 0.042137, recall 0.795881
2017-12-09T22:34:06.386266: step 720, loss 0.552163, acc 0.765625, prec 0.0421579, recall 0.796128
2017-12-09T22:34:06.685735: step 721, loss 1.07104, acc 0.804688, prec 0.0421255, recall 0.795647
2017-12-09T22:34:06.990818: step 722, loss 0.65784, acc 0.828125, prec 0.0420958, recall 0.795647
2017-12-09T22:34:07.287716: step 723, loss 0.692774, acc 0.804688, prec 0.0421234, recall 0.795894
2017-12-09T22:34:07.582831: step 724, loss 0.858092, acc 0.710938, prec 0.0421654, recall 0.796263
2017-12-09T22:34:07.880562: step 725, loss 0.988709, acc 0.796875, prec 0.0422221, recall 0.796631
2017-12-09T22:34:08.176529: step 726, loss 0.435065, acc 0.835938, prec 0.0422854, recall 0.796997
2017-12-09T22:34:08.475089: step 727, loss 0.554939, acc 0.804688, prec 0.0422517, recall 0.796997
2017-12-09T22:34:08.777340: step 728, loss 1.00977, acc 0.867188, prec 0.0423812, recall 0.797605
2017-12-09T22:34:09.075955: step 729, loss 0.306065, acc 0.882812, prec 0.042361, recall 0.797605
2017-12-09T22:34:09.373320: step 730, loss 2.15234, acc 0.875, prec 0.0424017, recall 0.79737
2017-12-09T22:34:09.675342: step 731, loss 1.9004, acc 0.820312, prec 0.0424329, recall 0.797136
2017-12-09T22:34:09.969675: step 732, loss 0.426135, acc 0.867188, prec 0.04241, recall 0.797136
2017-12-09T22:34:10.271770: step 733, loss 0.57648, acc 0.859375, prec 0.0424162, recall 0.797257
2017-12-09T22:34:10.579524: step 734, loss 0.47804, acc 0.8125, prec 0.0424143, recall 0.797378
2017-12-09T22:34:10.878166: step 735, loss 0.622751, acc 0.835938, prec 0.0424164, recall 0.797499
2017-12-09T22:34:11.174675: step 736, loss 0.443017, acc 0.8125, prec 0.0424145, recall 0.797619
2017-12-09T22:34:11.471325: step 737, loss 0.780583, acc 0.835938, prec 0.0424771, recall 0.79798
2017-12-09T22:34:11.769845: step 738, loss 0.46123, acc 0.828125, prec 0.0425081, recall 0.79822
2017-12-09T22:34:12.070728: step 739, loss 0.539018, acc 0.828125, prec 0.0425391, recall 0.798459
2017-12-09T22:34:12.365668: step 740, loss 0.650971, acc 0.835938, prec 0.0425411, recall 0.798578
2017-12-09T22:34:12.659088: step 741, loss 0.458475, acc 0.898438, prec 0.0425539, recall 0.798697
2017-12-09T22:34:12.964647: step 742, loss 0.907353, acc 0.875, prec 0.0425639, recall 0.798344
2017-12-09T22:34:13.265498: step 743, loss 1.8523, acc 0.796875, prec 0.0426209, recall 0.79823
2017-12-09T22:34:13.563522: step 744, loss 0.335776, acc 0.921875, prec 0.0426376, recall 0.798349
2017-12-09T22:34:13.863830: step 745, loss 1.99423, acc 0.890625, prec 0.0427708, recall 0.798472
2017-12-09T22:34:14.162877: step 746, loss 0.411761, acc 0.828125, prec 0.0427713, recall 0.798591
2017-12-09T22:34:14.344048: step 747, loss 0.227521, acc 0.882353, prec 0.0427633, recall 0.798591
2017-12-09T22:34:14.651629: step 748, loss 0.611691, acc 0.773438, prec 0.0427544, recall 0.798709
2017-12-09T22:34:14.957995: step 749, loss 0.785347, acc 0.835938, prec 0.0427562, recall 0.798827
2017-12-09T22:34:15.256459: step 750, loss 0.56023, acc 0.789062, prec 0.04272, recall 0.798827
2017-12-09T22:34:15.551913: step 751, loss 0.665139, acc 0.8125, prec 0.0427779, recall 0.79918
2017-12-09T22:34:15.854600: step 752, loss 0.932896, acc 0.742188, prec 0.0427936, recall 0.799415
2017-12-09T22:34:16.151678: step 753, loss 0.585747, acc 0.84375, prec 0.0428268, recall 0.79965
2017-12-09T22:34:16.447312: step 754, loss 0.645874, acc 0.804688, prec 0.0428232, recall 0.799766
2017-12-09T22:34:16.747370: step 755, loss 0.518958, acc 0.804688, prec 0.0428496, recall 0.8
2017-12-09T22:34:17.045517: step 756, loss 0.62426, acc 0.773438, prec 0.0428108, recall 0.8
2017-12-09T22:34:17.346289: step 757, loss 0.350336, acc 0.84375, prec 0.0427841, recall 0.8
2017-12-09T22:34:17.643828: step 758, loss 0.898782, acc 0.851562, prec 0.0428496, recall 0.799884
2017-12-09T22:34:17.947498: step 759, loss 0.261233, acc 0.898438, prec 0.0428322, recall 0.799884
2017-12-09T22:34:18.244200: step 760, loss 0.391076, acc 0.898438, prec 0.0428447, recall 0.8
2017-12-09T22:34:18.539073: step 761, loss 0.972126, acc 0.851562, prec 0.0429385, recall 0.800464
2017-12-09T22:34:18.845885: step 762, loss 0.370774, acc 0.882812, prec 0.042978, recall 0.800695
2017-12-09T22:34:19.142069: step 763, loss 0.259227, acc 0.914062, prec 0.0430823, recall 0.801156
2017-12-09T22:34:19.443325: step 764, loss 0.418156, acc 0.835938, prec 0.0431136, recall 0.801386
2017-12-09T22:34:19.740379: step 765, loss 0.332265, acc 0.929688, prec 0.043161, recall 0.801615
2017-12-09T22:34:20.038099: step 766, loss 0.248653, acc 0.890625, prec 0.0431422, recall 0.801615
2017-12-09T22:34:20.354515: step 767, loss 0.176141, acc 0.953125, prec 0.0431639, recall 0.801729
2017-12-09T22:34:20.652779: step 768, loss 0.260955, acc 0.9375, prec 0.0431829, recall 0.801843
2017-12-09T22:34:20.952729: step 769, loss 0.716073, acc 0.9375, prec 0.0432018, recall 0.801957
2017-12-09T22:34:21.255448: step 770, loss 0.251355, acc 0.898438, prec 0.0431844, recall 0.801957
2017-12-09T22:34:21.557682: step 771, loss 1.32157, acc 0.9375, prec 0.0432344, recall 0.801724
2017-12-09T22:34:21.860165: step 772, loss 0.160558, acc 0.929688, prec 0.043252, recall 0.801838
2017-12-09T22:34:22.159697: step 773, loss 0.13661, acc 0.953125, prec 0.0432439, recall 0.801838
2017-12-09T22:34:22.458412: step 774, loss 0.285753, acc 0.898438, prec 0.0432858, recall 0.802065
2017-12-09T22:34:22.760086: step 775, loss 0.249988, acc 0.90625, prec 0.0432697, recall 0.802065
2017-12-09T22:34:23.057926: step 776, loss 0.20984, acc 0.945312, prec 0.0432603, recall 0.802065
2017-12-09T22:34:23.353924: step 777, loss 1.49832, acc 0.898438, prec 0.0433626, recall 0.802059
2017-12-09T22:34:23.655908: step 778, loss 0.227471, acc 0.945312, prec 0.0434124, recall 0.802286
2017-12-09T22:34:23.950731: step 779, loss 2.78321, acc 0.898438, prec 0.0434554, recall 0.802054
2017-12-09T22:34:24.249406: step 780, loss 0.363771, acc 0.875, prec 0.0434339, recall 0.802054
2017-12-09T22:34:24.544879: step 781, loss 0.31047, acc 0.867188, prec 0.0434407, recall 0.802166
2017-12-09T22:34:24.851445: step 782, loss 0.526978, acc 0.875, prec 0.0435373, recall 0.802617
2017-12-09T22:34:25.151468: step 783, loss 0.684094, acc 0.765625, prec 0.043556, recall 0.802841
2017-12-09T22:34:25.448440: step 784, loss 0.63329, acc 0.773438, prec 0.0435465, recall 0.802953
2017-12-09T22:34:25.752637: step 785, loss 0.603267, acc 0.765625, prec 0.0435652, recall 0.803176
2017-12-09T22:34:26.053527: step 786, loss 0.717314, acc 0.789062, prec 0.0435585, recall 0.803288
2017-12-09T22:34:26.351064: step 787, loss 0.531804, acc 0.804688, prec 0.0435544, recall 0.803399
2017-12-09T22:34:26.648138: step 788, loss 0.424067, acc 0.851562, prec 0.0435877, recall 0.803622
2017-12-09T22:34:26.943727: step 789, loss 0.6191, acc 0.8125, prec 0.0436436, recall 0.803955
2017-12-09T22:34:27.243304: step 790, loss 0.379046, acc 0.835938, prec 0.0436741, recall 0.804176
2017-12-09T22:34:27.543636: step 791, loss 0.932055, acc 0.851562, prec 0.0436501, recall 0.803723
2017-12-09T22:34:27.843596: step 792, loss 0.271425, acc 0.90625, prec 0.0436926, recall 0.803944
2017-12-09T22:34:28.142599: step 793, loss 0.737827, acc 0.875, prec 0.043759, recall 0.804274
2017-12-09T22:34:28.443441: step 794, loss 0.397441, acc 0.890625, prec 0.0437987, recall 0.804494
2017-12-09T22:34:28.743806: step 795, loss 1.06136, acc 0.859375, prec 0.0438052, recall 0.804153
2017-12-09T22:34:29.043539: step 796, loss 0.407549, acc 0.84375, prec 0.0438661, recall 0.804482
2017-12-09T22:34:29.346405: step 797, loss 0.451087, acc 0.828125, prec 0.0438658, recall 0.804591
2017-12-09T22:34:29.644643: step 798, loss 0.352603, acc 0.875, prec 0.0439027, recall 0.80481
2017-12-09T22:34:29.944438: step 799, loss 0.355475, acc 0.84375, prec 0.0439051, recall 0.804919
2017-12-09T22:34:30.248526: step 800, loss 0.230906, acc 0.875, prec 0.0439128, recall 0.805028
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-800

2017-12-09T22:34:31.525291: step 801, loss 0.322417, acc 0.882812, prec 0.0439219, recall 0.805137
2017-12-09T22:34:31.823384: step 802, loss 0.376308, acc 0.867188, prec 0.0439574, recall 0.805354
2017-12-09T22:34:32.120795: step 803, loss 0.50106, acc 0.875, prec 0.0439942, recall 0.805571
2017-12-09T22:34:32.426074: step 804, loss 1.22609, acc 0.898438, prec 0.0439781, recall 0.805122
2017-12-09T22:34:32.729025: step 805, loss 1.51501, acc 0.921875, prec 0.0439951, recall 0.804783
2017-12-09T22:34:33.028878: step 806, loss 0.569076, acc 0.953125, prec 0.0440452, recall 0.805
2017-12-09T22:34:33.328466: step 807, loss 0.969316, acc 0.875, prec 0.0440252, recall 0.804553
2017-12-09T22:34:33.630070: step 808, loss 0.799157, acc 0.929688, prec 0.0440725, recall 0.804324
2017-12-09T22:34:33.936400: step 809, loss 0.247398, acc 0.882812, prec 0.0440525, recall 0.804324
2017-12-09T22:34:34.234966: step 810, loss 0.457061, acc 0.851562, prec 0.0440851, recall 0.80454
2017-12-09T22:34:34.536734: step 811, loss 0.72021, acc 0.765625, prec 0.044074, recall 0.804649
2017-12-09T22:34:34.832794: step 812, loss 0.448993, acc 0.820312, prec 0.0441012, recall 0.804865
2017-12-09T22:34:35.131859: step 813, loss 0.624169, acc 0.789062, prec 0.0441808, recall 0.805295
2017-12-09T22:34:35.441850: step 814, loss 0.288208, acc 0.875, prec 0.0442173, recall 0.80551
2017-12-09T22:34:35.734557: step 815, loss 0.497351, acc 0.835938, prec 0.044247, recall 0.805724
2017-12-09T22:34:36.037377: step 816, loss 2.44477, acc 0.84375, prec 0.0442216, recall 0.805281
2017-12-09T22:34:36.335016: step 817, loss 0.507302, acc 0.804688, prec 0.0442171, recall 0.805388
2017-12-09T22:34:36.645772: step 818, loss 0.567076, acc 0.804688, prec 0.0442414, recall 0.805601
2017-12-09T22:34:36.941578: step 819, loss 0.373319, acc 0.890625, prec 0.0442515, recall 0.805708
2017-12-09T22:34:37.242264: step 820, loss 0.424419, acc 0.851562, prec 0.044255, recall 0.805815
2017-12-09T22:34:37.537366: step 821, loss 0.558115, acc 0.804688, prec 0.0442217, recall 0.805815
2017-12-09T22:34:37.832677: step 822, loss 0.656883, acc 0.789062, prec 0.044272, recall 0.806134
2017-12-09T22:34:38.130396: step 823, loss 0.448057, acc 0.859375, prec 0.0443055, recall 0.806346
2017-12-09T22:34:38.428208: step 824, loss 0.826166, acc 0.859375, prec 0.0443403, recall 0.806117
2017-12-09T22:34:38.730192: step 825, loss 0.418854, acc 0.882812, prec 0.0444064, recall 0.806434
2017-12-09T22:34:39.028225: step 826, loss 0.313575, acc 0.890625, prec 0.0444451, recall 0.806645
2017-12-09T22:34:39.323486: step 827, loss 1.04282, acc 0.90625, prec 0.0444591, recall 0.806311
2017-12-09T22:34:39.622469: step 828, loss 0.420366, acc 0.882812, prec 0.0445251, recall 0.806627
2017-12-09T22:34:39.923328: step 829, loss 0.360781, acc 0.867188, prec 0.0445024, recall 0.806627
2017-12-09T22:34:40.219351: step 830, loss 0.276599, acc 0.898438, prec 0.0445137, recall 0.806732
2017-12-09T22:34:40.518830: step 831, loss 0.235626, acc 0.921875, prec 0.044529, recall 0.806837
2017-12-09T22:34:40.825060: step 832, loss 0.140667, acc 0.929688, prec 0.044517, recall 0.806837
2017-12-09T22:34:41.123437: step 833, loss 0.350534, acc 0.875, prec 0.0444956, recall 0.806837
2017-12-09T22:34:41.419882: step 834, loss 0.300618, acc 0.898438, prec 0.0444783, recall 0.806837
2017-12-09T22:34:41.721934: step 835, loss 0.277238, acc 0.921875, prec 0.0445222, recall 0.807046
2017-12-09T22:34:42.018035: step 836, loss 7.60717, acc 0.90625, prec 0.0446244, recall 0.806156
2017-12-09T22:34:42.324171: step 837, loss 1.51075, acc 0.921875, prec 0.0447266, recall 0.806139
2017-12-09T22:34:42.625779: step 838, loss 0.278122, acc 0.882812, prec 0.0447351, recall 0.806243
2017-12-09T22:34:42.927604: step 839, loss 0.574543, acc 0.835938, prec 0.0447641, recall 0.806452
2017-12-09T22:34:43.222832: step 840, loss 0.557614, acc 0.851562, prec 0.0448242, recall 0.806763
2017-12-09T22:34:43.523774: step 841, loss 0.533326, acc 0.84375, prec 0.0448544, recall 0.806971
2017-12-09T22:34:43.823618: step 842, loss 0.667726, acc 0.773438, prec 0.044901, recall 0.807281
2017-12-09T22:34:44.119014: step 843, loss 0.56022, acc 0.8125, prec 0.0449258, recall 0.807487
2017-12-09T22:34:44.418330: step 844, loss 0.536969, acc 0.773438, prec 0.0449154, recall 0.80759
2017-12-09T22:34:44.715001: step 845, loss 0.780085, acc 0.835938, prec 0.0449725, recall 0.807898
2017-12-09T22:34:45.015210: step 846, loss 0.812171, acc 0.8125, prec 0.0449688, recall 0.808
2017-12-09T22:34:45.314106: step 847, loss 0.737203, acc 0.734375, prec 0.0449518, recall 0.808102
2017-12-09T22:34:45.615343: step 848, loss 0.519364, acc 0.8125, prec 0.0449481, recall 0.808205
2017-12-09T22:34:45.913376: step 849, loss 0.545666, acc 0.828125, prec 0.0449471, recall 0.808307
2017-12-09T22:34:46.207714: step 850, loss 0.453657, acc 0.851562, prec 0.0449219, recall 0.808307
2017-12-09T22:34:46.511033: step 851, loss 0.363285, acc 0.859375, prec 0.0449545, recall 0.808511
2017-12-09T22:34:46.813218: step 852, loss 0.518291, acc 0.867188, prec 0.0449601, recall 0.808612
2017-12-09T22:34:47.116757: step 853, loss 0.484065, acc 0.851562, prec 0.0449631, recall 0.808714
2017-12-09T22:34:47.414593: step 854, loss 0.329238, acc 0.882812, prec 0.0449996, recall 0.808917
2017-12-09T22:34:47.711687: step 855, loss 0.287219, acc 0.914062, prec 0.0449849, recall 0.808917
2017-12-09T22:34:48.017244: step 856, loss 4.38062, acc 0.882812, prec 0.0450804, recall 0.808466
2017-12-09T22:34:48.316025: step 857, loss 0.319042, acc 0.890625, prec 0.0451463, recall 0.808769
2017-12-09T22:34:48.617209: step 858, loss 0.279896, acc 0.914062, prec 0.0451598, recall 0.80887
2017-12-09T22:34:48.917709: step 859, loss 0.496867, acc 0.882812, prec 0.0452242, recall 0.809172
2017-12-09T22:34:49.213219: step 860, loss 0.444586, acc 0.835938, prec 0.0452525, recall 0.809373
2017-12-09T22:34:49.513872: step 861, loss 0.913436, acc 0.914062, prec 0.0452672, recall 0.809048
2017-12-09T22:34:49.814900: step 862, loss 0.9823, acc 0.867188, prec 0.0453021, recall 0.808824
2017-12-09T22:34:50.121453: step 863, loss 0.45811, acc 0.835938, prec 0.0453303, recall 0.809024
2017-12-09T22:34:50.435709: step 864, loss 0.59089, acc 0.8125, prec 0.0453264, recall 0.809124
2017-12-09T22:34:50.733719: step 865, loss 0.65855, acc 0.835938, prec 0.0453825, recall 0.809424
2017-12-09T22:34:51.032826: step 866, loss 0.64802, acc 0.820312, prec 0.0454079, recall 0.809623
2017-12-09T22:34:51.330838: step 867, loss 0.491054, acc 0.796875, prec 0.0454013, recall 0.809723
2017-12-09T22:34:51.627876: step 868, loss 0.422688, acc 0.867188, prec 0.0454066, recall 0.809822
2017-12-09T22:34:51.926946: step 869, loss 0.472653, acc 0.820312, prec 0.045376, recall 0.809822
2017-12-09T22:34:52.226064: step 870, loss 0.479684, acc 0.875, prec 0.0453827, recall 0.809922
2017-12-09T22:34:52.522966: step 871, loss 0.701385, acc 0.929688, prec 0.0454266, recall 0.81012
2017-12-09T22:34:52.826656: step 872, loss 1.39317, acc 0.851562, prec 0.0454585, recall 0.809896
2017-12-09T22:34:53.128882: step 873, loss 0.347163, acc 0.882812, prec 0.0455502, recall 0.810291
2017-12-09T22:34:53.429365: step 874, loss 0.825131, acc 0.84375, prec 0.0456629, recall 0.810783
2017-12-09T22:34:53.727236: step 875, loss 1.02813, acc 0.84375, prec 0.0457755, recall 0.811272
2017-12-09T22:34:54.033816: step 876, loss 0.427569, acc 0.875, prec 0.0457541, recall 0.811272
2017-12-09T22:34:54.331109: step 877, loss 0.438677, acc 0.84375, prec 0.0457274, recall 0.811272
2017-12-09T22:34:54.628404: step 878, loss 0.362145, acc 0.882812, prec 0.0457631, recall 0.811467
2017-12-09T22:34:54.929527: step 879, loss 3.46638, acc 0.828125, prec 0.0458475, recall 0.81102
2017-12-09T22:34:55.228017: step 880, loss 0.57748, acc 0.796875, prec 0.0458684, recall 0.811214
2017-12-09T22:34:55.530984: step 881, loss 0.872427, acc 0.773438, prec 0.0458852, recall 0.811408
2017-12-09T22:34:55.828450: step 882, loss 0.940495, acc 0.6875, prec 0.0458873, recall 0.811602
2017-12-09T22:34:56.129239: step 883, loss 0.548931, acc 0.796875, prec 0.0458803, recall 0.811698
2017-12-09T22:34:56.426386: step 884, loss 0.671447, acc 0.796875, prec 0.0459011, recall 0.811891
2017-12-09T22:34:56.723873: step 885, loss 0.481882, acc 0.8125, prec 0.0458968, recall 0.811988
2017-12-09T22:34:57.017631: step 886, loss 0.620202, acc 0.78125, prec 0.0458872, recall 0.812084
2017-12-09T22:34:57.312565: step 887, loss 0.754035, acc 0.8125, prec 0.045883, recall 0.81218
2017-12-09T22:34:57.611815: step 888, loss 0.486528, acc 0.8125, prec 0.0459063, recall 0.812372
2017-12-09T22:34:57.909637: step 889, loss 0.668942, acc 0.8125, prec 0.045902, recall 0.812468
2017-12-09T22:34:58.210433: step 890, loss 0.489286, acc 0.804688, prec 0.045924, recall 0.81266
2017-12-09T22:34:58.511640: step 891, loss 0.56882, acc 0.84375, prec 0.0460075, recall 0.813041
2017-12-09T22:34:58.811504: step 892, loss 0.279972, acc 0.890625, prec 0.0460164, recall 0.813136
2017-12-09T22:34:59.110489: step 893, loss 0.826351, acc 0.890625, prec 0.0460253, recall 0.813232
2017-12-09T22:34:59.410109: step 894, loss 0.240225, acc 0.898438, prec 0.0460356, recall 0.813327
2017-12-09T22:34:59.711138: step 895, loss 1.27927, acc 0.890625, prec 0.0461268, recall 0.813706
2017-12-09T22:35:00.013465: step 896, loss 0.471889, acc 0.890625, prec 0.046218, recall 0.814083
2017-12-09T22:35:00.315123: step 897, loss 0.252719, acc 0.945312, prec 0.0462087, recall 0.814083
2017-12-09T22:35:00.612357: step 898, loss 0.23357, acc 0.953125, prec 0.0462556, recall 0.814271
2017-12-09T22:35:00.905828: step 899, loss 0.258377, acc 0.921875, prec 0.0463245, recall 0.814553
2017-12-09T22:35:01.201955: step 900, loss 0.192519, acc 0.929688, prec 0.0463399, recall 0.814646

Evaluation:
2017-12-09T22:35:05.907488: step 900, loss 1.88613, acc 0.93773, prec 0.0476446, recall 0.793055

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-900

2017-12-09T22:35:07.207070: step 901, loss 1.00175, acc 0.90625, prec 0.0476566, recall 0.79278
2017-12-09T22:35:07.508951: step 902, loss 0.157861, acc 0.960938, prec 0.0477036, recall 0.792974
2017-12-09T22:35:07.810366: step 903, loss 0.218366, acc 0.945312, prec 0.047721, recall 0.793071
2017-12-09T22:35:08.107387: step 904, loss 0.336321, acc 0.921875, prec 0.047788, recall 0.793361
2017-12-09T22:35:08.412628: step 905, loss 4.27796, acc 0.914062, prec 0.0478282, recall 0.793184
2017-12-09T22:35:08.715436: step 906, loss 0.258388, acc 0.921875, prec 0.0478415, recall 0.79328
2017-12-09T22:35:09.014053: step 907, loss 0.549689, acc 0.890625, prec 0.0479298, recall 0.793666
2017-12-09T22:35:09.320883: step 908, loss 1.21047, acc 0.875, prec 0.0480153, recall 0.794049
2017-12-09T22:35:09.618374: step 909, loss 0.504343, acc 0.921875, prec 0.0480553, recall 0.794241
2017-12-09T22:35:09.920495: step 910, loss 1.14492, acc 0.859375, prec 0.0480858, recall 0.794063
2017-12-09T22:35:10.221946: step 911, loss 0.568634, acc 0.828125, prec 0.0480561, recall 0.794063
2017-12-09T22:35:10.520060: step 912, loss 0.479322, acc 0.835938, prec 0.0480545, recall 0.794159
2017-12-09T22:35:10.823046: step 913, loss 0.727861, acc 0.757812, prec 0.0480928, recall 0.794444
2017-12-09T22:35:11.120257: step 914, loss 0.617533, acc 0.804688, prec 0.0481125, recall 0.794635
2017-12-09T22:35:11.418350: step 915, loss 0.917689, acc 0.765625, prec 0.0482052, recall 0.795108
2017-12-09T22:35:11.722088: step 916, loss 0.592908, acc 0.773438, prec 0.0481928, recall 0.795203
2017-12-09T22:35:12.018434: step 917, loss 0.667512, acc 0.796875, prec 0.0481844, recall 0.795297
2017-12-09T22:35:12.319289: step 918, loss 0.75797, acc 0.789062, prec 0.0482277, recall 0.79558
2017-12-09T22:35:12.622956: step 919, loss 0.534466, acc 0.828125, prec 0.0482778, recall 0.795862
2017-12-09T22:35:12.918746: step 920, loss 0.48754, acc 0.859375, prec 0.0484127, recall 0.796424
2017-12-09T22:35:13.219711: step 921, loss 0.91291, acc 0.789062, prec 0.0484293, recall 0.79661
2017-12-09T22:35:13.524636: step 922, loss 0.46736, acc 0.851562, prec 0.0484037, recall 0.79661
2017-12-09T22:35:13.826657: step 923, loss 0.605189, acc 0.890625, prec 0.0484908, recall 0.796982
2017-12-09T22:35:14.130700: step 924, loss 1.0151, acc 0.867188, prec 0.0484956, recall 0.796711
2017-12-09T22:35:14.433052: step 925, loss 0.288141, acc 0.867188, prec 0.0484727, recall 0.796711
2017-12-09T22:35:14.732277: step 926, loss 0.938106, acc 0.84375, prec 0.0484736, recall 0.79644
2017-12-09T22:35:15.028205: step 927, loss 0.365515, acc 0.914062, prec 0.0484852, recall 0.796533
2017-12-09T22:35:15.325781: step 928, loss 0.278549, acc 0.921875, prec 0.0484717, recall 0.796533
2017-12-09T22:35:15.624668: step 929, loss 0.513936, acc 0.882812, prec 0.0485308, recall 0.796811
2017-12-09T22:35:15.932899: step 930, loss 0.840442, acc 0.90625, prec 0.0486215, recall 0.796818
2017-12-09T22:35:16.231657: step 931, loss 0.47025, acc 0.921875, prec 0.0486608, recall 0.797003
2017-12-09T22:35:16.535668: step 932, loss 0.319839, acc 0.914062, prec 0.048646, recall 0.797003
2017-12-09T22:35:16.832428: step 933, loss 0.821193, acc 0.890625, prec 0.0486798, recall 0.797187
2017-12-09T22:35:17.131824: step 934, loss 0.305935, acc 0.890625, prec 0.0486873, recall 0.797279
2017-12-09T22:35:17.428349: step 935, loss 0.305427, acc 0.890625, prec 0.0486947, recall 0.797371
2017-12-09T22:35:17.725020: step 936, loss 1.87257, acc 0.890625, prec 0.0487299, recall 0.797193
2017-12-09T22:35:18.024149: step 937, loss 0.577037, acc 0.835938, prec 0.0487016, recall 0.797193
2017-12-09T22:35:18.324362: step 938, loss 1.41209, acc 0.804688, prec 0.0487218, recall 0.797016
2017-12-09T22:35:18.631186: step 939, loss 0.495079, acc 0.84375, prec 0.0487738, recall 0.797291
2017-12-09T22:35:18.930880: step 940, loss 0.761405, acc 0.796875, prec 0.0487913, recall 0.797474
2017-12-09T22:35:19.229099: step 941, loss 0.958086, acc 0.789062, prec 0.0488074, recall 0.797657
2017-12-09T22:35:19.523736: step 942, loss 0.436112, acc 0.875, prec 0.0488121, recall 0.797748
2017-12-09T22:35:19.822149: step 943, loss 0.473025, acc 0.820312, prec 0.0488336, recall 0.79793
2017-12-09T22:35:20.123874: step 944, loss 0.485322, acc 0.84375, prec 0.0488329, recall 0.798021
2017-12-09T22:35:20.427239: step 945, loss 0.544167, acc 0.773438, prec 0.0488201, recall 0.798111
2017-12-09T22:35:20.722540: step 946, loss 0.597467, acc 0.804688, prec 0.0487865, recall 0.798111
2017-12-09T22:35:21.018909: step 947, loss 0.553128, acc 0.789062, prec 0.0487503, recall 0.798111
2017-12-09T22:35:21.315899: step 948, loss 0.544425, acc 0.796875, prec 0.0487156, recall 0.798111
2017-12-09T22:35:21.617076: step 949, loss 0.585804, acc 0.804688, prec 0.0487343, recall 0.798293
2017-12-09T22:35:21.918161: step 950, loss 0.290167, acc 0.867188, prec 0.0487377, recall 0.798383
2017-12-09T22:35:22.220558: step 951, loss 0.35099, acc 0.90625, prec 0.0488259, recall 0.798745
2017-12-09T22:35:22.517742: step 952, loss 0.695077, acc 0.859375, prec 0.0488279, recall 0.798835
2017-12-09T22:35:22.814112: step 953, loss 0.40665, acc 0.914062, prec 0.0488653, recall 0.799015
2017-12-09T22:35:23.115652: step 954, loss 0.148613, acc 0.921875, prec 0.0488779, recall 0.799105
2017-12-09T22:35:23.418005: step 955, loss 2.70636, acc 0.890625, prec 0.0489139, recall 0.798571
2017-12-09T22:35:23.723262: step 956, loss 0.188392, acc 0.921875, prec 0.0489266, recall 0.798661
2017-12-09T22:35:24.020931: step 957, loss 0.874923, acc 0.921875, prec 0.0489665, recall 0.798484
2017-12-09T22:35:24.320375: step 958, loss 0.630514, acc 0.914062, prec 0.0490038, recall 0.798664
2017-12-09T22:35:24.630401: step 959, loss 0.693973, acc 0.945312, prec 0.0490724, recall 0.798932
2017-12-09T22:35:24.934622: step 960, loss 0.995819, acc 0.875, prec 0.0491042, recall 0.798756
2017-12-09T22:35:25.234001: step 961, loss 0.769302, acc 0.828125, prec 0.0491007, recall 0.798845
2017-12-09T22:35:25.537698: step 962, loss 0.890845, acc 0.765625, prec 0.0491902, recall 0.799291
2017-12-09T22:35:25.836338: step 963, loss 0.872183, acc 0.695312, prec 0.0491897, recall 0.799469
2017-12-09T22:35:26.138119: step 964, loss 0.619247, acc 0.8125, prec 0.0492093, recall 0.799646
2017-12-09T22:35:26.435211: step 965, loss 0.611139, acc 0.804688, prec 0.0491759, recall 0.799646
2017-12-09T22:35:26.735118: step 966, loss 0.523595, acc 0.773438, prec 0.049163, recall 0.799735
2017-12-09T22:35:27.033833: step 967, loss 0.793745, acc 0.734375, prec 0.049195, recall 0.8
2017-12-09T22:35:27.333361: step 968, loss 0.486343, acc 0.789062, prec 0.049159, recall 0.8
2017-12-09T22:35:27.629175: step 969, loss 0.747982, acc 0.75, prec 0.0491421, recall 0.800088
2017-12-09T22:35:27.930969: step 970, loss 0.557691, acc 0.828125, prec 0.0491901, recall 0.800353
2017-12-09T22:35:28.229789: step 971, loss 1.99589, acc 0.851562, prec 0.0491661, recall 0.8
2017-12-09T22:35:28.536582: step 972, loss 0.523063, acc 0.851562, prec 0.0491923, recall 0.800176
2017-12-09T22:35:28.837824: step 973, loss 0.313433, acc 0.898438, prec 0.0492007, recall 0.800264
2017-12-09T22:35:29.131253: step 974, loss 2.66474, acc 0.882812, prec 0.0492335, recall 0.800088
2017-12-09T22:35:29.439943: step 975, loss 0.540269, acc 0.859375, prec 0.0492096, recall 0.800088
2017-12-09T22:35:29.736487: step 976, loss 0.795773, acc 0.859375, prec 0.0492884, recall 0.800439
2017-12-09T22:35:30.039316: step 977, loss 0.712717, acc 0.867188, prec 0.0493171, recall 0.800614
2017-12-09T22:35:30.347494: step 978, loss 0.267948, acc 0.9375, prec 0.0493064, recall 0.800614
2017-12-09T22:35:30.645020: step 979, loss 0.499293, acc 0.898438, prec 0.0493661, recall 0.800875
2017-12-09T22:35:30.950085: step 980, loss 0.325952, acc 0.867188, prec 0.0493947, recall 0.801049
2017-12-09T22:35:31.254602: step 981, loss 0.484058, acc 0.84375, prec 0.0494193, recall 0.801223
2017-12-09T22:35:31.554499: step 982, loss 0.582542, acc 0.820312, prec 0.0494911, recall 0.80157
2017-12-09T22:35:31.851309: step 983, loss 0.463427, acc 0.882812, prec 0.0494967, recall 0.801656
2017-12-09T22:35:32.152428: step 984, loss 0.52616, acc 0.882812, prec 0.049579, recall 0.802002
2017-12-09T22:35:32.448014: step 985, loss 0.447317, acc 0.875, prec 0.0495832, recall 0.802088
2017-12-09T22:35:32.746414: step 986, loss 1.85184, acc 0.898438, prec 0.0496439, recall 0.801997
2017-12-09T22:35:33.050416: step 987, loss 0.539586, acc 0.875, prec 0.0496991, recall 0.802255
2017-12-09T22:35:33.347937: step 988, loss 0.46371, acc 0.859375, prec 0.0497261, recall 0.802426
2017-12-09T22:35:33.645070: step 989, loss 0.450768, acc 0.8125, prec 0.0497196, recall 0.802512
2017-12-09T22:35:33.945368: step 990, loss 0.268083, acc 0.898438, prec 0.0497532, recall 0.802683
2017-12-09T22:35:34.242015: step 991, loss 0.477469, acc 0.859375, prec 0.0497802, recall 0.802853
2017-12-09T22:35:34.540325: step 992, loss 0.388604, acc 0.882812, prec 0.0498366, recall 0.803109
2017-12-09T22:35:34.834991: step 993, loss 0.479531, acc 0.828125, prec 0.0498581, recall 0.803279
2017-12-09T22:35:35.137295: step 994, loss 0.414503, acc 0.859375, prec 0.0498341, recall 0.803279
2017-12-09T22:35:35.450031: step 995, loss 0.313641, acc 0.867188, prec 0.0498368, recall 0.803364
2017-12-09T22:35:35.630996: step 996, loss 0.273129, acc 0.901961, prec 0.049881, recall 0.803533
2017-12-09T22:35:35.932398: step 997, loss 0.245844, acc 0.914062, prec 0.0499679, recall 0.803871
2017-12-09T22:35:36.229718: step 998, loss 0.371969, acc 0.898438, prec 0.0499759, recall 0.803955
2017-12-09T22:35:36.529000: step 999, loss 0.276854, acc 0.890625, prec 0.0500334, recall 0.804208
2017-12-09T22:35:36.829347: step 1000, loss 0.388533, acc 0.945312, prec 0.0501001, recall 0.80446
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1000

2017-12-09T22:35:38.153085: step 1001, loss 1.63328, acc 0.898438, prec 0.0501602, recall 0.804366
2017-12-09T22:35:38.452972: step 1002, loss 0.273235, acc 0.898438, prec 0.0501681, recall 0.80445
2017-12-09T22:35:38.755747: step 1003, loss 1.72078, acc 0.960938, prec 0.0502388, recall 0.804357
2017-12-09T22:35:39.057079: step 1004, loss 0.585309, acc 0.867188, prec 0.050292, recall 0.804608
2017-12-09T22:35:39.357612: step 1005, loss 0.240963, acc 0.914062, prec 0.0503026, recall 0.804691
2017-12-09T22:35:39.654293: step 1006, loss 0.458316, acc 0.914062, prec 0.0503637, recall 0.80494
2017-12-09T22:35:39.959286: step 1007, loss 0.373447, acc 0.890625, prec 0.0504208, recall 0.805189
2017-12-09T22:35:40.261222: step 1008, loss 0.274057, acc 0.90625, prec 0.0504553, recall 0.805355
2017-12-09T22:35:40.557756: step 1009, loss 0.445981, acc 0.84375, prec 0.0504537, recall 0.805438
2017-12-09T22:35:40.854708: step 1010, loss 0.234921, acc 0.914062, prec 0.05054, recall 0.805768
2017-12-09T22:35:41.155684: step 1011, loss 0.316819, acc 0.867188, prec 0.0505171, recall 0.805768
2017-12-09T22:35:41.454415: step 1012, loss 0.289996, acc 0.875, prec 0.0505209, recall 0.80585
2017-12-09T22:35:41.757538: step 1013, loss 0.546885, acc 0.859375, prec 0.0505976, recall 0.806179
2017-12-09T22:35:42.060821: step 1014, loss 0.258526, acc 0.898438, prec 0.0506306, recall 0.806342
2017-12-09T22:35:42.356042: step 1015, loss 0.402992, acc 0.875, prec 0.0507098, recall 0.806669
2017-12-09T22:35:42.651069: step 1016, loss 0.369454, acc 0.890625, prec 0.0507162, recall 0.806751
2017-12-09T22:35:42.948028: step 1017, loss 0.421456, acc 0.882812, prec 0.0507212, recall 0.806833
2017-12-09T22:35:43.244156: step 1018, loss 0.473737, acc 0.921875, prec 0.0507832, recall 0.807077
2017-12-09T22:35:43.541007: step 1019, loss 0.791184, acc 0.929688, prec 0.0508479, recall 0.806981
2017-12-09T22:35:43.840244: step 1020, loss 1.11115, acc 0.882812, prec 0.0509045, recall 0.806885
2017-12-09T22:35:44.139633: step 1021, loss 0.498307, acc 0.882812, prec 0.0509345, recall 0.807047
2017-12-09T22:35:44.439917: step 1022, loss 0.693141, acc 0.867188, prec 0.0509618, recall 0.807209
2017-12-09T22:35:44.738358: step 1023, loss 0.88565, acc 0.867188, prec 0.0510156, recall 0.807113
2017-12-09T22:35:45.044280: step 1024, loss 0.404326, acc 0.875, prec 0.0510441, recall 0.807274
2017-12-09T22:35:45.342721: step 1025, loss 0.77459, acc 0.859375, prec 0.0510951, recall 0.807516
2017-12-09T22:35:45.639590: step 1026, loss 0.558564, acc 0.867188, prec 0.0511473, recall 0.807756
2017-12-09T22:35:45.940680: step 1027, loss 0.48254, acc 0.828125, prec 0.0511427, recall 0.807837
2017-12-09T22:35:46.241378: step 1028, loss 0.55001, acc 0.828125, prec 0.051138, recall 0.807917
2017-12-09T22:35:46.535408: step 1029, loss 0.473352, acc 0.828125, prec 0.0511834, recall 0.808156
2017-12-09T22:35:46.841340: step 1030, loss 0.610314, acc 0.828125, prec 0.0512787, recall 0.808555
2017-12-09T22:35:47.138703: step 1031, loss 0.494385, acc 0.867188, prec 0.0512807, recall 0.808634
2017-12-09T22:35:47.437080: step 1032, loss 0.338486, acc 0.875, prec 0.0512841, recall 0.808714
2017-12-09T22:35:47.734836: step 1033, loss 0.460642, acc 0.84375, prec 0.0512821, recall 0.808793
2017-12-09T22:35:48.034925: step 1034, loss 0.516609, acc 0.828125, prec 0.0512524, recall 0.808793
2017-12-09T22:35:48.333681: step 1035, loss 0.471569, acc 0.875, prec 0.0512558, recall 0.808872
2017-12-09T22:35:48.633870: step 1036, loss 0.501265, acc 0.859375, prec 0.0513312, recall 0.809189
2017-12-09T22:35:48.937962: step 1037, loss 0.239525, acc 0.945312, prec 0.0513467, recall 0.809268
2017-12-09T22:35:49.244053: step 1038, loss 0.211472, acc 0.921875, prec 0.051383, recall 0.809425
2017-12-09T22:35:49.542430: step 1039, loss 0.306091, acc 0.914062, prec 0.051393, recall 0.809504
2017-12-09T22:35:49.843241: step 1040, loss 0.162574, acc 0.953125, prec 0.0514098, recall 0.809583
2017-12-09T22:35:50.146056: step 1041, loss 0.122512, acc 0.96875, prec 0.0514542, recall 0.80974
2017-12-09T22:35:50.459552: step 1042, loss 0.215804, acc 0.9375, prec 0.0514683, recall 0.809819
2017-12-09T22:35:50.761853: step 1043, loss 1.02787, acc 0.96875, prec 0.051514, recall 0.809642
2017-12-09T22:35:51.065130: step 1044, loss 0.513739, acc 0.960938, prec 0.0516066, recall 0.809955
2017-12-09T22:35:51.369294: step 1045, loss 0.135108, acc 0.960938, prec 0.0515999, recall 0.809955
2017-12-09T22:35:51.671574: step 1046, loss 0.49644, acc 0.9375, prec 0.0516636, recall 0.810189
2017-12-09T22:35:51.972799: step 1047, loss 0.48557, acc 0.9375, prec 0.0517521, recall 0.8105
2017-12-09T22:35:52.273955: step 1048, loss 0.253622, acc 0.953125, prec 0.0517937, recall 0.810656
2017-12-09T22:35:52.571626: step 1049, loss 0.120859, acc 0.96875, prec 0.0518131, recall 0.810733
2017-12-09T22:35:52.877881: step 1050, loss 0.293455, acc 0.9375, prec 0.0519015, recall 0.811043
2017-12-09T22:35:53.184455: step 1051, loss 0.476057, acc 0.9375, prec 0.0519402, recall 0.811197
2017-12-09T22:35:53.483661: step 1052, loss 0.511973, acc 0.890625, prec 0.0519956, recall 0.811429
2017-12-09T22:35:53.782702: step 1053, loss 0.33628, acc 0.882812, prec 0.0519752, recall 0.811429
2017-12-09T22:35:54.081104: step 1054, loss 0.294289, acc 0.890625, prec 0.0520305, recall 0.811659
2017-12-09T22:35:54.386660: step 1055, loss 0.374334, acc 0.921875, prec 0.0520912, recall 0.811889
2017-12-09T22:35:54.684972: step 1056, loss 0.281398, acc 0.90625, prec 0.0520749, recall 0.811889
2017-12-09T22:35:54.982269: step 1057, loss 0.275584, acc 0.914062, prec 0.0521094, recall 0.812042
2017-12-09T22:35:55.283829: step 1058, loss 0.257041, acc 0.875, prec 0.0521124, recall 0.812119
2017-12-09T22:35:55.586449: step 1059, loss 0.150153, acc 0.945312, prec 0.0521276, recall 0.812195
2017-12-09T22:35:55.885846: step 1060, loss 0.8928, acc 0.90625, prec 0.0521621, recall 0.812018
2017-12-09T22:35:56.189704: step 1061, loss 0.33929, acc 0.914062, prec 0.052246, recall 0.812323
2017-12-09T22:35:56.486628: step 1062, loss 0.467258, acc 0.914062, prec 0.0523051, recall 0.812551
2017-12-09T22:35:56.793147: step 1063, loss 0.41523, acc 0.914062, prec 0.0523395, recall 0.812702
2017-12-09T22:35:57.099976: step 1064, loss 0.196189, acc 0.9375, prec 0.0523533, recall 0.812778
2017-12-09T22:35:57.397362: step 1065, loss 2.18849, acc 0.921875, prec 0.0523657, recall 0.812525
2017-12-09T22:35:57.697119: step 1066, loss 0.816442, acc 0.859375, prec 0.0524152, recall 0.812752
2017-12-09T22:35:57.994833: step 1067, loss 0.342623, acc 0.890625, prec 0.0524207, recall 0.812828
2017-12-09T22:35:58.293399: step 1068, loss 0.400219, acc 0.835938, prec 0.0524167, recall 0.812903
2017-12-09T22:35:58.592177: step 1069, loss 0.371773, acc 0.875, prec 0.0524442, recall 0.813054
2017-12-09T22:35:58.891374: step 1070, loss 0.565983, acc 0.859375, prec 0.0525427, recall 0.81343
2017-12-09T22:35:59.192223: step 1071, loss 0.47508, acc 0.820312, prec 0.0525851, recall 0.813655
2017-12-09T22:35:59.489488: step 1072, loss 0.510868, acc 0.757812, prec 0.0525674, recall 0.813729
2017-12-09T22:35:59.785583: step 1073, loss 0.442846, acc 0.804688, prec 0.0525579, recall 0.813804
2017-12-09T22:36:00.095396: step 1074, loss 0.45351, acc 0.78125, prec 0.0526179, recall 0.814103
2017-12-09T22:36:00.402922: step 1075, loss 0.441966, acc 0.828125, prec 0.0526125, recall 0.814177
2017-12-09T22:36:00.699227: step 1076, loss 0.37221, acc 0.882812, prec 0.0526901, recall 0.814474
2017-12-09T22:36:00.999664: step 1077, loss 0.526549, acc 0.835938, prec 0.052735, recall 0.814696
2017-12-09T22:36:01.298939: step 1078, loss 0.325376, acc 0.867188, prec 0.0527852, recall 0.814918
2017-12-09T22:36:01.600542: step 1079, loss 0.267062, acc 0.90625, prec 0.0527689, recall 0.814918
2017-12-09T22:36:01.904133: step 1080, loss 0.366607, acc 0.875, prec 0.0527471, recall 0.814918
2017-12-09T22:36:02.202757: step 1081, loss 0.328905, acc 0.90625, prec 0.0527552, recall 0.814992
2017-12-09T22:36:02.500822: step 1082, loss 0.359923, acc 0.921875, prec 0.052766, recall 0.815066
2017-12-09T22:36:02.799580: step 1083, loss 0.249049, acc 0.953125, prec 0.0528067, recall 0.815213
2017-12-09T22:36:03.099278: step 1084, loss 0.254313, acc 0.945312, prec 0.0528705, recall 0.815434
2017-12-09T22:36:03.397240: step 1085, loss 0.309109, acc 0.929688, prec 0.0529315, recall 0.815654
2017-12-09T22:36:03.695581: step 1086, loss 0.113508, acc 0.96875, prec 0.0529504, recall 0.815727
2017-12-09T22:36:03.997209: step 1087, loss 1.711, acc 0.9375, prec 0.0529897, recall 0.815549
2017-12-09T22:36:04.293703: step 1088, loss 1.72089, acc 0.9375, prec 0.0530289, recall 0.815372
2017-12-09T22:36:04.601946: step 1089, loss 0.179098, acc 0.945312, prec 0.0530682, recall 0.815519
2017-12-09T22:36:04.907950: step 1090, loss 0.447435, acc 0.921875, prec 0.0531033, recall 0.815665
2017-12-09T22:36:05.220443: step 1091, loss 2.15669, acc 0.898438, prec 0.0531112, recall 0.815415
2017-12-09T22:36:05.527715: step 1092, loss 0.329497, acc 0.9375, prec 0.0531247, recall 0.815488
2017-12-09T22:36:05.829085: step 1093, loss 0.423295, acc 0.867188, prec 0.0531258, recall 0.815561
2017-12-09T22:36:06.127062: step 1094, loss 0.433787, acc 0.898438, prec 0.0531324, recall 0.815634
2017-12-09T22:36:06.426734: step 1095, loss 0.309629, acc 0.875, prec 0.0531592, recall 0.815779
2017-12-09T22:36:06.721296: step 1096, loss 0.446551, acc 0.835938, prec 0.0531549, recall 0.815852
2017-12-09T22:36:07.014744: step 1097, loss 0.589437, acc 0.796875, prec 0.0531194, recall 0.815852
2017-12-09T22:36:07.307897: step 1098, loss 0.4414, acc 0.820312, prec 0.0531609, recall 0.816069
2017-12-09T22:36:07.609632: step 1099, loss 0.455388, acc 0.820312, prec 0.0531296, recall 0.816069
2017-12-09T22:36:07.909013: step 1100, loss 0.902765, acc 0.84375, prec 0.0531266, recall 0.816142
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1100

2017-12-09T22:36:09.142692: step 1101, loss 0.58496, acc 0.8125, prec 0.0530939, recall 0.816142
2017-12-09T22:36:09.439791: step 1102, loss 0.376401, acc 0.859375, prec 0.0530937, recall 0.816214
2017-12-09T22:36:09.741967: step 1103, loss 0.249764, acc 0.90625, prec 0.0531016, recall 0.816286
2017-12-09T22:36:10.039408: step 1104, loss 0.293463, acc 0.90625, prec 0.0531096, recall 0.816359
2017-12-09T22:36:10.349545: step 1105, loss 0.34591, acc 0.898438, prec 0.0530919, recall 0.816359
2017-12-09T22:36:10.647160: step 1106, loss 0.302872, acc 0.921875, prec 0.053151, recall 0.816575
2017-12-09T22:36:10.944239: step 1107, loss 0.376965, acc 0.890625, prec 0.0531561, recall 0.816647
2017-12-09T22:36:11.239537: step 1108, loss 1.08768, acc 0.921875, prec 0.0531439, recall 0.816327
2017-12-09T22:36:11.547492: step 1109, loss 1.14808, acc 0.882812, prec 0.0531733, recall 0.816151
2017-12-09T22:36:11.855567: step 1110, loss 0.259499, acc 0.96875, prec 0.0533129, recall 0.816582
2017-12-09T22:36:12.152568: step 1111, loss 0.214139, acc 0.9375, prec 0.0533504, recall 0.816725
2017-12-09T22:36:12.450051: step 1112, loss 1.70459, acc 0.929688, prec 0.0533636, recall 0.816478
2017-12-09T22:36:12.754566: step 1113, loss 0.355629, acc 0.890625, prec 0.0533687, recall 0.81655
2017-12-09T22:36:13.050233: step 1114, loss 0.514564, acc 0.882812, prec 0.0533724, recall 0.816621
2017-12-09T22:36:13.350808: step 1115, loss 0.31937, acc 0.890625, prec 0.0533775, recall 0.816693
2017-12-09T22:36:13.651349: step 1116, loss 0.354466, acc 0.859375, prec 0.053353, recall 0.816693
2017-12-09T22:36:13.952430: step 1117, loss 0.44447, acc 0.859375, prec 0.0533768, recall 0.816836
2017-12-09T22:36:14.249444: step 1118, loss 1.14219, acc 0.828125, prec 0.0533483, recall 0.816517
2017-12-09T22:36:14.551560: step 1119, loss 0.703052, acc 0.875, prec 0.0533747, recall 0.81666
2017-12-09T22:36:14.850383: step 1120, loss 0.811414, acc 0.835938, prec 0.0533703, recall 0.816732
2017-12-09T22:36:15.154350: step 1121, loss 0.476725, acc 0.8125, prec 0.0533618, recall 0.816803
2017-12-09T22:36:15.455250: step 1122, loss 0.400066, acc 0.835938, prec 0.0533574, recall 0.816874
2017-12-09T22:36:15.759261: step 1123, loss 0.464107, acc 0.828125, prec 0.0533756, recall 0.817016
2017-12-09T22:36:16.059641: step 1124, loss 0.520722, acc 0.765625, prec 0.053359, recall 0.817087
2017-12-09T22:36:16.357890: step 1125, loss 0.633143, acc 0.765625, prec 0.0533185, recall 0.817087
2017-12-09T22:36:16.654622: step 1126, loss 0.367468, acc 0.835938, prec 0.0533381, recall 0.817229
2017-12-09T22:36:16.958516: step 1127, loss 0.662832, acc 0.84375, prec 0.053359, recall 0.817371
2017-12-09T22:36:17.266916: step 1128, loss 0.48487, acc 0.828125, prec 0.0533293, recall 0.817371
2017-12-09T22:36:17.569842: step 1129, loss 0.445143, acc 0.875, prec 0.0533316, recall 0.817442
2017-12-09T22:36:17.866835: step 1130, loss 0.317894, acc 0.890625, prec 0.0533606, recall 0.817583
2017-12-09T22:36:18.165587: step 1131, loss 0.228793, acc 0.914062, prec 0.0533697, recall 0.817654
2017-12-09T22:36:18.467256: step 1132, loss 0.250234, acc 0.945312, prec 0.053432, recall 0.817865
2017-12-09T22:36:18.763654: step 1133, loss 0.918192, acc 0.898438, prec 0.0534397, recall 0.81762
2017-12-09T22:36:19.065840: step 1134, loss 1.10489, acc 0.882812, prec 0.0534686, recall 0.817445
2017-12-09T22:36:19.368933: step 1135, loss 0.255227, acc 0.921875, prec 0.053479, recall 0.817515
2017-12-09T22:36:19.665855: step 1136, loss 0.295637, acc 0.914062, prec 0.0535358, recall 0.817726
2017-12-09T22:36:19.967793: step 1137, loss 0.71292, acc 0.953125, prec 0.0536471, recall 0.818077
2017-12-09T22:36:20.272289: step 1138, loss 0.539867, acc 0.859375, prec 0.0536705, recall 0.818217
2017-12-09T22:36:20.576122: step 1139, loss 1.17396, acc 0.875, prec 0.0536979, recall 0.818042
2017-12-09T22:36:20.878872: step 1140, loss 0.308955, acc 0.90625, prec 0.0537293, recall 0.818182
2017-12-09T22:36:21.175984: step 1141, loss 0.635104, acc 0.914062, prec 0.0537621, recall 0.818321
2017-12-09T22:36:21.475279: step 1142, loss 0.991352, acc 0.851562, prec 0.0537615, recall 0.818077
2017-12-09T22:36:21.776594: step 1143, loss 0.415161, acc 0.859375, prec 0.053761, recall 0.818147
2017-12-09T22:36:22.079670: step 1144, loss 0.308943, acc 0.882812, prec 0.0537645, recall 0.818217
2017-12-09T22:36:22.377592: step 1145, loss 0.41328, acc 0.859375, prec 0.0537402, recall 0.818217
2017-12-09T22:36:22.675784: step 1146, loss 1.09535, acc 0.867188, prec 0.0537661, recall 0.818043
2017-12-09T22:36:22.973030: step 1147, loss 0.428098, acc 0.84375, prec 0.0537629, recall 0.818112
2017-12-09T22:36:23.272886: step 1148, loss 0.279189, acc 0.882812, prec 0.0537664, recall 0.818182
2017-12-09T22:36:23.577989: step 1149, loss 0.562557, acc 0.78125, prec 0.0537286, recall 0.818182
2017-12-09T22:36:23.874918: step 1150, loss 0.909637, acc 0.835938, prec 0.0537478, recall 0.818321
2017-12-09T22:36:24.181042: step 1151, loss 0.393659, acc 0.859375, prec 0.0537473, recall 0.81839
2017-12-09T22:36:24.473786: step 1152, loss 1.16164, acc 0.796875, prec 0.0537834, recall 0.818598
2017-12-09T22:36:24.772282: step 1153, loss 1.09315, acc 0.789062, prec 0.0537721, recall 0.818355
2017-12-09T22:36:25.072037: step 1154, loss 0.647798, acc 0.882812, prec 0.0537992, recall 0.818493
2017-12-09T22:36:25.374376: step 1155, loss 0.551948, acc 0.835938, prec 0.053771, recall 0.818493
2017-12-09T22:36:25.673111: step 1156, loss 0.527913, acc 0.84375, prec 0.0537914, recall 0.818631
2017-12-09T22:36:25.968869: step 1157, loss 0.401275, acc 0.84375, prec 0.0538118, recall 0.818769
2017-12-09T22:36:26.265544: step 1158, loss 0.629393, acc 0.859375, prec 0.0538584, recall 0.818975
2017-12-09T22:36:26.568042: step 1159, loss 0.393049, acc 0.835938, prec 0.0538774, recall 0.819113
2017-12-09T22:36:26.866067: step 1160, loss 0.444732, acc 0.867188, prec 0.0539254, recall 0.819318
2017-12-09T22:36:27.163460: step 1161, loss 0.535323, acc 0.804688, prec 0.0539625, recall 0.819523
2017-12-09T22:36:27.458537: step 1162, loss 0.340214, acc 0.898438, prec 0.0539921, recall 0.81966
2017-12-09T22:36:27.757737: step 1163, loss 0.410431, acc 0.84375, prec 0.0539653, recall 0.81966
2017-12-09T22:36:28.054009: step 1164, loss 0.461635, acc 0.875, prec 0.0540615, recall 0.82
2017-12-09T22:36:28.359558: step 1165, loss 1.2461, acc 0.898438, prec 0.0540688, recall 0.819759
2017-12-09T22:36:28.655968: step 1166, loss 2.4273, acc 0.914062, prec 0.0540554, recall 0.81945
2017-12-09T22:36:28.955434: step 1167, loss 0.442667, acc 0.914062, prec 0.0540876, recall 0.819586
2017-12-09T22:36:29.255531: step 1168, loss 0.698299, acc 0.921875, prec 0.0541212, recall 0.819722
2017-12-09T22:36:29.554145: step 1169, loss 0.26916, acc 0.914062, prec 0.0541064, recall 0.819722
2017-12-09T22:36:29.855401: step 1170, loss 0.339819, acc 0.898438, prec 0.0541124, recall 0.819789
2017-12-09T22:36:30.161661: step 1171, loss 0.367321, acc 0.875, prec 0.0541144, recall 0.819857
2017-12-09T22:36:30.459842: step 1172, loss 0.273489, acc 0.882812, prec 0.0541178, recall 0.819925
2017-12-09T22:36:30.757819: step 1173, loss 0.287601, acc 0.90625, prec 0.0541017, recall 0.819925
2017-12-09T22:36:31.057780: step 1174, loss 0.406762, acc 0.96875, prec 0.0541197, recall 0.819992
2017-12-09T22:36:31.361078: step 1175, loss 0.143474, acc 0.9375, prec 0.054109, recall 0.819992
2017-12-09T22:36:31.658042: step 1176, loss 1.18429, acc 0.90625, prec 0.0541177, recall 0.819752
2017-12-09T22:36:31.963018: step 1177, loss 0.74503, acc 0.914062, prec 0.0541277, recall 0.819512
2017-12-09T22:36:32.263125: step 1178, loss 0.282386, acc 0.9375, prec 0.0541873, recall 0.819715
2017-12-09T22:36:32.559027: step 1179, loss 1.32405, acc 0.929688, prec 0.0542703, recall 0.819678
2017-12-09T22:36:32.865332: step 1180, loss 0.28354, acc 0.867188, prec 0.0542474, recall 0.819678
2017-12-09T22:36:33.164510: step 1181, loss 0.57596, acc 0.796875, prec 0.0542125, recall 0.819678
2017-12-09T22:36:33.461078: step 1182, loss 0.360371, acc 0.898438, prec 0.0542419, recall 0.819813
2017-12-09T22:36:33.754343: step 1183, loss 0.424399, acc 0.867188, prec 0.0542892, recall 0.820015
2017-12-09T22:36:34.054601: step 1184, loss 0.777676, acc 0.78125, prec 0.0543218, recall 0.820216
2017-12-09T22:36:34.348960: step 1185, loss 0.393731, acc 0.859375, prec 0.0542976, recall 0.820216
2017-12-09T22:36:34.645456: step 1186, loss 0.683276, acc 0.835938, prec 0.0543628, recall 0.820484
2017-12-09T22:36:34.946225: step 1187, loss 0.674618, acc 0.796875, prec 0.0543746, recall 0.820618
2017-12-09T22:36:35.258569: step 1188, loss 0.593721, acc 0.84375, prec 0.0543944, recall 0.820751
2017-12-09T22:36:35.563165: step 1189, loss 0.36698, acc 0.890625, prec 0.0544223, recall 0.820884
2017-12-09T22:36:35.861032: step 1190, loss 0.386811, acc 0.851562, prec 0.0543968, recall 0.820884
2017-12-09T22:36:36.158737: step 1191, loss 0.319527, acc 0.921875, prec 0.0545231, recall 0.821283
2017-12-09T22:36:36.457587: step 1192, loss 0.425258, acc 0.828125, prec 0.0545168, recall 0.821349
2017-12-09T22:36:36.753062: step 1193, loss 2.56034, acc 0.882812, prec 0.0545446, recall 0.821177
2017-12-09T22:36:37.053346: step 1194, loss 0.46745, acc 0.898438, prec 0.0545736, recall 0.82131
2017-12-09T22:36:37.351330: step 1195, loss 0.711603, acc 0.882812, prec 0.0546232, recall 0.821508
2017-12-09T22:36:37.655432: step 1196, loss 0.249327, acc 0.914062, prec 0.0546084, recall 0.821508
2017-12-09T22:36:37.952858: step 1197, loss 2.02513, acc 0.898438, prec 0.0546156, recall 0.82127
2017-12-09T22:36:38.254266: step 1198, loss 1.03407, acc 0.851562, prec 0.0545914, recall 0.820967
2017-12-09T22:36:38.552613: step 1199, loss 0.348892, acc 0.882812, prec 0.0545945, recall 0.821033
2017-12-09T22:36:38.847884: step 1200, loss 0.360995, acc 0.882812, prec 0.054644, recall 0.821231

Evaluation:
2017-12-09T22:36:43.535413: step 1200, loss 1.15238, acc 0.856496, prec 0.0553576, recall 0.819134

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1200

2017-12-09T22:36:44.775039: step 1201, loss 0.410832, acc 0.859375, prec 0.0553787, recall 0.81926
2017-12-09T22:36:45.079006: step 1202, loss 0.472848, acc 0.859375, prec 0.0553552, recall 0.81926
2017-12-09T22:36:45.376299: step 1203, loss 0.493781, acc 0.84375, prec 0.0554404, recall 0.819575
2017-12-09T22:36:45.675803: step 1204, loss 0.568909, acc 0.789062, prec 0.0554496, recall 0.819701
2017-12-09T22:36:45.970826: step 1205, loss 0.457344, acc 0.835938, prec 0.0554667, recall 0.819826
2017-12-09T22:36:46.271646: step 1206, loss 0.559088, acc 0.835938, prec 0.0554837, recall 0.819951
2017-12-09T22:36:46.569245: step 1207, loss 0.387675, acc 0.890625, prec 0.0555098, recall 0.820076
2017-12-09T22:36:46.868907: step 1208, loss 0.585765, acc 0.8125, prec 0.0555451, recall 0.820264
2017-12-09T22:36:47.167201: step 1209, loss 1.04289, acc 0.898438, prec 0.0555516, recall 0.820042
2017-12-09T22:36:47.472558: step 1210, loss 0.528592, acc 0.890625, prec 0.0555999, recall 0.820229
2017-12-09T22:36:47.771618: step 1211, loss 0.874876, acc 0.882812, prec 0.0556038, recall 0.820007
2017-12-09T22:36:48.069177: step 1212, loss 0.293288, acc 0.90625, prec 0.0555881, recall 0.820007
2017-12-09T22:36:48.366240: step 1213, loss 0.541512, acc 0.828125, prec 0.0555816, recall 0.820069
2017-12-09T22:36:48.666340: step 1214, loss 0.377266, acc 0.859375, prec 0.0556246, recall 0.820256
2017-12-09T22:36:48.962544: step 1215, loss 0.612247, acc 0.921875, prec 0.0556558, recall 0.82038
2017-12-09T22:36:49.263625: step 1216, loss 1.21182, acc 0.898438, prec 0.0556402, recall 0.820097
2017-12-09T22:36:49.570324: step 1217, loss 0.357884, acc 0.867188, prec 0.0556401, recall 0.820159
2017-12-09T22:36:49.866250: step 1218, loss 0.590856, acc 0.851562, prec 0.0556817, recall 0.820345
2017-12-09T22:36:50.167715: step 1219, loss 0.295161, acc 0.914062, prec 0.0557115, recall 0.820469
2017-12-09T22:36:50.486297: step 1220, loss 1.26569, acc 0.882812, prec 0.0556933, recall 0.820186
2017-12-09T22:36:50.783970: step 1221, loss 0.248362, acc 0.90625, prec 0.0556998, recall 0.820248
2017-12-09T22:36:51.086401: step 1222, loss 1.05799, acc 0.820312, prec 0.055714, recall 0.820372
2017-12-09T22:36:51.386789: step 1223, loss 0.46715, acc 0.84375, prec 0.0556879, recall 0.820372
2017-12-09T22:36:51.685320: step 1224, loss 0.465504, acc 0.875, prec 0.0556892, recall 0.820433
2017-12-09T22:36:51.980072: step 1225, loss 0.48554, acc 0.84375, prec 0.0556632, recall 0.820433
2017-12-09T22:36:52.279762: step 1226, loss 0.639222, acc 0.804688, prec 0.0556527, recall 0.820495
2017-12-09T22:36:52.581899: step 1227, loss 0.328609, acc 0.914062, prec 0.0556605, recall 0.820557
2017-12-09T22:36:52.882020: step 1228, loss 0.383336, acc 0.8125, prec 0.0556514, recall 0.820619
2017-12-09T22:36:53.179591: step 1229, loss 0.296334, acc 0.898438, prec 0.0556565, recall 0.82068
2017-12-09T22:36:53.476562: step 1230, loss 0.307173, acc 0.882812, prec 0.0556591, recall 0.820742
2017-12-09T22:36:53.774459: step 1231, loss 0.470522, acc 0.851562, prec 0.0557224, recall 0.820988
2017-12-09T22:36:54.073020: step 1232, loss 0.153025, acc 0.953125, prec 0.0557146, recall 0.820988
2017-12-09T22:36:54.372055: step 1233, loss 0.340099, acc 0.898438, prec 0.0557197, recall 0.821049
2017-12-09T22:36:54.667838: step 1234, loss 0.500315, acc 0.898438, prec 0.0557468, recall 0.821172
2017-12-09T22:36:54.975007: step 1235, loss 0.653257, acc 0.9375, prec 0.0557584, recall 0.821233
2017-12-09T22:36:55.273463: step 1236, loss 0.693022, acc 0.921875, prec 0.0558112, recall 0.821416
2017-12-09T22:36:55.579125: step 1237, loss 0.235784, acc 0.90625, prec 0.0558176, recall 0.821477
2017-12-09T22:36:55.877588: step 1238, loss 0.164164, acc 0.945312, prec 0.0558744, recall 0.82166
2017-12-09T22:36:56.176539: step 1239, loss 0.395733, acc 0.90625, prec 0.0558807, recall 0.821721
2017-12-09T22:36:56.483574: step 1240, loss 0.123879, acc 0.953125, prec 0.0558729, recall 0.821721
2017-12-09T22:36:56.783751: step 1241, loss 0.21534, acc 0.960938, prec 0.0559541, recall 0.821965
2017-12-09T22:36:57.081934: step 1242, loss 0.185702, acc 0.945312, prec 0.055945, recall 0.821965
2017-12-09T22:36:57.381656: step 1243, loss 0.272599, acc 0.945312, prec 0.0560017, recall 0.822147
2017-12-09T22:36:57.677347: step 1244, loss 0.331883, acc 0.914062, prec 0.0560312, recall 0.822268
2017-12-09T22:36:57.859521: step 1245, loss 10.1555, acc 0.921569, prec 0.0560492, recall 0.822048
2017-12-09T22:36:58.167026: step 1246, loss 0.431851, acc 0.898438, prec 0.0561417, recall 0.822351
2017-12-09T22:36:58.467529: step 1247, loss 0.427007, acc 0.914062, prec 0.0561931, recall 0.822531
2017-12-09T22:36:58.765653: step 1248, loss 0.296984, acc 0.890625, prec 0.0562404, recall 0.822712
2017-12-09T22:36:59.060277: step 1249, loss 0.478199, acc 0.78125, prec 0.0562695, recall 0.822892
2017-12-09T22:36:59.365769: step 1250, loss 0.538697, acc 0.78125, prec 0.0562768, recall 0.823012
2017-12-09T22:36:59.661994: step 1251, loss 0.81457, acc 0.773438, prec 0.0562827, recall 0.823132
2017-12-09T22:36:59.966404: step 1252, loss 0.9626, acc 0.726562, prec 0.0562808, recall 0.823251
2017-12-09T22:37:00.269280: step 1253, loss 0.979313, acc 0.695312, prec 0.0562954, recall 0.82343
2017-12-09T22:37:00.568000: step 1254, loss 0.847104, acc 0.710938, prec 0.0562909, recall 0.823549
2017-12-09T22:37:00.867916: step 1255, loss 0.65637, acc 0.78125, prec 0.0562981, recall 0.823668
2017-12-09T22:37:01.162564: step 1256, loss 0.810956, acc 0.757812, prec 0.0563448, recall 0.823906
2017-12-09T22:37:01.459327: step 1257, loss 0.463941, acc 0.820312, prec 0.056467, recall 0.82432
2017-12-09T22:37:01.759054: step 1258, loss 0.737366, acc 0.757812, prec 0.0564702, recall 0.824438
2017-12-09T22:37:02.071572: step 1259, loss 0.538609, acc 0.796875, prec 0.0564581, recall 0.824497
2017-12-09T22:37:02.364739: step 1260, loss 0.56804, acc 0.78125, prec 0.0564435, recall 0.824556
2017-12-09T22:37:02.671057: step 1261, loss 0.541256, acc 0.835938, prec 0.0564812, recall 0.824732
2017-12-09T22:37:02.968777: step 1262, loss 0.428621, acc 0.882812, prec 0.0565267, recall 0.824908
2017-12-09T22:37:03.269267: step 1263, loss 0.183359, acc 0.914062, prec 0.0565341, recall 0.824967
2017-12-09T22:37:03.567677: step 1264, loss 0.419238, acc 0.96875, prec 0.0565722, recall 0.825084
2017-12-09T22:37:03.868452: step 1265, loss 0.140459, acc 0.953125, prec 0.0565644, recall 0.825084
2017-12-09T22:37:04.167468: step 1266, loss 0.222868, acc 0.929688, prec 0.056596, recall 0.825201
2017-12-09T22:37:04.468227: step 1267, loss 0.136069, acc 0.953125, prec 0.0566098, recall 0.825259
2017-12-09T22:37:04.772114: step 1268, loss 0.209045, acc 0.945312, prec 0.056644, recall 0.825376
2017-12-09T22:37:05.073822: step 1269, loss 1.65356, acc 0.914062, prec 0.0566755, recall 0.824942
2017-12-09T22:37:05.382275: step 1270, loss 0.166791, acc 0.953125, prec 0.0567109, recall 0.825058
2017-12-09T22:37:05.683780: step 1271, loss 0.190662, acc 0.921875, prec 0.0567196, recall 0.825117
2017-12-09T22:37:05.985470: step 1272, loss 0.0830873, acc 0.96875, prec 0.0567144, recall 0.825117
2017-12-09T22:37:06.284304: step 1273, loss 0.0972385, acc 0.976562, prec 0.0567105, recall 0.825117
2017-12-09T22:37:06.584796: step 1274, loss 2.07339, acc 0.960938, prec 0.0567269, recall 0.8249
2017-12-09T22:37:06.889346: step 1275, loss 0.162322, acc 0.9375, prec 0.0568028, recall 0.825133
2017-12-09T22:37:07.187223: step 1276, loss 0.27825, acc 0.921875, prec 0.0568114, recall 0.825191
2017-12-09T22:37:07.487653: step 1277, loss 0.0982982, acc 0.96875, prec 0.0568062, recall 0.825191
2017-12-09T22:37:07.790337: step 1278, loss 0.198848, acc 0.9375, prec 0.056839, recall 0.825307
2017-12-09T22:37:08.095481: step 1279, loss 0.26911, acc 0.9375, prec 0.0568717, recall 0.825423
2017-12-09T22:37:08.396711: step 1280, loss 0.376168, acc 0.929688, prec 0.0569031, recall 0.825539
2017-12-09T22:37:08.696476: step 1281, loss 1.51872, acc 0.898438, prec 0.0569306, recall 0.825381
2017-12-09T22:37:08.993568: step 1282, loss 0.260768, acc 0.890625, prec 0.0569986, recall 0.825612
2017-12-09T22:37:09.294030: step 1283, loss 0.278981, acc 0.90625, prec 0.0570045, recall 0.82567
2017-12-09T22:37:09.594036: step 1284, loss 2.06328, acc 0.890625, prec 0.0570307, recall 0.825512
2017-12-09T22:37:09.896923: step 1285, loss 0.253238, acc 0.882812, prec 0.0570111, recall 0.825512
2017-12-09T22:37:10.195333: step 1286, loss 0.661289, acc 0.875, prec 0.0570334, recall 0.825628
2017-12-09T22:37:10.503406: step 1287, loss 0.64992, acc 0.867188, prec 0.0570758, recall 0.8258
2017-12-09T22:37:10.798257: step 1288, loss 0.394726, acc 0.875, prec 0.0570764, recall 0.825858
2017-12-09T22:37:11.099215: step 1289, loss 0.47588, acc 0.835938, prec 0.0570921, recall 0.825972
2017-12-09T22:37:11.406858: step 1290, loss 0.383595, acc 0.859375, prec 0.0571331, recall 0.826144
2017-12-09T22:37:11.708708: step 1291, loss 0.438701, acc 0.820312, prec 0.0571246, recall 0.826201
2017-12-09T22:37:12.000752: step 1292, loss 0.617066, acc 0.765625, prec 0.0571071, recall 0.826259
2017-12-09T22:37:12.297305: step 1293, loss 0.697049, acc 0.78125, prec 0.0570708, recall 0.826259
2017-12-09T22:37:12.592745: step 1294, loss 0.505094, acc 0.796875, prec 0.0570799, recall 0.826373
2017-12-09T22:37:12.886981: step 1295, loss 0.266445, acc 0.898438, prec 0.0570845, recall 0.82643
2017-12-09T22:37:13.182903: step 1296, loss 0.38228, acc 0.875, prec 0.0570637, recall 0.82643
2017-12-09T22:37:13.481042: step 1297, loss 0.357833, acc 0.875, prec 0.0570644, recall 0.826487
2017-12-09T22:37:13.781745: step 1298, loss 0.305677, acc 0.90625, prec 0.0570917, recall 0.826601
2017-12-09T22:37:14.080899: step 1299, loss 0.206996, acc 0.9375, prec 0.0571241, recall 0.826715
2017-12-09T22:37:14.380967: step 1300, loss 0.287051, acc 0.90625, prec 0.0571513, recall 0.826828
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1300

2017-12-09T22:37:15.655971: step 1301, loss 0.180903, acc 0.945312, prec 0.0571636, recall 0.826885
2017-12-09T22:37:15.956262: step 1302, loss 0.501096, acc 0.914062, prec 0.0572134, recall 0.827055
2017-12-09T22:37:16.260874: step 1303, loss 0.714106, acc 0.9375, prec 0.0572458, recall 0.827169
2017-12-09T22:37:16.562398: step 1304, loss 0.212485, acc 0.960938, prec 0.0573033, recall 0.827338
2017-12-09T22:37:16.864782: step 1305, loss 0.187303, acc 0.921875, prec 0.0573331, recall 0.827451
2017-12-09T22:37:17.163742: step 1306, loss 1.75573, acc 0.898438, prec 0.0573815, recall 0.82735
2017-12-09T22:37:17.462832: step 1307, loss 0.123156, acc 0.96875, prec 0.057419, recall 0.827462
2017-12-09T22:37:17.766787: step 1308, loss 0.172826, acc 0.9375, prec 0.0574086, recall 0.827462
2017-12-09T22:37:18.064631: step 1309, loss 0.26604, acc 0.90625, prec 0.0574143, recall 0.827519
2017-12-09T22:37:18.362997: step 1310, loss 0.345406, acc 0.929688, prec 0.0574453, recall 0.827631
2017-12-09T22:37:18.664684: step 1311, loss 0.213927, acc 0.953125, prec 0.0574801, recall 0.827743
2017-12-09T22:37:18.960704: step 1312, loss 0.215261, acc 0.960938, prec 0.0575162, recall 0.827856
2017-12-09T22:37:19.258536: step 1313, loss 0.287551, acc 0.953125, prec 0.0575723, recall 0.828023
2017-12-09T22:37:19.556631: step 1314, loss 0.213668, acc 0.90625, prec 0.0575567, recall 0.828023
2017-12-09T22:37:19.854986: step 1315, loss 1.09494, acc 0.875, prec 0.0575372, recall 0.827754
2017-12-09T22:37:20.157610: step 1316, loss 0.217347, acc 0.921875, prec 0.0575668, recall 0.827866
2017-12-09T22:37:20.466895: step 1317, loss 0.29381, acc 0.882812, prec 0.0575899, recall 0.827978
2017-12-09T22:37:20.770276: step 1318, loss 0.181027, acc 0.90625, prec 0.0575955, recall 0.828034
2017-12-09T22:37:21.068728: step 1319, loss 0.254813, acc 0.914062, prec 0.0576238, recall 0.828145
2017-12-09T22:37:21.366432: step 1320, loss 0.815356, acc 0.90625, prec 0.0576095, recall 0.827877
2017-12-09T22:37:21.667553: step 1321, loss 0.377401, acc 0.921875, prec 0.0576602, recall 0.828044
2017-12-09T22:37:21.971473: step 1322, loss 0.209069, acc 0.914062, prec 0.0577096, recall 0.828211
2017-12-09T22:37:22.270991: step 1323, loss 0.654274, acc 0.898438, prec 0.0577564, recall 0.828377
2017-12-09T22:37:22.571984: step 1324, loss 0.243097, acc 0.867188, prec 0.0578192, recall 0.828599
2017-12-09T22:37:22.874588: step 1325, loss 0.315755, acc 0.882812, prec 0.0578421, recall 0.82871
2017-12-09T22:37:23.174972: step 1326, loss 0.55201, acc 0.90625, prec 0.0578901, recall 0.828875
2017-12-09T22:37:23.480820: step 1327, loss 0.627164, acc 0.890625, prec 0.0579143, recall 0.828986
2017-12-09T22:37:23.780083: step 1328, loss 0.364246, acc 0.890625, prec 0.0579172, recall 0.829041
2017-12-09T22:37:24.081352: step 1329, loss 0.324768, acc 0.882812, prec 0.0579401, recall 0.829151
2017-12-09T22:37:24.377520: step 1330, loss 0.540627, acc 0.8125, prec 0.0579511, recall 0.82926
2017-12-09T22:37:24.676386: step 1331, loss 0.240021, acc 0.929688, prec 0.0579394, recall 0.82926
2017-12-09T22:37:24.970880: step 1332, loss 0.301116, acc 0.867188, prec 0.0579808, recall 0.829425
2017-12-09T22:37:25.266949: step 1333, loss 0.273537, acc 0.867188, prec 0.0580009, recall 0.829535
2017-12-09T22:37:25.566607: step 1334, loss 0.303305, acc 0.851562, prec 0.0580185, recall 0.829644
2017-12-09T22:37:25.860948: step 1335, loss 0.294554, acc 0.914062, prec 0.0580253, recall 0.829699
2017-12-09T22:37:26.158023: step 1336, loss 0.235912, acc 0.921875, prec 0.0580545, recall 0.829808
2017-12-09T22:37:26.456525: step 1337, loss 0.298263, acc 0.929688, prec 0.0580639, recall 0.829862
2017-12-09T22:37:26.761379: step 1338, loss 0.462788, acc 0.960938, prec 0.0581208, recall 0.830026
2017-12-09T22:37:27.062012: step 1339, loss 0.231613, acc 0.9375, prec 0.0581526, recall 0.830134
2017-12-09T22:37:27.363403: step 1340, loss 0.246467, acc 0.9375, prec 0.0581843, recall 0.830243
2017-12-09T22:37:27.668202: step 1341, loss 0.206812, acc 0.96875, prec 0.0582002, recall 0.830297
2017-12-09T22:37:27.970022: step 1342, loss 0.218741, acc 0.960938, prec 0.0582359, recall 0.830406
2017-12-09T22:37:28.272466: step 1343, loss 0.176497, acc 0.953125, prec 0.0582281, recall 0.830406
2017-12-09T22:37:28.572453: step 1344, loss 0.269608, acc 0.90625, prec 0.0582335, recall 0.83046
2017-12-09T22:37:28.877879: step 1345, loss 0.146932, acc 0.976562, prec 0.0582929, recall 0.830622
2017-12-09T22:37:29.173507: step 1346, loss 0.108938, acc 0.96875, prec 0.0583087, recall 0.830676
2017-12-09T22:37:29.472684: step 1347, loss 1.20727, acc 0.945312, prec 0.0583009, recall 0.830411
2017-12-09T22:37:29.771919: step 1348, loss 0.23046, acc 0.960938, prec 0.0583576, recall 0.830573
2017-12-09T22:37:30.086379: step 1349, loss 0.597012, acc 0.984375, prec 0.0584603, recall 0.830843
2017-12-09T22:37:30.397590: step 1350, loss 0.831147, acc 0.945312, prec 0.0584933, recall 0.83095
2017-12-09T22:37:30.707891: step 1351, loss 0.0611281, acc 0.992188, prec 0.058513, recall 0.831004
2017-12-09T22:37:31.006093: step 1352, loss 0.188138, acc 0.929688, prec 0.0585012, recall 0.831004
2017-12-09T22:37:31.300748: step 1353, loss 0.195727, acc 0.945312, prec 0.0585131, recall 0.831057
2017-12-09T22:37:31.599067: step 1354, loss 0.267701, acc 0.929688, prec 0.0585435, recall 0.831165
2017-12-09T22:37:31.895897: step 1355, loss 0.514563, acc 0.921875, prec 0.0586145, recall 0.831379
2017-12-09T22:37:32.198385: step 1356, loss 0.385279, acc 0.9375, prec 0.0586461, recall 0.831486
2017-12-09T22:37:32.499303: step 1357, loss 0.32674, acc 0.914062, prec 0.0587368, recall 0.831752
2017-12-09T22:37:32.799204: step 1358, loss 0.322896, acc 0.914062, prec 0.0587434, recall 0.831805
2017-12-09T22:37:33.101919: step 1359, loss 0.221407, acc 0.921875, prec 0.0587303, recall 0.831805
2017-12-09T22:37:33.398230: step 1360, loss 0.259656, acc 0.929688, prec 0.0587605, recall 0.831912
2017-12-09T22:37:33.701820: step 1361, loss 0.212414, acc 0.914062, prec 0.0587881, recall 0.832018
2017-12-09T22:37:34.001739: step 1362, loss 0.305727, acc 0.953125, prec 0.0588432, recall 0.832177
2017-12-09T22:37:34.304063: step 1363, loss 0.141464, acc 0.929688, prec 0.0588734, recall 0.832282
2017-12-09T22:37:34.606144: step 1364, loss 0.270884, acc 0.960938, prec 0.0589298, recall 0.832441
2017-12-09T22:37:34.906908: step 1365, loss 0.313026, acc 0.96875, prec 0.0589665, recall 0.832546
2017-12-09T22:37:35.212380: step 1366, loss 0.480831, acc 0.921875, prec 0.0590372, recall 0.832757
2017-12-09T22:37:35.524864: step 1367, loss 0.0699153, acc 0.96875, prec 0.0590529, recall 0.83281
2017-12-09T22:37:35.830613: step 1368, loss 0.298049, acc 0.90625, prec 0.0590581, recall 0.832862
2017-12-09T22:37:36.133720: step 1369, loss 0.211318, acc 0.914062, prec 0.0590646, recall 0.832915
2017-12-09T22:37:36.434705: step 1370, loss 0.243978, acc 0.898438, prec 0.0590894, recall 0.833019
2017-12-09T22:37:36.737463: step 1371, loss 0.17309, acc 0.9375, prec 0.0590789, recall 0.833019
2017-12-09T22:37:37.031218: step 1372, loss 0.222093, acc 0.945312, prec 0.0591534, recall 0.833229
2017-12-09T22:37:37.332060: step 1373, loss 0.435611, acc 0.90625, prec 0.0592423, recall 0.83349
2017-12-09T22:37:37.632351: step 1374, loss 0.204085, acc 0.929688, prec 0.0592513, recall 0.833542
2017-12-09T22:37:37.927818: step 1375, loss 0.160358, acc 0.914062, prec 0.0592369, recall 0.833542
2017-12-09T22:37:38.228084: step 1376, loss 0.196689, acc 0.9375, prec 0.0592891, recall 0.833698
2017-12-09T22:37:38.530625: step 1377, loss 0.159296, acc 0.953125, prec 0.059323, recall 0.833802
2017-12-09T22:37:38.830788: step 1378, loss 0.212728, acc 0.953125, prec 0.0593987, recall 0.834009
2017-12-09T22:37:39.126573: step 1379, loss 0.138778, acc 0.953125, prec 0.0594326, recall 0.834113
2017-12-09T22:37:39.426542: step 1380, loss 0.436568, acc 0.96875, prec 0.0594691, recall 0.834216
2017-12-09T22:37:39.729196: step 1381, loss 0.130096, acc 0.929688, prec 0.0594781, recall 0.834268
2017-12-09T22:37:40.033382: step 1382, loss 2.05757, acc 0.914062, prec 0.0595066, recall 0.834111
2017-12-09T22:37:40.338800: step 1383, loss 0.390945, acc 0.960938, prec 0.0595627, recall 0.834266
2017-12-09T22:37:40.640746: step 1384, loss 0.147042, acc 0.945312, prec 0.0595952, recall 0.834369
2017-12-09T22:37:40.933705: step 1385, loss 0.173165, acc 0.9375, prec 0.0595846, recall 0.834369
2017-12-09T22:37:41.228383: step 1386, loss 1.56796, acc 0.921875, prec 0.0596353, recall 0.834264
2017-12-09T22:37:41.537844: step 1387, loss 0.356143, acc 0.875, prec 0.0596767, recall 0.834419
2017-12-09T22:37:41.839462: step 1388, loss 0.299146, acc 0.90625, prec 0.0597025, recall 0.834521
2017-12-09T22:37:42.137178: step 1389, loss 0.252865, acc 0.945312, prec 0.0597766, recall 0.834726
2017-12-09T22:37:42.434217: step 1390, loss 0.303025, acc 0.898438, prec 0.0597802, recall 0.834777
2017-12-09T22:37:42.727654: step 1391, loss 0.34155, acc 0.90625, prec 0.0597851, recall 0.834828
2017-12-09T22:37:43.024133: step 1392, loss 0.489957, acc 0.882812, prec 0.0598486, recall 0.835032
2017-12-09T22:37:43.320220: step 1393, loss 0.209717, acc 0.898438, prec 0.059873, recall 0.835134
2017-12-09T22:37:43.618071: step 1394, loss 0.480675, acc 0.828125, prec 0.0598438, recall 0.835134
2017-12-09T22:37:43.915151: step 1395, loss 0.325303, acc 0.898438, prec 0.0598266, recall 0.835134
2017-12-09T22:37:44.213377: step 1396, loss 0.350437, acc 0.890625, prec 0.0598497, recall 0.835236
2017-12-09T22:37:44.515721: step 1397, loss 0.48838, acc 0.882812, prec 0.0598714, recall 0.835338
2017-12-09T22:37:44.813427: step 1398, loss 0.284877, acc 0.921875, prec 0.0598789, recall 0.835388
2017-12-09T22:37:45.111386: step 1399, loss 0.205308, acc 0.882812, prec 0.0599214, recall 0.83554
2017-12-09T22:37:45.416444: step 1400, loss 0.240498, acc 0.9375, prec 0.0599523, recall 0.835642
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1400

2017-12-09T22:37:46.785386: step 1401, loss 0.879268, acc 0.882812, prec 0.0600155, recall 0.835844
2017-12-09T22:37:47.085950: step 1402, loss 0.737654, acc 0.945312, prec 0.060049, recall 0.835688
2017-12-09T22:37:47.384022: step 1403, loss 0.352871, acc 0.882812, prec 0.0600706, recall 0.835789
2017-12-09T22:37:47.686786: step 1404, loss 0.281389, acc 0.890625, prec 0.0600935, recall 0.83589
2017-12-09T22:37:47.994547: step 1405, loss 0.437669, acc 0.875, prec 0.060093, recall 0.83594
2017-12-09T22:37:48.293783: step 1406, loss 0.159703, acc 0.9375, prec 0.0600824, recall 0.83594
2017-12-09T22:37:48.596855: step 1407, loss 0.119785, acc 0.960938, prec 0.0601379, recall 0.836091
2017-12-09T22:37:48.892019: step 1408, loss 0.544802, acc 0.960938, prec 0.060152, recall 0.836141
2017-12-09T22:37:49.190467: step 1409, loss 0.537216, acc 0.945312, prec 0.0601635, recall 0.836191
2017-12-09T22:37:49.496401: step 1410, loss 0.226957, acc 0.898438, prec 0.0601669, recall 0.836241
2017-12-09T22:37:49.808224: step 1411, loss 0.166926, acc 0.9375, prec 0.0602391, recall 0.836441
2017-12-09T22:37:50.113484: step 1412, loss 0.324009, acc 0.890625, prec 0.0602412, recall 0.836491
2017-12-09T22:37:50.424945: step 1413, loss 0.769145, acc 0.921875, prec 0.0603107, recall 0.836691
2017-12-09T22:37:50.730542: step 1414, loss 0.303653, acc 0.882812, prec 0.0602908, recall 0.836691
2017-12-09T22:37:51.026585: step 1415, loss 0.729766, acc 0.898438, prec 0.0603149, recall 0.836791
2017-12-09T22:37:51.323445: step 1416, loss 0.477584, acc 0.953125, prec 0.0603689, recall 0.83694
2017-12-09T22:37:51.625025: step 1417, loss 0.18596, acc 0.945312, prec 0.0604009, recall 0.837039
2017-12-09T22:37:51.921382: step 1418, loss 0.324614, acc 0.921875, prec 0.0604289, recall 0.837139
2017-12-09T22:37:52.234464: step 1419, loss 0.28333, acc 0.914062, prec 0.0604556, recall 0.837238
2017-12-09T22:37:52.532160: step 1420, loss 0.32073, acc 0.875, prec 0.0604344, recall 0.837238
2017-12-09T22:37:52.834576: step 1421, loss 0.428878, acc 0.890625, prec 0.0604571, recall 0.837337
2017-12-09T22:37:53.130910: step 1422, loss 0.244075, acc 0.90625, prec 0.060503, recall 0.837485
2017-12-09T22:37:53.425607: step 1423, loss 0.374689, acc 0.90625, prec 0.0605901, recall 0.837731
2017-12-09T22:37:53.727131: step 1424, loss 0.242401, acc 0.929688, prec 0.0605781, recall 0.837731
2017-12-09T22:37:54.028229: step 1425, loss 0.230853, acc 0.929688, prec 0.0605868, recall 0.83778
2017-12-09T22:37:54.326676: step 1426, loss 0.231328, acc 0.921875, prec 0.0606559, recall 0.837977
2017-12-09T22:37:54.623524: step 1427, loss 0.290551, acc 0.921875, prec 0.0606632, recall 0.838026
2017-12-09T22:37:54.927548: step 1428, loss 0.268093, acc 0.898438, prec 0.0607282, recall 0.838222
2017-12-09T22:37:55.230218: step 1429, loss 0.310587, acc 0.945312, prec 0.0607806, recall 0.838369
2017-12-09T22:37:55.531525: step 1430, loss 0.637292, acc 0.929688, prec 0.0608303, recall 0.838515
2017-12-09T22:37:55.833114: step 1431, loss 0.842882, acc 0.9375, prec 0.0608416, recall 0.838311
2017-12-09T22:37:56.134093: step 1432, loss 0.157547, acc 0.929688, prec 0.0608296, recall 0.838311
2017-12-09T22:37:56.433166: step 1433, loss 0.194372, acc 0.9375, prec 0.0608189, recall 0.838311
2017-12-09T22:37:56.735670: step 1434, loss 0.181294, acc 0.945312, prec 0.0608096, recall 0.838311
2017-12-09T22:37:57.033011: step 1435, loss 0.120672, acc 0.945312, prec 0.0608003, recall 0.838311
2017-12-09T22:37:57.334083: step 1436, loss 0.167819, acc 0.929688, prec 0.0607883, recall 0.838311
2017-12-09T22:37:57.636696: step 1437, loss 0.109869, acc 0.960938, prec 0.0608022, recall 0.838359
2017-12-09T22:37:57.932708: step 1438, loss 0.12483, acc 0.960938, prec 0.0607956, recall 0.838359
2017-12-09T22:37:58.233839: step 1439, loss 0.190018, acc 0.9375, prec 0.0608055, recall 0.838408
2017-12-09T22:37:58.534597: step 1440, loss 0.382924, acc 0.96875, prec 0.0608412, recall 0.838506
2017-12-09T22:37:58.833295: step 1441, loss 0.114795, acc 0.960938, prec 0.0608551, recall 0.838554
2017-12-09T22:37:59.132695: step 1442, loss 0.432671, acc 0.945312, prec 0.0609484, recall 0.838797
2017-12-09T22:37:59.438827: step 1443, loss 0.172032, acc 0.960938, prec 0.0609828, recall 0.838894
2017-12-09T22:37:59.738886: step 1444, loss 0.142634, acc 0.9375, prec 0.0609721, recall 0.838894
2017-12-09T22:38:00.042706: step 1445, loss 0.225161, acc 0.921875, prec 0.0609588, recall 0.838894
2017-12-09T22:38:00.347231: step 1446, loss 0.266438, acc 0.945312, prec 0.06097, recall 0.838942
2017-12-09T22:38:00.651086: step 1447, loss 0.470565, acc 0.960938, prec 0.0610044, recall 0.839039
2017-12-09T22:38:00.949804: step 1448, loss 0.200269, acc 0.96875, prec 0.06104, recall 0.839136
2017-12-09T22:38:01.249463: step 1449, loss 0.394618, acc 0.984375, prec 0.0610579, recall 0.839184
2017-12-09T22:38:01.546973: step 1450, loss 0.0755963, acc 0.96875, prec 0.0610525, recall 0.839184
2017-12-09T22:38:01.841155: step 1451, loss 0.174067, acc 0.9375, prec 0.0610419, recall 0.839184
2017-12-09T22:38:02.142086: step 1452, loss 0.907703, acc 0.921875, prec 0.0610914, recall 0.839077
2017-12-09T22:38:02.447254: step 1453, loss 0.140447, acc 0.929688, prec 0.0610998, recall 0.839125
2017-12-09T22:38:02.745734: step 1454, loss 0.116812, acc 0.9375, prec 0.0610892, recall 0.839125
2017-12-09T22:38:03.049561: step 1455, loss 0.161517, acc 0.945312, prec 0.0611208, recall 0.839222
2017-12-09T22:38:03.345781: step 1456, loss 0.784973, acc 0.96875, prec 0.0611577, recall 0.839067
2017-12-09T22:38:03.644979: step 1457, loss 0.332797, acc 0.875, prec 0.0611569, recall 0.839115
2017-12-09T22:38:03.943196: step 1458, loss 0.262503, acc 0.914062, prec 0.0611422, recall 0.839115
2017-12-09T22:38:04.235959: step 1459, loss 0.556344, acc 0.945312, prec 0.0611738, recall 0.839211
2017-12-09T22:38:04.533964: step 1460, loss 0.36821, acc 0.90625, prec 0.0611987, recall 0.839307
2017-12-09T22:38:04.832582: step 1461, loss 0.264982, acc 0.921875, prec 0.0612467, recall 0.839451
2017-12-09T22:38:05.134191: step 1462, loss 0.303381, acc 0.914062, prec 0.0612933, recall 0.839595
2017-12-09T22:38:05.441504: step 1463, loss 0.306661, acc 0.890625, prec 0.0612951, recall 0.839642
2017-12-09T22:38:05.750573: step 1464, loss 0.918917, acc 0.898438, prec 0.061339, recall 0.839786
2017-12-09T22:38:06.052073: step 1465, loss 0.504098, acc 0.859375, prec 0.0613762, recall 0.839929
2017-12-09T22:38:06.348513: step 1466, loss 0.42281, acc 0.953125, prec 0.0614294, recall 0.840071
2017-12-09T22:38:06.650694: step 1467, loss 0.369788, acc 0.890625, prec 0.0614515, recall 0.840166
2017-12-09T22:38:06.948631: step 1468, loss 0.292756, acc 0.890625, prec 0.0614532, recall 0.840214
2017-12-09T22:38:07.255066: step 1469, loss 0.2348, acc 0.914062, prec 0.0614589, recall 0.840261
2017-12-09T22:38:08.255224: step 1470, loss 0.548337, acc 0.851562, prec 0.0614947, recall 0.840403
2017-12-09T22:38:08.887704: step 1471, loss 0.3059, acc 0.890625, prec 0.0614964, recall 0.840451
2017-12-09T22:38:09.441307: step 1472, loss 0.394899, acc 0.890625, prec 0.0615184, recall 0.840545
2017-12-09T22:38:09.770840: step 1473, loss 0.302683, acc 0.898438, prec 0.0615214, recall 0.840593
2017-12-09T22:38:10.108676: step 1474, loss 0.304568, acc 0.859375, prec 0.0615178, recall 0.84064
2017-12-09T22:38:10.444158: step 1475, loss 0.188298, acc 0.929688, prec 0.0615058, recall 0.84064
2017-12-09T22:38:10.759931: step 1476, loss 0.412007, acc 0.9375, prec 0.0615358, recall 0.840734
2017-12-09T22:38:11.084728: step 1477, loss 0.150703, acc 0.929688, prec 0.0615441, recall 0.840781
2017-12-09T22:38:11.395810: step 1478, loss 0.247546, acc 0.929688, prec 0.0615525, recall 0.840828
2017-12-09T22:38:11.705114: step 1479, loss 0.787257, acc 0.90625, prec 0.0615771, recall 0.840923
2017-12-09T22:38:12.013349: step 1480, loss 0.274876, acc 0.921875, prec 0.0615841, recall 0.84097
2017-12-09T22:38:12.324639: step 1481, loss 1.14373, acc 0.9375, prec 0.0616154, recall 0.840815
2017-12-09T22:38:12.643310: step 1482, loss 0.21175, acc 0.929688, prec 0.0616237, recall 0.840862
2017-12-09T22:38:12.947659: step 1483, loss 0.145238, acc 0.953125, prec 0.0616157, recall 0.840862
2017-12-09T22:38:13.250683: step 1484, loss 0.249125, acc 0.953125, prec 0.0616889, recall 0.84105
2017-12-09T22:38:13.557092: step 1485, loss 0.720767, acc 0.9375, prec 0.0616985, recall 0.841097
2017-12-09T22:38:13.861561: step 1486, loss 0.276758, acc 0.882812, prec 0.0617393, recall 0.841237
2017-12-09T22:38:14.157857: step 1487, loss 0.265236, acc 0.921875, prec 0.0617463, recall 0.841284
2017-12-09T22:38:14.456776: step 1488, loss 0.285478, acc 0.898438, prec 0.0617492, recall 0.841331
2017-12-09T22:38:14.756157: step 1489, loss 0.249764, acc 0.914062, prec 0.0617548, recall 0.841377
2017-12-09T22:38:15.059067: step 1490, loss 0.362184, acc 0.867188, prec 0.0617726, recall 0.841471
2017-12-09T22:38:15.364302: step 1491, loss 0.680546, acc 0.90625, prec 0.0618376, recall 0.841657
2017-12-09T22:38:15.666837: step 1492, loss 0.295246, acc 0.914062, prec 0.061823, recall 0.841657
2017-12-09T22:38:15.970706: step 1493, loss 0.315842, acc 0.929688, prec 0.0618514, recall 0.84175
2017-12-09T22:38:16.149077: step 1494, loss 0.120973, acc 0.921569, prec 0.0618461, recall 0.84175
2017-12-09T22:38:16.451308: step 1495, loss 0.174644, acc 0.945312, prec 0.061857, recall 0.841796
2017-12-09T22:38:16.750544: step 1496, loss 0.214613, acc 0.921875, prec 0.0618841, recall 0.841889
2017-12-09T22:38:17.049919: step 1497, loss 0.470116, acc 0.898438, prec 0.0619477, recall 0.842074
2017-12-09T22:38:17.345193: step 1498, loss 0.166212, acc 0.945312, prec 0.0619787, recall 0.842167
2017-12-09T22:38:17.641770: step 1499, loss 0.293841, acc 0.875, prec 0.0619776, recall 0.842213
2017-12-09T22:38:17.942318: step 1500, loss 0.319702, acc 0.921875, prec 0.0620047, recall 0.842305

Evaluation:
2017-12-09T22:38:22.669617: step 1500, loss 1.9316, acc 0.942353, prec 0.062925, recall 0.829644

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1500

2017-12-09T22:38:23.970794: step 1501, loss 0.319575, acc 0.890625, prec 0.0629063, recall 0.829644
2017-12-09T22:38:24.273756: step 1502, loss 0.23269, acc 0.914062, prec 0.0629513, recall 0.829787
2017-12-09T22:38:24.573247: step 1503, loss 0.144127, acc 0.953125, prec 0.0629632, recall 0.829835
2017-12-09T22:38:24.874572: step 1504, loss 0.625953, acc 0.945312, prec 0.0630135, recall 0.829978
2017-12-09T22:38:25.174623: step 1505, loss 0.121434, acc 0.945312, prec 0.0630241, recall 0.830025
2017-12-09T22:38:25.475776: step 1506, loss 0.204921, acc 0.945312, prec 0.0630346, recall 0.830073
2017-12-09T22:38:25.772195: step 1507, loss 0.109585, acc 0.984375, prec 0.0630717, recall 0.830168
2017-12-09T22:38:26.073716: step 1508, loss 0.718708, acc 0.945312, prec 0.0631034, recall 0.830031
2017-12-09T22:38:26.375231: step 1509, loss 0.224488, acc 0.929688, prec 0.0630913, recall 0.830031
2017-12-09T22:38:26.674827: step 1510, loss 0.134261, acc 0.96875, prec 0.0631456, recall 0.830173
2017-12-09T22:38:26.974947: step 1511, loss 0.0940076, acc 0.960938, prec 0.0631588, recall 0.83022
2017-12-09T22:38:27.273782: step 1512, loss 0.0844191, acc 0.976562, prec 0.0631548, recall 0.83022
2017-12-09T22:38:27.570970: step 1513, loss 0.184163, acc 0.921875, prec 0.0631612, recall 0.830268
2017-12-09T22:38:27.870133: step 1514, loss 0.219189, acc 0.929688, prec 0.0631492, recall 0.830268
2017-12-09T22:38:28.172473: step 1515, loss 0.0997452, acc 0.96875, prec 0.0631836, recall 0.830362
2017-12-09T22:38:28.476450: step 1516, loss 0.139921, acc 0.945312, prec 0.0631742, recall 0.830362
2017-12-09T22:38:28.777869: step 1517, loss 0.111456, acc 0.96875, prec 0.0632085, recall 0.830457
2017-12-09T22:38:29.077224: step 1518, loss 0.0976688, acc 0.960938, prec 0.0632018, recall 0.830457
2017-12-09T22:38:29.376341: step 1519, loss 0.578875, acc 0.9375, prec 0.0632507, recall 0.830598
2017-12-09T22:38:29.677548: step 1520, loss 0.410564, acc 0.984375, prec 0.0633075, recall 0.830739
2017-12-09T22:38:29.983699: step 1521, loss 0.339405, acc 0.953125, prec 0.063359, recall 0.83088
2017-12-09T22:38:30.287672: step 1522, loss 0.157855, acc 0.921875, prec 0.0633455, recall 0.83088
2017-12-09T22:38:30.588015: step 1523, loss 0.922999, acc 0.921875, prec 0.0633335, recall 0.83065
2017-12-09T22:38:30.890082: step 1524, loss 0.235806, acc 0.960938, prec 0.0633466, recall 0.830697
2017-12-09T22:38:31.189257: step 1525, loss 0.263686, acc 0.921875, prec 0.0633728, recall 0.830791
2017-12-09T22:38:31.489718: step 1526, loss 0.123285, acc 0.953125, prec 0.0633648, recall 0.830791
2017-12-09T22:38:31.789893: step 1527, loss 0.371614, acc 0.9375, prec 0.0633937, recall 0.830884
2017-12-09T22:38:32.087321: step 1528, loss 0.239804, acc 0.929688, prec 0.0634807, recall 0.831118
2017-12-09T22:38:32.384481: step 1529, loss 0.199021, acc 0.953125, prec 0.0635122, recall 0.831212
2017-12-09T22:38:32.686203: step 1530, loss 0.353737, acc 0.867188, prec 0.063529, recall 0.831305
2017-12-09T22:38:32.983982: step 1531, loss 0.432211, acc 0.9375, prec 0.0635578, recall 0.831399
2017-12-09T22:38:33.282752: step 1532, loss 0.139906, acc 0.96875, prec 0.0636118, recall 0.831538
2017-12-09T22:38:33.584621: step 1533, loss 0.264337, acc 0.921875, prec 0.063717, recall 0.831817
2017-12-09T22:38:33.884719: step 1534, loss 0.61845, acc 0.9375, prec 0.0638051, recall 0.832048
2017-12-09T22:38:34.188718: step 1535, loss 0.371365, acc 0.929688, prec 0.0638127, recall 0.832095
2017-12-09T22:38:34.487776: step 1536, loss 0.24131, acc 0.90625, prec 0.0638361, recall 0.832187
2017-12-09T22:38:34.789547: step 1537, loss 0.322442, acc 0.945312, prec 0.0639056, recall 0.832372
2017-12-09T22:38:35.087412: step 1538, loss 0.264428, acc 0.890625, prec 0.0639658, recall 0.832556
2017-12-09T22:38:35.397145: step 1539, loss 0.170522, acc 0.945312, prec 0.063976, recall 0.832602
2017-12-09T22:38:35.699469: step 1540, loss 0.27201, acc 0.921875, prec 0.0639823, recall 0.832647
2017-12-09T22:38:35.997578: step 1541, loss 0.377616, acc 0.875, prec 0.0640002, recall 0.832739
2017-12-09T22:38:36.298444: step 1542, loss 0.839453, acc 0.9375, prec 0.064088, recall 0.832968
2017-12-09T22:38:36.600338: step 1543, loss 0.25413, acc 0.921875, prec 0.0641139, recall 0.83306
2017-12-09T22:38:36.903616: step 1544, loss 0.257556, acc 0.914062, prec 0.0641385, recall 0.833151
2017-12-09T22:38:37.207066: step 1545, loss 0.290931, acc 0.921875, prec 0.0641644, recall 0.833242
2017-12-09T22:38:37.508414: step 1546, loss 0.2928, acc 0.921875, prec 0.0641903, recall 0.833333
2017-12-09T22:38:37.807320: step 1547, loss 0.253281, acc 0.90625, prec 0.064174, recall 0.833333
2017-12-09T22:38:38.111213: step 1548, loss 0.180668, acc 0.953125, prec 0.0642053, recall 0.833424
2017-12-09T22:38:38.406615: step 1549, loss 0.283356, acc 0.882812, prec 0.0642244, recall 0.833515
2017-12-09T22:38:38.704801: step 1550, loss 0.189214, acc 0.929688, prec 0.0642319, recall 0.833561
2017-12-09T22:38:39.006644: step 1551, loss 0.190007, acc 0.921875, prec 0.0642184, recall 0.833561
2017-12-09T22:38:39.303265: step 1552, loss 0.159447, acc 0.945312, prec 0.0642483, recall 0.833651
2017-12-09T22:38:39.599336: step 1553, loss 0.118305, acc 0.953125, prec 0.0642402, recall 0.833651
2017-12-09T22:38:39.900770: step 1554, loss 0.160013, acc 0.929688, prec 0.0642477, recall 0.833697
2017-12-09T22:38:40.200714: step 1555, loss 0.253385, acc 0.976562, prec 0.064342, recall 0.833923
2017-12-09T22:38:40.501623: step 1556, loss 0.235408, acc 0.945312, prec 0.0644308, recall 0.834149
2017-12-09T22:38:40.803065: step 1557, loss 0.163517, acc 0.921875, prec 0.0644369, recall 0.834194
2017-12-09T22:38:41.105067: step 1558, loss 0.139658, acc 0.953125, prec 0.0644288, recall 0.834194
2017-12-09T22:38:41.408006: step 1559, loss 0.510122, acc 0.992188, prec 0.064506, recall 0.834374
2017-12-09T22:38:41.709542: step 1560, loss 0.102719, acc 0.96875, prec 0.0645398, recall 0.834464
2017-12-09T22:38:42.011101: step 1561, loss 0.197121, acc 0.9375, prec 0.0645486, recall 0.834509
2017-12-09T22:38:42.308940: step 1562, loss 0.190407, acc 0.945312, prec 0.0645588, recall 0.834554
2017-12-09T22:38:42.611086: step 1563, loss 0.173475, acc 0.960938, prec 0.0646109, recall 0.834688
2017-12-09T22:38:42.909822: step 1564, loss 0.081688, acc 0.976562, prec 0.0646068, recall 0.834688
2017-12-09T22:38:43.206441: step 1565, loss 0.191142, acc 0.929688, prec 0.0646142, recall 0.834733
2017-12-09T22:38:43.508716: step 1566, loss 0.225082, acc 0.929688, prec 0.064602, recall 0.834733
2017-12-09T22:38:43.807527: step 1567, loss 1.12682, acc 0.9375, prec 0.0646318, recall 0.834597
2017-12-09T22:38:44.111348: step 1568, loss 0.153431, acc 0.96875, prec 0.0646852, recall 0.834731
2017-12-09T22:38:44.411169: step 1569, loss 0.41446, acc 0.9375, prec 0.0647135, recall 0.83482
2017-12-09T22:38:44.714263: step 1570, loss 0.140209, acc 0.976562, prec 0.0647487, recall 0.834909
2017-12-09T22:38:45.012711: step 1571, loss 0.120117, acc 0.976562, prec 0.0647838, recall 0.834999
2017-12-09T22:38:45.309851: step 1572, loss 0.132473, acc 0.976562, prec 0.0647993, recall 0.835043
2017-12-09T22:38:45.603123: step 1573, loss 0.122601, acc 0.945312, prec 0.0648094, recall 0.835088
2017-12-09T22:38:45.900959: step 1574, loss 0.285857, acc 0.945312, prec 0.0648586, recall 0.835221
2017-12-09T22:38:46.210066: step 1575, loss 0.265276, acc 0.945312, prec 0.0648883, recall 0.83531
2017-12-09T22:38:46.508415: step 1576, loss 0.273113, acc 0.921875, prec 0.064953, recall 0.835487
2017-12-09T22:38:46.804332: step 1577, loss 0.258468, acc 0.945312, prec 0.0649631, recall 0.835532
2017-12-09T22:38:47.105722: step 1578, loss 1.22438, acc 0.9375, prec 0.0650318, recall 0.835484
2017-12-09T22:38:47.407506: step 1579, loss 0.646467, acc 0.9375, prec 0.0650796, recall 0.835616
2017-12-09T22:38:47.709046: step 1580, loss 0.365952, acc 0.882812, prec 0.0650787, recall 0.835661
2017-12-09T22:38:48.008855: step 1581, loss 0.227792, acc 0.914062, prec 0.0650833, recall 0.835705
2017-12-09T22:38:48.309643: step 1582, loss 0.294827, acc 0.914062, prec 0.065127, recall 0.835837
2017-12-09T22:38:48.604992: step 1583, loss 0.387978, acc 0.851562, prec 0.0651597, recall 0.835969
2017-12-09T22:38:48.904793: step 1584, loss 0.158158, acc 0.960938, prec 0.065192, recall 0.836057
2017-12-09T22:38:49.207161: step 1585, loss 0.277372, acc 0.898438, prec 0.0652523, recall 0.836232
2017-12-09T22:38:49.509626: step 1586, loss 0.252686, acc 0.882812, prec 0.0653295, recall 0.836451
2017-12-09T22:38:49.805910: step 1587, loss 0.323874, acc 0.867188, prec 0.0653258, recall 0.836495
2017-12-09T22:38:50.106961: step 1588, loss 0.164263, acc 0.945312, prec 0.0653163, recall 0.836495
2017-12-09T22:38:50.416492: step 1589, loss 0.430937, acc 0.84375, prec 0.065328, recall 0.836582
2017-12-09T22:38:50.713514: step 1590, loss 0.36071, acc 0.945312, prec 0.0653964, recall 0.836756
2017-12-09T22:38:51.010726: step 1591, loss 0.465116, acc 0.921875, prec 0.0654802, recall 0.836974
2017-12-09T22:38:51.314831: step 1592, loss 0.280831, acc 0.9375, prec 0.0655277, recall 0.837104
2017-12-09T22:38:51.612348: step 1593, loss 0.97262, acc 0.898438, prec 0.0655502, recall 0.836968
2017-12-09T22:38:51.918477: step 1594, loss 0.399358, acc 0.890625, prec 0.06557, recall 0.837055
2017-12-09T22:38:52.218866: step 1595, loss 0.373294, acc 0.921875, prec 0.0655953, recall 0.837141
2017-12-09T22:38:52.517393: step 1596, loss 0.228964, acc 0.914062, prec 0.065658, recall 0.837314
2017-12-09T22:38:52.814044: step 1597, loss 0.258036, acc 0.875, prec 0.0656751, recall 0.837401
2017-12-09T22:38:53.117813: step 1598, loss 0.223599, acc 0.929688, prec 0.0656822, recall 0.837444
2017-12-09T22:38:53.419345: step 1599, loss 0.174148, acc 0.945312, prec 0.0657309, recall 0.837573
2017-12-09T22:38:53.717136: step 1600, loss 0.212005, acc 0.9375, prec 0.0657588, recall 0.837659
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1600

2017-12-09T22:38:55.011140: step 1601, loss 0.19677, acc 0.90625, prec 0.0657813, recall 0.837745
2017-12-09T22:38:55.308781: step 1602, loss 0.291341, acc 0.890625, prec 0.0657815, recall 0.837788
2017-12-09T22:38:55.606441: step 1603, loss 0.453555, acc 0.9375, prec 0.06579, recall 0.837831
2017-12-09T22:38:55.911351: step 1604, loss 0.583361, acc 0.960938, prec 0.065822, recall 0.837916
2017-12-09T22:38:56.213483: step 1605, loss 0.202244, acc 0.929688, prec 0.0658291, recall 0.837959
2017-12-09T22:38:56.512179: step 1606, loss 0.0847758, acc 0.96875, prec 0.0658236, recall 0.837959
2017-12-09T22:38:56.814079: step 1607, loss 0.0892459, acc 0.96875, prec 0.0658376, recall 0.838002
2017-12-09T22:38:57.115555: step 1608, loss 0.157857, acc 0.9375, prec 0.0658266, recall 0.838002
2017-12-09T22:38:57.413612: step 1609, loss 0.341679, acc 0.929688, prec 0.0658531, recall 0.838088
2017-12-09T22:38:57.713878: step 1610, loss 0.112682, acc 0.976562, prec 0.0658684, recall 0.83813
2017-12-09T22:38:58.016027: step 1611, loss 0.768765, acc 0.9375, prec 0.0659156, recall 0.838259
2017-12-09T22:38:58.316677: step 1612, loss 0.132528, acc 0.953125, prec 0.0659655, recall 0.838386
2017-12-09T22:38:58.622344: step 1613, loss 0.480563, acc 0.953125, prec 0.0660542, recall 0.838599
2017-12-09T22:38:58.921837: step 1614, loss 0.141715, acc 0.953125, prec 0.0660847, recall 0.838684
2017-12-09T22:38:59.221185: step 1615, loss 0.198033, acc 0.929688, prec 0.0660724, recall 0.838684
2017-12-09T22:38:59.522606: step 1616, loss 1.07759, acc 0.953125, prec 0.0661042, recall 0.838549
2017-12-09T22:38:59.822043: step 1617, loss 0.277402, acc 0.914062, prec 0.0661085, recall 0.838591
2017-12-09T22:39:00.129992: step 1618, loss 0.315213, acc 0.898438, prec 0.0661681, recall 0.83876
2017-12-09T22:39:00.432752: step 1619, loss 0.254336, acc 0.875, prec 0.0661462, recall 0.83876
2017-12-09T22:39:00.727378: step 1620, loss 0.271819, acc 0.929688, prec 0.0661338, recall 0.83876
2017-12-09T22:39:01.024806: step 1621, loss 0.35922, acc 0.914062, prec 0.0661768, recall 0.838887
2017-12-09T22:39:01.322956: step 1622, loss 0.253487, acc 0.921875, prec 0.0661824, recall 0.83893
2017-12-09T22:39:01.621950: step 1623, loss 0.333461, acc 0.898438, prec 0.0661839, recall 0.838972
2017-12-09T22:39:01.919263: step 1624, loss 0.166473, acc 0.9375, prec 0.0662309, recall 0.839099
2017-12-09T22:39:02.217557: step 1625, loss 0.138853, acc 0.953125, prec 0.0662227, recall 0.839099
2017-12-09T22:39:02.516021: step 1626, loss 0.232218, acc 0.9375, prec 0.0662311, recall 0.839141
2017-12-09T22:39:02.813581: step 1627, loss 0.627779, acc 0.890625, prec 0.0662698, recall 0.839267
2017-12-09T22:39:03.113213: step 1628, loss 0.148768, acc 0.960938, prec 0.0662822, recall 0.839309
2017-12-09T22:39:03.416150: step 1629, loss 0.735522, acc 0.90625, prec 0.0662851, recall 0.839351
2017-12-09T22:39:03.716863: step 1630, loss 0.228991, acc 0.9375, prec 0.0662934, recall 0.839393
2017-12-09T22:39:04.020071: step 1631, loss 0.336301, acc 0.898438, prec 0.0663335, recall 0.839519
2017-12-09T22:39:04.316246: step 1632, loss 0.200955, acc 0.953125, prec 0.0663445, recall 0.839561
2017-12-09T22:39:04.611546: step 1633, loss 0.584501, acc 0.921875, prec 0.0664465, recall 0.839812
2017-12-09T22:39:04.911972: step 1634, loss 0.207421, acc 0.898438, prec 0.0664479, recall 0.839854
2017-12-09T22:39:05.220442: step 1635, loss 0.383697, acc 0.867188, prec 0.0664631, recall 0.839937
2017-12-09T22:39:05.534504: step 1636, loss 0.243736, acc 0.921875, prec 0.0664687, recall 0.839979
2017-12-09T22:39:05.834819: step 1637, loss 0.263527, acc 0.898438, prec 0.0664701, recall 0.840021
2017-12-09T22:39:06.135583: step 1638, loss 0.244521, acc 0.921875, prec 0.0664757, recall 0.840062
2017-12-09T22:39:06.437859: step 1639, loss 0.207541, acc 0.929688, prec 0.066521, recall 0.840187
2017-12-09T22:39:06.741802: step 1640, loss 0.527649, acc 0.953125, prec 0.0665898, recall 0.840354
2017-12-09T22:39:07.043642: step 1641, loss 0.236713, acc 0.945312, prec 0.0665994, recall 0.840395
2017-12-09T22:39:07.349136: step 1642, loss 0.186811, acc 0.9375, prec 0.0665884, recall 0.840395
2017-12-09T22:39:07.652943: step 1643, loss 0.161414, acc 0.976562, prec 0.0666227, recall 0.840478
2017-12-09T22:39:07.951037: step 1644, loss 0.578031, acc 0.953125, prec 0.0666529, recall 0.840561
2017-12-09T22:39:08.249286: step 1645, loss 0.340831, acc 0.914062, prec 0.0666378, recall 0.840561
2017-12-09T22:39:08.549519: step 1646, loss 1.15021, acc 0.890625, prec 0.0667161, recall 0.84055
2017-12-09T22:39:08.851312: step 1647, loss 0.291228, acc 0.90625, prec 0.066738, recall 0.840632
2017-12-09T22:39:09.144436: step 1648, loss 0.625108, acc 0.859375, prec 0.0667325, recall 0.840674
2017-12-09T22:39:09.443312: step 1649, loss 0.627405, acc 0.890625, prec 0.0667516, recall 0.840756
2017-12-09T22:39:09.745312: step 1650, loss 0.230728, acc 0.90625, prec 0.0667544, recall 0.840797
2017-12-09T22:39:10.039353: step 1651, loss 0.519014, acc 0.890625, prec 0.0667543, recall 0.840838
2017-12-09T22:39:10.343403: step 1652, loss 0.281141, acc 0.898438, prec 0.0667365, recall 0.840838
2017-12-09T22:39:10.642896: step 1653, loss 0.375523, acc 0.882812, prec 0.0667351, recall 0.84088
2017-12-09T22:39:10.939563: step 1654, loss 0.362266, acc 0.875, prec 0.0667515, recall 0.840962
2017-12-09T22:39:11.237471: step 1655, loss 0.437335, acc 0.851562, prec 0.0667829, recall 0.841085
2017-12-09T22:39:11.535741: step 1656, loss 0.234997, acc 0.929688, prec 0.0668089, recall 0.841167
2017-12-09T22:39:11.839134: step 1657, loss 0.402155, acc 0.890625, prec 0.066828, recall 0.841249
2017-12-09T22:39:12.136323: step 1658, loss 0.142796, acc 0.945312, prec 0.0668567, recall 0.841331
2017-12-09T22:39:12.435616: step 1659, loss 0.280696, acc 0.898438, prec 0.0668771, recall 0.841413
2017-12-09T22:39:12.729767: step 1660, loss 0.368081, acc 0.898438, prec 0.0669166, recall 0.841536
2017-12-09T22:39:13.027866: step 1661, loss 0.292132, acc 0.921875, prec 0.0669029, recall 0.841536
2017-12-09T22:39:13.323953: step 1662, loss 1.25647, acc 0.945312, prec 0.0669138, recall 0.84136
2017-12-09T22:39:13.624131: step 1663, loss 0.375886, acc 0.898438, prec 0.0669151, recall 0.841401
2017-12-09T22:39:13.922571: step 1664, loss 0.16548, acc 0.929688, prec 0.0669219, recall 0.841441
2017-12-09T22:39:14.227398: step 1665, loss 0.557675, acc 0.921875, prec 0.0669655, recall 0.841564
2017-12-09T22:39:14.533583: step 1666, loss 0.329319, acc 0.875, prec 0.0669817, recall 0.841645
2017-12-09T22:39:14.827856: step 1667, loss 0.206479, acc 0.929688, prec 0.0670076, recall 0.841727
2017-12-09T22:39:15.131079: step 1668, loss 0.191631, acc 0.953125, prec 0.0670375, recall 0.841808
2017-12-09T22:39:15.428456: step 1669, loss 0.51585, acc 0.984375, prec 0.0671111, recall 0.84197
2017-12-09T22:39:15.729313: step 1670, loss 0.22765, acc 0.945312, prec 0.0671205, recall 0.842011
2017-12-09T22:39:16.027479: step 1671, loss 0.16291, acc 0.9375, prec 0.0671477, recall 0.842092
2017-12-09T22:39:16.325069: step 1672, loss 0.0995104, acc 0.960938, prec 0.0671408, recall 0.842092
2017-12-09T22:39:16.628352: step 1673, loss 0.188821, acc 0.929688, prec 0.0671476, recall 0.842132
2017-12-09T22:39:16.923221: step 1674, loss 0.150197, acc 0.9375, prec 0.0671556, recall 0.842173
2017-12-09T22:39:17.231315: step 1675, loss 0.812932, acc 0.953125, prec 0.0671678, recall 0.841997
2017-12-09T22:39:17.532263: step 1676, loss 0.231914, acc 0.945312, prec 0.0672344, recall 0.842159
2017-12-09T22:39:17.831691: step 1677, loss 0.259275, acc 0.890625, prec 0.0672152, recall 0.842159
2017-12-09T22:39:18.131064: step 1678, loss 0.603894, acc 0.929688, prec 0.0672219, recall 0.8422
2017-12-09T22:39:18.427888: step 1679, loss 0.166215, acc 0.960938, prec 0.0672912, recall 0.842361
2017-12-09T22:39:18.726906: step 1680, loss 0.143345, acc 0.929688, prec 0.0672788, recall 0.842361
2017-12-09T22:39:19.025765: step 1681, loss 0.245832, acc 0.9375, prec 0.0672679, recall 0.842361
2017-12-09T22:39:19.327131: step 1682, loss 0.206406, acc 0.953125, prec 0.0672977, recall 0.842441
2017-12-09T22:39:19.624380: step 1683, loss 0.292728, acc 0.898438, prec 0.0672989, recall 0.842481
2017-12-09T22:39:19.923256: step 1684, loss 0.11312, acc 0.96875, prec 0.0673504, recall 0.842602
2017-12-09T22:39:20.225666: step 1685, loss 0.226835, acc 0.9375, prec 0.0673775, recall 0.842682
2017-12-09T22:39:20.539468: step 1686, loss 0.32505, acc 0.90625, prec 0.067418, recall 0.842803
2017-12-09T22:39:20.838283: step 1687, loss 0.189018, acc 0.953125, prec 0.0674478, recall 0.842883
2017-12-09T22:39:21.140787: step 1688, loss 0.290829, acc 0.9375, prec 0.0674558, recall 0.842923
2017-12-09T22:39:21.442735: step 1689, loss 0.189971, acc 0.945312, prec 0.0674462, recall 0.842923
2017-12-09T22:39:21.745550: step 1690, loss 0.1368, acc 0.953125, prec 0.0674379, recall 0.842923
2017-12-09T22:39:22.044616: step 1691, loss 0.147305, acc 0.953125, prec 0.0674297, recall 0.842923
2017-12-09T22:39:22.341047: step 1692, loss 0.0683274, acc 0.96875, prec 0.0674242, recall 0.842923
2017-12-09T22:39:22.639891: step 1693, loss 0.163476, acc 0.960938, prec 0.0674743, recall 0.843042
2017-12-09T22:39:22.934303: step 1694, loss 0.115585, acc 1, prec 0.0675123, recall 0.843122
2017-12-09T22:39:23.237025: step 1695, loss 0.118873, acc 0.96875, prec 0.0675637, recall 0.843242
2017-12-09T22:39:23.536906: step 1696, loss 0.19659, acc 0.976562, prec 0.0675975, recall 0.843322
2017-12-09T22:39:23.833739: step 1697, loss 0.155692, acc 0.96875, prec 0.067592, recall 0.843322
2017-12-09T22:39:24.132610: step 1698, loss 0.143214, acc 0.976562, prec 0.0676069, recall 0.843361
2017-12-09T22:39:24.431865: step 1699, loss 0.0858993, acc 0.976562, prec 0.0676597, recall 0.84348
2017-12-09T22:39:24.729730: step 1700, loss 0.127981, acc 0.953125, prec 0.0676514, recall 0.84348
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1700

2017-12-09T22:39:26.231520: step 1701, loss 0.51029, acc 0.984375, prec 0.0676866, recall 0.84356
2017-12-09T22:39:26.530835: step 1702, loss 0.222532, acc 0.96875, prec 0.067738, recall 0.843679
2017-12-09T22:39:26.831938: step 1703, loss 0.0986096, acc 0.96875, prec 0.0677325, recall 0.843679
2017-12-09T22:39:27.130692: step 1704, loss 0.0634582, acc 0.976562, prec 0.0677473, recall 0.843718
2017-12-09T22:39:27.427409: step 1705, loss 1.10185, acc 0.96875, prec 0.0678555, recall 0.843956
2017-12-09T22:39:27.731822: step 1706, loss 1.66052, acc 0.953125, prec 0.0679245, recall 0.8439
2017-12-09T22:39:28.036408: step 1707, loss 0.444106, acc 0.945312, prec 0.0679527, recall 0.843979
2017-12-09T22:39:28.335205: step 1708, loss 0.255566, acc 0.921875, prec 0.0679578, recall 0.844018
2017-12-09T22:39:28.636754: step 1709, loss 0.382887, acc 0.882812, prec 0.0679371, recall 0.844018
2017-12-09T22:39:28.936309: step 1710, loss 0.206281, acc 0.898438, prec 0.067957, recall 0.844097
2017-12-09T22:39:29.235977: step 1711, loss 0.319363, acc 0.90625, prec 0.0680162, recall 0.844254
2017-12-09T22:39:29.532002: step 1712, loss 0.553864, acc 0.84375, prec 0.0680453, recall 0.844372
2017-12-09T22:39:29.828453: step 1713, loss 0.611349, acc 0.851562, prec 0.0680947, recall 0.844528
2017-12-09T22:39:30.135255: step 1714, loss 0.396954, acc 0.882812, prec 0.0681307, recall 0.844646
2017-12-09T22:39:30.440013: step 1715, loss 0.385001, acc 0.804688, prec 0.0680961, recall 0.844646
2017-12-09T22:39:30.736781: step 1716, loss 0.431066, acc 0.84375, prec 0.0680874, recall 0.844685
2017-12-09T22:39:31.038609: step 1717, loss 0.505423, acc 0.851562, prec 0.068099, recall 0.844763
2017-12-09T22:39:31.342687: step 1718, loss 0.592624, acc 0.820312, prec 0.068105, recall 0.844841
2017-12-09T22:39:31.641100: step 1719, loss 0.310026, acc 0.875, prec 0.0681207, recall 0.844918
2017-12-09T22:39:31.942765: step 1720, loss 0.269036, acc 0.898438, prec 0.0681216, recall 0.844957
2017-12-09T22:39:32.249880: step 1721, loss 0.417458, acc 0.867188, prec 0.0681359, recall 0.845035
2017-12-09T22:39:32.551145: step 1722, loss 0.268601, acc 0.921875, prec 0.0681786, recall 0.845152
2017-12-09T22:39:32.854472: step 1723, loss 0.206323, acc 0.921875, prec 0.0681648, recall 0.845152
2017-12-09T22:39:33.159356: step 1724, loss 0.124457, acc 0.96875, prec 0.0681781, recall 0.84519
2017-12-09T22:39:33.457131: step 1725, loss 0.1573, acc 0.945312, prec 0.0681685, recall 0.84519
2017-12-09T22:39:33.756157: step 1726, loss 0.133484, acc 0.90625, prec 0.0681896, recall 0.845268
2017-12-09T22:39:34.056690: step 1727, loss 0.270937, acc 0.921875, prec 0.0682323, recall 0.845384
2017-12-09T22:39:34.360901: step 1728, loss 0.599843, acc 0.953125, prec 0.0683181, recall 0.845577
2017-12-09T22:39:34.667323: step 1729, loss 0.117545, acc 0.953125, prec 0.0683098, recall 0.845577
2017-12-09T22:39:34.965570: step 1730, loss 0.673776, acc 0.960938, prec 0.0683593, recall 0.845693
2017-12-09T22:39:35.276409: step 1731, loss 0.151063, acc 0.984375, prec 0.0683754, recall 0.845731
2017-12-09T22:39:35.582743: step 1732, loss 0.0804998, acc 0.976562, prec 0.06839, recall 0.84577
2017-12-09T22:39:35.880447: step 1733, loss 0.336859, acc 0.960938, prec 0.0684207, recall 0.845847
2017-12-09T22:39:36.179523: step 1734, loss 0.181402, acc 0.976562, prec 0.0684542, recall 0.845924
2017-12-09T22:39:36.481953: step 1735, loss 0.0392117, acc 0.984375, prec 0.0684514, recall 0.845924
2017-12-09T22:39:36.782049: step 1736, loss 0.143806, acc 0.96875, prec 0.0684647, recall 0.845962
2017-12-09T22:39:37.083427: step 1737, loss 0.0777393, acc 0.984375, prec 0.0684807, recall 0.846
2017-12-09T22:39:37.381654: step 1738, loss 0.405267, acc 0.96875, prec 0.0685128, recall 0.846077
2017-12-09T22:39:37.690154: step 1739, loss 0.116489, acc 0.96875, prec 0.068526, recall 0.846116
2017-12-09T22:39:37.989468: step 1740, loss 0.29085, acc 0.960938, prec 0.0685755, recall 0.84623
2017-12-09T22:39:38.288738: step 1741, loss 1.70647, acc 0.945312, prec 0.0686235, recall 0.846135
2017-12-09T22:39:38.592163: step 1742, loss 0.15456, acc 0.976562, prec 0.0686381, recall 0.846173
2017-12-09T22:39:38.769091: step 1743, loss 0.161721, acc 0.941176, prec 0.068634, recall 0.846173
2017-12-09T22:39:39.079882: step 1744, loss 0.106341, acc 0.953125, prec 0.0686444, recall 0.846211
2017-12-09T22:39:39.376316: step 1745, loss 0.272506, acc 0.96875, prec 0.0686577, recall 0.846249
2017-12-09T22:39:39.679005: step 1746, loss 0.212673, acc 0.914062, prec 0.0686612, recall 0.846288
2017-12-09T22:39:39.981154: step 1747, loss 0.397833, acc 0.921875, prec 0.0687412, recall 0.846478
2017-12-09T22:39:40.277870: step 1748, loss 0.492431, acc 0.835938, prec 0.0687496, recall 0.846554
2017-12-09T22:39:40.580640: step 1749, loss 0.194108, acc 0.929688, prec 0.0687372, recall 0.846554
2017-12-09T22:39:40.881976: step 1750, loss 0.1485, acc 0.953125, prec 0.0687851, recall 0.846668
2017-12-09T22:39:41.180711: step 1751, loss 0.267363, acc 0.90625, prec 0.0687872, recall 0.846706
2017-12-09T22:39:41.481523: step 1752, loss 0.194876, acc 0.90625, prec 0.0687706, recall 0.846706
2017-12-09T22:39:41.780275: step 1753, loss 0.24948, acc 0.898438, prec 0.0687526, recall 0.846706
2017-12-09T22:39:42.080164: step 1754, loss 0.364093, acc 0.898438, prec 0.0687721, recall 0.846782
2017-12-09T22:39:42.383106: step 1755, loss 0.335873, acc 0.859375, prec 0.0687472, recall 0.846782
2017-12-09T22:39:42.686224: step 1756, loss 0.253727, acc 0.929688, prec 0.0687722, recall 0.846858
2017-12-09T22:39:42.988266: step 1757, loss 0.28648, acc 0.890625, prec 0.0687903, recall 0.846934
2017-12-09T22:39:43.289133: step 1758, loss 0.132882, acc 0.960938, prec 0.0687834, recall 0.846934
2017-12-09T22:39:43.587173: step 1759, loss 0.156884, acc 0.921875, prec 0.0687883, recall 0.846972
2017-12-09T22:39:43.886759: step 1760, loss 0.170271, acc 0.945312, prec 0.0687973, recall 0.847009
2017-12-09T22:39:44.188489: step 1761, loss 0.172196, acc 0.945312, prec 0.0688811, recall 0.847198
2017-12-09T22:39:44.490439: step 1762, loss 0.336236, acc 0.945312, prec 0.0689648, recall 0.847387
2017-12-09T22:39:44.794378: step 1763, loss 0.0924115, acc 0.976562, prec 0.0689794, recall 0.847424
2017-12-09T22:39:45.091633: step 1764, loss 0.0855805, acc 0.992188, prec 0.0690153, recall 0.847499
2017-12-09T22:39:45.396431: step 1765, loss 0.264256, acc 0.953125, prec 0.0691004, recall 0.847687
2017-12-09T22:39:45.703568: step 1766, loss 0.162634, acc 0.945312, prec 0.0691094, recall 0.847724
2017-12-09T22:39:45.999851: step 1767, loss 0.413431, acc 0.96875, prec 0.0691598, recall 0.847837
2017-12-09T22:39:46.300491: step 1768, loss 0.356274, acc 0.945312, prec 0.0692061, recall 0.847949
2017-12-09T22:39:46.606846: step 1769, loss 0.355373, acc 0.976562, prec 0.0692579, recall 0.848061
2017-12-09T22:39:46.913943: step 1770, loss 0.0881218, acc 0.976562, prec 0.0692724, recall 0.848098
2017-12-09T22:39:47.213660: step 1771, loss 0.469447, acc 0.984375, prec 0.0692883, recall 0.848135
2017-12-09T22:39:47.523923: step 1772, loss 0.809301, acc 0.953125, prec 0.0694664, recall 0.848507
2017-12-09T22:39:47.823017: step 1773, loss 0.184435, acc 0.945312, prec 0.0694567, recall 0.848507
2017-12-09T22:39:48.119781: step 1774, loss 0.136141, acc 0.960938, prec 0.069487, recall 0.848581
2017-12-09T22:39:48.417677: step 1775, loss 0.251949, acc 0.90625, prec 0.0694703, recall 0.848581
2017-12-09T22:39:48.715492: step 1776, loss 0.310222, acc 0.890625, prec 0.0694508, recall 0.848581
2017-12-09T22:39:49.016611: step 1777, loss 0.293744, acc 0.9375, prec 0.0694583, recall 0.848618
2017-12-09T22:39:49.314997: step 1778, loss 0.155329, acc 0.9375, prec 0.0694658, recall 0.848655
2017-12-09T22:39:49.615310: step 1779, loss 0.147513, acc 0.953125, prec 0.0694575, recall 0.848655
2017-12-09T22:39:49.912238: step 1780, loss 0.230779, acc 0.921875, prec 0.0694436, recall 0.848655
2017-12-09T22:39:50.218655: step 1781, loss 0.271034, acc 0.914062, prec 0.0694656, recall 0.848729
2017-12-09T22:39:50.529294: step 1782, loss 0.243406, acc 0.921875, prec 0.0695261, recall 0.848877
2017-12-09T22:39:50.825929: step 1783, loss 0.132206, acc 0.953125, prec 0.0695736, recall 0.848988
2017-12-09T22:39:51.126028: step 1784, loss 0.161907, acc 0.953125, prec 0.0696024, recall 0.849061
2017-12-09T22:39:51.425045: step 1785, loss 0.185344, acc 0.929688, prec 0.0696085, recall 0.849098
2017-12-09T22:39:51.723360: step 1786, loss 0.26285, acc 0.921875, prec 0.0696504, recall 0.849208
2017-12-09T22:39:52.019759: step 1787, loss 0.159581, acc 0.953125, prec 0.069642, recall 0.849208
2017-12-09T22:39:52.319813: step 1788, loss 0.199961, acc 0.9375, prec 0.0696495, recall 0.849245
2017-12-09T22:39:52.625784: step 1789, loss 0.0617175, acc 0.992188, prec 0.0696852, recall 0.849318
2017-12-09T22:39:52.929261: step 1790, loss 0.223813, acc 0.914062, prec 0.0697071, recall 0.849392
2017-12-09T22:39:53.231951: step 1791, loss 0.608325, acc 0.960938, prec 0.0697373, recall 0.849465
2017-12-09T22:39:53.536938: step 1792, loss 0.0727665, acc 0.984375, prec 0.0697531, recall 0.849502
2017-12-09T22:39:53.837953: step 1793, loss 0.0634849, acc 0.976562, prec 0.0697489, recall 0.849502
2017-12-09T22:39:54.135029: step 1794, loss 0.338633, acc 0.96875, prec 0.0698361, recall 0.849684
2017-12-09T22:39:54.435060: step 1795, loss 0.122032, acc 0.96875, prec 0.0698677, recall 0.849757
2017-12-09T22:39:54.734864: step 1796, loss 0.204137, acc 0.984375, prec 0.0699577, recall 0.849939
2017-12-09T22:39:55.037416: step 1797, loss 0.169129, acc 0.929688, prec 0.0699637, recall 0.849976
2017-12-09T22:39:55.336214: step 1798, loss 0.102835, acc 0.953125, prec 0.0699924, recall 0.850048
2017-12-09T22:39:55.640123: step 1799, loss 0.170495, acc 0.953125, prec 0.069984, recall 0.850048
2017-12-09T22:39:55.936610: step 1800, loss 0.136177, acc 0.945312, prec 0.0699743, recall 0.850048

Evaluation:
2017-12-09T22:40:00.650742: step 1800, loss 2.5373, acc 0.959996, prec 0.0707362, recall 0.835709

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1800

2017-12-09T22:40:01.964400: step 1801, loss 0.23674, acc 0.960938, prec 0.070766, recall 0.835786
2017-12-09T22:40:02.270019: step 1802, loss 0.110767, acc 0.960938, prec 0.0707774, recall 0.835824
2017-12-09T22:40:02.568376: step 1803, loss 0.201836, acc 0.96875, prec 0.0707902, recall 0.835863
2017-12-09T22:40:02.871780: step 1804, loss 0.202929, acc 0.929688, prec 0.0707959, recall 0.835901
2017-12-09T22:40:03.172655: step 1805, loss 0.186664, acc 0.96875, prec 0.0708087, recall 0.835939
2017-12-09T22:40:03.469088: step 1806, loss 0.157651, acc 0.953125, prec 0.0708554, recall 0.836054
2017-12-09T22:40:03.767960: step 1807, loss 0.0922623, acc 0.960938, prec 0.0708484, recall 0.836054
2017-12-09T22:40:04.064322: step 1808, loss 0.331913, acc 0.96875, prec 0.0708612, recall 0.836092
2017-12-09T22:40:04.367935: step 1809, loss 0.156452, acc 0.960938, prec 0.0708542, recall 0.836092
2017-12-09T22:40:04.665933: step 1810, loss 0.0968771, acc 0.976562, prec 0.0708683, recall 0.836131
2017-12-09T22:40:04.962707: step 1811, loss 0.20988, acc 0.960938, prec 0.0709164, recall 0.836245
2017-12-09T22:40:05.275013: step 1812, loss 0.602773, acc 0.976562, prec 0.0709136, recall 0.83605
2017-12-09T22:40:05.583682: step 1813, loss 0.125781, acc 0.960938, prec 0.0709249, recall 0.836088
2017-12-09T22:40:05.888506: step 1814, loss 0.826766, acc 0.96875, prec 0.070956, recall 0.836165
2017-12-09T22:40:06.188680: step 1815, loss 0.220582, acc 0.96875, prec 0.0709871, recall 0.836241
2017-12-09T22:40:06.492239: step 1816, loss 0.0848745, acc 0.976562, prec 0.0710196, recall 0.836317
2017-12-09T22:40:06.793988: step 1817, loss 0.131922, acc 0.953125, prec 0.0710295, recall 0.836355
2017-12-09T22:40:07.090667: step 1818, loss 0.162421, acc 0.945312, prec 0.0710747, recall 0.836469
2017-12-09T22:40:07.392068: step 1819, loss 0.167326, acc 0.945312, prec 0.0711382, recall 0.836621
2017-12-09T22:40:07.690518: step 1820, loss 0.244484, acc 0.914062, prec 0.0711777, recall 0.836735
2017-12-09T22:40:07.986287: step 1821, loss 0.138137, acc 0.960938, prec 0.0712257, recall 0.836848
2017-12-09T22:40:08.282091: step 1822, loss 0.194575, acc 0.929688, prec 0.0712314, recall 0.836886
2017-12-09T22:40:08.584305: step 1823, loss 0.401422, acc 0.875, prec 0.0712638, recall 0.836999
2017-12-09T22:40:08.886569: step 1824, loss 0.205965, acc 0.9375, prec 0.0712709, recall 0.837037
2017-12-09T22:40:09.188311: step 1825, loss 0.087274, acc 0.960938, prec 0.0712822, recall 0.837075
2017-12-09T22:40:09.482466: step 1826, loss 0.196576, acc 0.945312, prec 0.0713272, recall 0.837188
2017-12-09T22:40:09.778141: step 1827, loss 0.119608, acc 0.945312, prec 0.0713357, recall 0.837225
2017-12-09T22:40:10.076752: step 1828, loss 0.182982, acc 0.96875, prec 0.071385, recall 0.837338
2017-12-09T22:40:10.381611: step 1829, loss 0.168629, acc 0.9375, prec 0.0714103, recall 0.837413
2017-12-09T22:40:10.685119: step 1830, loss 0.145101, acc 0.9375, prec 0.0714356, recall 0.837488
2017-12-09T22:40:10.991593: step 1831, loss 0.233515, acc 0.976562, prec 0.0714862, recall 0.837601
2017-12-09T22:40:11.291914: step 1832, loss 0.284115, acc 0.921875, prec 0.0715635, recall 0.837788
2017-12-09T22:40:11.592091: step 1833, loss 0.179035, acc 0.96875, prec 0.0716127, recall 0.8379
2017-12-09T22:40:11.893779: step 1834, loss 0.394159, acc 0.945312, prec 0.0716394, recall 0.837975
2017-12-09T22:40:12.198527: step 1835, loss 0.248922, acc 0.960938, prec 0.0716689, recall 0.838049
2017-12-09T22:40:12.498320: step 1836, loss 0.691482, acc 0.945312, prec 0.0716604, recall 0.837856
2017-12-09T22:40:12.797026: step 1837, loss 0.741499, acc 0.960938, prec 0.0717278, recall 0.837813
2017-12-09T22:40:13.100067: step 1838, loss 0.109968, acc 0.976562, prec 0.0717601, recall 0.837887
2017-12-09T22:40:13.397209: step 1839, loss 0.113673, acc 0.96875, prec 0.0717909, recall 0.837962
2017-12-09T22:40:13.696593: step 1840, loss 0.15232, acc 0.96875, prec 0.0718218, recall 0.838036
2017-12-09T22:40:13.995915: step 1841, loss 0.202026, acc 0.945312, prec 0.0718666, recall 0.838148
2017-12-09T22:40:14.295401: step 1842, loss 0.257466, acc 0.945312, prec 0.0719115, recall 0.838259
2017-12-09T22:40:14.596671: step 1843, loss 0.233851, acc 0.945312, prec 0.0719016, recall 0.838259
2017-12-09T22:40:14.894024: step 1844, loss 0.161811, acc 0.953125, prec 0.0719296, recall 0.838333
2017-12-09T22:40:15.192072: step 1845, loss 0.256361, acc 0.914062, prec 0.0719323, recall 0.83837
2017-12-09T22:40:15.490172: step 1846, loss 0.175248, acc 0.9375, prec 0.0719574, recall 0.838444
2017-12-09T22:40:15.787917: step 1847, loss 0.444242, acc 0.929688, prec 0.0719994, recall 0.838555
2017-12-09T22:40:16.090759: step 1848, loss 0.618369, acc 0.90625, prec 0.0720188, recall 0.838629
2017-12-09T22:40:16.397278: step 1849, loss 0.186357, acc 0.945312, prec 0.0720272, recall 0.838665
2017-12-09T22:40:16.698220: step 1850, loss 0.193502, acc 0.9375, prec 0.0720705, recall 0.838776
2017-12-09T22:40:16.995694: step 1851, loss 0.127909, acc 0.953125, prec 0.0720984, recall 0.83885
2017-12-09T22:40:17.296832: step 1852, loss 0.146551, acc 0.945312, prec 0.0721067, recall 0.838886
2017-12-09T22:40:17.596182: step 1853, loss 0.240245, acc 0.921875, prec 0.072129, recall 0.83896
2017-12-09T22:40:17.897921: step 1854, loss 0.0992453, acc 0.953125, prec 0.0721205, recall 0.83896
2017-12-09T22:40:18.200812: step 1855, loss 0.240782, acc 0.953125, prec 0.0721666, recall 0.83907
2017-12-09T22:40:18.501153: step 1856, loss 0.250635, acc 0.929688, prec 0.0721538, recall 0.83907
2017-12-09T22:40:18.801881: step 1857, loss 0.193331, acc 0.929688, prec 0.0721775, recall 0.839143
2017-12-09T22:40:19.102112: step 1858, loss 0.350625, acc 0.976562, prec 0.0722096, recall 0.839217
2017-12-09T22:40:19.405874: step 1859, loss 0.148667, acc 0.945312, prec 0.0722542, recall 0.839326
2017-12-09T22:40:19.706607: step 1860, loss 0.0987875, acc 0.953125, prec 0.0722639, recall 0.839363
2017-12-09T22:40:20.004715: step 1861, loss 0.161301, acc 0.953125, prec 0.0722918, recall 0.839436
2017-12-09T22:40:20.318769: step 1862, loss 0.133458, acc 0.9375, prec 0.0722804, recall 0.839436
2017-12-09T22:40:20.623653: step 1863, loss 0.136372, acc 0.953125, prec 0.0723264, recall 0.839545
2017-12-09T22:40:20.929541: step 1864, loss 0.511343, acc 0.929688, prec 0.0723863, recall 0.839691
2017-12-09T22:40:21.232394: step 1865, loss 0.316984, acc 0.976562, prec 0.0724365, recall 0.8398
2017-12-09T22:40:21.536582: step 1866, loss 0.21034, acc 0.960938, prec 0.0724839, recall 0.839909
2017-12-09T22:40:21.838994: step 1867, loss 0.111241, acc 0.976562, prec 0.0725341, recall 0.840018
2017-12-09T22:40:22.144605: step 1868, loss 0.303539, acc 0.984375, prec 0.0726038, recall 0.840163
2017-12-09T22:40:22.449657: step 1869, loss 0.239039, acc 0.9375, prec 0.0726288, recall 0.840235
2017-12-09T22:40:22.746385: step 1870, loss 0.0937321, acc 0.953125, prec 0.0726202, recall 0.840235
2017-12-09T22:40:23.044227: step 1871, loss 0.271436, acc 0.9375, prec 0.072627, recall 0.840271
2017-12-09T22:40:23.347613: step 1872, loss 0.132982, acc 0.96875, prec 0.0726395, recall 0.840308
2017-12-09T22:40:23.645755: step 1873, loss 0.104251, acc 0.953125, prec 0.0726853, recall 0.840416
2017-12-09T22:40:23.942716: step 1874, loss 0.214758, acc 0.953125, prec 0.0727131, recall 0.840488
2017-12-09T22:40:24.240357: step 1875, loss 0.138963, acc 0.96875, prec 0.0727436, recall 0.84056
2017-12-09T22:40:24.542874: step 1876, loss 0.470554, acc 0.9375, prec 0.0727504, recall 0.840596
2017-12-09T22:40:24.839826: step 1877, loss 1.30411, acc 0.960938, prec 0.0728171, recall 0.84055
2017-12-09T22:40:25.144448: step 1878, loss 0.273956, acc 0.929688, prec 0.0728225, recall 0.840586
2017-12-09T22:40:25.441854: step 1879, loss 0.257287, acc 0.953125, prec 0.0728501, recall 0.840658
2017-12-09T22:40:25.741805: step 1880, loss 0.345564, acc 0.945312, prec 0.0728764, recall 0.84073
2017-12-09T22:40:26.037146: step 1881, loss 0.189096, acc 0.9375, prec 0.072865, recall 0.84073
2017-12-09T22:40:26.330666: step 1882, loss 0.244667, acc 0.914062, prec 0.0729037, recall 0.840837
2017-12-09T22:40:26.629922: step 1883, loss 0.405769, acc 0.875, prec 0.072899, recall 0.840873
2017-12-09T22:40:26.931852: step 1884, loss 0.176648, acc 0.960938, prec 0.072928, recall 0.840945
2017-12-09T22:40:27.231784: step 1885, loss 0.337723, acc 0.953125, prec 0.0729918, recall 0.841088
2017-12-09T22:40:27.538128: step 1886, loss 0.240653, acc 0.9375, prec 0.0730347, recall 0.841195
2017-12-09T22:40:27.838600: step 1887, loss 0.28612, acc 0.898438, prec 0.0730523, recall 0.841266
2017-12-09T22:40:28.137733: step 1888, loss 0.194834, acc 0.9375, prec 0.0730771, recall 0.841338
2017-12-09T22:40:28.435823: step 1889, loss 0.3174, acc 0.90625, prec 0.073078, recall 0.841373
2017-12-09T22:40:28.738088: step 1890, loss 0.168402, acc 0.9375, prec 0.0730847, recall 0.841409
2017-12-09T22:40:29.043401: step 1891, loss 0.192124, acc 0.914062, prec 0.0731052, recall 0.84148
2017-12-09T22:40:29.338982: step 1892, loss 0.223445, acc 0.929688, prec 0.0731104, recall 0.841515
2017-12-09T22:40:29.637122: step 1893, loss 0.170004, acc 0.953125, prec 0.0731019, recall 0.841515
2017-12-09T22:40:29.938251: step 1894, loss 0.3376, acc 0.929688, prec 0.0731251, recall 0.841586
2017-12-09T22:40:30.244062: step 1895, loss 0.539692, acc 0.96875, prec 0.0731736, recall 0.841693
2017-12-09T22:40:30.555047: step 1896, loss 0.163985, acc 0.960938, prec 0.0732025, recall 0.841764
2017-12-09T22:40:30.851201: step 1897, loss 0.212343, acc 0.929688, prec 0.0732438, recall 0.84187
2017-12-09T22:40:31.153251: step 1898, loss 0.153778, acc 0.96875, prec 0.0732742, recall 0.841941
2017-12-09T22:40:31.449142: step 1899, loss 0.0957202, acc 0.96875, prec 0.0733045, recall 0.842011
2017-12-09T22:40:31.748501: step 1900, loss 0.210563, acc 0.929688, prec 0.0733097, recall 0.842046
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-1900

2017-12-09T22:40:33.754029: step 1901, loss 0.296303, acc 0.984375, prec 0.0733609, recall 0.842152
2017-12-09T22:40:34.360452: step 1902, loss 0.236868, acc 0.914062, prec 0.0733813, recall 0.842223
2017-12-09T22:40:34.942729: step 1903, loss 0.149677, acc 0.945312, prec 0.0734073, recall 0.842293
2017-12-09T22:40:35.276685: step 1904, loss 0.357735, acc 0.984375, prec 0.0734405, recall 0.842363
2017-12-09T22:40:35.619393: step 1905, loss 0.269944, acc 0.960938, prec 0.0734694, recall 0.842434
2017-12-09T22:40:35.949370: step 1906, loss 0.131233, acc 0.96875, prec 0.0734817, recall 0.842469
2017-12-09T22:40:36.261668: step 1907, loss 0.0918922, acc 0.976562, prec 0.0735134, recall 0.842539
2017-12-09T22:40:36.585669: step 1908, loss 0.627768, acc 0.953125, prec 0.0735228, recall 0.842574
2017-12-09T22:40:36.907481: step 1909, loss 0.176864, acc 0.9375, prec 0.0735294, recall 0.842609
2017-12-09T22:40:37.211617: step 1910, loss 0.109726, acc 0.960938, prec 0.0735223, recall 0.842609
2017-12-09T22:40:37.516158: step 1911, loss 0.191859, acc 0.945312, prec 0.0735303, recall 0.842644
2017-12-09T22:40:37.823342: step 1912, loss 0.212665, acc 0.921875, prec 0.073534, recall 0.842679
2017-12-09T22:40:38.126696: step 1913, loss 0.170425, acc 0.9375, prec 0.0735765, recall 0.842784
2017-12-09T22:40:38.432863: step 1914, loss 0.154171, acc 0.9375, prec 0.0735651, recall 0.842784
2017-12-09T22:40:38.733862: step 1915, loss 0.173128, acc 0.929688, prec 0.0735702, recall 0.842819
2017-12-09T22:40:39.036487: step 1916, loss 0.159345, acc 0.9375, prec 0.0735768, recall 0.842854
2017-12-09T22:40:39.341786: step 1917, loss 0.137132, acc 0.9375, prec 0.0735654, recall 0.842854
2017-12-09T22:40:39.637119: step 1918, loss 0.162483, acc 0.9375, prec 0.0736258, recall 0.842994
2017-12-09T22:40:39.946259: step 1919, loss 0.141553, acc 0.953125, prec 0.0736711, recall 0.843098
2017-12-09T22:40:40.244101: step 1920, loss 0.106868, acc 0.96875, prec 0.0736654, recall 0.843098
2017-12-09T22:40:40.542650: step 1921, loss 0.942472, acc 0.945312, prec 0.0736569, recall 0.842911
2017-12-09T22:40:40.845362: step 1922, loss 0.0868455, acc 0.960938, prec 0.0737036, recall 0.843015
2017-12-09T22:40:41.145679: step 1923, loss 0.171446, acc 0.953125, prec 0.073713, recall 0.84305
2017-12-09T22:40:41.444828: step 1924, loss 0.180827, acc 0.945312, prec 0.073703, recall 0.84305
2017-12-09T22:40:41.744289: step 1925, loss 0.123913, acc 0.960938, prec 0.0737317, recall 0.84312
2017-12-09T22:40:42.049710: step 1926, loss 0.293057, acc 0.96875, prec 0.0737799, recall 0.843224
2017-12-09T22:40:42.351616: step 1927, loss 0.263822, acc 0.960938, prec 0.0738265, recall 0.843328
2017-12-09T22:40:42.655213: step 1928, loss 0.122243, acc 0.960938, prec 0.0738373, recall 0.843363
2017-12-09T22:40:42.957478: step 1929, loss 0.236786, acc 0.976562, prec 0.0738689, recall 0.843432
2017-12-09T22:40:43.260785: step 1930, loss 0.182146, acc 0.953125, prec 0.0739141, recall 0.843536
2017-12-09T22:40:43.564808: step 1931, loss 0.364694, acc 0.9375, prec 0.0739385, recall 0.843605
2017-12-09T22:40:43.866636: step 1932, loss 0.216492, acc 0.9375, prec 0.0739809, recall 0.843709
2017-12-09T22:40:44.166739: step 1933, loss 0.0961562, acc 0.953125, prec 0.0739723, recall 0.843709
2017-12-09T22:40:44.462232: step 1934, loss 0.0850954, acc 0.953125, prec 0.0739637, recall 0.843709
2017-12-09T22:40:44.760571: step 1935, loss 0.329157, acc 0.96875, prec 0.0740117, recall 0.843812
2017-12-09T22:40:45.059638: step 1936, loss 0.122514, acc 0.96875, prec 0.0740239, recall 0.843846
2017-12-09T22:40:45.361832: step 1937, loss 0.0614298, acc 0.976562, prec 0.0740196, recall 0.843846
2017-12-09T22:40:45.667998: step 1938, loss 0.467453, acc 0.953125, prec 0.0740827, recall 0.843984
2017-12-09T22:40:45.979998: step 1939, loss 0.527978, acc 0.914062, prec 0.0741206, recall 0.844087
2017-12-09T22:40:46.278621: step 1940, loss 0.239275, acc 0.96875, prec 0.0741328, recall 0.844122
2017-12-09T22:40:46.583862: step 1941, loss 0.0580407, acc 0.976562, prec 0.0741822, recall 0.844224
2017-12-09T22:40:46.880138: step 1942, loss 0.0644863, acc 0.976562, prec 0.0742495, recall 0.844361
2017-12-09T22:40:47.184005: step 1943, loss 0.150406, acc 0.960938, prec 0.0742781, recall 0.84443
2017-12-09T22:40:47.486990: step 1944, loss 0.154107, acc 0.953125, prec 0.074341, recall 0.844566
2017-12-09T22:40:47.790341: step 1945, loss 0.141142, acc 0.96875, prec 0.074389, recall 0.844669
2017-12-09T22:40:48.087091: step 1946, loss 0.126343, acc 0.953125, prec 0.0744161, recall 0.844737
2017-12-09T22:40:48.384234: step 1947, loss 0.362555, acc 0.898438, prec 0.0744153, recall 0.844771
2017-12-09T22:40:48.684294: step 1948, loss 0.212987, acc 0.914062, prec 0.074471, recall 0.844907
2017-12-09T22:40:48.983315: step 1949, loss 0.15274, acc 0.929688, prec 0.0744938, recall 0.844975
2017-12-09T22:40:49.278925: step 1950, loss 0.135733, acc 0.953125, prec 0.0744851, recall 0.844975
2017-12-09T22:40:49.579750: step 1951, loss 0.106537, acc 0.984375, prec 0.074518, recall 0.845043
2017-12-09T22:40:49.878883: step 1952, loss 0.151332, acc 0.960938, prec 0.0745465, recall 0.84511
2017-12-09T22:40:50.180533: step 1953, loss 0.247318, acc 0.9375, prec 0.0745707, recall 0.845178
2017-12-09T22:40:50.496099: step 1954, loss 0.173748, acc 0.9375, prec 0.0745592, recall 0.845178
2017-12-09T22:40:50.800825: step 1955, loss 0.262164, acc 0.9375, prec 0.0745656, recall 0.845212
2017-12-09T22:40:51.100747: step 1956, loss 0.106005, acc 0.953125, prec 0.0745748, recall 0.845246
2017-12-09T22:40:51.398731: step 1957, loss 0.11781, acc 0.953125, prec 0.0745661, recall 0.845246
2017-12-09T22:40:51.698093: step 1958, loss 0.0612688, acc 0.96875, prec 0.0745604, recall 0.845246
2017-12-09T22:40:51.998042: step 1959, loss 0.0664414, acc 0.976562, prec 0.0745561, recall 0.845246
2017-12-09T22:40:52.303344: step 1960, loss 1.27692, acc 0.945312, prec 0.0746366, recall 0.84523
2017-12-09T22:40:52.603067: step 1961, loss 0.122604, acc 0.992188, prec 0.074653, recall 0.845264
2017-12-09T22:40:52.906110: step 1962, loss 0.273531, acc 0.984375, prec 0.0747215, recall 0.845399
2017-12-09T22:40:53.214631: step 1963, loss 0.451968, acc 0.960938, prec 0.0747856, recall 0.845534
2017-12-09T22:40:53.518274: step 1964, loss 0.0669325, acc 0.976562, prec 0.0747813, recall 0.845534
2017-12-09T22:40:53.813974: step 1965, loss 0.374838, acc 0.976562, prec 0.0748126, recall 0.845601
2017-12-09T22:40:54.115149: step 1966, loss 0.225047, acc 0.929688, prec 0.0747997, recall 0.845601
2017-12-09T22:40:54.414935: step 1967, loss 0.0836762, acc 0.984375, prec 0.0748502, recall 0.845702
2017-12-09T22:40:54.717173: step 1968, loss 0.0689822, acc 0.976562, prec 0.0748637, recall 0.845735
2017-12-09T22:40:55.015657: step 1969, loss 0.822941, acc 0.914062, prec 0.0749013, recall 0.845836
2017-12-09T22:40:55.316054: step 1970, loss 0.195943, acc 0.945312, prec 0.0749268, recall 0.845903
2017-12-09T22:40:55.613425: step 1971, loss 0.261541, acc 0.9375, prec 0.0749509, recall 0.84597
2017-12-09T22:40:55.911331: step 1972, loss 0.200983, acc 0.929688, prec 0.0749735, recall 0.846037
2017-12-09T22:40:56.918013: step 1973, loss 0.29656, acc 0.914062, prec 0.0749755, recall 0.84607
2017-12-09T22:40:57.325829: step 1974, loss 0.260587, acc 0.898438, prec 0.0749745, recall 0.846104
2017-12-09T22:40:58.235554: step 1975, loss 0.230642, acc 0.898438, prec 0.0749736, recall 0.846137
2017-12-09T22:40:59.218787: step 1976, loss 0.367854, acc 0.898438, prec 0.0749904, recall 0.846204
2017-12-09T22:41:00.255503: step 1977, loss 0.361465, acc 0.882812, prec 0.0750221, recall 0.846304
2017-12-09T22:41:00.670687: step 1978, loss 0.375363, acc 0.9375, prec 0.0750639, recall 0.846404
2017-12-09T22:41:01.049621: step 1979, loss 0.203828, acc 0.945312, prec 0.0751071, recall 0.846504
2017-12-09T22:41:01.361410: step 1980, loss 0.324073, acc 0.90625, prec 0.0751075, recall 0.846537
2017-12-09T22:41:01.692386: step 1981, loss 0.223794, acc 0.921875, prec 0.0751464, recall 0.846636
2017-12-09T22:41:02.016704: step 1982, loss 0.614084, acc 0.914062, prec 0.0751483, recall 0.84667
2017-12-09T22:41:02.317023: step 1983, loss 0.105707, acc 0.960938, prec 0.0751411, recall 0.84667
2017-12-09T22:41:02.627222: step 1984, loss 0.168938, acc 0.9375, prec 0.075165, recall 0.846736
2017-12-09T22:41:02.925516: step 1985, loss 0.195499, acc 0.929688, prec 0.0751698, recall 0.846769
2017-12-09T22:41:03.225141: step 1986, loss 0.1603, acc 0.953125, prec 0.0751789, recall 0.846802
2017-12-09T22:41:03.526011: step 1987, loss 0.166638, acc 0.953125, prec 0.0751702, recall 0.846802
2017-12-09T22:41:03.829521: step 1988, loss 1.78359, acc 0.929688, prec 0.0751942, recall 0.846685
2017-12-09T22:41:04.130845: step 1989, loss 0.143493, acc 0.960938, prec 0.0752579, recall 0.846818
2017-12-09T22:41:04.429621: step 1990, loss 0.4356, acc 0.953125, prec 0.0753379, recall 0.846983
2017-12-09T22:41:04.730743: step 1991, loss 0.224471, acc 0.914062, prec 0.0753574, recall 0.847049
2017-12-09T22:41:04.916578: step 1992, loss 0.118868, acc 0.960784, prec 0.0753545, recall 0.847049
2017-12-09T22:41:05.234652: step 1993, loss 0.190326, acc 0.914062, prec 0.0753564, recall 0.847082
2017-12-09T22:41:05.543598: step 1994, loss 0.147298, acc 0.953125, prec 0.0754009, recall 0.84718
2017-12-09T22:41:05.839921: step 1995, loss 0.231522, acc 0.9375, prec 0.0753893, recall 0.84718
2017-12-09T22:41:06.145674: step 1996, loss 0.2068, acc 0.945312, prec 0.0754677, recall 0.847345
2017-12-09T22:41:06.447255: step 1997, loss 0.285878, acc 0.898438, prec 0.0754843, recall 0.84741
2017-12-09T22:41:06.743502: step 1998, loss 0.139806, acc 0.953125, prec 0.0754757, recall 0.84741
2017-12-09T22:41:07.041903: step 1999, loss 0.202694, acc 0.9375, prec 0.0755526, recall 0.847574
2017-12-09T22:41:07.344228: step 2000, loss 0.21644, acc 0.929688, prec 0.0755749, recall 0.84764
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-2000

2017-12-09T22:41:08.643640: step 2001, loss 0.139872, acc 0.953125, prec 0.0755663, recall 0.84764
2017-12-09T22:41:08.946386: step 2002, loss 0.791836, acc 0.90625, prec 0.0755681, recall 0.84749
2017-12-09T22:41:09.249341: step 2003, loss 0.1442, acc 0.96875, prec 0.0756153, recall 0.847588
2017-12-09T22:41:09.548954: step 2004, loss 0.107837, acc 0.976562, prec 0.0756286, recall 0.847621
2017-12-09T22:41:09.849339: step 2005, loss 0.174388, acc 0.929688, prec 0.0756686, recall 0.847719
2017-12-09T22:41:10.151345: step 2006, loss 0.19311, acc 0.953125, prec 0.0756953, recall 0.847784
2017-12-09T22:41:10.447636: step 2007, loss 0.162859, acc 0.945312, prec 0.0757558, recall 0.847914
2017-12-09T22:41:10.745172: step 2008, loss 0.310306, acc 0.984375, prec 0.0758589, recall 0.848109
2017-12-09T22:41:11.042415: step 2009, loss 0.101917, acc 0.976562, prec 0.0758899, recall 0.848174
2017-12-09T22:41:11.344771: step 2010, loss 0.16986, acc 0.953125, prec 0.0759341, recall 0.848271
2017-12-09T22:41:11.645400: step 2011, loss 0.229123, acc 0.914062, prec 0.0759358, recall 0.848304
2017-12-09T22:41:11.951664: step 2012, loss 0.16985, acc 0.953125, prec 0.0759624, recall 0.848369
2017-12-09T22:41:12.248441: step 2013, loss 0.0763586, acc 0.96875, prec 0.0759743, recall 0.848401
2017-12-09T22:41:12.545264: step 2014, loss 0.188065, acc 0.929688, prec 0.0759612, recall 0.848401
2017-12-09T22:41:12.847188: step 2015, loss 0.12819, acc 0.945312, prec 0.0759687, recall 0.848433
2017-12-09T22:41:13.146806: step 2016, loss 0.132677, acc 0.945312, prec 0.0760114, recall 0.84853
2017-12-09T22:41:13.447749: step 2017, loss 0.387884, acc 0.960938, prec 0.0760923, recall 0.848691
2017-12-09T22:41:13.746945: step 2018, loss 0.0640272, acc 0.976562, prec 0.0761409, recall 0.848788
2017-12-09T22:41:14.046162: step 2019, loss 0.152072, acc 0.960938, prec 0.0762041, recall 0.848916
2017-12-09T22:41:14.343707: step 2020, loss 0.20012, acc 0.984375, prec 0.0762188, recall 0.848948
2017-12-09T22:41:14.641855: step 2021, loss 0.191732, acc 0.960938, prec 0.0762468, recall 0.849013
2017-12-09T22:41:14.940369: step 2022, loss 0.052041, acc 0.96875, prec 0.0762938, recall 0.849109
2017-12-09T22:41:15.237358: step 2023, loss 0.155467, acc 1, prec 0.076329, recall 0.849173
2017-12-09T22:41:15.546693: step 2024, loss 0.0530502, acc 1, prec 0.0763819, recall 0.849269
2017-12-09T22:41:15.847394: step 2025, loss 0.0848876, acc 0.960938, prec 0.0764098, recall 0.849333
2017-12-09T22:41:16.145599: step 2026, loss 0.270735, acc 0.960938, prec 0.0764201, recall 0.849364
2017-12-09T22:41:16.447988: step 2027, loss 0.0777605, acc 0.96875, prec 0.0764143, recall 0.849364
2017-12-09T22:41:16.745058: step 2028, loss 0.320149, acc 0.945312, prec 0.0764921, recall 0.849524
2017-12-09T22:41:17.046487: step 2029, loss 0.544034, acc 0.953125, prec 0.0765537, recall 0.849651
2017-12-09T22:41:17.359281: step 2030, loss 0.381047, acc 0.96875, prec 0.0766359, recall 0.84981
2017-12-09T22:41:17.662621: step 2031, loss 0.143925, acc 0.960938, prec 0.0766462, recall 0.849842
2017-12-09T22:41:17.962526: step 2032, loss 0.227726, acc 0.929688, prec 0.0766506, recall 0.849873
2017-12-09T22:41:18.257887: step 2033, loss 0.130349, acc 0.945312, prec 0.076658, recall 0.849905
2017-12-09T22:41:18.559353: step 2034, loss 0.124102, acc 0.945312, prec 0.0766478, recall 0.849905
2017-12-09T22:41:18.855185: step 2035, loss 0.127308, acc 0.9375, prec 0.0766712, recall 0.849968
2017-12-09T22:41:19.159644: step 2036, loss 0.124085, acc 0.9375, prec 0.0766771, recall 0.85
2017-12-09T22:41:19.458237: step 2037, loss 0.106596, acc 0.960938, prec 0.076705, recall 0.850063
2017-12-09T22:41:19.757365: step 2038, loss 0.179366, acc 0.929688, prec 0.0767094, recall 0.850095
2017-12-09T22:41:20.059430: step 2039, loss 0.1729, acc 0.929688, prec 0.076749, recall 0.85019
2017-12-09T22:41:20.372588: step 2040, loss 0.273098, acc 0.929688, prec 0.0768236, recall 0.850347
2017-12-09T22:41:20.668997: step 2041, loss 0.191019, acc 0.945312, prec 0.076866, recall 0.850442
2017-12-09T22:41:20.970529: step 2042, loss 0.168093, acc 0.960938, prec 0.0768938, recall 0.850505
2017-12-09T22:41:21.272182: step 2043, loss 0.204413, acc 0.945312, prec 0.0769362, recall 0.850599
2017-12-09T22:41:21.572364: step 2044, loss 0.0920025, acc 0.96875, prec 0.076983, recall 0.850693
2017-12-09T22:41:21.874087: step 2045, loss 0.0817952, acc 0.976562, prec 0.0769962, recall 0.850724
2017-12-09T22:41:22.172104: step 2046, loss 0.246066, acc 0.945312, prec 0.077021, recall 0.850787
2017-12-09T22:41:22.469523: step 2047, loss 0.12589, acc 0.960938, prec 0.0770663, recall 0.850881
2017-12-09T22:41:22.772805: step 2048, loss 0.159335, acc 0.953125, prec 0.0771101, recall 0.850975
2017-12-09T22:41:23.073777: step 2049, loss 0.136636, acc 0.976562, prec 0.0771232, recall 0.851006
2017-12-09T22:41:23.370476: step 2050, loss 0.467603, acc 0.96875, prec 0.0771524, recall 0.851068
2017-12-09T22:41:23.675549: step 2051, loss 1.43682, acc 0.984375, prec 0.077186, recall 0.850953
2017-12-09T22:41:23.975786: step 2052, loss 0.145195, acc 0.953125, prec 0.0772122, recall 0.851015
2017-12-09T22:41:24.275611: step 2053, loss 0.0484821, acc 0.96875, prec 0.0772414, recall 0.851077
2017-12-09T22:41:24.573626: step 2054, loss 0.174181, acc 0.929688, prec 0.0772282, recall 0.851077
2017-12-09T22:41:24.878265: step 2055, loss 0.252615, acc 0.945312, prec 0.077288, recall 0.851202
2017-12-09T22:41:25.178291: step 2056, loss 0.165231, acc 0.945312, prec 0.0772952, recall 0.851233
2017-12-09T22:41:25.477496: step 2057, loss 0.104186, acc 0.96875, prec 0.0773244, recall 0.851295
2017-12-09T22:41:25.774795: step 2058, loss 0.369069, acc 0.9375, prec 0.0773826, recall 0.851419
2017-12-09T22:41:26.083015: step 2059, loss 0.129594, acc 0.96875, prec 0.0773943, recall 0.85145
2017-12-09T22:41:26.381504: step 2060, loss 0.182066, acc 0.953125, prec 0.0774205, recall 0.851512
2017-12-09T22:41:26.685227: step 2061, loss 0.118527, acc 0.96875, prec 0.0774671, recall 0.851605
2017-12-09T22:41:26.991963: step 2062, loss 0.182352, acc 0.953125, prec 0.0774757, recall 0.851636
2017-12-09T22:41:27.295635: step 2063, loss 0.68262, acc 0.929688, prec 0.0774975, recall 0.851698
2017-12-09T22:41:27.601467: step 2064, loss 0.235514, acc 0.929688, prec 0.0775367, recall 0.85179
2017-12-09T22:41:27.905171: step 2065, loss 0.293305, acc 0.90625, prec 0.0775191, recall 0.85179
2017-12-09T22:41:28.206739: step 2066, loss 0.194434, acc 0.929688, prec 0.0775233, recall 0.851821
2017-12-09T22:41:28.504296: step 2067, loss 0.134781, acc 0.960938, prec 0.077516, recall 0.851821
2017-12-09T22:41:28.804542: step 2068, loss 0.297277, acc 0.875, prec 0.0775624, recall 0.851944
2017-12-09T22:41:29.105865: step 2069, loss 0.142783, acc 0.929688, prec 0.0775666, recall 0.851975
2017-12-09T22:41:29.403080: step 2070, loss 0.163342, acc 0.953125, prec 0.0775578, recall 0.851975
2017-12-09T22:41:29.709390: step 2071, loss 0.0673327, acc 0.96875, prec 0.0775519, recall 0.851975
2017-12-09T22:41:30.018185: step 2072, loss 0.149055, acc 0.945312, prec 0.0775417, recall 0.851975
2017-12-09T22:41:30.325147: step 2073, loss 0.129722, acc 0.960938, prec 0.0775518, recall 0.852006
2017-12-09T22:41:30.629803: step 2074, loss 0.225598, acc 0.953125, prec 0.0775604, recall 0.852037
2017-12-09T22:41:30.926893: step 2075, loss 0.136378, acc 0.96875, prec 0.0776069, recall 0.852129
2017-12-09T22:41:31.223979: step 2076, loss 0.0814728, acc 0.960938, prec 0.0775996, recall 0.852129
2017-12-09T22:41:31.527176: step 2077, loss 0.52772, acc 0.976562, prec 0.0776126, recall 0.852159
2017-12-09T22:41:31.832795: step 2078, loss 0.0879646, acc 0.984375, prec 0.0776446, recall 0.852221
2017-12-09T22:41:32.134136: step 2079, loss 0.0478096, acc 0.984375, prec 0.0777114, recall 0.852343
2017-12-09T22:41:32.436113: step 2080, loss 0.141775, acc 0.953125, prec 0.0777549, recall 0.852435
2017-12-09T22:41:32.737156: step 2081, loss 0.0280943, acc 0.992188, prec 0.0777534, recall 0.852435
2017-12-09T22:41:33.037273: step 2082, loss 0.247829, acc 0.945312, prec 0.0777606, recall 0.852466
2017-12-09T22:41:33.338789: step 2083, loss 0.356164, acc 0.976562, prec 0.077791, recall 0.852527
2017-12-09T22:41:33.642621: step 2084, loss 0.0870051, acc 0.976562, prec 0.077804, recall 0.852557
2017-12-09T22:41:33.941888: step 2085, loss 0.137822, acc 0.953125, prec 0.0778301, recall 0.852619
2017-12-09T22:41:34.257363: step 2086, loss 0.269116, acc 0.96875, prec 0.077859, recall 0.852679
2017-12-09T22:41:34.559898: step 2087, loss 0.0341175, acc 0.984375, prec 0.0778735, recall 0.85271
2017-12-09T22:41:34.858272: step 2088, loss 0.147297, acc 0.960938, prec 0.077901, recall 0.852771
2017-12-09T22:41:35.165893: step 2089, loss 0.115207, acc 0.96875, prec 0.0779125, recall 0.852801
2017-12-09T22:41:35.483566: step 2090, loss 0.0725828, acc 0.984375, prec 0.0779444, recall 0.852862
2017-12-09T22:41:35.787831: step 2091, loss 0.107884, acc 0.953125, prec 0.077953, recall 0.852893
2017-12-09T22:41:36.085298: step 2092, loss 0.193729, acc 0.984375, prec 0.0779674, recall 0.852923
2017-12-09T22:41:36.386500: step 2093, loss 0.0783476, acc 0.992188, prec 0.077966, recall 0.852923
2017-12-09T22:41:36.685853: step 2094, loss 0.590035, acc 0.96875, prec 0.0780297, recall 0.853044
2017-12-09T22:41:36.986580: step 2095, loss 0.155838, acc 0.96875, prec 0.0780934, recall 0.853166
2017-12-09T22:41:37.293030: step 2096, loss 0.216033, acc 0.945312, prec 0.0781179, recall 0.853226
2017-12-09T22:41:37.590351: step 2097, loss 0.128695, acc 0.953125, prec 0.0781265, recall 0.853256
2017-12-09T22:41:37.893207: step 2098, loss 0.188125, acc 0.96875, prec 0.0781554, recall 0.853317
2017-12-09T22:41:38.190433: step 2099, loss 0.192353, acc 0.953125, prec 0.0781813, recall 0.853377
2017-12-09T22:41:38.491060: step 2100, loss 0.111524, acc 0.945312, prec 0.078171, recall 0.853377

Evaluation:
2017-12-09T22:41:43.247740: step 2100, loss 2.88382, acc 0.961317, prec 0.0787714, recall 0.840024

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-2100

2017-12-09T22:41:44.462849: step 2101, loss 0.204974, acc 0.96875, prec 0.0788, recall 0.840088
2017-12-09T22:41:44.762037: step 2102, loss 0.210653, acc 0.984375, prec 0.0788661, recall 0.840215
2017-12-09T22:41:45.064935: step 2103, loss 0.247493, acc 0.976562, prec 0.0788789, recall 0.840247
2017-12-09T22:41:45.365742: step 2104, loss 0.245181, acc 0.960938, prec 0.0789232, recall 0.840343
2017-12-09T22:41:45.673798: step 2105, loss 0.170875, acc 0.929688, prec 0.0789444, recall 0.840406
2017-12-09T22:41:45.976166: step 2106, loss 0.148464, acc 0.953125, prec 0.0789528, recall 0.840438
2017-12-09T22:41:46.275096: step 2107, loss 0.156669, acc 0.945312, prec 0.0789769, recall 0.840502
2017-12-09T22:41:46.579068: step 2108, loss 0.135054, acc 0.9375, prec 0.0789651, recall 0.840502
2017-12-09T22:41:46.878186: step 2109, loss 0.117155, acc 0.96875, prec 0.0789764, recall 0.840534
2017-12-09T22:41:47.177995: step 2110, loss 0.127348, acc 0.960938, prec 0.0790035, recall 0.840597
2017-12-09T22:41:47.484027: step 2111, loss 0.0756111, acc 0.976562, prec 0.078999, recall 0.840597
2017-12-09T22:41:47.783874: step 2112, loss 0.0660979, acc 0.976562, prec 0.0790118, recall 0.840629
2017-12-09T22:41:48.080497: step 2113, loss 0.125726, acc 0.96875, prec 0.0790059, recall 0.840629
2017-12-09T22:41:48.385786: step 2114, loss 0.108844, acc 0.953125, prec 0.0790143, recall 0.84066
2017-12-09T22:41:48.695006: step 2115, loss 0.367079, acc 0.960938, prec 0.079093, recall 0.840819
2017-12-09T22:41:48.996748: step 2116, loss 0.181724, acc 0.984375, prec 0.0791761, recall 0.840977
2017-12-09T22:41:49.302192: step 2117, loss 0.128103, acc 0.960938, prec 0.0792031, recall 0.84104
2017-12-09T22:41:49.610898: step 2118, loss 0.108782, acc 0.953125, prec 0.0791942, recall 0.84104
2017-12-09T22:41:49.916434: step 2119, loss 0.304996, acc 0.960938, prec 0.079204, recall 0.841071
2017-12-09T22:41:50.219893: step 2120, loss 0.199907, acc 1, prec 0.0792384, recall 0.841134
2017-12-09T22:41:50.530316: step 2121, loss 0.288722, acc 0.921875, prec 0.0792924, recall 0.84126
2017-12-09T22:41:50.827582: step 2122, loss 0.234196, acc 0.992188, prec 0.0793253, recall 0.841323
2017-12-09T22:41:51.129128: step 2123, loss 0.116694, acc 0.953125, prec 0.0793337, recall 0.841355
2017-12-09T22:41:51.427115: step 2124, loss 0.476914, acc 0.945312, prec 0.0793749, recall 0.841449
2017-12-09T22:41:51.726541: step 2125, loss 0.098189, acc 0.960938, prec 0.0793846, recall 0.84148
2017-12-09T22:41:52.025674: step 2126, loss 0.0912594, acc 0.984375, prec 0.0793989, recall 0.841512
2017-12-09T22:41:52.322283: step 2127, loss 0.130064, acc 0.96875, prec 0.0793929, recall 0.841512
2017-12-09T22:41:52.623109: step 2128, loss 0.162688, acc 0.929688, prec 0.079414, recall 0.841574
2017-12-09T22:41:52.927746: step 2129, loss 0.148808, acc 0.953125, prec 0.0794394, recall 0.841637
2017-12-09T22:41:53.225080: step 2130, loss 0.116053, acc 0.960938, prec 0.0794835, recall 0.841731
2017-12-09T22:41:53.523908: step 2131, loss 0.127082, acc 0.960938, prec 0.0795276, recall 0.841825
2017-12-09T22:41:53.821990: step 2132, loss 0.389845, acc 0.945312, prec 0.0795516, recall 0.841887
2017-12-09T22:41:54.123882: step 2133, loss 0.220944, acc 0.9375, prec 0.0796084, recall 0.842012
2017-12-09T22:41:54.424250: step 2134, loss 0.250665, acc 0.921875, prec 0.0795935, recall 0.842012
2017-12-09T22:41:54.719599: step 2135, loss 0.145959, acc 0.96875, prec 0.0796219, recall 0.842074
2017-12-09T22:41:55.018111: step 2136, loss 0.192908, acc 0.9375, prec 0.0796444, recall 0.842136
2017-12-09T22:41:55.317512: step 2137, loss 0.291503, acc 0.929688, prec 0.0796653, recall 0.842199
2017-12-09T22:41:55.616001: step 2138, loss 0.124749, acc 0.976562, prec 0.0796609, recall 0.842199
2017-12-09T22:41:55.919110: step 2139, loss 0.167834, acc 0.960938, prec 0.0796706, recall 0.84223
2017-12-09T22:41:56.221398: step 2140, loss 0.191444, acc 0.9375, prec 0.079693, recall 0.842292
2017-12-09T22:41:56.524157: step 2141, loss 0.0801189, acc 0.96875, prec 0.0797213, recall 0.842354
2017-12-09T22:41:56.829657: step 2142, loss 0.0338024, acc 0.992188, prec 0.0797199, recall 0.842354
2017-12-09T22:41:57.132369: step 2143, loss 0.122164, acc 0.984375, prec 0.0797683, recall 0.842447
2017-12-09T22:41:57.431639: step 2144, loss 0.101641, acc 0.984375, prec 0.0798168, recall 0.84254
2017-12-09T22:41:57.734966: step 2145, loss 0.150404, acc 0.976562, prec 0.079898, recall 0.842694
2017-12-09T22:41:58.036414: step 2146, loss 0.0735624, acc 0.976562, prec 0.0799278, recall 0.842756
2017-12-09T22:41:58.337088: step 2147, loss 0.0856483, acc 0.960938, prec 0.0799375, recall 0.842787
2017-12-09T22:41:58.641975: step 2148, loss 0.123323, acc 0.976562, prec 0.0799844, recall 0.84288
2017-12-09T22:41:58.945601: step 2149, loss 0.139388, acc 0.976562, prec 0.0800826, recall 0.843064
2017-12-09T22:41:59.250834: step 2150, loss 0.0408796, acc 0.984375, prec 0.0800797, recall 0.843064
2017-12-09T22:41:59.554468: step 2151, loss 0.0423836, acc 0.992188, prec 0.0800953, recall 0.843095
2017-12-09T22:41:59.852675: step 2152, loss 0.101171, acc 0.984375, prec 0.0801094, recall 0.843126
2017-12-09T22:42:00.167915: step 2153, loss 0.0733487, acc 0.984375, prec 0.0801064, recall 0.843126
2017-12-09T22:42:00.469838: step 2154, loss 0.119787, acc 0.984375, prec 0.0801548, recall 0.843218
2017-12-09T22:42:00.769601: step 2155, loss 0.521476, acc 0.96875, prec 0.0801831, recall 0.843279
2017-12-09T22:42:01.070236: step 2156, loss 0.048256, acc 0.984375, prec 0.0801972, recall 0.84331
2017-12-09T22:42:01.370990: step 2157, loss 0.0248052, acc 0.992188, prec 0.0801957, recall 0.84331
2017-12-09T22:42:01.673064: step 2158, loss 1.63066, acc 0.960938, prec 0.080308, recall 0.843524
2017-12-09T22:42:01.971382: step 2159, loss 0.0913913, acc 0.984375, prec 0.0803734, recall 0.843646
2017-12-09T22:42:02.272932: step 2160, loss 0.210193, acc 0.9375, prec 0.0803957, recall 0.843707
2017-12-09T22:42:02.571747: step 2161, loss 0.200274, acc 0.953125, prec 0.0804038, recall 0.843738
2017-12-09T22:42:02.874479: step 2162, loss 0.0898076, acc 0.96875, prec 0.080432, recall 0.843799
2017-12-09T22:42:03.171023: step 2163, loss 0.138204, acc 0.9375, prec 0.0804542, recall 0.84386
2017-12-09T22:42:03.471048: step 2164, loss 0.128779, acc 0.945312, prec 0.0804437, recall 0.84386
2017-12-09T22:42:03.775475: step 2165, loss 0.233429, acc 0.953125, prec 0.0804519, recall 0.84389
2017-12-09T22:42:04.085035: step 2166, loss 0.217296, acc 0.929688, prec 0.0804726, recall 0.843951
2017-12-09T22:42:04.379573: step 2167, loss 0.187059, acc 0.914062, prec 0.0804561, recall 0.843951
2017-12-09T22:42:04.679281: step 2168, loss 0.345577, acc 0.890625, prec 0.0804694, recall 0.844012
2017-12-09T22:42:04.978302: step 2169, loss 0.448739, acc 0.875, prec 0.0804625, recall 0.844042
2017-12-09T22:42:05.287476: step 2170, loss 0.262365, acc 0.914062, prec 0.0804632, recall 0.844072
2017-12-09T22:42:05.605036: step 2171, loss 0.40462, acc 0.890625, prec 0.0805446, recall 0.844254
2017-12-09T22:42:05.906627: step 2172, loss 0.149693, acc 0.960938, prec 0.0806054, recall 0.844375
2017-12-09T22:42:06.206866: step 2173, loss 0.272687, acc 0.882812, prec 0.0806, recall 0.844406
2017-12-09T22:42:06.502067: step 2174, loss 0.153269, acc 0.953125, prec 0.0806592, recall 0.844526
2017-12-09T22:42:06.803517: step 2175, loss 0.239523, acc 0.9375, prec 0.0806643, recall 0.844557
2017-12-09T22:42:07.104500: step 2176, loss 0.23758, acc 0.921875, prec 0.0806664, recall 0.844587
2017-12-09T22:42:07.406103: step 2177, loss 0.135783, acc 0.945312, prec 0.080673, recall 0.844617
2017-12-09T22:42:07.711758: step 2178, loss 0.23928, acc 0.9375, prec 0.0806951, recall 0.844677
2017-12-09T22:42:08.015446: step 2179, loss 0.124899, acc 0.953125, prec 0.0806861, recall 0.844677
2017-12-09T22:42:08.311894: step 2180, loss 0.0853192, acc 0.96875, prec 0.0807142, recall 0.844737
2017-12-09T22:42:08.619561: step 2181, loss 0.12665, acc 0.96875, prec 0.0807082, recall 0.844737
2017-12-09T22:42:08.918787: step 2182, loss 0.108551, acc 0.96875, prec 0.0807192, recall 0.844767
2017-12-09T22:42:09.217175: step 2183, loss 0.0516661, acc 0.984375, prec 0.0807503, recall 0.844828
2017-12-09T22:42:09.516711: step 2184, loss 0.132862, acc 0.992188, prec 0.0807828, recall 0.844888
2017-12-09T22:42:09.817746: step 2185, loss 0.0577336, acc 0.984375, prec 0.0808139, recall 0.844948
2017-12-09T22:42:10.121265: step 2186, loss 0.0818362, acc 0.976562, prec 0.0808264, recall 0.844978
2017-12-09T22:42:10.418485: step 2187, loss 0.484497, acc 0.976562, prec 0.0808389, recall 0.845008
2017-12-09T22:42:10.721748: step 2188, loss 0.912999, acc 0.96875, prec 0.0808515, recall 0.844874
2017-12-09T22:42:11.026743: step 2189, loss 0.0391695, acc 1, prec 0.0808855, recall 0.844934
2017-12-09T22:42:11.326883: step 2190, loss 0.327726, acc 0.992188, prec 0.080935, recall 0.845024
2017-12-09T22:42:11.632454: step 2191, loss 0.126273, acc 0.96875, prec 0.080963, recall 0.845084
2017-12-09T22:42:11.931450: step 2192, loss 0.461971, acc 0.945312, prec 0.0809866, recall 0.845144
2017-12-09T22:42:12.234639: step 2193, loss 0.14985, acc 0.992188, prec 0.0810021, recall 0.845174
2017-12-09T22:42:12.536755: step 2194, loss 0.133457, acc 0.96875, prec 0.0810131, recall 0.845204
2017-12-09T22:42:12.833783: step 2195, loss 0.106728, acc 0.976562, prec 0.0810086, recall 0.845204
2017-12-09T22:42:13.134602: step 2196, loss 0.157012, acc 0.960938, prec 0.0810521, recall 0.845293
2017-12-09T22:42:13.431709: step 2197, loss 0.170973, acc 0.945312, prec 0.0810586, recall 0.845323
2017-12-09T22:42:13.731780: step 2198, loss 0.185738, acc 0.929688, prec 0.0810621, recall 0.845353
2017-12-09T22:42:14.035104: step 2199, loss 0.10954, acc 0.984375, prec 0.0810931, recall 0.845412
2017-12-09T22:42:14.332904: step 2200, loss 0.0899297, acc 0.96875, prec 0.0811041, recall 0.845442
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-2200

2017-12-09T22:42:15.656319: step 2201, loss 0.230667, acc 0.953125, prec 0.08118, recall 0.845591
2017-12-09T22:42:15.959106: step 2202, loss 0.21521, acc 0.921875, prec 0.0812159, recall 0.84568
2017-12-09T22:42:16.258065: step 2203, loss 0.0961001, acc 0.976562, prec 0.0812284, recall 0.84571
2017-12-09T22:42:16.556876: step 2204, loss 0.187899, acc 0.960938, prec 0.0812379, recall 0.84574
2017-12-09T22:42:16.857685: step 2205, loss 0.145146, acc 0.960938, prec 0.0812473, recall 0.845769
2017-12-09T22:42:17.160118: step 2206, loss 0.324564, acc 0.914062, prec 0.0812308, recall 0.845769
2017-12-09T22:42:17.457589: step 2207, loss 0.0983483, acc 0.984375, prec 0.0812278, recall 0.845769
2017-12-09T22:42:17.755539: step 2208, loss 0.285533, acc 0.945312, prec 0.0812852, recall 0.845888
2017-12-09T22:42:18.059925: step 2209, loss 0.0952132, acc 0.976562, prec 0.0812807, recall 0.845888
2017-12-09T22:42:18.361797: step 2210, loss 0.121314, acc 0.984375, prec 0.0813116, recall 0.845947
2017-12-09T22:42:18.664143: step 2211, loss 0.346792, acc 0.960938, prec 0.081338, recall 0.846006
2017-12-09T22:42:18.965552: step 2212, loss 0.659446, acc 0.960938, prec 0.081349, recall 0.845873
2017-12-09T22:42:19.265677: step 2213, loss 0.137668, acc 0.960938, prec 0.0813754, recall 0.845932
2017-12-09T22:42:19.563972: step 2214, loss 0.0579688, acc 0.976562, prec 0.0814048, recall 0.845992
2017-12-09T22:42:19.859305: step 2215, loss 0.121478, acc 0.953125, prec 0.0814127, recall 0.846021
2017-12-09T22:42:20.160238: step 2216, loss 0.276661, acc 0.929688, prec 0.0814162, recall 0.846051
2017-12-09T22:42:20.468315: step 2217, loss 0.0697434, acc 0.976562, prec 0.0814117, recall 0.846051
2017-12-09T22:42:20.768129: step 2218, loss 0.0656537, acc 0.984375, prec 0.0814086, recall 0.846051
2017-12-09T22:42:21.065944: step 2219, loss 0.124187, acc 0.96875, prec 0.0814196, recall 0.84608
2017-12-09T22:42:21.374472: step 2220, loss 0.107481, acc 0.96875, prec 0.0814475, recall 0.846139
2017-12-09T22:42:21.675827: step 2221, loss 0.245682, acc 0.9375, prec 0.0814693, recall 0.846198
2017-12-09T22:42:21.972825: step 2222, loss 0.682048, acc 0.945312, prec 0.0815604, recall 0.846375
2017-12-09T22:42:22.273535: step 2223, loss 1.39507, acc 0.960938, prec 0.0815713, recall 0.846242
2017-12-09T22:42:22.574234: step 2224, loss 0.34852, acc 0.921875, prec 0.0815902, recall 0.846301
2017-12-09T22:42:22.875328: step 2225, loss 0.139117, acc 0.960938, prec 0.0815996, recall 0.84633
2017-12-09T22:42:23.175263: step 2226, loss 0.200486, acc 0.921875, prec 0.0816353, recall 0.846418
2017-12-09T22:42:23.473962: step 2227, loss 0.145293, acc 0.945312, prec 0.0816417, recall 0.846448
2017-12-09T22:42:23.774743: step 2228, loss 0.267125, acc 0.921875, prec 0.0816436, recall 0.846477
2017-12-09T22:42:24.071124: step 2229, loss 0.237702, acc 0.921875, prec 0.0816454, recall 0.846506
2017-12-09T22:42:24.366625: step 2230, loss 0.164249, acc 0.945312, prec 0.0816856, recall 0.846594
2017-12-09T22:42:24.669251: step 2231, loss 0.224671, acc 0.921875, prec 0.0816875, recall 0.846623
2017-12-09T22:42:24.965935: step 2232, loss 0.201592, acc 0.9375, prec 0.0817093, recall 0.846682
2017-12-09T22:42:25.260201: step 2233, loss 0.515202, acc 0.90625, prec 0.0817081, recall 0.846711
2017-12-09T22:42:25.566806: step 2234, loss 0.198128, acc 0.945312, prec 0.0816976, recall 0.846711
2017-12-09T22:42:25.868114: step 2235, loss 0.346138, acc 0.945312, prec 0.0817378, recall 0.846799
2017-12-09T22:42:26.169872: step 2236, loss 0.192965, acc 0.914062, prec 0.0817719, recall 0.846886
2017-12-09T22:42:26.471434: step 2237, loss 0.544199, acc 0.921875, prec 0.0818075, recall 0.846974
2017-12-09T22:42:26.775727: step 2238, loss 0.244186, acc 0.9375, prec 0.0818123, recall 0.847003
2017-12-09T22:42:27.074901: step 2239, loss 0.250773, acc 0.914062, prec 0.0818633, recall 0.847119
2017-12-09T22:42:27.373659: step 2240, loss 2.50087, acc 0.914062, prec 0.0818482, recall 0.846958
2017-12-09T22:42:27.557260: step 2241, loss 0.0448739, acc 1, prec 0.0818482, recall 0.846958
2017-12-09T22:42:27.864970: step 2242, loss 0.257041, acc 0.890625, prec 0.0818272, recall 0.846958
2017-12-09T22:42:28.165844: step 2243, loss 0.207425, acc 0.921875, prec 0.0818459, recall 0.847016
2017-12-09T22:42:28.463658: step 2244, loss 0.218956, acc 0.914062, prec 0.0818799, recall 0.847104
2017-12-09T22:42:28.763678: step 2245, loss 0.169442, acc 0.945312, prec 0.0818694, recall 0.847104
2017-12-09T22:42:29.062451: step 2246, loss 0.26339, acc 0.90625, prec 0.0818514, recall 0.847104
2017-12-09T22:42:29.359234: step 2247, loss 0.270457, acc 0.914062, prec 0.0818854, recall 0.847191
2017-12-09T22:42:29.661082: step 2248, loss 1.79252, acc 0.960938, prec 0.0819131, recall 0.847088
2017-12-09T22:42:29.965407: step 2249, loss 0.274747, acc 0.890625, prec 0.0819089, recall 0.847117
2017-12-09T22:42:30.268245: step 2250, loss 0.251567, acc 0.929688, prec 0.0819122, recall 0.847146
2017-12-09T22:42:30.578180: step 2251, loss 0.261656, acc 0.890625, prec 0.0818912, recall 0.847146
2017-12-09T22:42:30.874827: step 2252, loss 0.274684, acc 0.914062, prec 0.0818747, recall 0.847146
2017-12-09T22:42:31.178222: step 2253, loss 0.281682, acc 0.898438, prec 0.081872, recall 0.847175
2017-12-09T22:42:31.476707: step 2254, loss 0.186788, acc 0.90625, prec 0.0819044, recall 0.847262
2017-12-09T22:42:31.777316: step 2255, loss 0.14933, acc 0.960938, prec 0.0819138, recall 0.847291
2017-12-09T22:42:32.076849: step 2256, loss 0.199594, acc 0.929688, prec 0.0819171, recall 0.84732
2017-12-09T22:42:32.378857: step 2257, loss 0.318004, acc 0.851562, prec 0.0819054, recall 0.847349
2017-12-09T22:42:32.675858: step 2258, loss 0.252657, acc 0.898438, prec 0.0819195, recall 0.847406
2017-12-09T22:42:32.979874: step 2259, loss 0.321132, acc 0.929688, prec 0.0819228, recall 0.847435
2017-12-09T22:42:33.284293: step 2260, loss 0.276632, acc 0.929688, prec 0.0819261, recall 0.847464
2017-12-09T22:42:33.580902: step 2261, loss 0.172206, acc 0.960938, prec 0.0819522, recall 0.847522
2017-12-09T22:42:33.877703: step 2262, loss 0.976383, acc 0.929688, prec 0.0820074, recall 0.847477
2017-12-09T22:42:34.180113: step 2263, loss 0.119884, acc 0.96875, prec 0.0820014, recall 0.847477
2017-12-09T22:42:34.477675: step 2264, loss 0.114447, acc 0.960938, prec 0.0820275, recall 0.847534
2017-12-09T22:42:34.777084: step 2265, loss 0.203192, acc 0.90625, prec 0.082043, recall 0.847592
2017-12-09T22:42:35.073778: step 2266, loss 0.182161, acc 0.960938, prec 0.0820691, recall 0.84765
2017-12-09T22:42:35.391163: step 2267, loss 0.194858, acc 0.953125, prec 0.0821104, recall 0.847736
2017-12-09T22:42:35.695070: step 2268, loss 0.155709, acc 0.9375, prec 0.0821152, recall 0.847765
2017-12-09T22:42:35.990520: step 2269, loss 0.166139, acc 0.9375, prec 0.0821032, recall 0.847765
2017-12-09T22:42:36.293100: step 2270, loss 0.263622, acc 0.96875, prec 0.082114, recall 0.847793
2017-12-09T22:42:36.595204: step 2271, loss 0.112966, acc 0.960938, prec 0.0821065, recall 0.847793
2017-12-09T22:42:36.897157: step 2272, loss 0.199198, acc 0.945312, prec 0.0821127, recall 0.847822
2017-12-09T22:42:37.197150: step 2273, loss 0.140973, acc 0.953125, prec 0.0821372, recall 0.847879
2017-12-09T22:42:37.495095: step 2274, loss 0.04115, acc 0.984375, prec 0.082151, recall 0.847908
2017-12-09T22:42:37.789856: step 2275, loss 0.339458, acc 0.96875, prec 0.0821618, recall 0.847937
2017-12-09T22:42:38.091710: step 2276, loss 0.103727, acc 0.976562, prec 0.0821573, recall 0.847937
2017-12-09T22:42:38.393612: step 2277, loss 0.124946, acc 0.96875, prec 0.0821848, recall 0.847994
2017-12-09T22:42:38.697664: step 2278, loss 0.273701, acc 0.96875, prec 0.082229, recall 0.84808
2017-12-09T22:42:39.000026: step 2279, loss 0.259727, acc 0.976562, prec 0.0822413, recall 0.848108
2017-12-09T22:42:39.302377: step 2280, loss 0.0696165, acc 0.976562, prec 0.0822703, recall 0.848166
2017-12-09T22:42:39.608831: step 2281, loss 0.0776732, acc 0.984375, prec 0.0823175, recall 0.848251
2017-12-09T22:42:39.905367: step 2282, loss 0.202944, acc 0.945312, prec 0.082307, recall 0.848251
2017-12-09T22:42:40.207442: step 2283, loss 0.330714, acc 0.96875, prec 0.0823512, recall 0.848337
2017-12-09T22:42:40.514400: step 2284, loss 0.16537, acc 0.953125, prec 0.0823924, recall 0.848422
2017-12-09T22:42:40.815356: step 2285, loss 0.156459, acc 0.976562, prec 0.0824381, recall 0.848508
2017-12-09T22:42:41.114704: step 2286, loss 0.0976881, acc 0.96875, prec 0.0824488, recall 0.848536
2017-12-09T22:42:41.423930: step 2287, loss 0.161923, acc 0.976562, prec 0.0824778, recall 0.848593
2017-12-09T22:42:41.732897: step 2288, loss 0.742624, acc 0.984375, prec 0.082525, recall 0.848678
2017-12-09T22:42:42.033623: step 2289, loss 0.297468, acc 0.984375, prec 0.0825387, recall 0.848706
2017-12-09T22:42:42.337465: step 2290, loss 0.449124, acc 0.976562, prec 0.0825844, recall 0.848791
2017-12-09T22:42:42.634462: step 2291, loss 0.19461, acc 0.960938, prec 0.0826604, recall 0.848933
2017-12-09T22:42:42.935063: step 2292, loss 0.148574, acc 0.929688, prec 0.0826803, recall 0.84899
2017-12-09T22:42:43.232359: step 2293, loss 0.175828, acc 0.960938, prec 0.0826728, recall 0.84899
2017-12-09T22:42:43.535276: step 2294, loss 0.115822, acc 0.953125, prec 0.0826638, recall 0.84899
2017-12-09T22:42:43.830927: step 2295, loss 0.105755, acc 0.960938, prec 0.0826729, recall 0.849018
2017-12-09T22:42:44.127384: step 2296, loss 0.254704, acc 0.914062, prec 0.0826898, recall 0.849074
2017-12-09T22:42:44.430028: step 2297, loss 0.324364, acc 0.882812, prec 0.0827006, recall 0.849131
2017-12-09T22:42:44.729158: step 2298, loss 0.281683, acc 0.890625, prec 0.0827129, recall 0.849187
2017-12-09T22:42:45.026477: step 2299, loss 0.220714, acc 0.914062, prec 0.0827464, recall 0.849272
2017-12-09T22:42:45.325605: step 2300, loss 0.149916, acc 0.9375, prec 0.0827678, recall 0.849328
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-2300

2017-12-09T22:42:46.761529: step 2301, loss 0.144014, acc 0.953125, prec 0.0827921, recall 0.849384
2017-12-09T22:42:47.056579: step 2302, loss 0.0698518, acc 0.976562, prec 0.0828043, recall 0.849412
2017-12-09T22:42:47.362119: step 2303, loss 0.612571, acc 0.929688, prec 0.0829075, recall 0.849609
2017-12-09T22:42:47.663819: step 2304, loss 0.151391, acc 0.960938, prec 0.0829166, recall 0.849637
2017-12-09T22:42:47.963879: step 2305, loss 0.131639, acc 0.960938, prec 0.0829424, recall 0.849693
2017-12-09T22:42:48.259020: step 2306, loss 0.210589, acc 0.953125, prec 0.0829834, recall 0.849777
2017-12-09T22:42:48.560799: step 2307, loss 0.129989, acc 0.929688, prec 0.0829698, recall 0.849777
2017-12-09T22:42:48.865634: step 2308, loss 0.120191, acc 0.929688, prec 0.0829729, recall 0.849805
2017-12-09T22:42:49.173620: step 2309, loss 0.779726, acc 0.976562, prec 0.0830199, recall 0.84973
2017-12-09T22:42:49.474098: step 2310, loss 0.250013, acc 0.921875, prec 0.0830048, recall 0.84973
2017-12-09T22:42:49.773117: step 2311, loss 0.0606239, acc 0.976562, prec 0.0830169, recall 0.849758
2017-12-09T22:42:50.072119: step 2312, loss 0.145426, acc 0.960938, prec 0.0830594, recall 0.849842
2017-12-09T22:42:50.381626: step 2313, loss 0.180433, acc 0.953125, prec 0.0830503, recall 0.849842
2017-12-09T22:42:50.684920: step 2314, loss 0.208351, acc 0.953125, prec 0.0830579, recall 0.84987
2017-12-09T22:42:50.985081: step 2315, loss 0.170215, acc 0.9375, prec 0.0830459, recall 0.84987
2017-12-09T22:42:51.291331: step 2316, loss 0.114415, acc 0.976562, prec 0.083058, recall 0.849898
2017-12-09T22:42:51.587718: step 2317, loss 0.140405, acc 0.953125, prec 0.0830656, recall 0.849926
2017-12-09T22:42:51.889068: step 2318, loss 0.0907533, acc 0.960938, prec 0.0831246, recall 0.850037
2017-12-09T22:42:52.201918: step 2319, loss 0.588927, acc 0.976562, prec 0.08317, recall 0.850121
2017-12-09T22:42:52.507680: step 2320, loss 0.162944, acc 0.960938, prec 0.0831957, recall 0.850176
2017-12-09T22:42:52.808342: step 2321, loss 0.15877, acc 0.945312, prec 0.0832018, recall 0.850204
2017-12-09T22:42:53.108635: step 2322, loss 0.135827, acc 0.96875, prec 0.083229, recall 0.850259
2017-12-09T22:42:53.409706: step 2323, loss 0.0911264, acc 0.992188, prec 0.083294, recall 0.85037
2017-12-09T22:42:53.712264: step 2324, loss 0.074757, acc 0.976562, prec 0.0833061, recall 0.850398
2017-12-09T22:42:54.011875: step 2325, loss 0.166664, acc 0.945312, prec 0.0833122, recall 0.850426
2017-12-09T22:42:54.312764: step 2326, loss 0.0387672, acc 0.984375, prec 0.0833092, recall 0.850426
2017-12-09T22:42:54.613334: step 2327, loss 0.229803, acc 0.914062, prec 0.0832925, recall 0.850426
2017-12-09T22:42:54.913114: step 2328, loss 0.340349, acc 0.96875, prec 0.083353, recall 0.850536
2017-12-09T22:42:55.211740: step 2329, loss 0.0985633, acc 0.976562, prec 0.0833983, recall 0.850619
2017-12-09T22:42:55.514729: step 2330, loss 0.0509261, acc 0.976562, prec 0.0833938, recall 0.850619
2017-12-09T22:42:55.813878: step 2331, loss 0.106277, acc 0.96875, prec 0.0834375, recall 0.850702
2017-12-09T22:42:56.113187: step 2332, loss 0.199191, acc 0.960938, prec 0.0834964, recall 0.850812
2017-12-09T22:42:56.419892: step 2333, loss 0.081103, acc 0.96875, prec 0.083507, recall 0.85084
2017-12-09T22:42:56.719927: step 2334, loss 0.13001, acc 0.953125, prec 0.0835145, recall 0.850867
2017-12-09T22:42:57.023593: step 2335, loss 0.107946, acc 0.96875, prec 0.083525, recall 0.850895
2017-12-09T22:42:57.324760: step 2336, loss 1.79599, acc 0.945312, prec 0.0835507, recall 0.850636
2017-12-09T22:42:57.626502: step 2337, loss 0.0690004, acc 0.96875, prec 0.0835446, recall 0.850636
2017-12-09T22:42:57.929978: step 2338, loss 0.124638, acc 0.960938, prec 0.0836366, recall 0.850801
2017-12-09T22:42:58.232424: step 2339, loss 0.152557, acc 0.960938, prec 0.0836788, recall 0.850884
2017-12-09T22:42:58.531785: step 2340, loss 0.139821, acc 0.953125, prec 0.0836863, recall 0.850911
2017-12-09T22:42:58.839270: step 2341, loss 0.135353, acc 0.976562, prec 0.0837813, recall 0.851076
2017-12-09T22:42:59.143413: step 2342, loss 0.192474, acc 0.960938, prec 0.08384, recall 0.851185
2017-12-09T22:42:59.439368: step 2343, loss 0.124501, acc 0.953125, prec 0.0838641, recall 0.85124
2017-12-09T22:42:59.738604: step 2344, loss 0.372181, acc 0.898438, prec 0.0839272, recall 0.851376
2017-12-09T22:43:00.045493: step 2345, loss 0.19861, acc 0.921875, prec 0.0839452, recall 0.851431
2017-12-09T22:43:00.349402: step 2346, loss 0.396958, acc 0.960938, prec 0.0839707, recall 0.851485
2017-12-09T22:43:00.649647: step 2347, loss 0.148876, acc 0.960938, prec 0.0839631, recall 0.851485
2017-12-09T22:43:00.951045: step 2348, loss 0.239594, acc 0.929688, prec 0.0839826, recall 0.85154
2017-12-09T22:43:01.253670: step 2349, loss 0.372032, acc 0.9375, prec 0.0840201, recall 0.851621
2017-12-09T22:43:01.554647: step 2350, loss 0.138273, acc 0.9375, prec 0.0840245, recall 0.851648
2017-12-09T22:43:01.854576: step 2351, loss 0.160439, acc 0.953125, prec 0.084065, recall 0.85173
2017-12-09T22:43:02.158824: step 2352, loss 0.267197, acc 0.914062, prec 0.0840649, recall 0.851757
2017-12-09T22:43:02.459512: step 2353, loss 0.289474, acc 0.929688, prec 0.0840678, recall 0.851784
2017-12-09T22:43:02.754780: step 2354, loss 0.155589, acc 0.9375, prec 0.0840556, recall 0.851784
2017-12-09T22:43:03.051642: step 2355, loss 0.173562, acc 0.9375, prec 0.08406, recall 0.851811
2017-12-09T22:43:03.347538: step 2356, loss 0.172729, acc 0.96875, prec 0.084087, recall 0.851865
2017-12-09T22:43:03.642989: step 2357, loss 0.201815, acc 0.953125, prec 0.084111, recall 0.85192
2017-12-09T22:43:03.948251: step 2358, loss 0.168807, acc 0.96875, prec 0.0841049, recall 0.85192
2017-12-09T22:43:04.247482: step 2359, loss 0.223244, acc 0.945312, prec 0.0841108, recall 0.851947
2017-12-09T22:43:04.547492: step 2360, loss 0.115161, acc 0.976562, prec 0.0841062, recall 0.851947
2017-12-09T22:43:04.845061: step 2361, loss 0.054899, acc 0.984375, prec 0.0841197, recall 0.851974
2017-12-09T22:43:05.141374: step 2362, loss 0.221911, acc 0.976562, prec 0.0841648, recall 0.852055
2017-12-09T22:43:05.465096: step 2363, loss 0.466339, acc 0.9375, prec 0.0841857, recall 0.852109
2017-12-09T22:43:05.764116: step 2364, loss 0.457636, acc 0.953125, prec 0.0842096, recall 0.852163
2017-12-09T22:43:06.058868: step 2365, loss 0.0430858, acc 0.992188, prec 0.0842411, recall 0.852217
2017-12-09T22:43:06.357023: step 2366, loss 0.170615, acc 0.976562, prec 0.0842861, recall 0.852298
2017-12-09T22:43:06.658092: step 2367, loss 0.046837, acc 0.992188, prec 0.0843011, recall 0.852325
2017-12-09T22:43:06.955143: step 2368, loss 0.0904324, acc 0.960938, prec 0.08431, recall 0.852351
2017-12-09T22:43:07.254121: step 2369, loss 0.0926106, acc 0.976562, prec 0.0843219, recall 0.852378
2017-12-09T22:43:07.553761: step 2370, loss 0.0880406, acc 0.984375, prec 0.0843684, recall 0.852459
2017-12-09T22:43:07.856774: step 2371, loss 0.0876816, acc 0.984375, prec 0.0843819, recall 0.852486
2017-12-09T22:43:08.153084: step 2372, loss 0.142161, acc 0.945312, prec 0.0844207, recall 0.852566
2017-12-09T22:43:08.456756: step 2373, loss 0.203694, acc 0.96875, prec 0.0844971, recall 0.8527
2017-12-09T22:43:08.760068: step 2374, loss 0.0873597, acc 0.976562, prec 0.0844926, recall 0.8527
2017-12-09T22:43:09.059798: step 2375, loss 0.0365149, acc 0.984375, prec 0.084506, recall 0.852727
2017-12-09T22:43:09.359159: step 2376, loss 0.0266458, acc 1, prec 0.084506, recall 0.852727
2017-12-09T22:43:09.664611: step 2377, loss 0.127371, acc 0.960938, prec 0.0844984, recall 0.852727
2017-12-09T22:43:09.968699: step 2378, loss 0.0850866, acc 0.976562, prec 0.0844938, recall 0.852727
2017-12-09T22:43:10.265130: step 2379, loss 0.248791, acc 0.953125, prec 0.0845012, recall 0.852754
2017-12-09T22:43:10.565345: step 2380, loss 0.140894, acc 0.945312, prec 0.084507, recall 0.852781
2017-12-09T22:43:10.867654: step 2381, loss 0.142836, acc 0.984375, prec 0.0845535, recall 0.852861
2017-12-09T22:43:11.164778: step 2382, loss 0.0822671, acc 0.96875, prec 0.0845474, recall 0.852861
2017-12-09T22:43:11.464934: step 2383, loss 0.0625635, acc 0.992188, prec 0.0846118, recall 0.852968
2017-12-09T22:43:11.764061: step 2384, loss 0.136789, acc 0.984375, prec 0.0846417, recall 0.853021
2017-12-09T22:43:12.063922: step 2385, loss 0.148599, acc 0.960938, prec 0.084667, recall 0.853075
2017-12-09T22:43:12.365506: step 2386, loss 0.149721, acc 0.984375, prec 0.0847134, recall 0.853154
2017-12-09T22:43:12.667472: step 2387, loss 0.256521, acc 0.96875, prec 0.0847238, recall 0.853181
2017-12-09T22:43:12.964161: step 2388, loss 0.461836, acc 0.960938, prec 0.0847491, recall 0.853234
2017-12-09T22:43:13.267663: step 2389, loss 0.160432, acc 0.976562, prec 0.084794, recall 0.853314
2017-12-09T22:43:13.568220: step 2390, loss 0.100029, acc 0.976562, prec 0.0848388, recall 0.853394
2017-12-09T22:43:13.869890: step 2391, loss 0.233269, acc 0.96875, prec 0.0848821, recall 0.853473
2017-12-09T22:43:14.171137: step 2392, loss 0.0338663, acc 0.992188, prec 0.0848805, recall 0.853473
2017-12-09T22:43:14.471172: step 2393, loss 0.0648074, acc 0.96875, prec 0.0849403, recall 0.853579
2017-12-09T22:43:14.773100: step 2394, loss 0.171914, acc 0.960938, prec 0.0849985, recall 0.853685
2017-12-09T22:43:15.075039: step 2395, loss 0.0856531, acc 0.984375, prec 0.0850448, recall 0.853764
2017-12-09T22:43:15.378194: step 2396, loss 0.0801724, acc 0.960938, prec 0.08507, recall 0.853817
2017-12-09T22:43:15.677048: step 2397, loss 0.0977958, acc 0.96875, prec 0.0851133, recall 0.853896
2017-12-09T22:43:15.975675: step 2398, loss 0.39595, acc 0.953125, prec 0.085137, recall 0.853949
2017-12-09T22:43:16.272592: step 2399, loss 0.16564, acc 0.96875, prec 0.0851802, recall 0.854028
2017-12-09T22:43:16.573734: step 2400, loss 0.038441, acc 1, prec 0.0852295, recall 0.854107

Evaluation:
2017-12-09T22:43:21.328631: step 2400, loss 2.80608, acc 0.95877, prec 0.085727, recall 0.842539

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_1/1512876607/checkpoints/model-2400

2017-12-09T22:43:22.594146: step 2401, loss 0.146793, acc 0.960938, prec 0.085752, recall 0.842594
2017-12-09T22:43:22.898596: step 2402, loss 0.445614, acc 0.976562, prec 0.08578, recall 0.842649
2017-12-09T22:43:23.200312: step 2403, loss 0.199788, acc 0.953125, prec 0.0857872, recall 0.842677
2017-12-09T22:43:23.502937: step 2404, loss 0.26188, acc 0.90625, prec 0.0857688, recall 0.842677
2017-12-09T22:43:23.812662: step 2405, loss 0.133141, acc 0.960938, prec 0.0857775, recall 0.842704
2017-12-09T22:43:24.111047: step 2406, loss 0.116834, acc 0.960938, prec 0.0858024, recall 0.84276
2017-12-09T22:43:24.408769: step 2407, loss 0.255794, acc 0.984375, prec 0.0858482, recall 0.842842
2017-12-09T22:43:24.709079: step 2408, loss 0.0292528, acc 1, prec 0.0858645, recall 0.84287
2017-12-09T22:43:25.009595: step 2409, loss 0.287508, acc 0.960938, prec 0.0858895, recall 0.842925
2017-12-09T22:43:25.314827: step 2410, loss 0.200852, acc 0.960938, prec 0.0859307, recall 0.843007
2017-12-09T22:43:25.615810: step 2411, loss 0.0826794, acc 0.976562, prec 0.0859749, recall 0.843089
2017-12-09T22:43:25.917523: step 2412, loss 0.200142, acc 0.953125, prec 0.085982, recall 0.843117
2017-12-09T22:43:26.224205: step 2413, loss 0.266186, acc 0.960938, prec 0.0859907, recall 0.843144
2017-12-09T22:43:26.521611: step 2414, loss 0.0962202, acc 0.960938, prec 0.0860156, recall 0.843199
2017-12-09T22:43:26.823372: step 2415, loss 0.129704, acc 0.929688, prec 0.0860181, recall 0.843226
2017-12-09T22:43:27.128724: step 2416, loss 0.0973996, acc 0.984375, prec 0.0860475, recall 0.843281
2017-12-09T22:43:27.436903: step 2417, loss 0.0924985, acc 0.976562, prec 0.0860592, recall 0.843308
2017-12-09T22:43:27.740545: step 2418, loss 0.111104, acc 0.976562, prec 0.0861034, recall 0.84339
2017-12-09T22:43:28.036188: step 2419, loss 0.0940867, acc 0.976562, prec 0.0861477, recall 0.843472
2017-12-09T22:43:28.335029: step 2420, loss 0.279913, acc 0.960938, prec 0.0861888, recall 0.843554
2017-12-09T22:43:28.637616: step 2421, loss 0.919067, acc 0.929688, prec 0.0861928, recall 0.843434
2017-12-09T22:43:28.940591: step 2422, loss 0.053988, acc 0.984375, prec 0.0861897, recall 0.843434
2017-12-09T22:43:29.239575: step 2423, loss 0.153898, acc 0.945312, prec 0.0861952, recall 0.843462
2017-12-09T22:43:29.536271: step 2424, loss 0.316123, acc 0.929688, prec 0.0862302, recall 0.843543
2017-12-09T22:43:29.837290: step 2425, loss 0.11496, acc 0.96875, prec 0.0862403, recall 0.843571
2017-12-09T22:43:30.144787: step 2426, loss 0.182846, acc 0.992188, prec 0.0862876, recall 0.843652
2017-12-09T22:43:30.445919: step 2427, loss 0.109432, acc 0.953125, prec 0.0863108, recall 0.843707
2017-12-09T22:43:30.747851: step 2428, loss 0.208109, acc 0.929688, prec 0.0863295, recall 0.843761
2017-12-09T22:43:31.050951: step 2429, loss 0.0768404, acc 0.96875, prec 0.0863559, recall 0.843815
2017-12-09T22:43:31.350363: step 2430, loss 0.0955984, acc 0.960938, prec 0.0863644, recall 0.843842
2017-12-09T22:43:31.647046: step 2431, loss 0.209167, acc 0.945312, prec 0.0863537, recall 0.843842
2017-12-09T22:43:31.946098: step 2432, loss 0.124879, acc 0.945312, prec 0.0863754, recall 0.843897
2017-12-09T22:43:32.245394: step 2433, loss 0.229128, acc 0.960938, prec 0.0864002, recall 0.843951
2017-12-09T22:43:32.547852: step 2434, loss 0.610638, acc 0.960938, prec 0.086425, recall 0.844005
2017-12-09T22:43:32.849252: step 2435, loss 0.134127, acc 0.960938, prec 0.0864173, recall 0.844005
2017-12-09T22:43:33.155625: step 2436, loss 0.157596, acc 0.96875, prec 0.0864274, recall 0.844032
2017-12-09T22:43:33.457056: step 2437, loss 0.132687, acc 0.960938, prec 0.0864684, recall 0.844113
2017-12-09T22:43:33.754918: step 2438, loss 0.114452, acc 0.96875, prec 0.0864947, recall 0.844167
2017-12-09T22:43:34.054577: step 2439, loss 0.233577, acc 0.945312, prec 0.0865002, recall 0.844194
2017-12-09T22:43:34.355906: step 2440, loss 0.168668, acc 0.953125, prec 0.0865234, recall 0.844248
2017-12-09T22:43:34.653763: step 2441, loss 0.154969, acc 0.953125, prec 0.0865304, recall 0.844275
2017-12-09T22:43:34.951770: step 2442, loss 0.162057, acc 0.9375, prec 0.0865506, recall 0.844329
2017-12-09T22:43:35.255460: step 2443, loss 0.675611, acc 0.960938, prec 0.086624, recall 0.844464
2017-12-09T22:43:35.564823: step 2444, loss 0.162645, acc 0.953125, prec 0.0866471, recall 0.844517
2017-12-09T22:43:35.866085: step 2445, loss 0.24114, acc 0.898438, prec 0.0866596, recall 0.844571
2017-12-09T22:43:36.165979: step 2446, loss 0.187065, acc 0.953125, prec 0.0866828, recall 0.844625
2017-12-09T22:43:36.466827: step 2447, loss 0.229851, acc 0.914062, prec 0.0867144, recall 0.844705
2017-12-09T22:43:36.768118: step 2448, loss 0.0749588, acc 0.96875, prec 0.0867245, recall 0.844732
2017-12-09T22:43:37.068740: step 2449, loss 0.241005, acc 0.96875, prec 0.0867345, recall 0.844759
2017-12-09T22:43:37.367534: step 2450, loss 0.190337, acc 0.945312, prec 0.0867561, recall 0.844813
2017-12-09T22:43:37.665720: step 2451, loss 0.109323, acc 0.960938, prec 0.0867484, recall 0.844813
2017-12-09T22:43:37.963767: step 2452, loss 0.0487261, acc 0.992188, prec 0.0867631, recall 0.84484
2017-12-09T22:43:38.263772: step 2453, loss 0.228674, acc 0.9375, prec 0.0867832, recall 0.844893
2017-12-09T22:43:38.563245: step 2454, loss 0.169689, acc 0.960938, prec 0.0867755, recall 0.844893
2017-12-09T22:43:38.861203: step 2455, loss 0.10998, acc 0.984375, prec 0.0868048, recall 0.844947
2017-12-09T22:43:39.162929: step 2456, loss 0.124069, acc 0.976562, prec 0.0868163, recall 0.844973
2017-12-09T22:43:39.465615: step 2457, loss 0.137085, acc 0.960938, prec 0.0868248, recall 0.845
2017-12-09T22:43:39.763159: step 2458, loss 0.0840824, acc 0.96875, prec 0.086851, recall 0.845053
2017-12-09T22:43:40.062157: step 2459, loss 0.349329, acc 0.96875, prec 0.0868934, recall 0.845133
2017-12-09T22:43:40.363991: step 2460, loss 0.14233, acc 0.976562, prec 0.0869049, recall 0.84516
2017-12-09T22:43:40.668965: step 2461, loss 0.646026, acc 0.960938, prec 0.0869781, recall 0.845293
2017-12-09T22:43:40.974193: step 2462, loss 0.136138, acc 0.96875, prec 0.0870043, recall 0.845347
2017-12-09T22:43:41.271782: step 2463, loss 0.135048, acc 0.960938, prec 0.0869965, recall 0.845347
2017-12-09T22:43:41.574217: step 2464, loss 0.260034, acc 0.984375, prec 0.0870904, recall 0.845506
2017-12-09T22:43:41.881828: step 2465, loss 0.100944, acc 0.976562, prec 0.0871343, recall 0.845586
2017-12-09T22:43:42.183532: step 2466, loss 0.32326, acc 0.953125, prec 0.0871735, recall 0.845665
2017-12-09T22:43:42.488365: step 2467, loss 1.15529, acc 0.96875, prec 0.087185, recall 0.845547
2017-12-09T22:43:42.794768: step 2468, loss 0.0829211, acc 0.96875, prec 0.0871788, recall 0.845547
2017-12-09T22:43:43.096907: step 2469, loss 0.247497, acc 0.9375, prec 0.087215, recall 0.845626
2017-12-09T22:43:43.397125: step 2470, loss 0.232441, acc 0.960938, prec 0.0872557, recall 0.845705
2017-12-09T22:43:43.697372: step 2471, loss 0.254524, acc 0.929688, prec 0.0872579, recall 0.845732
2017-12-09T22:43:43.996882: step 2472, loss 0.375016, acc 0.90625, prec 0.0872717, recall 0.845785
2017-12-09T22:43:44.301896: step 2473, loss 0.167183, acc 0.9375, prec 0.0872755, recall 0.845811
2017-12-09T22:43:44.610170: step 2474, loss 0.317933, acc 0.882812, prec 0.0873491, recall 0.84597
2017-12-09T22:43:44.911739: step 2475, loss 0.326577, acc 0.90625, prec 0.0873467, recall 0.845996
2017-12-09T22:43:45.216801: step 2476, loss 0.210548, acc 0.96875, prec 0.0873889, recall 0.846075
2017-12-09T22:43:45.519960: step 2477, loss 0.284888, acc 0.898438, prec 0.0874333, recall 0.84618
2017-12-09T22:43:45.824004: step 2478, loss 0.186682, acc 0.921875, prec 0.0874179, recall 0.84618
2017-12-09T22:43:46.125026: step 2479, loss 0.236152, acc 0.898438, prec 0.0874462, recall 0.846259
2017-12-09T22:43:46.423471: step 2480, loss 0.285377, acc 0.890625, prec 0.0874407, recall 0.846285
2017-12-09T22:43:46.723306: step 2481, loss 0.120775, acc 0.960938, prec 0.0874652, recall 0.846338
2017-12-09T22:43:47.026685: step 2482, loss 0.153562, acc 0.945312, prec 0.0874866, recall 0.84639
2017-12-09T22:43:47.325162: step 2483, loss 0.189284, acc 0.921875, prec 0.0875355, recall 0.846495
2017-12-09T22:43:47.627967: step 2484, loss 0.130193, acc 0.945312, prec 0.0875891, recall 0.8466
2017-12-09T22:43:47.926773: step 2485, loss 0.166113, acc 0.960938, prec 0.0875974, recall 0.846626
2017-12-09T22:43:48.229427: step 2486, loss 0.142892, acc 0.945312, prec 0.0876027, recall 0.846652
2017-12-09T22:43:48.527777: step 2487, loss 0.808841, acc 0.960938, prec 0.0876287, recall 0.84656
2017-12-09T22:43:48.831074: step 2488, loss 0.0699813, acc 0.984375, prec 0.0876738, recall 0.846638
2017-12-09T22:43:49.131766: step 2489, loss 0.0569982, acc 0.992188, prec 0.0877205, recall 0.846717
2017-12-09T22:43:49.312030: step 2490, loss 0.0678348, acc 0.960784, prec 0.0877174, recall 0.846717
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 128
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_128_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430

Start training
2017-12-09T22:43:53.265294: step 1, loss 7.5288, acc 0.0859375, prec 0.0168067, recall 1
2017-12-09T22:43:53.566056: step 2, loss 3.44437, acc 0.328125, prec 0.0193237, recall 1
2017-12-09T22:43:53.868061: step 3, loss 1.47141, acc 0.664062, prec 0.016, recall 1
2017-12-09T22:43:54.170250: step 4, loss 3.33047, acc 0.914062, prec 0.0153846, recall 0.8
2017-12-09T22:43:54.471744: step 5, loss 6.89014, acc 0.953125, prec 0.0150943, recall 0.666667
2017-12-09T22:43:54.773184: step 6, loss 4.10743, acc 0.976562, prec 0.0149813, recall 0.571429
2017-12-09T22:43:55.083586: step 7, loss 0.0706944, acc 0.976562, prec 0.0148148, recall 0.571429
2017-12-09T22:43:55.387710: step 8, loss 18.9083, acc 0.945312, prec 0.0145455, recall 0.444444
2017-12-09T22:43:55.692909: step 9, loss 14.0388, acc 0.929688, prec 0.0142349, recall 0.333333
2017-12-09T22:43:56.000737: step 10, loss 14.0201, acc 0.90625, prec 0.0138408, recall 0.25
2017-12-09T22:43:56.307556: step 11, loss 7.62279, acc 0.789062, prec 0.0127796, recall 0.210526
2017-12-09T22:43:56.608059: step 12, loss 1.88055, acc 0.585938, prec 0.013624, recall 0.25
2017-12-09T22:43:56.907748: step 13, loss 5.63058, acc 0.46875, prec 0.0137931, recall 0.272727
2017-12-09T22:43:57.218934: step 14, loss 3.6003, acc 0.34375, prec 0.0153551, recall 0.333333
2017-12-09T22:43:57.522869: step 15, loss 5.38405, acc 0.171875, prec 0.0127592, recall 0.333333
2017-12-09T22:43:57.818839: step 16, loss 6.29956, acc 0.109375, prec 0.0121294, recall 0.36
2017-12-09T22:43:58.117896: step 17, loss 7.03273, acc 0.117188, prec 0.013986, recall 0.428571
2017-12-09T22:43:58.410889: step 18, loss 6.47112, acc 0.132812, prec 0.0134021, recall 0.448276
2017-12-09T22:43:58.710451: step 19, loss 7.54472, acc 0.0234375, prec 0.0118721, recall 0.448276
2017-12-09T22:43:59.010328: step 20, loss 6.57927, acc 0.148438, prec 0.0124378, recall 0.483871
2017-12-09T22:43:59.304729: step 21, loss 6.21192, acc 0.140625, prec 0.0121488, recall 0.5
2017-12-09T22:43:59.604600: step 22, loss 5.14856, acc 0.210938, prec 0.0133709, recall 0.542857
2017-12-09T22:43:59.902720: step 23, loss 3.17921, acc 0.351562, prec 0.012633, recall 0.542857
2017-12-09T22:44:00.218305: step 24, loss 8.46387, acc 0.40625, prec 0.0126662, recall 0.526316
2017-12-09T22:44:00.520586: step 25, loss 6.03383, acc 0.515625, prec 0.0133983, recall 0.536585
2017-12-09T22:44:00.824662: step 26, loss 3.72771, acc 0.453125, prec 0.0140105, recall 0.545455
2017-12-09T22:44:01.130448: step 27, loss 8.17607, acc 0.5625, prec 0.0152542, recall 0.55102
2017-12-09T22:44:01.431838: step 28, loss 3.69133, acc 0.578125, prec 0.0158904, recall 0.557692
2017-12-09T22:44:01.729007: step 29, loss 12.2735, acc 0.539062, prec 0.0169761, recall 0.561404
2017-12-09T22:44:02.030484: step 30, loss 5.69525, acc 0.507812, prec 0.0174538, recall 0.557377
2017-12-09T22:44:02.333815: step 31, loss 6.63251, acc 0.421875, prec 0.0173096, recall 0.555556
2017-12-09T22:44:02.633323: step 32, loss 3.74443, acc 0.445312, prec 0.0172002, recall 0.553846
2017-12-09T22:44:02.931123: step 33, loss 4.61082, acc 0.375, prec 0.017931, recall 0.565217
2017-12-09T22:44:03.234236: step 34, loss 5.47252, acc 0.273438, prec 0.0176367, recall 0.56338
2017-12-09T22:44:03.529980: step 35, loss 5.12841, acc 0.273438, prec 0.0181895, recall 0.581081
2017-12-09T22:44:03.829194: step 36, loss 5.02562, acc 0.296875, prec 0.0175296, recall 0.573333
2017-12-09T22:44:04.125442: step 37, loss 5.20069, acc 0.265625, prec 0.017654, recall 0.584416
2017-12-09T22:44:04.424867: step 38, loss 5.96615, acc 0.210938, prec 0.0173519, recall 0.589744
2017-12-09T22:44:04.721442: step 39, loss 5.08191, acc 0.296875, prec 0.0171408, recall 0.594937
2017-12-09T22:44:05.014760: step 40, loss 4.18794, acc 0.359375, prec 0.0180339, recall 0.614458
2017-12-09T22:44:05.347442: step 41, loss 4.02252, acc 0.367188, prec 0.0182068, recall 0.623529
2017-12-09T22:44:05.650309: step 42, loss 3.83952, acc 0.398438, prec 0.0183946, recall 0.632184
2017-12-09T22:44:05.946519: step 43, loss 3.55484, acc 0.414062, prec 0.0182648, recall 0.636364
2017-12-09T22:44:06.244084: step 44, loss 2.68819, acc 0.578125, prec 0.0182634, recall 0.640449
2017-12-09T22:44:06.542091: step 45, loss 6.87907, acc 0.5625, prec 0.018262, recall 0.630435
2017-12-09T22:44:06.841025: step 46, loss 1.38538, acc 0.6875, prec 0.0183401, recall 0.634409
2017-12-09T22:44:07.139732: step 47, loss 6.05146, acc 0.679688, prec 0.0187231, recall 0.628866
2017-12-09T22:44:07.436047: step 48, loss 4.5946, acc 0.570312, prec 0.0184179, recall 0.622449
2017-12-09T22:44:07.739053: step 49, loss 7.79877, acc 0.625, prec 0.0187444, recall 0.623762
2017-12-09T22:44:08.040331: step 50, loss 8.10724, acc 0.664062, prec 0.0190896, recall 0.625
2017-12-09T22:44:08.343631: step 51, loss 1.96383, acc 0.625, prec 0.0193922, recall 0.632075
2017-12-09T22:44:08.656537: step 52, loss 2.62981, acc 0.585938, prec 0.0191046, recall 0.626168
2017-12-09T22:44:08.952166: step 53, loss 2.06982, acc 0.609375, prec 0.0193875, recall 0.633027
2017-12-09T22:44:09.259232: step 54, loss 3.3287, acc 0.554688, prec 0.0190871, recall 0.627273
2017-12-09T22:44:09.554437: step 55, loss 2.23189, acc 0.523438, prec 0.0187704, recall 0.627273
2017-12-09T22:44:09.848815: step 56, loss 2.46903, acc 0.578125, prec 0.0187617, recall 0.630631
2017-12-09T22:44:10.144023: step 57, loss 2.40685, acc 0.632812, prec 0.0195664, recall 0.643478
2017-12-09T22:44:10.443477: step 58, loss 2.17275, acc 0.617188, prec 0.0193161, recall 0.643478
2017-12-09T22:44:10.741728: step 59, loss 1.92847, acc 0.640625, prec 0.0190869, recall 0.643478
2017-12-09T22:44:11.041600: step 60, loss 1.52413, acc 0.664062, prec 0.0188776, recall 0.643478
2017-12-09T22:44:11.342979: step 61, loss 1.40653, acc 0.648438, prec 0.0189107, recall 0.646552
2017-12-09T22:44:11.640988: step 62, loss 1.12118, acc 0.734375, prec 0.0194854, recall 0.655462
2017-12-09T22:44:11.946732: step 63, loss 3.83817, acc 0.757812, prec 0.0200694, recall 0.658537
2017-12-09T22:44:12.249776: step 64, loss 5.59091, acc 0.75, prec 0.0199164, recall 0.653226
2017-12-09T22:44:12.556781: step 65, loss 6.69824, acc 0.757812, prec 0.0202488, recall 0.653543
2017-12-09T22:44:12.858897: step 66, loss 7.8941, acc 0.710938, prec 0.0203095, recall 0.651163
2017-12-09T22:44:13.159812: step 67, loss 5.31112, acc 0.726562, prec 0.0203837, recall 0.643939
2017-12-09T22:44:13.471082: step 68, loss 6.0433, acc 0.703125, prec 0.0206749, recall 0.639706
2017-12-09T22:44:13.780228: step 69, loss 4.78233, acc 0.671875, prec 0.0207059, recall 0.637681
2017-12-09T22:44:14.078378: step 70, loss 3.48816, acc 0.609375, prec 0.021153, recall 0.640845
2017-12-09T22:44:14.380262: step 71, loss 2.65263, acc 0.523438, prec 0.02153, recall 0.648276
2017-12-09T22:44:14.689846: step 72, loss 3.11265, acc 0.515625, prec 0.0212285, recall 0.648276
2017-12-09T22:44:14.991233: step 73, loss 3.67975, acc 0.445312, prec 0.0208935, recall 0.648276
2017-12-09T22:44:15.292275: step 74, loss 3.28874, acc 0.4375, prec 0.0207787, recall 0.650685
2017-12-09T22:44:15.592394: step 75, loss 5.41789, acc 0.40625, prec 0.0212857, recall 0.655629
2017-12-09T22:44:15.893637: step 76, loss 3.43249, acc 0.421875, prec 0.0215736, recall 0.662338
2017-12-09T22:44:16.194016: step 77, loss 3.70706, acc 0.375, prec 0.0216216, recall 0.666667
2017-12-09T22:44:16.497970: step 78, loss 4.31222, acc 0.375, prec 0.021468, recall 0.66879
2017-12-09T22:44:16.794668: step 79, loss 3.36098, acc 0.421875, prec 0.0217391, recall 0.675
2017-12-09T22:44:17.090150: step 80, loss 3.06372, acc 0.53125, prec 0.0214797, recall 0.675
2017-12-09T22:44:17.385974: step 81, loss 2.57948, acc 0.484375, prec 0.0212014, recall 0.675
2017-12-09T22:44:17.690426: step 82, loss 3.01232, acc 0.585938, prec 0.0211774, recall 0.67284
2017-12-09T22:44:17.990713: step 83, loss 2.14812, acc 0.617188, prec 0.0213544, recall 0.676829
2017-12-09T22:44:18.291737: step 84, loss 1.74441, acc 0.679688, prec 0.021374, recall 0.678788
2017-12-09T22:44:18.597378: step 85, loss 1.57722, acc 0.710938, prec 0.0217803, recall 0.684524
2017-12-09T22:44:18.906122: step 86, loss 3.16781, acc 0.78125, prec 0.0216695, recall 0.680473
2017-12-09T22:44:19.214094: step 87, loss 5.95571, acc 0.71875, prec 0.0220767, recall 0.682081
2017-12-09T22:44:19.516633: step 88, loss 3.54188, acc 0.703125, prec 0.021929, recall 0.674286
2017-12-09T22:44:19.818965: step 89, loss 2.42558, acc 0.734375, prec 0.021976, recall 0.672316
2017-12-09T22:44:20.119808: step 90, loss 0.820637, acc 0.75, prec 0.0218469, recall 0.672316
2017-12-09T22:44:20.433876: step 91, loss 1.3643, acc 0.703125, prec 0.0227645, recall 0.68306
2017-12-09T22:44:20.739626: step 92, loss 13.1174, acc 0.640625, prec 0.0229325, recall 0.682796
2017-12-09T22:44:21.047485: step 93, loss 3.67247, acc 0.734375, prec 0.0231473, recall 0.68254
2017-12-09T22:44:21.351901: step 94, loss 5.80521, acc 0.65625, prec 0.0229701, recall 0.678947
2017-12-09T22:44:21.660071: step 95, loss 1.8214, acc 0.664062, prec 0.0231408, recall 0.682292
2017-12-09T22:44:21.963584: step 96, loss 3.5768, acc 0.71875, prec 0.0231701, recall 0.680412
2017-12-09T22:44:22.271069: step 97, loss 1.78851, acc 0.617188, prec 0.0231425, recall 0.682051
2017-12-09T22:44:22.572981: step 98, loss 1.98238, acc 0.585938, prec 0.0232678, recall 0.685279
2017-12-09T22:44:22.870980: step 99, loss 4.68403, acc 0.640625, prec 0.0235897, recall 0.686567
2017-12-09T22:44:23.171368: step 100, loss 2.74194, acc 0.5, prec 0.0233345, recall 0.686567
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-100

2017-12-09T22:44:24.392389: step 101, loss 2.86585, acc 0.492188, prec 0.0232441, recall 0.688119
2017-12-09T22:44:24.696459: step 102, loss 2.74469, acc 0.507812, prec 0.0231635, recall 0.689655
2017-12-09T22:44:24.991875: step 103, loss 3.30836, acc 0.484375, prec 0.0229133, recall 0.689655
2017-12-09T22:44:25.291316: step 104, loss 5.59828, acc 0.445312, prec 0.0228118, recall 0.687805
2017-12-09T22:44:25.592847: step 105, loss 2.8642, acc 0.492188, prec 0.0227309, recall 0.68932
2017-12-09T22:44:25.890778: step 106, loss 2.41319, acc 0.539062, prec 0.0226732, recall 0.690821
2017-12-09T22:44:26.190318: step 107, loss 1.62214, acc 0.609375, prec 0.0226486, recall 0.692308
2017-12-09T22:44:26.492946: step 108, loss 3.02195, acc 0.546875, prec 0.0224474, recall 0.688995
2017-12-09T22:44:26.790065: step 109, loss 5.10793, acc 0.710938, prec 0.0226251, recall 0.688679
2017-12-09T22:44:27.089963: step 110, loss 8.25968, acc 0.671875, prec 0.0226328, recall 0.686916
2017-12-09T22:44:27.393872: step 111, loss 6.41451, acc 0.671875, prec 0.0226438, recall 0.682028
2017-12-09T22:44:27.699776: step 112, loss 5.50312, acc 0.585938, prec 0.02291, recall 0.683258
2017-12-09T22:44:27.999177: step 113, loss 1.95837, acc 0.59375, prec 0.0228778, recall 0.684685
2017-12-09T22:44:28.300755: step 114, loss 2.25069, acc 0.664062, prec 0.0228802, recall 0.683036
2017-12-09T22:44:28.599833: step 115, loss 4.62093, acc 0.578125, prec 0.0228487, recall 0.678414
2017-12-09T22:44:28.905620: step 116, loss 2.25006, acc 0.539062, prec 0.0227941, recall 0.679825
2017-12-09T22:44:29.200985: step 117, loss 3.25091, acc 0.445312, prec 0.0227008, recall 0.681223
2017-12-09T22:44:29.497614: step 118, loss 2.86885, acc 0.5, prec 0.0226323, recall 0.682609
2017-12-09T22:44:29.800277: step 119, loss 2.82494, acc 0.445312, prec 0.022403, recall 0.682609
2017-12-09T22:44:30.108220: step 120, loss 2.8601, acc 0.453125, prec 0.0223195, recall 0.683983
2017-12-09T22:44:30.415007: step 121, loss 2.80057, acc 0.53125, prec 0.0222689, recall 0.685345
2017-12-09T22:44:30.717442: step 122, loss 2.43795, acc 0.539062, prec 0.0222222, recall 0.686695
2017-12-09T22:44:31.023372: step 123, loss 1.94646, acc 0.570312, prec 0.0221885, recall 0.688034
2017-12-09T22:44:31.328848: step 124, loss 2.28727, acc 0.632812, prec 0.0224504, recall 0.689076
2017-12-09T22:44:31.627101: step 125, loss 2.19926, acc 0.632812, prec 0.0224398, recall 0.690377
2017-12-09T22:44:31.931502: step 126, loss 2.46612, acc 0.726562, prec 0.0223365, recall 0.6875
2017-12-09T22:44:32.225213: step 127, loss 1.46286, acc 0.703125, prec 0.0223539, recall 0.688797
2017-12-09T22:44:32.522450: step 128, loss 3.79897, acc 0.664062, prec 0.0222282, recall 0.68595
2017-12-09T22:44:32.822493: step 129, loss 0.797298, acc 0.773438, prec 0.0225333, recall 0.689796
2017-12-09T22:44:33.118387: step 130, loss 0.879035, acc 0.78125, prec 0.0224495, recall 0.689796
2017-12-09T22:44:33.422849: step 131, loss 0.874882, acc 0.789062, prec 0.0224987, recall 0.691057
2017-12-09T22:44:33.726165: step 132, loss 0.842834, acc 0.75, prec 0.0224038, recall 0.691057
2017-12-09T22:44:34.028163: step 133, loss 7.26752, acc 0.78125, prec 0.0223273, recall 0.685484
2017-12-09T22:44:34.334310: step 134, loss 0.842779, acc 0.8125, prec 0.0223851, recall 0.686747
2017-12-09T22:44:34.631671: step 135, loss 3.33924, acc 0.835938, prec 0.0224572, recall 0.68254
2017-12-09T22:44:34.934799: step 136, loss 0.499639, acc 0.851562, prec 0.0224017, recall 0.68254
2017-12-09T22:44:35.242425: step 137, loss 5.33413, acc 0.71875, prec 0.0224297, recall 0.678431
2017-12-09T22:44:35.550128: step 138, loss 0.868261, acc 0.78125, prec 0.0223485, recall 0.678431
2017-12-09T22:44:35.849242: step 139, loss 1.60729, acc 0.65625, prec 0.0223478, recall 0.679688
2017-12-09T22:44:36.147918: step 140, loss 2.12672, acc 0.632812, prec 0.0227128, recall 0.684615
2017-12-09T22:44:36.449394: step 141, loss 1.41729, acc 0.59375, prec 0.0229346, recall 0.688213
2017-12-09T22:44:36.747548: step 142, loss 5.17775, acc 0.632812, prec 0.0231709, recall 0.689139
2017-12-09T22:44:37.049192: step 143, loss 3.55238, acc 0.625, prec 0.0231568, recall 0.687732
2017-12-09T22:44:37.352800: step 144, loss 2.37617, acc 0.53125, prec 0.0234695, recall 0.692308
2017-12-09T22:44:37.652640: step 145, loss 2.78381, acc 0.492188, prec 0.0235222, recall 0.694545
2017-12-09T22:44:37.951766: step 146, loss 2.25137, acc 0.59375, prec 0.0236115, recall 0.696751
2017-12-09T22:44:38.252534: step 147, loss 2.90992, acc 0.515625, prec 0.0236708, recall 0.698925
2017-12-09T22:44:38.551234: step 148, loss 5.17852, acc 0.554688, prec 0.0236287, recall 0.697509
2017-12-09T22:44:38.854160: step 149, loss 2.83103, acc 0.421875, prec 0.0236531, recall 0.699647
2017-12-09T22:44:39.154954: step 150, loss 2.64389, acc 0.523438, prec 0.0237135, recall 0.701754
2017-12-09T22:44:39.449274: step 151, loss 2.1221, acc 0.585938, prec 0.0239105, recall 0.704861
2017-12-09T22:44:39.749974: step 152, loss 3.34034, acc 0.609375, prec 0.0238904, recall 0.701031
2017-12-09T22:44:40.051003: step 153, loss 1.86039, acc 0.648438, prec 0.0237651, recall 0.701031
2017-12-09T22:44:40.349696: step 154, loss 2.53593, acc 0.632812, prec 0.0236385, recall 0.69863
2017-12-09T22:44:40.653409: step 155, loss 1.93319, acc 0.601562, prec 0.0234996, recall 0.69863
2017-12-09T22:44:40.952075: step 156, loss 1.51466, acc 0.664062, prec 0.0234957, recall 0.699659
2017-12-09T22:44:41.248804: step 157, loss 1.32395, acc 0.664062, prec 0.0234918, recall 0.70068
2017-12-09T22:44:41.549048: step 158, loss 4.39991, acc 0.648438, prec 0.0233772, recall 0.695946
2017-12-09T22:44:41.858946: step 159, loss 1.98896, acc 0.679688, prec 0.0234896, recall 0.697987
2017-12-09T22:44:42.158037: step 160, loss 1.76644, acc 0.742188, prec 0.023405, recall 0.695652
2017-12-09T22:44:42.459829: step 161, loss 2.84596, acc 0.679688, prec 0.0233001, recall 0.693333
2017-12-09T22:44:42.757930: step 162, loss 1.33309, acc 0.695312, prec 0.0233077, recall 0.694352
2017-12-09T22:44:43.063375: step 163, loss 3.50484, acc 0.703125, prec 0.0234288, recall 0.694079
2017-12-09T22:44:43.367514: step 164, loss 1.44086, acc 0.71875, prec 0.0236595, recall 0.697068
2017-12-09T22:44:43.667929: step 165, loss 6.61197, acc 0.679688, prec 0.0238803, recall 0.695513
2017-12-09T22:44:43.969592: step 166, loss 7.96711, acc 0.625, prec 0.0239737, recall 0.693038
2017-12-09T22:44:44.270888: step 167, loss 2.05355, acc 0.546875, prec 0.0238225, recall 0.693038
2017-12-09T22:44:44.571155: step 168, loss 3.2001, acc 0.546875, prec 0.0237812, recall 0.691824
2017-12-09T22:44:44.870668: step 169, loss 1.78471, acc 0.570312, prec 0.0237456, recall 0.69279
2017-12-09T22:44:45.170650: step 170, loss 2.44235, acc 0.515625, prec 0.023901, recall 0.695652
2017-12-09T22:44:45.466470: step 171, loss 7.01109, acc 0.515625, prec 0.0239534, recall 0.695385
2017-12-09T22:44:45.770002: step 172, loss 2.50075, acc 0.515625, prec 0.0238998, recall 0.696319
2017-12-09T22:44:46.069890: step 173, loss 3.09809, acc 0.492188, prec 0.0239415, recall 0.698171
2017-12-09T22:44:46.369328: step 174, loss 2.26096, acc 0.539062, prec 0.0242002, recall 0.701807
2017-12-09T22:44:46.670381: step 175, loss 2.73405, acc 0.46875, prec 0.024433, recall 0.705357
2017-12-09T22:44:46.970737: step 176, loss 4.16896, acc 0.515625, prec 0.0244802, recall 0.705015
2017-12-09T22:44:47.268201: step 177, loss 2.43321, acc 0.546875, prec 0.0244349, recall 0.705882
2017-12-09T22:44:47.567947: step 178, loss 1.92755, acc 0.578125, prec 0.0244001, recall 0.706745
2017-12-09T22:44:47.871901: step 179, loss 2.79522, acc 0.632812, prec 0.024287, recall 0.704678
2017-12-09T22:44:48.176190: step 180, loss 4.07772, acc 0.640625, prec 0.0242753, recall 0.703488
2017-12-09T22:44:48.475762: step 181, loss 1.16118, acc 0.710938, prec 0.024283, recall 0.704348
2017-12-09T22:44:48.773989: step 182, loss 1.59875, acc 0.601562, prec 0.0243539, recall 0.706052
2017-12-09T22:44:49.070574: step 183, loss 1.31661, acc 0.710938, prec 0.0242646, recall 0.706052
2017-12-09T22:44:49.378548: step 184, loss 3.04097, acc 0.648438, prec 0.0242556, recall 0.704871
2017-12-09T22:44:49.686272: step 185, loss 2.23829, acc 0.726562, prec 0.0242704, recall 0.703704
2017-12-09T22:44:49.989385: step 186, loss 1.92062, acc 0.640625, prec 0.0242567, recall 0.704545
2017-12-09T22:44:50.295780: step 187, loss 1.72585, acc 0.695312, prec 0.0242595, recall 0.705382
2017-12-09T22:44:50.605649: step 188, loss 4.04284, acc 0.6875, prec 0.0241677, recall 0.70339
2017-12-09T22:44:50.907151: step 189, loss 1.18769, acc 0.710938, prec 0.0241756, recall 0.704225
2017-12-09T22:44:51.207830: step 190, loss 1.10575, acc 0.710938, prec 0.0240894, recall 0.704225
2017-12-09T22:44:51.507489: step 191, loss 2.55542, acc 0.742188, prec 0.0240154, recall 0.702247
2017-12-09T22:44:51.807778: step 192, loss 5.72305, acc 0.765625, prec 0.0241356, recall 0.70195
2017-12-09T22:44:52.111139: step 193, loss 1.2448, acc 0.734375, prec 0.0240573, recall 0.70195
2017-12-09T22:44:52.417837: step 194, loss 1.29301, acc 0.695312, prec 0.0240609, recall 0.702778
2017-12-09T22:44:52.720663: step 195, loss 1.10102, acc 0.742188, prec 0.0239856, recall 0.702778
2017-12-09T22:44:53.021106: step 196, loss 8.57096, acc 0.6875, prec 0.0238995, recall 0.698895
2017-12-09T22:44:53.325627: step 197, loss 1.1811, acc 0.710938, prec 0.0239081, recall 0.699724
2017-12-09T22:44:53.621825: step 198, loss 2.23201, acc 0.671875, prec 0.0240907, recall 0.700273
2017-12-09T22:44:53.922572: step 199, loss 1.14878, acc 0.726562, prec 0.0241943, recall 0.701897
2017-12-09T22:44:54.226460: step 200, loss 2.10384, acc 0.648438, prec 0.024456, recall 0.705094
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-200

2017-12-09T22:44:55.635641: step 201, loss 1.39543, acc 0.6875, prec 0.0245461, recall 0.706667
2017-12-09T22:44:55.939103: step 202, loss 1.97907, acc 0.742188, prec 0.0244736, recall 0.704787
2017-12-09T22:44:56.242927: step 203, loss 1.35627, acc 0.703125, prec 0.024747, recall 0.707895
2017-12-09T22:44:56.546036: step 204, loss 1.82397, acc 0.632812, prec 0.0248191, recall 0.709424
2017-12-09T22:44:56.851032: step 205, loss 4.79851, acc 0.6875, prec 0.0248198, recall 0.708333
2017-12-09T22:44:57.149633: step 206, loss 1.11399, acc 0.695312, prec 0.0248204, recall 0.709091
2017-12-09T22:44:57.450406: step 207, loss 3.02564, acc 0.679688, prec 0.0249955, recall 0.709512
2017-12-09T22:44:57.752166: step 208, loss 1.28333, acc 0.671875, prec 0.0249887, recall 0.710256
2017-12-09T22:44:58.051039: step 209, loss 1.24638, acc 0.664062, prec 0.0249798, recall 0.710997
2017-12-09T22:44:58.348964: step 210, loss 0.827716, acc 0.796875, prec 0.0249216, recall 0.710997
2017-12-09T22:44:58.654670: step 211, loss 1.95944, acc 0.726562, prec 0.024933, recall 0.709924
2017-12-09T22:44:58.957882: step 212, loss 1.17594, acc 0.710938, prec 0.0250245, recall 0.711392
2017-12-09T22:44:59.261833: step 213, loss 4.83535, acc 0.757812, prec 0.0250444, recall 0.710327
2017-12-09T22:44:59.563533: step 214, loss 0.934219, acc 0.726562, prec 0.0251394, recall 0.711779
2017-12-09T22:44:59.861793: step 215, loss 1.27221, acc 0.757812, prec 0.0253287, recall 0.71393
2017-12-09T22:45:00.166707: step 216, loss 1.50318, acc 0.789062, prec 0.0254424, recall 0.71358
2017-12-09T22:45:00.474657: step 217, loss 1.04608, acc 0.773438, prec 0.0254632, recall 0.714286
2017-12-09T22:45:00.776251: step 218, loss 0.822697, acc 0.796875, prec 0.0255759, recall 0.715686
2017-12-09T22:45:01.075688: step 219, loss 2.89411, acc 0.710938, prec 0.0255806, recall 0.714634
2017-12-09T22:45:01.378777: step 220, loss 1.46641, acc 0.804688, prec 0.0257817, recall 0.714976
2017-12-09T22:45:01.683753: step 221, loss 1.86266, acc 0.78125, prec 0.0259752, recall 0.715311
2017-12-09T22:45:01.986106: step 222, loss 1.00076, acc 0.78125, prec 0.0260809, recall 0.716667
2017-12-09T22:45:02.281922: step 223, loss 1.11555, acc 0.71875, prec 0.026084, recall 0.71734
2017-12-09T22:45:02.586474: step 224, loss 2.4936, acc 0.671875, prec 0.0260757, recall 0.716312
2017-12-09T22:45:02.889283: step 225, loss 6.66039, acc 0.648438, prec 0.0259774, recall 0.714623
2017-12-09T22:45:03.185129: step 226, loss 1.79384, acc 0.546875, prec 0.026015, recall 0.715962
2017-12-09T22:45:03.489280: step 227, loss 2.94692, acc 0.632812, prec 0.0259961, recall 0.714953
2017-12-09T22:45:03.792713: step 228, loss 3.4694, acc 0.640625, prec 0.0259795, recall 0.713953
2017-12-09T22:45:04.091399: step 229, loss 11.7247, acc 0.640625, prec 0.0260474, recall 0.711982
2017-12-09T22:45:04.392832: step 230, loss 2.24522, acc 0.546875, prec 0.0260023, recall 0.712644
2017-12-09T22:45:04.691828: step 231, loss 4.78623, acc 0.4375, prec 0.0258484, recall 0.711009
2017-12-09T22:45:04.992653: step 232, loss 2.40731, acc 0.5, prec 0.025792, recall 0.71167
2017-12-09T22:45:05.297836: step 233, loss 3.41438, acc 0.398438, prec 0.0257108, recall 0.710706
2017-12-09T22:45:05.600775: step 234, loss 2.99153, acc 0.359375, prec 0.0258571, recall 0.713318
2017-12-09T22:45:05.893580: step 235, loss 2.65635, acc 0.453125, prec 0.0258684, recall 0.714607
2017-12-09T22:45:06.188245: step 236, loss 2.90664, acc 0.398438, prec 0.0259436, recall 0.716518
2017-12-09T22:45:06.483866: step 237, loss 3.01826, acc 0.367188, prec 0.0257749, recall 0.716518
2017-12-09T22:45:06.781225: step 238, loss 2.3394, acc 0.484375, prec 0.025639, recall 0.716518
2017-12-09T22:45:07.079192: step 239, loss 2.17481, acc 0.5, prec 0.0255086, recall 0.716518
2017-12-09T22:45:07.381526: step 240, loss 2.08475, acc 0.539062, prec 0.0255437, recall 0.717778
2017-12-09T22:45:07.682969: step 241, loss 1.71264, acc 0.570312, prec 0.0255865, recall 0.719027
2017-12-09T22:45:07.982834: step 242, loss 5.98914, acc 0.703125, prec 0.0257416, recall 0.719298
2017-12-09T22:45:08.283837: step 243, loss 5.8521, acc 0.695312, prec 0.0258176, recall 0.718954
2017-12-09T22:45:08.579759: step 244, loss 1.09867, acc 0.695312, prec 0.025815, recall 0.719565
2017-12-09T22:45:08.876633: step 245, loss 5.94945, acc 0.734375, prec 0.0257507, recall 0.71645
2017-12-09T22:45:09.178595: step 246, loss 0.795164, acc 0.773438, prec 0.025844, recall 0.717672
2017-12-09T22:45:09.481726: step 247, loss 4.74828, acc 0.71875, prec 0.025778, recall 0.713062
2017-12-09T22:45:09.781440: step 248, loss 1.39786, acc 0.65625, prec 0.0259159, recall 0.714894
2017-12-09T22:45:09.965069: step 249, loss 1.43594, acc 0.615385, prec 0.025876, recall 0.714894
2017-12-09T22:45:10.274396: step 250, loss 1.06953, acc 0.742188, prec 0.0260349, recall 0.716702
2017-12-09T22:45:10.568346: step 251, loss 1.16281, acc 0.710938, prec 0.0261848, recall 0.718487
2017-12-09T22:45:10.866547: step 252, loss 1.35244, acc 0.632812, prec 0.0260909, recall 0.718487
2017-12-09T22:45:11.169158: step 253, loss 4.37849, acc 0.664062, prec 0.0263038, recall 0.719335
2017-12-09T22:45:11.465844: step 254, loss 1.6189, acc 0.609375, prec 0.0262779, recall 0.719917
2017-12-09T22:45:11.769224: step 255, loss 1.34522, acc 0.640625, prec 0.0263337, recall 0.721074
2017-12-09T22:45:12.068972: step 256, loss 1.44665, acc 0.578125, prec 0.0263, recall 0.721649
2017-12-09T22:45:12.369877: step 257, loss 0.876664, acc 0.757812, prec 0.0266037, recall 0.72449
2017-12-09T22:45:12.671008: step 258, loss 1.28998, acc 0.648438, prec 0.0267324, recall 0.726166
2017-12-09T22:45:12.970403: step 259, loss 0.953213, acc 0.695312, prec 0.0267272, recall 0.726721
2017-12-09T22:45:13.267855: step 260, loss 2.17928, acc 0.804688, prec 0.0267519, recall 0.725806
2017-12-09T22:45:13.569009: step 261, loss 0.592338, acc 0.773438, prec 0.0267665, recall 0.726358
2017-12-09T22:45:13.868312: step 262, loss 1.07845, acc 0.859375, prec 0.026875, recall 0.727455
2017-12-09T22:45:14.165000: step 263, loss 0.571411, acc 0.8125, prec 0.0268992, recall 0.728
2017-12-09T22:45:14.465933: step 264, loss 0.90699, acc 0.828125, prec 0.0269273, recall 0.728543
2017-12-09T22:45:14.772923: step 265, loss 4.73771, acc 0.8125, prec 0.0268837, recall 0.725646
2017-12-09T22:45:15.072013: step 266, loss 0.723843, acc 0.796875, prec 0.0269039, recall 0.72619
2017-12-09T22:45:15.372783: step 267, loss 0.997841, acc 0.789062, prec 0.0269219, recall 0.726733
2017-12-09T22:45:15.673990: step 268, loss 0.668151, acc 0.820312, prec 0.0268766, recall 0.726733
2017-12-09T22:45:15.971907: step 269, loss 2.16445, acc 0.796875, prec 0.0269697, recall 0.726378
2017-12-09T22:45:16.275021: step 270, loss 0.666987, acc 0.765625, prec 0.0269817, recall 0.726916
2017-12-09T22:45:16.575714: step 271, loss 5.99465, acc 0.734375, prec 0.0269169, recall 0.72549
2017-12-09T22:45:16.877014: step 272, loss 0.928614, acc 0.703125, prec 0.0268427, recall 0.72549
2017-12-09T22:45:17.183196: step 273, loss 2.5324, acc 0.765625, prec 0.0267864, recall 0.72407
2017-12-09T22:45:17.489649: step 274, loss 1.85647, acc 0.523438, prec 0.0267387, recall 0.724609
2017-12-09T22:45:17.790672: step 275, loss 0.877737, acc 0.742188, prec 0.0268852, recall 0.726214
2017-12-09T22:45:18.087164: step 276, loss 1.05861, acc 0.734375, prec 0.0269592, recall 0.727273
2017-12-09T22:45:18.388245: step 277, loss 2.40142, acc 0.75, prec 0.0270386, recall 0.726923
2017-12-09T22:45:18.694148: step 278, loss 0.972493, acc 0.734375, prec 0.026973, recall 0.726923
2017-12-09T22:45:18.993754: step 279, loss 4.30028, acc 0.6875, prec 0.0269674, recall 0.726054
2017-12-09T22:45:19.293090: step 280, loss 0.893416, acc 0.75, prec 0.0269752, recall 0.726577
2017-12-09T22:45:19.589323: step 281, loss 1.43043, acc 0.695312, prec 0.0270385, recall 0.727619
2017-12-09T22:45:19.889851: step 282, loss 1.25693, acc 0.679688, prec 0.0270976, recall 0.728653
2017-12-09T22:45:20.198878: step 283, loss 1.67615, acc 0.710938, prec 0.0270289, recall 0.727273
2017-12-09T22:45:20.509130: step 284, loss 1.4175, acc 0.632812, prec 0.0269398, recall 0.727273
2017-12-09T22:45:20.804927: step 285, loss 0.922621, acc 0.734375, prec 0.0270119, recall 0.728302
2017-12-09T22:45:21.104821: step 286, loss 1.05925, acc 0.726562, prec 0.0270817, recall 0.729323
2017-12-09T22:45:21.400113: step 287, loss 1.13909, acc 0.726562, prec 0.0271512, recall 0.730337
2017-12-09T22:45:21.700798: step 288, loss 0.819586, acc 0.75, prec 0.0271584, recall 0.730841
2017-12-09T22:45:22.003701: step 289, loss 4.03095, acc 0.695312, prec 0.0271543, recall 0.729981
2017-12-09T22:45:22.299654: step 290, loss 1.34138, acc 0.734375, prec 0.027225, recall 0.730983
2017-12-09T22:45:22.600017: step 291, loss 0.90009, acc 0.726562, prec 0.0272934, recall 0.731978
2017-12-09T22:45:22.903062: step 292, loss 0.775623, acc 0.757812, prec 0.0272352, recall 0.731978
2017-12-09T22:45:23.204001: step 293, loss 0.726809, acc 0.773438, prec 0.0272478, recall 0.732472
2017-12-09T22:45:23.505666: step 294, loss 0.703565, acc 0.796875, prec 0.0272659, recall 0.732965
2017-12-09T22:45:23.809249: step 295, loss 9.15507, acc 0.789062, prec 0.0272858, recall 0.730769
2017-12-09T22:45:24.112215: step 296, loss 0.688305, acc 0.804688, prec 0.0273056, recall 0.731261
2017-12-09T22:45:24.413847: step 297, loss 0.550025, acc 0.820312, prec 0.0273954, recall 0.73224
2017-12-09T22:45:24.711663: step 298, loss 1.43508, acc 0.796875, prec 0.027415, recall 0.731397
2017-12-09T22:45:25.008430: step 299, loss 1.66705, acc 0.679688, prec 0.0274066, recall 0.730561
2017-12-09T22:45:25.311847: step 300, loss 4.69156, acc 0.734375, prec 0.0275428, recall 0.7307

Evaluation:
2017-12-09T22:45:30.021023: step 300, loss 1.37451, acc 0.763823, prec 0.0300115, recall 0.739007

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-300

2017-12-09T22:45:31.337204: step 301, loss 1.06172, acc 0.710938, prec 0.0300592, recall 0.739745
2017-12-09T22:45:31.634887: step 302, loss 1.10302, acc 0.703125, prec 0.0300493, recall 0.740113
2017-12-09T22:45:31.936436: step 303, loss 1.02105, acc 0.710938, prec 0.0301522, recall 0.74121
2017-12-09T22:45:32.240980: step 304, loss 1.69482, acc 0.734375, prec 0.0302615, recall 0.741259
2017-12-09T22:45:32.547848: step 305, loss 1.13353, acc 0.632812, prec 0.0302357, recall 0.74162
2017-12-09T22:45:32.846777: step 306, loss 1.39593, acc 0.617188, prec 0.0303718, recall 0.743056
2017-12-09T22:45:33.145790: step 307, loss 1.28398, acc 0.679688, prec 0.0303013, recall 0.743056
2017-12-09T22:45:33.443506: step 308, loss 1.15657, acc 0.695312, prec 0.0303441, recall 0.743767
2017-12-09T22:45:33.748890: step 309, loss 0.76481, acc 0.726562, prec 0.0302842, recall 0.743767
2017-12-09T22:45:34.052605: step 310, loss 4.68577, acc 0.71875, prec 0.0302792, recall 0.743094
2017-12-09T22:45:34.359179: step 311, loss 1.00697, acc 0.773438, prec 0.0302298, recall 0.743094
2017-12-09T22:45:34.655139: step 312, loss 5.04268, acc 0.742188, prec 0.03023, recall 0.742424
2017-12-09T22:45:34.959266: step 313, loss 3.90587, acc 0.75, prec 0.0302861, recall 0.742112
2017-12-09T22:45:35.261999: step 314, loss 0.691309, acc 0.765625, prec 0.0302353, recall 0.742112
2017-12-09T22:45:35.567748: step 315, loss 0.6654, acc 0.773438, prec 0.0302405, recall 0.742466
2017-12-09T22:45:35.865346: step 316, loss 1.06857, acc 0.703125, prec 0.0302305, recall 0.742818
2017-12-09T22:45:36.168469: step 317, loss 0.915443, acc 0.734375, prec 0.0302811, recall 0.74352
2017-12-09T22:45:36.472907: step 318, loss 0.827919, acc 0.742188, prec 0.0302795, recall 0.743869
2017-12-09T22:45:36.772212: step 319, loss 4.73702, acc 0.671875, prec 0.0302661, recall 0.742198
2017-12-09T22:45:37.076379: step 320, loss 1.07042, acc 0.664062, prec 0.0303549, recall 0.743243
2017-12-09T22:45:37.372210: step 321, loss 0.68253, acc 0.765625, prec 0.0303581, recall 0.74359
2017-12-09T22:45:37.676591: step 322, loss 1.01023, acc 0.703125, prec 0.0304013, recall 0.74428
2017-12-09T22:45:37.979416: step 323, loss 2.39749, acc 0.726562, prec 0.0305059, recall 0.743316
2017-12-09T22:45:38.285320: step 324, loss 0.601022, acc 0.8125, prec 0.0307313, recall 0.74502
2017-12-09T22:45:38.583618: step 325, loss 0.965252, acc 0.757812, prec 0.0308382, recall 0.746032
2017-12-09T22:45:38.884108: step 326, loss 2.26492, acc 0.6875, prec 0.0308783, recall 0.745718
2017-12-09T22:45:39.184752: step 327, loss 0.875729, acc 0.78125, prec 0.0308312, recall 0.745718
2017-12-09T22:45:39.487056: step 328, loss 4.06338, acc 0.6875, prec 0.0310309, recall 0.745431
2017-12-09T22:45:39.784825: step 329, loss 0.839583, acc 0.726562, prec 0.030972, recall 0.745431
2017-12-09T22:45:40.083710: step 330, loss 1.17813, acc 0.71875, prec 0.031069, recall 0.746424
2017-12-09T22:45:40.382527: step 331, loss 6.79083, acc 0.570312, prec 0.0310847, recall 0.745149
2017-12-09T22:45:40.683860: step 332, loss 1.38783, acc 0.609375, prec 0.0310532, recall 0.745478
2017-12-09T22:45:40.979543: step 333, loss 1.4174, acc 0.570312, prec 0.0310136, recall 0.745806
2017-12-09T22:45:41.278237: step 334, loss 1.32617, acc 0.664062, prec 0.0310459, recall 0.746461
2017-12-09T22:45:41.576239: step 335, loss 1.47415, acc 0.570312, prec 0.0311099, recall 0.747436
2017-12-09T22:45:41.879616: step 336, loss 2.91474, acc 0.632812, prec 0.0310853, recall 0.746803
2017-12-09T22:45:42.184342: step 337, loss 1.3399, acc 0.6875, prec 0.0310193, recall 0.746803
2017-12-09T22:45:42.481251: step 338, loss 1.77464, acc 0.554688, prec 0.0310283, recall 0.747449
2017-12-09T22:45:42.777400: step 339, loss 4.27696, acc 0.648438, prec 0.0311097, recall 0.747462
2017-12-09T22:45:43.078056: step 340, loss 1.17745, acc 0.664062, prec 0.0312945, recall 0.749054
2017-12-09T22:45:43.377695: step 341, loss 1.29183, acc 0.648438, prec 0.0312713, recall 0.74937
2017-12-09T22:45:43.671913: step 342, loss 1.70967, acc 0.648438, prec 0.0313499, recall 0.750314
2017-12-09T22:45:43.973109: step 343, loss 3.21516, acc 0.6875, prec 0.0313366, recall 0.749687
2017-12-09T22:45:44.276551: step 344, loss 1.05394, acc 0.726562, prec 0.03133, recall 0.75
2017-12-09T22:45:44.581029: step 345, loss 1.63485, acc 0.679688, prec 0.0315153, recall 0.751553
2017-12-09T22:45:44.876609: step 346, loss 0.987877, acc 0.742188, prec 0.0314613, recall 0.751553
2017-12-09T22:45:45.178285: step 347, loss 1.25232, acc 0.679688, prec 0.0315451, recall 0.752475
2017-12-09T22:45:45.478931: step 348, loss 2.15145, acc 0.726562, prec 0.0315899, recall 0.752158
2017-12-09T22:45:45.780929: step 349, loss 1.02478, acc 0.65625, prec 0.0316181, recall 0.752768
2017-12-09T22:45:46.079912: step 350, loss 1.54927, acc 0.78125, prec 0.031624, recall 0.752147
2017-12-09T22:45:46.382376: step 351, loss 1.14331, acc 0.757812, prec 0.0317231, recall 0.753056
2017-12-09T22:45:46.691130: step 352, loss 0.768819, acc 0.75, prec 0.0317207, recall 0.753358
2017-12-09T22:45:46.995161: step 353, loss 0.875062, acc 0.773438, prec 0.0317729, recall 0.753959
2017-12-09T22:45:47.293101: step 354, loss 0.711193, acc 0.78125, prec 0.0317769, recall 0.754258
2017-12-09T22:45:47.594335: step 355, loss 0.612552, acc 0.8125, prec 0.0317379, recall 0.754258
2017-12-09T22:45:47.896727: step 356, loss 5.46917, acc 0.84375, prec 0.0317566, recall 0.753641
2017-12-09T22:45:48.198071: step 357, loss 0.516582, acc 0.835938, prec 0.031772, recall 0.753939
2017-12-09T22:45:48.503285: step 358, loss 1.0833, acc 0.835938, prec 0.031789, recall 0.753325
2017-12-09T22:45:48.806944: step 359, loss 1.32461, acc 0.789062, prec 0.0317962, recall 0.752714
2017-12-09T22:45:49.107254: step 360, loss 0.40776, acc 0.867188, prec 0.0317687, recall 0.752714
2017-12-09T22:45:49.407593: step 361, loss 1.65456, acc 0.78125, prec 0.0318235, recall 0.752404
2017-12-09T22:45:49.707958: step 362, loss 0.825413, acc 0.828125, prec 0.0319354, recall 0.753293
2017-12-09T22:45:50.010654: step 363, loss 4.40366, acc 0.773438, prec 0.0318917, recall 0.751493
2017-12-09T22:45:50.330858: step 364, loss 0.809203, acc 0.757812, prec 0.0318417, recall 0.751493
2017-12-09T22:45:50.637783: step 365, loss 3.25332, acc 0.710938, prec 0.0318816, recall 0.75119
2017-12-09T22:45:50.938952: step 366, loss 2.14964, acc 0.742188, prec 0.0318789, recall 0.750594
2017-12-09T22:45:51.250715: step 367, loss 1.22034, acc 0.71875, prec 0.0318699, recall 0.75089
2017-12-09T22:45:51.552769: step 368, loss 1.57963, acc 0.65625, prec 0.0318967, recall 0.751479
2017-12-09T22:45:51.850513: step 369, loss 1.79498, acc 0.5625, prec 0.0319042, recall 0.752066
2017-12-09T22:45:52.145806: step 370, loss 1.74656, acc 0.554688, prec 0.0318618, recall 0.752358
2017-12-09T22:45:52.450265: step 371, loss 1.97981, acc 0.546875, prec 0.0317697, recall 0.752358
2017-12-09T22:45:52.748520: step 372, loss 1.49227, acc 0.625, prec 0.0317902, recall 0.752941
2017-12-09T22:45:53.052878: step 373, loss 1.58772, acc 0.59375, prec 0.0317563, recall 0.753231
2017-12-09T22:45:53.352017: step 374, loss 1.7167, acc 0.523438, prec 0.0317562, recall 0.75381
2017-12-09T22:45:53.654056: step 375, loss 1.69054, acc 0.570312, prec 0.0319563, recall 0.75553
2017-12-09T22:45:53.950491: step 376, loss 1.372, acc 0.648438, prec 0.0319807, recall 0.756098
2017-12-09T22:45:54.247685: step 377, loss 1.18842, acc 0.703125, prec 0.0319212, recall 0.756098
2017-12-09T22:45:54.551970: step 378, loss 2.06577, acc 0.710938, prec 0.032007, recall 0.756069
2017-12-09T22:45:54.854798: step 379, loss 1.0819, acc 0.726562, prec 0.0320469, recall 0.756632
2017-12-09T22:45:55.154957: step 380, loss 0.708196, acc 0.78125, prec 0.0320031, recall 0.756632
2017-12-09T22:45:55.452700: step 381, loss 2.05859, acc 0.828125, prec 0.0320175, recall 0.756041
2017-12-09T22:45:55.751380: step 382, loss 1.99895, acc 0.75, prec 0.0320634, recall 0.755734
2017-12-09T22:45:56.054097: step 383, loss 0.31457, acc 0.867188, prec 0.032084, recall 0.756014
2017-12-09T22:45:56.352655: step 384, loss 3.53474, acc 0.804688, prec 0.0320466, recall 0.755149
2017-12-09T22:45:56.655602: step 385, loss 1.27143, acc 0.78125, prec 0.0320047, recall 0.754286
2017-12-09T22:45:56.960310: step 386, loss 0.705811, acc 0.804688, prec 0.0319659, recall 0.754286
2017-12-09T22:45:57.264215: step 387, loss 0.542417, acc 0.789062, prec 0.0319242, recall 0.754286
2017-12-09T22:45:57.563692: step 388, loss 0.954576, acc 0.78125, prec 0.0319745, recall 0.754846
2017-12-09T22:45:57.868032: step 389, loss 1.42038, acc 0.765625, prec 0.0320683, recall 0.755682
2017-12-09T22:45:58.170211: step 390, loss 0.878462, acc 0.75, prec 0.0320655, recall 0.755959
2017-12-09T22:45:58.474855: step 391, loss 0.85907, acc 0.828125, prec 0.0322177, recall 0.757062
2017-12-09T22:45:58.774668: step 392, loss 0.920955, acc 0.765625, prec 0.0322643, recall 0.75761
2017-12-09T22:45:59.077545: step 393, loss 0.806455, acc 0.765625, prec 0.0322643, recall 0.757883
2017-12-09T22:45:59.375932: step 394, loss 1.16932, acc 0.835938, prec 0.0322333, recall 0.75703
2017-12-09T22:45:59.676890: step 395, loss 0.692412, acc 0.804688, prec 0.0322411, recall 0.757303
2017-12-09T22:45:59.983727: step 396, loss 1.00586, acc 0.703125, prec 0.032275, recall 0.757848
2017-12-09T22:46:00.293264: step 397, loss 0.913729, acc 0.742188, prec 0.0323626, recall 0.758659
2017-12-09T22:46:00.598730: step 398, loss 0.625365, acc 0.8125, prec 0.0323717, recall 0.758929
2017-12-09T22:46:00.900479: step 399, loss 5.02603, acc 0.851562, prec 0.03239, recall 0.758352
2017-12-09T22:46:01.204500: step 400, loss 0.841431, acc 0.773438, prec 0.0323454, recall 0.758352
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-400

2017-12-09T22:46:02.478793: step 401, loss 0.856105, acc 0.757812, prec 0.0323896, recall 0.758889
2017-12-09T22:46:02.778041: step 402, loss 0.767596, acc 0.773438, prec 0.032391, recall 0.759156
2017-12-09T22:46:03.078595: step 403, loss 6.72714, acc 0.828125, prec 0.0324046, recall 0.758583
2017-12-09T22:46:03.385376: step 404, loss 0.580869, acc 0.820312, prec 0.0323693, recall 0.758583
2017-12-09T22:46:03.691850: step 405, loss 0.913132, acc 0.773438, prec 0.0324164, recall 0.759116
2017-12-09T22:46:03.989066: step 406, loss 0.514583, acc 0.851562, prec 0.0324329, recall 0.759382
2017-12-09T22:46:04.292738: step 407, loss 1.84945, acc 0.75, prec 0.0325678, recall 0.759605
2017-12-09T22:46:04.601433: step 408, loss 6.3704, acc 0.789062, prec 0.032528, recall 0.758772
2017-12-09T22:46:04.904155: step 409, loss 0.74335, acc 0.75, prec 0.0325699, recall 0.7593
2017-12-09T22:46:05.211760: step 410, loss 0.904379, acc 0.734375, prec 0.032654, recall 0.760087
2017-12-09T22:46:05.519436: step 411, loss 0.859325, acc 0.859375, prec 0.0327171, recall 0.760609
2017-12-09T22:46:05.819403: step 412, loss 1.15642, acc 0.710938, prec 0.0327509, recall 0.761129
2017-12-09T22:46:06.121873: step 413, loss 1.04927, acc 0.742188, prec 0.0327907, recall 0.761647
2017-12-09T22:46:06.424670: step 414, loss 0.722954, acc 0.75, prec 0.032877, recall 0.762419
2017-12-09T22:46:06.729746: step 415, loss 0.735024, acc 0.773438, prec 0.0329226, recall 0.762931
2017-12-09T22:46:07.034493: step 416, loss 0.617352, acc 0.84375, prec 0.0329369, recall 0.763186
2017-12-09T22:46:07.331105: step 417, loss 2.04798, acc 0.757812, prec 0.0329808, recall 0.762876
2017-12-09T22:46:07.634706: step 418, loss 0.51482, acc 0.84375, prec 0.0329502, recall 0.762876
2017-12-09T22:46:07.936339: step 419, loss 0.704666, acc 0.8125, prec 0.0329584, recall 0.76313
2017-12-09T22:46:08.231764: step 420, loss 0.815872, acc 0.78125, prec 0.0329604, recall 0.763383
2017-12-09T22:46:08.529233: step 421, loss 0.764163, acc 0.78125, prec 0.0329625, recall 0.763636
2017-12-09T22:46:08.833003: step 422, loss 0.837202, acc 0.789062, prec 0.0330106, recall 0.764141
2017-12-09T22:46:09.136478: step 423, loss 6.08523, acc 0.78125, prec 0.0331031, recall 0.764081
2017-12-09T22:46:09.434477: step 424, loss 0.506327, acc 0.835938, prec 0.0331601, recall 0.764581
2017-12-09T22:46:09.731329: step 425, loss 2.84689, acc 0.828125, prec 0.033174, recall 0.763214
2017-12-09T22:46:10.030861: step 426, loss 0.763339, acc 0.8125, prec 0.0331819, recall 0.763464
2017-12-09T22:46:10.334915: step 427, loss 0.731286, acc 0.8125, prec 0.0331897, recall 0.763713
2017-12-09T22:46:10.630521: step 428, loss 1.18588, acc 0.75, prec 0.0332311, recall 0.763407
2017-12-09T22:46:10.931094: step 429, loss 0.903626, acc 0.75, prec 0.0332267, recall 0.763655
2017-12-09T22:46:11.235263: step 430, loss 0.98137, acc 0.710938, prec 0.033347, recall 0.764644
2017-12-09T22:46:11.534815: step 431, loss 0.712489, acc 0.789062, prec 0.0333941, recall 0.765136
2017-12-09T22:46:11.831458: step 432, loss 1.47806, acc 0.703125, prec 0.0335122, recall 0.766112
2017-12-09T22:46:12.128957: step 433, loss 1.25071, acc 0.679688, prec 0.0334937, recall 0.766355
2017-12-09T22:46:12.429363: step 434, loss 1.22619, acc 0.664062, prec 0.0334722, recall 0.766598
2017-12-09T22:46:12.721375: step 435, loss 3.95324, acc 0.664062, prec 0.0334961, recall 0.766288
2017-12-09T22:46:13.028698: step 436, loss 2.00024, acc 0.742188, prec 0.0335349, recall 0.765979
2017-12-09T22:46:13.332374: step 437, loss 1.00468, acc 0.71875, prec 0.0335676, recall 0.766461
2017-12-09T22:46:13.628695: step 438, loss 2.7011, acc 0.726562, prec 0.0336032, recall 0.766154
2017-12-09T22:46:13.931858: step 439, loss 1.45586, acc 0.570312, prec 0.033607, recall 0.766633
2017-12-09T22:46:14.228614: step 440, loss 1.4353, acc 0.632812, prec 0.0336228, recall 0.767109
2017-12-09T22:46:14.527019: step 441, loss 1.1032, acc 0.664062, prec 0.0336878, recall 0.767821
2017-12-09T22:46:14.825278: step 442, loss 1.41719, acc 0.617188, prec 0.0337004, recall 0.768293
2017-12-09T22:46:15.122310: step 443, loss 1.01059, acc 0.679688, prec 0.0337249, recall 0.768763
2017-12-09T22:46:15.421088: step 444, loss 0.82593, acc 0.796875, prec 0.0337288, recall 0.768997
2017-12-09T22:46:15.721897: step 445, loss 1.50822, acc 0.726562, prec 0.033678, recall 0.768219
2017-12-09T22:46:16.021608: step 446, loss 2.49653, acc 0.75, prec 0.0337173, recall 0.767911
2017-12-09T22:46:16.326920: step 447, loss 0.876477, acc 0.742188, prec 0.0336681, recall 0.767911
2017-12-09T22:46:16.630026: step 448, loss 0.689477, acc 0.789062, prec 0.0337133, recall 0.768379
2017-12-09T22:46:16.930979: step 449, loss 0.673059, acc 0.8125, prec 0.0337203, recall 0.768612
2017-12-09T22:46:17.232659: step 450, loss 0.475225, acc 0.804688, prec 0.0337257, recall 0.768844
2017-12-09T22:46:17.528582: step 451, loss 1.56401, acc 0.859375, prec 0.033743, recall 0.768305
2017-12-09T22:46:17.826576: step 452, loss 2.56897, acc 0.851562, prec 0.0337603, recall 0.767
2017-12-09T22:46:18.131611: step 453, loss 0.670846, acc 0.8125, prec 0.0338096, recall 0.767465
2017-12-09T22:46:18.425378: step 454, loss 0.512056, acc 0.804688, prec 0.0337725, recall 0.767465
2017-12-09T22:46:18.727570: step 455, loss 0.711843, acc 0.757812, prec 0.033769, recall 0.767697
2017-12-09T22:46:19.027656: step 456, loss 0.719598, acc 0.78125, prec 0.0338545, recall 0.76839
2017-12-09T22:46:19.326892: step 457, loss 4.17969, acc 0.828125, prec 0.0339079, recall 0.768087
2017-12-09T22:46:19.631456: step 458, loss 2.74048, acc 0.742188, prec 0.0338605, recall 0.767327
2017-12-09T22:46:19.930314: step 459, loss 0.933683, acc 0.796875, prec 0.0338643, recall 0.767557
2017-12-09T22:46:20.230080: step 460, loss 3.92583, acc 0.6875, prec 0.0338067, recall 0.766798
2017-12-09T22:46:20.538467: step 461, loss 1.09421, acc 0.726562, prec 0.0338814, recall 0.767488
2017-12-09T22:46:20.837932: step 462, loss 0.949043, acc 0.679688, prec 0.0339049, recall 0.767945
2017-12-09T22:46:21.133887: step 463, loss 1.18658, acc 0.679688, prec 0.0338447, recall 0.767945
2017-12-09T22:46:21.430787: step 464, loss 2.52913, acc 0.6875, prec 0.0338712, recall 0.767647
2017-12-09T22:46:21.736000: step 465, loss 1.37606, acc 0.632812, prec 0.0338025, recall 0.767647
2017-12-09T22:46:22.036644: step 466, loss 1.20076, acc 0.664062, prec 0.0338647, recall 0.768328
2017-12-09T22:46:22.332432: step 467, loss 1.07341, acc 0.648438, prec 0.0337992, recall 0.768328
2017-12-09T22:46:22.629051: step 468, loss 0.688521, acc 0.757812, prec 0.0337542, recall 0.768328
2017-12-09T22:46:22.927218: step 469, loss 1.88178, acc 0.679688, prec 0.0337377, recall 0.767805
2017-12-09T22:46:23.232443: step 470, loss 0.886171, acc 0.710938, prec 0.0337257, recall 0.768031
2017-12-09T22:46:23.535579: step 471, loss 0.899262, acc 0.710938, prec 0.0337962, recall 0.768707
2017-12-09T22:46:23.836568: step 472, loss 0.975652, acc 0.796875, prec 0.0338824, recall 0.76938
2017-12-09T22:46:24.133817: step 473, loss 0.736055, acc 0.789062, prec 0.0339669, recall 0.770048
2017-12-09T22:46:24.435715: step 474, loss 2.26133, acc 0.726562, prec 0.0339589, recall 0.769527
2017-12-09T22:46:24.732583: step 475, loss 1.74255, acc 0.789062, prec 0.0339624, recall 0.769009
2017-12-09T22:46:25.033350: step 476, loss 0.541346, acc 0.804688, prec 0.0339264, recall 0.769009
2017-12-09T22:46:25.329674: step 477, loss 1.99656, acc 0.835938, prec 0.0339386, recall 0.768492
2017-12-09T22:46:25.629339: step 478, loss 0.488828, acc 0.84375, prec 0.0339507, recall 0.768714
2017-12-09T22:46:25.933637: step 479, loss 0.548746, acc 0.789062, prec 0.0339528, recall 0.768936
2017-12-09T22:46:26.236089: step 480, loss 0.527782, acc 0.84375, prec 0.0339241, recall 0.768936
2017-12-09T22:46:26.531512: step 481, loss 0.583757, acc 0.820312, prec 0.0339728, recall 0.769378
2017-12-09T22:46:26.833739: step 482, loss 1.80014, acc 0.78125, prec 0.0339748, recall 0.768863
2017-12-09T22:46:27.139811: step 483, loss 1.06874, acc 0.765625, prec 0.0340947, recall 0.769743
2017-12-09T22:46:27.439713: step 484, loss 2.10524, acc 0.820312, prec 0.0341445, recall 0.76945
2017-12-09T22:46:27.740044: step 485, loss 1.86478, acc 0.710938, prec 0.0343349, recall 0.770755
2017-12-09T22:46:28.044995: step 486, loss 0.590608, acc 0.789062, prec 0.034296, recall 0.770755
2017-12-09T22:46:28.343846: step 487, loss 0.558262, acc 0.765625, prec 0.0342934, recall 0.770971
2017-12-09T22:46:28.644028: step 488, loss 1.87807, acc 0.695312, prec 0.0343197, recall 0.770677
2017-12-09T22:46:28.942726: step 489, loss 1.04517, acc 0.734375, prec 0.0344323, recall 0.771536
2017-12-09T22:46:29.244442: step 490, loss 1.33134, acc 0.664062, prec 0.0345719, recall 0.7726
2017-12-09T22:46:29.548994: step 491, loss 3.21893, acc 0.632812, prec 0.0346263, recall 0.772516
2017-12-09T22:46:29.847643: step 492, loss 1.74344, acc 0.726562, prec 0.0346576, recall 0.772222
2017-12-09T22:46:30.151375: step 493, loss 1.47556, acc 0.609375, prec 0.0346258, recall 0.772433
2017-12-09T22:46:30.453389: step 494, loss 0.868923, acc 0.664062, prec 0.0345641, recall 0.772433
2017-12-09T22:46:30.759000: step 495, loss 0.781562, acc 0.703125, prec 0.0345497, recall 0.772643
2017-12-09T22:46:31.059252: step 496, loss 1.21929, acc 0.695312, prec 0.0345738, recall 0.773063
2017-12-09T22:46:31.362084: step 497, loss 0.917107, acc 0.742188, prec 0.0346064, recall 0.773481
2017-12-09T22:46:31.540946: step 498, loss 0.733853, acc 0.788462, prec 0.0346702, recall 0.773897
2017-12-09T22:46:31.847568: step 499, loss 0.726092, acc 0.71875, prec 0.0346189, recall 0.773897
2017-12-09T22:46:32.156523: step 500, loss 4.66541, acc 0.71875, prec 0.0346088, recall 0.773395
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-500

2017-12-09T22:46:33.553592: step 501, loss 4.07048, acc 0.773438, prec 0.034569, recall 0.772686
2017-12-09T22:46:33.860159: step 502, loss 0.617645, acc 0.8125, prec 0.0345746, recall 0.772894
2017-12-09T22:46:34.154303: step 503, loss 0.791022, acc 0.757812, prec 0.0346097, recall 0.773309
2017-12-09T22:46:34.448409: step 504, loss 0.730907, acc 0.757812, prec 0.0346053, recall 0.773516
2017-12-09T22:46:34.752914: step 505, loss 1.97938, acc 0.789062, prec 0.034608, recall 0.773017
2017-12-09T22:46:35.055139: step 506, loss 0.690123, acc 0.8125, prec 0.0346135, recall 0.773224
2017-12-09T22:46:35.368168: step 507, loss 0.434789, acc 0.8125, prec 0.034619, recall 0.77343
2017-12-09T22:46:35.672538: step 508, loss 2.41004, acc 0.773438, prec 0.0345796, recall 0.772727
2017-12-09T22:46:35.979300: step 509, loss 0.56527, acc 0.773438, prec 0.0345388, recall 0.772727
2017-12-09T22:46:36.275077: step 510, loss 0.580375, acc 0.804688, prec 0.0345429, recall 0.772934
2017-12-09T22:46:36.574107: step 511, loss 1.43246, acc 0.84375, prec 0.0345163, recall 0.772232
2017-12-09T22:46:36.880088: step 512, loss 0.779145, acc 0.8125, prec 0.034561, recall 0.772645
2017-12-09T22:46:37.188072: step 513, loss 1.00987, acc 0.8125, prec 0.0346446, recall 0.773261
2017-12-09T22:46:37.486155: step 514, loss 0.469674, acc 0.859375, prec 0.0346975, recall 0.77367
2017-12-09T22:46:37.784404: step 515, loss 0.617015, acc 0.742188, prec 0.0346513, recall 0.77367
2017-12-09T22:46:38.085918: step 516, loss 0.775221, acc 0.789062, prec 0.0346914, recall 0.774077
2017-12-09T22:46:38.386580: step 517, loss 0.636492, acc 0.859375, prec 0.0347441, recall 0.774483
2017-12-09T22:46:38.687882: step 518, loss 0.660643, acc 0.796875, prec 0.0347465, recall 0.774686
2017-12-09T22:46:38.988641: step 519, loss 0.685704, acc 0.78125, prec 0.0347462, recall 0.774888
2017-12-09T22:46:39.288681: step 520, loss 0.701854, acc 0.789062, prec 0.0347861, recall 0.775291
2017-12-09T22:46:39.591239: step 521, loss 0.535222, acc 0.820312, prec 0.034754, recall 0.775291
2017-12-09T22:46:39.889223: step 522, loss 1.95735, acc 0.867188, prec 0.0347704, recall 0.774799
2017-12-09T22:46:40.189323: step 523, loss 3.60171, acc 0.84375, prec 0.03486, recall 0.774711
2017-12-09T22:46:40.501214: step 524, loss 0.54887, acc 0.820312, prec 0.0348279, recall 0.774711
2017-12-09T22:46:40.803313: step 525, loss 0.671205, acc 0.828125, prec 0.0348358, recall 0.774911
2017-12-09T22:46:41.103557: step 526, loss 5.57312, acc 0.859375, prec 0.0349664, recall 0.775022
2017-12-09T22:46:41.403937: step 527, loss 1.16746, acc 0.773438, prec 0.0351185, recall 0.776014
2017-12-09T22:46:41.704453: step 528, loss 0.858692, acc 0.75, prec 0.0352275, recall 0.776801
2017-12-09T22:46:42.002945: step 529, loss 0.925015, acc 0.703125, prec 0.0352127, recall 0.776997
2017-12-09T22:46:42.300879: step 530, loss 0.950551, acc 0.648438, prec 0.0352647, recall 0.777583
2017-12-09T22:46:42.603398: step 531, loss 1.14572, acc 0.664062, prec 0.0352811, recall 0.777972
2017-12-09T22:46:42.911069: step 532, loss 1.50813, acc 0.679688, prec 0.0353383, recall 0.778553
2017-12-09T22:46:43.207709: step 533, loss 0.905321, acc 0.6875, prec 0.0353206, recall 0.778746
2017-12-09T22:46:43.501804: step 534, loss 1.09384, acc 0.65625, prec 0.0353735, recall 0.779322
2017-12-09T22:46:43.802721: step 535, loss 0.930638, acc 0.742188, prec 0.0354035, recall 0.779705
2017-12-09T22:46:44.105148: step 536, loss 1.16465, acc 0.726562, prec 0.0354306, recall 0.780087
2017-12-09T22:46:44.402601: step 537, loss 0.868164, acc 0.765625, prec 0.0354646, recall 0.780467
2017-12-09T22:46:44.708600: step 538, loss 0.594576, acc 0.757812, prec 0.0354972, recall 0.780846
2017-12-09T22:46:45.004656: step 539, loss 0.62881, acc 0.773438, prec 0.0354946, recall 0.781034
2017-12-09T22:46:45.302850: step 540, loss 0.564, acc 0.789062, prec 0.0355703, recall 0.781599
2017-12-09T22:46:45.614355: step 541, loss 0.739199, acc 0.789062, prec 0.0355328, recall 0.781599
2017-12-09T22:46:45.916627: step 542, loss 3.74146, acc 0.851562, prec 0.0355455, recall 0.781116
2017-12-09T22:46:46.224396: step 543, loss 6.34733, acc 0.828125, prec 0.0356307, recall 0.780342
2017-12-09T22:46:46.527886: step 544, loss 0.538656, acc 0.820312, prec 0.0357115, recall 0.780904
2017-12-09T22:46:46.832983: step 545, loss 0.963873, acc 0.820312, prec 0.0357546, recall 0.781277
2017-12-09T22:46:47.139985: step 546, loss 0.62261, acc 0.773438, prec 0.0357893, recall 0.781648
2017-12-09T22:46:47.441802: step 547, loss 0.779188, acc 0.773438, prec 0.0358239, recall 0.782019
2017-12-09T22:46:47.742863: step 548, loss 0.70733, acc 0.757812, prec 0.0358182, recall 0.782203
2017-12-09T22:46:48.041863: step 549, loss 0.524008, acc 0.84375, prec 0.0359026, recall 0.782756
2017-12-09T22:46:48.345126: step 550, loss 0.85416, acc 0.757812, prec 0.0359342, recall 0.783122
2017-12-09T22:46:48.648150: step 551, loss 0.472566, acc 0.820312, prec 0.0359768, recall 0.783488
2017-12-09T22:46:48.940650: step 552, loss 0.649663, acc 0.820312, prec 0.0359821, recall 0.78367
2017-12-09T22:46:49.244395: step 553, loss 0.690421, acc 0.75, prec 0.036012, recall 0.784034
2017-12-09T22:46:49.542852: step 554, loss 2.04351, acc 0.820312, prec 0.0360558, recall 0.783738
2017-12-09T22:46:49.847171: step 555, loss 0.625088, acc 0.828125, prec 0.0360624, recall 0.78392
2017-12-09T22:46:50.142884: step 556, loss 0.664237, acc 0.828125, prec 0.0361061, recall 0.784281
2017-12-09T22:46:50.452992: step 557, loss 0.508183, acc 0.859375, prec 0.0361182, recall 0.784461
2017-12-09T22:46:50.756534: step 558, loss 0.536216, acc 0.859375, prec 0.0361302, recall 0.784641
2017-12-09T22:46:51.054369: step 559, loss 0.445619, acc 0.84375, prec 0.0361395, recall 0.784821
2017-12-09T22:46:51.357892: step 560, loss 0.479342, acc 0.835938, prec 0.0362213, recall 0.785358
2017-12-09T22:46:51.661822: step 561, loss 0.406204, acc 0.835938, prec 0.0362661, recall 0.785714
2017-12-09T22:46:51.962744: step 562, loss 0.261241, acc 0.898438, prec 0.0363218, recall 0.78607
2017-12-09T22:46:52.262816: step 563, loss 3.30766, acc 0.882812, prec 0.0363762, recall 0.785773
2017-12-09T22:46:52.572885: step 564, loss 0.364091, acc 0.875, prec 0.0363908, recall 0.78595
2017-12-09T22:46:52.872737: step 565, loss 6.91569, acc 0.882812, prec 0.0365215, recall 0.784716
2017-12-09T22:46:53.177328: step 566, loss 5.40956, acc 0.835938, prec 0.0366054, recall 0.783961
2017-12-09T22:46:53.483905: step 567, loss 0.99316, acc 0.703125, prec 0.0366258, recall 0.784314
2017-12-09T22:46:53.787383: step 568, loss 0.829652, acc 0.710938, prec 0.0366109, recall 0.78449
2017-12-09T22:46:54.084642: step 569, loss 1.05667, acc 0.679688, prec 0.0365905, recall 0.784666
2017-12-09T22:46:54.383170: step 570, loss 1.49309, acc 0.554688, prec 0.0365479, recall 0.784841
2017-12-09T22:46:54.686533: step 571, loss 1.52481, acc 0.554688, prec 0.0365055, recall 0.785016
2017-12-09T22:46:54.981756: step 572, loss 1.54736, acc 0.539062, prec 0.0364241, recall 0.785016
2017-12-09T22:46:55.277167: step 573, loss 1.81113, acc 0.539062, prec 0.0364157, recall 0.785366
2017-12-09T22:46:55.574846: step 574, loss 1.58814, acc 0.523438, prec 0.0364409, recall 0.785888
2017-12-09T22:46:55.868727: step 575, loss 1.41446, acc 0.554688, prec 0.0364353, recall 0.786235
2017-12-09T22:46:56.170395: step 576, loss 1.2841, acc 0.640625, prec 0.0364447, recall 0.78658
2017-12-09T22:46:56.475672: step 577, loss 1.74903, acc 0.65625, prec 0.0364942, recall 0.786463
2017-12-09T22:46:56.771765: step 578, loss 0.904283, acc 0.726562, prec 0.0365184, recall 0.786806
2017-12-09T22:46:57.070486: step 579, loss 0.993488, acc 0.671875, prec 0.0364972, recall 0.786977
2017-12-09T22:46:57.374310: step 580, loss 0.884315, acc 0.78125, prec 0.0364591, recall 0.786977
2017-12-09T22:46:57.669653: step 581, loss 0.800917, acc 0.8125, prec 0.0364624, recall 0.787149
2017-12-09T22:46:57.967306: step 582, loss 1.01815, acc 0.789062, prec 0.036463, recall 0.786688
2017-12-09T22:46:58.269377: step 583, loss 0.55808, acc 0.875, prec 0.0365129, recall 0.78703
2017-12-09T22:46:58.570195: step 584, loss 0.345277, acc 0.867188, prec 0.0365614, recall 0.78737
2017-12-09T22:46:58.865453: step 585, loss 0.463245, acc 0.898438, prec 0.036651, recall 0.787879
2017-12-09T22:46:59.167108: step 586, loss 0.359554, acc 0.90625, prec 0.0366704, recall 0.788048
2017-12-09T22:46:59.474116: step 587, loss 3.4102, acc 0.882812, prec 0.0366514, recall 0.78742
2017-12-09T22:46:59.781772: step 588, loss 2.67443, acc 0.96875, prec 0.0367187, recall 0.787133
2017-12-09T22:47:00.093129: step 589, loss 0.405014, acc 0.945312, prec 0.0367448, recall 0.787302
2017-12-09T22:47:00.398559: step 590, loss 0.471523, acc 0.898438, prec 0.0368341, recall 0.787807
2017-12-09T22:47:00.706357: step 591, loss 0.268802, acc 0.914062, prec 0.0368191, recall 0.787807
2017-12-09T22:47:01.006627: step 592, loss 0.766557, acc 0.859375, prec 0.0368658, recall 0.788142
2017-12-09T22:47:01.308472: step 593, loss 1.15262, acc 0.867188, prec 0.0368796, recall 0.787687
2017-12-09T22:47:01.616677: step 594, loss 0.307542, acc 0.898438, prec 0.0368975, recall 0.787855
2017-12-09T22:47:01.915602: step 595, loss 0.551905, acc 0.78125, prec 0.0368594, recall 0.787855
2017-12-09T22:47:02.219014: step 596, loss 3.27355, acc 0.820312, prec 0.036865, recall 0.787402
2017-12-09T22:47:02.523272: step 597, loss 0.796617, acc 0.757812, prec 0.0368938, recall 0.787736
2017-12-09T22:47:02.820167: step 598, loss 0.603751, acc 0.851562, prec 0.0369389, recall 0.788069
2017-12-09T22:47:03.120165: step 599, loss 0.612479, acc 0.773438, prec 0.0369349, recall 0.788235
2017-12-09T22:47:03.418714: step 600, loss 0.829475, acc 0.773438, prec 0.0369663, recall 0.788567

Evaluation:
2017-12-09T22:47:08.086745: step 600, loss 1.22744, acc 0.766843, prec 0.0378447, recall 0.791579

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-600

2017-12-09T22:47:09.345033: step 601, loss 0.731323, acc 0.734375, prec 0.0378983, recall 0.792017
2017-12-09T22:47:09.645187: step 602, loss 2.69788, acc 0.789062, prec 0.0378976, recall 0.791608
2017-12-09T22:47:09.944851: step 603, loss 1.00059, acc 0.617188, prec 0.0379963, recall 0.792334
2017-12-09T22:47:10.243943: step 604, loss 1.5857, acc 0.757812, prec 0.0380533, recall 0.792768
2017-12-09T22:47:10.543569: step 605, loss 0.924004, acc 0.726562, prec 0.0380409, recall 0.792912
2017-12-09T22:47:10.841107: step 606, loss 1.59711, acc 0.625, prec 0.0380122, recall 0.793056
2017-12-09T22:47:11.141024: step 607, loss 1.23673, acc 0.625, prec 0.0380475, recall 0.793486
2017-12-09T22:47:11.444056: step 608, loss 1.09035, acc 0.640625, prec 0.0380532, recall 0.793772
2017-12-09T22:47:11.743032: step 609, loss 1.43391, acc 0.726562, prec 0.0380422, recall 0.793366
2017-12-09T22:47:12.047013: step 610, loss 0.831593, acc 0.703125, prec 0.0379944, recall 0.793366
2017-12-09T22:47:12.346239: step 611, loss 1.02387, acc 0.664062, prec 0.038004, recall 0.793651
2017-12-09T22:47:12.645809: step 612, loss 0.921149, acc 0.734375, prec 0.0379613, recall 0.793651
2017-12-09T22:47:12.947897: step 613, loss 1.19526, acc 0.757812, prec 0.0380177, recall 0.794077
2017-12-09T22:47:13.249032: step 614, loss 0.79838, acc 0.75, prec 0.0380093, recall 0.794219
2017-12-09T22:47:13.546670: step 615, loss 0.370679, acc 0.851562, prec 0.0379855, recall 0.794219
2017-12-09T22:47:13.841970: step 616, loss 0.367714, acc 0.867188, prec 0.0380276, recall 0.794502
2017-12-09T22:47:14.140517: step 617, loss 1.8296, acc 0.929688, prec 0.0381125, recall 0.79438
2017-12-09T22:47:14.449217: step 618, loss 0.39996, acc 0.890625, prec 0.0381898, recall 0.794802
2017-12-09T22:47:14.760674: step 619, loss 1.82771, acc 0.875, prec 0.0382038, recall 0.793857
2017-12-09T22:47:15.065575: step 620, loss 1.37302, acc 0.890625, prec 0.0382822, recall 0.793737
2017-12-09T22:47:15.363622: step 621, loss 2.08815, acc 0.804688, prec 0.0383467, recall 0.793618
2017-12-09T22:47:15.667897: step 622, loss 0.428789, acc 0.867188, prec 0.0383569, recall 0.793758
2017-12-09T22:47:15.967672: step 623, loss 0.813614, acc 0.78125, prec 0.0384162, recall 0.794177
2017-12-09T22:47:16.270028: step 624, loss 0.765427, acc 0.726562, prec 0.0384037, recall 0.794317
2017-12-09T22:47:16.568332: step 625, loss 0.754958, acc 0.765625, prec 0.038366, recall 0.794317
2017-12-09T22:47:16.868495: step 626, loss 0.841858, acc 0.726562, prec 0.0384163, recall 0.794733
2017-12-09T22:47:17.164786: step 627, loss 0.692586, acc 0.78125, prec 0.0384126, recall 0.794872
2017-12-09T22:47:17.457230: step 628, loss 0.598037, acc 0.789062, prec 0.0384415, recall 0.795148
2017-12-09T22:47:17.756160: step 629, loss 0.644924, acc 0.789062, prec 0.038439, recall 0.795286
2017-12-09T22:47:18.059110: step 630, loss 0.761997, acc 0.742188, prec 0.0384603, recall 0.795562
2017-12-09T22:47:18.359082: step 631, loss 0.576124, acc 0.84375, prec 0.038529, recall 0.795973
2017-12-09T22:47:18.655757: step 632, loss 0.51223, acc 0.804688, prec 0.0384977, recall 0.795973
2017-12-09T22:47:18.957591: step 633, loss 0.438918, acc 0.859375, prec 0.0385688, recall 0.796383
2017-12-09T22:47:19.254335: step 634, loss 4.46347, acc 0.8125, prec 0.0386336, recall 0.796259
2017-12-09T22:47:19.554541: step 635, loss 0.448036, acc 0.859375, prec 0.0386733, recall 0.796531
2017-12-09T22:47:19.848475: step 636, loss 0.660525, acc 0.804688, prec 0.0387354, recall 0.796937
2017-12-09T22:47:20.156305: step 637, loss 0.8643, acc 0.867188, prec 0.0387464, recall 0.796543
2017-12-09T22:47:20.470994: step 638, loss 0.598193, acc 0.796875, prec 0.0387449, recall 0.796678
2017-12-09T22:47:20.773070: step 639, loss 0.520643, acc 0.851562, prec 0.0387832, recall 0.796948
2017-12-09T22:47:21.075338: step 640, loss 0.463597, acc 0.78125, prec 0.0387792, recall 0.797082
2017-12-09T22:47:21.370044: step 641, loss 0.318439, acc 0.875, prec 0.0388212, recall 0.797351
2017-12-09T22:47:21.678507: step 642, loss 0.478685, acc 0.835938, prec 0.0388259, recall 0.797485
2017-12-09T22:47:21.979538: step 643, loss 0.267993, acc 0.890625, prec 0.0388084, recall 0.797485
2017-12-09T22:47:22.282120: step 644, loss 0.442535, acc 0.929688, prec 0.0388281, recall 0.797619
2017-12-09T22:47:22.585138: step 645, loss 0.376952, acc 0.828125, prec 0.0388006, recall 0.797619
2017-12-09T22:47:22.886452: step 646, loss 0.531608, acc 0.867188, prec 0.0388721, recall 0.79802
2017-12-09T22:47:23.187308: step 647, loss 1.15189, acc 0.929688, prec 0.038893, recall 0.797627
2017-12-09T22:47:23.487232: step 648, loss 0.874808, acc 0.882812, prec 0.038936, recall 0.797893
2017-12-09T22:47:23.792039: step 649, loss 2.00145, acc 0.828125, prec 0.0390948, recall 0.798165
2017-12-09T22:47:24.092163: step 650, loss 0.40119, acc 0.851562, prec 0.0391327, recall 0.798429
2017-12-09T22:47:24.388345: step 651, loss 0.35175, acc 0.898438, prec 0.0391472, recall 0.798561
2017-12-09T22:47:24.687619: step 652, loss 0.482563, acc 0.804688, prec 0.0391466, recall 0.798693
2017-12-09T22:47:24.984531: step 653, loss 0.409523, acc 0.875, prec 0.0391573, recall 0.798824
2017-12-09T22:47:25.282210: step 654, loss 0.434037, acc 0.84375, prec 0.0391322, recall 0.798824
2017-12-09T22:47:25.578512: step 655, loss 1.20654, acc 0.875, prec 0.0391737, recall 0.799087
2017-12-09T22:47:25.877072: step 656, loss 0.581868, acc 0.859375, prec 0.0391818, recall 0.799218
2017-12-09T22:47:26.178693: step 657, loss 0.303733, acc 0.898438, prec 0.0391656, recall 0.799218
2017-12-09T22:47:26.484124: step 658, loss 0.523071, acc 0.914062, prec 0.0391825, recall 0.799349
2017-12-09T22:47:26.792023: step 659, loss 0.411726, acc 0.859375, prec 0.0391907, recall 0.799479
2017-12-09T22:47:27.092109: step 660, loss 0.35103, acc 0.84375, prec 0.0392269, recall 0.79974
2017-12-09T22:47:27.395192: step 661, loss 0.655565, acc 0.8125, prec 0.0392582, recall 0.8
2017-12-09T22:47:27.698538: step 662, loss 0.608693, acc 0.78125, prec 0.0392843, recall 0.800259
2017-12-09T22:47:28.007298: step 663, loss 0.536217, acc 0.851562, prec 0.0392912, recall 0.800389
2017-12-09T22:47:28.311137: step 664, loss 1.18221, acc 0.835938, prec 0.0393273, recall 0.800129
2017-12-09T22:47:28.611290: step 665, loss 0.344165, acc 0.90625, prec 0.0393733, recall 0.800388
2017-12-09T22:47:28.909223: step 666, loss 0.533365, acc 0.851562, prec 0.0394411, recall 0.800774
2017-12-09T22:47:29.212805: step 667, loss 0.507996, acc 0.851562, prec 0.0394173, recall 0.800774
2017-12-09T22:47:29.512660: step 668, loss 0.491891, acc 0.851562, prec 0.0393936, recall 0.800774
2017-12-09T22:47:29.814067: step 669, loss 0.84927, acc 0.875, prec 0.0394649, recall 0.801158
2017-12-09T22:47:30.117805: step 670, loss 4.80005, acc 0.851562, prec 0.0395654, recall 0.800641
2017-12-09T22:47:30.423818: step 671, loss 1.34185, acc 0.898438, prec 0.0395503, recall 0.800128
2017-12-09T22:47:30.721067: step 672, loss 1.65922, acc 0.84375, prec 0.0396469, recall 0.800639
2017-12-09T22:47:31.021514: step 673, loss 3.39948, acc 0.773438, prec 0.0396118, recall 0.800128
2017-12-09T22:47:31.317415: step 674, loss 0.739971, acc 0.71875, prec 0.0395971, recall 0.800255
2017-12-09T22:47:31.617776: step 675, loss 1.03805, acc 0.710938, prec 0.0395812, recall 0.800383
2017-12-09T22:47:31.918889: step 676, loss 1.22927, acc 0.601562, prec 0.0396083, recall 0.800764
2017-12-09T22:47:32.215184: step 677, loss 1.33362, acc 0.546875, prec 0.0396267, recall 0.801144
2017-12-09T22:47:32.511797: step 678, loss 1.13511, acc 0.617188, prec 0.0395959, recall 0.80127
2017-12-09T22:47:32.809294: step 679, loss 1.27339, acc 0.5625, prec 0.0395866, recall 0.801522
2017-12-09T22:47:33.107888: step 680, loss 1.34319, acc 0.578125, prec 0.0396399, recall 0.802024
2017-12-09T22:47:33.411251: step 681, loss 1.44866, acc 0.671875, prec 0.0397078, recall 0.802524
2017-12-09T22:47:33.710685: step 682, loss 0.69251, acc 0.71875, prec 0.0397531, recall 0.802897
2017-12-09T22:47:34.005298: step 683, loss 0.948403, acc 0.734375, prec 0.0397708, recall 0.803145
2017-12-09T22:47:34.302055: step 684, loss 0.869529, acc 0.671875, prec 0.0397487, recall 0.803268
2017-12-09T22:47:34.603734: step 685, loss 0.754485, acc 0.773438, prec 0.0398024, recall 0.803639
2017-12-09T22:47:34.902151: step 686, loss 0.541289, acc 0.804688, prec 0.0398311, recall 0.803885
2017-12-09T22:47:35.197525: step 687, loss 0.389437, acc 0.859375, prec 0.0398089, recall 0.803885
2017-12-09T22:47:35.508411: step 688, loss 0.972033, acc 0.828125, prec 0.0399306, recall 0.804497
2017-12-09T22:47:35.808065: step 689, loss 0.354109, acc 0.898438, prec 0.0400037, recall 0.804863
2017-12-09T22:47:36.109016: step 690, loss 0.473579, acc 0.84375, prec 0.0400384, recall 0.805106
2017-12-09T22:47:36.406067: step 691, loss 1.37074, acc 0.867188, prec 0.040078, recall 0.804848
2017-12-09T22:47:36.705802: step 692, loss 2.67077, acc 0.875, prec 0.0401485, recall 0.804712
2017-12-09T22:47:37.008956: step 693, loss 2.33948, acc 0.867188, prec 0.0401583, recall 0.804334
2017-12-09T22:47:37.308685: step 694, loss 0.367627, acc 0.890625, prec 0.0402595, recall 0.804818
2017-12-09T22:47:37.614829: step 695, loss 0.279937, acc 0.929688, prec 0.040278, recall 0.804938
2017-12-09T22:47:37.915192: step 696, loss 0.376904, acc 0.84375, prec 0.0402531, recall 0.804938
2017-12-09T22:47:38.212780: step 697, loss 0.396068, acc 0.882812, prec 0.0402937, recall 0.805179
2017-12-09T22:47:38.514841: step 698, loss 0.493937, acc 0.835938, prec 0.0402972, recall 0.805299
2017-12-09T22:47:38.818552: step 699, loss 0.372633, acc 0.84375, prec 0.0403315, recall 0.805538
2017-12-09T22:47:39.116348: step 700, loss 0.38068, acc 0.882812, prec 0.0403425, recall 0.805658
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-700

2017-12-09T22:47:40.378128: step 701, loss 0.419558, acc 0.867188, prec 0.0403509, recall 0.805777
2017-12-09T22:47:40.676145: step 702, loss 0.779606, acc 0.84375, prec 0.0403556, recall 0.805897
2017-12-09T22:47:40.976147: step 703, loss 0.356961, acc 0.898438, prec 0.0403394, recall 0.805897
2017-12-09T22:47:41.275060: step 704, loss 0.255757, acc 0.921875, prec 0.040327, recall 0.805897
2017-12-09T22:47:41.576048: step 705, loss 2.51768, acc 0.835938, prec 0.040333, recall 0.805028
2017-12-09T22:47:41.885042: step 706, loss 0.709304, acc 0.9375, prec 0.040382, recall 0.805266
2017-12-09T22:47:42.184581: step 707, loss 0.417311, acc 0.851562, prec 0.0403879, recall 0.805386
2017-12-09T22:47:42.483514: step 708, loss 0.828502, acc 0.851562, prec 0.0403938, recall 0.805505
2017-12-09T22:47:42.781290: step 709, loss 0.489117, acc 0.820312, prec 0.0403948, recall 0.805623
2017-12-09T22:47:43.079717: step 710, loss 0.519009, acc 0.820312, prec 0.0404251, recall 0.805861
2017-12-09T22:47:43.377877: step 711, loss 0.567986, acc 0.820312, prec 0.040426, recall 0.805979
2017-12-09T22:47:43.679857: step 712, loss 1.6777, acc 0.773438, prec 0.0405087, recall 0.805961
2017-12-09T22:47:43.976187: step 713, loss 0.704739, acc 0.78125, prec 0.0405327, recall 0.806197
2017-12-09T22:47:44.276147: step 714, loss 0.539892, acc 0.820312, prec 0.0405335, recall 0.806315
2017-12-09T22:47:44.574080: step 715, loss 0.449614, acc 0.820312, prec 0.0405344, recall 0.806432
2017-12-09T22:47:44.876304: step 716, loss 0.780931, acc 0.765625, prec 0.0405265, recall 0.806549
2017-12-09T22:47:45.172861: step 717, loss 1.25589, acc 0.679688, prec 0.0405344, recall 0.806784
2017-12-09T22:47:45.476171: step 718, loss 0.593999, acc 0.75, prec 0.0405241, recall 0.806901
2017-12-09T22:47:45.774879: step 719, loss 0.593373, acc 0.789062, prec 0.0405492, recall 0.807134
2017-12-09T22:47:46.075115: step 720, loss 0.679134, acc 0.789062, prec 0.0405451, recall 0.807251
2017-12-09T22:47:46.372626: step 721, loss 0.386978, acc 0.90625, prec 0.0405594, recall 0.807367
2017-12-09T22:47:46.671656: step 722, loss 0.343361, acc 0.859375, prec 0.0405373, recall 0.807367
2017-12-09T22:47:46.971413: step 723, loss 0.677172, acc 0.875, prec 0.040692, recall 0.808063
2017-12-09T22:47:47.272038: step 724, loss 0.506092, acc 0.835938, prec 0.0407533, recall 0.808408
2017-12-09T22:47:47.568429: step 725, loss 0.60609, acc 0.84375, prec 0.0407867, recall 0.808638
2017-12-09T22:47:47.875223: step 726, loss 0.38765, acc 0.90625, prec 0.0408009, recall 0.808753
2017-12-09T22:47:48.177609: step 727, loss 0.398665, acc 0.859375, prec 0.0408367, recall 0.808982
2017-12-09T22:47:48.473867: step 728, loss 2.19977, acc 0.882812, prec 0.0408786, recall 0.808244
2017-12-09T22:47:48.774066: step 729, loss 0.367628, acc 0.9375, prec 0.0409267, recall 0.808473
2017-12-09T22:47:49.071977: step 730, loss 0.349829, acc 0.867188, prec 0.0409346, recall 0.808587
2017-12-09T22:47:49.375328: step 731, loss 0.294974, acc 0.945312, prec 0.0410417, recall 0.809042
2017-12-09T22:47:49.676228: step 732, loss 4.05944, acc 0.867188, prec 0.0410521, recall 0.808195
2017-12-09T22:47:49.974296: step 733, loss 0.479636, acc 0.835938, prec 0.0410839, recall 0.808422
2017-12-09T22:47:50.276310: step 734, loss 0.387608, acc 0.851562, prec 0.0410604, recall 0.808422
2017-12-09T22:47:50.582612: step 735, loss 3.46981, acc 0.796875, prec 0.041032, recall 0.806986
2017-12-09T22:47:50.881410: step 736, loss 0.618556, acc 0.78125, prec 0.0410263, recall 0.807101
2017-12-09T22:47:51.176635: step 737, loss 0.895484, acc 0.734375, prec 0.041042, recall 0.807329
2017-12-09T22:47:51.470682: step 738, loss 0.725859, acc 0.710938, prec 0.0410827, recall 0.80767
2017-12-09T22:47:51.772207: step 739, loss 0.81534, acc 0.75, prec 0.0410721, recall 0.807783
2017-12-09T22:47:52.068070: step 740, loss 1.24084, acc 0.617188, prec 0.0410692, recall 0.808009
2017-12-09T22:47:52.370880: step 741, loss 1.26579, acc 0.609375, prec 0.0410365, recall 0.808122
2017-12-09T22:47:52.666930: step 742, loss 1.04521, acc 0.65625, prec 0.0409826, recall 0.808122
2017-12-09T22:47:52.964798: step 743, loss 1.17778, acc 0.710938, prec 0.0409946, recall 0.808348
2017-12-09T22:47:53.263984: step 744, loss 1.04624, acc 0.671875, prec 0.0410576, recall 0.808798
2017-12-09T22:47:53.560441: step 745, loss 1.1524, acc 0.734375, prec 0.0410446, recall 0.80891
2017-12-09T22:47:53.853775: step 746, loss 0.890229, acc 0.6875, prec 0.0409958, recall 0.80891
2017-12-09T22:47:54.031344: step 747, loss 0.790293, acc 0.75, prec 0.04098, recall 0.80891
2017-12-09T22:47:54.338956: step 748, loss 0.493004, acc 0.828125, prec 0.0409817, recall 0.809022
2017-12-09T22:47:54.641935: step 749, loss 0.769661, acc 0.75, prec 0.0409996, recall 0.809245
2017-12-09T22:47:54.941580: step 750, loss 0.354068, acc 0.859375, prec 0.0410346, recall 0.809468
2017-12-09T22:47:55.239858: step 751, loss 1.14652, acc 0.8125, prec 0.0410906, recall 0.809802
2017-12-09T22:47:55.538340: step 752, loss 0.503016, acc 0.84375, prec 0.0410947, recall 0.809913
2017-12-09T22:47:55.837935: step 753, loss 0.445541, acc 0.851562, prec 0.0411283, recall 0.810134
2017-12-09T22:47:56.137237: step 754, loss 1.60026, acc 0.859375, prec 0.0411643, recall 0.809884
2017-12-09T22:47:56.440895: step 755, loss 0.366194, acc 0.890625, prec 0.0412039, recall 0.810105
2017-12-09T22:47:56.746603: step 756, loss 0.308973, acc 0.898438, prec 0.0412164, recall 0.810215
2017-12-09T22:47:57.052126: step 757, loss 0.467309, acc 0.914062, prec 0.0413162, recall 0.810654
2017-12-09T22:47:57.351304: step 758, loss 0.589639, acc 0.921875, prec 0.0413606, recall 0.810873
2017-12-09T22:47:57.650137: step 759, loss 0.42373, acc 0.875, prec 0.0413976, recall 0.811092
2017-12-09T22:47:57.949252: step 760, loss 0.247016, acc 0.921875, prec 0.0414419, recall 0.81131
2017-12-09T22:47:58.247227: step 761, loss 0.226984, acc 0.898438, prec 0.0414543, recall 0.811419
2017-12-09T22:47:58.543705: step 762, loss 0.2726, acc 0.921875, prec 0.0414421, recall 0.811419
2017-12-09T22:47:58.846726: step 763, loss 0.281873, acc 0.914062, prec 0.0414287, recall 0.811419
2017-12-09T22:47:59.148914: step 764, loss 0.841036, acc 0.953125, prec 0.041506, recall 0.811744
2017-12-09T22:47:59.448956: step 765, loss 0.856525, acc 0.890625, prec 0.0415465, recall 0.811494
2017-12-09T22:47:59.750519: step 766, loss 2.48093, acc 0.898438, prec 0.0415882, recall 0.811245
2017-12-09T22:48:00.060008: step 767, loss 0.394132, acc 0.875, prec 0.0415687, recall 0.811245
2017-12-09T22:48:00.368050: step 768, loss 0.471475, acc 0.828125, prec 0.04157, recall 0.811353
2017-12-09T22:48:00.671576: step 769, loss 0.757727, acc 0.84375, prec 0.0416018, recall 0.811569
2017-12-09T22:48:00.969513: step 770, loss 0.592998, acc 0.84375, prec 0.0416055, recall 0.811677
2017-12-09T22:48:01.276499: step 771, loss 0.619133, acc 0.867188, prec 0.041641, recall 0.811893
2017-12-09T22:48:01.576841: step 772, loss 0.90213, acc 0.859375, prec 0.0416764, recall 0.811644
2017-12-09T22:48:01.873754: step 773, loss 0.638929, acc 0.820312, prec 0.0416764, recall 0.811751
2017-12-09T22:48:02.172098: step 774, loss 0.678571, acc 0.796875, prec 0.0417008, recall 0.811966
2017-12-09T22:48:02.471832: step 775, loss 0.621242, acc 0.757812, prec 0.041663, recall 0.811966
2017-12-09T22:48:02.775703: step 776, loss 0.994181, acc 0.84375, prec 0.0416947, recall 0.81218
2017-12-09T22:48:03.070965: step 777, loss 0.446681, acc 0.835938, prec 0.041753, recall 0.8125
2017-12-09T22:48:03.374931: step 778, loss 0.56534, acc 0.765625, prec 0.0417445, recall 0.812606
2017-12-09T22:48:03.670989: step 779, loss 0.628175, acc 0.835938, prec 0.0418027, recall 0.812925
2017-12-09T22:48:03.970255: step 780, loss 0.678553, acc 0.789062, prec 0.0418815, recall 0.813348
2017-12-09T22:48:04.268266: step 781, loss 0.474523, acc 0.8125, prec 0.041908, recall 0.813559
2017-12-09T22:48:04.568501: step 782, loss 0.750469, acc 0.804688, prec 0.0419332, recall 0.81377
2017-12-09T22:48:04.866626: step 783, loss 0.45533, acc 0.835938, prec 0.0419912, recall 0.814085
2017-12-09T22:48:05.169971: step 784, loss 0.323028, acc 0.859375, prec 0.0420249, recall 0.814294
2017-12-09T22:48:05.484030: step 785, loss 0.439166, acc 0.84375, prec 0.0420005, recall 0.814294
2017-12-09T22:48:05.780661: step 786, loss 0.319213, acc 0.914062, prec 0.0420427, recall 0.814503
2017-12-09T22:48:06.075273: step 787, loss 0.323338, acc 0.890625, prec 0.0421089, recall 0.814815
2017-12-09T22:48:06.371476: step 788, loss 2.37475, acc 0.882812, prec 0.0421196, recall 0.814462
2017-12-09T22:48:06.672728: step 789, loss 2.8394, acc 0.859375, prec 0.0421544, recall 0.814214
2017-12-09T22:48:06.973076: step 790, loss 0.41164, acc 0.882812, prec 0.0421915, recall 0.814421
2017-12-09T22:48:07.273041: step 791, loss 0.2488, acc 0.929688, prec 0.0422083, recall 0.814525
2017-12-09T22:48:07.570913: step 792, loss 0.917782, acc 0.890625, prec 0.042302, recall 0.814939
2017-12-09T22:48:07.873216: step 793, loss 0.499071, acc 0.882812, prec 0.0423944, recall 0.81535
2017-12-09T22:48:08.177502: step 794, loss 0.531713, acc 0.851562, prec 0.0423988, recall 0.815453
2017-12-09T22:48:08.483430: step 795, loss 0.476698, acc 0.828125, prec 0.0425102, recall 0.815965
2017-12-09T22:48:08.781754: step 796, loss 0.285906, acc 0.867188, prec 0.042517, recall 0.816067
2017-12-09T22:48:09.079786: step 797, loss 0.54237, acc 0.835938, prec 0.0426017, recall 0.816473
2017-12-09T22:48:09.376607: step 798, loss 0.406021, acc 0.882812, prec 0.0425832, recall 0.816473
2017-12-09T22:48:09.673780: step 799, loss 0.598224, acc 0.875, prec 0.0426188, recall 0.816676
2017-12-09T22:48:09.976280: step 800, loss 0.403239, acc 0.875, prec 0.0426819, recall 0.816979
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-800

2017-12-09T22:48:11.257572: step 801, loss 0.598159, acc 0.828125, prec 0.0427375, recall 0.817281
2017-12-09T22:48:11.554995: step 802, loss 0.571804, acc 0.867188, prec 0.0427992, recall 0.817582
2017-12-09T22:48:11.858461: step 803, loss 0.191654, acc 0.9375, prec 0.0428169, recall 0.817683
2017-12-09T22:48:12.152922: step 804, loss 0.791082, acc 0.835938, prec 0.0428461, recall 0.817883
2017-12-09T22:48:12.447132: step 805, loss 0.805703, acc 0.804688, prec 0.0428703, recall 0.818082
2017-12-09T22:48:12.749382: step 806, loss 0.288836, acc 0.90625, prec 0.042883, recall 0.818182
2017-12-09T22:48:13.041179: step 807, loss 0.30659, acc 0.921875, prec 0.0429256, recall 0.818381
2017-12-09T22:48:13.337305: step 808, loss 0.437577, acc 0.8125, prec 0.0429235, recall 0.81848
2017-12-09T22:48:13.637381: step 809, loss 1.27617, acc 0.828125, prec 0.0429525, recall 0.818231
2017-12-09T22:48:13.936718: step 810, loss 1.49812, acc 0.867188, prec 0.0430151, recall 0.818083
2017-12-09T22:48:14.241956: step 811, loss 0.254073, acc 0.914062, prec 0.0430015, recall 0.818083
2017-12-09T22:48:14.539661: step 812, loss 0.237105, acc 0.90625, prec 0.0429868, recall 0.818083
2017-12-09T22:48:14.840931: step 813, loss 0.435943, acc 0.914062, prec 0.043028, recall 0.818281
2017-12-09T22:48:15.146163: step 814, loss 0.211144, acc 0.929688, prec 0.0430169, recall 0.818281
2017-12-09T22:48:15.449822: step 815, loss 1.94533, acc 0.882812, prec 0.043027, recall 0.817935
2017-12-09T22:48:15.750003: step 816, loss 1.15982, acc 0.890625, prec 0.0430658, recall 0.817689
2017-12-09T22:48:16.053935: step 817, loss 0.987293, acc 0.867188, prec 0.0431007, recall 0.817443
2017-12-09T22:48:16.356691: step 818, loss 0.499768, acc 0.828125, prec 0.0431283, recall 0.817641
2017-12-09T22:48:16.654366: step 819, loss 0.465866, acc 0.835938, prec 0.0431298, recall 0.817739
2017-12-09T22:48:16.957789: step 820, loss 0.639185, acc 0.75, prec 0.0431177, recall 0.817838
2017-12-09T22:48:17.252241: step 821, loss 0.559528, acc 0.804688, prec 0.0431142, recall 0.817936
2017-12-09T22:48:17.547885: step 822, loss 0.864292, acc 0.820312, prec 0.0431133, recall 0.818035
2017-12-09T22:48:17.847148: step 823, loss 0.930531, acc 0.765625, prec 0.0431581, recall 0.818329
2017-12-09T22:48:18.148257: step 824, loss 0.618238, acc 0.820312, prec 0.0431843, recall 0.818524
2017-12-09T22:48:18.449128: step 825, loss 0.676018, acc 0.789062, prec 0.0432055, recall 0.81872
2017-12-09T22:48:18.744502: step 826, loss 0.665354, acc 0.820312, prec 0.0432587, recall 0.819012
2017-12-09T22:48:19.041741: step 827, loss 0.658957, acc 0.8125, prec 0.0432564, recall 0.819109
2017-12-09T22:48:19.342367: step 828, loss 0.562532, acc 0.804688, prec 0.0432529, recall 0.819206
2017-12-09T22:48:19.643952: step 829, loss 0.683804, acc 0.84375, prec 0.0433097, recall 0.819497
2017-12-09T22:48:19.945946: step 830, loss 0.497826, acc 0.84375, prec 0.0433122, recall 0.819593
2017-12-09T22:48:20.250715: step 831, loss 0.877991, acc 0.820312, prec 0.0433381, recall 0.819786
2017-12-09T22:48:20.559784: step 832, loss 0.377604, acc 0.851562, prec 0.0433689, recall 0.819979
2017-12-09T22:48:20.857401: step 833, loss 0.491632, acc 0.859375, prec 0.0434009, recall 0.820171
2017-12-09T22:48:21.154565: step 834, loss 0.299249, acc 0.851562, prec 0.0433776, recall 0.820171
2017-12-09T22:48:21.454244: step 835, loss 0.333055, acc 0.867188, prec 0.0433568, recall 0.820171
2017-12-09T22:48:21.752169: step 836, loss 0.172151, acc 0.945312, prec 0.0433753, recall 0.820267
2017-12-09T22:48:22.048961: step 837, loss 1.14025, acc 0.898438, prec 0.0433876, recall 0.819925
2017-12-09T22:48:22.348198: step 838, loss 1.14277, acc 0.859375, prec 0.0433937, recall 0.819585
2017-12-09T22:48:22.644632: step 839, loss 0.525297, acc 0.921875, prec 0.0434354, recall 0.819777
2017-12-09T22:48:22.940114: step 840, loss 0.28782, acc 0.929688, prec 0.0434783, recall 0.819968
2017-12-09T22:48:23.237437: step 841, loss 0.215654, acc 0.898438, prec 0.0434893, recall 0.820064
2017-12-09T22:48:23.534588: step 842, loss 0.377464, acc 0.859375, prec 0.0434942, recall 0.820159
2017-12-09T22:48:23.837200: step 843, loss 0.194406, acc 0.921875, prec 0.0434819, recall 0.820159
2017-12-09T22:48:24.134195: step 844, loss 2.06055, acc 0.90625, prec 0.0434697, recall 0.81929
2017-12-09T22:48:24.434056: step 845, loss 4.34527, acc 0.882812, prec 0.0435076, recall 0.818614
2017-12-09T22:48:24.735905: step 846, loss 0.646615, acc 0.84375, prec 0.0435369, recall 0.818806
2017-12-09T22:48:25.039274: step 847, loss 0.272158, acc 0.890625, prec 0.0435198, recall 0.818806
2017-12-09T22:48:25.337686: step 848, loss 0.56358, acc 0.835938, prec 0.0435478, recall 0.818997
2017-12-09T22:48:25.635960: step 849, loss 0.979975, acc 0.703125, prec 0.0435282, recall 0.819093
2017-12-09T22:48:25.935770: step 850, loss 0.529026, acc 0.84375, prec 0.0435574, recall 0.819283
2017-12-09T22:48:26.234643: step 851, loss 1.48469, acc 0.804688, prec 0.0435817, recall 0.819043
2017-12-09T22:48:26.533370: step 852, loss 0.749612, acc 0.796875, prec 0.043657, recall 0.819423
2017-12-09T22:48:26.831845: step 853, loss 0.610566, acc 0.734375, prec 0.0436155, recall 0.819423
2017-12-09T22:48:27.128450: step 854, loss 0.776315, acc 0.773438, prec 0.043687, recall 0.819801
2017-12-09T22:48:27.426932: step 855, loss 0.934793, acc 0.757812, prec 0.0437026, recall 0.81999
2017-12-09T22:48:27.735044: step 856, loss 0.663176, acc 0.742188, prec 0.0436624, recall 0.81999
2017-12-09T22:48:28.037170: step 857, loss 1.2832, acc 0.859375, prec 0.043695, recall 0.819749
2017-12-09T22:48:28.336814: step 858, loss 0.969118, acc 0.757812, prec 0.0437903, recall 0.820219
2017-12-09T22:48:28.635914: step 859, loss 0.643263, acc 0.773438, prec 0.0437816, recall 0.820312
2017-12-09T22:48:28.934219: step 860, loss 0.82386, acc 0.78125, prec 0.0438272, recall 0.820593
2017-12-09T22:48:29.234842: step 861, loss 0.804541, acc 0.734375, prec 0.0438124, recall 0.820686
2017-12-09T22:48:29.532093: step 862, loss 0.458842, acc 0.828125, prec 0.0437857, recall 0.820686
2017-12-09T22:48:29.826830: step 863, loss 0.425935, acc 0.84375, prec 0.0437614, recall 0.820686
2017-12-09T22:48:30.130374: step 864, loss 0.578344, acc 0.835938, prec 0.043736, recall 0.820686
2017-12-09T22:48:30.439572: step 865, loss 0.590474, acc 0.851562, prec 0.0437394, recall 0.820779
2017-12-09T22:48:30.741977: step 866, loss 0.334101, acc 0.859375, prec 0.0437177, recall 0.820779
2017-12-09T22:48:31.042164: step 867, loss 1.29903, acc 0.898438, prec 0.0438342, recall 0.821244
2017-12-09T22:48:31.342162: step 868, loss 0.221713, acc 0.945312, prec 0.043905, recall 0.821521
2017-12-09T22:48:31.637943: step 869, loss 0.315344, acc 0.875, prec 0.0439649, recall 0.821798
2017-12-09T22:48:31.940866: step 870, loss 0.360697, acc 0.875, prec 0.0439454, recall 0.821798
2017-12-09T22:48:32.238715: step 871, loss 0.36933, acc 0.882812, prec 0.0439272, recall 0.821798
2017-12-09T22:48:32.538759: step 872, loss 0.377495, acc 0.90625, prec 0.0439918, recall 0.822073
2017-12-09T22:48:32.841108: step 873, loss 0.606553, acc 0.914062, prec 0.0440312, recall 0.822257
2017-12-09T22:48:33.142083: step 874, loss 0.468669, acc 0.851562, prec 0.0440345, recall 0.822348
2017-12-09T22:48:33.442294: step 875, loss 0.39774, acc 0.9375, prec 0.0441302, recall 0.822713
2017-12-09T22:48:33.741688: step 876, loss 0.560547, acc 0.921875, prec 0.0442234, recall 0.823077
2017-12-09T22:48:34.042770: step 877, loss 0.50981, acc 0.875, prec 0.0442829, recall 0.823349
2017-12-09T22:48:34.345275: step 878, loss 1.01582, acc 0.921875, prec 0.0443496, recall 0.82362
2017-12-09T22:48:34.645491: step 879, loss 0.266212, acc 0.90625, prec 0.0443876, recall 0.8238
2017-12-09T22:48:34.946575: step 880, loss 0.811199, acc 0.882812, prec 0.0444481, recall 0.824069
2017-12-09T22:48:35.249097: step 881, loss 0.454214, acc 0.921875, prec 0.0444622, recall 0.824159
2017-12-09T22:48:35.556322: step 882, loss 0.297129, acc 0.882812, prec 0.0444964, recall 0.824338
2017-12-09T22:48:35.859176: step 883, loss 0.701559, acc 0.898438, prec 0.0445592, recall 0.824606
2017-12-09T22:48:36.157822: step 884, loss 0.738894, acc 0.914062, prec 0.0446245, recall 0.824873
2017-12-09T22:48:36.471091: step 885, loss 0.62208, acc 0.875, prec 0.0446835, recall 0.825139
2017-12-09T22:48:36.770263: step 886, loss 0.436909, acc 0.882812, prec 0.0446914, recall 0.825228
2017-12-09T22:48:37.073730: step 887, loss 0.467992, acc 0.84375, prec 0.0447454, recall 0.825493
2017-12-09T22:48:37.375800: step 888, loss 0.60023, acc 0.828125, prec 0.0447446, recall 0.825581
2017-12-09T22:48:37.678383: step 889, loss 0.822009, acc 0.796875, prec 0.0447651, recall 0.825758
2017-12-09T22:48:37.975947: step 890, loss 0.515452, acc 0.820312, prec 0.0447631, recall 0.825846
2017-12-09T22:48:38.278354: step 891, loss 1.45889, acc 0.851562, prec 0.0447671, recall 0.825517
2017-12-09T22:48:38.578303: step 892, loss 0.626087, acc 0.859375, prec 0.0447973, recall 0.825693
2017-12-09T22:48:38.875549: step 893, loss 0.331019, acc 0.921875, prec 0.0448112, recall 0.82578
2017-12-09T22:48:39.174761: step 894, loss 0.488521, acc 0.828125, prec 0.0449147, recall 0.826218
2017-12-09T22:48:39.472450: step 895, loss 0.719693, acc 0.84375, prec 0.0449683, recall 0.826479
2017-12-09T22:48:39.771827: step 896, loss 1.87638, acc 0.78125, prec 0.0450134, recall 0.826326
2017-12-09T22:48:40.076452: step 897, loss 0.480112, acc 0.828125, prec 0.0449864, recall 0.826326
2017-12-09T22:48:40.374292: step 898, loss 0.529121, acc 0.84375, prec 0.0449879, recall 0.826413
2017-12-09T22:48:40.671548: step 899, loss 0.520648, acc 0.828125, prec 0.0449869, recall 0.8265
2017-12-09T22:48:40.969997: step 900, loss 0.6242, acc 0.78125, prec 0.0450046, recall 0.826673

Evaluation:
2017-12-09T22:48:45.694616: step 900, loss 1.301, acc 0.824023, prec 0.0454534, recall 0.81814

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-900

2017-12-09T22:48:46.988968: step 901, loss 0.800593, acc 0.765625, prec 0.0454675, recall 0.818309
2017-12-09T22:48:47.289590: step 902, loss 0.483041, acc 0.835938, prec 0.0454921, recall 0.818477
2017-12-09T22:48:47.592916: step 903, loss 0.547256, acc 0.789062, prec 0.0454604, recall 0.818477
2017-12-09T22:48:47.890711: step 904, loss 0.436097, acc 0.867188, prec 0.0454651, recall 0.818561
2017-12-09T22:48:48.190895: step 905, loss 0.45352, acc 0.890625, prec 0.0454733, recall 0.818646
2017-12-09T22:48:48.497712: step 906, loss 1.411, acc 0.921875, prec 0.0454873, recall 0.81835
2017-12-09T22:48:48.799883: step 907, loss 0.917255, acc 0.890625, prec 0.0454967, recall 0.818056
2017-12-09T22:48:49.103483: step 908, loss 0.264144, acc 0.898438, prec 0.0455306, recall 0.818224
2017-12-09T22:48:49.405603: step 909, loss 0.180061, acc 0.898438, prec 0.0455154, recall 0.818224
2017-12-09T22:48:49.707242: step 910, loss 1.22522, acc 0.9375, prec 0.0456042, recall 0.81856
2017-12-09T22:48:50.007912: step 911, loss 0.200423, acc 0.921875, prec 0.0455925, recall 0.81856
2017-12-09T22:48:50.308748: step 912, loss 0.493474, acc 0.859375, prec 0.0456204, recall 0.818727
2017-12-09T22:48:50.612455: step 913, loss 0.256291, acc 0.90625, prec 0.0456309, recall 0.818811
2017-12-09T22:48:50.913001: step 914, loss 0.31002, acc 0.921875, prec 0.0456682, recall 0.818977
2017-12-09T22:48:51.221012: step 915, loss 0.27246, acc 0.914062, prec 0.0457043, recall 0.819144
2017-12-09T22:48:51.523892: step 916, loss 1.51398, acc 0.90625, prec 0.0457649, recall 0.819017
2017-12-09T22:48:51.828321: step 917, loss 2.37227, acc 0.867188, prec 0.0457706, recall 0.818724
2017-12-09T22:48:52.131347: step 918, loss 0.365171, acc 0.875, prec 0.0457763, recall 0.818807
2017-12-09T22:48:52.431160: step 919, loss 0.398011, acc 0.890625, prec 0.0457843, recall 0.81889
2017-12-09T22:48:52.727738: step 920, loss 0.494044, acc 0.851562, prec 0.045762, recall 0.81889
2017-12-09T22:48:53.022158: step 921, loss 0.49199, acc 0.804688, prec 0.0457572, recall 0.818973
2017-12-09T22:48:53.318454: step 922, loss 0.82538, acc 0.828125, prec 0.0458047, recall 0.819222
2017-12-09T22:48:53.617637: step 923, loss 0.877491, acc 0.742188, prec 0.0458392, recall 0.81947
2017-12-09T22:48:53.913415: step 924, loss 0.358974, acc 0.835938, prec 0.0458146, recall 0.81947
2017-12-09T22:48:54.205390: step 925, loss 0.579143, acc 0.789062, prec 0.0459292, recall 0.819964
2017-12-09T22:48:54.501274: step 926, loss 0.482024, acc 0.804688, prec 0.0459242, recall 0.820046
2017-12-09T22:48:54.803525: step 927, loss 0.365836, acc 0.859375, prec 0.0459031, recall 0.820046
2017-12-09T22:48:55.104438: step 928, loss 0.587479, acc 0.8125, prec 0.0458751, recall 0.820046
2017-12-09T22:48:55.403557: step 929, loss 0.555961, acc 0.859375, prec 0.0459512, recall 0.820373
2017-12-09T22:48:55.705928: step 930, loss 1.53261, acc 0.828125, prec 0.045951, recall 0.820082
2017-12-09T22:48:56.003811: step 931, loss 0.473384, acc 0.867188, prec 0.0459554, recall 0.820163
2017-12-09T22:48:56.297161: step 932, loss 0.485031, acc 0.851562, prec 0.0459332, recall 0.820163
2017-12-09T22:48:56.596759: step 933, loss 0.319702, acc 0.882812, prec 0.0459156, recall 0.820163
2017-12-09T22:48:56.899428: step 934, loss 0.347567, acc 0.851562, prec 0.0458935, recall 0.820163
2017-12-09T22:48:57.192973: step 935, loss 0.366465, acc 0.898438, prec 0.045951, recall 0.820408
2017-12-09T22:48:57.493359: step 936, loss 0.396259, acc 0.859375, prec 0.0459542, recall 0.82049
2017-12-09T22:48:57.792863: step 937, loss 3.60925, acc 0.867188, prec 0.0459852, recall 0.81991
2017-12-09T22:48:58.097650: step 938, loss 0.627248, acc 0.875, prec 0.0460391, recall 0.820154
2017-12-09T22:48:58.396468: step 939, loss 0.900687, acc 0.90625, prec 0.0460263, recall 0.819783
2017-12-09T22:48:58.698157: step 940, loss 0.227724, acc 0.9375, prec 0.0460411, recall 0.819865
2017-12-09T22:48:58.996262: step 941, loss 0.402142, acc 0.875, prec 0.0460466, recall 0.819946
2017-12-09T22:48:59.293681: step 942, loss 0.506431, acc 0.84375, prec 0.0460233, recall 0.819946
2017-12-09T22:48:59.595399: step 943, loss 1.02501, acc 0.835938, prec 0.0460483, recall 0.819739
2017-12-09T22:48:59.898433: step 944, loss 0.332377, acc 0.851562, prec 0.0460503, recall 0.81982
2017-12-09T22:49:00.204450: step 945, loss 0.54312, acc 0.859375, prec 0.0461258, recall 0.820144
2017-12-09T22:49:00.513959: step 946, loss 0.780217, acc 0.84375, prec 0.0461507, recall 0.820305
2017-12-09T22:49:00.814410: step 947, loss 0.484026, acc 0.804688, prec 0.0461698, recall 0.820467
2017-12-09T22:49:01.110909: step 948, loss 0.621727, acc 0.804688, prec 0.046261, recall 0.820869
2017-12-09T22:49:01.413037: step 949, loss 0.419334, acc 0.851562, prec 0.0462629, recall 0.820949
2017-12-09T22:49:01.713447: step 950, loss 0.475688, acc 0.859375, prec 0.0462659, recall 0.821029
2017-12-09T22:49:02.016858: step 951, loss 0.660217, acc 0.765625, prec 0.046255, recall 0.821109
2017-12-09T22:49:02.317379: step 952, loss 0.985922, acc 0.851562, prec 0.0463049, recall 0.821349
2017-12-09T22:49:02.617229: step 953, loss 0.360966, acc 0.851562, prec 0.0462828, recall 0.821349
2017-12-09T22:49:02.922497: step 954, loss 0.559388, acc 0.835938, prec 0.0463063, recall 0.821508
2017-12-09T22:49:03.223973: step 955, loss 2.0039, acc 0.84375, prec 0.0463561, recall 0.821381
2017-12-09T22:49:03.529344: step 956, loss 0.343257, acc 0.890625, prec 0.0463638, recall 0.82146
2017-12-09T22:49:03.828967: step 957, loss 2.35102, acc 0.851562, prec 0.0463907, recall 0.821254
2017-12-09T22:49:04.132314: step 958, loss 0.467468, acc 0.828125, prec 0.046389, recall 0.821333
2017-12-09T22:49:04.430070: step 959, loss 0.35326, acc 0.796875, prec 0.0463588, recall 0.821333
2017-12-09T22:49:04.730016: step 960, loss 0.477112, acc 0.828125, prec 0.0464766, recall 0.821809
2017-12-09T22:49:05.027239: step 961, loss 0.454115, acc 0.835938, prec 0.0465478, recall 0.822124
2017-12-09T22:49:05.346930: step 962, loss 0.427356, acc 0.796875, prec 0.0465413, recall 0.822203
2017-12-09T22:49:05.645274: step 963, loss 0.421311, acc 0.859375, prec 0.0466396, recall 0.822595
2017-12-09T22:49:05.942856: step 964, loss 0.393764, acc 0.859375, prec 0.0466425, recall 0.822673
2017-12-09T22:49:06.241383: step 965, loss 0.618517, acc 0.851562, prec 0.0467395, recall 0.823063
2017-12-09T22:49:06.539078: step 966, loss 0.398523, acc 0.867188, prec 0.0467434, recall 0.823141
2017-12-09T22:49:06.837592: step 967, loss 0.367861, acc 0.898438, prec 0.0467521, recall 0.823219
2017-12-09T22:49:07.136124: step 968, loss 0.393194, acc 0.859375, prec 0.0467787, recall 0.823374
2017-12-09T22:49:07.434786: step 969, loss 0.528981, acc 0.859375, prec 0.0467814, recall 0.823452
2017-12-09T22:49:07.732753: step 970, loss 0.495237, acc 0.859375, prec 0.0468555, recall 0.823762
2017-12-09T22:49:08.031248: step 971, loss 0.612964, acc 0.867188, prec 0.0468832, recall 0.823916
2017-12-09T22:49:08.325409: step 972, loss 1.41044, acc 0.914062, prec 0.0469665, recall 0.823864
2017-12-09T22:49:08.627892: step 973, loss 1.91063, acc 0.882812, prec 0.0469976, recall 0.823658
2017-12-09T22:49:08.924959: step 974, loss 0.402902, acc 0.851562, prec 0.0469991, recall 0.823735
2017-12-09T22:49:09.226431: step 975, loss 0.465482, acc 0.898438, prec 0.047055, recall 0.823965
2017-12-09T22:49:09.523145: step 976, loss 1.32849, acc 0.804688, prec 0.0470269, recall 0.823606
2017-12-09T22:49:09.824870: step 977, loss 0.641322, acc 0.8125, prec 0.0470463, recall 0.82376
2017-12-09T22:49:10.121614: step 978, loss 0.555777, acc 0.84375, prec 0.0470702, recall 0.823913
2017-12-09T22:49:10.418374: step 979, loss 0.487963, acc 0.828125, prec 0.0470682, recall 0.82399
2017-12-09T22:49:10.718365: step 980, loss 0.678926, acc 0.796875, prec 0.0471324, recall 0.824295
2017-12-09T22:49:11.013595: step 981, loss 0.64425, acc 0.796875, prec 0.0471492, recall 0.824447
2017-12-09T22:49:11.309845: step 982, loss 0.435282, acc 0.84375, prec 0.0471495, recall 0.824523
2017-12-09T22:49:11.617470: step 983, loss 0.673291, acc 0.734375, prec 0.047157, recall 0.824675
2017-12-09T22:49:11.920274: step 984, loss 0.510709, acc 0.820312, prec 0.0471301, recall 0.824675
2017-12-09T22:49:12.224548: step 985, loss 0.61192, acc 0.804688, prec 0.047101, recall 0.824675
2017-12-09T22:49:12.522628: step 986, loss 0.452658, acc 0.851562, prec 0.0471024, recall 0.824751
2017-12-09T22:49:12.818712: step 987, loss 1.08839, acc 0.875, prec 0.047085, recall 0.824394
2017-12-09T22:49:13.122362: step 988, loss 0.328277, acc 0.921875, prec 0.047144, recall 0.824622
2017-12-09T22:49:13.421734: step 989, loss 0.772602, acc 0.882812, prec 0.0471735, recall 0.824773
2017-12-09T22:49:13.718939: step 990, loss 0.282124, acc 0.867188, prec 0.0471537, recall 0.824773
2017-12-09T22:49:14.018809: step 991, loss 0.337452, acc 0.859375, prec 0.0471563, recall 0.824849
2017-12-09T22:49:14.319203: step 992, loss 0.219652, acc 0.929688, prec 0.0471693, recall 0.824925
2017-12-09T22:49:14.619361: step 993, loss 0.521016, acc 0.914062, prec 0.0472035, recall 0.825075
2017-12-09T22:49:14.916554: step 994, loss 0.149262, acc 0.945312, prec 0.0472423, recall 0.825226
2017-12-09T22:49:15.213166: step 995, loss 0.485205, acc 0.890625, prec 0.0472495, recall 0.825301
2017-12-09T22:49:15.394025: step 996, loss 0.117656, acc 0.961538, prec 0.0472472, recall 0.825301
2017-12-09T22:49:15.702849: step 997, loss 0.179287, acc 0.953125, prec 0.0472637, recall 0.825376
2017-12-09T22:49:15.998815: step 998, loss 0.287066, acc 0.898438, prec 0.0472955, recall 0.825526
2017-12-09T22:49:16.296165: step 999, loss 0.187436, acc 0.945312, prec 0.0473342, recall 0.825676
2017-12-09T22:49:16.595792: step 1000, loss 0.0875112, acc 0.96875, prec 0.047353, recall 0.825751
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1000

2017-12-09T22:49:17.883088: step 1001, loss 0.39157, acc 0.953125, prec 0.0473695, recall 0.825826
2017-12-09T22:49:18.183686: step 1002, loss 0.233593, acc 0.945312, prec 0.0473847, recall 0.8259
2017-12-09T22:49:18.478662: step 1003, loss 0.492005, acc 0.960938, prec 0.0474023, recall 0.825975
2017-12-09T22:49:18.784117: step 1004, loss 0.27775, acc 0.960938, prec 0.0474434, recall 0.826124
2017-12-09T22:49:19.086376: step 1005, loss 0.267407, acc 0.960938, prec 0.047461, recall 0.826199
2017-12-09T22:49:19.386901: step 1006, loss 0.252884, acc 0.953125, prec 0.0475242, recall 0.826422
2017-12-09T22:49:19.685307: step 1007, loss 0.220854, acc 0.921875, prec 0.0475125, recall 0.826422
2017-12-09T22:49:19.983664: step 1008, loss 1.42583, acc 0.9375, prec 0.0475278, recall 0.826143
2017-12-09T22:49:20.288778: step 1009, loss 0.483, acc 0.96875, prec 0.0476167, recall 0.826439
2017-12-09T22:49:20.600496: step 1010, loss 0.238983, acc 0.929688, prec 0.0476062, recall 0.826439
2017-12-09T22:49:20.905841: step 1011, loss 1.89422, acc 0.9375, prec 0.0476916, recall 0.826383
2017-12-09T22:49:21.209595: step 1012, loss 0.16914, acc 0.929688, prec 0.0477044, recall 0.826457
2017-12-09T22:49:21.508763: step 1013, loss 0.355277, acc 0.867188, prec 0.0476845, recall 0.826457
2017-12-09T22:49:21.810684: step 1014, loss 0.423455, acc 0.851562, prec 0.0476623, recall 0.826457
2017-12-09T22:49:22.106903: step 1015, loss 0.556685, acc 0.828125, prec 0.0476833, recall 0.826604
2017-12-09T22:49:22.405226: step 1016, loss 0.644002, acc 0.828125, prec 0.0478209, recall 0.827119
2017-12-09T22:49:22.713021: step 1017, loss 0.41266, acc 0.851562, prec 0.0478219, recall 0.827192
2017-12-09T22:49:23.011671: step 1018, loss 0.48767, acc 0.789062, prec 0.0478835, recall 0.827484
2017-12-09T22:49:23.310521: step 1019, loss 0.742756, acc 0.804688, prec 0.0479241, recall 0.827703
2017-12-09T22:49:23.610240: step 1020, loss 0.593046, acc 0.78125, prec 0.0478913, recall 0.827703
2017-12-09T22:49:23.912166: step 1021, loss 0.648127, acc 0.804688, prec 0.0479086, recall 0.827848
2017-12-09T22:49:24.216850: step 1022, loss 0.404, acc 0.867188, prec 0.0479584, recall 0.828066
2017-12-09T22:49:24.513090: step 1023, loss 0.401874, acc 0.890625, prec 0.0479653, recall 0.828138
2017-12-09T22:49:24.813474: step 1024, loss 0.501198, acc 0.875, prec 0.047993, recall 0.828283
2017-12-09T22:49:25.117064: step 1025, loss 0.330356, acc 0.90625, prec 0.0480254, recall 0.828427
2017-12-09T22:49:25.420490: step 1026, loss 0.39257, acc 0.882812, prec 0.0480542, recall 0.828571
2017-12-09T22:49:25.721478: step 1027, loss 0.269488, acc 0.890625, prec 0.0480378, recall 0.828571
2017-12-09T22:49:26.025380: step 1028, loss 0.373233, acc 0.867188, prec 0.0480875, recall 0.828787
2017-12-09T22:49:26.321869: step 1029, loss 0.292063, acc 0.898438, prec 0.0480722, recall 0.828787
2017-12-09T22:49:26.621413: step 1030, loss 0.194174, acc 0.9375, prec 0.0480629, recall 0.828787
2017-12-09T22:49:26.919817: step 1031, loss 0.391955, acc 0.914062, prec 0.0481195, recall 0.829002
2017-12-09T22:49:27.225356: step 1032, loss 0.221036, acc 0.929688, prec 0.0481553, recall 0.829146
2017-12-09T22:49:27.525667: step 1033, loss 0.130851, acc 0.960938, prec 0.0481494, recall 0.829146
2017-12-09T22:49:27.828273: step 1034, loss 0.304337, acc 0.96875, prec 0.0482142, recall 0.82936
2017-12-09T22:49:28.127424: step 1035, loss 2.42653, acc 0.945312, prec 0.0483471, recall 0.829095
2017-12-09T22:49:28.425908: step 1036, loss 0.504712, acc 0.976562, prec 0.0483898, recall 0.829238
2017-12-09T22:49:28.727836: step 1037, loss 0.251938, acc 0.9375, prec 0.0484498, recall 0.829451
2017-12-09T22:49:29.031738: step 1038, loss 0.202082, acc 0.9375, prec 0.0484404, recall 0.829451
2017-12-09T22:49:29.331081: step 1039, loss 0.297084, acc 0.9375, prec 0.0484772, recall 0.829593
2017-12-09T22:49:29.626582: step 1040, loss 0.225372, acc 0.914062, prec 0.0485567, recall 0.829876
2017-12-09T22:49:29.923764: step 1041, loss 0.207595, acc 0.945312, prec 0.0485715, recall 0.829946
2017-12-09T22:49:30.230503: step 1042, loss 0.227117, acc 0.921875, prec 0.0486059, recall 0.830087
2017-12-09T22:49:30.537137: step 1043, loss 0.201033, acc 0.929688, prec 0.0486414, recall 0.830228
2017-12-09T22:49:30.836448: step 1044, loss 0.315625, acc 0.898438, prec 0.0486492, recall 0.830298
2017-12-09T22:49:31.133102: step 1045, loss 3.08178, acc 0.875, prec 0.0487018, recall 0.829822
2017-12-09T22:49:31.430471: step 1046, loss 0.411754, acc 0.851562, prec 0.0487486, recall 0.830033
2017-12-09T22:49:31.729351: step 1047, loss 0.34232, acc 0.90625, prec 0.0487344, recall 0.830033
2017-12-09T22:49:32.025646: step 1048, loss 0.590203, acc 0.835938, prec 0.0487327, recall 0.830103
2017-12-09T22:49:32.324005: step 1049, loss 0.384888, acc 0.867188, prec 0.0488277, recall 0.830453
2017-12-09T22:49:32.621677: step 1050, loss 0.656381, acc 0.75, prec 0.0489049, recall 0.830801
2017-12-09T22:49:32.921745: step 1051, loss 0.534559, acc 0.8125, prec 0.0488995, recall 0.83087
2017-12-09T22:49:33.221398: step 1052, loss 0.572431, acc 0.835938, prec 0.0488977, recall 0.83094
2017-12-09T22:49:33.523024: step 1053, loss 0.469335, acc 0.875, prec 0.0489247, recall 0.831078
2017-12-09T22:49:33.830315: step 1054, loss 0.339655, acc 0.859375, prec 0.0489264, recall 0.831148
2017-12-09T22:49:34.133016: step 1055, loss 0.676272, acc 0.84375, prec 0.0489487, recall 0.831286
2017-12-09T22:49:34.437611: step 1056, loss 0.504673, acc 0.875, prec 0.0489986, recall 0.831493
2017-12-09T22:49:34.731325: step 1057, loss 0.371468, acc 0.875, prec 0.0490026, recall 0.831562
2017-12-09T22:49:35.028866: step 1058, loss 0.511406, acc 0.84375, prec 0.0490477, recall 0.831768
2017-12-09T22:49:35.336675: step 1059, loss 0.298228, acc 0.898438, prec 0.0490552, recall 0.831837
2017-12-09T22:49:35.636926: step 1060, loss 0.568755, acc 0.859375, prec 0.0490798, recall 0.831974
2017-12-09T22:49:35.936298: step 1061, loss 0.424842, acc 0.9375, prec 0.0490932, recall 0.832042
2017-12-09T22:49:36.240214: step 1062, loss 3.13654, acc 0.875, prec 0.0491681, recall 0.831639
2017-12-09T22:49:36.538573: step 1063, loss 0.31793, acc 0.898438, prec 0.0491527, recall 0.831639
2017-12-09T22:49:36.840784: step 1064, loss 0.406308, acc 0.890625, prec 0.0492276, recall 0.831912
2017-12-09T22:49:37.140905: step 1065, loss 0.781789, acc 0.851562, prec 0.0492736, recall 0.832117
2017-12-09T22:49:37.436032: step 1066, loss 0.386525, acc 0.851562, prec 0.0493424, recall 0.832389
2017-12-09T22:49:37.737614: step 1067, loss 0.50346, acc 0.835938, prec 0.0493632, recall 0.832524
2017-12-09T22:49:38.032099: step 1068, loss 0.506155, acc 0.851562, prec 0.0493635, recall 0.832592
2017-12-09T22:49:38.327966: step 1069, loss 0.47553, acc 0.835938, prec 0.0493842, recall 0.832727
2017-12-09T22:49:38.633318: step 1070, loss 0.800877, acc 0.78125, prec 0.0493966, recall 0.832862
2017-12-09T22:49:38.934198: step 1071, loss 0.445265, acc 0.835938, prec 0.0493718, recall 0.832862
2017-12-09T22:49:39.231284: step 1072, loss 0.462495, acc 0.796875, prec 0.0493411, recall 0.832862
2017-12-09T22:49:39.529900: step 1073, loss 1.05014, acc 0.859375, prec 0.0493653, recall 0.832997
2017-12-09T22:49:39.832444: step 1074, loss 0.647665, acc 0.804688, prec 0.0493812, recall 0.833132
2017-12-09T22:49:40.128796: step 1075, loss 0.756044, acc 0.898438, prec 0.049434, recall 0.833333
2017-12-09T22:49:40.430249: step 1076, loss 0.510966, acc 0.851562, prec 0.0494797, recall 0.833534
2017-12-09T22:49:40.729030: step 1077, loss 0.646371, acc 0.84375, prec 0.0495014, recall 0.833668
2017-12-09T22:49:41.021392: step 1078, loss 0.452621, acc 0.828125, prec 0.0495887, recall 0.834002
2017-12-09T22:49:41.317783: step 1079, loss 0.525769, acc 0.796875, prec 0.0496033, recall 0.834135
2017-12-09T22:49:41.612899: step 1080, loss 0.675855, acc 0.84375, prec 0.0496476, recall 0.834334
2017-12-09T22:49:41.910613: step 1081, loss 0.541036, acc 0.8125, prec 0.0496418, recall 0.8344
2017-12-09T22:49:42.206951: step 1082, loss 0.372369, acc 0.851562, prec 0.049642, recall 0.834466
2017-12-09T22:49:42.501589: step 1083, loss 0.229121, acc 0.882812, prec 0.0496243, recall 0.834466
2017-12-09T22:49:42.799316: step 1084, loss 0.396381, acc 0.867188, prec 0.0496043, recall 0.834466
2017-12-09T22:49:43.100347: step 1085, loss 0.379136, acc 0.90625, prec 0.0496353, recall 0.834598
2017-12-09T22:49:43.398191: step 1086, loss 0.556844, acc 0.898438, prec 0.0497328, recall 0.834928
2017-12-09T22:49:43.696776: step 1087, loss 0.195536, acc 0.945312, prec 0.0497471, recall 0.834994
2017-12-09T22:49:43.995567: step 1088, loss 0.260633, acc 0.90625, prec 0.0497555, recall 0.83506
2017-12-09T22:49:44.293366: step 1089, loss 0.127327, acc 0.945312, prec 0.0497472, recall 0.83506
2017-12-09T22:49:44.593519: step 1090, loss 0.406256, acc 0.929688, prec 0.0498042, recall 0.835257
2017-12-09T22:49:44.895582: step 1091, loss 1.27478, acc 0.96875, prec 0.0498458, recall 0.835056
2017-12-09T22:49:45.204247: step 1092, loss 0.159721, acc 0.953125, prec 0.0498838, recall 0.835187
2017-12-09T22:49:45.513005: step 1093, loss 0.45921, acc 0.898438, prec 0.0499585, recall 0.835448
2017-12-09T22:49:45.819254: step 1094, loss 0.25144, acc 0.929688, prec 0.0499929, recall 0.835578
2017-12-09T22:49:46.118757: step 1095, loss 1.68091, acc 0.929688, prec 0.0499846, recall 0.834917
2017-12-09T22:49:46.420300: step 1096, loss 0.281971, acc 0.90625, prec 0.0499929, recall 0.834982
2017-12-09T22:49:46.715385: step 1097, loss 0.249749, acc 0.921875, prec 0.0500261, recall 0.835113
2017-12-09T22:49:47.012556: step 1098, loss 0.354414, acc 0.882812, prec 0.0500083, recall 0.835113
2017-12-09T22:49:47.312006: step 1099, loss 0.325339, acc 0.914062, prec 0.0500178, recall 0.835178
2017-12-09T22:49:47.610080: step 1100, loss 0.503899, acc 0.921875, prec 0.0500284, recall 0.835243
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1100

2017-12-09T22:49:49.153966: step 1101, loss 0.392858, acc 0.867188, prec 0.0500532, recall 0.835373
2017-12-09T22:49:49.455791: step 1102, loss 0.381751, acc 0.867188, prec 0.0500331, recall 0.835373
2017-12-09T22:49:49.758957: step 1103, loss 0.470034, acc 0.890625, prec 0.0500615, recall 0.835503
2017-12-09T22:49:50.056819: step 1104, loss 0.486939, acc 0.882812, prec 0.0500886, recall 0.835633
2017-12-09T22:49:50.357639: step 1105, loss 0.455639, acc 0.882812, prec 0.0500709, recall 0.835633
2017-12-09T22:49:50.655938: step 1106, loss 0.411726, acc 0.84375, prec 0.0500472, recall 0.835633
2017-12-09T22:49:50.952748: step 1107, loss 0.500721, acc 0.84375, prec 0.0500236, recall 0.835633
2017-12-09T22:49:51.252497: step 1108, loss 0.306783, acc 0.875, prec 0.0500719, recall 0.835827
2017-12-09T22:49:51.553924: step 1109, loss 0.366211, acc 0.882812, prec 0.0500542, recall 0.835827
2017-12-09T22:49:51.853615: step 1110, loss 1.1161, acc 0.898438, prec 0.0501061, recall 0.83602
2017-12-09T22:49:52.157165: step 1111, loss 0.774957, acc 0.859375, prec 0.0501296, recall 0.836149
2017-12-09T22:49:52.462065: step 1112, loss 0.45143, acc 0.828125, prec 0.0501483, recall 0.836278
2017-12-09T22:49:52.767050: step 1113, loss 1.24235, acc 0.890625, prec 0.050133, recall 0.83595
2017-12-09T22:49:53.073908: step 1114, loss 0.315738, acc 0.898438, prec 0.0501176, recall 0.83595
2017-12-09T22:49:53.379805: step 1115, loss 2.36412, acc 0.867188, prec 0.0500988, recall 0.835622
2017-12-09T22:49:53.683520: step 1116, loss 0.335368, acc 0.859375, prec 0.0501222, recall 0.835751
2017-12-09T22:49:53.984140: step 1117, loss 0.298974, acc 0.867188, prec 0.0501245, recall 0.835815
2017-12-09T22:49:54.286496: step 1118, loss 0.380315, acc 0.898438, prec 0.0501316, recall 0.835879
2017-12-09T22:49:54.581940: step 1119, loss 0.46295, acc 0.859375, prec 0.0501104, recall 0.835879
2017-12-09T22:49:54.881563: step 1120, loss 0.463449, acc 0.835938, prec 0.050108, recall 0.835944
2017-12-09T22:49:55.180101: step 1121, loss 0.611516, acc 0.867188, prec 0.0501548, recall 0.836136
2017-12-09T22:49:55.475761: step 1122, loss 0.220581, acc 0.914062, prec 0.0501642, recall 0.8362
2017-12-09T22:49:55.778303: step 1123, loss 0.494468, acc 0.867188, prec 0.0502555, recall 0.83652
2017-12-09T22:49:56.075708: step 1124, loss 0.421402, acc 0.875, prec 0.0502367, recall 0.83652
2017-12-09T22:49:56.368665: step 1125, loss 0.307907, acc 0.882812, prec 0.050219, recall 0.83652
2017-12-09T22:49:56.671749: step 1126, loss 0.318438, acc 0.898438, prec 0.0502704, recall 0.836711
2017-12-09T22:49:56.971578: step 1127, loss 0.361332, acc 0.921875, prec 0.0503254, recall 0.836902
2017-12-09T22:49:57.267195: step 1128, loss 0.852154, acc 0.914062, prec 0.0504013, recall 0.837155
2017-12-09T22:49:57.567349: step 1129, loss 0.330891, acc 0.929688, prec 0.0503907, recall 0.837155
2017-12-09T22:49:57.867761: step 1130, loss 0.278359, acc 0.921875, prec 0.0503789, recall 0.837155
2017-12-09T22:49:58.162385: step 1131, loss 0.345397, acc 0.84375, prec 0.0503553, recall 0.837155
2017-12-09T22:49:58.463830: step 1132, loss 1.09765, acc 0.90625, prec 0.05043, recall 0.837408
2017-12-09T22:49:58.761597: step 1133, loss 0.32078, acc 0.890625, prec 0.0504357, recall 0.837471
2017-12-09T22:49:59.060541: step 1134, loss 0.345486, acc 0.875, prec 0.050439, recall 0.837534
2017-12-09T22:49:59.360948: step 1135, loss 0.584613, acc 0.945312, prec 0.0504751, recall 0.83766
2017-12-09T22:49:59.657465: step 1136, loss 0.300638, acc 0.867188, prec 0.0504994, recall 0.837786
2017-12-09T22:49:59.962424: step 1137, loss 0.755905, acc 0.921875, prec 0.0505098, recall 0.837848
2017-12-09T22:50:00.272170: step 1138, loss 0.60783, acc 0.898438, prec 0.0505609, recall 0.838036
2017-12-09T22:50:00.575631: step 1139, loss 0.400126, acc 0.851562, prec 0.0505606, recall 0.838099
2017-12-09T22:50:00.874695: step 1140, loss 0.816909, acc 0.867188, prec 0.0506512, recall 0.838411
2017-12-09T22:50:01.171619: step 1141, loss 0.397275, acc 0.898438, prec 0.050658, recall 0.838473
2017-12-09T22:50:01.470936: step 1142, loss 0.354716, acc 0.890625, prec 0.0507078, recall 0.83866
2017-12-09T22:50:01.769799: step 1143, loss 0.710834, acc 0.828125, prec 0.050726, recall 0.838784
2017-12-09T22:50:02.070772: step 1144, loss 0.56795, acc 0.820312, prec 0.0507651, recall 0.83897
2017-12-09T22:50:02.370610: step 1145, loss 0.580244, acc 0.828125, prec 0.0507612, recall 0.839032
2017-12-09T22:50:02.668819: step 1146, loss 0.434193, acc 0.875, prec 0.0507864, recall 0.839155
2017-12-09T22:50:02.968807: step 1147, loss 0.810554, acc 0.84375, prec 0.0508069, recall 0.839279
2017-12-09T22:50:03.268562: step 1148, loss 0.372673, acc 0.882812, prec 0.0508553, recall 0.839464
2017-12-09T22:50:03.566782: step 1149, loss 0.218625, acc 0.890625, prec 0.0508608, recall 0.839525
2017-12-09T22:50:03.868840: step 1150, loss 0.508856, acc 0.851562, prec 0.0508824, recall 0.839648
2017-12-09T22:50:04.168091: step 1151, loss 0.291382, acc 0.882812, prec 0.0509088, recall 0.839771
2017-12-09T22:50:04.467278: step 1152, loss 0.332583, acc 0.890625, prec 0.0509142, recall 0.839832
2017-12-09T22:50:04.764983: step 1153, loss 0.382136, acc 0.945312, prec 0.0509939, recall 0.840076
2017-12-09T22:50:05.061232: step 1154, loss 2.08913, acc 0.882812, prec 0.0510214, recall 0.839878
2017-12-09T22:50:05.373483: step 1155, loss 0.207659, acc 0.953125, prec 0.0510802, recall 0.840061
2017-12-09T22:50:05.673351: step 1156, loss 0.136252, acc 0.945312, prec 0.0510719, recall 0.840061
2017-12-09T22:50:05.974632: step 1157, loss 0.516889, acc 0.867188, prec 0.0510957, recall 0.840183
2017-12-09T22:50:06.274788: step 1158, loss 0.185606, acc 0.945312, prec 0.0511094, recall 0.840243
2017-12-09T22:50:06.575281: step 1159, loss 0.460904, acc 0.882812, prec 0.0511575, recall 0.840426
2017-12-09T22:50:06.871343: step 1160, loss 0.574775, acc 0.929688, prec 0.0511908, recall 0.840547
2017-12-09T22:50:07.175929: step 1161, loss 0.369909, acc 0.835938, prec 0.0511878, recall 0.840607
2017-12-09T22:50:07.472702: step 1162, loss 0.457224, acc 0.929688, prec 0.051221, recall 0.840728
2017-12-09T22:50:07.775345: step 1163, loss 0.278313, acc 0.914062, prec 0.0512299, recall 0.840788
2017-12-09T22:50:08.076592: step 1164, loss 0.388322, acc 0.882812, prec 0.0512998, recall 0.84103
2017-12-09T22:50:08.374117: step 1165, loss 0.297952, acc 0.898438, prec 0.0513501, recall 0.84121
2017-12-09T22:50:08.680078: step 1166, loss 0.612977, acc 0.882812, prec 0.051398, recall 0.84139
2017-12-09T22:50:08.976111: step 1167, loss 0.410614, acc 0.96875, prec 0.051437, recall 0.841509
2017-12-09T22:50:09.275986: step 1168, loss 2.1105, acc 0.90625, prec 0.0514677, recall 0.841312
2017-12-09T22:50:09.580338: step 1169, loss 0.292065, acc 0.929688, prec 0.0515008, recall 0.841431
2017-12-09T22:50:09.875484: step 1170, loss 0.4002, acc 0.875, prec 0.0515473, recall 0.84161
2017-12-09T22:50:10.173741: step 1171, loss 0.257263, acc 0.898438, prec 0.0515319, recall 0.84161
2017-12-09T22:50:10.475785: step 1172, loss 0.456794, acc 0.890625, prec 0.0515808, recall 0.841789
2017-12-09T22:50:10.775485: step 1173, loss 0.280346, acc 0.898438, prec 0.0515872, recall 0.841848
2017-12-09T22:50:11.074143: step 1174, loss 1.18623, acc 0.796875, prec 0.0515575, recall 0.841532
2017-12-09T22:50:11.374486: step 1175, loss 0.45786, acc 0.851562, prec 0.0515786, recall 0.841651
2017-12-09T22:50:11.675034: step 1176, loss 0.834152, acc 0.882812, prec 0.051562, recall 0.841335
2017-12-09T22:50:11.983216: step 1177, loss 0.371051, acc 0.859375, prec 0.0515407, recall 0.841335
2017-12-09T22:50:12.284126: step 1178, loss 0.469588, acc 0.867188, prec 0.0515424, recall 0.841395
2017-12-09T22:50:12.587196: step 1179, loss 0.529399, acc 0.921875, prec 0.0515523, recall 0.841454
2017-12-09T22:50:12.887861: step 1180, loss 0.631505, acc 0.804688, prec 0.051588, recall 0.841632
2017-12-09T22:50:13.186979: step 1181, loss 0.374973, acc 0.867188, prec 0.0516114, recall 0.841751
2017-12-09T22:50:13.486171: step 1182, loss 1.17436, acc 0.8125, prec 0.0516277, recall 0.841555
2017-12-09T22:50:13.791187: step 1183, loss 0.396562, acc 0.875, prec 0.0516305, recall 0.841614
2017-12-09T22:50:14.091662: step 1184, loss 0.233576, acc 0.90625, prec 0.0516815, recall 0.841791
2017-12-09T22:50:14.392135: step 1185, loss 0.323436, acc 0.890625, prec 0.0517301, recall 0.841968
2017-12-09T22:50:14.692892: step 1186, loss 0.61475, acc 0.851562, prec 0.0517293, recall 0.842027
2017-12-09T22:50:14.990742: step 1187, loss 0.311019, acc 0.890625, prec 0.0517561, recall 0.842144
2017-12-09T22:50:15.288700: step 1188, loss 0.394527, acc 0.867188, prec 0.051801, recall 0.842321
2017-12-09T22:50:15.589118: step 1189, loss 1.12248, acc 0.921875, prec 0.0519204, recall 0.842359
2017-12-09T22:50:15.896038: step 1190, loss 0.315047, acc 0.90625, prec 0.0519279, recall 0.842417
2017-12-09T22:50:16.202253: step 1191, loss 0.252135, acc 0.898438, prec 0.0519558, recall 0.842534
2017-12-09T22:50:16.501892: step 1192, loss 0.293709, acc 0.90625, prec 0.0519415, recall 0.842534
2017-12-09T22:50:16.799332: step 1193, loss 0.773085, acc 0.921875, prec 0.0520163, recall 0.842767
2017-12-09T22:50:17.096031: step 1194, loss 0.364247, acc 0.921875, prec 0.0520477, recall 0.842884
2017-12-09T22:50:17.394583: step 1195, loss 0.18413, acc 0.914062, prec 0.0520562, recall 0.842942
2017-12-09T22:50:17.692905: step 1196, loss 0.232505, acc 0.90625, prec 0.0520636, recall 0.843
2017-12-09T22:50:17.995793: step 1197, loss 0.898238, acc 0.859375, prec 0.0521287, recall 0.843231
2017-12-09T22:50:18.299672: step 1198, loss 0.262275, acc 0.929688, prec 0.0521612, recall 0.843347
2017-12-09T22:50:18.598618: step 1199, loss 0.229016, acc 0.921875, prec 0.0521709, recall 0.843405
2017-12-09T22:50:18.894221: step 1200, loss 0.365696, acc 0.867188, prec 0.0521723, recall 0.843462

Evaluation:
2017-12-09T22:50:23.598254: step 1200, loss 1.76358, acc 0.915078, prec 0.0529757, recall 0.8292

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1200

2017-12-09T22:50:24.890681: step 1201, loss 0.323463, acc 0.90625, prec 0.0529826, recall 0.82926
2017-12-09T22:50:25.191549: step 1202, loss 0.269261, acc 0.929688, prec 0.0530354, recall 0.829438
2017-12-09T22:50:25.488803: step 1203, loss 0.282041, acc 0.914062, prec 0.0530435, recall 0.829498
2017-12-09T22:50:25.785517: step 1204, loss 0.311038, acc 0.914062, prec 0.0530727, recall 0.829617
2017-12-09T22:50:26.093513: step 1205, loss 1.11175, acc 0.9375, prec 0.0530855, recall 0.829387
2017-12-09T22:50:26.391383: step 1206, loss 0.226422, acc 0.945312, prec 0.0531194, recall 0.829506
2017-12-09T22:50:26.691819: step 1207, loss 0.287427, acc 0.90625, prec 0.0531052, recall 0.829506
2017-12-09T22:50:26.988138: step 1208, loss 0.171328, acc 0.9375, prec 0.0531379, recall 0.829624
2017-12-09T22:50:27.286601: step 1209, loss 0.882785, acc 0.96875, prec 0.0531965, recall 0.829802
2017-12-09T22:50:27.582941: step 1210, loss 0.419071, acc 0.867188, prec 0.0532396, recall 0.829979
2017-12-09T22:50:27.876430: step 1211, loss 0.273404, acc 0.890625, prec 0.053244, recall 0.830038
2017-12-09T22:50:28.177897: step 1212, loss 0.191183, acc 0.921875, prec 0.0532322, recall 0.830038
2017-12-09T22:50:28.476690: step 1213, loss 0.213025, acc 0.921875, prec 0.0532204, recall 0.830038
2017-12-09T22:50:28.774319: step 1214, loss 0.710108, acc 0.898438, prec 0.0532681, recall 0.830215
2017-12-09T22:50:29.073702: step 1215, loss 0.329328, acc 0.890625, prec 0.0533147, recall 0.830391
2017-12-09T22:50:29.375309: step 1216, loss 0.289679, acc 0.898438, prec 0.0533203, recall 0.83045
2017-12-09T22:50:29.669209: step 1217, loss 0.254655, acc 0.929688, prec 0.0533307, recall 0.830508
2017-12-09T22:50:29.977436: step 1218, loss 0.32588, acc 0.875, prec 0.0533117, recall 0.830508
2017-12-09T22:50:30.289197: step 1219, loss 1.15583, acc 0.882812, prec 0.0533582, recall 0.830397
2017-12-09T22:50:30.583952: step 1220, loss 0.269056, acc 0.898438, prec 0.0533848, recall 0.830514
2017-12-09T22:50:30.880911: step 1221, loss 0.313617, acc 0.914062, prec 0.0534138, recall 0.830631
2017-12-09T22:50:31.176616: step 1222, loss 0.382047, acc 0.921875, prec 0.0534649, recall 0.830806
2017-12-09T22:50:31.480302: step 1223, loss 0.23647, acc 0.9375, prec 0.0534974, recall 0.830923
2017-12-09T22:50:31.779602: step 1224, loss 0.363918, acc 0.898438, prec 0.0535449, recall 0.831097
2017-12-09T22:50:32.081117: step 1225, loss 0.859749, acc 0.882812, prec 0.05359, recall 0.831271
2017-12-09T22:50:32.378970: step 1226, loss 0.61776, acc 0.929688, prec 0.0536632, recall 0.831503
2017-12-09T22:50:32.683498: step 1227, loss 0.45477, acc 0.90625, prec 0.0537118, recall 0.831676
2017-12-09T22:50:32.979399: step 1228, loss 0.3409, acc 0.882812, prec 0.053694, recall 0.831676
2017-12-09T22:50:33.272124: step 1229, loss 0.316044, acc 0.875, prec 0.0537168, recall 0.831792
2017-12-09T22:50:33.571024: step 1230, loss 1.5974, acc 0.859375, prec 0.0537594, recall 0.83168
2017-12-09T22:50:33.871119: step 1231, loss 0.423018, acc 0.859375, prec 0.053738, recall 0.83168
2017-12-09T22:50:34.169206: step 1232, loss 0.410477, acc 0.851562, prec 0.0537154, recall 0.83168
2017-12-09T22:50:34.466553: step 1233, loss 0.424154, acc 0.835938, prec 0.0536905, recall 0.83168
2017-12-09T22:50:34.759457: step 1234, loss 2.29684, acc 0.820312, prec 0.0536645, recall 0.831395
2017-12-09T22:50:35.057698: step 1235, loss 0.520957, acc 0.835938, prec 0.0537022, recall 0.831568
2017-12-09T22:50:35.370101: step 1236, loss 0.546481, acc 0.84375, prec 0.0537203, recall 0.831683
2017-12-09T22:50:35.670671: step 1237, loss 0.398434, acc 0.882812, prec 0.053786, recall 0.831913
2017-12-09T22:50:35.974635: step 1238, loss 0.469994, acc 0.851562, prec 0.0537634, recall 0.831913
2017-12-09T22:50:36.283881: step 1239, loss 0.320496, acc 0.90625, prec 0.0538118, recall 0.832084
2017-12-09T22:50:36.577873: step 1240, loss 0.550867, acc 0.804688, prec 0.053803, recall 0.832142
2017-12-09T22:50:36.883995: step 1241, loss 0.486878, acc 0.84375, prec 0.0538209, recall 0.832256
2017-12-09T22:50:37.187122: step 1242, loss 0.253312, acc 0.929688, prec 0.0538311, recall 0.832313
2017-12-09T22:50:37.480967: step 1243, loss 0.189562, acc 0.914062, prec 0.0538181, recall 0.832313
2017-12-09T22:50:37.781029: step 1244, loss 0.383007, acc 0.859375, prec 0.0538176, recall 0.83237
2017-12-09T22:50:37.958561: step 1245, loss 0.367456, acc 0.884615, prec 0.0538313, recall 0.832427
2017-12-09T22:50:38.264370: step 1246, loss 0.155256, acc 0.945312, prec 0.0538854, recall 0.832598
2017-12-09T22:50:38.566813: step 1247, loss 0.291023, acc 0.945312, prec 0.0539602, recall 0.832825
2017-12-09T22:50:38.867055: step 1248, loss 0.159655, acc 0.953125, prec 0.0539739, recall 0.832881
2017-12-09T22:50:39.165583: step 1249, loss 0.76355, acc 0.9375, prec 0.0540683, recall 0.833164
2017-12-09T22:50:39.471752: step 1250, loss 0.0842009, acc 0.960938, prec 0.0540624, recall 0.833164
2017-12-09T22:50:39.773661: step 1251, loss 0.205065, acc 0.960938, prec 0.054098, recall 0.833277
2017-12-09T22:50:40.077733: step 1252, loss 0.12979, acc 0.953125, prec 0.0540908, recall 0.833277
2017-12-09T22:50:40.378805: step 1253, loss 0.144069, acc 0.945312, prec 0.0541033, recall 0.833333
2017-12-09T22:50:40.679823: step 1254, loss 0.191162, acc 0.960938, prec 0.0541181, recall 0.83339
2017-12-09T22:50:40.978915: step 1255, loss 0.071273, acc 0.976562, prec 0.0541146, recall 0.83339
2017-12-09T22:50:41.278844: step 1256, loss 0.153297, acc 0.945312, prec 0.0541062, recall 0.83339
2017-12-09T22:50:41.583717: step 1257, loss 0.43473, acc 0.9375, prec 0.0541175, recall 0.833446
2017-12-09T22:50:41.889406: step 1258, loss 0.11462, acc 0.960938, prec 0.0541531, recall 0.833558
2017-12-09T22:50:42.190732: step 1259, loss 1.10971, acc 0.945312, prec 0.0541459, recall 0.833277
2017-12-09T22:50:42.491464: step 1260, loss 0.862335, acc 0.96875, prec 0.0542241, recall 0.833502
2017-12-09T22:50:42.794488: step 1261, loss 0.391832, acc 0.953125, prec 0.0542792, recall 0.83367
2017-12-09T22:50:43.093956: step 1262, loss 0.381457, acc 0.914062, prec 0.0543076, recall 0.833782
2017-12-09T22:50:43.392461: step 1263, loss 0.181804, acc 0.921875, prec 0.0542957, recall 0.833782
2017-12-09T22:50:43.689485: step 1264, loss 0.197806, acc 0.9375, prec 0.054369, recall 0.834005
2017-12-09T22:50:43.984758: step 1265, loss 0.947113, acc 0.945312, prec 0.0544033, recall 0.833837
2017-12-09T22:50:44.282251: step 1266, loss 0.144853, acc 0.9375, prec 0.0543938, recall 0.833837
2017-12-09T22:50:44.582909: step 1267, loss 0.346299, acc 0.90625, prec 0.0544209, recall 0.833948
2017-12-09T22:50:44.882086: step 1268, loss 0.229736, acc 0.890625, prec 0.0544249, recall 0.834004
2017-12-09T22:50:45.178274: step 1269, loss 1.01983, acc 0.929688, prec 0.0544567, recall 0.833836
2017-12-09T22:50:45.480623: step 1270, loss 0.354097, acc 0.921875, prec 0.0545276, recall 0.834058
2017-12-09T22:50:45.779025: step 1271, loss 0.453873, acc 0.859375, prec 0.0545268, recall 0.834114
2017-12-09T22:50:46.077578: step 1272, loss 0.293523, acc 0.898438, prec 0.0545526, recall 0.834225
2017-12-09T22:50:46.382681: step 1273, loss 0.298896, acc 0.898438, prec 0.0545784, recall 0.834335
2017-12-09T22:50:46.676598: step 1274, loss 0.569579, acc 0.820312, prec 0.0545923, recall 0.834446
2017-12-09T22:50:46.977875: step 1275, loss 0.388616, acc 0.851562, prec 0.0546109, recall 0.834556
2017-12-09T22:50:47.280280: step 1276, loss 0.445728, acc 0.84375, prec 0.0546284, recall 0.834667
2017-12-09T22:50:47.578624: step 1277, loss 0.634158, acc 0.820312, prec 0.0546422, recall 0.834777
2017-12-09T22:50:47.878036: step 1278, loss 0.347474, acc 0.898438, prec 0.0546679, recall 0.834887
2017-12-09T22:50:48.173762: step 1279, loss 0.517483, acc 0.84375, prec 0.0547059, recall 0.835052
2017-12-09T22:50:48.473480: step 1280, loss 0.379702, acc 0.859375, prec 0.0547462, recall 0.835216
2017-12-09T22:50:48.773932: step 1281, loss 0.553983, acc 0.929688, prec 0.0547766, recall 0.835325
2017-12-09T22:50:49.071799: step 1282, loss 0.249823, acc 0.898438, prec 0.0547817, recall 0.83538
2017-12-09T22:50:49.367178: step 1283, loss 0.344105, acc 0.882812, prec 0.0548461, recall 0.835598
2017-12-09T22:50:49.668876: step 1284, loss 0.413086, acc 0.882812, prec 0.0548282, recall 0.835598
2017-12-09T22:50:49.967741: step 1285, loss 0.454122, acc 0.898438, prec 0.0548332, recall 0.835653
2017-12-09T22:50:50.275789: step 1286, loss 0.774358, acc 0.921875, prec 0.0549035, recall 0.83587
2017-12-09T22:50:50.585659: step 1287, loss 0.145338, acc 0.960938, prec 0.0549181, recall 0.835925
2017-12-09T22:50:50.888276: step 1288, loss 0.213207, acc 0.890625, prec 0.0549014, recall 0.835925
2017-12-09T22:50:51.182030: step 1289, loss 0.340484, acc 0.859375, prec 0.0549004, recall 0.835979
2017-12-09T22:50:51.487323: step 1290, loss 2.29021, acc 0.9375, prec 0.0549331, recall 0.835811
2017-12-09T22:50:51.789214: step 1291, loss 0.301994, acc 0.960938, prec 0.0549887, recall 0.835974
2017-12-09T22:50:52.086538: step 1292, loss 0.223591, acc 0.9375, prec 0.0550202, recall 0.836082
2017-12-09T22:50:52.383114: step 1293, loss 0.347652, acc 0.914062, prec 0.0550276, recall 0.836136
2017-12-09T22:50:52.685058: step 1294, loss 0.165233, acc 0.953125, prec 0.0551229, recall 0.836406
2017-12-09T22:50:52.981739: step 1295, loss 0.649788, acc 0.953125, prec 0.0551567, recall 0.836513
2017-12-09T22:50:53.282728: step 1296, loss 0.211522, acc 0.882812, prec 0.0551388, recall 0.836513
2017-12-09T22:50:53.586376: step 1297, loss 0.210772, acc 0.914062, prec 0.0551461, recall 0.836567
2017-12-09T22:50:53.889030: step 1298, loss 0.168276, acc 0.929688, prec 0.0551763, recall 0.836674
2017-12-09T22:50:54.188855: step 1299, loss 0.263247, acc 0.921875, prec 0.0551848, recall 0.836728
2017-12-09T22:50:54.488666: step 1300, loss 0.227508, acc 0.890625, prec 0.055209, recall 0.836835
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1300

2017-12-09T22:50:55.895927: step 1301, loss 0.279951, acc 0.914062, prec 0.0552777, recall 0.837049
2017-12-09T22:50:56.196858: step 1302, loss 0.190108, acc 0.945312, prec 0.0553102, recall 0.837156
2017-12-09T22:50:56.500780: step 1303, loss 0.167221, acc 0.9375, prec 0.0553006, recall 0.837156
2017-12-09T22:50:56.801806: step 1304, loss 0.758851, acc 0.9375, prec 0.055374, recall 0.837095
2017-12-09T22:50:57.105748: step 1305, loss 0.311823, acc 0.890625, prec 0.0553981, recall 0.837202
2017-12-09T22:50:57.406553: step 1306, loss 0.178006, acc 0.929688, prec 0.0554691, recall 0.837414
2017-12-09T22:50:57.704040: step 1307, loss 0.427858, acc 0.921875, prec 0.0555387, recall 0.837626
2017-12-09T22:50:58.012068: step 1308, loss 0.211297, acc 0.945312, prec 0.0555508, recall 0.837679
2017-12-09T22:50:58.312350: step 1309, loss 0.205744, acc 0.914062, prec 0.0555375, recall 0.837679
2017-12-09T22:50:58.609805: step 1310, loss 0.251467, acc 0.882812, prec 0.0556012, recall 0.837891
2017-12-09T22:50:58.908255: step 1311, loss 1.4001, acc 0.9375, prec 0.0556131, recall 0.837671
2017-12-09T22:50:59.208544: step 1312, loss 0.230133, acc 0.929688, prec 0.0556635, recall 0.837829
2017-12-09T22:50:59.511549: step 1313, loss 0.387342, acc 0.898438, prec 0.055709, recall 0.837987
2017-12-09T22:50:59.810982: step 1314, loss 0.191082, acc 0.929688, prec 0.0556982, recall 0.837987
2017-12-09T22:51:00.120653: step 1315, loss 0.435906, acc 0.90625, prec 0.0557246, recall 0.838092
2017-12-09T22:51:00.428530: step 1316, loss 0.261315, acc 0.890625, prec 0.0557077, recall 0.838092
2017-12-09T22:51:00.729031: step 1317, loss 0.273934, acc 0.929688, prec 0.0557376, recall 0.838197
2017-12-09T22:51:01.035013: step 1318, loss 0.178324, acc 0.914062, prec 0.0557448, recall 0.83825
2017-12-09T22:51:01.329763: step 1319, loss 0.441668, acc 0.90625, prec 0.0558117, recall 0.838459
2017-12-09T22:51:01.633356: step 1320, loss 1.05548, acc 0.90625, prec 0.055899, recall 0.83872
2017-12-09T22:51:01.937819: step 1321, loss 0.416749, acc 0.867188, prec 0.0559192, recall 0.838824
2017-12-09T22:51:02.239873: step 1322, loss 0.452404, acc 0.890625, prec 0.055943, recall 0.838928
2017-12-09T22:51:02.536618: step 1323, loss 0.43533, acc 0.867188, prec 0.0559429, recall 0.83898
2017-12-09T22:51:02.835454: step 1324, loss 0.422004, acc 0.867188, prec 0.055963, recall 0.839084
2017-12-09T22:51:03.136905: step 1325, loss 0.314823, acc 0.875, prec 0.0559641, recall 0.839136
2017-12-09T22:51:03.432801: step 1326, loss 0.352071, acc 0.890625, prec 0.0559878, recall 0.83924
2017-12-09T22:51:03.733303: step 1327, loss 0.383126, acc 0.875, prec 0.0559888, recall 0.839291
2017-12-09T22:51:04.033445: step 1328, loss 0.17724, acc 0.945312, prec 0.0559804, recall 0.839291
2017-12-09T22:51:04.332641: step 1329, loss 0.188084, acc 0.914062, prec 0.0559875, recall 0.839343
2017-12-09T22:51:04.632279: step 1330, loss 0.181411, acc 0.921875, prec 0.0559957, recall 0.839395
2017-12-09T22:51:04.927848: step 1331, loss 0.223333, acc 0.960938, prec 0.0560505, recall 0.83955
2017-12-09T22:51:05.229609: step 1332, loss 0.281212, acc 0.898438, prec 0.0560754, recall 0.839653
2017-12-09T22:51:05.539348: step 1333, loss 1.24491, acc 0.890625, prec 0.0561002, recall 0.839486
2017-12-09T22:51:05.839354: step 1334, loss 0.214744, acc 0.921875, prec 0.0560882, recall 0.839486
2017-12-09T22:51:06.136927: step 1335, loss 0.124174, acc 0.96875, prec 0.0561239, recall 0.839589
2017-12-09T22:51:06.437074: step 1336, loss 0.225605, acc 0.9375, prec 0.0561547, recall 0.839692
2017-12-09T22:51:06.732230: step 1337, loss 0.256108, acc 0.929688, prec 0.0562046, recall 0.839846
2017-12-09T22:51:07.028780: step 1338, loss 0.19211, acc 0.953125, prec 0.0562176, recall 0.839898
2017-12-09T22:51:07.324767: step 1339, loss 0.155195, acc 0.945312, prec 0.0562092, recall 0.839898
2017-12-09T22:51:07.627236: step 1340, loss 0.335646, acc 0.9375, prec 0.0562602, recall 0.840051
2017-12-09T22:51:07.935958: step 1341, loss 1.07023, acc 0.953125, prec 0.0562744, recall 0.839834
2017-12-09T22:51:08.240021: step 1342, loss 0.235665, acc 0.90625, prec 0.0563003, recall 0.839936
2017-12-09T22:51:08.536236: step 1343, loss 0.2664, acc 0.921875, prec 0.0563287, recall 0.840038
2017-12-09T22:51:08.837748: step 1344, loss 0.266566, acc 0.914062, prec 0.0563558, recall 0.84014
2017-12-09T22:51:09.136170: step 1345, loss 0.159247, acc 0.953125, prec 0.0563486, recall 0.84014
2017-12-09T22:51:09.438581: step 1346, loss 0.327028, acc 0.890625, prec 0.0563519, recall 0.840191
2017-12-09T22:51:09.735812: step 1347, loss 0.292692, acc 0.929688, prec 0.0563612, recall 0.840242
2017-12-09T22:51:10.032652: step 1348, loss 0.146966, acc 0.9375, prec 0.0563516, recall 0.840242
2017-12-09T22:51:10.336150: step 1349, loss 0.197112, acc 0.9375, prec 0.0563823, recall 0.840344
2017-12-09T22:51:10.634965: step 1350, loss 0.180052, acc 0.929688, prec 0.0563715, recall 0.840344
2017-12-09T22:51:10.932786: step 1351, loss 0.515798, acc 0.882812, prec 0.0563735, recall 0.840395
2017-12-09T22:51:11.231495: step 1352, loss 0.237839, acc 0.945312, prec 0.0564256, recall 0.840547
2017-12-09T22:51:11.528065: step 1353, loss 0.687077, acc 0.929688, prec 0.0564551, recall 0.840649
2017-12-09T22:51:11.835213: step 1354, loss 0.158663, acc 0.960938, prec 0.0564893, recall 0.84075
2017-12-09T22:51:12.129787: step 1355, loss 0.266694, acc 0.898438, prec 0.0564737, recall 0.84075
2017-12-09T22:51:12.432802: step 1356, loss 0.178242, acc 0.929688, prec 0.0564829, recall 0.840801
2017-12-09T22:51:12.729430: step 1357, loss 0.151947, acc 0.945312, prec 0.0564745, recall 0.840801
2017-12-09T22:51:13.032948: step 1358, loss 0.253174, acc 0.953125, prec 0.0565277, recall 0.840952
2017-12-09T22:51:13.340455: step 1359, loss 0.129047, acc 0.96875, prec 0.0565631, recall 0.841053
2017-12-09T22:51:13.639487: step 1360, loss 0.154272, acc 0.929688, prec 0.0565523, recall 0.841053
2017-12-09T22:51:13.940654: step 1361, loss 0.200107, acc 0.9375, prec 0.0565828, recall 0.841154
2017-12-09T22:51:14.242714: step 1362, loss 0.245075, acc 0.953125, prec 0.0565957, recall 0.841204
2017-12-09T22:51:14.548545: step 1363, loss 0.295351, acc 0.9375, prec 0.0566464, recall 0.841355
2017-12-09T22:51:14.849194: step 1364, loss 0.200183, acc 0.9375, prec 0.0566569, recall 0.841406
2017-12-09T22:51:15.145397: step 1365, loss 0.228302, acc 0.953125, prec 0.0567099, recall 0.841556
2017-12-09T22:51:15.443212: step 1366, loss 0.396566, acc 0.921875, prec 0.056738, recall 0.841656
2017-12-09T22:51:15.743450: step 1367, loss 0.0679068, acc 0.96875, prec 0.0567332, recall 0.841656
2017-12-09T22:51:16.047918: step 1368, loss 0.383639, acc 0.96875, prec 0.0567485, recall 0.841706
2017-12-09T22:51:16.349813: step 1369, loss 1.35141, acc 0.9375, prec 0.0567601, recall 0.84149
2017-12-09T22:51:16.658553: step 1370, loss 1.09146, acc 0.945312, prec 0.0568332, recall 0.841425
2017-12-09T22:51:16.965613: step 1371, loss 0.362245, acc 0.929688, prec 0.0568625, recall 0.841525
2017-12-09T22:51:17.266121: step 1372, loss 0.265065, acc 0.960938, prec 0.0568966, recall 0.841625
2017-12-09T22:51:17.564144: step 1373, loss 0.317365, acc 0.921875, prec 0.0569045, recall 0.841675
2017-12-09T22:51:17.863017: step 1374, loss 0.406511, acc 0.890625, prec 0.0569076, recall 0.841724
2017-12-09T22:51:18.159536: step 1375, loss 0.617509, acc 0.875, prec 0.0569685, recall 0.841923
2017-12-09T22:51:18.457707: step 1376, loss 0.445859, acc 0.851562, prec 0.0570257, recall 0.842122
2017-12-09T22:51:18.758365: step 1377, loss 0.472471, acc 0.84375, prec 0.0570816, recall 0.84232
2017-12-09T22:51:19.058170: step 1378, loss 0.384958, acc 0.867188, prec 0.057141, recall 0.842517
2017-12-09T22:51:19.357851: step 1379, loss 0.436856, acc 0.835938, prec 0.0571556, recall 0.842616
2017-12-09T22:51:19.655971: step 1380, loss 0.432884, acc 0.859375, prec 0.0571938, recall 0.842763
2017-12-09T22:51:19.958680: step 1381, loss 0.400178, acc 0.835938, prec 0.0571883, recall 0.842812
2017-12-09T22:51:20.263182: step 1382, loss 0.659454, acc 0.8125, prec 0.0571792, recall 0.842862
2017-12-09T22:51:20.572771: step 1383, loss 0.499352, acc 0.835938, prec 0.0571538, recall 0.842862
2017-12-09T22:51:20.870567: step 1384, loss 0.624953, acc 0.898438, prec 0.0571779, recall 0.84296
2017-12-09T22:51:21.165232: step 1385, loss 0.37979, acc 0.867188, prec 0.0571773, recall 0.843009
2017-12-09T22:51:21.470568: step 1386, loss 0.50428, acc 0.851562, prec 0.0571942, recall 0.843107
2017-12-09T22:51:21.772402: step 1387, loss 0.340269, acc 0.867188, prec 0.0572136, recall 0.843204
2017-12-09T22:51:22.071851: step 1388, loss 0.182644, acc 0.953125, prec 0.0572462, recall 0.843302
2017-12-09T22:51:22.373771: step 1389, loss 0.782919, acc 0.859375, prec 0.0572643, recall 0.8434
2017-12-09T22:51:22.669824: step 1390, loss 1.62294, acc 0.929688, prec 0.0573741, recall 0.84343
2017-12-09T22:51:22.968997: step 1391, loss 0.458965, acc 0.875, prec 0.0574144, recall 0.843575
2017-12-09T22:51:23.272810: step 1392, loss 0.214898, acc 0.921875, prec 0.0574222, recall 0.843624
2017-12-09T22:51:23.573522: step 1393, loss 0.196973, acc 0.929688, prec 0.0574312, recall 0.843672
2017-12-09T22:51:23.870794: step 1394, loss 0.246619, acc 0.914062, prec 0.0574776, recall 0.843818
2017-12-09T22:51:24.166116: step 1395, loss 0.305319, acc 0.90625, prec 0.0575028, recall 0.843915
2017-12-09T22:51:24.461210: step 1396, loss 0.302649, acc 0.921875, prec 0.0575304, recall 0.844011
2017-12-09T22:51:24.759536: step 1397, loss 0.16523, acc 0.945312, prec 0.0575816, recall 0.844156
2017-12-09T22:51:25.062489: step 1398, loss 0.241539, acc 0.882812, prec 0.0575634, recall 0.844156
2017-12-09T22:51:25.360521: step 1399, loss 0.269626, acc 0.898438, prec 0.0575873, recall 0.844252
2017-12-09T22:51:25.663145: step 1400, loss 0.289612, acc 0.921875, prec 0.057595, recall 0.8443
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1400

2017-12-09T22:51:26.990080: step 1401, loss 1.04597, acc 0.9375, prec 0.0576648, recall 0.844492
2017-12-09T22:51:27.291958: step 1402, loss 0.220032, acc 0.945312, prec 0.057696, recall 0.844588
2017-12-09T22:51:27.593400: step 1403, loss 0.540508, acc 0.921875, prec 0.0577036, recall 0.844636
2017-12-09T22:51:27.896227: step 1404, loss 0.277707, acc 0.929688, prec 0.0577324, recall 0.844732
2017-12-09T22:51:28.194497: step 1405, loss 0.184773, acc 0.9375, prec 0.0577227, recall 0.844732
2017-12-09T22:51:28.490580: step 1406, loss 0.797584, acc 0.945312, prec 0.0577538, recall 0.844828
2017-12-09T22:51:28.788894: step 1407, loss 0.149634, acc 0.9375, prec 0.0577838, recall 0.844923
2017-12-09T22:51:29.092550: step 1408, loss 0.310977, acc 0.921875, prec 0.0578509, recall 0.845114
2017-12-09T22:51:29.388926: step 1409, loss 0.802804, acc 0.953125, prec 0.0578646, recall 0.844902
2017-12-09T22:51:29.686335: step 1410, loss 0.142354, acc 0.953125, prec 0.0578573, recall 0.844902
2017-12-09T22:51:29.990899: step 1411, loss 0.239748, acc 0.921875, prec 0.0579046, recall 0.845044
2017-12-09T22:51:30.301615: step 1412, loss 0.376763, acc 0.867188, prec 0.0579037, recall 0.845092
2017-12-09T22:51:30.604974: step 1413, loss 0.224028, acc 0.914062, prec 0.0578903, recall 0.845092
2017-12-09T22:51:30.901694: step 1414, loss 1.17564, acc 0.914062, prec 0.0579177, recall 0.844928
2017-12-09T22:51:31.205569: step 1415, loss 0.289993, acc 0.882812, prec 0.0579391, recall 0.845023
2017-12-09T22:51:31.509406: step 1416, loss 0.439108, acc 0.882812, prec 0.0579801, recall 0.845165
2017-12-09T22:51:31.810865: step 1417, loss 0.421899, acc 0.867188, prec 0.057999, recall 0.84526
2017-12-09T22:51:32.113139: step 1418, loss 0.377596, acc 0.898438, prec 0.0580029, recall 0.845307
2017-12-09T22:51:32.411627: step 1419, loss 0.331617, acc 0.898438, prec 0.0580266, recall 0.845402
2017-12-09T22:51:32.713902: step 1420, loss 0.335079, acc 0.859375, prec 0.0580047, recall 0.845402
2017-12-09T22:51:33.012129: step 1421, loss 0.258183, acc 0.898438, prec 0.0579889, recall 0.845402
2017-12-09T22:51:33.308290: step 1422, loss 0.296092, acc 0.898438, prec 0.0579929, recall 0.845449
2017-12-09T22:51:33.605154: step 1423, loss 0.219273, acc 0.921875, prec 0.0580202, recall 0.845543
2017-12-09T22:51:33.904971: step 1424, loss 0.229288, acc 0.914062, prec 0.058066, recall 0.845685
2017-12-09T22:51:34.201134: step 1425, loss 0.225867, acc 0.921875, prec 0.0580933, recall 0.845779
2017-12-09T22:51:34.499767: step 1426, loss 0.162995, acc 0.945312, prec 0.0580848, recall 0.845779
2017-12-09T22:51:34.801479: step 1427, loss 0.260555, acc 0.929688, prec 0.0581133, recall 0.845873
2017-12-09T22:51:35.100493: step 1428, loss 0.445576, acc 0.96875, prec 0.0581478, recall 0.845967
2017-12-09T22:51:35.409930: step 1429, loss 0.184378, acc 0.9375, prec 0.0581775, recall 0.84606
2017-12-09T22:51:35.717798: step 1430, loss 0.70764, acc 0.914062, prec 0.0582035, recall 0.846154
2017-12-09T22:51:36.021089: step 1431, loss 0.174018, acc 0.929688, prec 0.0581925, recall 0.846154
2017-12-09T22:51:36.320260: step 1432, loss 0.190259, acc 0.945312, prec 0.0582037, recall 0.846201
2017-12-09T22:51:36.624403: step 1433, loss 0.186985, acc 0.976562, prec 0.0582394, recall 0.846294
2017-12-09T22:51:36.936145: step 1434, loss 0.157281, acc 0.960938, prec 0.058253, recall 0.846341
2017-12-09T22:51:37.239439: step 1435, loss 0.464628, acc 0.96875, prec 0.0583072, recall 0.846481
2017-12-09T22:51:37.539489: step 1436, loss 0.680927, acc 0.9375, prec 0.0582987, recall 0.846224
2017-12-09T22:51:37.848541: step 1437, loss 0.614265, acc 0.945312, prec 0.0583688, recall 0.84641
2017-12-09T22:51:38.151861: step 1438, loss 0.209922, acc 0.953125, prec 0.0584009, recall 0.846503
2017-12-09T22:51:38.450361: step 1439, loss 0.532521, acc 0.921875, prec 0.058428, recall 0.846596
2017-12-09T22:51:38.752902: step 1440, loss 0.266306, acc 0.9375, prec 0.0584772, recall 0.846735
2017-12-09T22:51:39.055193: step 1441, loss 0.869656, acc 0.921875, prec 0.0585436, recall 0.84692
2017-12-09T22:51:39.359912: step 1442, loss 0.187292, acc 0.914062, prec 0.0585302, recall 0.84692
2017-12-09T22:51:39.663806: step 1443, loss 0.152701, acc 0.9375, prec 0.0585204, recall 0.84692
2017-12-09T22:51:39.965379: step 1444, loss 0.354005, acc 0.890625, prec 0.0585426, recall 0.847013
2017-12-09T22:51:40.267834: step 1445, loss 0.202912, acc 0.9375, prec 0.0585525, recall 0.847059
2017-12-09T22:51:40.570668: step 1446, loss 0.39807, acc 0.898438, prec 0.0586347, recall 0.847289
2017-12-09T22:51:40.871187: step 1447, loss 0.386639, acc 0.882812, prec 0.058636, recall 0.847335
2017-12-09T22:51:41.174659: step 1448, loss 0.630557, acc 0.789062, prec 0.0586618, recall 0.847473
2017-12-09T22:51:41.478338: step 1449, loss 0.259627, acc 0.90625, prec 0.0586472, recall 0.847473
2017-12-09T22:51:41.776694: step 1450, loss 0.460482, acc 0.851562, prec 0.0586436, recall 0.847519
2017-12-09T22:51:42.072352: step 1451, loss 0.781653, acc 0.859375, prec 0.0586608, recall 0.84761
2017-12-09T22:51:42.376761: step 1452, loss 0.337466, acc 0.859375, prec 0.0587171, recall 0.847793
2017-12-09T22:51:42.678412: step 1453, loss 0.280351, acc 0.890625, prec 0.0587196, recall 0.847839
2017-12-09T22:51:42.978248: step 1454, loss 0.363129, acc 0.875, prec 0.0587588, recall 0.847976
2017-12-09T22:51:43.277980: step 1455, loss 0.325147, acc 0.914062, prec 0.058804, recall 0.848113
2017-12-09T22:51:43.579627: step 1456, loss 0.374622, acc 0.90625, prec 0.0588284, recall 0.848204
2017-12-09T22:51:43.879478: step 1457, loss 0.369164, acc 0.90625, prec 0.0588724, recall 0.84834
2017-12-09T22:51:44.177801: step 1458, loss 0.461644, acc 0.921875, prec 0.0589383, recall 0.848521
2017-12-09T22:51:44.481357: step 1459, loss 0.172965, acc 0.96875, prec 0.058992, recall 0.848657
2017-12-09T22:51:44.782611: step 1460, loss 0.315429, acc 0.90625, prec 0.0589968, recall 0.848702
2017-12-09T22:51:45.086918: step 1461, loss 0.789632, acc 0.914062, prec 0.0590809, recall 0.848927
2017-12-09T22:51:45.392031: step 1462, loss 0.165933, acc 0.929688, prec 0.0590894, recall 0.848972
2017-12-09T22:51:45.693708: step 1463, loss 0.267123, acc 0.945312, prec 0.0591393, recall 0.849107
2017-12-09T22:51:45.994664: step 1464, loss 0.296157, acc 0.984375, prec 0.0591954, recall 0.849242
2017-12-09T22:51:46.292871: step 1465, loss 0.329061, acc 0.960938, prec 0.0592672, recall 0.849421
2017-12-09T22:51:46.595697: step 1466, loss 0.212196, acc 0.898438, prec 0.0592903, recall 0.84951
2017-12-09T22:51:46.898876: step 1467, loss 0.469778, acc 0.921875, prec 0.0592975, recall 0.849555
2017-12-09T22:51:47.201897: step 1468, loss 0.103009, acc 0.953125, prec 0.0592901, recall 0.849555
2017-12-09T22:51:47.505733: step 1469, loss 1.03726, acc 0.929688, prec 0.0592803, recall 0.849303
2017-12-09T22:51:47.807448: step 1470, loss 0.248749, acc 0.945312, prec 0.0593106, recall 0.849392
2017-12-09T22:51:48.110249: step 1471, loss 0.451862, acc 0.851562, prec 0.0593262, recall 0.849481
2017-12-09T22:51:48.413128: step 1472, loss 0.283565, acc 0.882812, prec 0.0593078, recall 0.849481
2017-12-09T22:51:48.713267: step 1473, loss 0.220006, acc 0.890625, prec 0.059349, recall 0.849615
2017-12-09T22:51:49.018563: step 1474, loss 0.210118, acc 0.9375, prec 0.0593975, recall 0.849749
2017-12-09T22:51:49.324450: step 1475, loss 0.226016, acc 0.921875, prec 0.0594047, recall 0.849793
2017-12-09T22:51:49.621734: step 1476, loss 0.253108, acc 0.875, prec 0.0593851, recall 0.849793
2017-12-09T22:51:49.921754: step 1477, loss 0.297359, acc 0.90625, prec 0.0594092, recall 0.849882
2017-12-09T22:51:50.220827: step 1478, loss 0.254677, acc 0.898438, prec 0.0593933, recall 0.849882
2017-12-09T22:51:50.540565: step 1479, loss 0.233306, acc 0.914062, prec 0.0594186, recall 0.84997
2017-12-09T22:51:50.840786: step 1480, loss 0.277949, acc 0.898438, prec 0.0594609, recall 0.850103
2017-12-09T22:51:51.140021: step 1481, loss 0.345261, acc 0.890625, prec 0.0594631, recall 0.850147
2017-12-09T22:51:51.436682: step 1482, loss 0.248803, acc 0.9375, prec 0.0594533, recall 0.850147
2017-12-09T22:51:51.741222: step 1483, loss 0.214633, acc 0.9375, prec 0.0594629, recall 0.850192
2017-12-09T22:51:52.043551: step 1484, loss 0.72858, acc 0.945312, prec 0.0594931, recall 0.85028
2017-12-09T22:51:52.347801: step 1485, loss 0.505609, acc 0.921875, prec 0.059539, recall 0.850412
2017-12-09T22:51:52.651113: step 1486, loss 0.146662, acc 0.953125, prec 0.0595704, recall 0.8505
2017-12-09T22:51:52.957221: step 1487, loss 0.184316, acc 0.945312, prec 0.05962, recall 0.850632
2017-12-09T22:51:53.253943: step 1488, loss 0.320364, acc 0.914062, prec 0.0596646, recall 0.850764
2017-12-09T22:51:53.556156: step 1489, loss 0.210115, acc 0.945312, prec 0.0596754, recall 0.850808
2017-12-09T22:51:53.856655: step 1490, loss 0.15739, acc 0.945312, prec 0.0596668, recall 0.850808
2017-12-09T22:51:54.162640: step 1491, loss 0.249904, acc 0.914062, prec 0.059692, recall 0.850895
2017-12-09T22:51:54.466495: step 1492, loss 0.264518, acc 0.992188, prec 0.0597488, recall 0.851026
2017-12-09T22:51:54.767473: step 1493, loss 0.207218, acc 0.945312, prec 0.0597596, recall 0.85107
2017-12-09T22:51:54.944795: step 1494, loss 1.10174, acc 0.923077, prec 0.059774, recall 0.851114
2017-12-09T22:51:55.254800: step 1495, loss 0.286165, acc 0.914062, prec 0.0597605, recall 0.851114
2017-12-09T22:51:55.554558: step 1496, loss 0.264048, acc 0.90625, prec 0.0597651, recall 0.851157
2017-12-09T22:51:55.853477: step 1497, loss 0.245468, acc 0.90625, prec 0.0597696, recall 0.851201
2017-12-09T22:51:56.153281: step 1498, loss 0.274305, acc 0.929688, prec 0.0597972, recall 0.851288
2017-12-09T22:51:56.448072: step 1499, loss 0.320361, acc 0.914062, prec 0.0598031, recall 0.851332
2017-12-09T22:51:56.745636: step 1500, loss 0.296136, acc 0.882812, prec 0.0598039, recall 0.851375

Evaluation:
2017-12-09T22:52:01.460917: step 1500, loss 1.97356, acc 0.931213, prec 0.0604907, recall 0.837914

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1500

2017-12-09T22:52:02.825176: step 1501, loss 0.224544, acc 0.945312, prec 0.0605202, recall 0.838004
2017-12-09T22:52:03.125978: step 1502, loss 0.202047, acc 0.9375, prec 0.0605294, recall 0.83805
2017-12-09T22:52:03.423446: step 1503, loss 0.156969, acc 0.945312, prec 0.0605398, recall 0.838095
2017-12-09T22:52:03.720323: step 1504, loss 0.198014, acc 0.9375, prec 0.060549, recall 0.838141
2017-12-09T22:52:04.023678: step 1505, loss 0.420861, acc 0.898438, prec 0.0606091, recall 0.838322
2017-12-09T22:52:04.323865: step 1506, loss 0.13937, acc 0.945312, prec 0.0606195, recall 0.838367
2017-12-09T22:52:04.619590: step 1507, loss 0.136595, acc 0.953125, prec 0.0606502, recall 0.838457
2017-12-09T22:52:04.916923: step 1508, loss 0.224298, acc 0.976562, prec 0.0606845, recall 0.838547
2017-12-09T22:52:05.217006: step 1509, loss 0.117092, acc 0.960938, prec 0.0606973, recall 0.838593
2017-12-09T22:52:05.534086: step 1510, loss 0.184083, acc 0.945312, prec 0.0607267, recall 0.838683
2017-12-09T22:52:05.833642: step 1511, loss 0.0793519, acc 0.96875, prec 0.0607218, recall 0.838683
2017-12-09T22:52:06.132173: step 1512, loss 0.171462, acc 0.960938, prec 0.0607346, recall 0.838728
2017-12-09T22:52:06.428235: step 1513, loss 1.43961, acc 0.9375, prec 0.060783, recall 0.838629
2017-12-09T22:52:06.731693: step 1514, loss 1.74405, acc 0.960938, prec 0.060797, recall 0.83844
2017-12-09T22:52:07.034958: step 1515, loss 0.244642, acc 0.945312, prec 0.0608074, recall 0.838485
2017-12-09T22:52:07.337788: step 1516, loss 0.425896, acc 0.921875, prec 0.060852, recall 0.83862
2017-12-09T22:52:07.643183: step 1517, loss 0.287159, acc 0.914062, prec 0.0608385, recall 0.83862
2017-12-09T22:52:07.943043: step 1518, loss 0.26292, acc 0.921875, prec 0.0608641, recall 0.83871
2017-12-09T22:52:08.241254: step 1519, loss 0.304286, acc 0.898438, prec 0.0609239, recall 0.838889
2017-12-09T22:52:08.540619: step 1520, loss 0.236895, acc 0.9375, prec 0.0609709, recall 0.839023
2017-12-09T22:52:08.842301: step 1521, loss 0.266737, acc 0.929688, prec 0.0610167, recall 0.839157
2017-12-09T22:52:09.144823: step 1522, loss 0.25249, acc 0.898438, prec 0.0610764, recall 0.839335
2017-12-09T22:52:09.446394: step 1523, loss 0.284287, acc 0.914062, prec 0.0611196, recall 0.839469
2017-12-09T22:52:09.747547: step 1524, loss 0.392813, acc 0.890625, prec 0.0611402, recall 0.839557
2017-12-09T22:52:10.043753: step 1525, loss 0.973217, acc 0.945312, prec 0.0611706, recall 0.839414
2017-12-09T22:52:10.347366: step 1526, loss 0.198532, acc 0.929688, prec 0.0611974, recall 0.839503
2017-12-09T22:52:10.645283: step 1527, loss 0.304435, acc 0.90625, prec 0.0612393, recall 0.839636
2017-12-09T22:52:10.940301: step 1528, loss 0.738103, acc 0.867188, prec 0.061275, recall 0.839768
2017-12-09T22:52:11.243661: step 1529, loss 0.212023, acc 0.898438, prec 0.0613156, recall 0.839901
2017-12-09T22:52:11.546345: step 1530, loss 0.374128, acc 0.875, prec 0.0613337, recall 0.839989
2017-12-09T22:52:11.843886: step 1531, loss 0.381584, acc 0.890625, prec 0.0614107, recall 0.840209
2017-12-09T22:52:12.142354: step 1532, loss 0.433797, acc 0.851562, prec 0.0613873, recall 0.840209
2017-12-09T22:52:12.439470: step 1533, loss 0.410508, acc 0.898438, prec 0.061409, recall 0.840297
2017-12-09T22:52:12.740024: step 1534, loss 0.297442, acc 0.898438, prec 0.0614118, recall 0.840341
2017-12-09T22:52:13.045409: step 1535, loss 0.249104, acc 0.890625, prec 0.0613945, recall 0.840341
2017-12-09T22:52:13.344057: step 1536, loss 0.275493, acc 0.898438, prec 0.0613785, recall 0.840341
2017-12-09T22:52:13.640709: step 1537, loss 0.251071, acc 0.914062, prec 0.0614026, recall 0.840428
2017-12-09T22:52:13.941297: step 1538, loss 0.504338, acc 0.9375, prec 0.0615057, recall 0.840691
2017-12-09T22:52:14.241055: step 1539, loss 0.25447, acc 0.921875, prec 0.0615311, recall 0.840778
2017-12-09T22:52:14.535984: step 1540, loss 0.190485, acc 0.9375, prec 0.06154, recall 0.840822
2017-12-09T22:52:14.834690: step 1541, loss 0.256262, acc 0.960938, prec 0.0615527, recall 0.840865
2017-12-09T22:52:15.136973: step 1542, loss 0.465079, acc 0.96875, prec 0.0616042, recall 0.840996
2017-12-09T22:52:15.436596: step 1543, loss 0.592025, acc 0.945312, prec 0.0616519, recall 0.841127
2017-12-09T22:52:15.738481: step 1544, loss 0.130737, acc 0.953125, prec 0.0617197, recall 0.8413
2017-12-09T22:52:16.038399: step 1545, loss 0.262014, acc 0.882812, prec 0.0617388, recall 0.841387
2017-12-09T22:52:16.339878: step 1546, loss 0.315665, acc 0.9375, prec 0.0617853, recall 0.841517
2017-12-09T22:52:16.642316: step 1547, loss 0.685513, acc 0.890625, prec 0.0618055, recall 0.841603
2017-12-09T22:52:16.940436: step 1548, loss 0.128105, acc 0.96875, prec 0.0618193, recall 0.841646
2017-12-09T22:52:17.240228: step 1549, loss 0.183727, acc 0.945312, prec 0.061867, recall 0.841776
2017-12-09T22:52:17.538376: step 1550, loss 0.191235, acc 0.921875, prec 0.0618922, recall 0.841862
2017-12-09T22:52:17.837824: step 1551, loss 0.278798, acc 0.921875, prec 0.0618798, recall 0.841862
2017-12-09T22:52:18.138263: step 1552, loss 0.246366, acc 0.90625, prec 0.0618837, recall 0.841905
2017-12-09T22:52:18.438863: step 1553, loss 0.13666, acc 0.96875, prec 0.0618788, recall 0.841905
2017-12-09T22:52:18.734951: step 1554, loss 0.321273, acc 0.914062, prec 0.0619027, recall 0.841991
2017-12-09T22:52:19.034856: step 1555, loss 0.238434, acc 0.921875, prec 0.0618903, recall 0.841991
2017-12-09T22:52:19.339617: step 1556, loss 0.0739191, acc 0.976562, prec 0.0619053, recall 0.842034
2017-12-09T22:52:19.639631: step 1557, loss 0.479629, acc 0.953125, prec 0.0619354, recall 0.84212
2017-12-09T22:52:19.942172: step 1558, loss 0.331412, acc 0.9375, prec 0.0619443, recall 0.842162
2017-12-09T22:52:20.248283: step 1559, loss 0.240451, acc 0.921875, prec 0.0619694, recall 0.842248
2017-12-09T22:52:20.564958: step 1560, loss 0.133158, acc 0.96875, prec 0.0619831, recall 0.842291
2017-12-09T22:52:20.863131: step 1561, loss 0.556261, acc 0.96875, prec 0.0620157, recall 0.842377
2017-12-09T22:52:21.168433: step 1562, loss 0.575791, acc 0.960938, prec 0.0621031, recall 0.84259
2017-12-09T22:52:21.469167: step 1563, loss 0.226308, acc 0.9375, prec 0.0621681, recall 0.842761
2017-12-09T22:52:21.770402: step 1564, loss 0.233433, acc 0.984375, prec 0.0622218, recall 0.842888
2017-12-09T22:52:22.066153: step 1565, loss 0.59946, acc 0.9375, prec 0.062268, recall 0.843015
2017-12-09T22:52:22.368071: step 1566, loss 0.669816, acc 0.921875, prec 0.062293, recall 0.8431
2017-12-09T22:52:22.671530: step 1567, loss 0.0937402, acc 0.960938, prec 0.0623242, recall 0.843185
2017-12-09T22:52:22.971882: step 1568, loss 0.315047, acc 0.921875, prec 0.0623679, recall 0.843312
2017-12-09T22:52:23.273294: step 1569, loss 0.26243, acc 0.875, prec 0.0624041, recall 0.843438
2017-12-09T22:52:23.573590: step 1570, loss 0.281859, acc 0.890625, prec 0.0624053, recall 0.843481
2017-12-09T22:52:23.872395: step 1571, loss 0.272913, acc 0.890625, prec 0.0624253, recall 0.843565
2017-12-09T22:52:24.168312: step 1572, loss 0.276852, acc 0.875, prec 0.0624241, recall 0.843607
2017-12-09T22:52:24.476462: step 1573, loss 0.187057, acc 0.9375, prec 0.0624328, recall 0.843649
2017-12-09T22:52:24.780556: step 1574, loss 0.194758, acc 0.929688, prec 0.0624776, recall 0.843775
2017-12-09T22:52:25.077923: step 1575, loss 0.467854, acc 0.882812, prec 0.0625336, recall 0.843943
2017-12-09T22:52:25.376941: step 1576, loss 0.220107, acc 0.921875, prec 0.0625584, recall 0.844027
2017-12-09T22:52:25.679779: step 1577, loss 0.335249, acc 0.898438, prec 0.0625609, recall 0.844069
2017-12-09T22:52:25.980184: step 1578, loss 0.221913, acc 0.921875, prec 0.0625858, recall 0.844152
2017-12-09T22:52:26.282085: step 1579, loss 0.347276, acc 0.890625, prec 0.0626429, recall 0.844319
2017-12-09T22:52:26.585836: step 1580, loss 0.245645, acc 0.898438, prec 0.0626453, recall 0.844361
2017-12-09T22:52:26.891572: step 1581, loss 0.212912, acc 0.914062, prec 0.0626316, recall 0.844361
2017-12-09T22:52:27.193735: step 1582, loss 0.385319, acc 0.890625, prec 0.0626328, recall 0.844403
2017-12-09T22:52:27.494841: step 1583, loss 0.16098, acc 0.945312, prec 0.0626428, recall 0.844444
2017-12-09T22:52:27.790276: step 1584, loss 0.180738, acc 0.9375, prec 0.0626514, recall 0.844486
2017-12-09T22:52:28.088271: step 1585, loss 0.0930143, acc 0.96875, prec 0.0626837, recall 0.844569
2017-12-09T22:52:28.388742: step 1586, loss 0.128175, acc 0.960938, prec 0.0627333, recall 0.844694
2017-12-09T22:52:28.691261: step 1587, loss 0.487847, acc 0.914062, prec 0.0627382, recall 0.844735
2017-12-09T22:52:29.000384: step 1588, loss 0.351697, acc 0.945312, prec 0.0627853, recall 0.84486
2017-12-09T22:52:29.303127: step 1589, loss 0.441453, acc 0.960938, prec 0.0628534, recall 0.845025
2017-12-09T22:52:29.606644: step 1590, loss 0.323456, acc 0.945312, prec 0.062919, recall 0.845191
2017-12-09T22:52:29.912928: step 1591, loss 0.109129, acc 0.953125, prec 0.0629487, recall 0.845273
2017-12-09T22:52:30.221644: step 1592, loss 0.15028, acc 0.945312, prec 0.0629586, recall 0.845314
2017-12-09T22:52:30.530201: step 1593, loss 0.209717, acc 0.976562, prec 0.062992, recall 0.845396
2017-12-09T22:52:30.843002: step 1594, loss 0.254276, acc 0.960938, prec 0.06306, recall 0.845561
2017-12-09T22:52:31.145319: step 1595, loss 0.145984, acc 0.953125, prec 0.0630711, recall 0.845602
2017-12-09T22:52:31.442070: step 1596, loss 0.521946, acc 0.953125, prec 0.0631193, recall 0.845725
2017-12-09T22:52:31.750148: step 1597, loss 0.16524, acc 0.9375, prec 0.0631093, recall 0.845725
2017-12-09T22:52:32.046902: step 1598, loss 0.116172, acc 0.960938, prec 0.0631216, recall 0.845766
2017-12-09T22:52:32.343964: step 1599, loss 0.290657, acc 0.914062, prec 0.0631264, recall 0.845807
2017-12-09T22:52:32.643986: step 1600, loss 0.166947, acc 0.992188, prec 0.0631808, recall 0.845929
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1600

2017-12-09T22:52:33.887289: step 1601, loss 0.140459, acc 0.929688, prec 0.0632067, recall 0.846011
2017-12-09T22:52:34.184995: step 1602, loss 0.172001, acc 0.9375, prec 0.0631967, recall 0.846011
2017-12-09T22:52:34.482036: step 1603, loss 0.240014, acc 0.960938, prec 0.063246, recall 0.846133
2017-12-09T22:52:34.777488: step 1604, loss 0.17183, acc 0.914062, prec 0.0632879, recall 0.846256
2017-12-09T22:52:35.077446: step 1605, loss 0.185965, acc 0.953125, prec 0.0633174, recall 0.846337
2017-12-09T22:52:35.381511: step 1606, loss 0.0862817, acc 0.960938, prec 0.0633297, recall 0.846378
2017-12-09T22:52:35.686495: step 1607, loss 0.279367, acc 0.9375, prec 0.0633567, recall 0.846459
2017-12-09T22:52:35.986825: step 1608, loss 0.632411, acc 0.945312, prec 0.0633665, recall 0.846499
2017-12-09T22:52:36.284992: step 1609, loss 0.118033, acc 0.953125, prec 0.0633775, recall 0.84654
2017-12-09T22:52:36.583764: step 1610, loss 0.289421, acc 0.929688, prec 0.0633847, recall 0.84658
2017-12-09T22:52:36.884402: step 1611, loss 0.180133, acc 0.96875, prec 0.0634353, recall 0.846702
2017-12-09T22:52:37.189824: step 1612, loss 0.889713, acc 0.9375, prec 0.063482, recall 0.8466
2017-12-09T22:52:37.491371: step 1613, loss 0.118582, acc 0.960938, prec 0.0634943, recall 0.84664
2017-12-09T22:52:37.793259: step 1614, loss 0.28909, acc 0.914062, prec 0.063499, recall 0.846681
2017-12-09T22:52:38.089493: step 1615, loss 0.479648, acc 0.9375, prec 0.0635814, recall 0.846882
2017-12-09T22:52:38.394253: step 1616, loss 0.313291, acc 0.929688, prec 0.0635886, recall 0.846923
2017-12-09T22:52:38.698994: step 1617, loss 0.197184, acc 0.9375, prec 0.0635786, recall 0.846923
2017-12-09T22:52:38.995174: step 1618, loss 0.168633, acc 0.953125, prec 0.0635895, recall 0.846963
2017-12-09T22:52:39.296320: step 1619, loss 0.221125, acc 0.9375, prec 0.0636534, recall 0.847124
2017-12-09T22:52:39.603827: step 1620, loss 0.155595, acc 0.9375, prec 0.0636434, recall 0.847124
2017-12-09T22:52:39.901990: step 1621, loss 0.108661, acc 0.953125, prec 0.0636543, recall 0.847164
2017-12-09T22:52:40.201338: step 1622, loss 0.226478, acc 0.921875, prec 0.0636787, recall 0.847244
2017-12-09T22:52:40.503709: step 1623, loss 0.718547, acc 0.921875, prec 0.0636858, recall 0.847062
2017-12-09T22:52:40.805551: step 1624, loss 0.219284, acc 0.9375, prec 0.0636943, recall 0.847102
2017-12-09T22:52:41.108767: step 1625, loss 0.2818, acc 0.9375, prec 0.0637396, recall 0.847222
2017-12-09T22:52:41.408649: step 1626, loss 0.183242, acc 0.953125, prec 0.0637505, recall 0.847262
2017-12-09T22:52:41.704658: step 1627, loss 0.139751, acc 0.945312, prec 0.0637602, recall 0.847302
2017-12-09T22:52:42.006014: step 1628, loss 0.238405, acc 0.96875, prec 0.0638289, recall 0.847462
2017-12-09T22:52:42.307052: step 1629, loss 1.06243, acc 0.890625, prec 0.063831, recall 0.84728
2017-12-09T22:52:42.618463: step 1630, loss 0.12341, acc 0.96875, prec 0.0638629, recall 0.84736
2017-12-09T22:52:42.923833: step 1631, loss 0.196171, acc 0.96875, prec 0.0638947, recall 0.84744
2017-12-09T22:52:43.220041: step 1632, loss 0.145538, acc 0.96875, prec 0.0639635, recall 0.847599
2017-12-09T22:52:43.516710: step 1633, loss 0.238728, acc 0.929688, prec 0.063989, recall 0.847679
2017-12-09T22:52:43.815688: step 1634, loss 0.197204, acc 0.929688, prec 0.0640329, recall 0.847798
2017-12-09T22:52:44.116267: step 1635, loss 0.170754, acc 0.960938, prec 0.0640266, recall 0.847798
2017-12-09T22:52:44.412927: step 1636, loss 0.873279, acc 0.953125, prec 0.0640203, recall 0.847577
2017-12-09T22:52:44.710442: step 1637, loss 0.386409, acc 0.921875, prec 0.0640445, recall 0.847656
2017-12-09T22:52:45.006520: step 1638, loss 0.251168, acc 0.9375, prec 0.0641081, recall 0.847815
2017-12-09T22:52:45.302186: step 1639, loss 0.108324, acc 0.976562, prec 0.0641411, recall 0.847894
2017-12-09T22:52:45.601925: step 1640, loss 0.617927, acc 0.945312, prec 0.0641875, recall 0.848012
2017-12-09T22:52:45.906207: step 1641, loss 0.382554, acc 0.921875, prec 0.0641933, recall 0.848052
2017-12-09T22:52:46.201210: step 1642, loss 0.562202, acc 0.90625, prec 0.0642333, recall 0.84817
2017-12-09T22:52:46.498987: step 1643, loss 0.207133, acc 0.945312, prec 0.0642613, recall 0.848249
2017-12-09T22:52:46.797190: step 1644, loss 0.22134, acc 0.914062, prec 0.0642842, recall 0.848328
2017-12-09T22:52:47.091498: step 1645, loss 0.217306, acc 0.90625, prec 0.064269, recall 0.848328
2017-12-09T22:52:47.385308: step 1646, loss 0.523429, acc 0.914062, prec 0.0643286, recall 0.848485
2017-12-09T22:52:47.693967: step 1647, loss 0.355891, acc 0.898438, prec 0.064349, recall 0.848563
2017-12-09T22:52:47.990140: step 1648, loss 0.333555, acc 0.867188, prec 0.0643642, recall 0.848642
2017-12-09T22:52:48.286041: step 1649, loss 0.281767, acc 0.90625, prec 0.0643858, recall 0.84872
2017-12-09T22:52:48.583960: step 1650, loss 0.291081, acc 0.90625, prec 0.0643706, recall 0.84872
2017-12-09T22:52:48.878851: step 1651, loss 0.247072, acc 0.898438, prec 0.0643725, recall 0.848759
2017-12-09T22:52:49.180132: step 1652, loss 0.282477, acc 0.859375, prec 0.0643498, recall 0.848759
2017-12-09T22:52:49.480933: step 1653, loss 0.239502, acc 0.90625, prec 0.064353, recall 0.848798
2017-12-09T22:52:49.784892: step 1654, loss 0.266317, acc 0.898438, prec 0.064355, recall 0.848837
2017-12-09T22:52:50.087187: step 1655, loss 0.171281, acc 0.960938, prec 0.0643487, recall 0.848837
2017-12-09T22:52:50.394495: step 1656, loss 1.006, acc 0.929688, prec 0.064374, recall 0.848915
2017-12-09T22:52:50.697741: step 1657, loss 0.174628, acc 0.9375, prec 0.0644189, recall 0.849032
2017-12-09T22:52:50.992912: step 1658, loss 0.0699584, acc 0.976562, prec 0.0644151, recall 0.849032
2017-12-09T22:52:51.289041: step 1659, loss 0.245347, acc 0.921875, prec 0.0644208, recall 0.849071
2017-12-09T22:52:51.593963: step 1660, loss 0.219423, acc 0.945312, prec 0.0644669, recall 0.849188
2017-12-09T22:52:51.891432: step 1661, loss 0.151358, acc 0.9375, prec 0.0644934, recall 0.849266
2017-12-09T22:52:52.191369: step 1662, loss 0.127869, acc 0.960938, prec 0.0645237, recall 0.849343
2017-12-09T22:52:52.487983: step 1663, loss 0.199164, acc 0.921875, prec 0.0645294, recall 0.849382
2017-12-09T22:52:52.782354: step 1664, loss 0.26149, acc 0.9375, prec 0.0645559, recall 0.84946
2017-12-09T22:52:53.078109: step 1665, loss 0.137912, acc 0.960938, prec 0.0645679, recall 0.849498
2017-12-09T22:52:53.383319: step 1666, loss 0.0838098, acc 0.96875, prec 0.0645628, recall 0.849498
2017-12-09T22:52:53.686292: step 1667, loss 0.0566745, acc 0.992188, prec 0.0645981, recall 0.849576
2017-12-09T22:52:53.986560: step 1668, loss 0.123562, acc 0.953125, prec 0.0645905, recall 0.849576
2017-12-09T22:52:54.288442: step 1669, loss 0.137123, acc 0.976562, prec 0.0646416, recall 0.849692
2017-12-09T22:52:54.596258: step 1670, loss 0.17281, acc 0.953125, prec 0.0646889, recall 0.849807
2017-12-09T22:52:54.898884: step 1671, loss 1.28795, acc 0.96875, prec 0.0647216, recall 0.849666
2017-12-09T22:52:55.201436: step 1672, loss 0.246374, acc 0.96875, prec 0.0647348, recall 0.849705
2017-12-09T22:52:55.500370: step 1673, loss 2.32898, acc 0.914062, prec 0.0647965, recall 0.849424
2017-12-09T22:52:55.801891: step 1674, loss 0.21078, acc 0.929688, prec 0.0648034, recall 0.849462
2017-12-09T22:52:56.103130: step 1675, loss 0.301287, acc 0.914062, prec 0.0648443, recall 0.849578
2017-12-09T22:52:56.399296: step 1676, loss 0.301645, acc 0.898438, prec 0.0648278, recall 0.849578
2017-12-09T22:52:56.700278: step 1677, loss 0.24257, acc 0.921875, prec 0.0648152, recall 0.849578
2017-12-09T22:52:57.001643: step 1678, loss 0.379039, acc 0.84375, prec 0.0648081, recall 0.849616
2017-12-09T22:52:57.302481: step 1679, loss 0.426747, acc 0.867188, prec 0.0648049, recall 0.849655
2017-12-09T22:52:57.602756: step 1680, loss 0.379359, acc 0.84375, prec 0.0647978, recall 0.849693
2017-12-09T22:52:57.903358: step 1681, loss 0.249234, acc 0.898438, prec 0.0647997, recall 0.849732
2017-12-09T22:52:58.203822: step 1682, loss 0.305686, acc 0.882812, prec 0.0647989, recall 0.84977
2017-12-09T22:52:58.506774: step 1683, loss 0.376531, acc 0.851562, prec 0.0648114, recall 0.849847
2017-12-09T22:52:58.802864: step 1684, loss 0.450736, acc 0.859375, prec 0.0648069, recall 0.849885
2017-12-09T22:52:59.098187: step 1685, loss 0.320432, acc 0.859375, prec 0.0648206, recall 0.849962
2017-12-09T22:52:59.395367: step 1686, loss 0.266627, acc 0.90625, prec 0.0648418, recall 0.850038
2017-12-09T22:52:59.693722: step 1687, loss 0.305863, acc 0.929688, prec 0.0648851, recall 0.850153
2017-12-09T22:53:00.000441: step 1688, loss 0.225945, acc 0.9375, prec 0.0649113, recall 0.850229
2017-12-09T22:53:00.304832: step 1689, loss 0.358375, acc 0.929688, prec 0.0649363, recall 0.850305
2017-12-09T22:53:00.610181: step 1690, loss 0.147323, acc 0.953125, prec 0.0649833, recall 0.85042
2017-12-09T22:53:00.909368: step 1691, loss 0.231424, acc 0.945312, prec 0.0650108, recall 0.850496
2017-12-09T22:53:01.206732: step 1692, loss 0.315747, acc 0.921875, prec 0.0650527, recall 0.85061
2017-12-09T22:53:01.505083: step 1693, loss 0.144365, acc 0.9375, prec 0.0650425, recall 0.85061
2017-12-09T22:53:01.808383: step 1694, loss 0.229276, acc 0.953125, prec 0.0650531, recall 0.850648
2017-12-09T22:53:02.106842: step 1695, loss 0.910359, acc 0.960938, prec 0.0650844, recall 0.850508
2017-12-09T22:53:02.409666: step 1696, loss 0.242495, acc 0.953125, prec 0.065095, recall 0.850546
2017-12-09T22:53:02.714745: step 1697, loss 0.947866, acc 0.929688, prec 0.0650849, recall 0.85033
2017-12-09T22:53:03.018772: step 1698, loss 0.19349, acc 0.945312, prec 0.0650942, recall 0.850368
2017-12-09T22:53:03.316259: step 1699, loss 0.151177, acc 0.9375, prec 0.0651203, recall 0.850444
2017-12-09T22:53:03.618240: step 1700, loss 0.0692418, acc 0.976562, prec 0.0651347, recall 0.850482
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1700

2017-12-09T22:53:05.093648: step 1701, loss 0.335542, acc 0.96875, prec 0.0652022, recall 0.850633
2017-12-09T22:53:05.407559: step 1702, loss 0.0735747, acc 0.984375, prec 0.0652178, recall 0.850671
2017-12-09T22:53:05.713080: step 1703, loss 0.288433, acc 0.945312, prec 0.0652271, recall 0.850708
2017-12-09T22:53:06.013048: step 1704, loss 0.26358, acc 0.921875, prec 0.0652688, recall 0.850822
2017-12-09T22:53:06.314615: step 1705, loss 0.356931, acc 0.945312, prec 0.0652962, recall 0.850897
2017-12-09T22:53:06.616165: step 1706, loss 0.282944, acc 0.890625, prec 0.0653147, recall 0.850972
2017-12-09T22:53:06.917728: step 1707, loss 0.364082, acc 0.890625, prec 0.0653151, recall 0.85101
2017-12-09T22:53:07.223489: step 1708, loss 0.227956, acc 0.929688, prec 0.0653762, recall 0.85116
2017-12-09T22:53:07.526586: step 1709, loss 0.180585, acc 0.914062, prec 0.0653804, recall 0.851198
2017-12-09T22:53:07.827607: step 1710, loss 0.210896, acc 0.914062, prec 0.0653845, recall 0.851236
2017-12-09T22:53:08.128941: step 1711, loss 0.119063, acc 0.953125, prec 0.0654131, recall 0.85131
2017-12-09T22:53:08.427476: step 1712, loss 0.325885, acc 0.945312, prec 0.0654405, recall 0.851385
2017-12-09T22:53:08.733881: step 1713, loss 0.174491, acc 0.953125, prec 0.065451, recall 0.851423
2017-12-09T22:53:09.031054: step 1714, loss 0.1348, acc 0.945312, prec 0.0654421, recall 0.851423
2017-12-09T22:53:09.335528: step 1715, loss 0.176105, acc 0.945312, prec 0.0654513, recall 0.85146
2017-12-09T22:53:09.635796: step 1716, loss 0.0893295, acc 0.976562, prec 0.0654656, recall 0.851498
2017-12-09T22:53:09.935510: step 1717, loss 1.49839, acc 0.9375, prec 0.0654929, recall 0.851358
2017-12-09T22:53:10.243507: step 1718, loss 0.241845, acc 0.921875, prec 0.0655164, recall 0.851433
2017-12-09T22:53:10.541793: step 1719, loss 0.228566, acc 0.953125, prec 0.0655268, recall 0.85147
2017-12-09T22:53:10.841898: step 1720, loss 0.33309, acc 0.921875, prec 0.0655503, recall 0.851545
2017-12-09T22:53:11.139187: step 1721, loss 0.307285, acc 0.90625, prec 0.0655532, recall 0.851582
2017-12-09T22:53:11.438382: step 1722, loss 0.159328, acc 0.9375, prec 0.0655611, recall 0.851619
2017-12-09T22:53:11.741652: step 1723, loss 0.575069, acc 0.914062, prec 0.0656013, recall 0.851731
2017-12-09T22:53:12.044587: step 1724, loss 0.175941, acc 0.921875, prec 0.0656248, recall 0.851805
2017-12-09T22:53:12.344724: step 1725, loss 0.236578, acc 0.96875, prec 0.0657099, recall 0.851991
2017-12-09T22:53:12.644067: step 1726, loss 0.239227, acc 0.953125, prec 0.0657925, recall 0.852176
2017-12-09T22:53:12.942857: step 1727, loss 0.201807, acc 0.929688, prec 0.0657991, recall 0.852213
2017-12-09T22:53:13.245130: step 1728, loss 0.278144, acc 0.914062, prec 0.0658032, recall 0.85225
2017-12-09T22:53:13.546102: step 1729, loss 0.294056, acc 0.9375, prec 0.0658111, recall 0.852287
2017-12-09T22:53:13.850962: step 1730, loss 0.287775, acc 0.898438, prec 0.0658486, recall 0.852398
2017-12-09T22:53:14.146545: step 1731, loss 0.121094, acc 0.976562, prec 0.0658448, recall 0.852398
2017-12-09T22:53:14.443959: step 1732, loss 0.286258, acc 0.945312, prec 0.06589, recall 0.852508
2017-12-09T22:53:14.740777: step 1733, loss 0.356242, acc 0.929688, prec 0.0659146, recall 0.852582
2017-12-09T22:53:15.042946: step 1734, loss 0.29066, acc 0.953125, prec 0.066015, recall 0.852802
2017-12-09T22:53:15.345242: step 1735, loss 0.210288, acc 0.96875, prec 0.0660639, recall 0.852912
2017-12-09T22:53:15.642431: step 1736, loss 0.227787, acc 0.953125, prec 0.0661103, recall 0.853022
2017-12-09T22:53:15.944396: step 1737, loss 0.0873369, acc 0.96875, prec 0.0661232, recall 0.853058
2017-12-09T22:53:16.243205: step 1738, loss 0.280538, acc 0.953125, prec 0.0661515, recall 0.853131
2017-12-09T22:53:16.543552: step 1739, loss 0.179096, acc 0.96875, prec 0.0661644, recall 0.853168
2017-12-09T22:53:16.839840: step 1740, loss 0.15381, acc 0.960938, prec 0.066176, recall 0.853204
2017-12-09T22:53:17.140614: step 1741, loss 0.196826, acc 0.929688, prec 0.0661646, recall 0.853204
2017-12-09T22:53:17.440472: step 1742, loss 0.183993, acc 0.945312, prec 0.0661736, recall 0.853241
2017-12-09T22:53:17.618588: step 1743, loss 0.07632, acc 1, prec 0.0662276, recall 0.85335
2017-12-09T22:53:17.924729: step 1744, loss 0.197117, acc 0.9375, prec 0.0662533, recall 0.853423
2017-12-09T22:53:18.226357: step 1745, loss 0.155551, acc 0.960938, prec 0.066247, recall 0.853423
2017-12-09T22:53:18.523110: step 1746, loss 0.254047, acc 0.976562, prec 0.0662971, recall 0.853532
2017-12-09T22:53:18.827187: step 1747, loss 0.523354, acc 0.96875, prec 0.0663638, recall 0.853677
2017-12-09T22:53:19.130159: step 1748, loss 0.138046, acc 0.953125, prec 0.0663742, recall 0.853713
2017-12-09T22:53:19.435248: step 1749, loss 0.207427, acc 0.929688, prec 0.0663806, recall 0.853749
2017-12-09T22:53:19.737062: step 1750, loss 0.366944, acc 0.96875, prec 0.0664114, recall 0.853821
2017-12-09T22:53:20.039927: step 1751, loss 0.482494, acc 0.960938, prec 0.066441, recall 0.853894
2017-12-09T22:53:20.356590: step 1752, loss 0.144798, acc 0.953125, prec 0.0665051, recall 0.854038
2017-12-09T22:53:20.655805: step 1753, loss 0.128528, acc 0.9375, prec 0.0665128, recall 0.854074
2017-12-09T22:53:20.951791: step 1754, loss 0.178225, acc 0.960938, prec 0.0665244, recall 0.85411
2017-12-09T22:53:21.253611: step 1755, loss 0.30906, acc 0.890625, prec 0.0665244, recall 0.854146
2017-12-09T22:53:21.551763: step 1756, loss 0.189332, acc 0.9375, prec 0.0665142, recall 0.854146
2017-12-09T22:53:21.854713: step 1757, loss 0.12605, acc 0.953125, prec 0.0665424, recall 0.854218
2017-12-09T22:53:22.151538: step 1758, loss 0.122225, acc 0.953125, prec 0.0665885, recall 0.854326
2017-12-09T22:53:22.445817: step 1759, loss 0.437766, acc 0.945312, prec 0.0666334, recall 0.854433
2017-12-09T22:53:22.750477: step 1760, loss 0.210715, acc 0.945312, prec 0.0666244, recall 0.854433
2017-12-09T22:53:23.051428: step 1761, loss 0.467666, acc 0.953125, prec 0.0667601, recall 0.85472
2017-12-09T22:53:23.351549: step 1762, loss 0.305289, acc 0.929688, prec 0.0668023, recall 0.854827
2017-12-09T22:53:23.652791: step 1763, loss 0.242784, acc 0.929688, prec 0.0668266, recall 0.854898
2017-12-09T22:53:23.950430: step 1764, loss 0.180558, acc 0.945312, prec 0.0668355, recall 0.854934
2017-12-09T22:53:24.244730: step 1765, loss 0.162027, acc 0.9375, prec 0.0668611, recall 0.855005
2017-12-09T22:53:24.541410: step 1766, loss 0.192484, acc 0.960938, prec 0.0668726, recall 0.85504
2017-12-09T22:53:24.840961: step 1767, loss 0.197549, acc 0.945312, prec 0.0668636, recall 0.85504
2017-12-09T22:53:25.138829: step 1768, loss 0.140469, acc 0.976562, prec 0.0668955, recall 0.855112
2017-12-09T22:53:25.436439: step 1769, loss 0.206034, acc 0.96875, prec 0.0669083, recall 0.855147
2017-12-09T22:53:25.735561: step 1770, loss 0.148029, acc 0.945312, prec 0.0669709, recall 0.855289
2017-12-09T22:53:26.037659: step 1771, loss 0.211923, acc 0.960938, prec 0.0669823, recall 0.855324
2017-12-09T22:53:26.336637: step 1772, loss 0.260709, acc 0.921875, prec 0.0669874, recall 0.85536
2017-12-09T22:53:26.633748: step 1773, loss 0.181031, acc 0.9375, prec 0.0670129, recall 0.855431
2017-12-09T22:53:26.928452: step 1774, loss 0.188092, acc 0.921875, prec 0.0670179, recall 0.855466
2017-12-09T22:53:27.230394: step 1775, loss 0.15348, acc 0.953125, prec 0.0670102, recall 0.855466
2017-12-09T22:53:27.532139: step 1776, loss 0.0637923, acc 0.984375, prec 0.0670255, recall 0.855501
2017-12-09T22:53:27.833775: step 1777, loss 0.171976, acc 0.929688, prec 0.0670318, recall 0.855537
2017-12-09T22:53:28.130697: step 1778, loss 0.111215, acc 0.992188, prec 0.0670841, recall 0.855642
2017-12-09T22:53:28.432911: step 1779, loss 1.16591, acc 0.960938, prec 0.067079, recall 0.855433
2017-12-09T22:53:28.733798: step 1780, loss 0.107554, acc 0.96875, prec 0.0670917, recall 0.855469
2017-12-09T22:53:29.036088: step 1781, loss 0.205895, acc 0.96875, prec 0.0671223, recall 0.855539
2017-12-09T22:53:29.342484: step 1782, loss 0.368429, acc 0.976562, prec 0.067172, recall 0.855645
2017-12-09T22:53:29.643109: step 1783, loss 0.0415304, acc 0.992188, prec 0.0672065, recall 0.855715
2017-12-09T22:53:29.945826: step 1784, loss 0.178942, acc 0.945312, prec 0.067251, recall 0.855821
2017-12-09T22:53:30.250575: step 1785, loss 0.145959, acc 0.984375, prec 0.0672663, recall 0.855856
2017-12-09T22:53:30.554026: step 1786, loss 0.414298, acc 0.945312, prec 0.0673108, recall 0.855961
2017-12-09T22:53:30.853172: step 1787, loss 0.171418, acc 0.953125, prec 0.0673031, recall 0.855961
2017-12-09T22:53:31.150709: step 1788, loss 0.235905, acc 0.96875, prec 0.0673515, recall 0.856066
2017-12-09T22:53:31.449901: step 1789, loss 0.237682, acc 0.929688, prec 0.0673934, recall 0.856171
2017-12-09T22:53:31.753044: step 1790, loss 0.397784, acc 0.960938, prec 0.0674226, recall 0.856241
2017-12-09T22:53:32.055541: step 1791, loss 0.255027, acc 0.914062, prec 0.0674441, recall 0.856311
2017-12-09T22:53:32.356653: step 1792, loss 0.12562, acc 0.96875, prec 0.0674746, recall 0.85638
2017-12-09T22:53:32.657399: step 1793, loss 0.123978, acc 0.960938, prec 0.067486, recall 0.856415
2017-12-09T22:53:32.955473: step 1794, loss 0.369266, acc 0.929688, prec 0.0675278, recall 0.85652
2017-12-09T22:53:33.256179: step 1795, loss 0.294644, acc 0.90625, prec 0.0675836, recall 0.856659
2017-12-09T22:53:33.554448: step 1796, loss 0.271924, acc 0.960938, prec 0.0676484, recall 0.856797
2017-12-09T22:53:33.851987: step 1797, loss 0.352184, acc 0.929688, prec 0.0676901, recall 0.856901
2017-12-09T22:53:34.152016: step 1798, loss 0.240604, acc 0.921875, prec 0.0677306, recall 0.857005
2017-12-09T22:53:34.450103: step 1799, loss 0.188296, acc 0.914062, prec 0.0677342, recall 0.857039
2017-12-09T22:53:34.744854: step 1800, loss 0.151865, acc 0.9375, prec 0.0677238, recall 0.857039

Evaluation:
2017-12-09T22:53:39.406582: step 1800, loss 1.91361, acc 0.931497, prec 0.0682887, recall 0.846118

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1800

2017-12-09T22:53:40.857688: step 1801, loss 0.128491, acc 0.953125, prec 0.0683511, recall 0.846261
2017-12-09T22:53:41.152675: step 1802, loss 0.192343, acc 0.945312, prec 0.0684122, recall 0.846404
2017-12-09T22:53:41.449678: step 1803, loss 0.17182, acc 0.9375, prec 0.068437, recall 0.846476
2017-12-09T22:53:41.747452: step 1804, loss 0.173553, acc 0.945312, prec 0.068463, recall 0.846547
2017-12-09T22:53:42.052630: step 1805, loss 0.213439, acc 0.945312, prec 0.068489, recall 0.846619
2017-12-09T22:53:42.351513: step 1806, loss 0.062154, acc 0.984375, prec 0.0685215, recall 0.84669
2017-12-09T22:53:42.651209: step 1807, loss 0.159121, acc 0.976562, prec 0.0685876, recall 0.846832
2017-12-09T22:53:42.962005: step 1808, loss 1.34794, acc 0.953125, prec 0.0685987, recall 0.846671
2017-12-09T22:53:43.269345: step 1809, loss 0.255793, acc 0.890625, prec 0.0685982, recall 0.846707
2017-12-09T22:53:43.568625: step 1810, loss 0.156303, acc 0.9375, prec 0.0686228, recall 0.846778
2017-12-09T22:53:43.871338: step 1811, loss 0.0996485, acc 0.96875, prec 0.0686177, recall 0.846778
2017-12-09T22:53:44.174537: step 1812, loss 0.266441, acc 0.921875, prec 0.0686223, recall 0.846813
2017-12-09T22:53:44.473479: step 1813, loss 0.215708, acc 0.960938, prec 0.0686858, recall 0.846955
2017-12-09T22:53:44.771861: step 1814, loss 0.984863, acc 0.960938, prec 0.0687331, recall 0.846866
2017-12-09T22:53:45.080783: step 1815, loss 0.449267, acc 0.945312, prec 0.068759, recall 0.846936
2017-12-09T22:53:45.383548: step 1816, loss 0.211769, acc 0.90625, prec 0.0687785, recall 0.847007
2017-12-09T22:53:45.680549: step 1817, loss 0.137456, acc 0.960938, prec 0.068807, recall 0.847078
2017-12-09T22:53:45.984374: step 1818, loss 0.260254, acc 0.929688, prec 0.0688303, recall 0.847148
2017-12-09T22:53:46.292542: step 1819, loss 0.292547, acc 0.960938, prec 0.0688588, recall 0.847219
2017-12-09T22:53:46.593611: step 1820, loss 0.188908, acc 0.929688, prec 0.0688646, recall 0.847254
2017-12-09T22:53:46.890199: step 1821, loss 0.530548, acc 0.929688, prec 0.0688879, recall 0.847325
2017-12-09T22:53:47.188509: step 1822, loss 0.2019, acc 0.929688, prec 0.0689112, recall 0.847395
2017-12-09T22:53:47.494841: step 1823, loss 0.153272, acc 0.96875, prec 0.068941, recall 0.847465
2017-12-09T22:53:47.792450: step 1824, loss 0.162408, acc 0.929688, prec 0.0689468, recall 0.847501
2017-12-09T22:53:48.092956: step 1825, loss 0.401247, acc 0.90625, prec 0.0689662, recall 0.847571
2017-12-09T22:53:48.394024: step 1826, loss 0.300261, acc 0.898438, prec 0.0689842, recall 0.847641
2017-12-09T22:53:48.691097: step 1827, loss 0.647514, acc 0.960938, prec 0.0689952, recall 0.847676
2017-12-09T22:53:48.996081: step 1828, loss 0.139308, acc 0.945312, prec 0.0689862, recall 0.847676
2017-12-09T22:53:49.297917: step 1829, loss 0.241707, acc 0.929688, prec 0.0690094, recall 0.847746
2017-12-09T22:53:49.595856: step 1830, loss 0.0848638, acc 0.992188, prec 0.069043, recall 0.847816
2017-12-09T22:53:49.895175: step 1831, loss 1.30269, acc 0.929688, prec 0.0691023, recall 0.847761
2017-12-09T22:53:50.203365: step 1832, loss 0.880024, acc 0.898438, prec 0.0691217, recall 0.847637
2017-12-09T22:53:50.525460: step 1833, loss 0.435446, acc 0.90625, prec 0.0691758, recall 0.847776
2017-12-09T22:53:50.825685: step 1834, loss 0.358796, acc 0.882812, prec 0.0691912, recall 0.847846
2017-12-09T22:53:51.124311: step 1835, loss 0.313121, acc 0.875, prec 0.0691879, recall 0.847881
2017-12-09T22:53:51.420516: step 1836, loss 0.56177, acc 0.8125, prec 0.0691569, recall 0.847881
2017-12-09T22:53:51.721793: step 1837, loss 0.338342, acc 0.84375, prec 0.0691311, recall 0.847881
2017-12-09T22:53:52.023114: step 1838, loss 0.385677, acc 0.851562, prec 0.0691587, recall 0.847985
2017-12-09T22:53:52.318329: step 1839, loss 0.499953, acc 0.828125, prec 0.0691303, recall 0.847985
2017-12-09T22:53:52.620519: step 1840, loss 0.416971, acc 0.875, prec 0.069127, recall 0.84802
2017-12-09T22:53:52.923556: step 1841, loss 0.350098, acc 0.882812, prec 0.0691424, recall 0.84809
2017-12-09T22:53:53.222889: step 1842, loss 0.254967, acc 0.929688, prec 0.0691655, recall 0.848159
2017-12-09T22:53:53.524861: step 1843, loss 0.400183, acc 0.929688, prec 0.069206, recall 0.848263
2017-12-09T22:53:53.826340: step 1844, loss 1.43344, acc 0.929688, prec 0.069213, recall 0.848104
2017-12-09T22:53:54.122798: step 1845, loss 0.597919, acc 0.9375, prec 0.0692547, recall 0.848208
2017-12-09T22:53:54.426836: step 1846, loss 0.251787, acc 0.929688, prec 0.0693298, recall 0.848381
2017-12-09T22:53:54.730133: step 1847, loss 0.844218, acc 0.90625, prec 0.0693329, recall 0.848222
2017-12-09T22:53:55.028577: step 1848, loss 0.35536, acc 0.90625, prec 0.0693521, recall 0.848292
2017-12-09T22:53:55.326000: step 1849, loss 0.275143, acc 0.859375, prec 0.0693289, recall 0.848292
2017-12-09T22:53:55.628765: step 1850, loss 0.303179, acc 0.914062, prec 0.0693493, recall 0.848361
2017-12-09T22:53:55.929745: step 1851, loss 0.202511, acc 0.9375, prec 0.0693563, recall 0.848395
2017-12-09T22:53:56.227257: step 1852, loss 0.375049, acc 0.867188, prec 0.069369, recall 0.848464
2017-12-09T22:53:56.526996: step 1853, loss 0.44468, acc 0.882812, prec 0.0693496, recall 0.848464
2017-12-09T22:53:56.827808: step 1854, loss 0.290184, acc 0.914062, prec 0.0693528, recall 0.848499
2017-12-09T22:53:57.123390: step 1855, loss 0.179696, acc 0.921875, prec 0.0693918, recall 0.848602
2017-12-09T22:53:57.421680: step 1856, loss 0.230804, acc 0.929688, prec 0.0694148, recall 0.848671
2017-12-09T22:53:57.717586: step 1857, loss 0.207603, acc 0.90625, prec 0.0693993, recall 0.848671
2017-12-09T22:53:58.018455: step 1858, loss 0.238554, acc 0.90625, prec 0.0694011, recall 0.848705
2017-12-09T22:53:58.316715: step 1859, loss 0.201406, acc 0.9375, prec 0.0694254, recall 0.848774
2017-12-09T22:53:58.617370: step 1860, loss 0.228028, acc 0.914062, prec 0.0694457, recall 0.848843
2017-12-09T22:53:58.917851: step 1861, loss 0.371087, acc 0.96875, prec 0.0694751, recall 0.848911
2017-12-09T22:53:59.215556: step 1862, loss 0.329266, acc 0.953125, prec 0.0695019, recall 0.84898
2017-12-09T22:53:59.516137: step 1863, loss 0.120747, acc 0.96875, prec 0.0695141, recall 0.849014
2017-12-09T22:53:59.819722: step 1864, loss 0.083314, acc 0.96875, prec 0.0695262, recall 0.849048
2017-12-09T22:54:00.129258: step 1865, loss 0.0904262, acc 0.96875, prec 0.0695728, recall 0.849151
2017-12-09T22:54:00.442154: step 1866, loss 0.122813, acc 0.96875, prec 0.0696022, recall 0.849219
2017-12-09T22:54:00.741747: step 1867, loss 0.310565, acc 0.960938, prec 0.0696302, recall 0.849287
2017-12-09T22:54:01.042992: step 1868, loss 0.276003, acc 0.984375, prec 0.0696794, recall 0.849389
2017-12-09T22:54:01.342972: step 1869, loss 0.0259897, acc 1, prec 0.0696794, recall 0.849389
2017-12-09T22:54:01.646950: step 1870, loss 0.0978802, acc 0.953125, prec 0.0697062, recall 0.849458
2017-12-09T22:54:01.950602: step 1871, loss 0.6571, acc 0.960938, prec 0.0697342, recall 0.849526
2017-12-09T22:54:02.255696: step 1872, loss 0.127889, acc 0.984375, prec 0.0697661, recall 0.849594
2017-12-09T22:54:02.561140: step 1873, loss 0.126321, acc 0.96875, prec 0.0697782, recall 0.849627
2017-12-09T22:54:02.862954: step 1874, loss 0.219495, acc 0.9375, prec 0.0697679, recall 0.849627
2017-12-09T22:54:03.161064: step 1875, loss 0.191709, acc 0.960938, prec 0.0698131, recall 0.849729
2017-12-09T22:54:03.460685: step 1876, loss 0.112569, acc 0.976562, prec 0.0698265, recall 0.849763
2017-12-09T22:54:03.758271: step 1877, loss 0.201691, acc 0.976562, prec 0.0699088, recall 0.849932
2017-12-09T22:54:04.064599: step 1878, loss 0.222674, acc 0.96875, prec 0.0699381, recall 0.85
2017-12-09T22:54:04.370490: step 1879, loss 0.138259, acc 0.960938, prec 0.0699661, recall 0.850068
2017-12-09T22:54:04.669294: step 1880, loss 0.317553, acc 0.9375, prec 0.0699902, recall 0.850135
2017-12-09T22:54:04.972068: step 1881, loss 0.153567, acc 0.945312, prec 0.0699811, recall 0.850135
2017-12-09T22:54:05.274870: step 1882, loss 0.148222, acc 0.929688, prec 0.0700039, recall 0.850202
2017-12-09T22:54:05.585206: step 1883, loss 0.180184, acc 0.914062, prec 0.0699896, recall 0.850202
2017-12-09T22:54:05.884358: step 1884, loss 0.216207, acc 0.945312, prec 0.0699806, recall 0.850202
2017-12-09T22:54:06.182529: step 1885, loss 0.117212, acc 0.96875, prec 0.070027, recall 0.850303
2017-12-09T22:54:06.484429: step 1886, loss 0.38989, acc 0.960938, prec 0.0700894, recall 0.850438
2017-12-09T22:54:06.787481: step 1887, loss 0.0847792, acc 0.960938, prec 0.0700829, recall 0.850438
2017-12-09T22:54:07.088087: step 1888, loss 0.186832, acc 0.9375, prec 0.0700725, recall 0.850438
2017-12-09T22:54:07.386538: step 1889, loss 0.125718, acc 0.96875, prec 0.0700673, recall 0.850438
2017-12-09T22:54:07.687332: step 1890, loss 0.140004, acc 0.96875, prec 0.0700966, recall 0.850505
2017-12-09T22:54:07.988982: step 1891, loss 0.0882519, acc 0.960938, prec 0.0701073, recall 0.850539
2017-12-09T22:54:08.291857: step 1892, loss 0.110401, acc 0.984375, prec 0.0701391, recall 0.850606
2017-12-09T22:54:08.588589: step 1893, loss 0.0573904, acc 0.976562, prec 0.0701352, recall 0.850606
2017-12-09T22:54:08.888200: step 1894, loss 0.629865, acc 0.929688, prec 0.0701923, recall 0.85074
2017-12-09T22:54:09.193466: step 1895, loss 0.0726757, acc 0.984375, prec 0.0702069, recall 0.850773
2017-12-09T22:54:09.492016: step 1896, loss 0.387032, acc 0.960938, prec 0.070252, recall 0.850873
2017-12-09T22:54:09.791836: step 1897, loss 0.154506, acc 0.976562, prec 0.0702825, recall 0.85094
2017-12-09T22:54:10.092100: step 1898, loss 0.120558, acc 0.960938, prec 0.0702932, recall 0.850973
2017-12-09T22:54:10.392139: step 1899, loss 0.230416, acc 0.929688, prec 0.0702986, recall 0.851007
2017-12-09T22:54:10.698052: step 1900, loss 0.130655, acc 0.953125, prec 0.070308, recall 0.85104
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-1900

2017-12-09T22:54:12.811072: step 1901, loss 0.188045, acc 0.9375, prec 0.0702976, recall 0.85104
2017-12-09T22:54:13.110336: step 1902, loss 0.326871, acc 0.96875, prec 0.0703096, recall 0.851073
2017-12-09T22:54:13.416080: step 1903, loss 1.177, acc 0.929688, prec 0.0703164, recall 0.850916
2017-12-09T22:54:13.722524: step 1904, loss 0.162971, acc 0.960938, prec 0.0703271, recall 0.85095
2017-12-09T22:54:14.024231: step 1905, loss 0.186584, acc 0.960938, prec 0.0703549, recall 0.851016
2017-12-09T22:54:14.327458: step 1906, loss 0.538022, acc 0.96875, prec 0.0704527, recall 0.851216
2017-12-09T22:54:14.630449: step 1907, loss 0.337697, acc 0.914062, prec 0.0704556, recall 0.851249
2017-12-09T22:54:14.928498: step 1908, loss 0.204972, acc 0.953125, prec 0.0704992, recall 0.851348
2017-12-09T22:54:15.232265: step 1909, loss 0.314885, acc 0.914062, prec 0.0705364, recall 0.851448
2017-12-09T22:54:15.534841: step 1910, loss 0.404336, acc 0.882812, prec 0.0705168, recall 0.851448
2017-12-09T22:54:15.836659: step 1911, loss 0.33724, acc 0.90625, prec 0.0705527, recall 0.851547
2017-12-09T22:54:16.137453: step 1912, loss 0.228787, acc 0.929688, prec 0.0706266, recall 0.851712
2017-12-09T22:54:16.437050: step 1913, loss 0.379135, acc 0.890625, prec 0.0706427, recall 0.851778
2017-12-09T22:54:16.740698: step 1914, loss 0.393079, acc 0.882812, prec 0.0706574, recall 0.851844
2017-12-09T22:54:17.042608: step 1915, loss 0.279957, acc 0.90625, prec 0.0707102, recall 0.851975
2017-12-09T22:54:17.345511: step 1916, loss 0.18803, acc 0.9375, prec 0.0707169, recall 0.852008
2017-12-09T22:54:17.644125: step 1917, loss 0.201238, acc 0.914062, prec 0.0707197, recall 0.852041
2017-12-09T22:54:17.945066: step 1918, loss 0.18684, acc 0.929688, prec 0.0707593, recall 0.852139
2017-12-09T22:54:18.247644: step 1919, loss 0.303242, acc 0.929688, prec 0.0708331, recall 0.852303
2017-12-09T22:54:18.549251: step 1920, loss 0.161639, acc 0.9375, prec 0.0708569, recall 0.852368
2017-12-09T22:54:18.851880: step 1921, loss 0.171811, acc 0.9375, prec 0.0708977, recall 0.852466
2017-12-09T22:54:19.153120: step 1922, loss 0.795438, acc 0.90625, prec 0.0709333, recall 0.852564
2017-12-09T22:54:19.457348: step 1923, loss 0.305073, acc 0.921875, prec 0.0709715, recall 0.852662
2017-12-09T22:54:19.764047: step 1924, loss 0.246854, acc 0.929688, prec 0.0710452, recall 0.852824
2017-12-09T22:54:20.067774: step 1925, loss 0.456039, acc 0.9375, prec 0.0710518, recall 0.852857
2017-12-09T22:54:20.373947: step 1926, loss 0.157644, acc 0.945312, prec 0.0710597, recall 0.852889
2017-12-09T22:54:20.684963: step 1927, loss 0.168523, acc 0.953125, prec 0.071069, recall 0.852922
2017-12-09T22:54:20.989257: step 1928, loss 1.3489, acc 0.976562, prec 0.0711176, recall 0.852831
2017-12-09T22:54:21.301011: step 1929, loss 0.23085, acc 0.945312, prec 0.0711425, recall 0.852896
2017-12-09T22:54:21.605049: step 1930, loss 0.152044, acc 0.945312, prec 0.0712187, recall 0.853058
2017-12-09T22:54:21.904810: step 1931, loss 0.333091, acc 0.921875, prec 0.0712227, recall 0.85309
2017-12-09T22:54:22.204209: step 1932, loss 0.265873, acc 0.898438, prec 0.0712227, recall 0.853122
2017-12-09T22:54:22.513504: step 1933, loss 0.248663, acc 0.984375, prec 0.0713053, recall 0.853284
2017-12-09T22:54:22.815776: step 1934, loss 0.121789, acc 0.96875, prec 0.0713171, recall 0.853316
2017-12-09T22:54:23.114781: step 1935, loss 0.281072, acc 0.945312, prec 0.0713591, recall 0.853412
2017-12-09T22:54:23.421596: step 1936, loss 0.200106, acc 0.960938, prec 0.0713696, recall 0.853445
2017-12-09T22:54:23.718595: step 1937, loss 0.275634, acc 0.921875, prec 0.0713735, recall 0.853477
2017-12-09T22:54:24.018679: step 1938, loss 0.211002, acc 0.945312, prec 0.0714155, recall 0.853573
2017-12-09T22:54:24.324276: step 1939, loss 0.240602, acc 0.914062, prec 0.0714181, recall 0.853605
2017-12-09T22:54:24.624676: step 1940, loss 0.209613, acc 0.945312, prec 0.071426, recall 0.853637
2017-12-09T22:54:24.924943: step 1941, loss 0.277051, acc 0.90625, prec 0.0714613, recall 0.853733
2017-12-09T22:54:25.224906: step 1942, loss 0.20484, acc 0.9375, prec 0.0714508, recall 0.853733
2017-12-09T22:54:25.519363: step 1943, loss 0.280051, acc 0.96875, prec 0.0714626, recall 0.853765
2017-12-09T22:54:25.819743: step 1944, loss 0.0545408, acc 0.976562, prec 0.0714587, recall 0.853765
2017-12-09T22:54:26.126004: step 1945, loss 0.230134, acc 0.960938, prec 0.0714691, recall 0.853797
2017-12-09T22:54:26.427590: step 1946, loss 0.468935, acc 0.914062, prec 0.0715228, recall 0.853925
2017-12-09T22:54:26.728224: step 1947, loss 0.173155, acc 0.945312, prec 0.0715306, recall 0.853957
2017-12-09T22:54:27.026094: step 1948, loss 0.108841, acc 0.976562, prec 0.0715437, recall 0.853989
2017-12-09T22:54:27.328163: step 1949, loss 0.164911, acc 0.96875, prec 0.0715894, recall 0.854085
2017-12-09T22:54:27.631275: step 1950, loss 0.177006, acc 0.929688, prec 0.0716626, recall 0.854244
2017-12-09T22:54:27.931761: step 1951, loss 0.150291, acc 0.976562, prec 0.0716757, recall 0.854276
2017-12-09T22:54:28.231690: step 1952, loss 0.183381, acc 0.945312, prec 0.0716665, recall 0.854276
2017-12-09T22:54:28.532870: step 1953, loss 0.0803299, acc 0.976562, prec 0.0716795, recall 0.854308
2017-12-09T22:54:28.836061: step 1954, loss 0.0658696, acc 0.984375, prec 0.0716939, recall 0.854339
2017-12-09T22:54:29.136544: step 1955, loss 0.182718, acc 0.929688, prec 0.0716991, recall 0.854371
2017-12-09T22:54:29.437088: step 1956, loss 0.307771, acc 0.96875, prec 0.0717618, recall 0.854498
2017-12-09T22:54:29.742764: step 1957, loss 0.178848, acc 0.96875, prec 0.0717735, recall 0.85453
2017-12-09T22:54:30.051405: step 1958, loss 0.880831, acc 0.976562, prec 0.0717709, recall 0.854344
2017-12-09T22:54:30.358476: step 1959, loss 1.56362, acc 0.984375, prec 0.0717865, recall 0.854189
2017-12-09T22:54:30.661059: step 1960, loss 0.258578, acc 0.976562, prec 0.0717996, recall 0.854221
2017-12-09T22:54:30.961959: step 1961, loss 0.137724, acc 0.953125, prec 0.0718256, recall 0.854284
2017-12-09T22:54:31.260204: step 1962, loss 0.236705, acc 0.898438, prec 0.0718595, recall 0.854379
2017-12-09T22:54:31.558282: step 1963, loss 0.276798, acc 0.921875, prec 0.0718633, recall 0.854411
2017-12-09T22:54:31.858865: step 1964, loss 0.258591, acc 0.914062, prec 0.0718658, recall 0.854443
2017-12-09T22:54:32.155332: step 1965, loss 0.230032, acc 0.9375, prec 0.0718892, recall 0.854506
2017-12-09T22:54:32.458696: step 1966, loss 0.414079, acc 0.914062, prec 0.0719426, recall 0.854632
2017-12-09T22:54:32.753891: step 1967, loss 0.192107, acc 0.945312, prec 0.0719673, recall 0.854695
2017-12-09T22:54:33.048162: step 1968, loss 0.320728, acc 0.929688, prec 0.0719724, recall 0.854727
2017-12-09T22:54:33.345665: step 1969, loss 0.426963, acc 0.882812, prec 0.0719696, recall 0.854758
2017-12-09T22:54:33.647838: step 1970, loss 0.344976, acc 0.898438, prec 0.0719695, recall 0.85479
2017-12-09T22:54:33.944660: step 1971, loss 0.290584, acc 0.914062, prec 0.071955, recall 0.85479
2017-12-09T22:54:34.244597: step 1972, loss 0.55989, acc 0.914062, prec 0.0719575, recall 0.854821
2017-12-09T22:54:34.545688: step 1973, loss 0.238813, acc 0.921875, prec 0.0719613, recall 0.854853
2017-12-09T22:54:34.847670: step 1974, loss 0.595142, acc 0.914062, prec 0.0719807, recall 0.854916
2017-12-09T22:54:35.148718: step 1975, loss 0.160387, acc 0.929688, prec 0.0720028, recall 0.854978
2017-12-09T22:54:35.456290: step 1976, loss 0.364483, acc 0.890625, prec 0.0720182, recall 0.855041
2017-12-09T22:54:35.755571: step 1977, loss 0.196195, acc 0.914062, prec 0.0720038, recall 0.855041
2017-12-09T22:54:36.053955: step 1978, loss 0.214018, acc 0.9375, prec 0.0720271, recall 0.855104
2017-12-09T22:54:36.350345: step 1979, loss 1.31008, acc 0.890625, prec 0.072027, recall 0.85495
2017-12-09T22:54:36.654450: step 1980, loss 0.355419, acc 0.9375, prec 0.0720503, recall 0.855013
2017-12-09T22:54:36.950897: step 1981, loss 0.245571, acc 0.929688, prec 0.0720384, recall 0.855013
2017-12-09T22:54:37.248167: step 1982, loss 0.241991, acc 0.90625, prec 0.0720227, recall 0.855013
2017-12-09T22:54:37.549493: step 1983, loss 0.28053, acc 0.945312, prec 0.0720642, recall 0.855107
2017-12-09T22:54:37.857709: step 1984, loss 0.158624, acc 0.945312, prec 0.0720888, recall 0.855169
2017-12-09T22:54:38.175507: step 1985, loss 0.279067, acc 0.898438, prec 0.072173, recall 0.855357
2017-12-09T22:54:38.476726: step 1986, loss 0.172686, acc 0.945312, prec 0.0721976, recall 0.855419
2017-12-09T22:54:38.773163: step 1987, loss 0.206755, acc 0.929688, prec 0.0722195, recall 0.855481
2017-12-09T22:54:39.070834: step 1988, loss 0.0714955, acc 0.976562, prec 0.0722493, recall 0.855544
2017-12-09T22:54:39.369345: step 1989, loss 0.111731, acc 0.960938, prec 0.0722427, recall 0.855544
2017-12-09T22:54:39.668485: step 1990, loss 0.152287, acc 0.960938, prec 0.072253, recall 0.855575
2017-12-09T22:54:39.973909: step 1991, loss 0.190304, acc 0.976562, prec 0.0722828, recall 0.855637
2017-12-09T22:54:40.154049: step 1992, loss 0.13771, acc 0.942308, prec 0.0722789, recall 0.855637
2017-12-09T22:54:40.460434: step 1993, loss 0.12732, acc 0.96875, prec 0.0723073, recall 0.855699
2017-12-09T22:54:40.758951: step 1994, loss 0.222641, acc 0.9375, prec 0.0722968, recall 0.855699
2017-12-09T22:54:41.059083: step 1995, loss 0.155738, acc 0.96875, prec 0.0723253, recall 0.855761
2017-12-09T22:54:41.363459: step 1996, loss 0.0414664, acc 0.992188, prec 0.072324, recall 0.855761
2017-12-09T22:54:41.664146: step 1997, loss 0.0976665, acc 0.96875, prec 0.0723861, recall 0.855885
2017-12-09T22:54:41.962376: step 1998, loss 0.550121, acc 0.953125, prec 0.0723951, recall 0.855916
2017-12-09T22:54:42.263695: step 1999, loss 0.0220404, acc 0.992188, prec 0.0724274, recall 0.855978
2017-12-09T22:54:42.563019: step 2000, loss 0.0442588, acc 0.984375, prec 0.0724248, recall 0.855978
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-2000

2017-12-09T22:54:43.891960: step 2001, loss 0.611768, acc 0.96875, prec 0.0724869, recall 0.856101
2017-12-09T22:54:44.198159: step 2002, loss 0.102384, acc 0.976562, prec 0.072483, recall 0.856101
2017-12-09T22:54:44.499380: step 2003, loss 0.948467, acc 0.984375, prec 0.0725153, recall 0.855979
2017-12-09T22:54:44.804475: step 2004, loss 0.148286, acc 0.953125, prec 0.0725411, recall 0.856041
2017-12-09T22:54:45.105627: step 2005, loss 0.134816, acc 0.96875, prec 0.0725527, recall 0.856072
2017-12-09T22:54:45.407128: step 2006, loss 0.272727, acc 0.9375, prec 0.0725926, recall 0.856164
2017-12-09T22:54:45.706942: step 2007, loss 0.22116, acc 0.90625, prec 0.0725937, recall 0.856195
2017-12-09T22:54:46.006652: step 2008, loss 0.246105, acc 0.914062, prec 0.0726297, recall 0.856287
2017-12-09T22:54:46.308278: step 2009, loss 0.411112, acc 0.9375, prec 0.0727032, recall 0.856441
2017-12-09T22:54:46.609808: step 2010, loss 0.231071, acc 0.90625, prec 0.072721, recall 0.856502
2017-12-09T22:54:46.904965: step 2011, loss 0.253274, acc 0.90625, prec 0.072722, recall 0.856533
2017-12-09T22:54:47.207444: step 2012, loss 0.221973, acc 0.914062, prec 0.0727243, recall 0.856564
2017-12-09T22:54:47.511733: step 2013, loss 0.277539, acc 0.898438, prec 0.0727072, recall 0.856564
2017-12-09T22:54:47.814670: step 2014, loss 0.19749, acc 0.929688, prec 0.0727121, recall 0.856594
2017-12-09T22:54:48.112174: step 2015, loss 0.275672, acc 0.914062, prec 0.0727144, recall 0.856625
2017-12-09T22:54:48.410996: step 2016, loss 0.146916, acc 0.960938, prec 0.0727414, recall 0.856686
2017-12-09T22:54:48.707776: step 2017, loss 0.179083, acc 0.945312, prec 0.072749, recall 0.856716
2017-12-09T22:54:49.005718: step 2018, loss 0.163001, acc 0.929688, prec 0.0727539, recall 0.856747
2017-12-09T22:54:49.299617: step 2019, loss 0.170539, acc 0.9375, prec 0.0728105, recall 0.856869
2017-12-09T22:54:49.598389: step 2020, loss 0.265348, acc 0.9375, prec 0.0728503, recall 0.85696
2017-12-09T22:54:49.898614: step 2021, loss 0.18441, acc 0.953125, prec 0.0728927, recall 0.857052
2017-12-09T22:54:50.199909: step 2022, loss 0.0845835, acc 0.960938, prec 0.0728861, recall 0.857052
2017-12-09T22:54:50.510173: step 2023, loss 0.135742, acc 0.945312, prec 0.0728769, recall 0.857052
2017-12-09T22:54:50.812772: step 2024, loss 0.0689182, acc 0.992188, prec 0.0729091, recall 0.857112
2017-12-09T22:54:51.109025: step 2025, loss 0.427498, acc 0.96875, prec 0.0729542, recall 0.857204
2017-12-09T22:54:51.408830: step 2026, loss 0.185547, acc 0.96875, prec 0.0729992, recall 0.857295
2017-12-09T22:54:51.706397: step 2027, loss 0.0726851, acc 0.976562, prec 0.0729952, recall 0.857295
2017-12-09T22:54:52.003291: step 2028, loss 0.220267, acc 0.96875, prec 0.0730234, recall 0.857355
2017-12-09T22:54:52.302978: step 2029, loss 0.702454, acc 0.984375, prec 0.0730878, recall 0.857476
2017-12-09T22:54:52.605082: step 2030, loss 0.133237, acc 0.953125, prec 0.0730799, recall 0.857476
2017-12-09T22:54:52.898399: step 2031, loss 0.118057, acc 0.953125, prec 0.0731557, recall 0.857627
2017-12-09T22:54:53.200645: step 2032, loss 0.092375, acc 0.96875, prec 0.073184, recall 0.857687
2017-12-09T22:54:53.504772: step 2033, loss 0.126098, acc 0.953125, prec 0.0732263, recall 0.857778
2017-12-09T22:54:53.806514: step 2034, loss 0.20704, acc 0.953125, prec 0.0732853, recall 0.857898
2017-12-09T22:54:54.109918: step 2035, loss 0.171678, acc 0.953125, prec 0.0733108, recall 0.857958
2017-12-09T22:54:54.405824: step 2036, loss 0.106959, acc 0.976562, prec 0.0733403, recall 0.858018
2017-12-09T22:54:54.703952: step 2037, loss 0.114739, acc 0.953125, prec 0.0733324, recall 0.858018
2017-12-09T22:54:55.002653: step 2038, loss 0.141359, acc 0.945312, prec 0.0733733, recall 0.858108
2017-12-09T22:54:55.301777: step 2039, loss 0.166152, acc 0.9375, prec 0.0734129, recall 0.858198
2017-12-09T22:54:55.600463: step 2040, loss 0.259573, acc 0.9375, prec 0.0734357, recall 0.858258
2017-12-09T22:54:55.907531: step 2041, loss 0.167854, acc 0.953125, prec 0.0734278, recall 0.858258
2017-12-09T22:54:56.206120: step 2042, loss 0.102766, acc 0.953125, prec 0.0734365, recall 0.858288
2017-12-09T22:54:56.503210: step 2043, loss 0.0686807, acc 0.96875, prec 0.0734312, recall 0.858288
2017-12-09T22:54:56.799189: step 2044, loss 0.143068, acc 0.953125, prec 0.0734567, recall 0.858347
2017-12-09T22:54:57.097363: step 2045, loss 0.141058, acc 0.976562, prec 0.0734695, recall 0.858377
2017-12-09T22:54:57.396796: step 2046, loss 0.316866, acc 0.976562, prec 0.0734822, recall 0.858407
2017-12-09T22:54:57.700846: step 2047, loss 0.364742, acc 0.953125, prec 0.0735411, recall 0.858526
2017-12-09T22:54:58.003078: step 2048, loss 0.179342, acc 0.96875, prec 0.0735692, recall 0.858586
2017-12-09T22:54:58.305481: step 2049, loss 0.104222, acc 0.960938, prec 0.0735793, recall 0.858616
2017-12-09T22:54:58.602805: step 2050, loss 0.145423, acc 0.953125, prec 0.0735713, recall 0.858616
2017-12-09T22:54:58.905733: step 2051, loss 0.0859833, acc 0.96875, prec 0.0735994, recall 0.858675
2017-12-09T22:54:59.208735: step 2052, loss 0.224538, acc 0.953125, prec 0.0736415, recall 0.858764
2017-12-09T22:54:59.506498: step 2053, loss 0.0932929, acc 0.96875, prec 0.0736529, recall 0.858794
2017-12-09T22:54:59.802724: step 2054, loss 0.154669, acc 0.945312, prec 0.0736937, recall 0.858883
2017-12-09T22:55:00.115396: step 2055, loss 0.53526, acc 0.945312, prec 0.0737011, recall 0.858912
2017-12-09T22:55:00.422715: step 2056, loss 0.986338, acc 0.953125, prec 0.0737278, recall 0.858791
2017-12-09T22:55:00.730128: step 2057, loss 0.241063, acc 0.945312, prec 0.0737185, recall 0.858791
2017-12-09T22:55:01.028733: step 2058, loss 0.0739489, acc 0.992188, prec 0.0737339, recall 0.858821
2017-12-09T22:55:01.327386: step 2059, loss 0.177182, acc 0.953125, prec 0.0737426, recall 0.858851
2017-12-09T22:55:01.623003: step 2060, loss 0.255219, acc 0.929688, prec 0.073814, recall 0.858999
2017-12-09T22:55:01.921178: step 2061, loss 0.243266, acc 0.90625, prec 0.0738147, recall 0.859028
2017-12-09T22:55:02.221761: step 2062, loss 0.190355, acc 0.921875, prec 0.0738181, recall 0.859058
2017-12-09T22:55:02.518415: step 2063, loss 0.170797, acc 0.945312, prec 0.0738422, recall 0.859117
2017-12-09T22:55:02.820544: step 2064, loss 0.399347, acc 0.898438, prec 0.0739082, recall 0.859264
2017-12-09T22:55:03.119935: step 2065, loss 0.0899425, acc 0.960938, prec 0.0739515, recall 0.859352
2017-12-09T22:55:03.421417: step 2066, loss 0.257184, acc 0.90625, prec 0.0740188, recall 0.859499
2017-12-09T22:55:03.723005: step 2067, loss 0.22492, acc 0.914062, prec 0.0740042, recall 0.859499
2017-12-09T22:55:04.019856: step 2068, loss 0.174496, acc 0.945312, prec 0.0740281, recall 0.859558
2017-12-09T22:55:04.320089: step 2069, loss 0.74266, acc 0.929688, prec 0.0740341, recall 0.859408
2017-12-09T22:55:04.624973: step 2070, loss 0.210627, acc 0.921875, prec 0.0740707, recall 0.859496
2017-12-09T22:55:04.918987: step 2071, loss 0.101347, acc 0.960938, prec 0.0740974, recall 0.859554
2017-12-09T22:55:05.217602: step 2072, loss 0.227995, acc 0.890625, prec 0.0740787, recall 0.859554
2017-12-09T22:55:05.528348: step 2073, loss 0.325647, acc 0.929688, prec 0.0741333, recall 0.859671
2017-12-09T22:55:05.828383: step 2074, loss 0.158928, acc 0.945312, prec 0.0741904, recall 0.859788
2017-12-09T22:55:06.130620: step 2075, loss 0.108913, acc 0.9375, prec 0.0741798, recall 0.859788
2017-12-09T22:55:06.431011: step 2076, loss 0.176444, acc 0.921875, prec 0.0741831, recall 0.859817
2017-12-09T22:55:06.733276: step 2077, loss 0.332202, acc 0.9375, prec 0.0742056, recall 0.859875
2017-12-09T22:55:07.034644: step 2078, loss 0.208137, acc 0.945312, prec 0.0742129, recall 0.859904
2017-12-09T22:55:07.331775: step 2079, loss 0.152095, acc 0.9375, prec 0.0742355, recall 0.859963
2017-12-09T22:55:07.625704: step 2080, loss 0.0655498, acc 0.976562, prec 0.0742647, recall 0.860021
2017-12-09T22:55:07.924625: step 2081, loss 0.125035, acc 0.929688, prec 0.0742527, recall 0.860021
2017-12-09T22:55:08.220438: step 2082, loss 0.351537, acc 0.9375, prec 0.0742919, recall 0.860108
2017-12-09T22:55:08.517327: step 2083, loss 0.925067, acc 0.953125, prec 0.0743018, recall 0.859959
2017-12-09T22:55:08.819660: step 2084, loss 0.102757, acc 0.960938, prec 0.0743283, recall 0.860017
2017-12-09T22:55:09.118706: step 2085, loss 0.314373, acc 0.96875, prec 0.0744059, recall 0.860162
2017-12-09T22:55:09.424191: step 2086, loss 0.105323, acc 0.96875, prec 0.0744172, recall 0.860191
2017-12-09T22:55:09.721169: step 2087, loss 0.163351, acc 0.984375, prec 0.0744643, recall 0.860277
2017-12-09T22:55:10.021885: step 2088, loss 0.277166, acc 0.921875, prec 0.0744509, recall 0.860277
2017-12-09T22:55:10.319220: step 2089, loss 0.198363, acc 0.960938, prec 0.0744774, recall 0.860335
2017-12-09T22:55:10.623081: step 2090, loss 0.134777, acc 0.960938, prec 0.0744873, recall 0.860364
2017-12-09T22:55:10.923815: step 2091, loss 0.220908, acc 0.9375, prec 0.0745429, recall 0.86048
2017-12-09T22:55:11.226708: step 2092, loss 0.148716, acc 0.960938, prec 0.0745694, recall 0.860537
2017-12-09T22:55:11.529892: step 2093, loss 0.170601, acc 0.976562, prec 0.0746151, recall 0.860624
2017-12-09T22:55:11.832974: step 2094, loss 0.343559, acc 0.945312, prec 0.0746389, recall 0.860681
2017-12-09T22:55:12.142763: step 2095, loss 0.246355, acc 0.9375, prec 0.0746448, recall 0.86071
2017-12-09T22:55:12.442644: step 2096, loss 0.123389, acc 0.976562, prec 0.0746408, recall 0.86071
2017-12-09T22:55:12.742403: step 2097, loss 0.101764, acc 0.96875, prec 0.0746851, recall 0.860796
2017-12-09T22:55:13.041615: step 2098, loss 0.126872, acc 0.945312, prec 0.0746923, recall 0.860825
2017-12-09T22:55:13.341908: step 2099, loss 0.247282, acc 0.945312, prec 0.0746995, recall 0.860853
2017-12-09T22:55:13.643313: step 2100, loss 0.144711, acc 0.960938, prec 0.0747259, recall 0.860911

Evaluation:
2017-12-09T22:55:18.329936: step 2100, loss 2.81174, acc 0.96037, prec 0.075226, recall 0.846831

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-2100

2017-12-09T22:55:19.601638: step 2101, loss 0.129858, acc 0.976562, prec 0.0752385, recall 0.846861
2017-12-09T22:55:19.905791: step 2102, loss 0.208718, acc 0.96875, prec 0.075266, recall 0.846922
2017-12-09T22:55:20.209850: step 2103, loss 0.203148, acc 0.960938, prec 0.0752757, recall 0.846953
2017-12-09T22:55:20.522353: step 2104, loss 0.0630016, acc 0.992188, prec 0.0753072, recall 0.847014
2017-12-09T22:55:20.821471: step 2105, loss 0.291761, acc 0.96875, prec 0.0753347, recall 0.847075
2017-12-09T22:55:21.132138: step 2106, loss 0.0400715, acc 0.984375, prec 0.0753484, recall 0.847106
2017-12-09T22:55:21.430956: step 2107, loss 0.0399169, acc 0.984375, prec 0.0753622, recall 0.847136
2017-12-09T22:55:21.733969: step 2108, loss 0.06687, acc 0.984375, prec 0.0753595, recall 0.847136
2017-12-09T22:55:22.037076: step 2109, loss 0.073651, acc 0.992188, prec 0.0753746, recall 0.847167
2017-12-09T22:55:22.344460: step 2110, loss 0.275883, acc 0.976562, prec 0.0754198, recall 0.847258
2017-12-09T22:55:22.648197: step 2111, loss 0.0682736, acc 0.976562, prec 0.0754322, recall 0.847289
2017-12-09T22:55:22.954457: step 2112, loss 0.169104, acc 0.960938, prec 0.0754747, recall 0.84738
2017-12-09T22:55:23.257811: step 2113, loss 0.0797849, acc 0.984375, prec 0.0754884, recall 0.84741
2017-12-09T22:55:23.556399: step 2114, loss 0.104806, acc 1, prec 0.0755048, recall 0.847441
2017-12-09T22:55:23.858596: step 2115, loss 0.358477, acc 0.992188, prec 0.0755855, recall 0.847593
2017-12-09T22:55:24.161153: step 2116, loss 0.0463679, acc 0.992188, prec 0.075617, recall 0.847653
2017-12-09T22:55:24.467885: step 2117, loss 0.087295, acc 0.976562, prec 0.0756458, recall 0.847714
2017-12-09T22:55:24.778236: step 2118, loss 0.406532, acc 0.96875, prec 0.075706, recall 0.847835
2017-12-09T22:55:25.082021: step 2119, loss 0.147657, acc 0.945312, prec 0.0757294, recall 0.847895
2017-12-09T22:55:25.382206: step 2120, loss 0.161164, acc 0.96875, prec 0.0757568, recall 0.847956
2017-12-09T22:55:25.684140: step 2121, loss 0.819083, acc 0.960938, prec 0.0757678, recall 0.847817
2017-12-09T22:55:25.991038: step 2122, loss 0.234192, acc 0.921875, prec 0.0757871, recall 0.847878
2017-12-09T22:55:26.294994: step 2123, loss 0.0996969, acc 0.953125, prec 0.0757791, recall 0.847878
2017-12-09T22:55:26.594396: step 2124, loss 0.195772, acc 0.945312, prec 0.075786, recall 0.847908
2017-12-09T22:55:26.894290: step 2125, loss 0.163434, acc 0.90625, prec 0.0757863, recall 0.847938
2017-12-09T22:55:27.192871: step 2126, loss 0.200451, acc 0.921875, prec 0.0757729, recall 0.847938
2017-12-09T22:55:27.490647: step 2127, loss 0.187584, acc 0.921875, prec 0.0758249, recall 0.848059
2017-12-09T22:55:27.796087: step 2128, loss 0.214005, acc 0.929688, prec 0.0758292, recall 0.848089
2017-12-09T22:55:28.100269: step 2129, loss 0.174114, acc 0.945312, prec 0.0758362, recall 0.848119
2017-12-09T22:55:28.399496: step 2130, loss 0.108309, acc 0.984375, prec 0.0758499, recall 0.848149
2017-12-09T22:55:28.700260: step 2131, loss 0.214195, acc 0.914062, prec 0.0758678, recall 0.848209
2017-12-09T22:55:28.999132: step 2132, loss 0.19044, acc 0.921875, prec 0.0758544, recall 0.848209
2017-12-09T22:55:29.300934: step 2133, loss 0.279214, acc 0.945312, prec 0.0758613, recall 0.848239
2017-12-09T22:55:29.603023: step 2134, loss 0.285726, acc 0.945312, prec 0.0758519, recall 0.848239
2017-12-09T22:55:29.903263: step 2135, loss 0.166441, acc 0.953125, prec 0.0758766, recall 0.848299
2017-12-09T22:55:30.207070: step 2136, loss 0.118048, acc 0.992188, prec 0.075957, recall 0.848449
2017-12-09T22:55:30.514654: step 2137, loss 0.0466878, acc 0.976562, prec 0.0759693, recall 0.848479
2017-12-09T22:55:30.814999: step 2138, loss 0.684058, acc 0.960938, prec 0.0760443, recall 0.848628
2017-12-09T22:55:31.112551: step 2139, loss 0.718631, acc 0.953125, prec 0.0760852, recall 0.848718
2017-12-09T22:55:31.411174: step 2140, loss 0.0962526, acc 0.976562, prec 0.0760975, recall 0.848748
2017-12-09T22:55:31.723769: step 2141, loss 0.0338208, acc 0.992188, prec 0.0760962, recall 0.848748
2017-12-09T22:55:32.025723: step 2142, loss 0.19533, acc 0.9375, prec 0.0761507, recall 0.848867
2017-12-09T22:55:32.326243: step 2143, loss 0.221628, acc 0.945312, prec 0.076174, recall 0.848927
2017-12-09T22:55:32.622755: step 2144, loss 0.145995, acc 0.945312, prec 0.0761646, recall 0.848927
2017-12-09T22:55:32.923984: step 2145, loss 0.701118, acc 0.90625, prec 0.0761974, recall 0.849016
2017-12-09T22:55:33.229765: step 2146, loss 0.231618, acc 0.9375, prec 0.0762192, recall 0.849075
2017-12-09T22:55:33.527896: step 2147, loss 0.204623, acc 0.953125, prec 0.076309, recall 0.849253
2017-12-09T22:55:33.829032: step 2148, loss 0.195491, acc 0.9375, prec 0.0763146, recall 0.849283
2017-12-09T22:55:34.126065: step 2149, loss 0.231393, acc 0.921875, prec 0.0763011, recall 0.849283
2017-12-09T22:55:34.427092: step 2150, loss 0.369481, acc 0.929688, prec 0.0763053, recall 0.849312
2017-12-09T22:55:34.729073: step 2151, loss 0.234126, acc 0.914062, prec 0.0763068, recall 0.849342
2017-12-09T22:55:35.028428: step 2152, loss 0.518304, acc 0.882812, prec 0.0763355, recall 0.849431
2017-12-09T22:55:35.333815: step 2153, loss 0.324484, acc 0.960938, prec 0.0763613, recall 0.84949
2017-12-09T22:55:35.641162: step 2154, loss 0.361637, acc 0.867188, prec 0.0764199, recall 0.849637
2017-12-09T22:55:35.938527: step 2155, loss 0.309589, acc 0.929688, prec 0.0764566, recall 0.849726
2017-12-09T22:55:36.239731: step 2156, loss 0.405357, acc 0.921875, prec 0.0764919, recall 0.849814
2017-12-09T22:55:36.541351: step 2157, loss 0.16392, acc 0.96875, prec 0.0765354, recall 0.849902
2017-12-09T22:55:36.846893: step 2158, loss 0.262697, acc 0.914062, prec 0.0765694, recall 0.84999
2017-12-09T22:55:37.145219: step 2159, loss 0.188256, acc 0.921875, prec 0.0765721, recall 0.85002
2017-12-09T22:55:37.442792: step 2160, loss 0.205771, acc 0.945312, prec 0.0766115, recall 0.850107
2017-12-09T22:55:37.739067: step 2161, loss 0.201514, acc 0.9375, prec 0.0766007, recall 0.850107
2017-12-09T22:55:38.041773: step 2162, loss 0.175947, acc 0.96875, prec 0.0766278, recall 0.850166
2017-12-09T22:55:38.340989: step 2163, loss 0.142358, acc 0.960938, prec 0.0766698, recall 0.850254
2017-12-09T22:55:38.644534: step 2164, loss 0.0468007, acc 0.984375, prec 0.0766834, recall 0.850283
2017-12-09T22:55:38.942190: step 2165, loss 0.0995241, acc 0.960938, prec 0.0766929, recall 0.850312
2017-12-09T22:55:39.242884: step 2166, loss 0.17117, acc 0.960938, prec 0.0767674, recall 0.850458
2017-12-09T22:55:39.546293: step 2167, loss 0.0987526, acc 0.960938, prec 0.0767769, recall 0.850487
2017-12-09T22:55:39.842801: step 2168, loss 0.13504, acc 0.96875, prec 0.076804, recall 0.850546
2017-12-09T22:55:40.143042: step 2169, loss 0.0909097, acc 0.96875, prec 0.0767986, recall 0.850546
2017-12-09T22:55:40.444176: step 2170, loss 0.056729, acc 0.984375, prec 0.0768121, recall 0.850575
2017-12-09T22:55:40.743281: step 2171, loss 0.0366715, acc 0.992188, prec 0.0768108, recall 0.850575
2017-12-09T22:55:41.047046: step 2172, loss 0.140024, acc 0.976562, prec 0.0768229, recall 0.850604
2017-12-09T22:55:41.347065: step 2173, loss 0.421223, acc 0.984375, prec 0.0768527, recall 0.850662
2017-12-09T22:55:41.649584: step 2174, loss 0.0928165, acc 0.976562, prec 0.0768487, recall 0.850662
2017-12-09T22:55:41.951624: step 2175, loss 0.140133, acc 0.976562, prec 0.0768771, recall 0.85072
2017-12-09T22:55:42.249932: step 2176, loss 0.101973, acc 0.976562, prec 0.076938, recall 0.850836
2017-12-09T22:55:42.550529: step 2177, loss 0.393289, acc 0.976562, prec 0.0769826, recall 0.850923
2017-12-09T22:55:42.851394: step 2178, loss 0.0581989, acc 0.96875, prec 0.0769772, recall 0.850923
2017-12-09T22:55:43.151298: step 2179, loss 0.0720955, acc 0.992188, prec 0.0769758, recall 0.850923
2017-12-09T22:55:43.450579: step 2180, loss 0.256264, acc 0.945312, prec 0.0769988, recall 0.850981
2017-12-09T22:55:43.745235: step 2181, loss 0.170489, acc 1, prec 0.0770475, recall 0.851068
2017-12-09T22:55:44.048163: step 2182, loss 0.102183, acc 0.96875, prec 0.0770421, recall 0.851068
2017-12-09T22:55:44.347330: step 2183, loss 0.0757438, acc 0.976562, prec 0.0770867, recall 0.851155
2017-12-09T22:55:44.651307: step 2184, loss 0.314724, acc 0.9375, prec 0.0771245, recall 0.851241
2017-12-09T22:55:44.954872: step 2185, loss 0.441162, acc 0.9375, prec 0.0771461, recall 0.851299
2017-12-09T22:55:45.257955: step 2186, loss 0.0868698, acc 0.945312, prec 0.077169, recall 0.851357
2017-12-09T22:55:45.558170: step 2187, loss 0.0990398, acc 0.984375, prec 0.0772149, recall 0.851443
2017-12-09T22:55:45.863703: step 2188, loss 0.447839, acc 0.976562, prec 0.0773081, recall 0.851615
2017-12-09T22:55:46.165026: step 2189, loss 0.271948, acc 0.9375, prec 0.0773296, recall 0.851673
2017-12-09T22:55:46.466516: step 2190, loss 0.125583, acc 0.953125, prec 0.0773863, recall 0.851787
2017-12-09T22:55:46.765834: step 2191, loss 0.153376, acc 0.96875, prec 0.0774294, recall 0.851873
2017-12-09T22:55:47.069489: step 2192, loss 0.166124, acc 0.945312, prec 0.0774199, recall 0.851873
2017-12-09T22:55:47.372717: step 2193, loss 0.318974, acc 0.859375, prec 0.0774278, recall 0.85193
2017-12-09T22:55:47.672745: step 2194, loss 0.269474, acc 0.976562, prec 0.0774561, recall 0.851988
2017-12-09T22:55:47.972916: step 2195, loss 0.12767, acc 0.96875, prec 0.0774507, recall 0.851988
2017-12-09T22:55:48.269273: step 2196, loss 0.216934, acc 0.898438, prec 0.0774492, recall 0.852016
2017-12-09T22:55:48.566901: step 2197, loss 0.272455, acc 0.890625, prec 0.0774464, recall 0.852045
2017-12-09T22:55:48.868497: step 2198, loss 0.196607, acc 0.96875, prec 0.0774733, recall 0.852102
2017-12-09T22:55:49.167412: step 2199, loss 0.244207, acc 0.914062, prec 0.0774584, recall 0.852102
2017-12-09T22:55:49.463559: step 2200, loss 0.148687, acc 0.953125, prec 0.0774987, recall 0.852187
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-2200

2017-12-09T22:55:50.878101: step 2201, loss 0.362615, acc 0.929688, prec 0.0775188, recall 0.852244
2017-12-09T22:55:51.181905: step 2202, loss 0.127727, acc 0.9375, prec 0.0775565, recall 0.85233
2017-12-09T22:55:51.480814: step 2203, loss 0.182425, acc 0.953125, prec 0.0775645, recall 0.852358
2017-12-09T22:55:51.780051: step 2204, loss 0.12593, acc 0.960938, prec 0.0776061, recall 0.852443
2017-12-09T22:55:52.080822: step 2205, loss 0.565531, acc 0.945312, prec 0.0776128, recall 0.852472
2017-12-09T22:55:52.384062: step 2206, loss 0.108422, acc 0.96875, prec 0.0776558, recall 0.852557
2017-12-09T22:55:52.680664: step 2207, loss 0.118052, acc 0.945312, prec 0.0776463, recall 0.852557
2017-12-09T22:55:52.982967: step 2208, loss 0.141817, acc 0.953125, prec 0.0776381, recall 0.852557
2017-12-09T22:55:53.286041: step 2209, loss 0.114913, acc 0.953125, prec 0.0776461, recall 0.852585
2017-12-09T22:55:53.583525: step 2210, loss 0.0409935, acc 0.984375, prec 0.0776595, recall 0.852613
2017-12-09T22:55:53.880700: step 2211, loss 0.125204, acc 0.945312, prec 0.0776662, recall 0.852642
2017-12-09T22:55:54.175675: step 2212, loss 1.94272, acc 0.984375, prec 0.0777294, recall 0.852591
2017-12-09T22:55:54.476383: step 2213, loss 0.0943984, acc 0.976562, prec 0.0777414, recall 0.852619
2017-12-09T22:55:54.776586: step 2214, loss 0.0605196, acc 0.992188, prec 0.0777723, recall 0.852676
2017-12-09T22:55:55.077364: step 2215, loss 0.250705, acc 0.976562, prec 0.0778005, recall 0.852732
2017-12-09T22:55:55.378724: step 2216, loss 0.145314, acc 0.960938, prec 0.0777937, recall 0.852732
2017-12-09T22:55:55.675679: step 2217, loss 0.144627, acc 0.960938, prec 0.077803, recall 0.852761
2017-12-09T22:55:55.971869: step 2218, loss 0.667551, acc 0.914062, prec 0.0778203, recall 0.852817
2017-12-09T22:55:56.271363: step 2219, loss 0.165306, acc 0.976562, prec 0.0778807, recall 0.85293
2017-12-09T22:55:56.573382: step 2220, loss 0.429952, acc 0.882812, prec 0.0778926, recall 0.852986
2017-12-09T22:55:56.871831: step 2221, loss 0.213328, acc 0.9375, prec 0.07793, recall 0.853071
2017-12-09T22:55:57.170236: step 2222, loss 0.189654, acc 0.945312, prec 0.078001, recall 0.853211
2017-12-09T22:55:57.472082: step 2223, loss 0.138726, acc 0.960938, prec 0.0780103, recall 0.853239
2017-12-09T22:55:57.769015: step 2224, loss 0.152457, acc 0.953125, prec 0.0780505, recall 0.853323
2017-12-09T22:55:58.073575: step 2225, loss 0.161776, acc 0.984375, prec 0.0780639, recall 0.853351
2017-12-09T22:55:58.376173: step 2226, loss 0.22791, acc 0.9375, prec 0.0780691, recall 0.853379
2017-12-09T22:55:58.677551: step 2227, loss 0.56927, acc 0.945312, prec 0.0781239, recall 0.853491
2017-12-09T22:55:58.982954: step 2228, loss 0.226027, acc 0.960938, prec 0.0781654, recall 0.853575
2017-12-09T22:55:59.284498: step 2229, loss 0.317702, acc 0.953125, prec 0.0782055, recall 0.853659
2017-12-09T22:55:59.583470: step 2230, loss 0.244889, acc 0.921875, prec 0.0782079, recall 0.853686
2017-12-09T22:55:59.892744: step 2231, loss 0.458969, acc 0.953125, prec 0.0782319, recall 0.853742
2017-12-09T22:56:00.209028: step 2232, loss 0.117084, acc 0.96875, prec 0.0782264, recall 0.853742
2017-12-09T22:56:00.510267: step 2233, loss 0.113934, acc 0.960938, prec 0.0782518, recall 0.853798
2017-12-09T22:56:00.808978: step 2234, loss 0.308973, acc 0.953125, prec 0.0782757, recall 0.853853
2017-12-09T22:56:01.111583: step 2235, loss 0.145003, acc 0.945312, prec 0.0782983, recall 0.853909
2017-12-09T22:56:01.418013: step 2236, loss 0.201121, acc 0.953125, prec 0.0783223, recall 0.853965
2017-12-09T22:56:01.719260: step 2237, loss 0.526149, acc 0.914062, prec 0.0783715, recall 0.854076
2017-12-09T22:56:02.018373: step 2238, loss 0.297482, acc 0.90625, prec 0.0783551, recall 0.854076
2017-12-09T22:56:02.312388: step 2239, loss 0.224469, acc 0.945312, prec 0.0783938, recall 0.854159
2017-12-09T22:56:02.611662: step 2240, loss 0.283171, acc 0.882812, prec 0.0783733, recall 0.854159
2017-12-09T22:56:02.797898: step 2241, loss 0.24589, acc 0.942308, prec 0.0783692, recall 0.854159
2017-12-09T22:56:03.112042: step 2242, loss 0.123339, acc 0.984375, prec 0.0783986, recall 0.854214
2017-12-09T22:56:03.412675: step 2243, loss 0.083518, acc 0.976562, prec 0.0783945, recall 0.854214
2017-12-09T22:56:03.715947: step 2244, loss 0.230606, acc 0.9375, prec 0.0784317, recall 0.854297
2017-12-09T22:56:04.015100: step 2245, loss 0.119713, acc 0.96875, prec 0.0784583, recall 0.854352
2017-12-09T22:56:04.317210: step 2246, loss 0.0402519, acc 0.992188, prec 0.078473, recall 0.85438
2017-12-09T22:56:04.615826: step 2247, loss 0.0866932, acc 0.976562, prec 0.078485, recall 0.854408
2017-12-09T22:56:04.923278: step 2248, loss 0.131793, acc 0.953125, prec 0.0784928, recall 0.854435
2017-12-09T22:56:05.226983: step 2249, loss 0.93627, acc 0.945312, prec 0.0785488, recall 0.854384
2017-12-09T22:56:05.543014: step 2250, loss 0.0408647, acc 1, prec 0.0785969, recall 0.854466
2017-12-09T22:56:05.845805: step 2251, loss 0.095795, acc 0.976562, prec 0.0786249, recall 0.854521
2017-12-09T22:56:06.149206: step 2252, loss 0.307426, acc 0.984375, prec 0.0786703, recall 0.854604
2017-12-09T22:56:06.456774: step 2253, loss 0.148122, acc 0.960938, prec 0.0787115, recall 0.854686
2017-12-09T22:56:06.759932: step 2254, loss 0.071538, acc 0.976562, prec 0.0787234, recall 0.854714
2017-12-09T22:56:07.056710: step 2255, loss 0.113715, acc 0.976562, prec 0.0787514, recall 0.854769
2017-12-09T22:56:07.364515: step 2256, loss 0.413803, acc 0.960938, prec 0.0788087, recall 0.854878
2017-12-09T22:56:07.671369: step 2257, loss 0.273087, acc 0.929688, prec 0.0788123, recall 0.854906
2017-12-09T22:56:07.972482: step 2258, loss 0.104697, acc 0.960938, prec 0.0788055, recall 0.854906
2017-12-09T22:56:08.271027: step 2259, loss 0.303171, acc 0.914062, prec 0.0788064, recall 0.854933
2017-12-09T22:56:08.573111: step 2260, loss 0.193697, acc 0.9375, prec 0.0788115, recall 0.85496
2017-12-09T22:56:08.870656: step 2261, loss 0.297608, acc 0.90625, prec 0.078795, recall 0.85496
2017-12-09T22:56:09.166310: step 2262, loss 0.281061, acc 0.9375, prec 0.0788641, recall 0.855097
2017-12-09T22:56:09.466837: step 2263, loss 0.209993, acc 0.96875, prec 0.0789067, recall 0.855179
2017-12-09T22:56:09.768250: step 2264, loss 0.257231, acc 0.921875, prec 0.078909, recall 0.855206
2017-12-09T22:56:10.068757: step 2265, loss 0.125538, acc 0.960938, prec 0.0789341, recall 0.855261
2017-12-09T22:56:10.381154: step 2266, loss 0.139557, acc 0.929688, prec 0.0789378, recall 0.855288
2017-12-09T22:56:10.678926: step 2267, loss 0.108987, acc 0.96875, prec 0.0789643, recall 0.855342
2017-12-09T22:56:10.977328: step 2268, loss 0.352359, acc 0.9375, prec 0.0790013, recall 0.855424
2017-12-09T22:56:11.273742: step 2269, loss 0.12377, acc 0.929688, prec 0.0790209, recall 0.855478
2017-12-09T22:56:11.574680: step 2270, loss 0.881711, acc 0.953125, prec 0.079078, recall 0.855426
2017-12-09T22:56:11.876348: step 2271, loss 0.127183, acc 0.953125, prec 0.0790858, recall 0.855453
2017-12-09T22:56:12.174382: step 2272, loss 0.0952533, acc 0.960938, prec 0.0790949, recall 0.85548
2017-12-09T22:56:12.480874: step 2273, loss 0.0917845, acc 0.976562, prec 0.0790908, recall 0.85548
2017-12-09T22:56:12.786668: step 2274, loss 0.0964574, acc 0.96875, prec 0.0790853, recall 0.85548
2017-12-09T22:56:13.090221: step 2275, loss 0.138488, acc 0.96875, prec 0.0791277, recall 0.855562
2017-12-09T22:56:13.391039: step 2276, loss 0.0455646, acc 0.984375, prec 0.0791409, recall 0.855589
2017-12-09T22:56:13.688393: step 2277, loss 0.153509, acc 0.960938, prec 0.079182, recall 0.85567
2017-12-09T22:56:13.988884: step 2278, loss 0.265492, acc 0.953125, prec 0.0791738, recall 0.85567
2017-12-09T22:56:14.292354: step 2279, loss 0.240775, acc 0.960938, prec 0.0791829, recall 0.855697
2017-12-09T22:56:14.586038: step 2280, loss 0.0624812, acc 0.984375, prec 0.079228, recall 0.855778
2017-12-09T22:56:14.893059: step 2281, loss 0.125766, acc 0.976562, prec 0.0792399, recall 0.855805
2017-12-09T22:56:15.189405: step 2282, loss 0.251493, acc 0.96875, prec 0.0792663, recall 0.855859
2017-12-09T22:56:15.486338: step 2283, loss 0.0590144, acc 0.976562, prec 0.0792622, recall 0.855859
2017-12-09T22:56:15.789656: step 2284, loss 0.282191, acc 0.96875, prec 0.0793205, recall 0.855967
2017-12-09T22:56:16.086648: step 2285, loss 0.0462471, acc 0.992188, prec 0.0793191, recall 0.855967
2017-12-09T22:56:16.384707: step 2286, loss 0.0517919, acc 0.984375, prec 0.0793323, recall 0.855994
2017-12-09T22:56:16.682416: step 2287, loss 0.171942, acc 0.96875, prec 0.0793428, recall 0.856021
2017-12-09T22:56:16.978930: step 2288, loss 0.16909, acc 0.960938, prec 0.0793838, recall 0.856102
2017-12-09T22:56:17.276024: step 2289, loss 0.142995, acc 0.953125, prec 0.0793755, recall 0.856102
2017-12-09T22:56:17.571999: step 2290, loss 0.124544, acc 0.960938, prec 0.0794006, recall 0.856155
2017-12-09T22:56:17.878272: step 2291, loss 0.0654424, acc 0.953125, prec 0.0794401, recall 0.856236
2017-12-09T22:56:18.176356: step 2292, loss 0.067653, acc 0.976562, prec 0.079452, recall 0.856263
2017-12-09T22:56:18.480410: step 2293, loss 0.480945, acc 0.992188, prec 0.0795144, recall 0.85637
2017-12-09T22:56:18.787373: step 2294, loss 0.252057, acc 0.976562, prec 0.0795421, recall 0.856424
2017-12-09T22:56:19.100474: step 2295, loss 0.154421, acc 0.929688, prec 0.0795616, recall 0.856477
2017-12-09T22:56:19.407988: step 2296, loss 0.152266, acc 0.960938, prec 0.0795706, recall 0.856504
2017-12-09T22:56:19.709905: step 2297, loss 0.20335, acc 0.992188, prec 0.079633, recall 0.856611
2017-12-09T22:56:20.019082: step 2298, loss 0.134553, acc 0.953125, prec 0.0796566, recall 0.856664
2017-12-09T22:56:20.334343: step 2299, loss 0.0686941, acc 0.96875, prec 0.0796511, recall 0.856664
2017-12-09T22:56:20.643626: step 2300, loss 0.155224, acc 0.96875, prec 0.0796933, recall 0.856744
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-2300

2017-12-09T22:56:22.047661: step 2301, loss 0.130716, acc 0.960938, prec 0.0797183, recall 0.856797
2017-12-09T22:56:22.341650: step 2302, loss 0.170956, acc 0.96875, prec 0.0797605, recall 0.856877
2017-12-09T22:56:22.637959: step 2303, loss 0.188767, acc 0.914062, prec 0.0797613, recall 0.856904
2017-12-09T22:56:22.933493: step 2304, loss 0.131712, acc 0.960938, prec 0.0797703, recall 0.85693
2017-12-09T22:56:23.237232: step 2305, loss 0.109348, acc 0.976562, prec 0.0797821, recall 0.856957
2017-12-09T22:56:23.540052: step 2306, loss 0.101834, acc 0.960938, prec 0.0797911, recall 0.856984
2017-12-09T22:56:23.840048: step 2307, loss 0.142617, acc 0.945312, prec 0.0798133, recall 0.857037
2017-12-09T22:56:24.140162: step 2308, loss 0.0874301, acc 0.960938, prec 0.0798382, recall 0.85709
2017-12-09T22:56:24.438428: step 2309, loss 0.143557, acc 0.953125, prec 0.0798776, recall 0.857169
2017-12-09T22:56:24.744170: step 2310, loss 0.106554, acc 0.984375, prec 0.0799544, recall 0.857302
2017-12-09T22:56:25.047803: step 2311, loss 0.0698272, acc 0.976562, prec 0.0799661, recall 0.857328
2017-12-09T22:56:25.341907: step 2312, loss 0.102125, acc 0.953125, prec 0.0799896, recall 0.857381
2017-12-09T22:56:25.638105: step 2313, loss 0.127122, acc 0.960938, prec 0.0799827, recall 0.857381
2017-12-09T22:56:25.935990: step 2314, loss 0.326544, acc 0.992188, prec 0.0800926, recall 0.857566
2017-12-09T22:56:26.238454: step 2315, loss 0.295949, acc 0.984375, prec 0.0801375, recall 0.857645
2017-12-09T22:56:26.540443: step 2316, loss 0.0749451, acc 0.984375, prec 0.0801665, recall 0.857697
2017-12-09T22:56:26.844120: step 2317, loss 0.201068, acc 0.945312, prec 0.0802045, recall 0.857776
2017-12-09T22:56:27.144493: step 2318, loss 0.263948, acc 0.953125, prec 0.0802438, recall 0.857855
2017-12-09T22:56:27.447777: step 2319, loss 0.0584873, acc 0.976562, prec 0.0802555, recall 0.857881
2017-12-09T22:56:27.755017: step 2320, loss 0.16265, acc 0.960938, prec 0.0802963, recall 0.85796
2017-12-09T22:56:28.056751: step 2321, loss 0.114399, acc 0.960938, prec 0.0803052, recall 0.857986
2017-12-09T22:56:28.364261: step 2322, loss 0.0872304, acc 0.96875, prec 0.0803314, recall 0.858038
2017-12-09T22:56:28.662878: step 2323, loss 0.179876, acc 0.960938, prec 0.0803721, recall 0.858117
2017-12-09T22:56:28.967094: step 2324, loss 0.144848, acc 0.929688, prec 0.0803596, recall 0.858117
2017-12-09T22:56:29.267307: step 2325, loss 0.13073, acc 0.960938, prec 0.0803844, recall 0.858169
2017-12-09T22:56:29.564903: step 2326, loss 0.0343251, acc 0.984375, prec 0.0803816, recall 0.858169
2017-12-09T22:56:29.867154: step 2327, loss 0.084506, acc 0.96875, prec 0.0804395, recall 0.858274
2017-12-09T22:56:30.170782: step 2328, loss 0.113284, acc 0.984375, prec 0.0804526, recall 0.8583
2017-12-09T22:56:30.479516: step 2329, loss 0.284171, acc 0.976562, prec 0.0804961, recall 0.858378
2017-12-09T22:56:30.781040: step 2330, loss 0.207794, acc 0.96875, prec 0.0805381, recall 0.858456
2017-12-09T22:56:31.078630: step 2331, loss 0.339509, acc 0.960938, prec 0.0805787, recall 0.858534
2017-12-09T22:56:31.376003: step 2332, loss 0.0695362, acc 0.960938, prec 0.0806352, recall 0.858638
2017-12-09T22:56:31.676963: step 2333, loss 0.0377272, acc 1, prec 0.0806985, recall 0.858742
2017-12-09T22:56:31.979611: step 2334, loss 0.0885133, acc 0.976562, prec 0.0807578, recall 0.858845
2017-12-09T22:56:32.277811: step 2335, loss 0.11264, acc 0.96875, prec 0.0807839, recall 0.858897
2017-12-09T22:56:32.576302: step 2336, loss 0.0823653, acc 0.984375, prec 0.0808445, recall 0.859
2017-12-09T22:56:32.880990: step 2337, loss 0.3714, acc 0.953125, prec 0.0808519, recall 0.859026
2017-12-09T22:56:33.183173: step 2338, loss 0.146187, acc 0.96875, prec 0.0808622, recall 0.859052
2017-12-09T22:56:33.484909: step 2339, loss 0.34009, acc 0.953125, prec 0.0809489, recall 0.859206
2017-12-09T22:56:33.785022: step 2340, loss 0.078973, acc 0.96875, prec 0.0809591, recall 0.859232
2017-12-09T22:56:34.086250: step 2341, loss 0.209646, acc 0.960938, prec 0.0809838, recall 0.859284
2017-12-09T22:56:34.385936: step 2342, loss 0.29992, acc 0.914062, prec 0.0810001, recall 0.859335
2017-12-09T22:56:34.691378: step 2343, loss 0.0745659, acc 0.984375, prec 0.0810448, recall 0.859412
2017-12-09T22:56:34.989156: step 2344, loss 0.0673293, acc 0.960938, prec 0.0810536, recall 0.859438
2017-12-09T22:56:35.288521: step 2345, loss 2.09978, acc 0.929688, prec 0.0810583, recall 0.859307
2017-12-09T22:56:35.596105: step 2346, loss 0.124467, acc 0.960938, prec 0.0810671, recall 0.859332
2017-12-09T22:56:35.898056: step 2347, loss 0.229921, acc 0.90625, prec 0.0810978, recall 0.859409
2017-12-09T22:56:36.203144: step 2348, loss 0.177135, acc 0.921875, prec 0.0811313, recall 0.859486
2017-12-09T22:56:36.505317: step 2349, loss 0.296072, acc 0.898438, prec 0.0811448, recall 0.859537
2017-12-09T22:56:36.808733: step 2350, loss 0.412897, acc 0.84375, prec 0.0811327, recall 0.859563
2017-12-09T22:56:37.111796: step 2351, loss 0.24116, acc 0.90625, prec 0.0812107, recall 0.859716
2017-12-09T22:56:37.412692: step 2352, loss 0.287609, acc 0.898438, prec 0.0812241, recall 0.859767
2017-12-09T22:56:37.716235: step 2353, loss 0.203893, acc 0.921875, prec 0.0812417, recall 0.859818
2017-12-09T22:56:38.015869: step 2354, loss 0.173476, acc 0.9375, prec 0.0812779, recall 0.859895
2017-12-09T22:56:38.314145: step 2355, loss 0.278593, acc 0.929688, prec 0.0812653, recall 0.859895
2017-12-09T22:56:38.616969: step 2356, loss 0.285166, acc 0.921875, prec 0.0812829, recall 0.859945
2017-12-09T22:56:38.915499: step 2357, loss 0.18289, acc 0.9375, prec 0.0812718, recall 0.859945
2017-12-09T22:56:39.221231: step 2358, loss 0.12754, acc 0.953125, prec 0.0813107, recall 0.860022
2017-12-09T22:56:39.522924: step 2359, loss 0.135846, acc 0.945312, prec 0.0813325, recall 0.860073
2017-12-09T22:56:39.822787: step 2360, loss 0.143859, acc 0.953125, prec 0.0813556, recall 0.860123
2017-12-09T22:56:40.121764: step 2361, loss 0.0757366, acc 0.976562, prec 0.081383, recall 0.860174
2017-12-09T22:56:40.429942: step 2362, loss 0.338824, acc 0.984375, prec 0.0814117, recall 0.860225
2017-12-09T22:56:40.731040: step 2363, loss 0.136513, acc 0.960938, prec 0.0814362, recall 0.860275
2017-12-09T22:56:41.027587: step 2364, loss 0.267966, acc 0.953125, prec 0.0814909, recall 0.860377
2017-12-09T22:56:41.325323: step 2365, loss 0.0587438, acc 0.976562, prec 0.0815024, recall 0.860402
2017-12-09T22:56:41.638301: step 2366, loss 0.0901227, acc 0.96875, prec 0.0815126, recall 0.860427
2017-12-09T22:56:41.942762: step 2367, loss 0.187351, acc 0.984375, prec 0.0815571, recall 0.860503
2017-12-09T22:56:42.246004: step 2368, loss 0.0539092, acc 0.96875, prec 0.0815672, recall 0.860528
2017-12-09T22:56:42.545941: step 2369, loss 0.0385704, acc 0.984375, prec 0.0815959, recall 0.860579
2017-12-09T22:56:42.850295: step 2370, loss 0.118085, acc 0.96875, prec 0.0816061, recall 0.860604
2017-12-09T22:56:43.149322: step 2371, loss 0.090548, acc 0.984375, prec 0.081619, recall 0.860629
2017-12-09T22:56:43.452968: step 2372, loss 0.0586989, acc 0.992188, prec 0.0816491, recall 0.860679
2017-12-09T22:56:43.754939: step 2373, loss 0.0580822, acc 0.984375, prec 0.081662, recall 0.860705
2017-12-09T22:56:44.059146: step 2374, loss 1.71454, acc 0.960938, prec 0.0817037, recall 0.860625
2017-12-09T22:56:44.367866: step 2375, loss 0.511144, acc 0.976562, prec 0.0817467, recall 0.8607
2017-12-09T22:56:44.675531: step 2376, loss 0.0720901, acc 0.984375, prec 0.0817911, recall 0.860775
2017-12-09T22:56:44.982160: step 2377, loss 0.0342544, acc 0.984375, prec 0.0818197, recall 0.860826
2017-12-09T22:56:45.282809: step 2378, loss 0.162177, acc 0.929688, prec 0.08187, recall 0.860926
2017-12-09T22:56:45.585911: step 2379, loss 0.0741551, acc 0.976562, prec 0.0818816, recall 0.860951
2017-12-09T22:56:45.886597: step 2380, loss 0.185592, acc 0.953125, prec 0.0819046, recall 0.861001
2017-12-09T22:56:46.189024: step 2381, loss 0.163106, acc 0.921875, prec 0.0819063, recall 0.861026
2017-12-09T22:56:46.492984: step 2382, loss 0.150217, acc 0.976562, prec 0.0819807, recall 0.861151
2017-12-09T22:56:46.794904: step 2383, loss 0.154761, acc 0.953125, prec 0.0820037, recall 0.861201
2017-12-09T22:56:47.095903: step 2384, loss 0.110814, acc 0.96875, prec 0.0820295, recall 0.861251
2017-12-09T22:56:47.394512: step 2385, loss 0.299303, acc 0.929688, prec 0.082064, recall 0.861326
2017-12-09T22:56:47.694900: step 2386, loss 0.233747, acc 0.9375, prec 0.0821156, recall 0.861425
2017-12-09T22:56:48.001325: step 2387, loss 0.33336, acc 0.921875, prec 0.0821644, recall 0.861525
2017-12-09T22:56:48.307476: step 2388, loss 0.201209, acc 0.945312, prec 0.0822173, recall 0.861624
2017-12-09T22:56:48.612376: step 2389, loss 0.278119, acc 0.953125, prec 0.0822246, recall 0.861649
2017-12-09T22:56:48.911348: step 2390, loss 0.106879, acc 0.96875, prec 0.082219, recall 0.861649
2017-12-09T22:56:49.214733: step 2391, loss 0.171909, acc 0.960938, prec 0.0822276, recall 0.861674
2017-12-09T22:56:49.519208: step 2392, loss 0.0712203, acc 0.976562, prec 0.0822234, recall 0.861674
2017-12-09T22:56:49.818821: step 2393, loss 0.16027, acc 0.96875, prec 0.0822335, recall 0.861698
2017-12-09T22:56:50.120164: step 2394, loss 0.372078, acc 0.96875, prec 0.0822906, recall 0.861797
2017-12-09T22:56:50.432952: step 2395, loss 0.0950738, acc 0.976562, prec 0.0823177, recall 0.861847
2017-12-09T22:56:50.735599: step 2396, loss 0.16508, acc 0.960938, prec 0.0823421, recall 0.861896
2017-12-09T22:56:51.038546: step 2397, loss 0.0668314, acc 0.992188, prec 0.0824034, recall 0.861995
2017-12-09T22:56:51.338829: step 2398, loss 0.112302, acc 0.953125, prec 0.082395, recall 0.861995
2017-12-09T22:56:51.637935: step 2399, loss 0.125036, acc 0.96875, prec 0.0823893, recall 0.861995
2017-12-09T22:56:51.939319: step 2400, loss 0.046331, acc 0.984375, prec 0.0824022, recall 0.86202

Evaluation:
2017-12-09T22:56:56.640782: step 2400, loss 3.23617, acc 0.963672, prec 0.0827657, recall 0.848337

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_2/1512877430/checkpoints/model-2400

2017-12-09T22:56:57.975831: step 2401, loss 0.404849, acc 0.960938, prec 0.0827742, recall 0.848364
2017-12-09T22:56:58.274902: step 2402, loss 0.317414, acc 0.984375, prec 0.0828181, recall 0.848443
2017-12-09T22:56:58.577765: step 2403, loss 0.18903, acc 0.9375, prec 0.0828069, recall 0.848443
2017-12-09T22:56:58.878672: step 2404, loss 0.0730229, acc 0.976562, prec 0.0828182, recall 0.848469
2017-12-09T22:56:59.181363: step 2405, loss 0.132928, acc 0.953125, prec 0.0828098, recall 0.848469
2017-12-09T22:56:59.480412: step 2406, loss 0.243048, acc 0.960938, prec 0.0828183, recall 0.848495
2017-12-09T22:56:59.785464: step 2407, loss 0.157791, acc 0.96875, prec 0.0828283, recall 0.848522
2017-12-09T22:57:00.095717: step 2408, loss 0.0298718, acc 1, prec 0.082875, recall 0.848601
2017-12-09T22:57:00.405944: step 2409, loss 0.078926, acc 0.96875, prec 0.0828694, recall 0.848601
2017-12-09T22:57:00.707709: step 2410, loss 0.170542, acc 0.976562, prec 0.0828963, recall 0.848653
2017-12-09T22:57:01.010289: step 2411, loss 0.118295, acc 0.953125, prec 0.082919, recall 0.848706
2017-12-09T22:57:01.309451: step 2412, loss 0.309008, acc 0.992188, prec 0.0829487, recall 0.848758
2017-12-09T22:57:01.612572: step 2413, loss 0.114967, acc 0.984375, prec 0.082977, recall 0.848811
2017-12-09T22:57:01.913410: step 2414, loss 0.131739, acc 0.960938, prec 0.0829855, recall 0.848837
2017-12-09T22:57:02.211101: step 2415, loss 0.146154, acc 0.96875, prec 0.0829954, recall 0.848863
2017-12-09T22:57:02.514692: step 2416, loss 0.122389, acc 0.992188, prec 0.0830251, recall 0.848916
2017-12-09T22:57:02.812924: step 2417, loss 0.0225449, acc 1, prec 0.0830251, recall 0.848916
2017-12-09T22:57:03.111972: step 2418, loss 0.0950131, acc 0.960938, prec 0.0830337, recall 0.848942
2017-12-09T22:57:03.413155: step 2419, loss 0.718086, acc 0.96875, prec 0.0830294, recall 0.848795
2017-12-09T22:57:03.713674: step 2420, loss 0.0553474, acc 0.992188, prec 0.0830436, recall 0.848821
2017-12-09T22:57:04.018108: step 2421, loss 0.0717249, acc 0.976562, prec 0.0830704, recall 0.848873
2017-12-09T22:57:04.324694: step 2422, loss 0.0493522, acc 1, prec 0.0831482, recall 0.849004
2017-12-09T22:57:04.624680: step 2423, loss 0.18874, acc 0.953125, prec 0.0831864, recall 0.849083
2017-12-09T22:57:04.927055: step 2424, loss 0.132407, acc 0.976562, prec 0.0831821, recall 0.849083
2017-12-09T22:57:05.238500: step 2425, loss 0.076459, acc 0.96875, prec 0.0831921, recall 0.849109
2017-12-09T22:57:05.550602: step 2426, loss 0.162482, acc 0.953125, prec 0.0832302, recall 0.849187
2017-12-09T22:57:05.850446: step 2427, loss 1.2095, acc 0.953125, prec 0.0832542, recall 0.849092
2017-12-09T22:57:06.156862: step 2428, loss 0.159705, acc 0.960938, prec 0.0832783, recall 0.849145
2017-12-09T22:57:06.457449: step 2429, loss 0.145843, acc 0.953125, prec 0.0832853, recall 0.849171
2017-12-09T22:57:06.760570: step 2430, loss 0.183903, acc 0.945312, prec 0.0832754, recall 0.849171
2017-12-09T22:57:07.056500: step 2431, loss 0.206155, acc 0.953125, prec 0.083267, recall 0.849171
2017-12-09T22:57:07.356955: step 2432, loss 0.3057, acc 0.90625, prec 0.0832811, recall 0.849223
2017-12-09T22:57:07.660757: step 2433, loss 0.155604, acc 0.9375, prec 0.0833009, recall 0.849275
2017-12-09T22:57:07.961054: step 2434, loss 0.252549, acc 0.898438, prec 0.0832981, recall 0.849301
2017-12-09T22:57:08.259948: step 2435, loss 0.370948, acc 0.882812, prec 0.0833079, recall 0.849353
2017-12-09T22:57:08.559731: step 2436, loss 0.132696, acc 0.960938, prec 0.083363, recall 0.849457
2017-12-09T22:57:08.858818: step 2437, loss 0.156344, acc 0.960938, prec 0.0834024, recall 0.849535
2017-12-09T22:57:09.158080: step 2438, loss 0.23508, acc 0.9375, prec 0.0834377, recall 0.849612
2017-12-09T22:57:09.458886: step 2439, loss 0.108635, acc 0.953125, prec 0.0834602, recall 0.849664
2017-12-09T22:57:09.762279: step 2440, loss 0.19391, acc 0.953125, prec 0.0834517, recall 0.849664
2017-12-09T22:57:10.063084: step 2441, loss 0.144619, acc 0.9375, prec 0.0834714, recall 0.849716
2017-12-09T22:57:10.361740: step 2442, loss 0.31073, acc 0.898438, prec 0.0834996, recall 0.849794
2017-12-09T22:57:10.661035: step 2443, loss 0.162927, acc 0.9375, prec 0.0835038, recall 0.849819
2017-12-09T22:57:10.973754: step 2444, loss 0.0855228, acc 0.96875, prec 0.0835136, recall 0.849845
2017-12-09T22:57:11.275836: step 2445, loss 0.13938, acc 0.9375, prec 0.0835178, recall 0.849871
2017-12-09T22:57:11.575608: step 2446, loss 0.565724, acc 0.984375, prec 0.0835305, recall 0.849897
2017-12-09T22:57:11.874762: step 2447, loss 0.163873, acc 0.976562, prec 0.0835417, recall 0.849923
2017-12-09T22:57:12.180509: step 2448, loss 0.0685735, acc 0.96875, prec 0.0835361, recall 0.849923
2017-12-09T22:57:12.477237: step 2449, loss 0.158737, acc 0.96875, prec 0.0835769, recall 0.85
2017-12-09T22:57:12.782860: step 2450, loss 0.170053, acc 0.976562, prec 0.0836036, recall 0.850052
2017-12-09T22:57:13.080613: step 2451, loss 0.0767066, acc 0.96875, prec 0.0836289, recall 0.850103
2017-12-09T22:57:13.383771: step 2452, loss 0.910721, acc 0.96875, prec 0.0836556, recall 0.850009
2017-12-09T22:57:13.684733: step 2453, loss 0.0806649, acc 0.960938, prec 0.0836486, recall 0.850009
2017-12-09T22:57:13.993376: step 2454, loss 0.187964, acc 0.984375, prec 0.0836922, recall 0.850086
2017-12-09T22:57:14.300711: step 2455, loss 0.18744, acc 0.945312, prec 0.0836978, recall 0.850111
2017-12-09T22:57:14.607309: step 2456, loss 0.110406, acc 0.960938, prec 0.0837216, recall 0.850163
2017-12-09T22:57:14.912675: step 2457, loss 0.259455, acc 0.960938, prec 0.08373, recall 0.850189
2017-12-09T22:57:15.215861: step 2458, loss 0.353916, acc 0.945312, prec 0.0837356, recall 0.850214
2017-12-09T22:57:15.515097: step 2459, loss 0.264807, acc 0.914062, prec 0.0837355, recall 0.85024
2017-12-09T22:57:15.812715: step 2460, loss 0.260374, acc 0.953125, prec 0.0837425, recall 0.850266
2017-12-09T22:57:16.109984: step 2461, loss 0.285032, acc 0.929688, prec 0.0837762, recall 0.850342
2017-12-09T22:57:16.414208: step 2462, loss 0.100206, acc 0.960938, prec 0.0837846, recall 0.850368
2017-12-09T22:57:16.719665: step 2463, loss 0.325608, acc 0.929688, prec 0.0838182, recall 0.850445
2017-12-09T22:57:17.024168: step 2464, loss 0.0962837, acc 0.953125, prec 0.0838252, recall 0.85047
2017-12-09T22:57:17.401297: step 2465, loss 0.207999, acc 0.921875, prec 0.083811, recall 0.85047
2017-12-09T22:57:17.700489: step 2466, loss 0.189194, acc 0.953125, prec 0.083818, recall 0.850496
2017-12-09T22:57:18.003462: step 2467, loss 0.242085, acc 0.9375, prec 0.083853, recall 0.850573
2017-12-09T22:57:18.303502: step 2468, loss 0.860191, acc 0.9375, prec 0.0838586, recall 0.850453
2017-12-09T22:57:18.607673: step 2469, loss 0.176007, acc 0.945312, prec 0.0838796, recall 0.850504
2017-12-09T22:57:18.909470: step 2470, loss 0.257082, acc 0.953125, prec 0.0839019, recall 0.850555
2017-12-09T22:57:19.207859: step 2471, loss 0.265873, acc 0.9375, prec 0.0839061, recall 0.850581
2017-12-09T22:57:19.510437: step 2472, loss 0.0480703, acc 0.984375, prec 0.0839032, recall 0.850581
2017-12-09T22:57:19.812196: step 2473, loss 0.193054, acc 0.9375, prec 0.0839382, recall 0.850657
2017-12-09T22:57:20.117131: step 2474, loss 0.240937, acc 0.929688, prec 0.0839255, recall 0.850657
2017-12-09T22:57:20.422754: step 2475, loss 0.207979, acc 0.953125, prec 0.0839633, recall 0.850734
2017-12-09T22:57:20.718924: step 2476, loss 0.137683, acc 0.976562, prec 0.0840053, recall 0.85081
2017-12-09T22:57:21.021696: step 2477, loss 0.131444, acc 0.953125, prec 0.0839968, recall 0.85081
2017-12-09T22:57:21.320046: step 2478, loss 0.175464, acc 0.945312, prec 0.0840178, recall 0.850861
2017-12-09T22:57:21.616866: step 2479, loss 0.0949572, acc 0.953125, prec 0.0840093, recall 0.850861
2017-12-09T22:57:21.911307: step 2480, loss 0.160268, acc 0.953125, prec 0.0840162, recall 0.850886
2017-12-09T22:57:22.214694: step 2481, loss 0.455745, acc 0.921875, prec 0.0840175, recall 0.850912
2017-12-09T22:57:22.521527: step 2482, loss 0.266859, acc 0.945312, prec 0.0840692, recall 0.851013
2017-12-09T22:57:22.822697: step 2483, loss 0.220975, acc 0.921875, prec 0.0840705, recall 0.851038
2017-12-09T22:57:23.121034: step 2484, loss 0.173591, acc 0.96875, prec 0.0840956, recall 0.851089
2017-12-09T22:57:23.427384: step 2485, loss 0.172079, acc 0.96875, prec 0.0841208, recall 0.85114
2017-12-09T22:57:23.732522: step 2486, loss 0.0855403, acc 0.976562, prec 0.0841319, recall 0.851165
2017-12-09T22:57:24.035095: step 2487, loss 0.0255613, acc 0.992188, prec 0.0841921, recall 0.851266
2017-12-09T22:57:24.329256: step 2488, loss 0.0289906, acc 0.992188, prec 0.0841907, recall 0.851266
2017-12-09T22:57:24.628772: step 2489, loss 0.0506452, acc 0.976562, prec 0.0841865, recall 0.851266
2017-12-09T22:57:24.810902: step 2490, loss 0.0212309, acc 1, prec 0.0842019, recall 0.851292
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 128
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_128_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245

Start training
2017-12-09T22:57:28.624053: step 1, loss 17.2305, acc 0.632812, prec 0.0227273, recall 0.2
2017-12-09T22:57:28.920893: step 2, loss 2.09094, acc 0.507812, prec 0.0275229, recall 0.428571
2017-12-09T22:57:29.213987: step 3, loss 7.94991, acc 0.296875, prec 0.0298507, recall 0.545455
2017-12-09T22:57:29.511454: step 4, loss 5.93918, acc 0.1875, prec 0.0229508, recall 0.538462
2017-12-09T22:57:29.804203: step 5, loss 4.75126, acc 0.1875, prec 0.0195122, recall 0.571429
2017-12-09T22:57:30.104765: step 6, loss 5.6414, acc 0.140625, prec 0.0191571, recall 0.625
2017-12-09T22:57:30.406627: step 7, loss 5.77629, acc 0.257812, prec 0.0210016, recall 0.65
2017-12-09T22:57:30.703852: step 8, loss 4.71762, acc 0.21875, prec 0.0194444, recall 0.666667
2017-12-09T22:57:31.000493: step 9, loss 3.63854, acc 0.328125, prec 0.0210136, recall 0.708333
2017-12-09T22:57:31.295835: step 10, loss 7.00399, acc 0.335938, prec 0.0201342, recall 0.692308
2017-12-09T22:57:31.597504: step 11, loss 2.46925, acc 0.46875, prec 0.018711, recall 0.692308
2017-12-09T22:57:31.895221: step 12, loss 3.01692, acc 0.46875, prec 0.0174927, recall 0.666667
2017-12-09T22:57:32.190641: step 13, loss 2.44674, acc 0.585938, prec 0.0184502, recall 0.689655
2017-12-09T22:57:32.486934: step 14, loss 1.28893, acc 0.6875, prec 0.0186667, recall 0.7
2017-12-09T22:57:32.785506: step 15, loss 20.0259, acc 0.71875, prec 0.0181347, recall 0.636364
2017-12-09T22:57:33.088340: step 16, loss 4.8654, acc 0.6875, prec 0.0191827, recall 0.638889
2017-12-09T22:57:33.390377: step 17, loss 1.10146, acc 0.710938, prec 0.0201939, recall 0.657895
2017-12-09T22:57:33.686375: step 18, loss 3.66754, acc 0.664062, prec 0.0202966, recall 0.65
2017-12-09T22:57:33.981685: step 19, loss 1.42195, acc 0.671875, prec 0.0203927, recall 0.658537
2017-12-09T22:57:34.277083: step 20, loss 1.10452, acc 0.6875, prec 0.0197947, recall 0.658537
2017-12-09T22:57:34.574643: step 21, loss 1.50165, acc 0.59375, prec 0.0190678, recall 0.658537
2017-12-09T22:57:34.869361: step 22, loss 10.0971, acc 0.65625, prec 0.0191781, recall 0.651163
2017-12-09T22:57:35.166890: step 23, loss 1.20044, acc 0.726562, prec 0.0200401, recall 0.666667
2017-12-09T22:57:35.473883: step 24, loss 4.73784, acc 0.632812, prec 0.0194553, recall 0.638298
2017-12-09T22:57:35.773385: step 25, loss 1.56564, acc 0.671875, prec 0.0189394, recall 0.638298
2017-12-09T22:57:36.067465: step 26, loss 2.20583, acc 0.585938, prec 0.0183374, recall 0.625
2017-12-09T22:57:36.365559: step 27, loss 1.11624, acc 0.757812, prec 0.0191732, recall 0.64
2017-12-09T22:57:36.662856: step 28, loss 1.29809, acc 0.679688, prec 0.019287, recall 0.647059
2017-12-09T22:57:36.958997: step 29, loss 6.13454, acc 0.695312, prec 0.0188679, recall 0.634615
2017-12-09T22:57:37.255535: step 30, loss 1.38878, acc 0.710938, prec 0.0190263, recall 0.641509
2017-12-09T22:57:37.555340: step 31, loss 5.91106, acc 0.664062, prec 0.0185894, recall 0.62963
2017-12-09T22:57:37.853826: step 32, loss 3.65613, acc 0.640625, prec 0.0191898, recall 0.631579
2017-12-09T22:57:38.151932: step 33, loss 5.66936, acc 0.710938, prec 0.0193413, recall 0.627119
2017-12-09T22:57:38.446514: step 34, loss 2.20487, acc 0.625, prec 0.0208651, recall 0.650794
2017-12-09T22:57:38.746971: step 35, loss 3.35214, acc 0.679688, prec 0.021425, recall 0.651515
2017-12-09T22:57:39.046228: step 36, loss 3.7465, acc 0.585938, prec 0.0213592, recall 0.647059
2017-12-09T22:57:39.346748: step 37, loss 2.39583, acc 0.523438, prec 0.0221281, recall 0.661972
2017-12-09T22:57:39.645874: step 38, loss 2.86904, acc 0.4375, prec 0.0227376, recall 0.675676
2017-12-09T22:57:39.937459: step 39, loss 5.57625, acc 0.34375, prec 0.0227671, recall 0.675325
2017-12-09T22:57:40.235390: step 40, loss 3.97281, acc 0.414062, prec 0.0237087, recall 0.682927
2017-12-09T22:57:40.533343: step 41, loss 6.52726, acc 0.390625, prec 0.0233607, recall 0.678571
2017-12-09T22:57:40.829263: step 42, loss 3.46244, acc 0.375, prec 0.0233941, recall 0.686047
2017-12-09T22:57:41.123736: step 43, loss 4.08147, acc 0.273438, prec 0.0233091, recall 0.693182
2017-12-09T22:57:41.419310: step 44, loss 3.80922, acc 0.375, prec 0.0237037, recall 0.703297
2017-12-09T22:57:41.715670: step 45, loss 3.33815, acc 0.421875, prec 0.0234234, recall 0.706522
2017-12-09T22:57:42.012361: step 46, loss 3.26084, acc 0.382812, prec 0.0227751, recall 0.706522
2017-12-09T22:57:42.308061: step 47, loss 2.74307, acc 0.484375, prec 0.0222603, recall 0.706522
2017-12-09T22:57:42.604041: step 48, loss 3.54594, acc 0.570312, prec 0.0218561, recall 0.698925
2017-12-09T22:57:42.903066: step 49, loss 9.17928, acc 0.632812, prec 0.0221707, recall 0.697917
2017-12-09T22:57:43.203651: step 50, loss 1.24821, acc 0.71875, prec 0.0222295, recall 0.701031
2017-12-09T22:57:43.497305: step 51, loss 1.39035, acc 0.6875, prec 0.0219426, recall 0.701031
2017-12-09T22:57:43.793688: step 52, loss 0.924674, acc 0.757812, prec 0.0223499, recall 0.707071
2017-12-09T22:57:44.091346: step 53, loss 0.674242, acc 0.835938, prec 0.0225111, recall 0.71
2017-12-09T22:57:44.388192: step 54, loss 1.28374, acc 0.789062, prec 0.022327, recall 0.70297
2017-12-09T22:57:44.693689: step 55, loss 0.80055, acc 0.789062, prec 0.0221391, recall 0.70297
2017-12-09T22:57:44.989489: step 56, loss 11.0419, acc 0.835938, prec 0.0220019, recall 0.696078
2017-12-09T22:57:45.293248: step 57, loss 7.12452, acc 0.875, prec 0.0219001, recall 0.68932
2017-12-09T22:57:45.593672: step 58, loss 0.443862, acc 0.914062, prec 0.0221266, recall 0.692308
2017-12-09T22:57:45.890064: step 59, loss 0.972535, acc 0.929688, prec 0.0220723, recall 0.685714
2017-12-09T22:57:46.190942: step 60, loss 10.5587, acc 0.90625, prec 0.0219982, recall 0.679245
2017-12-09T22:57:46.494715: step 61, loss 19.419, acc 0.78125, prec 0.021838, recall 0.654545
2017-12-09T22:57:46.795816: step 62, loss 1.97951, acc 0.789062, prec 0.0216672, recall 0.648649
2017-12-09T22:57:47.099653: step 63, loss 12.0859, acc 0.742188, prec 0.0223414, recall 0.646552
2017-12-09T22:57:47.399710: step 64, loss 1.85897, acc 0.578125, prec 0.0225608, recall 0.652542
2017-12-09T22:57:47.692627: step 65, loss 10.5835, acc 0.546875, prec 0.0227535, recall 0.652893
2017-12-09T22:57:47.996033: step 66, loss 5.9136, acc 0.398438, prec 0.0228169, recall 0.653226
2017-12-09T22:57:48.295523: step 67, loss 5.2875, acc 0.234375, prec 0.0227397, recall 0.65873
2017-12-09T22:57:48.593608: step 68, loss 5.06348, acc 0.21875, prec 0.022394, recall 0.661417
2017-12-09T22:57:48.892001: step 69, loss 6.14167, acc 0.148438, prec 0.0217617, recall 0.661417
2017-12-09T22:57:49.189721: step 70, loss 6.55082, acc 0.140625, prec 0.0214052, recall 0.664062
2017-12-09T22:57:49.490719: step 71, loss 6.18879, acc 0.117188, prec 0.0208129, recall 0.664062
2017-12-09T22:57:49.786708: step 72, loss 6.58846, acc 0.117188, prec 0.0204859, recall 0.666667
2017-12-09T22:57:50.080106: step 73, loss 8.72758, acc 0.25, prec 0.0207169, recall 0.669173
2017-12-09T22:57:50.387469: step 74, loss 5.63295, acc 0.234375, prec 0.0207006, recall 0.674074
2017-12-09T22:57:50.693836: step 75, loss 3.75752, acc 0.375, prec 0.0207682, recall 0.678832
2017-12-09T22:57:50.985368: step 76, loss 2.88543, acc 0.4375, prec 0.021085, recall 0.685714
2017-12-09T22:57:51.280014: step 77, loss 2.39126, acc 0.515625, prec 0.0210139, recall 0.687943
2017-12-09T22:57:51.577740: step 78, loss 2.25534, acc 0.539062, prec 0.0207487, recall 0.687943
2017-12-09T22:57:51.871996: step 79, loss 1.4778, acc 0.648438, prec 0.0207583, recall 0.690141
2017-12-09T22:57:52.170472: step 80, loss 14.7228, acc 0.671875, prec 0.0209996, recall 0.680272
2017-12-09T22:57:52.475170: step 81, loss 1.32449, acc 0.679688, prec 0.0208203, recall 0.680272
2017-12-09T22:57:52.770406: step 82, loss 3.76334, acc 0.789062, prec 0.020911, recall 0.677852
2017-12-09T22:57:53.071319: step 83, loss 11.214, acc 0.789062, prec 0.0208076, recall 0.664474
2017-12-09T22:57:53.373842: step 84, loss 10.28, acc 0.695312, prec 0.0206544, recall 0.651613
2017-12-09T22:57:53.676017: step 85, loss 6.2374, acc 0.789062, prec 0.0209477, recall 0.647799
2017-12-09T22:57:53.981224: step 86, loss 2.33389, acc 0.578125, prec 0.021114, recall 0.652174
2017-12-09T22:57:54.278862: step 87, loss 2.42692, acc 0.515625, prec 0.020854, recall 0.652174
2017-12-09T22:57:54.574801: step 88, loss 3.46015, acc 0.414062, prec 0.0209311, recall 0.656442
2017-12-09T22:57:54.872027: step 89, loss 3.89611, acc 0.335938, prec 0.0205888, recall 0.656442
2017-12-09T22:57:55.171437: step 90, loss 3.8629, acc 0.367188, prec 0.0204584, recall 0.658537
2017-12-09T22:57:55.470740: step 91, loss 4.52729, acc 0.335938, prec 0.0206819, recall 0.664671
2017-12-09T22:57:55.765520: step 92, loss 4.08378, acc 0.335938, prec 0.0207187, recall 0.668639
2017-12-09T22:57:56.065709: step 93, loss 4.09068, acc 0.328125, prec 0.0205739, recall 0.670588
2017-12-09T22:57:56.358463: step 94, loss 6.30545, acc 0.367188, prec 0.0208037, recall 0.672414
2017-12-09T22:57:56.654709: step 95, loss 3.87079, acc 0.34375, prec 0.0208406, recall 0.676136
2017-12-09T22:57:56.952186: step 96, loss 3.2041, acc 0.4375, prec 0.021089, recall 0.681564
2017-12-09T22:57:57.250579: step 97, loss 2.5071, acc 0.460938, prec 0.0210077, recall 0.683333
2017-12-09T22:57:57.545322: step 98, loss 4.95491, acc 0.578125, prec 0.0208228, recall 0.675824
2017-12-09T22:57:57.844117: step 99, loss 2.15009, acc 0.53125, prec 0.0209415, recall 0.679348
2017-12-09T22:57:58.139173: step 100, loss 2.04033, acc 0.585938, prec 0.0212448, recall 0.684492
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-100

2017-12-09T22:57:59.486726: step 101, loss 2.79558, acc 0.640625, prec 0.0210873, recall 0.680851
2017-12-09T22:57:59.786122: step 102, loss 4.14341, acc 0.695312, prec 0.0211164, recall 0.678947
2017-12-09T22:58:00.095490: step 103, loss 1.60128, acc 0.6875, prec 0.0212974, recall 0.682292
2017-12-09T22:58:00.409368: step 104, loss 0.911841, acc 0.742188, prec 0.0211837, recall 0.682292
2017-12-09T22:58:00.706716: step 105, loss 1.54213, acc 0.703125, prec 0.0216833, recall 0.688776
2017-12-09T22:58:01.001809: step 106, loss 5.15773, acc 0.726562, prec 0.0215689, recall 0.681818
2017-12-09T22:58:01.300592: step 107, loss 1.06377, acc 0.695312, prec 0.0214354, recall 0.681818
2017-12-09T22:58:01.605562: step 108, loss 6.02624, acc 0.6875, prec 0.0213035, recall 0.678392
2017-12-09T22:58:01.900425: step 109, loss 3.01244, acc 0.726562, prec 0.0215003, recall 0.674877
2017-12-09T22:58:02.205601: step 110, loss 7.97891, acc 0.664062, prec 0.0213595, recall 0.671569
2017-12-09T22:58:02.503136: step 111, loss 4.24395, acc 0.648438, prec 0.0216685, recall 0.673077
2017-12-09T22:58:02.805100: step 112, loss 3.39918, acc 0.484375, prec 0.0217525, recall 0.672986
2017-12-09T22:58:03.104024: step 113, loss 1.78722, acc 0.65625, prec 0.0217557, recall 0.674528
2017-12-09T22:58:03.398431: step 114, loss 2.87452, acc 0.523438, prec 0.0217031, recall 0.676056
2017-12-09T22:58:03.692032: step 115, loss 6.93195, acc 0.507812, prec 0.0217943, recall 0.675926
2017-12-09T22:58:03.985272: step 116, loss 3.32748, acc 0.390625, prec 0.0216878, recall 0.677419
2017-12-09T22:58:04.279847: step 117, loss 2.75747, acc 0.484375, prec 0.0216216, recall 0.678899
2017-12-09T22:58:04.577939: step 118, loss 2.49946, acc 0.507812, prec 0.0215661, recall 0.680365
2017-12-09T22:58:04.873108: step 119, loss 2.42116, acc 0.578125, prec 0.0218203, recall 0.684685
2017-12-09T22:58:05.171537: step 120, loss 2.23006, acc 0.554688, prec 0.0217825, recall 0.686099
2017-12-09T22:58:05.485914: step 121, loss 4.4272, acc 0.671875, prec 0.0217945, recall 0.684444
2017-12-09T22:58:05.784648: step 122, loss 2.11263, acc 0.59375, prec 0.0217727, recall 0.685841
2017-12-09T22:58:06.078359: step 123, loss 3.01083, acc 0.585938, prec 0.0217513, recall 0.684211
2017-12-09T22:58:06.377781: step 124, loss 2.23328, acc 0.617188, prec 0.0217421, recall 0.682609
2017-12-09T22:58:06.671530: step 125, loss 1.95939, acc 0.53125, prec 0.0216973, recall 0.683983
2017-12-09T22:58:06.970643: step 126, loss 1.89791, acc 0.609375, prec 0.0216828, recall 0.685345
2017-12-09T22:58:07.269700: step 127, loss 4.71769, acc 0.734375, prec 0.0217214, recall 0.680851
2017-12-09T22:58:07.565402: step 128, loss 1.56042, acc 0.625, prec 0.0217127, recall 0.682203
2017-12-09T22:58:07.866668: step 129, loss 1.56508, acc 0.742188, prec 0.0218792, recall 0.684874
2017-12-09T22:58:08.165795: step 130, loss 1.32954, acc 0.65625, prec 0.0217507, recall 0.684874
2017-12-09T22:58:08.469025: step 131, loss 5.70942, acc 0.625, prec 0.0217449, recall 0.683333
2017-12-09T22:58:08.774133: step 132, loss 1.73565, acc 0.695312, prec 0.02202, recall 0.687243
2017-12-09T22:58:09.074561: step 133, loss 8.58738, acc 0.679688, prec 0.0220328, recall 0.685714
2017-12-09T22:58:09.378381: step 134, loss 10.4504, acc 0.664062, prec 0.0220396, recall 0.684211
2017-12-09T22:58:09.674497: step 135, loss 1.2003, acc 0.742188, prec 0.0221991, recall 0.686747
2017-12-09T22:58:09.976162: step 136, loss 7.39289, acc 0.632812, prec 0.0220674, recall 0.684
2017-12-09T22:58:10.276710: step 137, loss 4.81623, acc 0.617188, prec 0.022433, recall 0.686275
2017-12-09T22:58:10.573181: step 138, loss 2.33508, acc 0.515625, prec 0.022629, recall 0.689922
2017-12-09T22:58:10.869858: step 139, loss 2.36072, acc 0.5625, prec 0.0225925, recall 0.69112
2017-12-09T22:58:11.167131: step 140, loss 12.4986, acc 0.539062, prec 0.0225536, recall 0.687023
2017-12-09T22:58:11.466656: step 141, loss 2.38206, acc 0.53125, prec 0.0223853, recall 0.687023
2017-12-09T22:58:11.767706: step 142, loss 4.71872, acc 0.367188, prec 0.0221648, recall 0.684411
2017-12-09T22:58:12.065614: step 143, loss 2.60693, acc 0.328125, prec 0.0222899, recall 0.68797
2017-12-09T22:58:12.362663: step 144, loss 3.98747, acc 0.421875, prec 0.0223295, recall 0.687732
2017-12-09T22:58:12.657881: step 145, loss 3.56842, acc 0.398438, prec 0.0223577, recall 0.690037
2017-12-09T22:58:12.959930: step 146, loss 3.07539, acc 0.34375, prec 0.0223669, recall 0.692308
2017-12-09T22:58:13.259926: step 147, loss 2.90308, acc 0.40625, prec 0.0225114, recall 0.695652
2017-12-09T22:58:13.552662: step 148, loss 2.5893, acc 0.453125, prec 0.0225555, recall 0.697842
2017-12-09T22:58:13.847620: step 149, loss 2.64286, acc 0.460938, prec 0.0224888, recall 0.698925
2017-12-09T22:58:14.143701: step 150, loss 2.28126, acc 0.554688, prec 0.0225659, recall 0.701068
2017-12-09T22:58:14.442538: step 151, loss 1.69625, acc 0.617188, prec 0.0226626, recall 0.70318
2017-12-09T22:58:14.740872: step 152, loss 1.79055, acc 0.601562, prec 0.0226424, recall 0.704225
2017-12-09T22:58:15.046118: step 153, loss 1.64369, acc 0.664062, prec 0.0226428, recall 0.705263
2017-12-09T22:58:15.345250: step 154, loss 12.8967, acc 0.65625, prec 0.0226432, recall 0.703833
2017-12-09T22:58:15.645517: step 155, loss 1.33568, acc 0.773438, prec 0.0227882, recall 0.705882
2017-12-09T22:58:15.942394: step 156, loss 0.67939, acc 0.789062, prec 0.0228285, recall 0.706897
2017-12-09T22:58:16.237460: step 157, loss 3.83428, acc 0.765625, prec 0.0228635, recall 0.705479
2017-12-09T22:58:16.543379: step 158, loss 0.553051, acc 0.835938, prec 0.0228103, recall 0.705479
2017-12-09T22:58:16.842711: step 159, loss 7.46126, acc 0.78125, prec 0.0228527, recall 0.701695
2017-12-09T22:58:17.145944: step 160, loss 1.26731, acc 0.820312, prec 0.022905, recall 0.700337
2017-12-09T22:58:17.448431: step 161, loss 2.78636, acc 0.742188, prec 0.0229318, recall 0.698997
2017-12-09T22:58:17.748504: step 162, loss 3.51277, acc 0.75, prec 0.0228565, recall 0.694352
2017-12-09T22:58:18.048112: step 163, loss 2.95669, acc 0.757812, prec 0.0229948, recall 0.694079
2017-12-09T22:58:18.346517: step 164, loss 1.19475, acc 0.695312, prec 0.0230035, recall 0.695082
2017-12-09T22:58:18.642837: step 165, loss 1.51485, acc 0.617188, prec 0.0229873, recall 0.696078
2017-12-09T22:58:18.940557: step 166, loss 1.29089, acc 0.648438, prec 0.0228762, recall 0.696078
2017-12-09T22:58:19.234598: step 167, loss 1.99135, acc 0.570312, prec 0.0227418, recall 0.696078
2017-12-09T22:58:19.533607: step 168, loss 1.68295, acc 0.578125, prec 0.0227152, recall 0.697068
2017-12-09T22:58:19.832113: step 169, loss 2.13099, acc 0.539062, prec 0.0226769, recall 0.698052
2017-12-09T22:58:20.129830: step 170, loss 1.97836, acc 0.578125, prec 0.0228583, recall 0.698718
2017-12-09T22:58:20.448895: step 171, loss 4.88385, acc 0.65625, prec 0.0230641, recall 0.697161
2017-12-09T22:58:20.745402: step 172, loss 2.07432, acc 0.585938, prec 0.0232413, recall 0.7
2017-12-09T22:58:21.048786: step 173, loss 5.80804, acc 0.523438, prec 0.0230999, recall 0.695652
2017-12-09T22:58:21.343727: step 174, loss 2.30553, acc 0.507812, prec 0.0230509, recall 0.696594
2017-12-09T22:58:21.638710: step 175, loss 2.27901, acc 0.523438, prec 0.0231067, recall 0.698462
2017-12-09T22:58:21.933492: step 176, loss 3.68642, acc 0.570312, prec 0.0230793, recall 0.697248
2017-12-09T22:58:22.227175: step 177, loss 2.21577, acc 0.492188, prec 0.0231249, recall 0.699088
2017-12-09T22:58:22.525020: step 178, loss 2.49311, acc 0.546875, prec 0.0230885, recall 0.7
2017-12-09T22:58:22.823609: step 179, loss 2.57507, acc 0.546875, prec 0.0231495, recall 0.701807
2017-12-09T22:58:23.118005: step 180, loss 2.04699, acc 0.523438, prec 0.0231065, recall 0.702703
2017-12-09T22:58:23.414373: step 181, loss 2.13071, acc 0.5625, prec 0.0230754, recall 0.703593
2017-12-09T22:58:23.709319: step 182, loss 1.39206, acc 0.6875, prec 0.0232717, recall 0.706231
2017-12-09T22:58:24.007922: step 183, loss 1.46261, acc 0.585938, prec 0.0231518, recall 0.706231
2017-12-09T22:58:24.304939: step 184, loss 1.05124, acc 0.695312, prec 0.0231589, recall 0.707101
2017-12-09T22:58:24.604640: step 185, loss 1.29843, acc 0.71875, prec 0.0230784, recall 0.707101
2017-12-09T22:58:24.903294: step 186, loss 1.06062, acc 0.773438, prec 0.023014, recall 0.707101
2017-12-09T22:58:25.200878: step 187, loss 0.944597, acc 0.8125, prec 0.0233362, recall 0.710526
2017-12-09T22:58:25.502880: step 188, loss 0.505914, acc 0.820312, prec 0.0232848, recall 0.710526
2017-12-09T22:58:25.798942: step 189, loss 0.618258, acc 0.773438, prec 0.0232203, recall 0.710526
2017-12-09T22:58:26.100275: step 190, loss 0.367745, acc 0.890625, prec 0.0232824, recall 0.71137
2017-12-09T22:58:26.395865: step 191, loss 2.87274, acc 0.84375, prec 0.0234263, recall 0.710983
2017-12-09T22:58:26.694758: step 192, loss 0.679131, acc 0.890625, prec 0.0235809, recall 0.712644
2017-12-09T22:58:26.992479: step 193, loss 2.12856, acc 0.882812, prec 0.0236422, recall 0.711429
2017-12-09T22:58:27.291985: step 194, loss 0.332997, acc 0.867188, prec 0.0236967, recall 0.712251
2017-12-09T22:58:27.593142: step 195, loss 9.78957, acc 0.898438, prec 0.023672, recall 0.708215
2017-12-09T22:58:27.890430: step 196, loss 0.37314, acc 0.867188, prec 0.0238185, recall 0.709859
2017-12-09T22:58:28.188541: step 197, loss 1.73985, acc 0.835938, prec 0.0239577, recall 0.709497
2017-12-09T22:58:28.488037: step 198, loss 4.10238, acc 0.835938, prec 0.0240045, recall 0.708333
2017-12-09T22:58:28.788993: step 199, loss 2.74516, acc 0.875, prec 0.0240624, recall 0.707182
2017-12-09T22:58:29.090297: step 200, loss 0.82262, acc 0.828125, prec 0.0240128, recall 0.707182
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-200

2017-12-09T22:58:30.578833: step 201, loss 11.4416, acc 0.820312, prec 0.0240569, recall 0.70411
2017-12-09T22:58:30.887176: step 202, loss 1.83055, acc 0.617188, prec 0.0241289, recall 0.705722
2017-12-09T22:58:31.185202: step 203, loss 2.00641, acc 0.609375, prec 0.024198, recall 0.707317
2017-12-09T22:58:31.482559: step 204, loss 1.87712, acc 0.570312, prec 0.0242553, recall 0.708895
2017-12-09T22:58:31.776613: step 205, loss 1.56769, acc 0.648438, prec 0.024155, recall 0.708895
2017-12-09T22:58:32.075250: step 206, loss 1.71507, acc 0.585938, prec 0.0242164, recall 0.710456
2017-12-09T22:58:32.370349: step 207, loss 3.2392, acc 0.578125, prec 0.0243659, recall 0.710875
2017-12-09T22:58:32.668033: step 208, loss 2.29417, acc 0.46875, prec 0.0243043, recall 0.71164
2017-12-09T22:58:32.966452: step 209, loss 2.42533, acc 0.539062, prec 0.0242631, recall 0.712401
2017-12-09T22:58:33.266709: step 210, loss 2.4591, acc 0.546875, prec 0.024399, recall 0.71466
2017-12-09T22:58:33.564302: step 211, loss 1.91614, acc 0.570312, prec 0.0243664, recall 0.715405
2017-12-09T22:58:33.859677: step 212, loss 3.92703, acc 0.601562, prec 0.0242607, recall 0.711688
2017-12-09T22:58:34.158665: step 213, loss 1.89858, acc 0.578125, prec 0.0241452, recall 0.711688
2017-12-09T22:58:34.458328: step 214, loss 6.31864, acc 0.664062, prec 0.0242297, recall 0.709512
2017-12-09T22:58:34.763164: step 215, loss 6.67377, acc 0.640625, prec 0.0243092, recall 0.705584
2017-12-09T22:58:35.067577: step 216, loss 5.64456, acc 0.570312, prec 0.0244497, recall 0.70603
2017-12-09T22:58:35.371290: step 217, loss 1.97653, acc 0.515625, prec 0.0244029, recall 0.706767
2017-12-09T22:58:35.670025: step 218, loss 1.58436, acc 0.632812, prec 0.0244722, recall 0.708229
2017-12-09T22:58:35.971447: step 219, loss 2.32635, acc 0.5, prec 0.0246723, recall 0.711111
2017-12-09T22:58:36.268456: step 220, loss 2.43021, acc 0.515625, prec 0.0246251, recall 0.711823
2017-12-09T22:58:36.563843: step 221, loss 3.40189, acc 0.507812, prec 0.0245784, recall 0.710784
2017-12-09T22:58:36.864185: step 222, loss 3.24135, acc 0.460938, prec 0.0248463, recall 0.714286
2017-12-09T22:58:37.156566: step 223, loss 3.94131, acc 0.460938, prec 0.0247865, recall 0.713253
2017-12-09T22:58:37.452526: step 224, loss 2.79474, acc 0.4375, prec 0.0246379, recall 0.713253
2017-12-09T22:58:37.753567: step 225, loss 2.74818, acc 0.578125, prec 0.0246105, recall 0.71223
2017-12-09T22:58:38.052808: step 226, loss 2.6213, acc 0.523438, prec 0.0245672, recall 0.712919
2017-12-09T22:58:38.352840: step 227, loss 1.87061, acc 0.578125, prec 0.0246984, recall 0.714964
2017-12-09T22:58:38.655557: step 228, loss 2.7897, acc 0.632812, prec 0.0246853, recall 0.713948
2017-12-09T22:58:38.954138: step 229, loss 1.76936, acc 0.664062, prec 0.0247577, recall 0.715294
2017-12-09T22:58:39.260621: step 230, loss 1.8581, acc 0.671875, prec 0.0248316, recall 0.716628
2017-12-09T22:58:39.563824: step 231, loss 1.25312, acc 0.6875, prec 0.0249879, recall 0.718605
2017-12-09T22:58:39.864344: step 232, loss 1.5119, acc 0.757812, prec 0.0249274, recall 0.716937
2017-12-09T22:58:40.159770: step 233, loss 1.25287, acc 0.726562, prec 0.0250141, recall 0.718245
2017-12-09T22:58:40.456297: step 234, loss 5.62682, acc 0.710938, prec 0.0250221, recall 0.715596
2017-12-09T22:58:40.759328: step 235, loss 2.19644, acc 0.734375, prec 0.0251879, recall 0.71754
2017-12-09T22:58:41.053863: step 236, loss 5.14191, acc 0.703125, prec 0.0251933, recall 0.714932
2017-12-09T22:58:41.347577: step 237, loss 1.62957, acc 0.65625, prec 0.0251053, recall 0.714932
2017-12-09T22:58:41.644764: step 238, loss 1.19159, acc 0.710938, prec 0.0251089, recall 0.715576
2017-12-09T22:58:41.940108: step 239, loss 5.97949, acc 0.71875, prec 0.0251935, recall 0.715247
2017-12-09T22:58:42.237882: step 240, loss 1.7097, acc 0.640625, prec 0.0252557, recall 0.716518
2017-12-09T22:58:42.537171: step 241, loss 1.83008, acc 0.632812, prec 0.0253155, recall 0.717778
2017-12-09T22:58:42.835274: step 242, loss 1.96865, acc 0.6875, prec 0.0253886, recall 0.719027
2017-12-09T22:58:43.133873: step 243, loss 1.88369, acc 0.601562, prec 0.0254396, recall 0.720264
2017-12-09T22:58:43.433285: step 244, loss 3.49056, acc 0.515625, prec 0.0253194, recall 0.718681
2017-12-09T22:58:43.731666: step 245, loss 2.06346, acc 0.609375, prec 0.0252969, recall 0.719298
2017-12-09T22:58:44.027632: step 246, loss 1.50637, acc 0.609375, prec 0.0251998, recall 0.719298
2017-12-09T22:58:44.328268: step 247, loss 1.41712, acc 0.632812, prec 0.0252583, recall 0.720524
2017-12-09T22:58:44.628561: step 248, loss 7.78748, acc 0.703125, prec 0.0253377, recall 0.718615
2017-12-09T22:58:44.810032: step 249, loss 2.23821, acc 0.576923, prec 0.0253695, recall 0.719222
2017-12-09T22:58:45.117184: step 250, loss 1.57175, acc 0.6875, prec 0.0256624, recall 0.722222
2017-12-09T22:58:45.420029: step 251, loss 1.42959, acc 0.671875, prec 0.0256546, recall 0.722815
2017-12-09T22:58:45.721492: step 252, loss 1.36222, acc 0.671875, prec 0.0255733, recall 0.722815
2017-12-09T22:58:46.019613: step 253, loss 1.46836, acc 0.703125, prec 0.0255735, recall 0.723404
2017-12-09T22:58:46.317231: step 254, loss 1.24552, acc 0.664062, prec 0.0257832, recall 0.725738
2017-12-09T22:58:46.614881: step 255, loss 1.31585, acc 0.679688, prec 0.0258498, recall 0.726891
2017-12-09T22:58:46.912281: step 256, loss 0.902727, acc 0.773438, prec 0.0258666, recall 0.727463
2017-12-09T22:58:47.209488: step 257, loss 0.995463, acc 0.734375, prec 0.0260184, recall 0.729167
2017-12-09T22:58:47.507703: step 258, loss 1.06123, acc 0.75, prec 0.0260289, recall 0.72973
2017-12-09T22:58:47.805247: step 259, loss 0.804038, acc 0.796875, prec 0.0259788, recall 0.72973
2017-12-09T22:58:48.103349: step 260, loss 0.571398, acc 0.84375, prec 0.0259404, recall 0.72973
2017-12-09T22:58:48.401597: step 261, loss 0.493918, acc 0.851562, prec 0.0259759, recall 0.73029
2017-12-09T22:58:48.703662: step 262, loss 0.639224, acc 0.789062, prec 0.0259243, recall 0.73029
2017-12-09T22:58:49.001864: step 263, loss 2.70107, acc 0.859375, prec 0.0259635, recall 0.729339
2017-12-09T22:58:49.301300: step 264, loss 0.586721, acc 0.851562, prec 0.0259273, recall 0.729339
2017-12-09T22:58:49.600268: step 265, loss 1.73918, acc 0.898438, prec 0.0259759, recall 0.728395
2017-12-09T22:58:49.899341: step 266, loss 0.845377, acc 0.867188, prec 0.026015, recall 0.728953
2017-12-09T22:58:50.200831: step 267, loss 8.50572, acc 0.875, prec 0.0261309, recall 0.727088
2017-12-09T22:58:50.517296: step 268, loss 0.429278, acc 0.84375, prec 0.0261639, recall 0.727642
2017-12-09T22:58:50.810337: step 269, loss 3.44551, acc 0.882812, prec 0.026139, recall 0.724696
2017-12-09T22:58:51.110117: step 270, loss 0.615129, acc 0.851562, prec 0.0261028, recall 0.724696
2017-12-09T22:58:51.407986: step 271, loss 0.973506, acc 0.78125, prec 0.0261913, recall 0.725806
2017-12-09T22:58:51.704842: step 272, loss 0.755372, acc 0.796875, prec 0.0262833, recall 0.726908
2017-12-09T22:58:52.003086: step 273, loss 3.53121, acc 0.726562, prec 0.0265711, recall 0.728175
2017-12-09T22:58:52.303831: step 274, loss 0.982369, acc 0.773438, prec 0.0266561, recall 0.729249
2017-12-09T22:58:52.599275: step 275, loss 1.69627, acc 0.664062, prec 0.0267137, recall 0.730315
2017-12-09T22:58:52.895309: step 276, loss 1.76321, acc 0.679688, prec 0.0269145, recall 0.732422
2017-12-09T22:58:53.192204: step 277, loss 1.51368, acc 0.664062, prec 0.0268317, recall 0.732422
2017-12-09T22:58:53.494927: step 278, loss 2.29988, acc 0.507812, prec 0.0268499, recall 0.733463
2017-12-09T22:58:53.793070: step 279, loss 1.83544, acc 0.609375, prec 0.0267547, recall 0.733463
2017-12-09T22:58:54.088062: step 280, loss 2.53405, acc 0.617188, prec 0.0266638, recall 0.732039
2017-12-09T22:58:54.387258: step 281, loss 1.34259, acc 0.648438, prec 0.0266479, recall 0.732558
2017-12-09T22:58:54.681626: step 282, loss 1.44781, acc 0.671875, prec 0.0265692, recall 0.732558
2017-12-09T22:58:54.977416: step 283, loss 1.31536, acc 0.671875, prec 0.0266274, recall 0.733591
2017-12-09T22:58:55.275556: step 284, loss 1.56901, acc 0.695312, prec 0.0268269, recall 0.735632
2017-12-09T22:58:55.571834: step 285, loss 7.34633, acc 0.65625, prec 0.0268143, recall 0.734733
2017-12-09T22:58:55.872139: step 286, loss 1.30082, acc 0.6875, prec 0.0268074, recall 0.735238
2017-12-09T22:58:56.167322: step 287, loss 0.817932, acc 0.765625, prec 0.0268866, recall 0.736243
2017-12-09T22:58:56.465703: step 288, loss 1.34371, acc 0.703125, prec 0.0270177, recall 0.737736
2017-12-09T22:58:56.768639: step 289, loss 0.435891, acc 0.875, prec 0.0271222, recall 0.738722
2017-12-09T22:58:57.069188: step 290, loss 2.78129, acc 0.78125, prec 0.0272746, recall 0.73743
2017-12-09T22:58:57.372232: step 291, loss 1.39171, acc 0.8125, prec 0.0273634, recall 0.738404
2017-12-09T22:58:57.672905: step 292, loss 0.611468, acc 0.804688, prec 0.0273164, recall 0.738404
2017-12-09T22:58:57.970200: step 293, loss 0.844098, acc 0.804688, prec 0.0275361, recall 0.740331
2017-12-09T22:58:58.269139: step 294, loss 0.88533, acc 0.757812, prec 0.0275443, recall 0.740809
2017-12-09T22:58:58.568353: step 295, loss 3.20375, acc 0.773438, prec 0.0276906, recall 0.740876
2017-12-09T22:58:58.868177: step 296, loss 0.694894, acc 0.8125, prec 0.0277778, recall 0.741818
2017-12-09T22:58:59.166202: step 297, loss 0.584693, acc 0.828125, prec 0.0278023, recall 0.742287
2017-12-09T22:58:59.465858: step 298, loss 0.627123, acc 0.804688, prec 0.0277552, recall 0.742287
2017-12-09T22:58:59.763094: step 299, loss 0.524973, acc 0.828125, prec 0.0277797, recall 0.742754
2017-12-09T22:59:00.066963: step 300, loss 2.30156, acc 0.84375, prec 0.0278097, recall 0.741877

Evaluation:
2017-12-09T22:59:04.759879: step 300, loss 1.92774, acc 0.840819, prec 0.0307217, recall 0.715092

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-300

2017-12-09T22:59:06.022984: step 301, loss 0.400357, acc 0.898438, prec 0.0307562, recall 0.715493
2017-12-09T22:59:06.316999: step 302, loss 0.884268, acc 0.789062, prec 0.0308232, recall 0.716292
2017-12-09T22:59:06.615993: step 303, loss 0.880259, acc 0.765625, prec 0.0309428, recall 0.717483
2017-12-09T22:59:06.909743: step 304, loss 0.756948, acc 0.796875, prec 0.0308943, recall 0.717483
2017-12-09T22:59:07.207896: step 305, loss 3.96877, acc 0.867188, prec 0.0309229, recall 0.716876
2017-12-09T22:59:07.507361: step 306, loss 2.00689, acc 0.765625, prec 0.0309854, recall 0.716667
2017-12-09T22:59:07.811956: step 307, loss 1.80167, acc 0.765625, prec 0.0311058, recall 0.716851
2017-12-09T22:59:08.111740: step 308, loss 4.30283, acc 0.710938, prec 0.0310968, recall 0.716253
2017-12-09T22:59:08.423335: step 309, loss 0.644321, acc 0.789062, prec 0.0311623, recall 0.717033
2017-12-09T22:59:08.723206: step 310, loss 1.06028, acc 0.65625, prec 0.0311384, recall 0.717421
2017-12-09T22:59:09.018937: step 311, loss 1.2168, acc 0.695312, prec 0.0311813, recall 0.718194
2017-12-09T22:59:09.319464: step 312, loss 2.43151, acc 0.671875, prec 0.0311056, recall 0.717213
2017-12-09T22:59:09.619213: step 313, loss 1.86561, acc 0.554688, prec 0.0312297, recall 0.71875
2017-12-09T22:59:09.917433: step 314, loss 1.17196, acc 0.671875, prec 0.0313806, recall 0.72027
2017-12-09T22:59:10.210551: step 315, loss 1.67372, acc 0.585938, prec 0.0315104, recall 0.721774
2017-12-09T22:59:10.505431: step 316, loss 1.73986, acc 0.554688, prec 0.0316319, recall 0.723262
2017-12-09T22:59:10.799148: step 317, loss 1.32331, acc 0.625, prec 0.0316563, recall 0.724
2017-12-09T22:59:11.099771: step 318, loss 1.19605, acc 0.65625, prec 0.0316879, recall 0.724734
2017-12-09T22:59:11.396827: step 319, loss 1.22703, acc 0.6875, prec 0.0317267, recall 0.725464
2017-12-09T22:59:11.695476: step 320, loss 0.593545, acc 0.757812, prec 0.0316698, recall 0.725464
2017-12-09T22:59:11.989863: step 321, loss 0.91414, acc 0.742188, prec 0.0316653, recall 0.725828
2017-12-09T22:59:12.285536: step 322, loss 0.965785, acc 0.726562, prec 0.0316014, recall 0.725828
2017-12-09T22:59:12.585740: step 323, loss 0.88325, acc 0.757812, prec 0.0317122, recall 0.726913
2017-12-09T22:59:12.886873: step 324, loss 2.9223, acc 0.835938, prec 0.0316758, recall 0.725955
2017-12-09T22:59:13.184975: step 325, loss 3.62791, acc 0.875, prec 0.0317077, recall 0.72346
2017-12-09T22:59:13.485507: step 326, loss 0.411817, acc 0.8125, prec 0.0316641, recall 0.72346
2017-12-09T22:59:13.780006: step 327, loss 0.520352, acc 0.796875, prec 0.0316169, recall 0.72346
2017-12-09T22:59:14.078555: step 328, loss 0.909872, acc 0.8125, prec 0.031795, recall 0.724902
2017-12-09T22:59:14.376076: step 329, loss 0.558438, acc 0.84375, prec 0.0317587, recall 0.724902
2017-12-09T22:59:14.673169: step 330, loss 3.20754, acc 0.84375, prec 0.0317243, recall 0.723958
2017-12-09T22:59:14.972231: step 331, loss 0.372698, acc 0.875, prec 0.0316954, recall 0.723958
2017-12-09T22:59:15.266630: step 332, loss 0.454904, acc 0.851562, prec 0.0317162, recall 0.724317
2017-12-09T22:59:15.562677: step 333, loss 1.96394, acc 0.804688, prec 0.0318381, recall 0.72445
2017-12-09T22:59:15.859738: step 334, loss 0.693388, acc 0.78125, prec 0.0319523, recall 0.725515
2017-12-09T22:59:16.158947: step 335, loss 2.41564, acc 0.820312, prec 0.0320222, recall 0.725289
2017-12-09T22:59:16.458053: step 336, loss 0.886033, acc 0.6875, prec 0.0320593, recall 0.725992
2017-12-09T22:59:16.753859: step 337, loss 1.03193, acc 0.710938, prec 0.0322653, recall 0.727735
2017-12-09T22:59:17.047763: step 338, loss 0.755391, acc 0.804688, prec 0.0322744, recall 0.728081
2017-12-09T22:59:17.348244: step 339, loss 0.877438, acc 0.765625, prec 0.0322744, recall 0.728426
2017-12-09T22:59:17.646018: step 340, loss 3.09201, acc 0.75, prec 0.0322725, recall 0.727848
2017-12-09T22:59:17.947891: step 341, loss 0.798263, acc 0.757812, prec 0.0322165, recall 0.727848
2017-12-09T22:59:18.244170: step 342, loss 1.69491, acc 0.679688, prec 0.0323049, recall 0.728878
2017-12-09T22:59:18.546169: step 343, loss 0.862498, acc 0.703125, prec 0.0322904, recall 0.729219
2017-12-09T22:59:18.839987: step 344, loss 1.1153, acc 0.6875, prec 0.0322724, recall 0.72956
2017-12-09T22:59:19.133989: step 345, loss 1.04038, acc 0.695312, prec 0.03231, recall 0.730238
2017-12-09T22:59:19.428594: step 346, loss 3.34756, acc 0.75, prec 0.0323081, recall 0.729662
2017-12-09T22:59:19.730049: step 347, loss 1.38897, acc 0.789062, prec 0.0323669, recall 0.730337
2017-12-09T22:59:20.029566: step 348, loss 1.10089, acc 0.6875, prec 0.0322955, recall 0.730337
2017-12-09T22:59:20.330112: step 349, loss 1.24184, acc 0.679688, prec 0.0324357, recall 0.731677
2017-12-09T22:59:20.636628: step 350, loss 2.91425, acc 0.757812, prec 0.0323822, recall 0.730769
2017-12-09T22:59:20.941551: step 351, loss 0.824819, acc 0.703125, prec 0.0324209, recall 0.731436
2017-12-09T22:59:21.238875: step 352, loss 1.03384, acc 0.679688, prec 0.0323481, recall 0.731436
2017-12-09T22:59:21.539396: step 353, loss 1.38529, acc 0.765625, prec 0.0324537, recall 0.732429
2017-12-09T22:59:21.836621: step 354, loss 1.06701, acc 0.671875, prec 0.0323794, recall 0.732429
2017-12-09T22:59:22.134615: step 355, loss 0.771417, acc 0.71875, prec 0.032316, recall 0.732429
2017-12-09T22:59:22.433201: step 356, loss 1.0266, acc 0.726562, prec 0.0322546, recall 0.732429
2017-12-09T22:59:22.734574: step 357, loss 0.802279, acc 0.820312, prec 0.0323193, recall 0.733087
2017-12-09T22:59:23.034031: step 358, loss 3.25108, acc 0.859375, prec 0.0323944, recall 0.732843
2017-12-09T22:59:23.332935: step 359, loss 0.643961, acc 0.789062, prec 0.0323994, recall 0.73317
2017-12-09T22:59:23.630600: step 360, loss 0.428652, acc 0.84375, prec 0.0324689, recall 0.733822
2017-12-09T22:59:23.930144: step 361, loss 2.01524, acc 0.796875, prec 0.0325295, recall 0.733577
2017-12-09T22:59:24.226081: step 362, loss 0.595716, acc 0.8125, prec 0.0325917, recall 0.734223
2017-12-09T22:59:24.523972: step 363, loss 1.49456, acc 0.765625, prec 0.0325929, recall 0.733656
2017-12-09T22:59:24.820172: step 364, loss 0.761608, acc 0.757812, prec 0.0325387, recall 0.733656
2017-12-09T22:59:25.124505: step 365, loss 1.18603, acc 0.765625, prec 0.0326938, recall 0.73494
2017-12-09T22:59:25.428391: step 366, loss 0.754682, acc 0.773438, prec 0.032643, recall 0.73494
2017-12-09T22:59:25.725365: step 367, loss 0.622708, acc 0.820312, prec 0.0326546, recall 0.735259
2017-12-09T22:59:26.019126: step 368, loss 0.653242, acc 0.757812, prec 0.0326522, recall 0.735577
2017-12-09T22:59:26.319307: step 369, loss 0.585068, acc 0.789062, prec 0.0326568, recall 0.735894
2017-12-09T22:59:26.617388: step 370, loss 0.57182, acc 0.84375, prec 0.0328793, recall 0.73747
2017-12-09T22:59:26.914485: step 371, loss 0.750184, acc 0.726562, prec 0.0328696, recall 0.737783
2017-12-09T22:59:27.215630: step 372, loss 7.42377, acc 0.828125, prec 0.0328895, recall 0.734597
2017-12-09T22:59:27.521663: step 373, loss 3.51221, acc 0.867188, prec 0.0330171, recall 0.733804
2017-12-09T22:59:27.824560: step 374, loss 1.34652, acc 0.710938, prec 0.0330037, recall 0.734118
2017-12-09T22:59:28.123017: step 375, loss 1.16124, acc 0.679688, prec 0.0330853, recall 0.735053
2017-12-09T22:59:28.421062: step 376, loss 1.08882, acc 0.65625, prec 0.0331614, recall 0.735981
2017-12-09T22:59:28.718205: step 377, loss 1.6217, acc 0.53125, prec 0.0331584, recall 0.736597
2017-12-09T22:59:29.018593: step 378, loss 1.28157, acc 0.632812, prec 0.0331781, recall 0.737209
2017-12-09T22:59:29.310667: step 379, loss 1.64812, acc 0.578125, prec 0.033135, recall 0.737514
2017-12-09T22:59:29.608320: step 380, loss 1.72432, acc 0.546875, prec 0.0330351, recall 0.737514
2017-12-09T22:59:29.904669: step 381, loss 2.02735, acc 0.554688, prec 0.0329876, recall 0.737819
2017-12-09T22:59:30.227062: step 382, loss 1.56927, acc 0.585938, prec 0.0330472, recall 0.738728
2017-12-09T22:59:30.529574: step 383, loss 1.76305, acc 0.632812, prec 0.0330668, recall 0.739331
2017-12-09T22:59:30.827413: step 384, loss 1.53631, acc 0.609375, prec 0.0330315, recall 0.739631
2017-12-09T22:59:31.119644: step 385, loss 1.46198, acc 0.671875, prec 0.0331092, recall 0.740528
2017-12-09T22:59:31.417522: step 386, loss 1.93858, acc 0.664062, prec 0.0330875, recall 0.739977
2017-12-09T22:59:31.715812: step 387, loss 2.04531, acc 0.710938, prec 0.0332243, recall 0.740319
2017-12-09T22:59:32.017194: step 388, loss 1.11701, acc 0.671875, prec 0.0332024, recall 0.740614
2017-12-09T22:59:32.322913: step 389, loss 2.11307, acc 0.6875, prec 0.0332349, recall 0.740363
2017-12-09T22:59:32.617072: step 390, loss 0.775014, acc 0.742188, prec 0.0332774, recall 0.74095
2017-12-09T22:59:32.915718: step 391, loss 1.38921, acc 0.742188, prec 0.0334669, recall 0.742407
2017-12-09T22:59:33.214161: step 392, loss 3.47034, acc 0.789062, prec 0.0334228, recall 0.741573
2017-12-09T22:59:33.511588: step 393, loss 0.812296, acc 0.742188, prec 0.0334159, recall 0.741863
2017-12-09T22:59:33.808718: step 394, loss 0.763269, acc 0.75, prec 0.0333619, recall 0.741863
2017-12-09T22:59:34.110875: step 395, loss 2.25598, acc 0.78125, prec 0.0333165, recall 0.741031
2017-12-09T22:59:34.414182: step 396, loss 1.26542, acc 0.804688, prec 0.0333249, recall 0.740492
2017-12-09T22:59:34.715040: step 397, loss 0.820125, acc 0.8125, prec 0.0334305, recall 0.74136
2017-12-09T22:59:35.016573: step 398, loss 1.59216, acc 0.726562, prec 0.0333735, recall 0.740535
2017-12-09T22:59:35.320231: step 399, loss 6.40562, acc 0.757812, prec 0.0333233, recall 0.739711
2017-12-09T22:59:35.629266: step 400, loss 0.901555, acc 0.742188, prec 0.0333167, recall 0.74
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-400

2017-12-09T22:59:37.040402: step 401, loss 1.04995, acc 0.78125, prec 0.0333183, recall 0.740289
2017-12-09T22:59:37.335562: step 402, loss 1.00128, acc 0.695312, prec 0.0333018, recall 0.740577
2017-12-09T22:59:37.636405: step 403, loss 0.991934, acc 0.710938, prec 0.0332885, recall 0.740864
2017-12-09T22:59:37.933165: step 404, loss 1.50283, acc 0.695312, prec 0.0333681, recall 0.741722
2017-12-09T22:59:38.233694: step 405, loss 1.193, acc 0.75, prec 0.0336505, recall 0.743702
2017-12-09T22:59:38.532962: step 406, loss 0.945363, acc 0.710938, prec 0.0336845, recall 0.744262
2017-12-09T22:59:38.833855: step 407, loss 0.925921, acc 0.734375, prec 0.0336757, recall 0.744541
2017-12-09T22:59:39.136506: step 408, loss 1.07459, acc 0.75, prec 0.0337655, recall 0.745375
2017-12-09T22:59:39.433047: step 409, loss 1.13213, acc 0.695312, prec 0.0338433, recall 0.746204
2017-12-09T22:59:39.730752: step 410, loss 0.788781, acc 0.710938, prec 0.0338767, recall 0.746753
2017-12-09T22:59:40.026847: step 411, loss 0.778171, acc 0.796875, prec 0.0339282, recall 0.7473
2017-12-09T22:59:40.325635: step 412, loss 3.63389, acc 0.734375, prec 0.033968, recall 0.74704
2017-12-09T22:59:40.625741: step 413, loss 0.469681, acc 0.828125, prec 0.0339787, recall 0.747312
2017-12-09T22:59:40.924246: step 414, loss 0.582907, acc 0.828125, prec 0.0340837, recall 0.748124
2017-12-09T22:59:41.220591: step 415, loss 0.590844, acc 0.8125, prec 0.0340438, recall 0.748124
2017-12-09T22:59:41.515455: step 416, loss 1.90177, acc 0.867188, prec 0.0342055, recall 0.748401
2017-12-09T22:59:41.813831: step 417, loss 7.10009, acc 0.804688, prec 0.0341656, recall 0.747604
2017-12-09T22:59:42.112232: step 418, loss 0.561442, acc 0.859375, prec 0.0341826, recall 0.747872
2017-12-09T22:59:42.410630: step 419, loss 1.77757, acc 0.835938, prec 0.0341963, recall 0.747346
2017-12-09T22:59:42.706495: step 420, loss 5.9672, acc 0.789062, prec 0.0342938, recall 0.747357
2017-12-09T22:59:43.010720: step 421, loss 3.99842, acc 0.773438, prec 0.0343876, recall 0.747368
2017-12-09T22:59:43.308756: step 422, loss 0.908566, acc 0.710938, prec 0.0344194, recall 0.747899
2017-12-09T22:59:43.605518: step 423, loss 1.07944, acc 0.6875, prec 0.0344927, recall 0.748691
2017-12-09T22:59:43.906117: step 424, loss 1.27171, acc 0.65625, prec 0.0345591, recall 0.749478
2017-12-09T22:59:44.204438: step 425, loss 0.960204, acc 0.710938, prec 0.0344977, recall 0.749478
2017-12-09T22:59:44.499269: step 426, loss 1.11978, acc 0.671875, prec 0.0344282, recall 0.749478
2017-12-09T22:59:44.796450: step 427, loss 0.986552, acc 0.671875, prec 0.034359, recall 0.749478
2017-12-09T22:59:45.093023: step 428, loss 1.39039, acc 0.648438, prec 0.0343313, recall 0.749739
2017-12-09T22:59:45.389580: step 429, loss 0.910753, acc 0.703125, prec 0.0342691, recall 0.749739
2017-12-09T22:59:45.691183: step 430, loss 1.01764, acc 0.679688, prec 0.0342941, recall 0.75026
2017-12-09T22:59:45.992134: step 431, loss 0.84527, acc 0.71875, prec 0.0343272, recall 0.750779
2017-12-09T22:59:46.290786: step 432, loss 0.72012, acc 0.765625, prec 0.0343242, recall 0.751037
2017-12-09T22:59:46.588848: step 433, loss 0.587021, acc 0.804688, prec 0.0342835, recall 0.751037
2017-12-09T22:59:46.888379: step 434, loss 0.540305, acc 0.820312, prec 0.0342463, recall 0.751037
2017-12-09T22:59:47.187273: step 435, loss 0.447936, acc 0.835938, prec 0.0342123, recall 0.751037
2017-12-09T22:59:47.482092: step 436, loss 1.24871, acc 0.851562, prec 0.0342288, recall 0.750518
2017-12-09T22:59:47.785658: step 437, loss 0.517739, acc 0.859375, prec 0.0342908, recall 0.751033
2017-12-09T22:59:48.080202: step 438, loss 6.97701, acc 0.859375, prec 0.034265, recall 0.749485
2017-12-09T22:59:48.385910: step 439, loss 4.17542, acc 0.90625, prec 0.0342927, recall 0.748971
2017-12-09T22:59:48.692504: step 440, loss 0.450878, acc 0.914062, prec 0.0343659, recall 0.749487
2017-12-09T22:59:48.993693: step 441, loss 0.464107, acc 0.828125, prec 0.0343303, recall 0.749487
2017-12-09T22:59:49.293736: step 442, loss 3.8962, acc 0.875, prec 0.0343077, recall 0.747951
2017-12-09T22:59:49.592501: step 443, loss 0.493416, acc 0.828125, prec 0.0343176, recall 0.748209
2017-12-09T22:59:49.889387: step 444, loss 1.43872, acc 0.789062, prec 0.0343211, recall 0.747702
2017-12-09T22:59:50.189371: step 445, loss 0.829538, acc 0.75, prec 0.0343149, recall 0.747959
2017-12-09T22:59:50.498031: step 446, loss 1.24632, acc 0.789062, prec 0.0343635, recall 0.747711
2017-12-09T22:59:50.795997: step 447, loss 0.93935, acc 0.789062, prec 0.0343201, recall 0.747711
2017-12-09T22:59:51.091697: step 448, loss 1.07341, acc 0.65625, prec 0.0343398, recall 0.748223
2017-12-09T22:59:51.386791: step 449, loss 0.998491, acc 0.734375, prec 0.0343304, recall 0.748479
2017-12-09T22:59:51.683973: step 450, loss 2.16165, acc 0.640625, prec 0.0344379, recall 0.748739
2017-12-09T22:59:51.981330: step 451, loss 1.25974, acc 0.664062, prec 0.0343694, recall 0.748739
2017-12-09T22:59:52.280643: step 452, loss 1.25032, acc 0.632812, prec 0.034384, recall 0.749245
2017-12-09T22:59:52.585606: step 453, loss 1.67826, acc 0.664062, prec 0.0344048, recall 0.749749
2017-12-09T22:59:52.881249: step 454, loss 1.80137, acc 0.609375, prec 0.034459, recall 0.750501
2017-12-09T22:59:53.175994: step 455, loss 2.22118, acc 0.679688, prec 0.0344843, recall 0.75025
2017-12-09T22:59:53.472543: step 456, loss 1.0889, acc 0.703125, prec 0.0344685, recall 0.750499
2017-12-09T22:59:53.771203: step 457, loss 1.30634, acc 0.695312, prec 0.0346279, recall 0.751738
2017-12-09T22:59:54.079232: step 458, loss 1.16986, acc 0.664062, prec 0.0345599, recall 0.751738
2017-12-09T22:59:54.377154: step 459, loss 1.10172, acc 0.71875, prec 0.0345912, recall 0.75223
2017-12-09T22:59:54.673808: step 460, loss 0.802098, acc 0.742188, prec 0.034671, recall 0.752964
2017-12-09T22:59:54.972975: step 461, loss 1.37972, acc 0.695312, prec 0.0346973, recall 0.753452
2017-12-09T22:59:55.271475: step 462, loss 2.72628, acc 0.742188, prec 0.0347783, recall 0.753438
2017-12-09T22:59:55.574438: step 463, loss 0.732694, acc 0.804688, prec 0.0347389, recall 0.753438
2017-12-09T22:59:55.875646: step 464, loss 0.952464, acc 0.789062, prec 0.0347401, recall 0.75368
2017-12-09T22:59:56.170660: step 465, loss 2.63756, acc 0.796875, prec 0.0347881, recall 0.753425
2017-12-09T22:59:56.471151: step 466, loss 0.953935, acc 0.710938, prec 0.0348171, recall 0.753906
2017-12-09T22:59:56.772174: step 467, loss 1.44858, acc 0.789062, prec 0.0348198, recall 0.753411
2017-12-09T22:59:57.074741: step 468, loss 1.00909, acc 0.710938, prec 0.0348487, recall 0.753891
2017-12-09T22:59:57.371163: step 469, loss 0.79481, acc 0.835938, prec 0.0348592, recall 0.75413
2017-12-09T22:59:57.669384: step 470, loss 0.937781, acc 0.742188, prec 0.0349374, recall 0.754845
2017-12-09T22:59:57.966752: step 471, loss 0.823717, acc 0.796875, prec 0.0350264, recall 0.755556
2017-12-09T22:59:58.267034: step 472, loss 0.829668, acc 0.773438, prec 0.0350242, recall 0.755791
2017-12-09T22:59:58.564272: step 473, loss 1.24543, acc 0.796875, prec 0.0350281, recall 0.755299
2017-12-09T22:59:58.865710: step 474, loss 0.572965, acc 0.828125, prec 0.0349938, recall 0.755299
2017-12-09T22:59:59.163729: step 475, loss 1.24903, acc 0.796875, prec 0.0351253, recall 0.756238
2017-12-09T22:59:59.460754: step 476, loss 8.5241, acc 0.773438, prec 0.0352533, recall 0.756447
2017-12-09T22:59:59.764373: step 477, loss 3.41536, acc 0.78125, prec 0.0352125, recall 0.755005
2017-12-09T23:00:00.075091: step 478, loss 0.886394, acc 0.773438, prec 0.0351672, recall 0.755005
2017-12-09T23:00:00.380078: step 479, loss 3.25048, acc 0.703125, prec 0.0351095, recall 0.754286
2017-12-09T23:00:00.680688: step 480, loss 1.23261, acc 0.671875, prec 0.0350869, recall 0.75452
2017-12-09T23:00:00.978498: step 481, loss 0.925758, acc 0.710938, prec 0.0350296, recall 0.75452
2017-12-09T23:00:01.272881: step 482, loss 1.27258, acc 0.679688, prec 0.0350514, recall 0.754986
2017-12-09T23:00:01.574409: step 483, loss 2.15023, acc 0.570312, prec 0.0350106, recall 0.754502
2017-12-09T23:00:01.873477: step 484, loss 1.08636, acc 0.664062, prec 0.0350715, recall 0.755198
2017-12-09T23:00:02.173907: step 485, loss 1.34459, acc 0.671875, prec 0.035007, recall 0.755198
2017-12-09T23:00:02.470317: step 486, loss 1.38253, acc 0.6875, prec 0.0350724, recall 0.755891
2017-12-09T23:00:02.769991: step 487, loss 1.07831, acc 0.664062, prec 0.0350908, recall 0.75635
2017-12-09T23:00:03.066758: step 488, loss 2.3551, acc 0.59375, prec 0.0350969, recall 0.756098
2017-12-09T23:00:03.364016: step 489, loss 1.06406, acc 0.734375, prec 0.0351289, recall 0.756554
2017-12-09T23:00:03.660584: step 490, loss 1.88408, acc 0.6875, prec 0.0351532, recall 0.756303
2017-12-09T23:00:03.962727: step 491, loss 0.888983, acc 0.6875, prec 0.0351341, recall 0.75653
2017-12-09T23:00:04.257772: step 492, loss 0.773521, acc 0.742188, prec 0.0350839, recall 0.75653
2017-12-09T23:00:04.553200: step 493, loss 0.869851, acc 0.726562, prec 0.0351976, recall 0.757435
2017-12-09T23:00:04.852738: step 494, loss 1.27872, acc 0.671875, prec 0.0351754, recall 0.75766
2017-12-09T23:00:05.148329: step 495, loss 0.909769, acc 0.710938, prec 0.0351194, recall 0.75766
2017-12-09T23:00:05.451496: step 496, loss 0.865312, acc 0.726562, prec 0.0351495, recall 0.758109
2017-12-09T23:00:05.751281: step 497, loss 2.2725, acc 0.828125, prec 0.0351593, recall 0.757632
2017-12-09T23:00:05.930312: step 498, loss 0.708858, acc 0.846154, prec 0.0351472, recall 0.757632
2017-12-09T23:00:06.232772: step 499, loss 1.19122, acc 0.820312, prec 0.0351968, recall 0.75738
2017-12-09T23:00:06.531907: step 500, loss 0.672011, acc 0.8125, prec 0.0352432, recall 0.757827
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-500

2017-12-09T23:00:07.938381: step 501, loss 0.713607, acc 0.875, prec 0.0353017, recall 0.758272
2017-12-09T23:00:08.239257: step 502, loss 1.57069, acc 0.828125, prec 0.03527, recall 0.757576
2017-12-09T23:00:08.535658: step 503, loss 0.416482, acc 0.859375, prec 0.0352429, recall 0.757576
2017-12-09T23:00:08.834980: step 504, loss 1.34525, acc 0.859375, prec 0.035382, recall 0.75777
2017-12-09T23:00:09.134983: step 505, loss 0.865527, acc 0.828125, prec 0.0353899, recall 0.757991
2017-12-09T23:00:09.436342: step 506, loss 0.644005, acc 0.828125, prec 0.0354389, recall 0.758432
2017-12-09T23:00:09.736726: step 507, loss 0.433606, acc 0.789062, prec 0.0354393, recall 0.758652
2017-12-09T23:00:10.033281: step 508, loss 1.6941, acc 0.804688, prec 0.0354441, recall 0.758182
2017-12-09T23:00:10.335269: step 509, loss 2.02127, acc 0.90625, prec 0.0354685, recall 0.757713
2017-12-09T23:00:10.640736: step 510, loss 0.465765, acc 0.84375, prec 0.0354794, recall 0.757933
2017-12-09T23:00:10.937537: step 511, loss 0.553924, acc 0.835938, prec 0.0354887, recall 0.758152
2017-12-09T23:00:11.238017: step 512, loss 0.388928, acc 0.859375, prec 0.0355433, recall 0.75859
2017-12-09T23:00:11.535368: step 513, loss 0.641825, acc 0.875, prec 0.0356417, recall 0.759243
2017-12-09T23:00:11.833046: step 514, loss 0.631477, acc 0.796875, prec 0.0356841, recall 0.759676
2017-12-09T23:00:12.130511: step 515, loss 0.548933, acc 0.835938, prec 0.0357339, recall 0.760108
2017-12-09T23:00:12.433232: step 516, loss 0.458584, acc 0.828125, prec 0.0357414, recall 0.760323
2017-12-09T23:00:12.727650: step 517, loss 0.553075, acc 0.851562, prec 0.0357941, recall 0.760753
2017-12-09T23:00:13.028397: step 518, loss 0.635526, acc 0.859375, prec 0.0358482, recall 0.761181
2017-12-09T23:00:13.328414: step 519, loss 0.61863, acc 0.8125, prec 0.0358526, recall 0.761394
2017-12-09T23:00:13.633071: step 520, loss 0.446477, acc 0.859375, prec 0.0358254, recall 0.761394
2017-12-09T23:00:13.934705: step 521, loss 0.411498, acc 0.914062, prec 0.0359304, recall 0.762032
2017-12-09T23:00:14.232537: step 522, loss 2.97973, acc 0.851562, prec 0.0359842, recall 0.761778
2017-12-09T23:00:14.537950: step 523, loss 0.575758, acc 0.851562, prec 0.0360364, recall 0.762201
2017-12-09T23:00:14.836887: step 524, loss 0.545307, acc 0.867188, prec 0.0360511, recall 0.762411
2017-12-09T23:00:15.137280: step 525, loss 0.435367, acc 0.867188, prec 0.0361062, recall 0.762832
2017-12-09T23:00:15.441552: step 526, loss 0.62955, acc 0.867188, prec 0.0361209, recall 0.763042
2017-12-09T23:00:15.735912: step 527, loss 0.616953, acc 0.90625, prec 0.0361431, recall 0.763251
2017-12-09T23:00:16.037927: step 528, loss 2.04251, acc 0.789062, prec 0.0362246, recall 0.763204
2017-12-09T23:00:16.339010: step 529, loss 0.482433, acc 0.882812, prec 0.0362422, recall 0.763412
2017-12-09T23:00:16.633634: step 530, loss 1.11668, acc 0.898438, prec 0.0363432, recall 0.764035
2017-12-09T23:00:16.931203: step 531, loss 1.80034, acc 0.828125, prec 0.0363917, recall 0.76378
2017-12-09T23:00:17.230889: step 532, loss 0.638558, acc 0.789062, prec 0.0363508, recall 0.76378
2017-12-09T23:00:17.532964: step 533, loss 0.903944, acc 0.765625, prec 0.0364657, recall 0.764603
2017-12-09T23:00:17.833928: step 534, loss 0.498327, acc 0.84375, prec 0.0365155, recall 0.765013
2017-12-09T23:00:18.130632: step 535, loss 0.629448, acc 0.773438, prec 0.0365914, recall 0.765625
2017-12-09T23:00:18.427787: step 536, loss 0.690942, acc 0.789062, prec 0.0365904, recall 0.765828
2017-12-09T23:00:18.730488: step 537, loss 1.01298, acc 0.75, prec 0.0365818, recall 0.766031
2017-12-09T23:00:19.030294: step 538, loss 0.870075, acc 0.710938, prec 0.0366055, recall 0.766436
2017-12-09T23:00:19.333121: step 539, loss 0.947948, acc 0.75, prec 0.0367162, recall 0.767241
2017-12-09T23:00:19.635880: step 540, loss 0.675236, acc 0.765625, prec 0.0366708, recall 0.767241
2017-12-09T23:00:19.935540: step 541, loss 0.65603, acc 0.78125, prec 0.0367078, recall 0.767642
2017-12-09T23:00:20.233305: step 542, loss 0.687441, acc 0.75, prec 0.0368179, recall 0.768439
2017-12-09T23:00:20.542672: step 543, loss 0.292199, acc 0.867188, prec 0.0368317, recall 0.768638
2017-12-09T23:00:20.837521: step 544, loss 0.416104, acc 0.859375, prec 0.0368836, recall 0.769033
2017-12-09T23:00:21.137475: step 545, loss 2.13493, acc 0.828125, prec 0.0368913, recall 0.768574
2017-12-09T23:00:21.445165: step 546, loss 0.712657, acc 0.867188, prec 0.0369445, recall 0.768968
2017-12-09T23:00:21.746091: step 547, loss 1.82504, acc 0.851562, prec 0.0371128, recall 0.769949
2017-12-09T23:00:22.049463: step 548, loss 0.386502, acc 0.867188, prec 0.037087, recall 0.769949
2017-12-09T23:00:22.346614: step 549, loss 0.601285, acc 0.851562, prec 0.0371762, recall 0.770533
2017-12-09T23:00:22.654255: step 550, loss 0.561927, acc 0.8125, prec 0.0371398, recall 0.770533
2017-12-09T23:00:22.952410: step 551, loss 0.503038, acc 0.84375, prec 0.0371881, recall 0.770921
2017-12-09T23:00:23.250898: step 552, loss 0.546557, acc 0.84375, prec 0.0371578, recall 0.770921
2017-12-09T23:00:23.545937: step 553, loss 1.33799, acc 0.820312, prec 0.0371637, recall 0.770464
2017-12-09T23:00:23.847415: step 554, loss 0.797869, acc 0.875, prec 0.0373353, recall 0.771429
2017-12-09T23:00:24.148013: step 555, loss 3.66091, acc 0.804688, prec 0.0374162, recall 0.771357
2017-12-09T23:00:24.446915: step 556, loss 0.571801, acc 0.796875, prec 0.0374158, recall 0.771548
2017-12-09T23:00:24.745250: step 557, loss 0.702336, acc 0.789062, prec 0.0374139, recall 0.771739
2017-12-09T23:00:25.047619: step 558, loss 0.595935, acc 0.804688, prec 0.037415, recall 0.77193
2017-12-09T23:00:25.350853: step 559, loss 0.748471, acc 0.789062, prec 0.037452, recall 0.77231
2017-12-09T23:00:25.648325: step 560, loss 0.622935, acc 0.789062, prec 0.03745, recall 0.7725
2017-12-09T23:00:25.946978: step 561, loss 0.680516, acc 0.773438, prec 0.037445, recall 0.772689
2017-12-09T23:00:26.245681: step 562, loss 0.507442, acc 0.789062, prec 0.0374043, recall 0.772689
2017-12-09T23:00:26.548536: step 563, loss 0.649569, acc 0.789062, prec 0.0374024, recall 0.772879
2017-12-09T23:00:26.845842: step 564, loss 0.636673, acc 0.859375, prec 0.0375302, recall 0.773632
2017-12-09T23:00:27.145155: step 565, loss 1.51238, acc 0.859375, prec 0.0375819, recall 0.773366
2017-12-09T23:00:27.440459: step 566, loss 0.725577, acc 0.859375, prec 0.0375934, recall 0.773554
2017-12-09T23:00:27.742716: step 567, loss 0.669565, acc 0.8125, prec 0.0376344, recall 0.773927
2017-12-09T23:00:28.041627: step 568, loss 0.670704, acc 0.8125, prec 0.0376368, recall 0.774114
2017-12-09T23:00:28.341518: step 569, loss 0.698118, acc 0.835938, prec 0.0376437, recall 0.7743
2017-12-09T23:00:28.640962: step 570, loss 0.426169, acc 0.851562, prec 0.037615, recall 0.7743
2017-12-09T23:00:28.945588: step 571, loss 0.605029, acc 0.789062, prec 0.0376514, recall 0.774671
2017-12-09T23:00:29.246048: step 572, loss 1.41663, acc 0.859375, prec 0.0378165, recall 0.775594
2017-12-09T23:00:29.550764: step 573, loss 0.729131, acc 0.796875, prec 0.0377772, recall 0.775594
2017-12-09T23:00:29.853071: step 574, loss 1.14535, acc 0.84375, prec 0.0378622, recall 0.776144
2017-12-09T23:00:30.156590: step 575, loss 0.457446, acc 0.859375, prec 0.0379117, recall 0.776509
2017-12-09T23:00:30.461298: step 576, loss 0.695988, acc 0.835938, prec 0.0379566, recall 0.776873
2017-12-09T23:00:30.762156: step 577, loss 0.517404, acc 0.851562, prec 0.0380044, recall 0.777236
2017-12-09T23:00:31.062077: step 578, loss 1.95698, acc 0.828125, prec 0.0380888, recall 0.776518
2017-12-09T23:00:31.364821: step 579, loss 0.649234, acc 0.804688, prec 0.0380892, recall 0.776699
2017-12-09T23:00:31.665441: step 580, loss 0.701016, acc 0.828125, prec 0.0381322, recall 0.77706
2017-12-09T23:00:31.965576: step 581, loss 0.498954, acc 0.804688, prec 0.0381326, recall 0.77724
2017-12-09T23:00:32.263320: step 582, loss 0.500185, acc 0.84375, prec 0.0381405, recall 0.777419
2017-12-09T23:00:32.567010: step 583, loss 0.621109, acc 0.789062, prec 0.0380998, recall 0.777419
2017-12-09T23:00:32.866872: step 584, loss 0.585947, acc 0.796875, prec 0.0380986, recall 0.777599
2017-12-09T23:00:33.165062: step 585, loss 0.525309, acc 0.828125, prec 0.0381794, recall 0.778135
2017-12-09T23:00:33.464540: step 586, loss 1.85295, acc 0.757812, prec 0.0381721, recall 0.777689
2017-12-09T23:00:33.764487: step 587, loss 1.85171, acc 0.78125, prec 0.0381694, recall 0.777244
2017-12-09T23:00:34.059762: step 588, loss 0.739405, acc 0.734375, prec 0.0381184, recall 0.777244
2017-12-09T23:00:34.355064: step 589, loss 0.669149, acc 0.789062, prec 0.0381536, recall 0.7776
2017-12-09T23:00:34.656435: step 590, loss 1.30757, acc 0.742188, prec 0.0381796, recall 0.777955
2017-12-09T23:00:34.955886: step 591, loss 0.692479, acc 0.820312, prec 0.0382959, recall 0.778662
2017-12-09T23:00:35.255609: step 592, loss 0.776328, acc 0.734375, prec 0.0382826, recall 0.778839
2017-12-09T23:00:35.562595: step 593, loss 1.01497, acc 0.757812, prec 0.0383489, recall 0.779365
2017-12-09T23:00:35.857134: step 594, loss 3.41797, acc 0.773438, prec 0.038346, recall 0.778306
2017-12-09T23:00:36.160729: step 595, loss 0.660347, acc 0.8125, prec 0.0383851, recall 0.778656
2017-12-09T23:00:36.457268: step 596, loss 0.73552, acc 0.78125, prec 0.0383807, recall 0.778831
2017-12-09T23:00:36.755022: step 597, loss 0.911241, acc 0.75, prec 0.0383703, recall 0.779006
2017-12-09T23:00:37.057853: step 598, loss 0.792596, acc 0.75, prec 0.03836, recall 0.77918
2017-12-09T23:00:37.355624: step 599, loss 0.701472, acc 0.75, prec 0.0383497, recall 0.779354
2017-12-09T23:00:37.656683: step 600, loss 0.820244, acc 0.789062, prec 0.0383841, recall 0.779701

Evaluation:
2017-12-09T23:00:42.402946: step 600, loss 1.51685, acc 0.82308, prec 0.0397969, recall 0.774895

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-600

2017-12-09T23:00:43.725482: step 601, loss 0.595183, acc 0.789062, prec 0.0397582, recall 0.774895
2017-12-09T23:00:44.026118: step 602, loss 0.665746, acc 0.820312, prec 0.0397944, recall 0.77521
2017-12-09T23:00:44.322767: step 603, loss 0.929207, acc 0.859375, prec 0.0398031, recall 0.775367
2017-12-09T23:00:44.620450: step 604, loss 0.582041, acc 0.859375, prec 0.0399153, recall 0.775994
2017-12-09T23:00:44.922637: step 605, loss 0.369019, acc 0.914062, prec 0.0400373, recall 0.776618
2017-12-09T23:00:45.220968: step 606, loss 0.346555, acc 0.921875, prec 0.0400918, recall 0.776928
2017-12-09T23:00:45.524006: step 607, loss 0.574274, acc 0.875, prec 0.0401376, recall 0.777238
2017-12-09T23:00:45.822370: step 608, loss 0.438059, acc 0.851562, prec 0.0401791, recall 0.777547
2017-12-09T23:00:46.122958: step 609, loss 0.292619, acc 0.9375, prec 0.0402019, recall 0.777701
2017-12-09T23:00:46.421092: step 610, loss 1.00339, acc 0.882812, prec 0.0401818, recall 0.777163
2017-12-09T23:00:46.719244: step 611, loss 1.83349, acc 0.875, prec 0.0401602, recall 0.776625
2017-12-09T23:00:47.019597: step 612, loss 2.26153, acc 0.9375, prec 0.0402188, recall 0.776398
2017-12-09T23:00:47.319602: step 613, loss 2.69032, acc 0.84375, prec 0.0401915, recall 0.775862
2017-12-09T23:00:47.616819: step 614, loss 0.54918, acc 0.835938, prec 0.0402299, recall 0.776171
2017-12-09T23:00:47.910831: step 615, loss 0.420081, acc 0.828125, prec 0.0401983, recall 0.776171
2017-12-09T23:00:48.209114: step 616, loss 0.559311, acc 0.820312, prec 0.0402338, recall 0.776479
2017-12-09T23:00:48.509710: step 617, loss 0.510405, acc 0.820312, prec 0.0402692, recall 0.776786
2017-12-09T23:00:48.807857: step 618, loss 0.721557, acc 0.734375, prec 0.0402887, recall 0.777092
2017-12-09T23:00:49.102767: step 619, loss 1.47744, acc 0.757812, prec 0.0403126, recall 0.777397
2017-12-09T23:00:49.411682: step 620, loss 0.522882, acc 0.804688, prec 0.0403449, recall 0.777702
2017-12-09T23:00:49.707753: step 621, loss 0.593084, acc 0.804688, prec 0.0403772, recall 0.778005
2017-12-09T23:00:50.009287: step 622, loss 0.6173, acc 0.78125, prec 0.0404051, recall 0.778308
2017-12-09T23:00:50.314565: step 623, loss 1.46732, acc 0.71875, prec 0.0404555, recall 0.778761
2017-12-09T23:00:50.619754: step 624, loss 0.796017, acc 0.757812, prec 0.040479, recall 0.779062
2017-12-09T23:00:50.913252: step 625, loss 0.596172, acc 0.835938, prec 0.0405167, recall 0.779362
2017-12-09T23:00:51.207049: step 626, loss 1.17209, acc 0.804688, prec 0.0405162, recall 0.778983
2017-12-09T23:00:51.503925: step 627, loss 0.65365, acc 0.796875, prec 0.0405129, recall 0.779133
2017-12-09T23:00:51.806520: step 628, loss 0.386926, acc 0.859375, prec 0.0405548, recall 0.779432
2017-12-09T23:00:52.109478: step 629, loss 0.443317, acc 0.851562, prec 0.0405952, recall 0.77973
2017-12-09T23:00:52.408904: step 630, loss 0.551555, acc 0.828125, prec 0.0405975, recall 0.779878
2017-12-09T23:00:52.705978: step 631, loss 0.476772, acc 0.867188, prec 0.0406744, recall 0.780323
2017-12-09T23:00:53.004938: step 632, loss 0.453261, acc 0.859375, prec 0.0407497, recall 0.780767
2017-12-09T23:00:53.300229: step 633, loss 0.572442, acc 0.773438, prec 0.0407083, recall 0.780767
2017-12-09T23:00:53.598287: step 634, loss 2.49124, acc 0.929688, prec 0.0407641, recall 0.780537
2017-12-09T23:00:53.898721: step 635, loss 1.34731, acc 0.875, prec 0.0407763, recall 0.780161
2017-12-09T23:00:54.200174: step 636, loss 0.434657, acc 0.84375, prec 0.0407813, recall 0.780308
2017-12-09T23:00:54.495833: step 637, loss 0.373408, acc 0.859375, prec 0.0407556, recall 0.780308
2017-12-09T23:00:54.796300: step 638, loss 0.431685, acc 0.890625, prec 0.0407357, recall 0.780308
2017-12-09T23:00:55.094708: step 639, loss 0.299206, acc 0.898438, prec 0.0407507, recall 0.780455
2017-12-09T23:00:55.391468: step 640, loss 0.363443, acc 0.875, prec 0.0407614, recall 0.780602
2017-12-09T23:00:55.701594: step 641, loss 2.71447, acc 0.867188, prec 0.0408056, recall 0.780374
2017-12-09T23:00:56.005476: step 642, loss 1.34311, acc 0.875, prec 0.0408177, recall 0.78
2017-12-09T23:00:56.302855: step 643, loss 0.591788, acc 0.828125, prec 0.0408533, recall 0.780293
2017-12-09T23:00:56.601254: step 644, loss 2.12498, acc 0.890625, prec 0.0408682, recall 0.77992
2017-12-09T23:00:56.896956: step 645, loss 0.647964, acc 0.84375, prec 0.0408732, recall 0.780066
2017-12-09T23:00:57.193659: step 646, loss 0.643521, acc 0.820312, prec 0.0409072, recall 0.780358
2017-12-09T23:00:57.490976: step 647, loss 0.622949, acc 0.804688, prec 0.0408716, recall 0.780358
2017-12-09T23:00:57.785380: step 648, loss 0.657531, acc 0.773438, prec 0.0408971, recall 0.780649
2017-12-09T23:00:58.082587: step 649, loss 1.48451, acc 0.757812, prec 0.0409211, recall 0.780423
2017-12-09T23:00:58.381746: step 650, loss 0.778525, acc 0.742188, prec 0.0409075, recall 0.780568
2017-12-09T23:00:58.679113: step 651, loss 0.833177, acc 0.71875, prec 0.0408566, recall 0.780568
2017-12-09T23:00:58.974614: step 652, loss 0.985704, acc 0.71875, prec 0.0409052, recall 0.781003
2017-12-09T23:00:59.270040: step 653, loss 0.654846, acc 0.734375, prec 0.0409234, recall 0.781291
2017-12-09T23:00:59.563650: step 654, loss 0.512718, acc 0.796875, prec 0.0408867, recall 0.781291
2017-12-09T23:00:59.858141: step 655, loss 1.37727, acc 0.773438, prec 0.0408803, recall 0.780921
2017-12-09T23:01:00.163457: step 656, loss 0.558656, acc 0.789062, prec 0.0408753, recall 0.781065
2017-12-09T23:01:00.465123: step 657, loss 0.655456, acc 0.8125, prec 0.0409735, recall 0.781639
2017-12-09T23:01:00.766200: step 658, loss 0.564692, acc 0.851562, prec 0.0410455, recall 0.782068
2017-12-09T23:01:01.064451: step 659, loss 0.686111, acc 0.859375, prec 0.0411189, recall 0.782495
2017-12-09T23:01:01.357291: step 660, loss 0.971211, acc 0.859375, prec 0.041258, recall 0.783203
2017-12-09T23:01:01.650606: step 661, loss 0.308953, acc 0.875, prec 0.0412682, recall 0.783344
2017-12-09T23:01:01.959892: step 662, loss 0.470479, acc 0.828125, prec 0.04127, recall 0.783485
2017-12-09T23:01:02.262438: step 663, loss 0.316898, acc 0.867188, prec 0.0412459, recall 0.783485
2017-12-09T23:01:02.560091: step 664, loss 1.04868, acc 0.84375, prec 0.0412833, recall 0.783766
2017-12-09T23:01:02.861004: step 665, loss 0.869941, acc 0.90625, prec 0.0413975, recall 0.784326
2017-12-09T23:01:03.160753: step 666, loss 4.21133, acc 0.867188, prec 0.0413748, recall 0.783819
2017-12-09T23:01:03.458038: step 667, loss 0.421911, acc 0.867188, prec 0.0413835, recall 0.783959
2017-12-09T23:01:03.753618: step 668, loss 0.499174, acc 0.789062, prec 0.0413454, recall 0.783959
2017-12-09T23:01:04.050705: step 669, loss 0.695758, acc 0.851562, prec 0.0413513, recall 0.784098
2017-12-09T23:01:04.352197: step 670, loss 0.428359, acc 0.851562, prec 0.0413246, recall 0.784098
2017-12-09T23:01:04.644963: step 671, loss 0.33845, acc 0.882812, prec 0.0413687, recall 0.784377
2017-12-09T23:01:04.946529: step 672, loss 0.450353, acc 0.890625, prec 0.0413817, recall 0.784516
2017-12-09T23:01:05.247457: step 673, loss 0.585667, acc 0.875, prec 0.0414569, recall 0.784932
2017-12-09T23:01:05.555317: step 674, loss 0.564669, acc 0.859375, prec 0.0415293, recall 0.785347
2017-12-09T23:01:05.851354: step 675, loss 0.615139, acc 0.835938, prec 0.0415648, recall 0.785623
2017-12-09T23:01:06.150925: step 676, loss 0.340488, acc 0.882812, prec 0.0415436, recall 0.785623
2017-12-09T23:01:06.447084: step 677, loss 1.61802, acc 0.820312, prec 0.0415451, recall 0.785256
2017-12-09T23:01:06.752149: step 678, loss 3.45496, acc 0.84375, prec 0.0416172, recall 0.784665
2017-12-09T23:01:07.058045: step 679, loss 0.864005, acc 0.828125, prec 0.0416836, recall 0.785077
2017-12-09T23:01:07.355131: step 680, loss 0.44774, acc 0.828125, prec 0.041685, recall 0.785214
2017-12-09T23:01:07.654363: step 681, loss 1.49565, acc 0.773438, prec 0.0416779, recall 0.78485
2017-12-09T23:01:07.953687: step 682, loss 0.709344, acc 0.796875, prec 0.0416413, recall 0.78485
2017-12-09T23:01:08.249844: step 683, loss 0.902711, acc 0.804688, prec 0.0416709, recall 0.785124
2017-12-09T23:01:08.546502: step 684, loss 0.8817, acc 0.726562, prec 0.041654, recall 0.78526
2017-12-09T23:01:08.838585: step 685, loss 1.36724, acc 0.742188, prec 0.0417059, recall 0.785171
2017-12-09T23:01:09.137220: step 686, loss 0.973281, acc 0.695312, prec 0.0416835, recall 0.785307
2017-12-09T23:01:09.437364: step 687, loss 0.722997, acc 0.734375, prec 0.0417324, recall 0.785714
2017-12-09T23:01:09.738304: step 688, loss 0.701276, acc 0.78125, prec 0.0416932, recall 0.785714
2017-12-09T23:01:10.038723: step 689, loss 3.28726, acc 0.742188, prec 0.0416485, recall 0.785218
2017-12-09T23:01:10.340476: step 690, loss 0.960609, acc 0.703125, prec 0.0416276, recall 0.785354
2017-12-09T23:01:10.635062: step 691, loss 1.37916, acc 0.679688, prec 0.0416987, recall 0.785894
2017-12-09T23:01:10.933605: step 692, loss 0.775356, acc 0.703125, prec 0.0417417, recall 0.786298
2017-12-09T23:01:11.230283: step 693, loss 0.806305, acc 0.789062, prec 0.0418319, recall 0.786834
2017-12-09T23:01:11.529830: step 694, loss 0.867237, acc 0.75, prec 0.0418831, recall 0.787234
2017-12-09T23:01:11.827975: step 695, loss 0.924065, acc 0.804688, prec 0.0418801, recall 0.787367
2017-12-09T23:01:12.128547: step 696, loss 1.7905, acc 0.773438, prec 0.0418411, recall 0.786875
2017-12-09T23:01:12.426466: step 697, loss 0.622374, acc 0.8125, prec 0.0419032, recall 0.787274
2017-12-09T23:01:12.723399: step 698, loss 0.695872, acc 0.8125, prec 0.0419017, recall 0.787407
2017-12-09T23:01:13.016285: step 699, loss 0.705231, acc 0.765625, prec 0.04186, recall 0.787407
2017-12-09T23:01:13.313999: step 700, loss 1.97062, acc 0.828125, prec 0.0419261, recall 0.787313
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-700

2017-12-09T23:01:14.739049: step 701, loss 0.528495, acc 0.835938, prec 0.0419604, recall 0.787578
2017-12-09T23:01:15.035825: step 702, loss 0.913921, acc 0.859375, prec 0.0419987, recall 0.787841
2017-12-09T23:01:15.337095: step 703, loss 0.527853, acc 0.8125, prec 0.0420604, recall 0.788235
2017-12-09T23:01:15.635468: step 704, loss 0.801216, acc 0.71875, prec 0.0421053, recall 0.788628
2017-12-09T23:01:15.933312: step 705, loss 0.519762, acc 0.78125, prec 0.0420664, recall 0.788628
2017-12-09T23:01:16.229036: step 706, loss 0.476473, acc 0.835938, prec 0.042132, recall 0.789019
2017-12-09T23:01:16.531813: step 707, loss 0.326521, acc 0.835938, prec 0.0421344, recall 0.789149
2017-12-09T23:01:16.824591: step 708, loss 0.544521, acc 0.835938, prec 0.0421053, recall 0.789149
2017-12-09T23:01:17.130608: step 709, loss 1.81627, acc 0.765625, prec 0.0420966, recall 0.788793
2017-12-09T23:01:17.433141: step 710, loss 0.503508, acc 0.835938, prec 0.042099, recall 0.788923
2017-12-09T23:01:17.732450: step 711, loss 0.39862, acc 0.820312, prec 0.0420987, recall 0.789053
2017-12-09T23:01:18.026825: step 712, loss 0.545632, acc 0.890625, prec 0.0422364, recall 0.7897
2017-12-09T23:01:18.332217: step 713, loss 5.33125, acc 0.835938, prec 0.0423043, recall 0.78912
2017-12-09T23:01:18.629024: step 714, loss 0.632326, acc 0.804688, prec 0.042301, recall 0.789249
2017-12-09T23:01:18.924609: step 715, loss 0.541339, acc 0.820312, prec 0.0423005, recall 0.789377
2017-12-09T23:01:19.219363: step 716, loss 0.48626, acc 0.789062, prec 0.0422632, recall 0.789377
2017-12-09T23:01:19.512687: step 717, loss 0.919319, acc 0.804688, prec 0.0422912, recall 0.789634
2017-12-09T23:01:19.812697: step 718, loss 1.13863, acc 0.75, prec 0.0423721, recall 0.790146
2017-12-09T23:01:20.111889: step 719, loss 1.49186, acc 0.78125, prec 0.0423972, recall 0.789921
2017-12-09T23:01:20.419412: step 720, loss 0.86674, acc 0.765625, prec 0.0424181, recall 0.790176
2017-12-09T23:01:20.717727: step 721, loss 1.0734, acc 0.625, prec 0.0423831, recall 0.790303
2017-12-09T23:01:21.015904: step 722, loss 0.951804, acc 0.6875, prec 0.042328, recall 0.790303
2017-12-09T23:01:21.312295: step 723, loss 1.25579, acc 0.640625, prec 0.042327, recall 0.790557
2017-12-09T23:01:21.605437: step 724, loss 0.530199, acc 0.75, prec 0.0422832, recall 0.790557
2017-12-09T23:01:21.901349: step 725, loss 0.880289, acc 0.75, prec 0.0423943, recall 0.791189
2017-12-09T23:01:22.198743: step 726, loss 0.640695, acc 0.765625, prec 0.0423532, recall 0.791189
2017-12-09T23:01:22.493668: step 727, loss 2.15151, acc 0.78125, prec 0.0423163, recall 0.790712
2017-12-09T23:01:22.802160: step 728, loss 0.602996, acc 0.8125, prec 0.0423453, recall 0.790964
2017-12-09T23:01:23.103472: step 729, loss 0.770318, acc 0.789062, prec 0.0424627, recall 0.791592
2017-12-09T23:01:23.400894: step 730, loss 0.405818, acc 0.8125, prec 0.0424916, recall 0.791842
2017-12-09T23:01:23.699252: step 731, loss 0.897125, acc 0.789062, prec 0.042547, recall 0.792216
2017-12-09T23:01:23.996252: step 732, loss 4.65124, acc 0.84375, prec 0.0425518, recall 0.791866
2017-12-09T23:01:24.301070: step 733, loss 0.586483, acc 0.8125, prec 0.042642, recall 0.792363
2017-12-09T23:01:24.603004: step 734, loss 0.321376, acc 0.898438, prec 0.0426857, recall 0.79261
2017-12-09T23:01:24.904217: step 735, loss 4.63814, acc 0.851562, prec 0.0428145, recall 0.792755
2017-12-09T23:01:25.202609: step 736, loss 0.510578, acc 0.835938, prec 0.0427857, recall 0.792755
2017-12-09T23:01:25.499231: step 737, loss 0.874205, acc 0.8125, prec 0.0428448, recall 0.793124
2017-12-09T23:01:25.801299: step 738, loss 1.63603, acc 0.78125, prec 0.0428384, recall 0.792777
2017-12-09T23:01:26.101772: step 739, loss 0.652648, acc 0.773438, prec 0.0427987, recall 0.792777
2017-12-09T23:01:26.400443: step 740, loss 0.678238, acc 0.757812, prec 0.0428175, recall 0.793022
2017-12-09T23:01:26.695499: step 741, loss 0.702957, acc 0.773438, prec 0.0428084, recall 0.793144
2017-12-09T23:01:26.991550: step 742, loss 0.831312, acc 0.75, prec 0.0428562, recall 0.79351
2017-12-09T23:01:27.291882: step 743, loss 0.768073, acc 0.757812, prec 0.0428139, recall 0.79351
2017-12-09T23:01:27.586080: step 744, loss 0.515632, acc 0.789062, prec 0.0428076, recall 0.793632
2017-12-09T23:01:27.882249: step 745, loss 0.528512, acc 0.820312, prec 0.0428372, recall 0.793875
2017-12-09T23:01:28.179056: step 746, loss 0.439849, acc 0.867188, prec 0.042814, recall 0.793875
2017-12-09T23:01:28.357045: step 747, loss 1.88302, acc 0.807692, prec 0.0428308, recall 0.793996
2017-12-09T23:01:28.664479: step 748, loss 0.840952, acc 0.765625, prec 0.0428204, recall 0.794118
2017-12-09T23:01:28.968255: step 749, loss 1.8267, acc 0.796875, prec 0.0428472, recall 0.793893
2017-12-09T23:01:29.264002: step 750, loss 0.449674, acc 0.859375, prec 0.0428834, recall 0.794135
2017-12-09T23:01:29.570261: step 751, loss 0.861884, acc 0.789062, prec 0.0429679, recall 0.794617
2017-12-09T23:01:29.868351: step 752, loss 0.658407, acc 0.78125, prec 0.0429904, recall 0.794857
2017-12-09T23:01:30.172321: step 753, loss 0.45108, acc 0.828125, prec 0.0429605, recall 0.794857
2017-12-09T23:01:30.471947: step 754, loss 0.316741, acc 0.882812, prec 0.0429704, recall 0.794977
2017-12-09T23:01:30.774576: step 755, loss 0.583051, acc 0.84375, prec 0.043064, recall 0.795455
2017-12-09T23:01:31.070383: step 756, loss 0.374872, acc 0.898438, prec 0.0430765, recall 0.795574
2017-12-09T23:01:31.366439: step 757, loss 0.587912, acc 0.8125, prec 0.0431043, recall 0.795812
2017-12-09T23:01:31.662750: step 758, loss 0.504143, acc 0.84375, prec 0.0431072, recall 0.79593
2017-12-09T23:01:31.959530: step 759, loss 0.9938, acc 0.867188, prec 0.0430855, recall 0.795468
2017-12-09T23:01:32.255006: step 760, loss 0.457772, acc 0.84375, prec 0.0430885, recall 0.795587
2017-12-09T23:01:32.559956: step 761, loss 0.306249, acc 0.882812, prec 0.0430983, recall 0.795705
2017-12-09T23:01:32.854570: step 762, loss 0.410543, acc 0.882812, prec 0.0432283, recall 0.796296
2017-12-09T23:01:33.150624: step 763, loss 1.18226, acc 0.90625, prec 0.0432735, recall 0.796072
2017-12-09T23:01:33.450024: step 764, loss 0.440948, acc 0.898438, prec 0.0433159, recall 0.796307
2017-12-09T23:01:33.746786: step 765, loss 0.335372, acc 0.875, prec 0.0433241, recall 0.796424
2017-12-09T23:01:34.042544: step 766, loss 0.424271, acc 0.859375, prec 0.0432997, recall 0.796424
2017-12-09T23:01:34.343022: step 767, loss 0.773564, acc 0.882812, prec 0.0433693, recall 0.796776
2017-12-09T23:01:34.638094: step 768, loss 0.521704, acc 0.875, prec 0.0433775, recall 0.796893
2017-12-09T23:01:34.931444: step 769, loss 0.469637, acc 0.804688, prec 0.0434034, recall 0.797126
2017-12-09T23:01:35.230704: step 770, loss 0.383585, acc 0.851562, prec 0.0433776, recall 0.797126
2017-12-09T23:01:35.544634: step 771, loss 0.588511, acc 0.859375, prec 0.0434728, recall 0.797592
2017-12-09T23:01:35.843695: step 772, loss 0.190223, acc 0.953125, prec 0.0434946, recall 0.797708
2017-12-09T23:01:36.142375: step 773, loss 1.30191, acc 0.851562, prec 0.0435, recall 0.797367
2017-12-09T23:01:36.443584: step 774, loss 0.239553, acc 0.914062, prec 0.0435448, recall 0.797599
2017-12-09T23:01:36.741241: step 775, loss 0.493277, acc 0.898438, prec 0.0436465, recall 0.79806
2017-12-09T23:01:37.039404: step 776, loss 0.234624, acc 0.898438, prec 0.0436586, recall 0.798176
2017-12-09T23:01:37.334806: step 777, loss 0.848879, acc 0.84375, prec 0.0437804, recall 0.798749
2017-12-09T23:01:37.632735: step 778, loss 0.387617, acc 0.84375, prec 0.0438127, recall 0.798978
2017-12-09T23:01:37.925667: step 779, loss 2.11978, acc 0.835938, prec 0.0438747, recall 0.798867
2017-12-09T23:01:38.224052: step 780, loss 0.484741, acc 0.851562, prec 0.0438785, recall 0.798981
2017-12-09T23:01:38.518280: step 781, loss 0.626567, acc 0.859375, prec 0.0439134, recall 0.799208
2017-12-09T23:01:38.815100: step 782, loss 0.543917, acc 0.851562, prec 0.0439765, recall 0.799548
2017-12-09T23:01:39.111245: step 783, loss 0.386321, acc 0.882812, prec 0.0440154, recall 0.799774
2017-12-09T23:01:39.420817: step 784, loss 0.415236, acc 0.859375, prec 0.0440205, recall 0.799887
2017-12-09T23:01:39.724878: step 785, loss 0.327628, acc 0.898438, prec 0.0440324, recall 0.8
2017-12-09T23:01:40.020385: step 786, loss 0.885344, acc 0.757812, prec 0.0440197, recall 0.800113
2017-12-09T23:01:40.320464: step 787, loss 0.411545, acc 0.859375, prec 0.0440248, recall 0.800225
2017-12-09T23:01:40.622394: step 788, loss 0.487275, acc 0.835938, prec 0.0440553, recall 0.80045
2017-12-09T23:01:40.923473: step 789, loss 0.353149, acc 0.851562, prec 0.0440885, recall 0.800674
2017-12-09T23:01:41.219331: step 790, loss 0.353425, acc 0.867188, prec 0.0440949, recall 0.800786
2017-12-09T23:01:41.517224: step 791, loss 0.624754, acc 0.882812, prec 0.0441631, recall 0.80112
2017-12-09T23:01:41.816102: step 792, loss 0.356216, acc 0.875, prec 0.0441413, recall 0.80112
2017-12-09T23:01:42.113210: step 793, loss 0.601325, acc 0.898438, prec 0.0442415, recall 0.801565
2017-12-09T23:01:42.416173: step 794, loss 0.389178, acc 0.90625, prec 0.0442841, recall 0.801787
2017-12-09T23:01:42.715873: step 795, loss 0.178863, acc 0.921875, prec 0.0442704, recall 0.801787
2017-12-09T23:01:43.015293: step 796, loss 1.9424, acc 0.921875, prec 0.0442876, recall 0.80145
2017-12-09T23:01:43.313371: step 797, loss 0.273163, acc 0.90625, prec 0.0442712, recall 0.80145
2017-12-09T23:01:43.613408: step 798, loss 0.202975, acc 0.898438, prec 0.0442535, recall 0.80145
2017-12-09T23:01:43.911043: step 799, loss 0.379189, acc 0.875, prec 0.0442611, recall 0.801561
2017-12-09T23:01:44.209455: step 800, loss 0.285337, acc 0.914062, prec 0.0442756, recall 0.801671
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-800

2017-12-09T23:01:45.521981: step 801, loss 0.342247, acc 0.90625, prec 0.0442886, recall 0.801782
2017-12-09T23:01:45.820217: step 802, loss 0.350506, acc 0.945312, prec 0.0443085, recall 0.801892
2017-12-09T23:01:46.118805: step 803, loss 0.179328, acc 0.921875, prec 0.0443242, recall 0.802002
2017-12-09T23:01:46.415903: step 804, loss 0.234227, acc 0.914062, prec 0.0443386, recall 0.802112
2017-12-09T23:01:46.713800: step 805, loss 0.162391, acc 0.929688, prec 0.0443264, recall 0.802112
2017-12-09T23:01:47.012558: step 806, loss 0.275347, acc 0.90625, prec 0.0443394, recall 0.802222
2017-12-09T23:01:47.312164: step 807, loss 0.832, acc 0.898438, prec 0.0443524, recall 0.801887
2017-12-09T23:01:47.611344: step 808, loss 1.27851, acc 0.9375, prec 0.0444601, recall 0.801882
2017-12-09T23:01:47.915153: step 809, loss 1.16148, acc 0.898438, prec 0.0444731, recall 0.801548
2017-12-09T23:01:48.212744: step 810, loss 0.269378, acc 0.90625, prec 0.0445153, recall 0.801767
2017-12-09T23:01:48.512387: step 811, loss 1.92186, acc 0.882812, prec 0.044584, recall 0.801653
2017-12-09T23:01:48.813987: step 812, loss 0.491262, acc 0.9375, prec 0.0446316, recall 0.801871
2017-12-09T23:01:49.111944: step 813, loss 0.472035, acc 0.84375, prec 0.0446921, recall 0.802198
2017-12-09T23:01:49.405077: step 814, loss 0.51433, acc 0.882812, prec 0.0447008, recall 0.802306
2017-12-09T23:01:49.706025: step 815, loss 0.664994, acc 0.851562, prec 0.044704, recall 0.802415
2017-12-09T23:01:50.001535: step 816, loss 0.423292, acc 0.84375, prec 0.0447351, recall 0.802632
2017-12-09T23:01:50.299623: step 817, loss 0.770028, acc 0.804688, prec 0.0447884, recall 0.802956
2017-12-09T23:01:50.606300: step 818, loss 0.820676, acc 0.804688, prec 0.0448125, recall 0.803171
2017-12-09T23:01:50.900089: step 819, loss 0.783638, acc 0.757812, prec 0.0448284, recall 0.803386
2017-12-09T23:01:51.200098: step 820, loss 0.607252, acc 0.789062, prec 0.0448207, recall 0.803493
2017-12-09T23:01:51.497360: step 821, loss 0.631186, acc 0.804688, prec 0.0448156, recall 0.803601
2017-12-09T23:01:51.790944: step 822, loss 0.700283, acc 0.765625, prec 0.0447748, recall 0.803601
2017-12-09T23:01:52.099056: step 823, loss 0.657623, acc 0.804688, prec 0.0448568, recall 0.804028
2017-12-09T23:01:52.408114: step 824, loss 0.675872, acc 0.75, prec 0.0448133, recall 0.804028
2017-12-09T23:01:52.706211: step 825, loss 0.758165, acc 0.75, prec 0.0448277, recall 0.804241
2017-12-09T23:01:53.006469: step 826, loss 0.398027, acc 0.84375, prec 0.0448295, recall 0.804348
2017-12-09T23:01:53.310088: step 827, loss 0.464084, acc 0.84375, prec 0.0448023, recall 0.804348
2017-12-09T23:01:53.606966: step 828, loss 1.52589, acc 0.867188, prec 0.0448384, recall 0.804124
2017-12-09T23:01:53.906949: step 829, loss 0.60297, acc 0.867188, prec 0.0448732, recall 0.804336
2017-12-09T23:01:54.205645: step 830, loss 0.29717, acc 0.945312, prec 0.0448925, recall 0.804442
2017-12-09T23:01:54.503847: step 831, loss 0.80409, acc 0.921875, prec 0.0448803, recall 0.804007
2017-12-09T23:01:54.800972: step 832, loss 0.275617, acc 0.914062, prec 0.0448943, recall 0.804113
2017-12-09T23:01:55.099701: step 833, loss 0.369878, acc 0.882812, prec 0.0449316, recall 0.804324
2017-12-09T23:01:55.396075: step 834, loss 4.3978, acc 0.84375, prec 0.0449347, recall 0.803996
2017-12-09T23:01:55.693896: step 835, loss 0.392901, acc 0.867188, prec 0.044998, recall 0.804313
2017-12-09T23:01:55.989278: step 836, loss 2.37077, acc 0.8125, prec 0.0449668, recall 0.803879
2017-12-09T23:01:56.288658: step 837, loss 1.79163, acc 0.890625, prec 0.0450643, recall 0.803869
2017-12-09T23:01:56.594247: step 838, loss 0.4305, acc 0.8125, prec 0.045118, recall 0.804185
2017-12-09T23:01:56.893987: step 839, loss 0.59299, acc 0.796875, prec 0.0451114, recall 0.80429
2017-12-09T23:01:57.195471: step 840, loss 0.70957, acc 0.765625, prec 0.0450995, recall 0.804394
2017-12-09T23:01:57.491498: step 841, loss 0.608333, acc 0.796875, prec 0.0451216, recall 0.804604
2017-12-09T23:01:57.789899: step 842, loss 0.679626, acc 0.71875, prec 0.0451015, recall 0.804708
2017-12-09T23:01:58.086515: step 843, loss 0.554595, acc 0.820312, prec 0.0450704, recall 0.804708
2017-12-09T23:01:58.379728: step 844, loss 0.821062, acc 0.75, prec 0.0450558, recall 0.804813
2017-12-09T23:01:58.676641: step 845, loss 0.606248, acc 0.773438, prec 0.0450453, recall 0.804917
2017-12-09T23:01:58.970931: step 846, loss 0.645668, acc 0.804688, prec 0.0450402, recall 0.805021
2017-12-09T23:01:59.272096: step 847, loss 0.803207, acc 0.789062, prec 0.0450894, recall 0.805333
2017-12-09T23:01:59.570361: step 848, loss 0.630893, acc 0.796875, prec 0.0451399, recall 0.805644
2017-12-09T23:01:59.864316: step 849, loss 0.431602, acc 0.90625, prec 0.0451522, recall 0.805748
2017-12-09T23:02:00.164676: step 850, loss 1.77806, acc 0.8125, prec 0.0451498, recall 0.805423
2017-12-09T23:02:00.465007: step 851, loss 0.650136, acc 0.78125, prec 0.0452543, recall 0.805938
2017-12-09T23:02:00.766121: step 852, loss 0.452061, acc 0.84375, prec 0.0452841, recall 0.806144
2017-12-09T23:02:01.061082: step 853, loss 0.495379, acc 0.859375, prec 0.0452883, recall 0.806247
2017-12-09T23:02:01.357991: step 854, loss 0.450323, acc 0.835938, prec 0.0453168, recall 0.806452
2017-12-09T23:02:01.659513: step 855, loss 0.302443, acc 0.867188, prec 0.0452939, recall 0.806452
2017-12-09T23:02:01.954641: step 856, loss 0.401315, acc 0.921875, prec 0.0453655, recall 0.806758
2017-12-09T23:02:02.250993: step 857, loss 0.354351, acc 0.890625, prec 0.0454599, recall 0.807165
2017-12-09T23:02:02.547325: step 858, loss 0.918559, acc 0.820312, prec 0.045627, recall 0.807874
2017-12-09T23:02:02.844654: step 859, loss 0.252825, acc 0.921875, prec 0.0456135, recall 0.807874
2017-12-09T23:02:03.137902: step 860, loss 0.301646, acc 0.890625, prec 0.0456229, recall 0.807975
2017-12-09T23:02:03.436260: step 861, loss 0.21965, acc 0.921875, prec 0.0456376, recall 0.808075
2017-12-09T23:02:03.737694: step 862, loss 0.235427, acc 0.9375, prec 0.0456551, recall 0.808176
2017-12-09T23:02:04.037040: step 863, loss 2.11979, acc 0.9375, prec 0.0456456, recall 0.807753
2017-12-09T23:02:04.339583: step 864, loss 0.308224, acc 0.914062, prec 0.045659, recall 0.807853
2017-12-09T23:02:04.643763: step 865, loss 0.152424, acc 0.9375, prec 0.0456764, recall 0.807954
2017-12-09T23:02:04.941090: step 866, loss 1.57826, acc 0.90625, prec 0.0457744, recall 0.807933
2017-12-09T23:02:05.240614: step 867, loss 1.31885, acc 0.945312, prec 0.0457945, recall 0.807612
2017-12-09T23:02:05.550813: step 868, loss 1.34798, acc 0.9375, prec 0.0458133, recall 0.807292
2017-12-09T23:02:05.852414: step 869, loss 0.317658, acc 0.882812, prec 0.0458211, recall 0.807392
2017-12-09T23:02:06.149287: step 870, loss 0.430622, acc 0.859375, prec 0.045825, recall 0.807492
2017-12-09T23:02:06.444273: step 871, loss 0.595209, acc 0.867188, prec 0.0458864, recall 0.807792
2017-12-09T23:02:06.738727: step 872, loss 0.504171, acc 0.8125, prec 0.0459102, recall 0.807992
2017-12-09T23:02:07.037153: step 873, loss 0.480237, acc 0.828125, prec 0.0459367, recall 0.808191
2017-12-09T23:02:07.336288: step 874, loss 0.594115, acc 0.796875, prec 0.0459015, recall 0.808191
2017-12-09T23:02:07.634739: step 875, loss 0.492281, acc 0.835938, prec 0.0459293, recall 0.808389
2017-12-09T23:02:07.927502: step 876, loss 0.426989, acc 0.859375, prec 0.0459892, recall 0.808687
2017-12-09T23:02:08.220272: step 877, loss 0.467023, acc 0.820312, prec 0.0460142, recall 0.808884
2017-12-09T23:02:08.517043: step 878, loss 0.40492, acc 0.835938, prec 0.0460138, recall 0.808983
2017-12-09T23:02:08.814702: step 879, loss 0.396276, acc 0.828125, prec 0.0460681, recall 0.809278
2017-12-09T23:02:09.113350: step 880, loss 0.3677, acc 0.914062, prec 0.0460812, recall 0.809377
2017-12-09T23:02:09.409972: step 881, loss 0.524393, acc 0.820312, prec 0.046134, recall 0.809671
2017-12-09T23:02:09.708300: step 882, loss 0.618048, acc 0.882812, prec 0.0462255, recall 0.810062
2017-12-09T23:02:10.005139: step 883, loss 0.193907, acc 0.929688, prec 0.046325, recall 0.810451
2017-12-09T23:02:10.307798: step 884, loss 2.24433, acc 0.921875, prec 0.0463128, recall 0.810036
2017-12-09T23:02:10.604225: step 885, loss 0.278422, acc 0.898438, prec 0.046351, recall 0.81023
2017-12-09T23:02:10.903077: step 886, loss 0.206981, acc 0.953125, prec 0.0463987, recall 0.810424
2017-12-09T23:02:11.200293: step 887, loss 0.23903, acc 0.929688, prec 0.0464423, recall 0.810618
2017-12-09T23:02:11.500314: step 888, loss 0.294795, acc 0.9375, prec 0.0464593, recall 0.810714
2017-12-09T23:02:11.800143: step 889, loss 0.292733, acc 0.898438, prec 0.0464416, recall 0.810714
2017-12-09T23:02:12.096475: step 890, loss 0.523795, acc 0.867188, prec 0.0465021, recall 0.811004
2017-12-09T23:02:12.396363: step 891, loss 0.192943, acc 0.921875, prec 0.0465164, recall 0.8111
2017-12-09T23:02:12.692316: step 892, loss 0.588904, acc 0.90625, prec 0.0465836, recall 0.811388
2017-12-09T23:02:12.994576: step 893, loss 0.171956, acc 0.945312, prec 0.0465741, recall 0.811388
2017-12-09T23:02:13.296142: step 894, loss 2.67416, acc 0.875, prec 0.0466093, recall 0.811168
2017-12-09T23:02:13.595358: step 895, loss 1.40174, acc 0.90625, prec 0.0466777, recall 0.811044
2017-12-09T23:02:13.897355: step 896, loss 0.164201, acc 0.9375, prec 0.0466946, recall 0.811139
2017-12-09T23:02:14.194581: step 897, loss 0.75864, acc 0.835938, prec 0.0467772, recall 0.811521
2017-12-09T23:02:14.491338: step 898, loss 0.822625, acc 0.882812, prec 0.04684, recall 0.811806
2017-12-09T23:02:14.785768: step 899, loss 0.394622, acc 0.882812, prec 0.0468473, recall 0.811901
2017-12-09T23:02:15.083652: step 900, loss 0.538485, acc 0.804688, prec 0.0468132, recall 0.811901

Evaluation:
2017-12-09T23:02:19.804068: step 900, loss 1.19563, acc 0.819494, prec 0.0474162, recall 0.806829

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-900

2017-12-09T23:02:21.142503: step 901, loss 0.605505, acc 0.773438, prec 0.0474307, recall 0.807009
2017-12-09T23:02:21.440148: step 902, loss 0.538949, acc 0.78125, prec 0.0473943, recall 0.807009
2017-12-09T23:02:21.733158: step 903, loss 0.993371, acc 0.835938, prec 0.0474454, recall 0.80728
2017-12-09T23:02:22.029948: step 904, loss 0.543367, acc 0.804688, prec 0.0474651, recall 0.807459
2017-12-09T23:02:22.328058: step 905, loss 0.392809, acc 0.820312, prec 0.0474612, recall 0.807549
2017-12-09T23:02:22.627280: step 906, loss 0.656898, acc 0.757812, prec 0.0474992, recall 0.807818
2017-12-09T23:02:22.922943: step 907, loss 0.823637, acc 0.71875, prec 0.0475566, recall 0.808175
2017-12-09T23:02:23.220559: step 908, loss 0.423784, acc 0.78125, prec 0.0475462, recall 0.808264
2017-12-09T23:02:23.517295: step 909, loss 0.491172, acc 0.796875, prec 0.0475645, recall 0.808442
2017-12-09T23:02:23.816425: step 910, loss 0.557793, acc 0.851562, prec 0.0475658, recall 0.80853
2017-12-09T23:02:24.121558: step 911, loss 0.833639, acc 0.882812, prec 0.0475983, recall 0.808708
2017-12-09T23:02:24.419996: step 912, loss 0.498284, acc 0.875, prec 0.0476294, recall 0.808885
2017-12-09T23:02:24.715993: step 913, loss 0.355919, acc 0.875, prec 0.0476087, recall 0.808885
2017-12-09T23:02:25.013551: step 914, loss 1.18136, acc 0.84375, prec 0.0476359, recall 0.808688
2017-12-09T23:02:25.313879: step 915, loss 0.273787, acc 0.945312, prec 0.0476787, recall 0.808864
2017-12-09T23:02:25.612193: step 916, loss 0.33234, acc 0.867188, prec 0.0476825, recall 0.808952
2017-12-09T23:02:25.910667: step 917, loss 0.144257, acc 0.945312, prec 0.0476734, recall 0.808952
2017-12-09T23:02:26.206308: step 918, loss 1.27033, acc 0.929688, prec 0.0477149, recall 0.808756
2017-12-09T23:02:26.504499: step 919, loss 0.315016, acc 0.890625, prec 0.0477743, recall 0.80902
2017-12-09T23:02:26.803574: step 920, loss 0.356072, acc 0.882812, prec 0.0477549, recall 0.80902
2017-12-09T23:02:27.097984: step 921, loss 0.201551, acc 0.929688, prec 0.0477691, recall 0.809108
2017-12-09T23:02:27.393574: step 922, loss 0.224991, acc 0.914062, prec 0.0477548, recall 0.809108
2017-12-09T23:02:27.691856: step 923, loss 0.617634, acc 0.921875, prec 0.0478452, recall 0.809458
2017-12-09T23:02:27.987791: step 924, loss 0.0829633, acc 0.976562, prec 0.0478413, recall 0.809458
2017-12-09T23:02:28.286808: step 925, loss 0.300621, acc 0.9375, prec 0.0478826, recall 0.809633
2017-12-09T23:02:28.588480: step 926, loss 0.211607, acc 0.914062, prec 0.0478683, recall 0.809633
2017-12-09T23:02:28.893276: step 927, loss 0.405337, acc 0.914062, prec 0.0479057, recall 0.809808
2017-12-09T23:02:29.193615: step 928, loss 0.334474, acc 0.914062, prec 0.047943, recall 0.809982
2017-12-09T23:02:29.494558: step 929, loss 0.183851, acc 0.921875, prec 0.04793, recall 0.809982
2017-12-09T23:02:29.795722: step 930, loss 0.209263, acc 0.921875, prec 0.0479428, recall 0.810069
2017-12-09T23:02:30.097153: step 931, loss 0.171619, acc 0.9375, prec 0.0479324, recall 0.810069
2017-12-09T23:02:30.403557: step 932, loss 0.380183, acc 0.9375, prec 0.0479478, recall 0.810156
2017-12-09T23:02:30.704102: step 933, loss 1.7732, acc 0.96875, prec 0.0480212, recall 0.810046
2017-12-09T23:02:31.007602: step 934, loss 0.506526, acc 0.921875, prec 0.0480598, recall 0.810219
2017-12-09T23:02:31.301406: step 935, loss 0.383831, acc 0.90625, prec 0.0480699, recall 0.810306
2017-12-09T23:02:31.597656: step 936, loss 0.373227, acc 0.867188, prec 0.0480735, recall 0.810392
2017-12-09T23:02:31.891302: step 937, loss 3.78731, acc 0.921875, prec 0.0480889, recall 0.809741
2017-12-09T23:02:32.193921: step 938, loss 0.516339, acc 0.851562, prec 0.0481156, recall 0.809914
2017-12-09T23:02:32.488904: step 939, loss 0.53279, acc 0.828125, prec 0.0481128, recall 0.81
2017-12-09T23:02:32.790287: step 940, loss 0.600857, acc 0.78125, prec 0.0481021, recall 0.810086
2017-12-09T23:02:33.088686: step 941, loss 0.7427, acc 0.757812, prec 0.0481132, recall 0.810259
2017-12-09T23:02:33.387330: step 942, loss 0.749111, acc 0.804688, prec 0.0481064, recall 0.810345
2017-12-09T23:02:33.683286: step 943, loss 0.718486, acc 0.78125, prec 0.0481214, recall 0.810517
2017-12-09T23:02:33.985240: step 944, loss 0.801335, acc 0.773438, prec 0.0481607, recall 0.810774
2017-12-09T23:02:34.282490: step 945, loss 0.734414, acc 0.726562, prec 0.0482177, recall 0.811116
2017-12-09T23:02:34.580236: step 946, loss 0.748833, acc 0.757812, prec 0.0481776, recall 0.811116
2017-12-09T23:02:34.876952: step 947, loss 0.646847, acc 0.757812, prec 0.0482396, recall 0.811457
2017-12-09T23:02:35.172438: step 948, loss 0.633386, acc 0.804688, prec 0.0482328, recall 0.811542
2017-12-09T23:02:35.489238: step 949, loss 0.507193, acc 0.820312, prec 0.0482796, recall 0.811796
2017-12-09T23:02:35.783963: step 950, loss 0.767078, acc 0.78125, prec 0.0483198, recall 0.81205
2017-12-09T23:02:36.079675: step 951, loss 0.684649, acc 0.820312, prec 0.048341, recall 0.812219
2017-12-09T23:02:36.381567: step 952, loss 1.67844, acc 0.773438, prec 0.0483302, recall 0.811939
2017-12-09T23:02:36.678204: step 953, loss 0.362875, acc 0.921875, prec 0.0483681, recall 0.812108
2017-12-09T23:02:36.972764: step 954, loss 0.253767, acc 0.890625, prec 0.0483755, recall 0.812192
2017-12-09T23:02:37.270752: step 955, loss 0.52897, acc 0.828125, prec 0.0483725, recall 0.812276
2017-12-09T23:02:37.565529: step 956, loss 0.312603, acc 0.882812, prec 0.0483531, recall 0.812276
2017-12-09T23:02:37.863007: step 957, loss 0.338237, acc 0.90625, prec 0.0483884, recall 0.812444
2017-12-09T23:02:38.161548: step 958, loss 1.17893, acc 0.914062, prec 0.0485264, recall 0.812946
2017-12-09T23:02:38.458649: step 959, loss 0.384289, acc 0.898438, prec 0.0485602, recall 0.813113
2017-12-09T23:02:38.757875: step 960, loss 0.347476, acc 0.882812, prec 0.0485662, recall 0.813197
2017-12-09T23:02:39.055975: step 961, loss 0.306819, acc 0.921875, prec 0.0485786, recall 0.81328
2017-12-09T23:02:39.353329: step 962, loss 1.61487, acc 0.890625, prec 0.0486377, recall 0.813167
2017-12-09T23:02:39.651313: step 963, loss 0.36919, acc 0.859375, prec 0.0486144, recall 0.813167
2017-12-09T23:02:39.955505: step 964, loss 0.332489, acc 0.890625, prec 0.0486469, recall 0.813333
2017-12-09T23:02:40.251581: step 965, loss 0.41254, acc 0.851562, prec 0.0486224, recall 0.813333
2017-12-09T23:02:40.554737: step 966, loss 0.333875, acc 0.828125, prec 0.0486192, recall 0.813416
2017-12-09T23:02:40.853324: step 967, loss 0.357999, acc 0.84375, prec 0.0486187, recall 0.813499
2017-12-09T23:02:41.150571: step 968, loss 0.518785, acc 0.84375, prec 0.0486938, recall 0.81383
2017-12-09T23:02:41.447761: step 969, loss 0.57481, acc 0.8125, prec 0.048688, recall 0.813912
2017-12-09T23:02:41.744410: step 970, loss 0.28299, acc 0.90625, prec 0.0487482, recall 0.814159
2017-12-09T23:02:42.042495: step 971, loss 0.415284, acc 0.84375, prec 0.0487979, recall 0.814406
2017-12-09T23:02:42.338258: step 972, loss 0.379974, acc 0.875, prec 0.0488528, recall 0.814651
2017-12-09T23:02:42.634345: step 973, loss 0.590951, acc 0.890625, prec 0.048885, recall 0.814815
2017-12-09T23:02:42.936679: step 974, loss 0.332621, acc 0.867188, prec 0.0488882, recall 0.814896
2017-12-09T23:02:43.233890: step 975, loss 0.330488, acc 0.914062, prec 0.0489242, recall 0.815059
2017-12-09T23:02:43.527939: step 976, loss 0.328451, acc 0.867188, prec 0.0489274, recall 0.815141
2017-12-09T23:02:43.827759: step 977, loss 1.69729, acc 0.898438, prec 0.0489621, recall 0.814945
2017-12-09T23:02:44.126105: step 978, loss 0.210575, acc 0.914062, prec 0.0489981, recall 0.815108
2017-12-09T23:02:44.425690: step 979, loss 0.953372, acc 0.929688, prec 0.049112, recall 0.815513
2017-12-09T23:02:44.728488: step 980, loss 0.195544, acc 0.9375, prec 0.0491518, recall 0.815674
2017-12-09T23:02:45.030036: step 981, loss 0.328198, acc 0.882812, prec 0.0491574, recall 0.815755
2017-12-09T23:02:45.327971: step 982, loss 0.515321, acc 0.921875, prec 0.0491695, recall 0.815836
2017-12-09T23:02:45.624943: step 983, loss 0.202467, acc 0.914062, prec 0.0492054, recall 0.815997
2017-12-09T23:02:45.924050: step 984, loss 0.501685, acc 0.882812, prec 0.049236, recall 0.816157
2017-12-09T23:02:46.221635: step 985, loss 0.42426, acc 0.890625, prec 0.049293, recall 0.816398
2017-12-09T23:02:46.520560: step 986, loss 1.28376, acc 0.890625, prec 0.0493262, recall 0.816202
2017-12-09T23:02:46.823141: step 987, loss 0.338537, acc 0.890625, prec 0.049333, recall 0.816282
2017-12-09T23:02:47.118475: step 988, loss 0.230536, acc 0.898438, prec 0.0493662, recall 0.816442
2017-12-09T23:02:47.416689: step 989, loss 0.696826, acc 0.898438, prec 0.0494243, recall 0.816681
2017-12-09T23:02:47.716093: step 990, loss 0.589324, acc 0.828125, prec 0.0495206, recall 0.817078
2017-12-09T23:02:48.022565: step 991, loss 0.215055, acc 0.921875, prec 0.0495076, recall 0.817078
2017-12-09T23:02:48.317552: step 992, loss 0.617756, acc 0.796875, prec 0.0495237, recall 0.817237
2017-12-09T23:02:48.613272: step 993, loss 0.402999, acc 0.820312, prec 0.0495686, recall 0.817474
2017-12-09T23:02:48.912457: step 994, loss 1.10769, acc 0.835938, prec 0.0495911, recall 0.817632
2017-12-09T23:02:49.212003: step 995, loss 0.584109, acc 0.820312, prec 0.0496359, recall 0.817868
2017-12-09T23:02:49.392601: step 996, loss 0.897388, acc 0.769231, prec 0.0496203, recall 0.817868
2017-12-09T23:02:49.694332: step 997, loss 0.370624, acc 0.828125, prec 0.0496664, recall 0.818103
2017-12-09T23:02:49.991480: step 998, loss 0.358274, acc 0.882812, prec 0.0497214, recall 0.818338
2017-12-09T23:02:50.291527: step 999, loss 0.36872, acc 0.84375, prec 0.0497203, recall 0.818417
2017-12-09T23:02:50.604554: step 1000, loss 0.940733, acc 0.882812, prec 0.0497505, recall 0.818573
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1000

2017-12-09T23:02:51.937744: step 1001, loss 0.33095, acc 0.90625, prec 0.0497597, recall 0.818651
2017-12-09T23:02:52.236766: step 1002, loss 0.307989, acc 0.867188, prec 0.0497376, recall 0.818651
2017-12-09T23:02:52.535687: step 1003, loss 0.353976, acc 0.882812, prec 0.0497429, recall 0.818729
2017-12-09T23:02:52.829931: step 1004, loss 0.399796, acc 0.890625, prec 0.0497991, recall 0.818962
2017-12-09T23:02:53.122723: step 1005, loss 0.265189, acc 0.90625, prec 0.0497836, recall 0.818962
2017-12-09T23:02:53.423900: step 1006, loss 0.190493, acc 0.914062, prec 0.0497693, recall 0.818962
2017-12-09T23:02:53.719002: step 1007, loss 0.173211, acc 0.9375, prec 0.0497837, recall 0.819039
2017-12-09T23:02:54.023717: step 1008, loss 2.39149, acc 0.90625, prec 0.0497942, recall 0.818766
2017-12-09T23:02:54.329718: step 1009, loss 0.271474, acc 0.921875, prec 0.0498059, recall 0.818844
2017-12-09T23:02:54.629777: step 1010, loss 0.821048, acc 0.929688, prec 0.0497956, recall 0.818493
2017-12-09T23:02:54.929408: step 1011, loss 0.80402, acc 0.921875, prec 0.0498334, recall 0.818298
2017-12-09T23:02:55.229798: step 1012, loss 0.166451, acc 0.9375, prec 0.0498972, recall 0.818531
2017-12-09T23:02:55.536016: step 1013, loss 0.351341, acc 0.929688, prec 0.0499349, recall 0.818686
2017-12-09T23:02:55.839855: step 1014, loss 0.282832, acc 0.921875, prec 0.0499714, recall 0.818841
2017-12-09T23:02:56.135259: step 1015, loss 0.294864, acc 0.890625, prec 0.0499779, recall 0.818918
2017-12-09T23:02:56.427534: step 1016, loss 0.4779, acc 0.859375, prec 0.0499792, recall 0.818995
2017-12-09T23:02:56.723748: step 1017, loss 0.433065, acc 0.921875, prec 0.0500156, recall 0.819149
2017-12-09T23:02:57.018908: step 1018, loss 0.399499, acc 0.929688, prec 0.0500286, recall 0.819226
2017-12-09T23:02:57.318071: step 1019, loss 0.3475, acc 0.90625, prec 0.0501363, recall 0.81961
2017-12-09T23:02:57.622627: step 1020, loss 0.230073, acc 0.945312, prec 0.0501765, recall 0.819763
2017-12-09T23:02:57.921822: step 1021, loss 0.383339, acc 0.898438, prec 0.0501842, recall 0.819839
2017-12-09T23:02:58.226235: step 1022, loss 0.302742, acc 0.921875, prec 0.0501959, recall 0.819915
2017-12-09T23:02:58.519982: step 1023, loss 0.977679, acc 0.90625, prec 0.0502308, recall 0.819721
2017-12-09T23:02:58.820122: step 1024, loss 0.369555, acc 0.914062, prec 0.0502903, recall 0.819949
2017-12-09T23:02:59.115545: step 1025, loss 0.762504, acc 0.867188, prec 0.0504158, recall 0.820405
2017-12-09T23:02:59.418539: step 1026, loss 0.286594, acc 0.882812, prec 0.0503962, recall 0.820405
2017-12-09T23:02:59.712621: step 1027, loss 0.427595, acc 0.851562, prec 0.0504452, recall 0.820632
2017-12-09T23:03:00.021948: step 1028, loss 0.295552, acc 0.875, prec 0.0504243, recall 0.820632
2017-12-09T23:03:00.328723: step 1029, loss 0.911358, acc 0.875, prec 0.0504047, recall 0.820286
2017-12-09T23:03:00.624685: step 1030, loss 0.833247, acc 0.804688, prec 0.0503967, recall 0.820362
2017-12-09T23:03:00.924794: step 1031, loss 0.611494, acc 0.820312, prec 0.0504404, recall 0.820588
2017-12-09T23:03:01.224790: step 1032, loss 0.511481, acc 0.84375, prec 0.0505614, recall 0.821039
2017-12-09T23:03:01.525811: step 1033, loss 0.319285, acc 0.898438, prec 0.0505934, recall 0.821189
2017-12-09T23:03:01.830669: step 1034, loss 0.34529, acc 0.898438, prec 0.0505764, recall 0.821189
2017-12-09T23:03:02.127635: step 1035, loss 0.559987, acc 0.859375, prec 0.0506019, recall 0.821339
2017-12-09T23:03:02.425743: step 1036, loss 0.305067, acc 0.898438, prec 0.0506094, recall 0.821414
2017-12-09T23:03:02.720100: step 1037, loss 0.314249, acc 0.914062, prec 0.050644, recall 0.821563
2017-12-09T23:03:03.018791: step 1038, loss 0.693513, acc 0.882812, prec 0.0506733, recall 0.821712
2017-12-09T23:03:03.319576: step 1039, loss 0.413386, acc 0.828125, prec 0.0506691, recall 0.821786
2017-12-09T23:03:03.623128: step 1040, loss 0.356761, acc 0.851562, prec 0.0506931, recall 0.821935
2017-12-09T23:03:03.923001: step 1041, loss 0.37511, acc 0.921875, prec 0.0507045, recall 0.822009
2017-12-09T23:03:04.222393: step 1042, loss 0.311476, acc 0.90625, prec 0.0506889, recall 0.822009
2017-12-09T23:03:04.522851: step 1043, loss 0.376359, acc 0.875, prec 0.0507412, recall 0.822231
2017-12-09T23:03:04.820454: step 1044, loss 0.472676, acc 0.914062, prec 0.0508, recall 0.822453
2017-12-09T23:03:05.117808: step 1045, loss 0.475305, acc 0.929688, prec 0.0509101, recall 0.822822
2017-12-09T23:03:05.429552: step 1046, loss 1.24296, acc 0.875, prec 0.0509636, recall 0.822701
2017-12-09T23:03:05.730297: step 1047, loss 0.178498, acc 0.953125, prec 0.0510288, recall 0.822921
2017-12-09T23:03:06.027058: step 1048, loss 0.185969, acc 0.953125, prec 0.0510696, recall 0.823067
2017-12-09T23:03:06.324077: step 1049, loss 0.252254, acc 0.929688, prec 0.0511065, recall 0.823214
2017-12-09T23:03:06.620369: step 1050, loss 0.220743, acc 0.867188, prec 0.0511085, recall 0.823287
2017-12-09T23:03:06.919176: step 1051, loss 0.225144, acc 0.921875, prec 0.0511684, recall 0.823505
2017-12-09T23:03:07.217902: step 1052, loss 0.151465, acc 0.953125, prec 0.0511605, recall 0.823505
2017-12-09T23:03:07.514116: step 1053, loss 1.14904, acc 0.914062, prec 0.0512433, recall 0.823796
2017-12-09T23:03:07.812819: step 1054, loss 0.436503, acc 0.90625, prec 0.051349, recall 0.824158
2017-12-09T23:03:08.107330: step 1055, loss 0.353134, acc 0.90625, prec 0.0514303, recall 0.824446
2017-12-09T23:03:08.409255: step 1056, loss 0.30369, acc 0.90625, prec 0.0514873, recall 0.824662
2017-12-09T23:03:08.707106: step 1057, loss 0.37046, acc 0.890625, prec 0.0515416, recall 0.824877
2017-12-09T23:03:09.001696: step 1058, loss 0.421714, acc 0.882812, prec 0.0515704, recall 0.82502
2017-12-09T23:03:09.300811: step 1059, loss 0.428155, acc 0.875, prec 0.0515977, recall 0.825163
2017-12-09T23:03:09.600034: step 1060, loss 0.503231, acc 0.898438, prec 0.0516533, recall 0.825377
2017-12-09T23:03:09.899944: step 1061, loss 0.421086, acc 0.851562, prec 0.051725, recall 0.825662
2017-12-09T23:03:10.198378: step 1062, loss 0.398661, acc 0.914062, prec 0.0517589, recall 0.825804
2017-12-09T23:03:10.499211: step 1063, loss 0.159487, acc 0.953125, prec 0.0517751, recall 0.825875
2017-12-09T23:03:10.797244: step 1064, loss 1.12627, acc 0.914062, prec 0.0517861, recall 0.82561
2017-12-09T23:03:11.094593: step 1065, loss 0.368496, acc 0.882812, prec 0.051863, recall 0.825893
2017-12-09T23:03:11.395077: step 1066, loss 0.275638, acc 0.875, prec 0.0518902, recall 0.826034
2017-12-09T23:03:11.691276: step 1067, loss 0.38965, acc 0.859375, prec 0.0519147, recall 0.826175
2017-12-09T23:03:11.994756: step 1068, loss 0.497923, acc 0.898438, prec 0.0519699, recall 0.826386
2017-12-09T23:03:12.290317: step 1069, loss 0.320734, acc 0.875, prec 0.0519487, recall 0.826386
2017-12-09T23:03:12.584706: step 1070, loss 0.423768, acc 0.882812, prec 0.0520012, recall 0.826597
2017-12-09T23:03:12.881997: step 1071, loss 0.340831, acc 0.898438, prec 0.051984, recall 0.826597
2017-12-09T23:03:13.177657: step 1072, loss 0.397005, acc 0.875, prec 0.051987, recall 0.826667
2017-12-09T23:03:13.476780: step 1073, loss 0.330818, acc 0.921875, prec 0.0519738, recall 0.826667
2017-12-09T23:03:13.774967: step 1074, loss 0.222621, acc 0.929688, prec 0.051986, recall 0.826737
2017-12-09T23:03:14.080914: step 1075, loss 0.562571, acc 0.929688, prec 0.0520704, recall 0.827016
2017-12-09T23:03:14.379816: step 1076, loss 0.172724, acc 0.9375, prec 0.0520598, recall 0.827016
2017-12-09T23:03:14.681686: step 1077, loss 0.214538, acc 0.9375, prec 0.0520973, recall 0.827156
2017-12-09T23:03:14.977634: step 1078, loss 2.26238, acc 0.90625, prec 0.0521563, recall 0.826699
2017-12-09T23:03:15.279068: step 1079, loss 0.243974, acc 0.945312, prec 0.052147, recall 0.826699
2017-12-09T23:03:15.573069: step 1080, loss 1.10215, acc 0.898438, prec 0.0521792, recall 0.826506
2017-12-09T23:03:15.873148: step 1081, loss 0.437064, acc 0.898438, prec 0.0522101, recall 0.826645
2017-12-09T23:03:16.165603: step 1082, loss 0.211202, acc 0.914062, prec 0.0521955, recall 0.826645
2017-12-09T23:03:16.463316: step 1083, loss 0.305276, acc 0.890625, prec 0.052249, recall 0.826854
2017-12-09T23:03:16.758902: step 1084, loss 0.379531, acc 0.875, prec 0.0522278, recall 0.826854
2017-12-09T23:03:17.060253: step 1085, loss 0.284506, acc 0.898438, prec 0.0522586, recall 0.826992
2017-12-09T23:03:17.357565: step 1086, loss 0.48447, acc 0.84375, prec 0.0522801, recall 0.827131
2017-12-09T23:03:17.657644: step 1087, loss 0.252647, acc 0.890625, prec 0.0522616, recall 0.827131
2017-12-09T23:03:17.952565: step 1088, loss 0.221897, acc 0.914062, prec 0.0522471, recall 0.827131
2017-12-09T23:03:18.244690: step 1089, loss 2.04028, acc 0.851562, prec 0.0522712, recall 0.826938
2017-12-09T23:03:18.544517: step 1090, loss 0.354265, acc 0.875, prec 0.052298, recall 0.827077
2017-12-09T23:03:18.844164: step 1091, loss 0.298734, acc 0.859375, prec 0.0522981, recall 0.827146
2017-12-09T23:03:19.136297: step 1092, loss 0.346041, acc 0.867188, prec 0.0522996, recall 0.827215
2017-12-09T23:03:19.430600: step 1093, loss 0.312796, acc 0.882812, prec 0.0522798, recall 0.827215
2017-12-09T23:03:19.726712: step 1094, loss 0.948647, acc 0.914062, prec 0.0523144, recall 0.827023
2017-12-09T23:03:20.023970: step 1095, loss 0.470495, acc 0.882812, prec 0.0522947, recall 0.827023
2017-12-09T23:03:20.329044: step 1096, loss 0.512613, acc 0.84375, prec 0.0522922, recall 0.827092
2017-12-09T23:03:20.630205: step 1097, loss 0.543256, acc 0.851562, prec 0.0523149, recall 0.827229
2017-12-09T23:03:20.930050: step 1098, loss 0.320998, acc 0.867188, prec 0.0523402, recall 0.827367
2017-12-09T23:03:21.225686: step 1099, loss 0.30027, acc 0.921875, prec 0.0523509, recall 0.827435
2017-12-09T23:03:21.523509: step 1100, loss 0.331565, acc 0.875, prec 0.0523775, recall 0.827573
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1100

2017-12-09T23:03:22.853217: step 1101, loss 0.242199, acc 0.90625, prec 0.0523855, recall 0.827641
2017-12-09T23:03:23.150268: step 1102, loss 0.645447, acc 0.890625, prec 0.0523909, recall 0.827709
2017-12-09T23:03:23.448785: step 1103, loss 0.254547, acc 0.921875, prec 0.0524015, recall 0.827778
2017-12-09T23:03:23.742942: step 1104, loss 0.352429, acc 0.851562, prec 0.0524241, recall 0.827914
2017-12-09T23:03:24.043453: step 1105, loss 0.340277, acc 0.945312, prec 0.0525338, recall 0.828255
2017-12-09T23:03:24.340990: step 1106, loss 0.501053, acc 0.921875, prec 0.0526395, recall 0.828594
2017-12-09T23:03:24.639132: step 1107, loss 0.231199, acc 0.9375, prec 0.0526527, recall 0.828662
2017-12-09T23:03:24.932679: step 1108, loss 0.261603, acc 0.960938, prec 0.0526699, recall 0.828729
2017-12-09T23:03:25.229626: step 1109, loss 0.270437, acc 0.921875, prec 0.0527279, recall 0.828932
2017-12-09T23:03:25.526319: step 1110, loss 1.45507, acc 0.914062, prec 0.0527622, recall 0.82874
2017-12-09T23:03:25.823866: step 1111, loss 0.27228, acc 0.914062, prec 0.0528188, recall 0.828942
2017-12-09T23:03:26.123229: step 1112, loss 1.12142, acc 0.9375, prec 0.0529282, recall 0.828953
2017-12-09T23:03:26.428312: step 1113, loss 0.243124, acc 0.945312, prec 0.0529664, recall 0.829087
2017-12-09T23:03:26.727104: step 1114, loss 0.594336, acc 0.875, prec 0.0529926, recall 0.829221
2017-12-09T23:03:27.025224: step 1115, loss 0.518405, acc 0.914062, prec 0.0530491, recall 0.829421
2017-12-09T23:03:27.324135: step 1116, loss 1.29792, acc 0.921875, prec 0.0530845, recall 0.82923
2017-12-09T23:03:27.622600: step 1117, loss 0.218113, acc 0.953125, prec 0.0531476, recall 0.82943
2017-12-09T23:03:27.919053: step 1118, loss 0.50017, acc 0.875, prec 0.0531737, recall 0.829563
2017-12-09T23:03:28.221920: step 1119, loss 0.315598, acc 0.90625, prec 0.0531814, recall 0.82963
2017-12-09T23:03:28.515449: step 1120, loss 0.582032, acc 0.851562, prec 0.0531798, recall 0.829696
2017-12-09T23:03:28.816999: step 1121, loss 0.402745, acc 0.859375, prec 0.0532268, recall 0.829895
2017-12-09T23:03:29.118350: step 1122, loss 0.439706, acc 0.851562, prec 0.0532725, recall 0.830093
2017-12-09T23:03:29.419302: step 1123, loss 0.452968, acc 0.828125, prec 0.0532904, recall 0.830225
2017-12-09T23:03:29.714372: step 1124, loss 0.633845, acc 0.851562, prec 0.053336, recall 0.830423
2017-12-09T23:03:30.020622: step 1125, loss 0.499516, acc 0.84375, prec 0.0533566, recall 0.830554
2017-12-09T23:03:30.325171: step 1126, loss 0.370506, acc 0.882812, prec 0.0533602, recall 0.83062
2017-12-09T23:03:30.624149: step 1127, loss 0.499722, acc 0.851562, prec 0.0533586, recall 0.830686
2017-12-09T23:03:30.925171: step 1128, loss 0.417301, acc 0.859375, prec 0.0533582, recall 0.830751
2017-12-09T23:03:31.229577: step 1129, loss 0.269073, acc 0.921875, prec 0.0534156, recall 0.830948
2017-12-09T23:03:31.531141: step 1130, loss 0.337752, acc 0.859375, prec 0.0534152, recall 0.831013
2017-12-09T23:03:31.830820: step 1131, loss 0.329632, acc 0.90625, prec 0.0534463, recall 0.831144
2017-12-09T23:03:32.129076: step 1132, loss 1.06967, acc 0.960938, prec 0.0534645, recall 0.830888
2017-12-09T23:03:32.429024: step 1133, loss 0.183935, acc 0.945312, prec 0.0535493, recall 0.831149
2017-12-09T23:03:32.725220: step 1134, loss 0.401422, acc 0.859375, prec 0.0535723, recall 0.831279
2017-12-09T23:03:33.026900: step 1135, loss 0.385206, acc 0.875, prec 0.053551, recall 0.831279
2017-12-09T23:03:33.322710: step 1136, loss 0.242998, acc 0.90625, prec 0.0535351, recall 0.831279
2017-12-09T23:03:33.622161: step 1137, loss 0.249456, acc 0.914062, prec 0.0535674, recall 0.831409
2017-12-09T23:03:33.920627: step 1138, loss 0.198105, acc 0.960938, prec 0.0536077, recall 0.831538
2017-12-09T23:03:34.217044: step 1139, loss 0.420729, acc 0.960938, prec 0.0536715, recall 0.831733
2017-12-09T23:03:34.519498: step 1140, loss 0.0921791, acc 0.976562, prec 0.0537144, recall 0.831862
2017-12-09T23:03:34.821649: step 1141, loss 0.31934, acc 0.9375, prec 0.0537506, recall 0.831991
2017-12-09T23:03:35.118803: step 1142, loss 0.212276, acc 0.929688, prec 0.0537621, recall 0.832055
2017-12-09T23:03:35.431225: step 1143, loss 0.461596, acc 0.921875, prec 0.0537722, recall 0.83212
2017-12-09T23:03:35.732167: step 1144, loss 0.263815, acc 0.945312, prec 0.0538098, recall 0.832248
2017-12-09T23:03:36.029641: step 1145, loss 0.237798, acc 0.945312, prec 0.0538473, recall 0.832377
2017-12-09T23:03:36.325940: step 1146, loss 0.221371, acc 0.921875, prec 0.053834, recall 0.832377
2017-12-09T23:03:36.623540: step 1147, loss 0.118113, acc 0.953125, prec 0.0538494, recall 0.832441
2017-12-09T23:03:36.923302: step 1148, loss 1.15008, acc 0.945312, prec 0.0538648, recall 0.832187
2017-12-09T23:03:37.224167: step 1149, loss 0.218858, acc 0.945312, prec 0.0538555, recall 0.832187
2017-12-09T23:03:37.529260: step 1150, loss 0.698836, acc 0.929688, prec 0.0538669, recall 0.832251
2017-12-09T23:03:37.829265: step 1151, loss 0.317094, acc 0.953125, prec 0.0538823, recall 0.832315
2017-12-09T23:03:38.125673: step 1152, loss 0.713763, acc 0.96875, prec 0.0539939, recall 0.832634
2017-12-09T23:03:38.431429: step 1153, loss 0.173152, acc 0.921875, prec 0.054004, recall 0.832698
2017-12-09T23:03:38.730254: step 1154, loss 0.186451, acc 0.945312, prec 0.0539946, recall 0.832698
2017-12-09T23:03:39.027893: step 1155, loss 0.34805, acc 0.890625, prec 0.054046, recall 0.832889
2017-12-09T23:03:39.329238: step 1156, loss 0.205579, acc 0.9375, prec 0.0541054, recall 0.83308
2017-12-09T23:03:39.625991: step 1157, loss 0.223744, acc 0.90625, prec 0.0541128, recall 0.833143
2017-12-09T23:03:39.921368: step 1158, loss 0.769699, acc 0.867188, prec 0.0542068, recall 0.83346
2017-12-09T23:03:40.219573: step 1159, loss 0.230991, acc 0.929688, prec 0.0542414, recall 0.833586
2017-12-09T23:03:40.520918: step 1160, loss 0.21722, acc 0.898438, prec 0.0542706, recall 0.833712
2017-12-09T23:03:40.821732: step 1161, loss 0.256293, acc 0.914062, prec 0.0543025, recall 0.833838
2017-12-09T23:03:41.123277: step 1162, loss 0.222916, acc 0.890625, prec 0.0543071, recall 0.833901
2017-12-09T23:03:41.418320: step 1163, loss 0.226684, acc 0.929688, prec 0.0542951, recall 0.833901
2017-12-09T23:03:41.711848: step 1164, loss 0.382843, acc 0.9375, prec 0.0543775, recall 0.834152
2017-12-09T23:03:42.008811: step 1165, loss 0.486603, acc 0.90625, prec 0.054408, recall 0.834277
2017-12-09T23:03:42.307670: step 1166, loss 0.247479, acc 0.90625, prec 0.0544385, recall 0.834402
2017-12-09T23:03:42.608662: step 1167, loss 0.22145, acc 0.914062, prec 0.0544238, recall 0.834402
2017-12-09T23:03:42.904591: step 1168, loss 0.389481, acc 0.898438, prec 0.0544296, recall 0.834465
2017-12-09T23:03:43.202115: step 1169, loss 0.276668, acc 0.90625, prec 0.0544136, recall 0.834465
2017-12-09T23:03:43.500142: step 1170, loss 0.158862, acc 0.953125, prec 0.0544288, recall 0.834527
2017-12-09T23:03:43.793108: step 1171, loss 0.26098, acc 0.9375, prec 0.0544878, recall 0.834714
2017-12-09T23:03:44.091309: step 1172, loss 0.228466, acc 0.914062, prec 0.0544731, recall 0.834714
2017-12-09T23:03:44.385499: step 1173, loss 1.72401, acc 0.9375, prec 0.0544869, recall 0.834462
2017-12-09T23:03:44.686083: step 1174, loss 0.147923, acc 0.929688, prec 0.0544749, recall 0.834462
2017-12-09T23:03:44.981295: step 1175, loss 0.184525, acc 0.929688, prec 0.0545093, recall 0.834586
2017-12-09T23:03:45.280990: step 1176, loss 0.14192, acc 0.9375, prec 0.0544986, recall 0.834586
2017-12-09T23:03:45.579115: step 1177, loss 0.218405, acc 0.9375, prec 0.0545111, recall 0.834649
2017-12-09T23:03:45.877604: step 1178, loss 0.155728, acc 0.945312, prec 0.0545481, recall 0.834773
2017-12-09T23:03:46.176226: step 1179, loss 0.231298, acc 0.9375, prec 0.0545838, recall 0.834897
2017-12-09T23:03:46.476612: step 1180, loss 0.260742, acc 0.9375, prec 0.0546195, recall 0.835021
2017-12-09T23:03:46.773452: step 1181, loss 0.159953, acc 0.929688, prec 0.0546306, recall 0.835082
2017-12-09T23:03:47.074818: step 1182, loss 1.57276, acc 0.914062, prec 0.0547331, recall 0.835079
2017-12-09T23:03:47.378812: step 1183, loss 1.14492, acc 0.929688, prec 0.054815, recall 0.835013
2017-12-09T23:03:47.677563: step 1184, loss 0.908065, acc 0.898438, prec 0.0548452, recall 0.834825
2017-12-09T23:03:47.979604: step 1185, loss 0.315774, acc 0.882812, prec 0.054825, recall 0.834825
2017-12-09T23:03:48.275520: step 1186, loss 0.372879, acc 0.875, prec 0.0548036, recall 0.834825
2017-12-09T23:03:48.568805: step 1187, loss 0.540706, acc 0.875, prec 0.0548746, recall 0.835071
2017-12-09T23:03:48.865935: step 1188, loss 0.386851, acc 0.875, prec 0.0548763, recall 0.835132
2017-12-09T23:03:49.161814: step 1189, loss 0.462178, acc 0.851562, prec 0.0548508, recall 0.835132
2017-12-09T23:03:49.460131: step 1190, loss 0.493911, acc 0.820312, prec 0.05482, recall 0.835132
2017-12-09T23:03:49.755559: step 1191, loss 0.590534, acc 0.804688, prec 0.0548096, recall 0.835193
2017-12-09T23:03:50.052575: step 1192, loss 0.438944, acc 0.867188, prec 0.0548099, recall 0.835255
2017-12-09T23:03:50.358000: step 1193, loss 0.637582, acc 0.773438, prec 0.0547711, recall 0.835255
2017-12-09T23:03:50.662273: step 1194, loss 0.421967, acc 0.851562, prec 0.0548379, recall 0.835499
2017-12-09T23:03:50.955299: step 1195, loss 0.677539, acc 0.804688, prec 0.0548736, recall 0.835683
2017-12-09T23:03:51.251607: step 1196, loss 1.14497, acc 0.84375, prec 0.0548482, recall 0.835373
2017-12-09T23:03:51.549639: step 1197, loss 0.382198, acc 0.882812, prec 0.0548742, recall 0.835495
2017-12-09T23:03:51.849260: step 1198, loss 0.400819, acc 0.859375, prec 0.0548961, recall 0.835616
2017-12-09T23:03:52.147067: step 1199, loss 0.637387, acc 0.820312, prec 0.0548654, recall 0.835616
2017-12-09T23:03:52.443464: step 1200, loss 0.395615, acc 0.921875, prec 0.054944, recall 0.83586

Evaluation:
2017-12-09T23:03:57.157412: step 1200, loss 1.53403, acc 0.895641, prec 0.055855, recall 0.826224

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1200

2017-12-09T23:03:58.449346: step 1201, loss 0.333065, acc 0.90625, prec 0.0559284, recall 0.826467
2017-12-09T23:03:58.748466: step 1202, loss 0.383988, acc 0.921875, prec 0.0559597, recall 0.826588
2017-12-09T23:03:59.048745: step 1203, loss 0.463421, acc 0.90625, prec 0.0560108, recall 0.826769
2017-12-09T23:03:59.346000: step 1204, loss 0.854223, acc 0.90625, prec 0.0561063, recall 0.82707
2017-12-09T23:03:59.648010: step 1205, loss 0.277237, acc 0.898438, prec 0.0561336, recall 0.827191
2017-12-09T23:03:59.949498: step 1206, loss 0.232872, acc 0.921875, prec 0.0561649, recall 0.827311
2017-12-09T23:04:00.260270: step 1207, loss 0.176669, acc 0.976562, prec 0.0562055, recall 0.827431
2017-12-09T23:04:00.556444: step 1208, loss 0.143919, acc 0.9375, prec 0.0561949, recall 0.827431
2017-12-09T23:04:00.853395: step 1209, loss 0.237482, acc 0.921875, prec 0.0562484, recall 0.82761
2017-12-09T23:04:01.145775: step 1210, loss 0.232117, acc 0.890625, prec 0.0562298, recall 0.82761
2017-12-09T23:04:01.444553: step 1211, loss 0.76782, acc 0.945312, prec 0.0562219, recall 0.827323
2017-12-09T23:04:01.748447: step 1212, loss 0.195098, acc 0.9375, prec 0.0562335, recall 0.827383
2017-12-09T23:04:02.048853: step 1213, loss 0.110217, acc 0.960938, prec 0.0562269, recall 0.827383
2017-12-09T23:04:02.343019: step 1214, loss 0.0900846, acc 0.960938, prec 0.0562425, recall 0.827443
2017-12-09T23:04:02.644780: step 1215, loss 0.251679, acc 0.960938, prec 0.0562803, recall 0.827562
2017-12-09T23:04:02.942820: step 1216, loss 0.801693, acc 0.9375, prec 0.056271, recall 0.827276
2017-12-09T23:04:03.241859: step 1217, loss 0.227472, acc 0.9375, prec 0.0563049, recall 0.827395
2017-12-09T23:04:03.539434: step 1218, loss 0.713, acc 0.953125, prec 0.0562982, recall 0.827109
2017-12-09T23:04:03.841788: step 1219, loss 0.11953, acc 0.945312, prec 0.0563334, recall 0.827229
2017-12-09T23:04:04.134829: step 1220, loss 0.203417, acc 0.960938, prec 0.0563268, recall 0.827229
2017-12-09T23:04:04.431314: step 1221, loss 0.345348, acc 0.875, prec 0.05635, recall 0.827348
2017-12-09T23:04:04.725759: step 1222, loss 0.223838, acc 0.929688, prec 0.0563602, recall 0.827408
2017-12-09T23:04:05.017314: step 1223, loss 0.219883, acc 0.9375, prec 0.0564162, recall 0.827586
2017-12-09T23:04:05.322003: step 1224, loss 0.281359, acc 0.9375, prec 0.0564277, recall 0.827646
2017-12-09T23:04:05.629244: step 1225, loss 0.44185, acc 0.921875, prec 0.0564588, recall 0.827764
2017-12-09T23:04:05.926816: step 1226, loss 0.240946, acc 0.90625, prec 0.0564429, recall 0.827764
2017-12-09T23:04:06.227917: step 1227, loss 0.46688, acc 0.90625, prec 0.0564935, recall 0.827942
2017-12-09T23:04:06.528742: step 1228, loss 0.39048, acc 0.921875, prec 0.0565245, recall 0.828061
2017-12-09T23:04:06.824633: step 1229, loss 0.242209, acc 0.898438, prec 0.0565073, recall 0.828061
2017-12-09T23:04:07.126412: step 1230, loss 0.329074, acc 0.914062, prec 0.0565591, recall 0.828238
2017-12-09T23:04:07.427724: step 1231, loss 1.16815, acc 0.90625, prec 0.0565666, recall 0.828012
2017-12-09T23:04:07.726482: step 1232, loss 0.253191, acc 0.914062, prec 0.0565741, recall 0.828071
2017-12-09T23:04:08.023610: step 1233, loss 0.377548, acc 0.890625, prec 0.0565998, recall 0.828189
2017-12-09T23:04:08.321231: step 1234, loss 0.317386, acc 0.921875, prec 0.0566528, recall 0.828366
2017-12-09T23:04:08.616868: step 1235, loss 0.470907, acc 0.929688, prec 0.0566851, recall 0.828483
2017-12-09T23:04:08.913301: step 1236, loss 0.16034, acc 0.9375, prec 0.0566745, recall 0.828483
2017-12-09T23:04:09.219722: step 1237, loss 0.349491, acc 0.867188, prec 0.0566519, recall 0.828483
2017-12-09T23:04:09.514681: step 1238, loss 0.148735, acc 0.960938, prec 0.0566894, recall 0.828601
2017-12-09T23:04:09.809254: step 1239, loss 0.303507, acc 0.898438, prec 0.0567384, recall 0.828776
2017-12-09T23:04:10.107293: step 1240, loss 0.344755, acc 0.882812, prec 0.0567626, recall 0.828893
2017-12-09T23:04:10.404672: step 1241, loss 0.562303, acc 0.90625, prec 0.0568129, recall 0.829069
2017-12-09T23:04:10.703153: step 1242, loss 0.179814, acc 0.929688, prec 0.0568671, recall 0.829243
2017-12-09T23:04:10.999903: step 1243, loss 0.214584, acc 0.9375, prec 0.0568564, recall 0.829243
2017-12-09T23:04:11.295687: step 1244, loss 0.145397, acc 0.953125, prec 0.0568485, recall 0.829243
2017-12-09T23:04:11.475315: step 1245, loss 0.20722, acc 0.961538, prec 0.0568678, recall 0.829302
2017-12-09T23:04:11.780782: step 1246, loss 0.147591, acc 0.960938, prec 0.0569053, recall 0.829418
2017-12-09T23:04:12.082317: step 1247, loss 0.200868, acc 0.914062, prec 0.0569127, recall 0.829476
2017-12-09T23:04:12.385149: step 1248, loss 0.726207, acc 0.9375, prec 0.0569681, recall 0.82965
2017-12-09T23:04:12.686242: step 1249, loss 0.441195, acc 0.914062, prec 0.0569755, recall 0.829708
2017-12-09T23:04:12.989900: step 1250, loss 0.106869, acc 0.953125, prec 0.0569895, recall 0.829766
2017-12-09T23:04:13.283502: step 1251, loss 0.719661, acc 0.921875, prec 0.0570202, recall 0.829881
2017-12-09T23:04:13.584316: step 1252, loss 0.651427, acc 0.96875, prec 0.0571249, recall 0.830169
2017-12-09T23:04:13.886691: step 1253, loss 0.139579, acc 0.953125, prec 0.0571389, recall 0.830227
2017-12-09T23:04:14.182912: step 1254, loss 0.296061, acc 0.914062, prec 0.0571462, recall 0.830285
2017-12-09T23:04:14.476888: step 1255, loss 0.160235, acc 0.929688, prec 0.0571562, recall 0.830342
2017-12-09T23:04:14.773624: step 1256, loss 0.351949, acc 0.882812, prec 0.0571362, recall 0.830342
2017-12-09T23:04:15.069428: step 1257, loss 0.21679, acc 0.9375, prec 0.0571914, recall 0.830514
2017-12-09T23:04:15.367193: step 1258, loss 0.431399, acc 0.875, prec 0.0571701, recall 0.830514
2017-12-09T23:04:15.662776: step 1259, loss 0.342993, acc 0.898438, prec 0.0572845, recall 0.830858
2017-12-09T23:04:15.960957: step 1260, loss 0.298641, acc 0.929688, prec 0.0573383, recall 0.831029
2017-12-09T23:04:16.265961: step 1261, loss 0.551354, acc 0.890625, prec 0.0574512, recall 0.83137
2017-12-09T23:04:16.570185: step 1262, loss 0.36758, acc 0.890625, prec 0.0574763, recall 0.831483
2017-12-09T23:04:16.867946: step 1263, loss 0.477515, acc 0.859375, prec 0.0574961, recall 0.831597
2017-12-09T23:04:17.165717: step 1264, loss 0.465921, acc 0.851562, prec 0.0574926, recall 0.831653
2017-12-09T23:04:17.470835: step 1265, loss 0.382106, acc 0.898438, prec 0.057519, recall 0.831766
2017-12-09T23:04:17.767759: step 1266, loss 0.332138, acc 0.90625, prec 0.0575249, recall 0.831823
2017-12-09T23:04:18.068348: step 1267, loss 0.257294, acc 0.890625, prec 0.0575281, recall 0.831879
2017-12-09T23:04:18.365098: step 1268, loss 0.228001, acc 0.882812, prec 0.0575299, recall 0.831936
2017-12-09T23:04:18.669133: step 1269, loss 0.275128, acc 0.882812, prec 0.0575536, recall 0.832048
2017-12-09T23:04:18.966066: step 1270, loss 0.16966, acc 0.953125, prec 0.0575893, recall 0.832161
2017-12-09T23:04:19.273317: step 1271, loss 0.270195, acc 0.953125, prec 0.0576032, recall 0.832217
2017-12-09T23:04:19.570623: step 1272, loss 0.16976, acc 0.976562, prec 0.0576428, recall 0.832329
2017-12-09T23:04:19.867540: step 1273, loss 0.116506, acc 0.96875, prec 0.0576812, recall 0.832441
2017-12-09T23:04:20.173251: step 1274, loss 0.242897, acc 0.914062, prec 0.0576665, recall 0.832441
2017-12-09T23:04:20.481612: step 1275, loss 0.180934, acc 0.929688, prec 0.0576981, recall 0.832553
2017-12-09T23:04:20.784811: step 1276, loss 0.413383, acc 0.953125, prec 0.0577119, recall 0.832609
2017-12-09T23:04:21.083337: step 1277, loss 0.276592, acc 0.984375, prec 0.0577747, recall 0.832777
2017-12-09T23:04:21.378682: step 1278, loss 0.143749, acc 0.96875, prec 0.0578348, recall 0.832944
2017-12-09T23:04:21.678194: step 1279, loss 0.071605, acc 0.984375, prec 0.0578321, recall 0.832944
2017-12-09T23:04:21.973870: step 1280, loss 0.141244, acc 0.9375, prec 0.057865, recall 0.833056
2017-12-09T23:04:22.280075: step 1281, loss 0.11809, acc 0.960938, prec 0.0578583, recall 0.833056
2017-12-09T23:04:22.582133: step 1282, loss 1.62761, acc 0.945312, prec 0.0578721, recall 0.832834
2017-12-09T23:04:22.883642: step 1283, loss 0.0486158, acc 0.976562, prec 0.0578681, recall 0.832834
2017-12-09T23:04:23.182395: step 1284, loss 0.0585022, acc 0.976562, prec 0.0579076, recall 0.832945
2017-12-09T23:04:23.476674: step 1285, loss 4.573, acc 0.945312, prec 0.0579009, recall 0.832391
2017-12-09T23:04:23.780974: step 1286, loss 0.33604, acc 0.9375, prec 0.0579556, recall 0.832558
2017-12-09T23:04:24.080449: step 1287, loss 0.191263, acc 0.929688, prec 0.0579435, recall 0.832558
2017-12-09T23:04:24.381558: step 1288, loss 0.405276, acc 0.882812, prec 0.057967, recall 0.832669
2017-12-09T23:04:24.686817: step 1289, loss 0.224757, acc 0.898438, prec 0.0579496, recall 0.832669
2017-12-09T23:04:24.981270: step 1290, loss 0.424748, acc 0.835938, prec 0.0579215, recall 0.832669
2017-12-09T23:04:25.279113: step 1291, loss 0.319715, acc 0.867188, prec 0.0578987, recall 0.832669
2017-12-09T23:04:25.573019: step 1292, loss 0.316976, acc 0.882812, prec 0.0578787, recall 0.832669
2017-12-09T23:04:25.868072: step 1293, loss 0.344901, acc 0.867188, prec 0.0579429, recall 0.832891
2017-12-09T23:04:26.166932: step 1294, loss 0.334819, acc 0.882812, prec 0.0580098, recall 0.833113
2017-12-09T23:04:26.470839: step 1295, loss 0.490421, acc 0.898438, prec 0.0580141, recall 0.833168
2017-12-09T23:04:26.766513: step 1296, loss 0.433718, acc 0.867188, prec 0.0580565, recall 0.833333
2017-12-09T23:04:27.061283: step 1297, loss 0.411911, acc 0.859375, prec 0.0580758, recall 0.833443
2017-12-09T23:04:27.357190: step 1298, loss 0.330862, acc 0.921875, prec 0.0581058, recall 0.833553
2017-12-09T23:04:27.660248: step 1299, loss 0.53959, acc 0.875, prec 0.0581494, recall 0.833718
2017-12-09T23:04:27.959131: step 1300, loss 0.5161, acc 0.867188, prec 0.0581484, recall 0.833773
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1300

2017-12-09T23:04:29.256576: step 1301, loss 0.395825, acc 0.890625, prec 0.0582812, recall 0.834156
2017-12-09T23:04:29.551279: step 1302, loss 0.35015, acc 0.882812, prec 0.0582611, recall 0.834156
2017-12-09T23:04:29.847290: step 1303, loss 0.21989, acc 0.9375, prec 0.0582937, recall 0.834265
2017-12-09T23:04:30.154586: step 1304, loss 0.309018, acc 0.914062, prec 0.0583655, recall 0.834483
2017-12-09T23:04:30.459772: step 1305, loss 0.288471, acc 0.882812, prec 0.0584102, recall 0.834646
2017-12-09T23:04:30.764094: step 1306, loss 0.769389, acc 0.890625, prec 0.0584347, recall 0.834754
2017-12-09T23:04:31.064616: step 1307, loss 0.0943204, acc 0.960938, prec 0.058428, recall 0.834754
2017-12-09T23:04:31.362940: step 1308, loss 0.313841, acc 0.945312, prec 0.0584834, recall 0.834916
2017-12-09T23:04:31.661777: step 1309, loss 0.291599, acc 0.90625, prec 0.0585321, recall 0.835079
2017-12-09T23:04:31.957062: step 1310, loss 0.155582, acc 0.953125, prec 0.0585672, recall 0.835186
2017-12-09T23:04:32.252987: step 1311, loss 2.7324, acc 0.921875, prec 0.0585983, recall 0.835021
2017-12-09T23:04:32.561025: step 1312, loss 0.244948, acc 0.9375, prec 0.0586092, recall 0.835075
2017-12-09T23:04:32.861795: step 1313, loss 0.24183, acc 0.921875, prec 0.0585957, recall 0.835075
2017-12-09T23:04:33.158109: step 1314, loss 0.485928, acc 0.929688, prec 0.0586483, recall 0.835237
2017-12-09T23:04:33.454981: step 1315, loss 0.416114, acc 0.898438, prec 0.058674, recall 0.835344
2017-12-09T23:04:33.750882: step 1316, loss 0.852013, acc 0.914062, prec 0.0587252, recall 0.835233
2017-12-09T23:04:34.049205: step 1317, loss 0.20669, acc 0.945312, prec 0.0587374, recall 0.835286
2017-12-09T23:04:34.347371: step 1318, loss 0.349834, acc 0.898438, prec 0.0587414, recall 0.83534
2017-12-09T23:04:34.644962: step 1319, loss 0.298079, acc 0.9375, prec 0.0587737, recall 0.835447
2017-12-09T23:04:34.940641: step 1320, loss 0.434375, acc 0.890625, prec 0.058841, recall 0.835661
2017-12-09T23:04:35.242703: step 1321, loss 0.309462, acc 0.898438, prec 0.058845, recall 0.835714
2017-12-09T23:04:35.554722: step 1322, loss 0.179936, acc 0.929688, prec 0.058876, recall 0.835821
2017-12-09T23:04:35.851274: step 1323, loss 0.273189, acc 0.898438, prec 0.05888, recall 0.835874
2017-12-09T23:04:36.150954: step 1324, loss 0.250424, acc 0.890625, prec 0.0588612, recall 0.835874
2017-12-09T23:04:36.450230: step 1325, loss 0.540442, acc 0.859375, prec 0.0589014, recall 0.836034
2017-12-09T23:04:36.754013: step 1326, loss 0.390585, acc 0.875, prec 0.0589014, recall 0.836087
2017-12-09T23:04:37.048089: step 1327, loss 0.338887, acc 0.90625, prec 0.0588853, recall 0.836087
2017-12-09T23:04:37.340584: step 1328, loss 0.282202, acc 0.890625, prec 0.0588665, recall 0.836087
2017-12-09T23:04:37.638319: step 1329, loss 0.337735, acc 0.890625, prec 0.0588691, recall 0.83614
2017-12-09T23:04:37.932070: step 1330, loss 0.201364, acc 0.90625, prec 0.0588959, recall 0.836246
2017-12-09T23:04:38.224130: step 1331, loss 0.273771, acc 0.90625, prec 0.0588798, recall 0.836246
2017-12-09T23:04:38.520928: step 1332, loss 0.0781702, acc 0.984375, prec 0.05892, recall 0.836352
2017-12-09T23:04:38.817995: step 1333, loss 0.612831, acc 0.9375, prec 0.0589736, recall 0.83651
2017-12-09T23:04:39.121646: step 1334, loss 0.141521, acc 0.976562, prec 0.058991, recall 0.836563
2017-12-09T23:04:39.416806: step 1335, loss 0.0851588, acc 0.976562, prec 0.0590084, recall 0.836616
2017-12-09T23:04:39.716388: step 1336, loss 0.263475, acc 0.960938, prec 0.059066, recall 0.836774
2017-12-09T23:04:40.017887: step 1337, loss 0.107842, acc 0.96875, prec 0.0591249, recall 0.836932
2017-12-09T23:04:40.316509: step 1338, loss 0.253901, acc 0.976562, prec 0.0591636, recall 0.837037
2017-12-09T23:04:40.614653: step 1339, loss 1.68586, acc 0.960938, prec 0.0591797, recall 0.83682
2017-12-09T23:04:40.912739: step 1340, loss 0.1681, acc 0.9375, prec 0.0591689, recall 0.83682
2017-12-09T23:04:41.209330: step 1341, loss 0.260239, acc 0.9375, prec 0.0592224, recall 0.836977
2017-12-09T23:04:41.508617: step 1342, loss 0.23037, acc 0.945312, prec 0.0592557, recall 0.837082
2017-12-09T23:04:41.811386: step 1343, loss 1.02682, acc 0.929688, prec 0.0592663, recall 0.836866
2017-12-09T23:04:42.109027: step 1344, loss 0.364006, acc 0.945312, prec 0.0592997, recall 0.83697
2017-12-09T23:04:42.412527: step 1345, loss 0.487456, acc 0.953125, prec 0.0593771, recall 0.837179
2017-12-09T23:04:42.706550: step 1346, loss 0.334064, acc 0.914062, prec 0.059405, recall 0.837284
2017-12-09T23:04:43.003552: step 1347, loss 0.407042, acc 0.859375, prec 0.0594021, recall 0.837336
2017-12-09T23:04:43.301391: step 1348, loss 0.254327, acc 0.898438, prec 0.0594273, recall 0.83744
2017-12-09T23:04:43.592791: step 1349, loss 0.345668, acc 0.867188, prec 0.0594684, recall 0.837596
2017-12-09T23:04:43.893324: step 1350, loss 0.340364, acc 0.914062, prec 0.0595389, recall 0.837803
2017-12-09T23:04:44.192046: step 1351, loss 0.492159, acc 0.789062, prec 0.0595025, recall 0.837803
2017-12-09T23:04:44.486940: step 1352, loss 1.10446, acc 0.828125, prec 0.0595168, recall 0.83764
2017-12-09T23:04:44.782031: step 1353, loss 0.347497, acc 0.867188, prec 0.0594939, recall 0.83764
2017-12-09T23:04:45.078028: step 1354, loss 0.409239, acc 0.875, prec 0.0595575, recall 0.837846
2017-12-09T23:04:45.377398: step 1355, loss 0.456054, acc 0.851562, prec 0.0595958, recall 0.838001
2017-12-09T23:04:45.673634: step 1356, loss 0.449818, acc 0.882812, prec 0.0595968, recall 0.838053
2017-12-09T23:04:45.971447: step 1357, loss 1.01998, acc 0.8125, prec 0.0596495, recall 0.838259
2017-12-09T23:04:46.271412: step 1358, loss 0.695877, acc 0.890625, prec 0.0596732, recall 0.838361
2017-12-09T23:04:46.569144: step 1359, loss 0.483744, acc 0.851562, prec 0.0596688, recall 0.838413
2017-12-09T23:04:46.871277: step 1360, loss 0.278736, acc 0.929688, prec 0.0597416, recall 0.838618
2017-12-09T23:04:47.170624: step 1361, loss 0.256303, acc 0.890625, prec 0.0597652, recall 0.83872
2017-12-09T23:04:47.467188: step 1362, loss 0.601362, acc 0.921875, prec 0.059879, recall 0.839026
2017-12-09T23:04:47.764199: step 1363, loss 0.185981, acc 0.90625, prec 0.0599264, recall 0.839179
2017-12-09T23:04:48.063460: step 1364, loss 0.169209, acc 0.921875, prec 0.0599129, recall 0.839179
2017-12-09T23:04:48.356920: step 1365, loss 0.417879, acc 0.859375, prec 0.059931, recall 0.83928
2017-12-09T23:04:48.654747: step 1366, loss 0.374832, acc 0.914062, prec 0.0599585, recall 0.839382
2017-12-09T23:04:48.949653: step 1367, loss 0.333096, acc 0.875, prec 0.0599581, recall 0.839432
2017-12-09T23:04:49.249059: step 1368, loss 0.181253, acc 0.945312, prec 0.059991, recall 0.839533
2017-12-09T23:04:49.547634: step 1369, loss 0.227828, acc 0.898438, prec 0.0600158, recall 0.839635
2017-12-09T23:04:49.840832: step 1370, loss 0.229499, acc 0.9375, prec 0.060005, recall 0.839635
2017-12-09T23:04:50.137914: step 1371, loss 0.297594, acc 0.921875, prec 0.0600338, recall 0.839736
2017-12-09T23:04:50.452596: step 1372, loss 0.111713, acc 0.96875, prec 0.0600495, recall 0.839786
2017-12-09T23:04:50.750844: step 1373, loss 1.44966, acc 0.90625, prec 0.0600981, recall 0.839673
2017-12-09T23:04:51.055428: step 1374, loss 0.243953, acc 0.9375, prec 0.0601507, recall 0.839824
2017-12-09T23:04:51.349444: step 1375, loss 0.322615, acc 0.9375, prec 0.0601822, recall 0.839925
2017-12-09T23:04:51.651989: step 1376, loss 0.35155, acc 0.960938, prec 0.0602599, recall 0.840125
2017-12-09T23:04:51.951845: step 1377, loss 0.536808, acc 0.960938, prec 0.0603588, recall 0.840376
2017-12-09T23:04:52.250174: step 1378, loss 0.200719, acc 0.9375, prec 0.0604113, recall 0.840525
2017-12-09T23:04:52.550846: step 1379, loss 0.113526, acc 0.9375, prec 0.0604004, recall 0.840525
2017-12-09T23:04:52.848507: step 1380, loss 0.297165, acc 0.882812, prec 0.0604223, recall 0.840625
2017-12-09T23:04:53.148592: step 1381, loss 0.275864, acc 0.914062, prec 0.0604707, recall 0.840774
2017-12-09T23:04:53.452053: step 1382, loss 0.300137, acc 0.9375, prec 0.060502, recall 0.840874
2017-12-09T23:04:53.753348: step 1383, loss 0.161603, acc 0.945312, prec 0.0604925, recall 0.840874
2017-12-09T23:04:54.053581: step 1384, loss 0.695774, acc 0.90625, prec 0.0604973, recall 0.840923
2017-12-09T23:04:54.355203: step 1385, loss 0.185034, acc 0.929688, prec 0.0605061, recall 0.840973
2017-12-09T23:04:54.656865: step 1386, loss 0.11591, acc 0.96875, prec 0.0605428, recall 0.841072
2017-12-09T23:04:54.953700: step 1387, loss 0.207194, acc 0.9375, prec 0.0605531, recall 0.841121
2017-12-09T23:04:55.252628: step 1388, loss 0.315268, acc 0.945312, prec 0.0605857, recall 0.84122
2017-12-09T23:04:55.554663: step 1389, loss 0.31182, acc 0.890625, prec 0.0606088, recall 0.841319
2017-12-09T23:04:55.849576: step 1390, loss 0.117798, acc 0.953125, prec 0.0606006, recall 0.841319
2017-12-09T23:04:56.150161: step 1391, loss 0.193742, acc 0.921875, prec 0.0606291, recall 0.841418
2017-12-09T23:04:56.441858: step 1392, loss 0.220003, acc 0.929688, prec 0.0606169, recall 0.841418
2017-12-09T23:04:56.737463: step 1393, loss 0.338411, acc 0.914062, prec 0.0606861, recall 0.841615
2017-12-09T23:04:57.038880: step 1394, loss 0.212903, acc 0.914062, prec 0.0606922, recall 0.841664
2017-12-09T23:04:57.338801: step 1395, loss 0.318351, acc 0.953125, prec 0.0607892, recall 0.841909
2017-12-09T23:04:57.637079: step 1396, loss 1.81464, acc 0.96875, prec 0.0608271, recall 0.841747
2017-12-09T23:04:57.942064: step 1397, loss 0.130769, acc 0.953125, prec 0.060819, recall 0.841747
2017-12-09T23:04:58.243675: step 1398, loss 0.15501, acc 0.929688, prec 0.0608488, recall 0.841845
2017-12-09T23:04:58.540787: step 1399, loss 0.16468, acc 0.9375, prec 0.0608799, recall 0.841942
2017-12-09T23:04:58.835967: step 1400, loss 0.238238, acc 0.898438, prec 0.0609042, recall 0.84204
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1400

2017-12-09T23:05:00.129043: step 1401, loss 0.27116, acc 0.929688, prec 0.0609129, recall 0.842089
2017-12-09T23:05:00.433851: step 1402, loss 0.170272, acc 0.953125, prec 0.0609047, recall 0.842089
2017-12-09T23:05:00.727282: step 1403, loss 0.260282, acc 0.921875, prec 0.0608911, recall 0.842089
2017-12-09T23:05:01.025621: step 1404, loss 0.210795, acc 0.921875, prec 0.0609405, recall 0.842235
2017-12-09T23:05:01.322072: step 1405, loss 0.274138, acc 0.945312, prec 0.0609729, recall 0.842333
2017-12-09T23:05:01.627700: step 1406, loss 0.373537, acc 0.882812, prec 0.0609944, recall 0.84243
2017-12-09T23:05:01.925088: step 1407, loss 0.216792, acc 0.929688, prec 0.061066, recall 0.842624
2017-12-09T23:05:02.218008: step 1408, loss 0.275332, acc 0.898438, prec 0.0611111, recall 0.842769
2017-12-09T23:05:02.513017: step 1409, loss 0.241219, acc 0.921875, prec 0.0611394, recall 0.842866
2017-12-09T23:05:02.813430: step 1410, loss 0.250114, acc 0.96875, prec 0.0611758, recall 0.842963
2017-12-09T23:05:03.111972: step 1411, loss 0.214435, acc 0.914062, prec 0.0611817, recall 0.843011
2017-12-09T23:05:03.408770: step 1412, loss 0.141792, acc 0.953125, prec 0.0611735, recall 0.843011
2017-12-09T23:05:03.706599: step 1413, loss 0.211871, acc 0.96875, prec 0.0612309, recall 0.843155
2017-12-09T23:05:04.008183: step 1414, loss 1.73202, acc 0.960938, prec 0.0612463, recall 0.842945
2017-12-09T23:05:04.310095: step 1415, loss 0.16043, acc 0.960938, prec 0.0612604, recall 0.842993
2017-12-09T23:05:04.607254: step 1416, loss 0.333546, acc 0.984375, prec 0.0612995, recall 0.843089
2017-12-09T23:05:04.906619: step 1417, loss 0.405795, acc 0.953125, prec 0.0613332, recall 0.843185
2017-12-09T23:05:05.204609: step 1418, loss 0.221192, acc 0.921875, prec 0.0613404, recall 0.843233
2017-12-09T23:05:05.518120: step 1419, loss 0.344948, acc 0.914062, prec 0.061409, recall 0.843425
2017-12-09T23:05:05.816791: step 1420, loss 0.277011, acc 0.914062, prec 0.0614148, recall 0.843473
2017-12-09T23:05:06.116230: step 1421, loss 0.244733, acc 0.929688, prec 0.0614234, recall 0.843521
2017-12-09T23:05:06.421485: step 1422, loss 0.246648, acc 0.914062, prec 0.0614084, recall 0.843521
2017-12-09T23:05:06.723998: step 1423, loss 0.783785, acc 0.914062, prec 0.061456, recall 0.843664
2017-12-09T23:05:07.023522: step 1424, loss 0.259548, acc 0.921875, prec 0.0615049, recall 0.843807
2017-12-09T23:05:07.322297: step 1425, loss 0.33149, acc 0.890625, prec 0.0615067, recall 0.843855
2017-12-09T23:05:07.623087: step 1426, loss 0.27994, acc 0.890625, prec 0.0614875, recall 0.843855
2017-12-09T23:05:07.918004: step 1427, loss 0.428852, acc 0.875, prec 0.0615074, recall 0.84395
2017-12-09T23:05:08.215505: step 1428, loss 0.852692, acc 0.882812, prec 0.0615494, recall 0.844093
2017-12-09T23:05:08.514138: step 1429, loss 0.19486, acc 0.9375, prec 0.0615801, recall 0.844187
2017-12-09T23:05:08.819989: step 1430, loss 0.170499, acc 0.945312, prec 0.0616539, recall 0.844377
2017-12-09T23:05:09.121638: step 1431, loss 0.314992, acc 0.90625, prec 0.0616791, recall 0.844471
2017-12-09T23:05:09.415715: step 1432, loss 0.385856, acc 0.851562, prec 0.0616739, recall 0.844519
2017-12-09T23:05:09.715894: step 1433, loss 0.34608, acc 0.875, prec 0.061652, recall 0.844519
2017-12-09T23:05:10.013613: step 1434, loss 0.176838, acc 0.90625, prec 0.0616564, recall 0.844566
2017-12-09T23:05:10.310099: step 1435, loss 0.250721, acc 0.921875, prec 0.0616844, recall 0.84466
2017-12-09T23:05:10.609222: step 1436, loss 0.416131, acc 0.929688, prec 0.0617344, recall 0.844801
2017-12-09T23:05:10.907393: step 1437, loss 0.267078, acc 0.96875, prec 0.0617497, recall 0.844849
2017-12-09T23:05:11.204615: step 1438, loss 0.317431, acc 0.921875, prec 0.0617984, recall 0.844989
2017-12-09T23:05:11.504446: step 1439, loss 0.353444, acc 0.953125, prec 0.0618525, recall 0.84513
2017-12-09T23:05:11.802675: step 1440, loss 0.206135, acc 0.9375, prec 0.0618415, recall 0.84513
2017-12-09T23:05:12.098354: step 1441, loss 0.42122, acc 0.9375, prec 0.0618929, recall 0.84527
2017-12-09T23:05:12.395411: step 1442, loss 0.235277, acc 0.953125, prec 0.0619677, recall 0.845457
2017-12-09T23:05:12.694047: step 1443, loss 0.694052, acc 0.945312, prec 0.0620411, recall 0.845644
2017-12-09T23:05:12.996565: step 1444, loss 0.399863, acc 0.929688, prec 0.0620702, recall 0.845737
2017-12-09T23:05:13.291764: step 1445, loss 0.246484, acc 0.921875, prec 0.0620979, recall 0.84583
2017-12-09T23:05:13.594531: step 1446, loss 0.19363, acc 0.945312, prec 0.0621091, recall 0.845876
2017-12-09T23:05:13.891369: step 1447, loss 0.512249, acc 0.9375, prec 0.0621395, recall 0.845969
2017-12-09T23:05:14.193951: step 1448, loss 0.196373, acc 0.953125, prec 0.062152, recall 0.846015
2017-12-09T23:05:14.489122: step 1449, loss 0.311509, acc 0.898438, prec 0.0621342, recall 0.846015
2017-12-09T23:05:14.789515: step 1450, loss 0.33218, acc 0.898438, prec 0.0621785, recall 0.846154
2017-12-09T23:05:15.086727: step 1451, loss 0.24176, acc 0.929688, prec 0.0622075, recall 0.846246
2017-12-09T23:05:15.388864: step 1452, loss 0.865007, acc 0.9375, prec 0.0622186, recall 0.846038
2017-12-09T23:05:15.687545: step 1453, loss 0.243893, acc 0.914062, prec 0.0622242, recall 0.846085
2017-12-09T23:05:15.982142: step 1454, loss 0.392895, acc 0.890625, prec 0.062205, recall 0.846085
2017-12-09T23:05:16.284973: step 1455, loss 0.42928, acc 0.90625, prec 0.0622505, recall 0.846223
2017-12-09T23:05:16.582653: step 1456, loss 0.26129, acc 0.867188, prec 0.0622479, recall 0.846269
2017-12-09T23:05:16.881010: step 1457, loss 0.630422, acc 0.890625, prec 0.0622494, recall 0.846315
2017-12-09T23:05:17.178839: step 1458, loss 0.600561, acc 0.898438, prec 0.0622728, recall 0.846407
2017-12-09T23:05:17.478201: step 1459, loss 0.371158, acc 0.921875, prec 0.0622591, recall 0.846407
2017-12-09T23:05:17.773978: step 1460, loss 0.313193, acc 0.90625, prec 0.0622427, recall 0.846407
2017-12-09T23:05:18.069653: step 1461, loss 0.299418, acc 0.914062, prec 0.0622482, recall 0.846453
2017-12-09T23:05:18.370383: step 1462, loss 0.220739, acc 0.929688, prec 0.0622566, recall 0.846499
2017-12-09T23:05:18.665647: step 1463, loss 0.286603, acc 0.90625, prec 0.0622607, recall 0.846545
2017-12-09T23:05:18.964712: step 1464, loss 0.229572, acc 0.929688, prec 0.062269, recall 0.846591
2017-12-09T23:05:19.265808: step 1465, loss 0.609426, acc 0.960938, prec 0.0623034, recall 0.846683
2017-12-09T23:05:19.564473: step 1466, loss 0.246614, acc 0.945312, prec 0.0623351, recall 0.846774
2017-12-09T23:05:19.861171: step 1467, loss 0.376746, acc 0.90625, prec 0.0623599, recall 0.846866
2017-12-09T23:05:20.163663: step 1468, loss 0.147919, acc 0.953125, prec 0.0623723, recall 0.846911
2017-12-09T23:05:20.467006: step 1469, loss 0.223977, acc 0.945312, prec 0.0624039, recall 0.847003
2017-12-09T23:05:20.766376: step 1470, loss 1.08063, acc 0.875, prec 0.0624451, recall 0.846887
2017-12-09T23:05:21.067118: step 1471, loss 0.362845, acc 0.914062, prec 0.0624506, recall 0.846933
2017-12-09T23:05:21.361981: step 1472, loss 0.237745, acc 0.921875, prec 0.062478, recall 0.847024
2017-12-09T23:05:21.661119: step 1473, loss 0.325386, acc 0.9375, prec 0.06257, recall 0.847251
2017-12-09T23:05:21.964477: step 1474, loss 0.241381, acc 0.898438, prec 0.0625932, recall 0.847342
2017-12-09T23:05:22.268250: step 1475, loss 0.363343, acc 0.914062, prec 0.0626398, recall 0.847478
2017-12-09T23:05:22.563016: step 1476, loss 0.231586, acc 0.898438, prec 0.0626631, recall 0.847568
2017-12-09T23:05:22.860444: step 1477, loss 0.241517, acc 0.921875, prec 0.0626699, recall 0.847613
2017-12-09T23:05:23.161561: step 1478, loss 0.269826, acc 0.945312, prec 0.0627014, recall 0.847704
2017-12-09T23:05:23.461425: step 1479, loss 0.363181, acc 0.90625, prec 0.0627259, recall 0.847794
2017-12-09T23:05:23.759122: step 1480, loss 0.139783, acc 0.945312, prec 0.0627369, recall 0.847839
2017-12-09T23:05:24.057931: step 1481, loss 0.233794, acc 0.921875, prec 0.0627436, recall 0.847884
2017-12-09T23:05:24.357633: step 1482, loss 0.141684, acc 0.953125, prec 0.0627354, recall 0.847884
2017-12-09T23:05:24.654563: step 1483, loss 0.176295, acc 0.9375, prec 0.0627449, recall 0.847929
2017-12-09T23:05:24.953082: step 1484, loss 0.151884, acc 0.945312, prec 0.0627558, recall 0.847974
2017-12-09T23:05:25.250074: step 1485, loss 0.103153, acc 0.96875, prec 0.0627503, recall 0.847974
2017-12-09T23:05:25.542908: step 1486, loss 0.400007, acc 0.921875, prec 0.0627776, recall 0.848064
2017-12-09T23:05:25.843638: step 1487, loss 0.235729, acc 0.953125, prec 0.0628309, recall 0.848198
2017-12-09T23:05:26.138360: step 1488, loss 0.176744, acc 0.929688, prec 0.0628595, recall 0.848288
2017-12-09T23:05:26.436891: step 1489, loss 3.93403, acc 0.960938, prec 0.0629155, recall 0.848172
2017-12-09T23:05:26.743887: step 1490, loss 1.02284, acc 0.9375, prec 0.0629264, recall 0.847967
2017-12-09T23:05:27.051639: step 1491, loss 0.252737, acc 0.960938, prec 0.0629605, recall 0.848057
2017-12-09T23:05:27.353014: step 1492, loss 0.228489, acc 0.929688, prec 0.062989, recall 0.848146
2017-12-09T23:05:27.647231: step 1493, loss 0.181639, acc 0.921875, prec 0.0629753, recall 0.848146
2017-12-09T23:05:27.824371: step 1494, loss 0.517642, acc 0.865385, prec 0.0629656, recall 0.848146
2017-12-09T23:05:28.126313: step 1495, loss 0.375936, acc 0.867188, prec 0.0630036, recall 0.84828
2017-12-09T23:05:28.421734: step 1496, loss 0.289043, acc 0.875, prec 0.0629816, recall 0.84828
2017-12-09T23:05:28.719829: step 1497, loss 0.366538, acc 0.914062, prec 0.0630074, recall 0.848369
2017-12-09T23:05:29.017354: step 1498, loss 0.331909, acc 0.898438, prec 0.0630304, recall 0.848458
2017-12-09T23:05:29.313574: step 1499, loss 0.206933, acc 0.9375, prec 0.0630603, recall 0.848547
2017-12-09T23:05:29.608608: step 1500, loss 0.225249, acc 0.898438, prec 0.0631242, recall 0.848725

Evaluation:
2017-12-09T23:05:34.297336: step 1500, loss 1.56617, acc 0.900925, prec 0.0636888, recall 0.838474

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1500

2017-12-09T23:05:35.629540: step 1501, loss 0.19263, acc 0.921875, prec 0.0637351, recall 0.83861
2017-12-09T23:05:35.929140: step 1502, loss 0.759926, acc 0.875, prec 0.0637931, recall 0.838791
2017-12-09T23:05:36.228597: step 1503, loss 0.242544, acc 0.882812, prec 0.0637927, recall 0.838836
2017-12-09T23:05:36.531473: step 1504, loss 0.456329, acc 0.90625, prec 0.0638162, recall 0.838926
2017-12-09T23:05:36.833210: step 1505, loss 0.209674, acc 0.9375, prec 0.0638452, recall 0.839016
2017-12-09T23:05:37.133040: step 1506, loss 0.269627, acc 0.929688, prec 0.0638728, recall 0.839106
2017-12-09T23:05:37.437136: step 1507, loss 0.200525, acc 0.9375, prec 0.0639017, recall 0.839196
2017-12-09T23:05:37.735855: step 1508, loss 0.214044, acc 0.921875, prec 0.063908, recall 0.839241
2017-12-09T23:05:38.037091: step 1509, loss 0.27997, acc 0.9375, prec 0.0639568, recall 0.839375
2017-12-09T23:05:38.336740: step 1510, loss 0.205147, acc 0.9375, prec 0.063946, recall 0.839375
2017-12-09T23:05:38.633402: step 1511, loss 0.334271, acc 0.984375, prec 0.0640228, recall 0.839554
2017-12-09T23:05:38.932760: step 1512, loss 0.370947, acc 0.953125, prec 0.0640345, recall 0.839599
2017-12-09T23:05:39.241741: step 1513, loss 0.110046, acc 0.953125, prec 0.0640462, recall 0.839644
2017-12-09T23:05:39.542947: step 1514, loss 0.793048, acc 0.953125, prec 0.0640977, recall 0.839777
2017-12-09T23:05:39.837952: step 1515, loss 0.197614, acc 0.9375, prec 0.0641066, recall 0.839822
2017-12-09T23:05:40.139588: step 1516, loss 0.242696, acc 0.929688, prec 0.0641143, recall 0.839867
2017-12-09T23:05:40.438997: step 1517, loss 0.369415, acc 0.96875, prec 0.0641684, recall 0.84
2017-12-09T23:05:40.746663: step 1518, loss 0.139386, acc 0.9375, prec 0.0641575, recall 0.84
2017-12-09T23:05:41.043783: step 1519, loss 0.101295, acc 0.96875, prec 0.0641719, recall 0.840044
2017-12-09T23:05:41.337272: step 1520, loss 0.147432, acc 0.929688, prec 0.0641994, recall 0.840133
2017-12-09T23:05:41.637771: step 1521, loss 0.289234, acc 0.914062, prec 0.0642439, recall 0.840266
2017-12-09T23:05:41.932445: step 1522, loss 0.186769, acc 0.929688, prec 0.0642713, recall 0.840355
2017-12-09T23:05:42.228788: step 1523, loss 0.228534, acc 0.90625, prec 0.0642946, recall 0.840443
2017-12-09T23:05:42.525579: step 1524, loss 0.172645, acc 0.9375, prec 0.0643036, recall 0.840487
2017-12-09T23:05:42.824401: step 1525, loss 0.166332, acc 0.945312, prec 0.0643139, recall 0.840532
2017-12-09T23:05:43.118403: step 1526, loss 0.737908, acc 0.960938, prec 0.0643679, recall 0.840431
2017-12-09T23:05:43.415017: step 1527, loss 0.200042, acc 0.914062, prec 0.0643925, recall 0.84052
2017-12-09T23:05:43.715330: step 1528, loss 0.174875, acc 0.9375, prec 0.0644014, recall 0.840564
2017-12-09T23:05:44.009809: step 1529, loss 0.682234, acc 0.921875, prec 0.0644274, recall 0.840652
2017-12-09T23:05:44.304936: step 1530, loss 0.249192, acc 0.90625, prec 0.0644506, recall 0.84074
2017-12-09T23:05:44.601461: step 1531, loss 0.272203, acc 0.953125, prec 0.064482, recall 0.840828
2017-12-09T23:05:44.900931: step 1532, loss 0.262855, acc 0.9375, prec 0.0645107, recall 0.840915
2017-12-09T23:05:45.198158: step 1533, loss 0.969646, acc 0.875, prec 0.0645877, recall 0.841134
2017-12-09T23:05:45.499063: step 1534, loss 0.347901, acc 0.890625, prec 0.0646477, recall 0.841309
2017-12-09T23:05:45.792368: step 1535, loss 0.584331, acc 0.867188, prec 0.064664, recall 0.841396
2017-12-09T23:05:46.087541: step 1536, loss 0.316576, acc 0.890625, prec 0.0646646, recall 0.84144
2017-12-09T23:05:46.385546: step 1537, loss 0.385765, acc 0.898438, prec 0.0646666, recall 0.841484
2017-12-09T23:05:46.682196: step 1538, loss 0.633573, acc 0.945312, prec 0.0647163, recall 0.841614
2017-12-09T23:05:46.980710: step 1539, loss 0.126304, acc 0.953125, prec 0.0647673, recall 0.841744
2017-12-09T23:05:47.283705: step 1540, loss 1.05124, acc 0.882812, prec 0.0648271, recall 0.841687
2017-12-09T23:05:47.586211: step 1541, loss 0.196435, acc 0.921875, prec 0.0648134, recall 0.841687
2017-12-09T23:05:47.888185: step 1542, loss 0.235287, acc 0.898438, prec 0.0648154, recall 0.841731
2017-12-09T23:05:48.180157: step 1543, loss 0.150009, acc 0.945312, prec 0.064865, recall 0.84186
2017-12-09T23:05:48.477130: step 1544, loss 0.217961, acc 0.945312, prec 0.0648751, recall 0.841904
2017-12-09T23:05:48.776937: step 1545, loss 0.212646, acc 0.945312, prec 0.0648656, recall 0.841904
2017-12-09T23:05:49.073089: step 1546, loss 0.341569, acc 0.90625, prec 0.0648689, recall 0.841947
2017-12-09T23:05:49.371538: step 1547, loss 0.246893, acc 0.921875, prec 0.0648552, recall 0.841947
2017-12-09T23:05:49.666717: step 1548, loss 0.202379, acc 0.9375, prec 0.064864, recall 0.84199
2017-12-09T23:05:49.962773: step 1549, loss 0.155375, acc 0.921875, prec 0.06487, recall 0.842033
2017-12-09T23:05:50.266738: step 1550, loss 0.308936, acc 0.929688, prec 0.0648971, recall 0.84212
2017-12-09T23:05:50.571586: step 1551, loss 0.312667, acc 0.929688, prec 0.0649635, recall 0.842292
2017-12-09T23:05:50.870818: step 1552, loss 0.0867702, acc 0.960938, prec 0.0649763, recall 0.842335
2017-12-09T23:05:51.166342: step 1553, loss 0.219069, acc 0.96875, prec 0.0650495, recall 0.842507
2017-12-09T23:05:51.465639: step 1554, loss 0.278011, acc 0.960938, prec 0.0650624, recall 0.84255
2017-12-09T23:05:51.770315: step 1555, loss 0.113567, acc 0.953125, prec 0.0650738, recall 0.842593
2017-12-09T23:05:52.068074: step 1556, loss 0.283371, acc 0.960938, prec 0.0650866, recall 0.842635
2017-12-09T23:05:52.381338: step 1557, loss 0.104995, acc 0.953125, prec 0.0650981, recall 0.842678
2017-12-09T23:05:52.675974: step 1558, loss 1.17633, acc 0.929688, prec 0.0651068, recall 0.842492
2017-12-09T23:05:52.971387: step 1559, loss 0.570886, acc 0.96875, prec 0.0651799, recall 0.842663
2017-12-09T23:05:53.269505: step 1560, loss 0.165772, acc 0.9375, prec 0.065169, recall 0.842663
2017-12-09T23:05:53.572894: step 1561, loss 0.241529, acc 0.9375, prec 0.0651973, recall 0.842749
2017-12-09T23:05:53.874631: step 1562, loss 0.274109, acc 0.945312, prec 0.0652073, recall 0.842791
2017-12-09T23:05:54.176711: step 1563, loss 0.198501, acc 0.960938, prec 0.0652201, recall 0.842834
2017-12-09T23:05:54.474620: step 1564, loss 0.272958, acc 0.945312, prec 0.0652694, recall 0.842962
2017-12-09T23:05:54.770328: step 1565, loss 0.20269, acc 0.921875, prec 0.0652754, recall 0.843004
2017-12-09T23:05:55.072741: step 1566, loss 0.236693, acc 0.945312, prec 0.0653639, recall 0.843217
2017-12-09T23:05:55.371563: step 1567, loss 0.409186, acc 0.882812, prec 0.0653825, recall 0.843302
2017-12-09T23:05:55.668316: step 1568, loss 0.282413, acc 0.882812, prec 0.0653815, recall 0.843344
2017-12-09T23:05:55.964250: step 1569, loss 1.09388, acc 0.875, prec 0.0654002, recall 0.843201
2017-12-09T23:05:56.265206: step 1570, loss 0.276672, acc 0.90625, prec 0.0653837, recall 0.843201
2017-12-09T23:05:56.563134: step 1571, loss 0.305589, acc 0.867188, prec 0.0653996, recall 0.843286
2017-12-09T23:05:56.860150: step 1572, loss 0.295201, acc 0.921875, prec 0.0654055, recall 0.843328
2017-12-09T23:05:57.155436: step 1573, loss 0.381384, acc 0.898438, prec 0.0654855, recall 0.843539
2017-12-09T23:05:57.451082: step 1574, loss 1.01201, acc 0.890625, prec 0.0655068, recall 0.843396
2017-12-09T23:05:57.754612: step 1575, loss 0.426576, acc 0.851562, prec 0.0655199, recall 0.843481
2017-12-09T23:05:58.058165: step 1576, loss 0.750816, acc 0.84375, prec 0.065512, recall 0.843523
2017-12-09T23:05:58.366530: step 1577, loss 0.258788, acc 0.929688, prec 0.0655193, recall 0.843565
2017-12-09T23:05:58.663326: step 1578, loss 0.37294, acc 0.835938, prec 0.0654905, recall 0.843565
2017-12-09T23:05:58.958903: step 1579, loss 0.397102, acc 0.867188, prec 0.0655649, recall 0.843775
2017-12-09T23:05:59.259722: step 1580, loss 0.165861, acc 0.9375, prec 0.0656125, recall 0.843901
2017-12-09T23:05:59.553781: step 1581, loss 0.213108, acc 0.914062, prec 0.0656364, recall 0.843985
2017-12-09T23:05:59.853449: step 1582, loss 0.235672, acc 0.945312, prec 0.0656854, recall 0.844111
2017-12-09T23:06:00.159927: step 1583, loss 0.386692, acc 0.851562, prec 0.0656983, recall 0.844194
2017-12-09T23:06:00.466950: step 1584, loss 0.231831, acc 0.929688, prec 0.0657445, recall 0.844319
2017-12-09T23:06:00.767885: step 1585, loss 0.533638, acc 0.960938, prec 0.0658156, recall 0.844486
2017-12-09T23:06:01.068513: step 1586, loss 0.309078, acc 0.921875, prec 0.0658213, recall 0.844528
2017-12-09T23:06:01.370593: step 1587, loss 0.294216, acc 0.929688, prec 0.0658674, recall 0.844652
2017-12-09T23:06:01.675838: step 1588, loss 0.133368, acc 0.96875, prec 0.0659203, recall 0.844777
2017-12-09T23:06:01.977768: step 1589, loss 0.393597, acc 0.960938, prec 0.0659329, recall 0.844818
2017-12-09T23:06:02.280583: step 1590, loss 0.127903, acc 0.953125, prec 0.0659247, recall 0.844818
2017-12-09T23:06:02.578583: step 1591, loss 2.46683, acc 0.953125, prec 0.0659178, recall 0.844593
2017-12-09T23:06:02.884810: step 1592, loss 0.194135, acc 0.9375, prec 0.0659457, recall 0.844676
2017-12-09T23:06:03.180873: step 1593, loss 0.255205, acc 0.976562, prec 0.066, recall 0.8448
2017-12-09T23:06:03.479127: step 1594, loss 0.305203, acc 0.929688, prec 0.0660265, recall 0.844883
2017-12-09T23:06:03.781117: step 1595, loss 0.287934, acc 0.914062, prec 0.0660698, recall 0.845007
2017-12-09T23:06:04.079753: step 1596, loss 0.274461, acc 0.890625, prec 0.0660699, recall 0.845048
2017-12-09T23:06:04.378458: step 1597, loss 0.443337, acc 0.859375, prec 0.0660452, recall 0.845048
2017-12-09T23:06:04.675718: step 1598, loss 0.329054, acc 0.882812, prec 0.066044, recall 0.845089
2017-12-09T23:06:04.974758: step 1599, loss 0.410699, acc 0.875, prec 0.0660415, recall 0.84513
2017-12-09T23:06:05.277675: step 1600, loss 0.309664, acc 0.914062, prec 0.0661428, recall 0.845377
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1600

2017-12-09T23:06:06.611089: step 1601, loss 0.31023, acc 0.898438, prec 0.0661444, recall 0.845418
2017-12-09T23:06:06.907789: step 1602, loss 0.552468, acc 0.921875, prec 0.0661694, recall 0.8455
2017-12-09T23:06:07.206138: step 1603, loss 0.257086, acc 0.921875, prec 0.0661751, recall 0.845541
2017-12-09T23:06:07.508912: step 1604, loss 0.488197, acc 0.84375, prec 0.066167, recall 0.845582
2017-12-09T23:06:07.809934: step 1605, loss 0.252503, acc 0.890625, prec 0.0661478, recall 0.845582
2017-12-09T23:06:08.121676: step 1606, loss 0.414698, acc 0.914062, prec 0.0661521, recall 0.845623
2017-12-09T23:06:08.420139: step 1607, loss 0.121684, acc 0.976562, prec 0.0661867, recall 0.845705
2017-12-09T23:06:08.719166: step 1608, loss 0.233153, acc 0.945312, prec 0.0662158, recall 0.845787
2017-12-09T23:06:09.015095: step 1609, loss 0.34753, acc 0.945312, prec 0.0662643, recall 0.845909
2017-12-09T23:06:09.317276: step 1610, loss 0.21261, acc 0.953125, prec 0.0662948, recall 0.845991
2017-12-09T23:06:09.615451: step 1611, loss 0.30831, acc 0.945312, prec 0.0663432, recall 0.846113
2017-12-09T23:06:09.914021: step 1612, loss 0.20517, acc 0.921875, prec 0.0663295, recall 0.846113
2017-12-09T23:06:10.216462: step 1613, loss 0.176685, acc 0.9375, prec 0.0663378, recall 0.846154
2017-12-09T23:06:10.511823: step 1614, loss 0.873781, acc 0.9375, prec 0.0664056, recall 0.846093
2017-12-09T23:06:10.811111: step 1615, loss 0.382235, acc 0.90625, prec 0.0664084, recall 0.846134
2017-12-09T23:06:11.105342: step 1616, loss 0.0951455, acc 0.976562, prec 0.0664043, recall 0.846134
2017-12-09T23:06:11.409361: step 1617, loss 0.209442, acc 0.96875, prec 0.0664568, recall 0.846255
2017-12-09T23:06:11.707379: step 1618, loss 0.393385, acc 0.96875, prec 0.06649, recall 0.846336
2017-12-09T23:06:12.008769: step 1619, loss 0.195928, acc 0.929688, prec 0.0665162, recall 0.846417
2017-12-09T23:06:12.307127: step 1620, loss 0.177514, acc 0.96875, prec 0.0666073, recall 0.846619
2017-12-09T23:06:12.603496: step 1621, loss 0.12332, acc 0.945312, prec 0.066617, recall 0.84666
2017-12-09T23:06:12.899891: step 1622, loss 0.134858, acc 0.945312, prec 0.0666073, recall 0.84666
2017-12-09T23:06:13.191113: step 1623, loss 0.233759, acc 0.960938, prec 0.0666391, recall 0.84674
2017-12-09T23:06:13.497720: step 1624, loss 0.132384, acc 0.953125, prec 0.0666308, recall 0.84674
2017-12-09T23:06:13.792989: step 1625, loss 0.198502, acc 0.921875, prec 0.0666556, recall 0.846821
2017-12-09T23:06:14.088630: step 1626, loss 0.888741, acc 0.9375, prec 0.0666653, recall 0.846639
2017-12-09T23:06:14.391566: step 1627, loss 0.281616, acc 0.953125, prec 0.0666956, recall 0.846719
2017-12-09T23:06:14.697954: step 1628, loss 0.148354, acc 0.945312, prec 0.066686, recall 0.846719
2017-12-09T23:06:14.992617: step 1629, loss 0.400887, acc 0.96875, prec 0.066719, recall 0.8468
2017-12-09T23:06:15.296873: step 1630, loss 0.200327, acc 0.953125, prec 0.06673, recall 0.84684
2017-12-09T23:06:15.592552: step 1631, loss 0.374601, acc 0.945312, prec 0.0667782, recall 0.84696
2017-12-09T23:06:15.889562: step 1632, loss 1.31538, acc 0.9375, prec 0.0668071, recall 0.846819
2017-12-09T23:06:16.191740: step 1633, loss 0.229487, acc 0.953125, prec 0.0668374, recall 0.846899
2017-12-09T23:06:16.487970: step 1634, loss 0.241249, acc 0.914062, prec 0.0668222, recall 0.846899
2017-12-09T23:06:16.781991: step 1635, loss 0.256035, acc 0.914062, prec 0.0668456, recall 0.846979
2017-12-09T23:06:17.083898: step 1636, loss 0.53859, acc 0.851562, prec 0.0668194, recall 0.846979
2017-12-09T23:06:17.382937: step 1637, loss 0.243691, acc 0.914062, prec 0.066862, recall 0.847099
2017-12-09T23:06:17.675482: step 1638, loss 0.222358, acc 0.914062, prec 0.066866, recall 0.847139
2017-12-09T23:06:17.970066: step 1639, loss 0.362903, acc 0.890625, prec 0.0668852, recall 0.847219
2017-12-09T23:06:18.267255: step 1640, loss 0.367059, acc 0.914062, prec 0.0668701, recall 0.847219
2017-12-09T23:06:18.565445: step 1641, loss 0.377122, acc 0.875, prec 0.0668865, recall 0.847298
2017-12-09T23:06:18.865207: step 1642, loss 0.343865, acc 0.890625, prec 0.0669056, recall 0.847378
2017-12-09T23:06:19.163354: step 1643, loss 0.352864, acc 0.9375, prec 0.066933, recall 0.847458
2017-12-09T23:06:19.468543: step 1644, loss 0.316875, acc 0.859375, prec 0.0669466, recall 0.847537
2017-12-09T23:06:19.765905: step 1645, loss 0.251, acc 0.914062, prec 0.0669891, recall 0.847656
2017-12-09T23:06:20.067883: step 1646, loss 0.300436, acc 0.953125, prec 0.0670192, recall 0.847736
2017-12-09T23:06:20.370836: step 1647, loss 0.135798, acc 0.96875, prec 0.0670329, recall 0.847775
2017-12-09T23:06:20.674754: step 1648, loss 0.0936963, acc 0.9375, prec 0.0670219, recall 0.847775
2017-12-09T23:06:20.971933: step 1649, loss 0.17576, acc 0.945312, prec 0.0670314, recall 0.847815
2017-12-09T23:06:21.269933: step 1650, loss 0.341937, acc 0.945312, prec 0.0670793, recall 0.847933
2017-12-09T23:06:21.575545: step 1651, loss 0.32305, acc 0.953125, prec 0.0670902, recall 0.847973
2017-12-09T23:06:21.880859: step 1652, loss 0.438925, acc 0.945312, prec 0.0671381, recall 0.848091
2017-12-09T23:06:22.173766: step 1653, loss 0.0737489, acc 0.992188, prec 0.0671751, recall 0.84817
2017-12-09T23:06:22.470621: step 1654, loss 0.127129, acc 0.976562, prec 0.0672284, recall 0.848288
2017-12-09T23:06:22.776505: step 1655, loss 0.293714, acc 0.90625, prec 0.0672694, recall 0.848406
2017-12-09T23:06:23.077430: step 1656, loss 0.201624, acc 0.960938, prec 0.0673199, recall 0.848524
2017-12-09T23:06:23.380586: step 1657, loss 1.21968, acc 0.898438, prec 0.0673417, recall 0.848383
2017-12-09T23:06:23.694858: step 1658, loss 0.122392, acc 0.96875, prec 0.0673553, recall 0.848422
2017-12-09T23:06:23.995776: step 1659, loss 0.231128, acc 0.96875, prec 0.0674072, recall 0.84854
2017-12-09T23:06:24.294848: step 1660, loss 0.109271, acc 0.953125, prec 0.0673989, recall 0.84854
2017-12-09T23:06:24.597093: step 1661, loss 0.202004, acc 0.945312, prec 0.0674275, recall 0.848618
2017-12-09T23:06:24.892992: step 1662, loss 0.0834414, acc 0.96875, prec 0.0674602, recall 0.848696
2017-12-09T23:06:25.185086: step 1663, loss 0.290337, acc 0.914062, prec 0.0674833, recall 0.848774
2017-12-09T23:06:25.482365: step 1664, loss 0.153048, acc 0.953125, prec 0.067475, recall 0.848774
2017-12-09T23:06:25.780620: step 1665, loss 0.236209, acc 0.914062, prec 0.067498, recall 0.848852
2017-12-09T23:06:26.080825: step 1666, loss 0.25511, acc 0.953125, prec 0.0675471, recall 0.848969
2017-12-09T23:06:26.377896: step 1667, loss 0.369219, acc 0.9375, prec 0.0676124, recall 0.849125
2017-12-09T23:06:26.678274: step 1668, loss 0.134266, acc 0.960938, prec 0.0676055, recall 0.849125
2017-12-09T23:06:26.974098: step 1669, loss 0.16089, acc 0.960938, prec 0.0675986, recall 0.849125
2017-12-09T23:06:27.276034: step 1670, loss 0.2335, acc 0.921875, prec 0.067623, recall 0.849202
2017-12-09T23:06:27.575812: step 1671, loss 0.125761, acc 0.984375, prec 0.0676775, recall 0.849319
2017-12-09T23:06:27.874648: step 1672, loss 0.177052, acc 0.921875, prec 0.0677209, recall 0.849435
2017-12-09T23:06:28.175865: step 1673, loss 0.103338, acc 0.960938, prec 0.0677331, recall 0.849473
2017-12-09T23:06:28.473148: step 1674, loss 0.112011, acc 0.960938, prec 0.0677261, recall 0.849473
2017-12-09T23:06:28.771821: step 1675, loss 0.107623, acc 0.953125, prec 0.0677369, recall 0.849512
2017-12-09T23:06:29.071843: step 1676, loss 0.1634, acc 0.960938, prec 0.0677491, recall 0.849551
2017-12-09T23:06:29.377576: step 1677, loss 0.145425, acc 0.992188, prec 0.0678049, recall 0.849666
2017-12-09T23:06:29.677701: step 1678, loss 0.0672414, acc 0.96875, prec 0.0677994, recall 0.849666
2017-12-09T23:06:29.978088: step 1679, loss 0.0718551, acc 0.976562, prec 0.0678525, recall 0.849782
2017-12-09T23:06:30.282137: step 1680, loss 0.297755, acc 0.976562, prec 0.0679055, recall 0.849898
2017-12-09T23:06:30.585210: step 1681, loss 0.144126, acc 0.953125, prec 0.0679163, recall 0.849936
2017-12-09T23:06:30.888901: step 1682, loss 0.0892834, acc 0.960938, prec 0.0679475, recall 0.850013
2017-12-09T23:06:31.193000: step 1683, loss 0.0626755, acc 0.976562, prec 0.0679624, recall 0.850051
2017-12-09T23:06:31.498690: step 1684, loss 0.137707, acc 0.953125, prec 0.067954, recall 0.850051
2017-12-09T23:06:31.805136: step 1685, loss 0.0751523, acc 0.96875, prec 0.0679485, recall 0.850051
2017-12-09T23:06:32.103712: step 1686, loss 0.234679, acc 0.960938, prec 0.0680178, recall 0.850205
2017-12-09T23:06:32.407728: step 1687, loss 0.314287, acc 0.976562, prec 0.0680707, recall 0.850319
2017-12-09T23:06:32.709878: step 1688, loss 1.50212, acc 0.960938, prec 0.0680842, recall 0.85014
2017-12-09T23:06:33.010176: step 1689, loss 0.089875, acc 0.96875, prec 0.0681168, recall 0.850217
2017-12-09T23:06:33.304439: step 1690, loss 0.726874, acc 0.9375, prec 0.0681818, recall 0.85037
2017-12-09T23:06:33.614381: step 1691, loss 0.15336, acc 0.929688, prec 0.0681693, recall 0.85037
2017-12-09T23:06:33.913695: step 1692, loss 0.131067, acc 0.960938, prec 0.0681814, recall 0.850408
2017-12-09T23:06:34.214577: step 1693, loss 0.266704, acc 0.921875, prec 0.0682245, recall 0.850522
2017-12-09T23:06:34.521209: step 1694, loss 0.445942, acc 0.914062, prec 0.0682663, recall 0.850636
2017-12-09T23:06:34.817628: step 1695, loss 0.228781, acc 0.9375, prec 0.0682551, recall 0.850636
2017-12-09T23:06:35.116719: step 1696, loss 0.350482, acc 0.953125, prec 0.0683038, recall 0.85075
2017-12-09T23:06:35.429510: step 1697, loss 0.312222, acc 0.90625, prec 0.0683251, recall 0.850826
2017-12-09T23:06:35.733428: step 1698, loss 0.260926, acc 0.890625, prec 0.0683056, recall 0.850826
2017-12-09T23:06:36.031648: step 1699, loss 0.40285, acc 0.898438, prec 0.0683445, recall 0.85094
2017-12-09T23:06:36.335968: step 1700, loss 0.240036, acc 0.898438, prec 0.0683264, recall 0.85094
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1700

2017-12-09T23:06:37.627588: step 1701, loss 0.38608, acc 0.835938, prec 0.0683161, recall 0.850977
2017-12-09T23:06:37.929162: step 1702, loss 0.553531, acc 0.796875, prec 0.0682989, recall 0.851015
2017-12-09T23:06:38.225838: step 1703, loss 0.218866, acc 0.898438, prec 0.0682998, recall 0.851053
2017-12-09T23:06:38.521495: step 1704, loss 0.340548, acc 0.90625, prec 0.0683021, recall 0.851091
2017-12-09T23:06:38.821074: step 1705, loss 0.332338, acc 0.859375, prec 0.068315, recall 0.851166
2017-12-09T23:06:39.117779: step 1706, loss 0.874255, acc 0.945312, prec 0.0683256, recall 0.850988
2017-12-09T23:06:39.421498: step 1707, loss 0.119069, acc 0.96875, prec 0.0683201, recall 0.850988
2017-12-09T23:06:39.721120: step 1708, loss 0.576608, acc 0.929688, prec 0.0683265, recall 0.851026
2017-12-09T23:06:40.017705: step 1709, loss 0.303902, acc 0.921875, prec 0.0683316, recall 0.851064
2017-12-09T23:06:40.316776: step 1710, loss 0.168523, acc 0.9375, prec 0.0683205, recall 0.851064
2017-12-09T23:06:40.614436: step 1711, loss 0.196817, acc 0.953125, prec 0.0683311, recall 0.851102
2017-12-09T23:06:40.909724: step 1712, loss 0.40281, acc 0.9375, prec 0.0684146, recall 0.85129
2017-12-09T23:06:41.207545: step 1713, loss 0.243087, acc 0.960938, prec 0.0684455, recall 0.851365
2017-12-09T23:06:41.503504: step 1714, loss 0.173426, acc 0.9375, prec 0.0684533, recall 0.851403
2017-12-09T23:06:41.804234: step 1715, loss 0.540191, acc 0.929688, prec 0.0684787, recall 0.851478
2017-12-09T23:06:42.104255: step 1716, loss 0.173484, acc 0.953125, prec 0.0684893, recall 0.851515
2017-12-09T23:06:42.400594: step 1717, loss 0.120405, acc 0.976562, prec 0.068504, recall 0.851553
2017-12-09T23:06:42.696771: step 1718, loss 0.201007, acc 0.953125, prec 0.0685713, recall 0.851702
2017-12-09T23:06:42.994470: step 1719, loss 0.170427, acc 0.976562, prec 0.0686428, recall 0.851852
2017-12-09T23:06:43.294381: step 1720, loss 0.186562, acc 0.9375, prec 0.0686694, recall 0.851926
2017-12-09T23:06:43.590429: step 1721, loss 0.422295, acc 0.960938, prec 0.0687192, recall 0.852038
2017-12-09T23:06:43.894517: step 1722, loss 0.276777, acc 0.929688, prec 0.0687255, recall 0.852075
2017-12-09T23:06:44.190154: step 1723, loss 0.0931414, acc 0.984375, prec 0.0687416, recall 0.852113
2017-12-09T23:06:44.488084: step 1724, loss 0.324602, acc 0.921875, prec 0.0687466, recall 0.85215
2017-12-09T23:06:44.792880: step 1725, loss 0.369012, acc 0.90625, prec 0.0687865, recall 0.852261
2017-12-09T23:06:45.095105: step 1726, loss 0.408862, acc 0.960938, prec 0.0687984, recall 0.852298
2017-12-09T23:06:45.393864: step 1727, loss 0.155982, acc 0.945312, prec 0.0688642, recall 0.852447
2017-12-09T23:06:45.696271: step 1728, loss 0.137128, acc 0.953125, prec 0.0688747, recall 0.852484
2017-12-09T23:06:45.996292: step 1729, loss 0.22744, acc 0.945312, prec 0.0689026, recall 0.852558
2017-12-09T23:06:46.295782: step 1730, loss 0.0997304, acc 0.960938, prec 0.0688956, recall 0.852558
2017-12-09T23:06:46.594574: step 1731, loss 0.182892, acc 0.945312, prec 0.0689047, recall 0.852595
2017-12-09T23:06:46.893582: step 1732, loss 1.08331, acc 0.945312, prec 0.0689529, recall 0.852492
2017-12-09T23:06:47.193644: step 1733, loss 0.560794, acc 0.953125, prec 0.0690388, recall 0.852676
2017-12-09T23:06:47.493760: step 1734, loss 0.183413, acc 0.960938, prec 0.0690507, recall 0.852713
2017-12-09T23:06:47.795895: step 1735, loss 0.478447, acc 0.945312, prec 0.0691352, recall 0.852897
2017-12-09T23:06:48.102685: step 1736, loss 0.135284, acc 0.953125, prec 0.0691268, recall 0.852897
2017-12-09T23:06:48.399666: step 1737, loss 0.146355, acc 0.945312, prec 0.0691546, recall 0.852971
2017-12-09T23:06:48.701331: step 1738, loss 0.13882, acc 0.921875, prec 0.0691595, recall 0.853007
2017-12-09T23:06:48.996815: step 1739, loss 0.19369, acc 0.921875, prec 0.0691643, recall 0.853044
2017-12-09T23:06:49.295880: step 1740, loss 0.336839, acc 0.914062, prec 0.0691678, recall 0.853081
2017-12-09T23:06:49.596831: step 1741, loss 0.722901, acc 0.84375, prec 0.0691774, recall 0.853154
2017-12-09T23:06:49.896399: step 1742, loss 0.199117, acc 0.945312, prec 0.0692241, recall 0.853264
2017-12-09T23:06:50.074357: step 1743, loss 0.888517, acc 0.923077, prec 0.0692561, recall 0.853337
2017-12-09T23:06:50.389402: step 1744, loss 0.494016, acc 0.882812, prec 0.0692539, recall 0.853373
2017-12-09T23:06:50.703237: step 1745, loss 0.202774, acc 0.929688, prec 0.0692601, recall 0.85341
2017-12-09T23:06:50.997342: step 1746, loss 0.227751, acc 0.898438, prec 0.0692983, recall 0.853519
2017-12-09T23:06:51.295512: step 1747, loss 0.266303, acc 0.914062, prec 0.0693205, recall 0.853592
2017-12-09T23:06:51.594146: step 1748, loss 0.175388, acc 0.9375, prec 0.0693657, recall 0.853701
2017-12-09T23:06:51.900150: step 1749, loss 0.307104, acc 0.898438, prec 0.0694226, recall 0.853846
2017-12-09T23:06:52.202234: step 1750, loss 0.385568, acc 0.898438, prec 0.0694232, recall 0.853882
2017-12-09T23:06:52.496581: step 1751, loss 0.208556, acc 0.921875, prec 0.0694467, recall 0.853955
2017-12-09T23:06:52.795546: step 1752, loss 0.268264, acc 0.914062, prec 0.0694688, recall 0.854027
2017-12-09T23:06:53.092678: step 1753, loss 0.250417, acc 0.929688, prec 0.0695125, recall 0.854136
2017-12-09T23:06:53.391508: step 1754, loss 0.304223, acc 0.929688, prec 0.0694999, recall 0.854136
2017-12-09T23:06:53.687280: step 1755, loss 0.329095, acc 0.929688, prec 0.0695435, recall 0.854244
2017-12-09T23:06:53.987122: step 1756, loss 0.194591, acc 0.953125, prec 0.0695538, recall 0.85428
2017-12-09T23:06:54.285528: step 1757, loss 0.2188, acc 0.945312, prec 0.0695815, recall 0.854352
2017-12-09T23:06:54.582450: step 1758, loss 0.275794, acc 0.921875, prec 0.0695862, recall 0.854388
2017-12-09T23:06:54.879612: step 1759, loss 0.197787, acc 0.953125, prec 0.0695966, recall 0.854424
2017-12-09T23:06:55.177690: step 1760, loss 0.0986932, acc 0.953125, prec 0.0696069, recall 0.85446
2017-12-09T23:06:55.474736: step 1761, loss 0.223382, acc 0.984375, prec 0.0696603, recall 0.854568
2017-12-09T23:06:55.776015: step 1762, loss 0.0861202, acc 0.976562, prec 0.0696748, recall 0.854604
2017-12-09T23:06:56.073871: step 1763, loss 0.230166, acc 0.992188, prec 0.0697295, recall 0.854711
2017-12-09T23:06:56.372893: step 1764, loss 0.209871, acc 0.984375, prec 0.0698016, recall 0.854855
2017-12-09T23:06:56.676477: step 1765, loss 0.065429, acc 0.976562, prec 0.0698161, recall 0.85489
2017-12-09T23:06:56.977679: step 1766, loss 0.0635394, acc 0.976562, prec 0.0698306, recall 0.854926
2017-12-09T23:06:57.275062: step 1767, loss 0.287119, acc 0.96875, prec 0.0698811, recall 0.855033
2017-12-09T23:06:57.580139: step 1768, loss 0.946274, acc 0.953125, prec 0.0699302, recall 0.85493
2017-12-09T23:06:57.882345: step 1769, loss 0.0150204, acc 1, prec 0.0699302, recall 0.85493
2017-12-09T23:06:58.185694: step 1770, loss 0.0605488, acc 0.984375, prec 0.0699274, recall 0.85493
2017-12-09T23:06:58.481385: step 1771, loss 0.181297, acc 0.953125, prec 0.0699938, recall 0.855072
2017-12-09T23:06:58.775164: step 1772, loss 0.0462722, acc 0.992188, prec 0.0699924, recall 0.855072
2017-12-09T23:06:59.071541: step 1773, loss 0.173053, acc 0.921875, prec 0.0700531, recall 0.855215
2017-12-09T23:06:59.375859: step 1774, loss 0.0893246, acc 0.976562, prec 0.0700862, recall 0.855286
2017-12-09T23:06:59.673729: step 1775, loss 0.200687, acc 0.984375, prec 0.0701395, recall 0.855392
2017-12-09T23:06:59.976067: step 1776, loss 0.165926, acc 0.945312, prec 0.070167, recall 0.855463
2017-12-09T23:07:00.284884: step 1777, loss 0.205988, acc 0.960938, prec 0.070216, recall 0.855569
2017-12-09T23:07:00.589572: step 1778, loss 0.509222, acc 0.96875, prec 0.0702664, recall 0.855675
2017-12-09T23:07:00.888737: step 1779, loss 0.224768, acc 0.929688, prec 0.070291, recall 0.855746
2017-12-09T23:07:01.190324: step 1780, loss 0.141136, acc 0.953125, prec 0.0703012, recall 0.855781
2017-12-09T23:07:01.489427: step 1781, loss 0.15302, acc 0.9375, prec 0.0703086, recall 0.855816
2017-12-09T23:07:01.787625: step 1782, loss 0.127561, acc 0.9375, prec 0.0703533, recall 0.855922
2017-12-09T23:07:02.085565: step 1783, loss 0.204157, acc 0.921875, prec 0.0703392, recall 0.855922
2017-12-09T23:07:02.385270: step 1784, loss 0.257047, acc 0.90625, prec 0.0703595, recall 0.855992
2017-12-09T23:07:02.686048: step 1785, loss 0.074039, acc 0.96875, prec 0.0703539, recall 0.855992
2017-12-09T23:07:02.983732: step 1786, loss 0.179361, acc 0.960938, prec 0.0704214, recall 0.856133
2017-12-09T23:07:03.283173: step 1787, loss 0.211602, acc 0.921875, prec 0.0704259, recall 0.856168
2017-12-09T23:07:03.579049: step 1788, loss 0.192849, acc 0.921875, prec 0.0704304, recall 0.856203
2017-12-09T23:07:03.879779: step 1789, loss 0.684757, acc 0.953125, prec 0.0704965, recall 0.856343
2017-12-09T23:07:04.177853: step 1790, loss 0.394288, acc 0.9375, prec 0.0705225, recall 0.856413
2017-12-09T23:07:04.479217: step 1791, loss 0.100564, acc 0.945312, prec 0.0705126, recall 0.856413
2017-12-09T23:07:04.781289: step 1792, loss 0.260861, acc 0.9375, prec 0.0705199, recall 0.856448
2017-12-09T23:07:05.076472: step 1793, loss 0.10101, acc 0.96875, prec 0.0705142, recall 0.856448
2017-12-09T23:07:05.386468: step 1794, loss 0.156688, acc 0.9375, prec 0.0705029, recall 0.856448
2017-12-09T23:07:05.689678: step 1795, loss 0.0581532, acc 0.976562, prec 0.0705359, recall 0.856517
2017-12-09T23:07:05.983996: step 1796, loss 0.0377449, acc 0.984375, prec 0.0705703, recall 0.856587
2017-12-09T23:07:06.280581: step 1797, loss 0.196098, acc 0.953125, prec 0.0705805, recall 0.856622
2017-12-09T23:07:06.581860: step 1798, loss 0.120439, acc 0.953125, prec 0.0705906, recall 0.856657
2017-12-09T23:07:06.880857: step 1799, loss 1.46043, acc 0.96875, prec 0.0706236, recall 0.856519
2017-12-09T23:07:07.187480: step 1800, loss 0.135528, acc 0.960938, prec 0.0706165, recall 0.856519

Evaluation:
2017-12-09T23:07:11.917629: step 1800, loss 2.3691, acc 0.94348, prec 0.0712194, recall 0.842302

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1800

2017-12-09T23:07:13.213105: step 1801, loss 0.200458, acc 0.921875, prec 0.0712605, recall 0.842413
2017-12-09T23:07:13.511901: step 1802, loss 0.111557, acc 0.960938, prec 0.0712718, recall 0.84245
2017-12-09T23:07:13.811702: step 1803, loss 0.288416, acc 0.960938, prec 0.0713198, recall 0.84256
2017-12-09T23:07:14.115291: step 1804, loss 0.26259, acc 0.90625, prec 0.0713396, recall 0.842634
2017-12-09T23:07:14.411486: step 1805, loss 0.168227, acc 0.945312, prec 0.0713848, recall 0.842744
2017-12-09T23:07:14.711364: step 1806, loss 0.144824, acc 0.96875, prec 0.0713975, recall 0.84278
2017-12-09T23:07:15.005502: step 1807, loss 0.147018, acc 0.992188, prec 0.0714512, recall 0.84289
2017-12-09T23:07:15.306002: step 1808, loss 0.374454, acc 0.945312, prec 0.0714963, recall 0.843
2017-12-09T23:07:15.605569: step 1809, loss 0.349085, acc 0.953125, prec 0.0715062, recall 0.843037
2017-12-09T23:07:15.906813: step 1810, loss 0.176248, acc 0.953125, prec 0.0715344, recall 0.84311
2017-12-09T23:07:16.204528: step 1811, loss 0.289016, acc 0.921875, prec 0.0715202, recall 0.84311
2017-12-09T23:07:16.507567: step 1812, loss 0.286728, acc 0.9375, prec 0.0715823, recall 0.843256
2017-12-09T23:07:16.801747: step 1813, loss 0.168162, acc 0.953125, prec 0.0716104, recall 0.843329
2017-12-09T23:07:17.098066: step 1814, loss 0.247187, acc 0.945312, prec 0.0716189, recall 0.843365
2017-12-09T23:07:17.398377: step 1815, loss 0.136435, acc 0.921875, prec 0.0716231, recall 0.843401
2017-12-09T23:07:17.696152: step 1816, loss 0.254057, acc 0.9375, prec 0.0716484, recall 0.843474
2017-12-09T23:07:17.994004: step 1817, loss 0.144468, acc 0.9375, prec 0.0716554, recall 0.843511
2017-12-09T23:07:18.290174: step 1818, loss 0.0747306, acc 0.953125, prec 0.0716652, recall 0.843547
2017-12-09T23:07:18.585792: step 1819, loss 0.27294, acc 0.9375, prec 0.0717271, recall 0.843692
2017-12-09T23:07:18.883238: step 1820, loss 0.11576, acc 0.96875, prec 0.0717398, recall 0.843728
2017-12-09T23:07:19.180088: step 1821, loss 0.113364, acc 0.96875, prec 0.0717341, recall 0.843728
2017-12-09T23:07:19.477792: step 1822, loss 0.102873, acc 0.96875, prec 0.0717468, recall 0.843764
2017-12-09T23:07:19.781707: step 1823, loss 0.190728, acc 0.960938, prec 0.0717763, recall 0.843837
2017-12-09T23:07:20.080378: step 1824, loss 0.0515935, acc 0.96875, prec 0.0717706, recall 0.843837
2017-12-09T23:07:20.382696: step 1825, loss 0.184879, acc 0.960938, prec 0.0717818, recall 0.843873
2017-12-09T23:07:20.684029: step 1826, loss 0.0741137, acc 0.960938, prec 0.0717748, recall 0.843873
2017-12-09T23:07:20.979174: step 1827, loss 0.0562478, acc 0.976562, prec 0.0717888, recall 0.843909
2017-12-09T23:07:21.272235: step 1828, loss 0.302651, acc 0.929688, prec 0.0718127, recall 0.843982
2017-12-09T23:07:21.571126: step 1829, loss 0.104168, acc 0.96875, prec 0.0718801, recall 0.844126
2017-12-09T23:07:21.872882: step 1830, loss 0.143944, acc 0.984375, prec 0.0719138, recall 0.844198
2017-12-09T23:07:22.175750: step 1831, loss 0.0761683, acc 0.96875, prec 0.0719264, recall 0.844234
2017-12-09T23:07:22.478529: step 1832, loss 0.0687177, acc 0.992188, prec 0.0719433, recall 0.84427
2017-12-09T23:07:22.774289: step 1833, loss 0.062957, acc 0.992188, prec 0.0719602, recall 0.844306
2017-12-09T23:07:23.075720: step 1834, loss 0.204155, acc 0.945312, prec 0.072005, recall 0.844414
2017-12-09T23:07:23.376155: step 1835, loss 0.196567, acc 0.96875, prec 0.072109, recall 0.844629
2017-12-09T23:07:23.675270: step 1836, loss 1.75647, acc 0.992188, prec 0.0721272, recall 0.84447
2017-12-09T23:07:23.977698: step 1837, loss 0.780512, acc 0.960938, prec 0.0722479, recall 0.84472
2017-12-09T23:07:24.273602: step 1838, loss 0.144529, acc 0.9375, prec 0.0722548, recall 0.844756
2017-12-09T23:07:24.571098: step 1839, loss 0.243004, acc 0.9375, prec 0.0722982, recall 0.844863
2017-12-09T23:07:24.870410: step 1840, loss 0.185752, acc 0.945312, prec 0.0723247, recall 0.844935
2017-12-09T23:07:25.163003: step 1841, loss 0.252767, acc 0.890625, prec 0.0723048, recall 0.844935
2017-12-09T23:07:25.462496: step 1842, loss 0.277747, acc 0.914062, prec 0.0723074, recall 0.84497
2017-12-09T23:07:25.765338: step 1843, loss 0.414383, acc 0.84375, prec 0.072279, recall 0.84497
2017-12-09T23:07:26.061374: step 1844, loss 0.386656, acc 0.90625, prec 0.0723166, recall 0.845077
2017-12-09T23:07:26.360585: step 1845, loss 0.206555, acc 0.9375, prec 0.0723599, recall 0.845183
2017-12-09T23:07:26.661201: step 1846, loss 0.199359, acc 0.945312, prec 0.0723682, recall 0.845219
2017-12-09T23:07:26.962735: step 1847, loss 0.341252, acc 0.90625, prec 0.0724057, recall 0.845325
2017-12-09T23:07:27.262135: step 1848, loss 0.259383, acc 0.9375, prec 0.0725036, recall 0.845538
2017-12-09T23:07:27.561023: step 1849, loss 0.216465, acc 0.9375, prec 0.0725468, recall 0.845644
2017-12-09T23:07:27.871342: step 1850, loss 0.301059, acc 0.890625, prec 0.0725632, recall 0.845714
2017-12-09T23:07:28.167907: step 1851, loss 0.282533, acc 0.898438, prec 0.0725993, recall 0.84582
2017-12-09T23:07:28.469020: step 1852, loss 0.172493, acc 0.9375, prec 0.0726061, recall 0.845855
2017-12-09T23:07:28.765314: step 1853, loss 0.341298, acc 0.960938, prec 0.0726353, recall 0.845926
2017-12-09T23:07:29.060993: step 1854, loss 0.240469, acc 0.960938, prec 0.0726646, recall 0.845996
2017-12-09T23:07:29.355086: step 1855, loss 0.15655, acc 0.945312, prec 0.0726728, recall 0.846031
2017-12-09T23:07:29.652092: step 1856, loss 0.128541, acc 0.921875, prec 0.0726767, recall 0.846066
2017-12-09T23:07:29.953969: step 1857, loss 0.198226, acc 0.9375, prec 0.0726835, recall 0.846101
2017-12-09T23:07:30.260542: step 1858, loss 0.200904, acc 0.960938, prec 0.0726945, recall 0.846136
2017-12-09T23:07:30.564335: step 1859, loss 0.113914, acc 0.945312, prec 0.0726845, recall 0.846136
2017-12-09T23:07:30.865896: step 1860, loss 0.0722032, acc 0.96875, prec 0.0727152, recall 0.846206
2017-12-09T23:07:31.172868: step 1861, loss 0.215266, acc 0.976562, prec 0.0727472, recall 0.846276
2017-12-09T23:07:31.474080: step 1862, loss 0.256477, acc 0.960938, prec 0.0727945, recall 0.846381
2017-12-09T23:07:31.776122: step 1863, loss 0.0684738, acc 0.976562, prec 0.0728084, recall 0.846416
2017-12-09T23:07:32.079123: step 1864, loss 0.6142, acc 0.953125, prec 0.0728906, recall 0.846591
2017-12-09T23:07:32.383688: step 1865, loss 0.15248, acc 0.976562, prec 0.0729226, recall 0.846661
2017-12-09T23:07:32.677732: step 1866, loss 1.13708, acc 0.960938, prec 0.072935, recall 0.846503
2017-12-09T23:07:32.980844: step 1867, loss 2.13494, acc 0.953125, prec 0.0729823, recall 0.846416
2017-12-09T23:07:33.282718: step 1868, loss 0.182911, acc 0.960938, prec 0.0730295, recall 0.84652
2017-12-09T23:07:33.578023: step 1869, loss 0.207775, acc 0.90625, prec 0.0730305, recall 0.846555
2017-12-09T23:07:33.873763: step 1870, loss 0.153782, acc 0.9375, prec 0.0730735, recall 0.846659
2017-12-09T23:07:34.170509: step 1871, loss 0.165402, acc 0.945312, prec 0.0730635, recall 0.846659
2017-12-09T23:07:34.467541: step 1872, loss 0.259911, acc 0.898438, prec 0.0730811, recall 0.846729
2017-12-09T23:07:34.768446: step 1873, loss 0.295389, acc 0.867188, prec 0.0730569, recall 0.846729
2017-12-09T23:07:35.065616: step 1874, loss 0.215773, acc 0.929688, prec 0.073044, recall 0.846729
2017-12-09T23:07:35.369440: step 1875, loss 0.400974, acc 0.851562, prec 0.0730531, recall 0.846798
2017-12-09T23:07:35.674487: step 1876, loss 0.493577, acc 0.882812, prec 0.0730679, recall 0.846867
2017-12-09T23:07:35.969926: step 1877, loss 0.349514, acc 0.882812, prec 0.0730646, recall 0.846902
2017-12-09T23:07:36.263922: step 1878, loss 0.605346, acc 0.851562, prec 0.0730918, recall 0.847006
2017-12-09T23:07:36.562196: step 1879, loss 0.426711, acc 0.859375, prec 0.0731203, recall 0.847109
2017-12-09T23:07:36.862816: step 1880, loss 0.462703, acc 0.828125, prec 0.073107, recall 0.847144
2017-12-09T23:07:37.157846: step 1881, loss 0.271573, acc 0.882812, prec 0.0731399, recall 0.847247
2017-12-09T23:07:37.459091: step 1882, loss 0.227089, acc 0.890625, prec 0.0731741, recall 0.847351
2017-12-09T23:07:37.752950: step 1883, loss 0.187716, acc 0.929688, prec 0.0731793, recall 0.847385
2017-12-09T23:07:38.052826: step 1884, loss 0.17666, acc 0.929688, prec 0.0731665, recall 0.847385
2017-12-09T23:07:38.349320: step 1885, loss 0.263677, acc 0.921875, prec 0.0731703, recall 0.847419
2017-12-09T23:07:38.648480: step 1886, loss 0.199236, acc 0.90625, prec 0.0731532, recall 0.847419
2017-12-09T23:07:38.945076: step 1887, loss 0.185882, acc 0.9375, prec 0.0731598, recall 0.847454
2017-12-09T23:07:39.240390: step 1888, loss 0.134928, acc 0.960938, prec 0.0731707, recall 0.847488
2017-12-09T23:07:39.540491: step 1889, loss 0.198462, acc 0.9375, prec 0.0731774, recall 0.847522
2017-12-09T23:07:39.842106: step 1890, loss 0.277325, acc 0.921875, prec 0.0732172, recall 0.847625
2017-12-09T23:07:40.144795: step 1891, loss 1.09176, acc 0.976562, prec 0.0732504, recall 0.847503
2017-12-09T23:07:40.446913: step 1892, loss 0.3032, acc 0.953125, prec 0.0732959, recall 0.847606
2017-12-09T23:07:40.749481: step 1893, loss 0.300346, acc 0.976562, prec 0.0733096, recall 0.84764
2017-12-09T23:07:41.049018: step 1894, loss 0.156729, acc 0.953125, prec 0.0733191, recall 0.847675
2017-12-09T23:07:41.344513: step 1895, loss 0.221336, acc 0.9375, prec 0.0733257, recall 0.847709
2017-12-09T23:07:41.642071: step 1896, loss 0.410995, acc 0.96875, prec 0.073392, recall 0.847846
2017-12-09T23:07:41.943756: step 1897, loss 0.135611, acc 0.976562, prec 0.0734237, recall 0.847914
2017-12-09T23:07:42.242595: step 1898, loss 0.136668, acc 0.96875, prec 0.073454, recall 0.847982
2017-12-09T23:07:42.545607: step 1899, loss 0.285955, acc 0.9375, prec 0.0734426, recall 0.847982
2017-12-09T23:07:42.837335: step 1900, loss 0.0701379, acc 0.960938, prec 0.0734355, recall 0.847982
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-1900

2017-12-09T23:07:44.117974: step 1901, loss 0.232777, acc 0.96875, prec 0.0735916, recall 0.848288
2017-12-09T23:07:44.420828: step 1902, loss 0.11129, acc 0.960938, prec 0.0735845, recall 0.848288
2017-12-09T23:07:44.715920: step 1903, loss 0.105073, acc 0.960938, prec 0.0735774, recall 0.848288
2017-12-09T23:07:45.012969: step 1904, loss 0.195221, acc 0.929688, prec 0.0735825, recall 0.848322
2017-12-09T23:07:45.309677: step 1905, loss 0.345909, acc 0.960938, prec 0.0736652, recall 0.848492
2017-12-09T23:07:45.609534: step 1906, loss 0.226754, acc 0.914062, prec 0.0736495, recall 0.848492
2017-12-09T23:07:45.908061: step 1907, loss 0.18184, acc 0.96875, prec 0.0736797, recall 0.848559
2017-12-09T23:07:46.205032: step 1908, loss 0.133672, acc 0.953125, prec 0.0736891, recall 0.848593
2017-12-09T23:07:46.502507: step 1909, loss 0.0851673, acc 0.960938, prec 0.0736999, recall 0.848627
2017-12-09T23:07:46.799685: step 1910, loss 0.54266, acc 0.945312, prec 0.0737438, recall 0.848728
2017-12-09T23:07:47.101950: step 1911, loss 0.15625, acc 0.945312, prec 0.0737338, recall 0.848728
2017-12-09T23:07:47.403351: step 1912, loss 0.313127, acc 0.96875, prec 0.0737819, recall 0.848829
2017-12-09T23:07:47.704833: step 1913, loss 0.106448, acc 0.96875, prec 0.0737942, recall 0.848863
2017-12-09T23:07:48.006665: step 1914, loss 0.0965974, acc 0.976562, prec 0.0738617, recall 0.848998
2017-12-09T23:07:48.302535: step 1915, loss 1.68592, acc 0.953125, prec 0.0738545, recall 0.848809
2017-12-09T23:07:48.604866: step 1916, loss 0.164751, acc 0.945312, prec 0.0738624, recall 0.848842
2017-12-09T23:07:48.901720: step 1917, loss 0.214235, acc 0.882812, prec 0.0738948, recall 0.848943
2017-12-09T23:07:49.197073: step 1918, loss 0.391244, acc 0.914062, prec 0.0739328, recall 0.849044
2017-12-09T23:07:49.497613: step 1919, loss 0.359316, acc 0.929688, prec 0.0739558, recall 0.849111
2017-12-09T23:07:49.798409: step 1920, loss 0.238826, acc 0.914062, prec 0.0740117, recall 0.849245
2017-12-09T23:07:50.095817: step 1921, loss 0.230735, acc 0.929688, prec 0.0740167, recall 0.849279
2017-12-09T23:07:50.399801: step 1922, loss 0.26145, acc 0.929688, prec 0.0740576, recall 0.849379
2017-12-09T23:07:50.706202: step 1923, loss 0.29101, acc 0.882812, prec 0.074054, recall 0.849412
2017-12-09T23:07:51.005200: step 1924, loss 0.661844, acc 0.882812, prec 0.0740504, recall 0.849446
2017-12-09T23:07:51.298873: step 1925, loss 0.309564, acc 0.875, prec 0.0740633, recall 0.849512
2017-12-09T23:07:51.597326: step 1926, loss 0.273956, acc 0.914062, prec 0.0741013, recall 0.849612
2017-12-09T23:07:51.891532: step 1927, loss 0.245301, acc 0.90625, prec 0.0741377, recall 0.849712
2017-12-09T23:07:52.188709: step 1928, loss 0.194081, acc 0.9375, prec 0.0741442, recall 0.849746
2017-12-09T23:07:52.485696: step 1929, loss 0.511037, acc 0.953125, prec 0.0741892, recall 0.849845
2017-12-09T23:07:52.783401: step 1930, loss 0.205296, acc 0.9375, prec 0.0742135, recall 0.849912
2017-12-09T23:07:53.084091: step 1931, loss 0.142675, acc 0.945312, prec 0.0742213, recall 0.849945
2017-12-09T23:07:53.380932: step 1932, loss 0.13031, acc 0.945312, prec 0.0742649, recall 0.850044
2017-12-09T23:07:53.679746: step 1933, loss 0.462843, acc 0.929688, prec 0.0742877, recall 0.85011
2017-12-09T23:07:53.977029: step 1934, loss 0.0864349, acc 0.96875, prec 0.074282, recall 0.85011
2017-12-09T23:07:54.271916: step 1935, loss 0.136222, acc 0.945312, prec 0.0742719, recall 0.85011
2017-12-09T23:07:54.571225: step 1936, loss 0.101733, acc 0.960938, prec 0.0743183, recall 0.85021
2017-12-09T23:07:54.869708: step 1937, loss 0.10363, acc 0.960938, prec 0.0743112, recall 0.85021
2017-12-09T23:07:55.168537: step 1938, loss 0.235203, acc 0.953125, prec 0.0743204, recall 0.850243
2017-12-09T23:07:55.466358: step 1939, loss 0.172475, acc 0.929688, prec 0.0743075, recall 0.850243
2017-12-09T23:07:55.764242: step 1940, loss 0.0912916, acc 0.960938, prec 0.0743004, recall 0.850243
2017-12-09T23:07:56.064107: step 1941, loss 0.191907, acc 0.960938, prec 0.0743646, recall 0.850375
2017-12-09T23:07:56.359134: step 1942, loss 0.24246, acc 0.976562, prec 0.0744138, recall 0.850473
2017-12-09T23:07:56.659657: step 1943, loss 2.3083, acc 0.96875, prec 0.0744095, recall 0.850286
2017-12-09T23:07:56.960172: step 1944, loss 0.496036, acc 0.96875, prec 0.0744572, recall 0.850385
2017-12-09T23:07:57.261943: step 1945, loss 0.284786, acc 0.9375, prec 0.0744992, recall 0.850484
2017-12-09T23:07:57.563112: step 1946, loss 0.165876, acc 0.976562, prec 0.0745306, recall 0.850549
2017-12-09T23:07:57.864379: step 1947, loss 0.0860249, acc 0.96875, prec 0.0745427, recall 0.850582
2017-12-09T23:07:58.161802: step 1948, loss 0.131248, acc 0.953125, prec 0.0745875, recall 0.850681
2017-12-09T23:07:58.463227: step 1949, loss 0.146784, acc 0.953125, prec 0.0745789, recall 0.850681
2017-12-09T23:07:58.757982: step 1950, loss 0.829403, acc 0.953125, prec 0.0746593, recall 0.850845
2017-12-09T23:07:59.059573: step 1951, loss 0.264469, acc 0.921875, prec 0.074645, recall 0.850845
2017-12-09T23:07:59.356337: step 1952, loss 0.174595, acc 0.945312, prec 0.0746527, recall 0.850877
2017-12-09T23:07:59.654919: step 1953, loss 0.195168, acc 0.929688, prec 0.0746932, recall 0.850975
2017-12-09T23:07:59.952993: step 1954, loss 0.15158, acc 0.9375, prec 0.0746995, recall 0.851008
2017-12-09T23:08:00.261108: step 1955, loss 0.319043, acc 0.914062, prec 0.0747015, recall 0.851041
2017-12-09T23:08:00.563552: step 1956, loss 0.259313, acc 0.875, prec 0.0746785, recall 0.851041
2017-12-09T23:08:00.859391: step 1957, loss 0.22497, acc 0.914062, prec 0.0746983, recall 0.851106
2017-12-09T23:08:01.156610: step 1958, loss 0.227369, acc 0.90625, prec 0.0747166, recall 0.851171
2017-12-09T23:08:01.452407: step 1959, loss 0.210671, acc 0.898438, prec 0.0747335, recall 0.851236
2017-12-09T23:08:01.752620: step 1960, loss 0.128947, acc 0.960938, prec 0.0747263, recall 0.851236
2017-12-09T23:08:02.047274: step 1961, loss 0.189429, acc 0.9375, prec 0.0747326, recall 0.851269
2017-12-09T23:08:02.345446: step 1962, loss 0.228163, acc 0.921875, prec 0.0747538, recall 0.851334
2017-12-09T23:08:02.644790: step 1963, loss 0.0701248, acc 0.96875, prec 0.0748013, recall 0.851431
2017-12-09T23:08:02.944120: step 1964, loss 0.207205, acc 0.960938, prec 0.0748297, recall 0.851496
2017-12-09T23:08:03.242736: step 1965, loss 0.179068, acc 0.960938, prec 0.0748402, recall 0.851528
2017-12-09T23:08:03.544940: step 1966, loss 0.153202, acc 0.960938, prec 0.0748686, recall 0.851593
2017-12-09T23:08:03.841222: step 1967, loss 0.150395, acc 0.9375, prec 0.0748748, recall 0.851626
2017-12-09T23:08:04.140222: step 1968, loss 0.173888, acc 0.96875, prec 0.0749046, recall 0.85169
2017-12-09T23:08:04.434434: step 1969, loss 1.92174, acc 0.96875, prec 0.0749712, recall 0.851634
2017-12-09T23:08:04.732961: step 1970, loss 0.151881, acc 0.953125, prec 0.0749803, recall 0.851666
2017-12-09T23:08:05.036296: step 1971, loss 0.122892, acc 0.9375, prec 0.0749688, recall 0.851666
2017-12-09T23:08:05.338310: step 1972, loss 0.172928, acc 0.9375, prec 0.0749751, recall 0.851699
2017-12-09T23:08:05.642113: step 1973, loss 0.230106, acc 0.9375, prec 0.0749636, recall 0.851699
2017-12-09T23:08:05.943970: step 1974, loss 0.184639, acc 0.953125, prec 0.074955, recall 0.851699
2017-12-09T23:08:06.236343: step 1975, loss 0.295266, acc 0.898438, prec 0.0749717, recall 0.851763
2017-12-09T23:08:06.540566: step 1976, loss 0.220432, acc 0.914062, prec 0.0749737, recall 0.851795
2017-12-09T23:08:06.837209: step 1977, loss 0.466133, acc 0.929688, prec 0.0750139, recall 0.851892
2017-12-09T23:08:07.135677: step 1978, loss 0.324365, acc 0.921875, prec 0.0750704, recall 0.852021
2017-12-09T23:08:07.439988: step 1979, loss 0.246852, acc 0.921875, prec 0.0750914, recall 0.852085
2017-12-09T23:08:07.735635: step 1980, loss 0.187117, acc 0.945312, prec 0.0751344, recall 0.852181
2017-12-09T23:08:08.032431: step 1981, loss 0.1597, acc 0.953125, prec 0.0751612, recall 0.852246
2017-12-09T23:08:08.326867: step 1982, loss 0.242434, acc 0.929688, prec 0.0751837, recall 0.85231
2017-12-09T23:08:08.626333: step 1983, loss 0.269456, acc 0.921875, prec 0.075187, recall 0.852342
2017-12-09T23:08:08.921219: step 1984, loss 0.494141, acc 0.914062, prec 0.0752065, recall 0.852406
2017-12-09T23:08:09.218926: step 1985, loss 0.316774, acc 0.929688, prec 0.0752289, recall 0.85247
2017-12-09T23:08:09.516800: step 1986, loss 0.330225, acc 0.914062, prec 0.0752838, recall 0.852597
2017-12-09T23:08:09.819566: step 1987, loss 0.238656, acc 0.929688, prec 0.0753239, recall 0.852693
2017-12-09T23:08:10.118040: step 1988, loss 0.284004, acc 0.914062, prec 0.075361, recall 0.852789
2017-12-09T23:08:10.412820: step 1989, loss 0.138019, acc 0.960938, prec 0.0753892, recall 0.852852
2017-12-09T23:08:10.712074: step 1990, loss 0.165141, acc 0.921875, prec 0.0754277, recall 0.852948
2017-12-09T23:08:11.010041: step 1991, loss 0.0695518, acc 0.976562, prec 0.0754411, recall 0.852979
2017-12-09T23:08:11.187347: step 1992, loss 0.215089, acc 0.942308, prec 0.0754721, recall 0.853043
2017-12-09T23:08:11.492531: step 1993, loss 0.224795, acc 0.9375, prec 0.0755311, recall 0.853169
2017-12-09T23:08:11.788093: step 1994, loss 0.146521, acc 0.953125, prec 0.0755754, recall 0.853264
2017-12-09T23:08:12.088102: step 1995, loss 0.299046, acc 0.9375, prec 0.0756168, recall 0.853359
2017-12-09T23:08:12.385830: step 1996, loss 0.0712177, acc 0.984375, prec 0.0756315, recall 0.853391
2017-12-09T23:08:12.684497: step 1997, loss 0.156989, acc 0.984375, prec 0.0756463, recall 0.853422
2017-12-09T23:08:12.984702: step 1998, loss 0.155101, acc 0.945312, prec 0.0756538, recall 0.853454
2017-12-09T23:08:13.280911: step 1999, loss 0.0824044, acc 0.96875, prec 0.0756833, recall 0.853517
2017-12-09T23:08:13.585119: step 2000, loss 0.198775, acc 0.953125, prec 0.0757452, recall 0.853643
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-2000

2017-12-09T23:08:14.843376: step 2001, loss 0.167255, acc 0.96875, prec 0.0757922, recall 0.853737
2017-12-09T23:08:15.138377: step 2002, loss 0.146539, acc 0.960938, prec 0.0758203, recall 0.8538
2017-12-09T23:08:15.444154: step 2003, loss 0.0283819, acc 0.984375, prec 0.0758174, recall 0.8538
2017-12-09T23:08:15.741112: step 2004, loss 0.0704734, acc 0.984375, prec 0.0758673, recall 0.853894
2017-12-09T23:08:16.041646: step 2005, loss 0.0841835, acc 0.976562, prec 0.0758982, recall 0.853957
2017-12-09T23:08:16.342595: step 2006, loss 0.158838, acc 0.992188, prec 0.075932, recall 0.854019
2017-12-09T23:08:16.643081: step 2007, loss 0.526586, acc 0.992188, prec 0.0759482, recall 0.854051
2017-12-09T23:08:16.942896: step 2008, loss 0.0433947, acc 0.976562, prec 0.0759614, recall 0.854082
2017-12-09T23:08:17.246625: step 2009, loss 0.139043, acc 0.953125, prec 0.0759527, recall 0.854082
2017-12-09T23:08:17.543155: step 2010, loss 0.116377, acc 0.96875, prec 0.0759646, recall 0.854113
2017-12-09T23:08:17.840026: step 2011, loss 0.28225, acc 0.96875, prec 0.075994, recall 0.854176
2017-12-09T23:08:18.142521: step 2012, loss 0.0493554, acc 0.992188, prec 0.0760453, recall 0.854269
2017-12-09T23:08:18.445750: step 2013, loss 0.104248, acc 0.953125, prec 0.0760542, recall 0.8543
2017-12-09T23:08:18.746922: step 2014, loss 0.0370436, acc 1, prec 0.0760894, recall 0.854363
2017-12-09T23:08:19.050671: step 2015, loss 0.0986904, acc 0.976562, prec 0.0761027, recall 0.854394
2017-12-09T23:08:19.349173: step 2016, loss 0.254697, acc 0.96875, prec 0.0761497, recall 0.854487
2017-12-09T23:08:19.651355: step 2017, loss 0.0818295, acc 0.960938, prec 0.07616, recall 0.854518
2017-12-09T23:08:19.950814: step 2018, loss 0.264665, acc 0.953125, prec 0.0762041, recall 0.854611
2017-12-09T23:08:20.249233: step 2019, loss 0.0918477, acc 0.96875, prec 0.0762334, recall 0.854674
2017-12-09T23:08:20.562055: step 2020, loss 0.439295, acc 0.921875, prec 0.0762893, recall 0.854797
2017-12-09T23:08:20.862278: step 2021, loss 0.132314, acc 0.960938, prec 0.0762996, recall 0.854828
2017-12-09T23:08:21.160299: step 2022, loss 0.127862, acc 0.976562, prec 0.0763128, recall 0.854859
2017-12-09T23:08:21.466539: step 2023, loss 0.171743, acc 0.945312, prec 0.0763202, recall 0.85489
2017-12-09T23:08:21.766519: step 2024, loss 0.0718679, acc 0.96875, prec 0.076332, recall 0.854921
2017-12-09T23:08:22.067619: step 2025, loss 0.28169, acc 0.90625, prec 0.0763497, recall 0.854983
2017-12-09T23:08:22.362954: step 2026, loss 0.124415, acc 0.984375, prec 0.0763819, recall 0.855045
2017-12-09T23:08:22.665211: step 2027, loss 0.0943625, acc 0.984375, prec 0.0763965, recall 0.855076
2017-12-09T23:08:22.967497: step 2028, loss 0.88639, acc 0.96875, prec 0.0764449, recall 0.854986
2017-12-09T23:08:23.269056: step 2029, loss 0.0757633, acc 0.96875, prec 0.0764391, recall 0.854986
2017-12-09T23:08:23.568127: step 2030, loss 0.108302, acc 0.976562, prec 0.0764522, recall 0.855017
2017-12-09T23:08:23.866204: step 2031, loss 1.7733, acc 0.945312, prec 0.0764786, recall 0.854897
2017-12-09T23:08:24.170884: step 2032, loss 0.256856, acc 0.9375, prec 0.076467, recall 0.854897
2017-12-09T23:08:24.468979: step 2033, loss 0.2212, acc 0.898438, prec 0.0764657, recall 0.854928
2017-12-09T23:08:24.766649: step 2034, loss 0.27209, acc 0.898438, prec 0.0764994, recall 0.85502
2017-12-09T23:08:25.067961: step 2035, loss 0.236738, acc 0.914062, prec 0.0765711, recall 0.855174
2017-12-09T23:08:25.365350: step 2036, loss 0.27198, acc 0.90625, prec 0.0765712, recall 0.855205
2017-12-09T23:08:25.663866: step 2037, loss 0.157806, acc 0.945312, prec 0.076561, recall 0.855205
2017-12-09T23:08:25.961820: step 2038, loss 0.590265, acc 0.929688, prec 0.076583, recall 0.855266
2017-12-09T23:08:26.259001: step 2039, loss 0.269351, acc 0.875, prec 0.0766123, recall 0.855358
2017-12-09T23:08:26.559713: step 2040, loss 0.349062, acc 0.90625, prec 0.0765949, recall 0.855358
2017-12-09T23:08:26.860122: step 2041, loss 0.160663, acc 0.953125, prec 0.0766387, recall 0.85545
2017-12-09T23:08:27.160756: step 2042, loss 0.226888, acc 0.9375, prec 0.0766796, recall 0.855541
2017-12-09T23:08:27.460679: step 2043, loss 0.369654, acc 0.882812, prec 0.0766753, recall 0.855572
2017-12-09T23:08:27.757484: step 2044, loss 0.171766, acc 0.976562, prec 0.0767059, recall 0.855633
2017-12-09T23:08:28.055801: step 2045, loss 0.215925, acc 0.90625, prec 0.0767234, recall 0.855694
2017-12-09T23:08:28.357011: step 2046, loss 0.1196, acc 0.960938, prec 0.0767511, recall 0.855755
2017-12-09T23:08:28.657402: step 2047, loss 0.180602, acc 0.929688, prec 0.0767555, recall 0.855785
2017-12-09T23:08:28.953636: step 2048, loss 0.2892, acc 0.960938, prec 0.0768182, recall 0.855907
2017-12-09T23:08:29.252731: step 2049, loss 0.253102, acc 0.96875, prec 0.0768473, recall 0.855968
2017-12-09T23:08:29.553148: step 2050, loss 0.233718, acc 0.992188, prec 0.0769507, recall 0.85615
2017-12-09T23:08:29.851374: step 2051, loss 0.10503, acc 0.96875, prec 0.0769449, recall 0.85615
2017-12-09T23:08:30.157744: step 2052, loss 0.114194, acc 0.976562, prec 0.0769406, recall 0.85615
2017-12-09T23:08:30.462022: step 2053, loss 0.10905, acc 0.96875, prec 0.0769347, recall 0.85615
2017-12-09T23:08:30.758282: step 2054, loss 1.38793, acc 0.960938, prec 0.0769813, recall 0.856061
2017-12-09T23:08:31.057613: step 2055, loss 0.10286, acc 0.960938, prec 0.0769915, recall 0.856091
2017-12-09T23:08:31.357413: step 2056, loss 0.129135, acc 0.96875, prec 0.0770031, recall 0.856121
2017-12-09T23:08:31.656218: step 2057, loss 0.130047, acc 0.96875, prec 0.0770148, recall 0.856151
2017-12-09T23:08:31.959324: step 2058, loss 0.0892285, acc 0.960938, prec 0.0770424, recall 0.856212
2017-12-09T23:08:32.263547: step 2059, loss 0.230332, acc 0.96875, prec 0.0770715, recall 0.856272
2017-12-09T23:08:32.558732: step 2060, loss 0.0900312, acc 0.96875, prec 0.0770831, recall 0.856302
2017-12-09T23:08:32.852610: step 2061, loss 0.511207, acc 0.953125, prec 0.0771267, recall 0.856393
2017-12-09T23:08:33.150508: step 2062, loss 0.203985, acc 0.9375, prec 0.0771499, recall 0.856453
2017-12-09T23:08:33.446694: step 2063, loss 0.176144, acc 0.945312, prec 0.0771572, recall 0.856483
2017-12-09T23:08:33.747538: step 2064, loss 0.198405, acc 0.921875, prec 0.0771426, recall 0.856483
2017-12-09T23:08:34.045310: step 2065, loss 0.212869, acc 0.921875, prec 0.077128, recall 0.856483
2017-12-09T23:08:34.342753: step 2066, loss 0.160862, acc 0.953125, prec 0.0771367, recall 0.856514
2017-12-09T23:08:34.639403: step 2067, loss 0.265923, acc 0.921875, prec 0.0771744, recall 0.856604
2017-12-09T23:08:34.937808: step 2068, loss 0.0764001, acc 0.96875, prec 0.0771686, recall 0.856604
2017-12-09T23:08:35.236163: step 2069, loss 0.134625, acc 0.945312, prec 0.0771932, recall 0.856664
2017-12-09T23:08:35.548496: step 2070, loss 0.0849969, acc 0.984375, prec 0.0772252, recall 0.856724
2017-12-09T23:08:35.848522: step 2071, loss 0.147952, acc 0.960938, prec 0.0772702, recall 0.856814
2017-12-09T23:08:36.143692: step 2072, loss 0.082744, acc 0.976562, prec 0.0773006, recall 0.856874
2017-12-09T23:08:36.444485: step 2073, loss 0.132863, acc 0.953125, prec 0.0773441, recall 0.856964
2017-12-09T23:08:36.740765: step 2074, loss 0.104239, acc 0.984375, prec 0.0773586, recall 0.856993
2017-12-09T23:08:37.043089: step 2075, loss 0.286848, acc 0.953125, prec 0.0773672, recall 0.857023
2017-12-09T23:08:37.346331: step 2076, loss 0.214793, acc 0.9375, prec 0.077373, recall 0.857053
2017-12-09T23:08:37.642551: step 2077, loss 0.0595691, acc 0.96875, prec 0.0773671, recall 0.857053
2017-12-09T23:08:37.940636: step 2078, loss 0.0944839, acc 0.976562, prec 0.0773628, recall 0.857053
2017-12-09T23:08:38.239405: step 2079, loss 0.231391, acc 0.976562, prec 0.0774106, recall 0.857143
2017-12-09T23:08:38.543083: step 2080, loss 0.229868, acc 0.96875, prec 0.0774396, recall 0.857203
2017-12-09T23:08:38.840209: step 2081, loss 0.156909, acc 0.9375, prec 0.0774279, recall 0.857203
2017-12-09T23:08:39.140815: step 2082, loss 2.06817, acc 0.929688, prec 0.0774162, recall 0.857024
2017-12-09T23:08:39.438928: step 2083, loss 0.145843, acc 0.96875, prec 0.0774277, recall 0.857053
2017-12-09T23:08:39.739835: step 2084, loss 0.687383, acc 0.960938, prec 0.07749, recall 0.857173
2017-12-09T23:08:40.035158: step 2085, loss 0.265863, acc 0.960938, prec 0.0775349, recall 0.857262
2017-12-09T23:08:40.332557: step 2086, loss 0.178096, acc 0.945312, prec 0.077542, recall 0.857292
2017-12-09T23:08:40.637834: step 2087, loss 0.330765, acc 0.914062, prec 0.0775433, recall 0.857321
2017-12-09T23:08:40.939805: step 2088, loss 0.170289, acc 0.914062, prec 0.0775273, recall 0.857321
2017-12-09T23:08:41.236298: step 2089, loss 0.239631, acc 0.921875, prec 0.0775648, recall 0.85741
2017-12-09T23:08:41.533256: step 2090, loss 0.141164, acc 0.914062, prec 0.0776182, recall 0.857529
2017-12-09T23:08:41.830627: step 2091, loss 0.293192, acc 0.882812, prec 0.0776136, recall 0.857559
2017-12-09T23:08:42.131087: step 2092, loss 0.413235, acc 0.882812, prec 0.0776091, recall 0.857588
2017-12-09T23:08:42.431020: step 2093, loss 0.159803, acc 0.9375, prec 0.0776147, recall 0.857618
2017-12-09T23:08:42.725262: step 2094, loss 0.279745, acc 0.90625, prec 0.0776319, recall 0.857677
2017-12-09T23:08:43.025755: step 2095, loss 0.173652, acc 0.9375, prec 0.0776549, recall 0.857736
2017-12-09T23:08:43.319677: step 2096, loss 0.138889, acc 0.929688, prec 0.0776591, recall 0.857766
2017-12-09T23:08:43.613036: step 2097, loss 0.221205, acc 0.929688, prec 0.077698, recall 0.857854
2017-12-09T23:08:43.906894: step 2098, loss 0.835157, acc 0.945312, prec 0.0777066, recall 0.857706
2017-12-09T23:08:44.210492: step 2099, loss 0.481588, acc 0.960938, prec 0.0777339, recall 0.857765
2017-12-09T23:08:44.511370: step 2100, loss 0.22048, acc 0.976562, prec 0.0777815, recall 0.857853

Evaluation:
2017-12-09T23:08:49.226040: step 2100, loss 1.81627, acc 0.924608, prec 0.0782297, recall 0.848826

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-2100

2017-12-09T23:08:50.670160: step 2101, loss 0.32846, acc 0.929688, prec 0.0782848, recall 0.848947
2017-12-09T23:08:50.966132: step 2102, loss 0.168371, acc 0.945312, prec 0.0782747, recall 0.848947
2017-12-09T23:08:51.262592: step 2103, loss 0.308804, acc 0.898438, prec 0.0782559, recall 0.848947
2017-12-09T23:08:51.561986: step 2104, loss 0.219673, acc 0.914062, prec 0.078257, recall 0.848977
2017-12-09T23:08:51.860890: step 2105, loss 0.213343, acc 0.929688, prec 0.078244, recall 0.848977
2017-12-09T23:08:52.165237: step 2106, loss 0.175433, acc 0.945312, prec 0.0782339, recall 0.848977
2017-12-09T23:08:52.462079: step 2107, loss 0.300632, acc 0.929688, prec 0.0782379, recall 0.849007
2017-12-09T23:08:52.764753: step 2108, loss 0.276381, acc 0.914062, prec 0.078222, recall 0.849007
2017-12-09T23:08:53.061432: step 2109, loss 0.253821, acc 0.960938, prec 0.0782829, recall 0.849128
2017-12-09T23:08:53.359086: step 2110, loss 0.286405, acc 0.953125, prec 0.0783593, recall 0.849279
2017-12-09T23:08:53.659956: step 2111, loss 0.274789, acc 0.898438, prec 0.0783575, recall 0.84931
2017-12-09T23:08:53.958221: step 2112, loss 0.182698, acc 0.9375, prec 0.078363, recall 0.84934
2017-12-09T23:08:54.254832: step 2113, loss 0.136206, acc 0.96875, prec 0.0784082, recall 0.84943
2017-12-09T23:08:54.555922: step 2114, loss 0.222674, acc 0.929688, prec 0.0784632, recall 0.84955
2017-12-09T23:08:54.858101: step 2115, loss 0.255611, acc 0.921875, prec 0.0784997, recall 0.849641
2017-12-09T23:08:55.155559: step 2116, loss 0.186534, acc 0.945312, prec 0.0784896, recall 0.849641
2017-12-09T23:08:55.452004: step 2117, loss 0.204146, acc 0.976562, prec 0.0785193, recall 0.849701
2017-12-09T23:08:55.747500: step 2118, loss 0.450607, acc 0.984375, prec 0.0786523, recall 0.84994
2017-12-09T23:08:56.051882: step 2119, loss 0.0998862, acc 0.984375, prec 0.0786664, recall 0.84997
2017-12-09T23:08:56.348902: step 2120, loss 0.175681, acc 0.953125, prec 0.0786747, recall 0.85
2017-12-09T23:08:56.646910: step 2121, loss 0.184834, acc 0.96875, prec 0.0787029, recall 0.85006
2017-12-09T23:08:56.945247: step 2122, loss 0.190886, acc 0.976562, prec 0.0787494, recall 0.850149
2017-12-09T23:08:57.243889: step 2123, loss 0.1757, acc 0.921875, prec 0.0787689, recall 0.850209
2017-12-09T23:08:57.537678: step 2124, loss 0.177377, acc 0.929688, prec 0.0787728, recall 0.850239
2017-12-09T23:08:57.836782: step 2125, loss 0.0531717, acc 0.984375, prec 0.0787699, recall 0.850239
2017-12-09T23:08:58.134999: step 2126, loss 0.69525, acc 0.976562, prec 0.0788179, recall 0.850159
2017-12-09T23:08:58.437563: step 2127, loss 0.114257, acc 0.953125, prec 0.0788262, recall 0.850189
2017-12-09T23:08:58.733867: step 2128, loss 0.188787, acc 0.929688, prec 0.078881, recall 0.850308
2017-12-09T23:08:59.035470: step 2129, loss 0.161132, acc 0.953125, prec 0.0789062, recall 0.850367
2017-12-09T23:08:59.334449: step 2130, loss 0.191998, acc 0.953125, prec 0.0789144, recall 0.850397
2017-12-09T23:08:59.633774: step 2131, loss 0.115311, acc 0.960938, prec 0.078958, recall 0.850486
2017-12-09T23:08:59.933397: step 2132, loss 0.117955, acc 0.953125, prec 0.0789493, recall 0.850486
2017-12-09T23:09:00.239404: step 2133, loss 0.254485, acc 0.960938, prec 0.078959, recall 0.850515
2017-12-09T23:09:00.542440: step 2134, loss 0.309175, acc 0.960938, prec 0.0789856, recall 0.850575
2017-12-09T23:09:00.841664: step 2135, loss 0.222938, acc 0.929688, prec 0.0790064, recall 0.850634
2017-12-09T23:09:01.143670: step 2136, loss 0.0516557, acc 0.984375, prec 0.0790205, recall 0.850663
2017-12-09T23:09:01.438624: step 2137, loss 0.175638, acc 0.953125, prec 0.0790626, recall 0.850752
2017-12-09T23:09:01.742363: step 2138, loss 0.119562, acc 0.953125, prec 0.0791047, recall 0.850841
2017-12-09T23:09:02.037222: step 2139, loss 0.584305, acc 0.984375, prec 0.0791864, recall 0.850988
2017-12-09T23:09:02.336499: step 2140, loss 0.209372, acc 0.945312, prec 0.079227, recall 0.851076
2017-12-09T23:09:02.633094: step 2141, loss 0.3264, acc 0.921875, prec 0.0792463, recall 0.851135
2017-12-09T23:09:02.930496: step 2142, loss 0.224442, acc 0.984375, prec 0.0792603, recall 0.851165
2017-12-09T23:09:03.228763: step 2143, loss 0.139414, acc 0.96875, prec 0.0792884, recall 0.851223
2017-12-09T23:09:03.530819: step 2144, loss 0.211514, acc 0.960938, prec 0.0793487, recall 0.851341
2017-12-09T23:09:03.829721: step 2145, loss 0.169704, acc 0.960938, prec 0.0793753, recall 0.851399
2017-12-09T23:09:04.125108: step 2146, loss 0.176095, acc 0.976562, prec 0.0794217, recall 0.851487
2017-12-09T23:09:04.429397: step 2147, loss 0.359358, acc 0.921875, prec 0.0794071, recall 0.851487
2017-12-09T23:09:04.727949: step 2148, loss 0.0948112, acc 0.96875, prec 0.0794181, recall 0.851516
2017-12-09T23:09:05.025643: step 2149, loss 0.0793723, acc 0.96875, prec 0.0794292, recall 0.851546
2017-12-09T23:09:05.328887: step 2150, loss 0.338461, acc 0.921875, prec 0.0794484, recall 0.851604
2017-12-09T23:09:05.641322: step 2151, loss 0.129384, acc 0.984375, prec 0.0794624, recall 0.851633
2017-12-09T23:09:05.943400: step 2152, loss 0.145334, acc 0.953125, prec 0.0794706, recall 0.851662
2017-12-09T23:09:06.248124: step 2153, loss 0.137936, acc 0.984375, prec 0.0795014, recall 0.851721
2017-12-09T23:09:06.548183: step 2154, loss 0.0985388, acc 0.96875, prec 0.0795632, recall 0.851837
2017-12-09T23:09:06.845123: step 2155, loss 0.102633, acc 0.984375, prec 0.0795772, recall 0.851866
2017-12-09T23:09:07.145943: step 2156, loss 0.0704159, acc 0.984375, prec 0.0795742, recall 0.851866
2017-12-09T23:09:07.441351: step 2157, loss 0.288918, acc 0.976562, prec 0.0796036, recall 0.851925
2017-12-09T23:09:07.741859: step 2158, loss 2.05064, acc 0.976562, prec 0.0796683, recall 0.851874
2017-12-09T23:09:08.044215: step 2159, loss 0.201164, acc 0.953125, prec 0.0796764, recall 0.851903
2017-12-09T23:09:08.339410: step 2160, loss 0.165927, acc 0.929688, prec 0.079697, recall 0.851961
2017-12-09T23:09:08.638858: step 2161, loss 0.275667, acc 0.953125, prec 0.0797895, recall 0.852135
2017-12-09T23:09:08.938730: step 2162, loss 0.249922, acc 0.9375, prec 0.0798621, recall 0.852279
2017-12-09T23:09:09.242792: step 2163, loss 0.161759, acc 0.953125, prec 0.0798533, recall 0.852279
2017-12-09T23:09:09.545228: step 2164, loss 0.213268, acc 0.90625, prec 0.0798358, recall 0.852279
2017-12-09T23:09:09.847746: step 2165, loss 0.205758, acc 0.90625, prec 0.0798688, recall 0.852366
2017-12-09T23:09:10.140203: step 2166, loss 0.351452, acc 0.90625, prec 0.0798513, recall 0.852366
2017-12-09T23:09:10.439814: step 2167, loss 0.351785, acc 0.890625, prec 0.0798813, recall 0.852453
2017-12-09T23:09:10.735050: step 2168, loss 0.254992, acc 0.914062, prec 0.0799158, recall 0.852539
2017-12-09T23:09:11.030537: step 2169, loss 0.210521, acc 0.929688, prec 0.07997, recall 0.852654
2017-12-09T23:09:11.332807: step 2170, loss 0.256866, acc 0.921875, prec 0.0799722, recall 0.852683
2017-12-09T23:09:11.632639: step 2171, loss 0.126686, acc 0.929688, prec 0.079959, recall 0.852683
2017-12-09T23:09:11.928072: step 2172, loss 0.215387, acc 0.945312, prec 0.0799824, recall 0.85274
2017-12-09T23:09:12.228295: step 2173, loss 0.242832, acc 0.945312, prec 0.079989, recall 0.852769
2017-12-09T23:09:12.526472: step 2174, loss 0.329064, acc 0.960938, prec 0.0800658, recall 0.852913
2017-12-09T23:09:12.827810: step 2175, loss 0.485996, acc 0.984375, prec 0.0800966, recall 0.85297
2017-12-09T23:09:13.130774: step 2176, loss 0.168419, acc 0.953125, prec 0.0801382, recall 0.853056
2017-12-09T23:09:13.426730: step 2177, loss 0.131919, acc 0.9375, prec 0.0801433, recall 0.853084
2017-12-09T23:09:13.724781: step 2178, loss 0.130765, acc 0.953125, prec 0.0801682, recall 0.853141
2017-12-09T23:09:14.028806: step 2179, loss 1.01261, acc 0.96875, prec 0.0801806, recall 0.853004
2017-12-09T23:09:14.334903: step 2180, loss 0.109718, acc 0.9375, prec 0.0801857, recall 0.853033
2017-12-09T23:09:14.628580: step 2181, loss 0.263316, acc 0.945312, prec 0.0802258, recall 0.853118
2017-12-09T23:09:14.930944: step 2182, loss 0.269745, acc 0.960938, prec 0.0802689, recall 0.853204
2017-12-09T23:09:15.235628: step 2183, loss 0.30448, acc 0.9375, prec 0.0803076, recall 0.853289
2017-12-09T23:09:15.536303: step 2184, loss 0.122246, acc 0.9375, prec 0.0802958, recall 0.853289
2017-12-09T23:09:15.835245: step 2185, loss 0.18296, acc 0.953125, prec 0.0803038, recall 0.853318
2017-12-09T23:09:16.133950: step 2186, loss 0.147157, acc 0.945312, prec 0.0803272, recall 0.853375
2017-12-09T23:09:16.427184: step 2187, loss 0.284665, acc 0.921875, prec 0.0803293, recall 0.853403
2017-12-09T23:09:16.727091: step 2188, loss 0.19973, acc 0.921875, prec 0.0803482, recall 0.85346
2017-12-09T23:09:17.021034: step 2189, loss 0.176948, acc 0.929688, prec 0.0803685, recall 0.853517
2017-12-09T23:09:17.317506: step 2190, loss 0.194976, acc 0.9375, prec 0.0804071, recall 0.853602
2017-12-09T23:09:17.616491: step 2191, loss 0.41245, acc 0.929688, prec 0.0804443, recall 0.853687
2017-12-09T23:09:17.917463: step 2192, loss 0.15995, acc 0.9375, prec 0.0804325, recall 0.853687
2017-12-09T23:09:18.213227: step 2193, loss 0.127266, acc 0.945312, prec 0.080439, recall 0.853715
2017-12-09T23:09:18.515556: step 2194, loss 0.174879, acc 0.953125, prec 0.0804638, recall 0.853772
2017-12-09T23:09:18.808487: step 2195, loss 0.257113, acc 0.945312, prec 0.080487, recall 0.853828
2017-12-09T23:09:19.104708: step 2196, loss 0.0706112, acc 0.984375, prec 0.0805008, recall 0.853857
2017-12-09T23:09:19.402673: step 2197, loss 0.169397, acc 0.976562, prec 0.0805467, recall 0.853941
2017-12-09T23:09:19.703310: step 2198, loss 0.15903, acc 0.976562, prec 0.0805926, recall 0.854026
2017-12-09T23:09:20.005291: step 2199, loss 0.0796672, acc 0.984375, prec 0.0806566, recall 0.854139
2017-12-09T23:09:20.302516: step 2200, loss 0.0989074, acc 0.992188, prec 0.0807221, recall 0.854251
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-2200

2017-12-09T23:09:21.638512: step 2201, loss 0.272366, acc 0.9375, prec 0.0807606, recall 0.854335
2017-12-09T23:09:21.943398: step 2202, loss 0.0545382, acc 0.984375, prec 0.0807912, recall 0.854391
2017-12-09T23:09:22.240406: step 2203, loss 0.180154, acc 0.9375, prec 0.0807794, recall 0.854391
2017-12-09T23:09:22.538653: step 2204, loss 0.245241, acc 0.976562, prec 0.0807917, recall 0.854419
2017-12-09T23:09:22.837049: step 2205, loss 0.10768, acc 0.960938, prec 0.0808178, recall 0.854475
2017-12-09T23:09:23.140271: step 2206, loss 0.36928, acc 0.976562, prec 0.0808301, recall 0.854503
2017-12-09T23:09:23.440629: step 2207, loss 0.0889641, acc 0.953125, prec 0.0808213, recall 0.854503
2017-12-09T23:09:23.735251: step 2208, loss 0.0878542, acc 0.984375, prec 0.0808351, recall 0.854531
2017-12-09T23:09:24.034728: step 2209, loss 0.0667084, acc 0.992188, prec 0.0808504, recall 0.854559
2017-12-09T23:09:24.330634: step 2210, loss 0.0803269, acc 0.976562, prec 0.0808627, recall 0.854587
2017-12-09T23:09:24.632240: step 2211, loss 0.0768674, acc 0.96875, prec 0.0808902, recall 0.854643
2017-12-09T23:09:24.933256: step 2212, loss 0.105777, acc 0.96875, prec 0.0809178, recall 0.854699
2017-12-09T23:09:25.232920: step 2213, loss 0.273795, acc 0.953125, prec 0.0809926, recall 0.854839
2017-12-09T23:09:25.533679: step 2214, loss 0.30317, acc 0.984375, prec 0.0810063, recall 0.854867
2017-12-09T23:09:25.840141: step 2215, loss 0.0670076, acc 0.96875, prec 0.0810005, recall 0.854867
2017-12-09T23:09:26.136341: step 2216, loss 0.0480635, acc 0.984375, prec 0.0810142, recall 0.854894
2017-12-09T23:09:26.432610: step 2217, loss 0.0932186, acc 0.96875, prec 0.081025, recall 0.854922
2017-12-09T23:09:26.732416: step 2218, loss 0.16054, acc 0.960938, prec 0.0810511, recall 0.854978
2017-12-09T23:09:27.034920: step 2219, loss 0.269316, acc 0.945312, prec 0.0810575, recall 0.855006
2017-12-09T23:09:27.335704: step 2220, loss 0.11831, acc 0.976562, prec 0.0810865, recall 0.855061
2017-12-09T23:09:27.636132: step 2221, loss 0.405665, acc 0.992188, prec 0.0811017, recall 0.855089
2017-12-09T23:09:27.943507: step 2222, loss 0.173943, acc 0.945312, prec 0.0811582, recall 0.8552
2017-12-09T23:09:28.246068: step 2223, loss 0.0508559, acc 0.984375, prec 0.0811887, recall 0.855256
2017-12-09T23:09:28.540495: step 2224, loss 0.104387, acc 0.96875, prec 0.0811828, recall 0.855256
2017-12-09T23:09:28.838159: step 2225, loss 0.654809, acc 0.929688, prec 0.0812363, recall 0.855366
2017-12-09T23:09:29.136495: step 2226, loss 0.124248, acc 0.960938, prec 0.0812289, recall 0.855366
2017-12-09T23:09:29.430763: step 2227, loss 0.349572, acc 0.96875, prec 0.0813064, recall 0.855505
2017-12-09T23:09:29.732863: step 2228, loss 0.12513, acc 0.976562, prec 0.0813187, recall 0.855532
2017-12-09T23:09:30.036068: step 2229, loss 0.204445, acc 0.921875, prec 0.0813206, recall 0.85556
2017-12-09T23:09:30.344641: step 2230, loss 0.148386, acc 0.9375, prec 0.0813255, recall 0.855587
2017-12-09T23:09:30.639158: step 2231, loss 0.112267, acc 0.960938, prec 0.0813348, recall 0.855615
2017-12-09T23:09:30.936907: step 2232, loss 0.219184, acc 0.953125, prec 0.0813593, recall 0.85567
2017-12-09T23:09:31.236564: step 2233, loss 0.166076, acc 0.929688, prec 0.0813626, recall 0.855698
2017-12-09T23:09:31.535250: step 2234, loss 0.231677, acc 0.945312, prec 0.081369, recall 0.855725
2017-12-09T23:09:31.838164: step 2235, loss 0.233668, acc 0.945312, prec 0.0813753, recall 0.855753
2017-12-09T23:09:32.134729: step 2236, loss 0.194235, acc 0.945312, prec 0.081415, recall 0.855835
2017-12-09T23:09:32.440057: step 2237, loss 0.319688, acc 0.929688, prec 0.0814183, recall 0.855863
2017-12-09T23:09:32.745444: step 2238, loss 0.15581, acc 0.945312, prec 0.081458, recall 0.855945
2017-12-09T23:09:33.045398: step 2239, loss 0.343551, acc 0.960938, prec 0.0815006, recall 0.856027
2017-12-09T23:09:33.345023: step 2240, loss 0.104443, acc 0.953125, prec 0.0815083, recall 0.856055
2017-12-09T23:09:33.525730: step 2241, loss 0.0746257, acc 0.942308, prec 0.0815039, recall 0.856055
2017-12-09T23:09:33.834678: step 2242, loss 0.560867, acc 0.976562, prec 0.0815494, recall 0.856137
2017-12-09T23:09:34.133973: step 2243, loss 0.0385662, acc 0.984375, prec 0.0815465, recall 0.856137
2017-12-09T23:09:34.429298: step 2244, loss 0.129921, acc 0.960938, prec 0.0815557, recall 0.856164
2017-12-09T23:09:34.724453: step 2245, loss 0.238893, acc 0.945312, prec 0.0815787, recall 0.856219
2017-12-09T23:09:35.029967: step 2246, loss 0.129832, acc 0.976562, prec 0.0815909, recall 0.856246
2017-12-09T23:09:35.333150: step 2247, loss 0.065227, acc 0.96875, prec 0.0816349, recall 0.856328
2017-12-09T23:09:35.641547: step 2248, loss 0.128266, acc 0.96875, prec 0.0816456, recall 0.856356
2017-12-09T23:09:35.939241: step 2249, loss 0.235129, acc 0.96875, prec 0.0816729, recall 0.85641
2017-12-09T23:09:36.234767: step 2250, loss 0.0605524, acc 0.984375, prec 0.0816866, recall 0.856438
2017-12-09T23:09:36.531045: step 2251, loss 0.290546, acc 0.945312, prec 0.0817594, recall 0.856574
2017-12-09T23:09:36.832007: step 2252, loss 0.177832, acc 0.984375, prec 0.0818063, recall 0.856655
2017-12-09T23:09:37.130837: step 2253, loss 0.16286, acc 0.953125, prec 0.0817974, recall 0.856655
2017-12-09T23:09:37.435631: step 2254, loss 0.0861733, acc 0.953125, prec 0.0818218, recall 0.85671
2017-12-09T23:09:37.732743: step 2255, loss 0.17062, acc 0.96875, prec 0.0818159, recall 0.85671
2017-12-09T23:09:38.031451: step 2256, loss 0.466505, acc 0.960938, prec 0.0818417, recall 0.856764
2017-12-09T23:09:38.334819: step 2257, loss 0.0987601, acc 0.976562, prec 0.0818539, recall 0.856791
2017-12-09T23:09:38.634822: step 2258, loss 0.213778, acc 0.945312, prec 0.0818601, recall 0.856818
2017-12-09T23:09:38.931687: step 2259, loss 0.102144, acc 0.96875, prec 0.081904, recall 0.856899
2017-12-09T23:09:39.242222: step 2260, loss 0.177424, acc 0.960938, prec 0.0819298, recall 0.856954
2017-12-09T23:09:39.545136: step 2261, loss 0.174379, acc 0.96875, prec 0.0819737, recall 0.857035
2017-12-09T23:09:39.840727: step 2262, loss 0.0541094, acc 0.984375, prec 0.082004, recall 0.857089
2017-12-09T23:09:40.136809: step 2263, loss 0.281353, acc 0.921875, prec 0.0820389, recall 0.85717
2017-12-09T23:09:40.437736: step 2264, loss 0.23954, acc 0.960938, prec 0.0820813, recall 0.857251
2017-12-09T23:09:40.736199: step 2265, loss 0.116939, acc 0.960938, prec 0.0820905, recall 0.857278
2017-12-09T23:09:41.037737: step 2266, loss 0.0689642, acc 0.96875, prec 0.0821012, recall 0.857305
2017-12-09T23:09:41.332821: step 2267, loss 0.15799, acc 0.984375, prec 0.0821314, recall 0.857359
2017-12-09T23:09:41.634808: step 2268, loss 0.228996, acc 0.945312, prec 0.0821707, recall 0.857439
2017-12-09T23:09:41.936746: step 2269, loss 0.215444, acc 0.984375, prec 0.0822341, recall 0.857547
2017-12-09T23:09:42.240272: step 2270, loss 0.026412, acc 1, prec 0.0822673, recall 0.8576
2017-12-09T23:09:42.539111: step 2271, loss 0.107597, acc 0.976562, prec 0.082296, recall 0.857654
2017-12-09T23:09:42.836326: step 2272, loss 0.0687419, acc 0.992188, prec 0.0823442, recall 0.857734
2017-12-09T23:09:43.130152: step 2273, loss 0.261843, acc 0.953125, prec 0.0823685, recall 0.857788
2017-12-09T23:09:43.426555: step 2274, loss 0.0782443, acc 0.976562, prec 0.0823806, recall 0.857815
2017-12-09T23:09:43.730700: step 2275, loss 0.258604, acc 0.929688, prec 0.0824003, recall 0.857868
2017-12-09T23:09:44.030636: step 2276, loss 0.134073, acc 0.96875, prec 0.0824441, recall 0.857948
2017-12-09T23:09:44.335840: step 2277, loss 0.124076, acc 0.976562, prec 0.0824727, recall 0.858002
2017-12-09T23:09:44.631787: step 2278, loss 0.285959, acc 0.96875, prec 0.0824833, recall 0.858028
2017-12-09T23:09:44.931764: step 2279, loss 0.109325, acc 0.9375, prec 0.082488, recall 0.858055
2017-12-09T23:09:45.237066: step 2280, loss 0.0842061, acc 0.976562, prec 0.0825001, recall 0.858081
2017-12-09T23:09:45.544321: step 2281, loss 0.0697762, acc 0.976562, prec 0.0825287, recall 0.858135
2017-12-09T23:09:45.841695: step 2282, loss 0.0902026, acc 0.96875, prec 0.0825228, recall 0.858135
2017-12-09T23:09:46.140787: step 2283, loss 0.0619593, acc 0.992188, prec 0.0825379, recall 0.858161
2017-12-09T23:09:46.444762: step 2284, loss 0.191465, acc 0.992188, prec 0.082586, recall 0.858241
2017-12-09T23:09:46.760969: step 2285, loss 0.0686965, acc 0.984375, prec 0.082583, recall 0.858241
2017-12-09T23:09:47.056903: step 2286, loss 0.0693013, acc 0.984375, prec 0.0826297, recall 0.858321
2017-12-09T23:09:47.352539: step 2287, loss 0.105589, acc 0.960938, prec 0.0826223, recall 0.858321
2017-12-09T23:09:47.654380: step 2288, loss 0.39447, acc 0.96875, prec 0.0826494, recall 0.858374
2017-12-09T23:09:47.956951: step 2289, loss 0.118201, acc 0.960938, prec 0.082675, recall 0.858427
2017-12-09T23:09:48.256171: step 2290, loss 0.134948, acc 0.96875, prec 0.0827352, recall 0.858533
2017-12-09T23:09:48.551573: step 2291, loss 0.131884, acc 0.976562, prec 0.0827969, recall 0.858639
2017-12-09T23:09:48.846761: step 2292, loss 0.137106, acc 0.976562, prec 0.082809, recall 0.858665
2017-12-09T23:09:49.139256: step 2293, loss 0.0769201, acc 0.984375, prec 0.0828391, recall 0.858718
2017-12-09T23:09:49.435982: step 2294, loss 0.0865972, acc 0.976562, prec 0.0828346, recall 0.858718
2017-12-09T23:09:49.733747: step 2295, loss 0.0502183, acc 0.984375, prec 0.0828481, recall 0.858744
2017-12-09T23:09:50.032785: step 2296, loss 0.0901445, acc 0.960938, prec 0.0828737, recall 0.858797
2017-12-09T23:09:50.334792: step 2297, loss 0.0780941, acc 0.976562, prec 0.0828692, recall 0.858797
2017-12-09T23:09:50.637372: step 2298, loss 0.0711592, acc 0.984375, prec 0.0829158, recall 0.858876
2017-12-09T23:09:50.934198: step 2299, loss 0.174839, acc 0.96875, prec 0.0829429, recall 0.858929
2017-12-09T23:09:51.233486: step 2300, loss 0.115951, acc 0.96875, prec 0.0829865, recall 0.859008
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-2300

2017-12-09T23:09:52.553577: step 2301, loss 0.0830645, acc 0.96875, prec 0.0830301, recall 0.859087
2017-12-09T23:09:52.851355: step 2302, loss 0.162199, acc 0.976562, prec 0.0830752, recall 0.859165
2017-12-09T23:09:53.147505: step 2303, loss 0.0494458, acc 0.984375, prec 0.0830887, recall 0.859192
2017-12-09T23:09:53.446088: step 2304, loss 0.376725, acc 0.984375, prec 0.0831187, recall 0.859244
2017-12-09T23:09:53.745153: step 2305, loss 0.153873, acc 0.960938, prec 0.0831277, recall 0.85927
2017-12-09T23:09:54.048652: step 2306, loss 0.0582968, acc 0.976562, prec 0.0831232, recall 0.85927
2017-12-09T23:09:54.348904: step 2307, loss 0.0825376, acc 0.976562, prec 0.0831353, recall 0.859297
2017-12-09T23:09:54.646042: step 2308, loss 0.120832, acc 0.96875, prec 0.0831458, recall 0.859323
2017-12-09T23:09:54.943184: step 2309, loss 0.287889, acc 0.96875, prec 0.0831893, recall 0.859401
2017-12-09T23:09:55.245006: step 2310, loss 0.1833, acc 0.976562, prec 0.0832343, recall 0.85948
2017-12-09T23:09:55.544795: step 2311, loss 0.0829275, acc 0.976562, prec 0.0832463, recall 0.859506
2017-12-09T23:09:55.842231: step 2312, loss 0.0386492, acc 0.992188, prec 0.0832613, recall 0.859532
2017-12-09T23:09:56.144708: step 2313, loss 0.154453, acc 0.96875, prec 0.0832718, recall 0.859558
2017-12-09T23:09:56.440757: step 2314, loss 0.126096, acc 0.984375, prec 0.0833018, recall 0.85961
2017-12-09T23:09:56.740204: step 2315, loss 0.145387, acc 0.945312, prec 0.0833078, recall 0.859636
2017-12-09T23:09:57.035040: step 2316, loss 0.978927, acc 0.96875, prec 0.0833198, recall 0.859503
2017-12-09T23:09:57.333635: step 2317, loss 0.0540102, acc 0.976562, prec 0.0833318, recall 0.859529
2017-12-09T23:09:57.627920: step 2318, loss 0.0881152, acc 0.96875, prec 0.0833918, recall 0.859633
2017-12-09T23:09:57.929349: step 2319, loss 0.0834985, acc 0.960938, prec 0.0833843, recall 0.859633
2017-12-09T23:09:58.228690: step 2320, loss 0.203393, acc 0.9375, prec 0.0833888, recall 0.859659
2017-12-09T23:09:58.527354: step 2321, loss 0.160465, acc 0.953125, prec 0.0834128, recall 0.859711
2017-12-09T23:09:58.829416: step 2322, loss 0.253031, acc 0.96875, prec 0.0834891, recall 0.859841
2017-12-09T23:09:59.132801: step 2323, loss 0.10677, acc 0.976562, prec 0.0835011, recall 0.859867
2017-12-09T23:09:59.428671: step 2324, loss 1.77799, acc 0.921875, prec 0.0835206, recall 0.85976
2017-12-09T23:09:59.729275: step 2325, loss 0.430856, acc 0.890625, prec 0.0835489, recall 0.859837
2017-12-09T23:10:00.041380: step 2326, loss 0.166978, acc 0.9375, prec 0.0835369, recall 0.859837
2017-12-09T23:10:00.344777: step 2327, loss 0.147336, acc 0.921875, prec 0.0835384, recall 0.859863
2017-12-09T23:10:00.635957: step 2328, loss 0.203343, acc 0.929688, prec 0.0835743, recall 0.859941
2017-12-09T23:10:00.937589: step 2329, loss 0.254212, acc 0.921875, prec 0.0835593, recall 0.859941
2017-12-09T23:10:01.234785: step 2330, loss 0.142014, acc 0.953125, prec 0.0835832, recall 0.859993
2017-12-09T23:10:01.529251: step 2331, loss 0.225508, acc 0.921875, prec 0.0836011, recall 0.860044
2017-12-09T23:10:01.824174: step 2332, loss 0.282894, acc 0.929688, prec 0.0836369, recall 0.860122
2017-12-09T23:10:02.122126: step 2333, loss 0.241135, acc 0.914062, prec 0.0836861, recall 0.860225
2017-12-09T23:10:02.418897: step 2334, loss 0.067495, acc 0.976562, prec 0.0836981, recall 0.860251
2017-12-09T23:10:02.713780: step 2335, loss 0.315229, acc 0.875, prec 0.0837069, recall 0.860302
2017-12-09T23:10:03.013801: step 2336, loss 0.154514, acc 0.921875, prec 0.0837083, recall 0.860328
2017-12-09T23:10:03.310825: step 2337, loss 0.154704, acc 0.953125, prec 0.0837322, recall 0.860379
2017-12-09T23:10:03.606960: step 2338, loss 0.337205, acc 0.921875, prec 0.0837664, recall 0.860457
2017-12-09T23:10:03.904061: step 2339, loss 0.31179, acc 0.960938, prec 0.0837918, recall 0.860508
2017-12-09T23:10:04.202087: step 2340, loss 0.0802378, acc 0.984375, prec 0.0838544, recall 0.860611
2017-12-09T23:10:04.503046: step 2341, loss 0.163935, acc 0.945312, prec 0.0839096, recall 0.860713
2017-12-09T23:10:04.811736: step 2342, loss 0.121374, acc 0.960938, prec 0.0839349, recall 0.860764
2017-12-09T23:10:05.112796: step 2343, loss 0.132622, acc 0.96875, prec 0.0839781, recall 0.860841
2017-12-09T23:10:05.426119: step 2344, loss 0.17806, acc 0.960938, prec 0.083987, recall 0.860866
2017-12-09T23:10:05.729830: step 2345, loss 0.107648, acc 0.96875, prec 0.0840138, recall 0.860917
2017-12-09T23:10:06.027862: step 2346, loss 0.218864, acc 0.984375, prec 0.0840599, recall 0.860994
2017-12-09T23:10:06.329938: step 2347, loss 0.164311, acc 0.976562, prec 0.0840882, recall 0.861045
2017-12-09T23:10:06.627611: step 2348, loss 0.168728, acc 0.976562, prec 0.0841165, recall 0.861096
2017-12-09T23:10:06.925070: step 2349, loss 0.123538, acc 0.984375, prec 0.0841463, recall 0.861147
2017-12-09T23:10:07.227580: step 2350, loss 0.0911874, acc 0.96875, prec 0.084173, recall 0.861198
2017-12-09T23:10:07.525817: step 2351, loss 0.0578416, acc 0.984375, prec 0.08417, recall 0.861198
2017-12-09T23:10:07.824801: step 2352, loss 0.120651, acc 0.976562, prec 0.0841819, recall 0.861223
2017-12-09T23:10:08.122400: step 2353, loss 0.036057, acc 0.992188, prec 0.0842296, recall 0.861299
2017-12-09T23:10:08.423156: step 2354, loss 0.0205919, acc 0.992188, prec 0.0842444, recall 0.861325
2017-12-09T23:10:08.722777: step 2355, loss 0.0397949, acc 0.976562, prec 0.0842563, recall 0.86135
2017-12-09T23:10:09.027414: step 2356, loss 1.00601, acc 0.992188, prec 0.0843367, recall 0.861477
2017-12-09T23:10:09.330745: step 2357, loss 0.0692697, acc 1, prec 0.0844022, recall 0.861578
2017-12-09T23:10:09.628810: step 2358, loss 0.141292, acc 0.960938, prec 0.0844111, recall 0.861603
2017-12-09T23:10:09.929359: step 2359, loss 0.204691, acc 0.976562, prec 0.0844557, recall 0.861679
2017-12-09T23:10:10.227880: step 2360, loss 0.104032, acc 0.984375, prec 0.0844854, recall 0.861729
2017-12-09T23:10:10.530464: step 2361, loss 0.252691, acc 0.953125, prec 0.0845254, recall 0.861805
2017-12-09T23:10:10.835076: step 2362, loss 0.06533, acc 0.976562, prec 0.0845209, recall 0.861805
2017-12-09T23:10:11.131712: step 2363, loss 0.311846, acc 0.945312, prec 0.0845758, recall 0.861906
2017-12-09T23:10:11.427361: step 2364, loss 0.0542221, acc 0.984375, prec 0.0845728, recall 0.861906
2017-12-09T23:10:11.723705: step 2365, loss 0.139698, acc 0.945312, prec 0.0845949, recall 0.861956
2017-12-09T23:10:12.018871: step 2366, loss 0.117902, acc 0.976562, prec 0.0846231, recall 0.862006
2017-12-09T23:10:12.323646: step 2367, loss 0.143221, acc 0.953125, prec 0.0846467, recall 0.862056
2017-12-09T23:10:12.622985: step 2368, loss 0.150979, acc 0.953125, prec 0.0846704, recall 0.862107
2017-12-09T23:10:12.918526: step 2369, loss 0.169658, acc 0.9375, prec 0.084691, recall 0.862157
2017-12-09T23:10:13.213890: step 2370, loss 0.152847, acc 0.96875, prec 0.0847176, recall 0.862207
2017-12-09T23:10:13.512708: step 2371, loss 0.123908, acc 0.953125, prec 0.0847576, recall 0.862282
2017-12-09T23:10:13.805106: step 2372, loss 0.209338, acc 0.9375, prec 0.0847945, recall 0.862357
2017-12-09T23:10:14.098828: step 2373, loss 0.179775, acc 0.960938, prec 0.0847869, recall 0.862357
2017-12-09T23:10:14.402805: step 2374, loss 0.252106, acc 0.914062, prec 0.0847703, recall 0.862357
2017-12-09T23:10:14.701739: step 2375, loss 0.131901, acc 0.960938, prec 0.084828, recall 0.862457
2017-12-09T23:10:15.005598: step 2376, loss 0.0978476, acc 0.960938, prec 0.0848368, recall 0.862482
2017-12-09T23:10:15.303983: step 2377, loss 0.163466, acc 0.976562, prec 0.0848649, recall 0.862532
2017-12-09T23:10:15.605069: step 2378, loss 0.198026, acc 0.929688, prec 0.0848513, recall 0.862532
2017-12-09T23:10:15.904695: step 2379, loss 0.0972599, acc 0.960938, prec 0.0848927, recall 0.862606
2017-12-09T23:10:16.204197: step 2380, loss 0.106581, acc 0.976562, prec 0.0849208, recall 0.862656
2017-12-09T23:10:16.504627: step 2381, loss 0.144319, acc 0.96875, prec 0.0849637, recall 0.862731
2017-12-09T23:10:16.804127: step 2382, loss 0.464062, acc 1, prec 0.0850127, recall 0.862805
2017-12-09T23:10:17.105085: step 2383, loss 0.252496, acc 0.976562, prec 0.0850897, recall 0.862929
2017-12-09T23:10:17.404151: step 2384, loss 0.0974364, acc 0.984375, prec 0.085103, recall 0.862954
2017-12-09T23:10:17.705927: step 2385, loss 0.11751, acc 0.953125, prec 0.0850939, recall 0.862954
2017-12-09T23:10:18.008868: step 2386, loss 0.224498, acc 0.984375, prec 0.0851071, recall 0.862979
2017-12-09T23:10:18.310182: step 2387, loss 0.0518391, acc 0.976562, prec 0.0851026, recall 0.862979
2017-12-09T23:10:18.606110: step 2388, loss 0.0677759, acc 0.976562, prec 0.0851307, recall 0.863029
2017-12-09T23:10:18.900758: step 2389, loss 0.100442, acc 0.960938, prec 0.0852046, recall 0.863152
2017-12-09T23:10:19.201995: step 2390, loss 0.133117, acc 0.992188, prec 0.085252, recall 0.863226
2017-12-09T23:10:19.502824: step 2391, loss 0.257301, acc 0.929688, prec 0.0852709, recall 0.863276
2017-12-09T23:10:19.800867: step 2392, loss 0.0375135, acc 1, prec 0.0853035, recall 0.863325
2017-12-09T23:10:20.098684: step 2393, loss 0.114305, acc 0.960938, prec 0.0852959, recall 0.863325
2017-12-09T23:10:20.409041: step 2394, loss 0.186193, acc 0.929688, prec 0.0853311, recall 0.863399
2017-12-09T23:10:20.711458: step 2395, loss 0.28428, acc 0.984375, prec 0.0853444, recall 0.863423
2017-12-09T23:10:21.009710: step 2396, loss 0.0830194, acc 0.96875, prec 0.0853871, recall 0.863497
2017-12-09T23:10:21.311444: step 2397, loss 0.179678, acc 0.96875, prec 0.0853973, recall 0.863522
2017-12-09T23:10:21.614160: step 2398, loss 0.124908, acc 0.953125, prec 0.0854045, recall 0.863546
2017-12-09T23:10:21.910995: step 2399, loss 0.288015, acc 0.953125, prec 0.0854117, recall 0.863571
2017-12-09T23:10:22.206349: step 2400, loss 0.119272, acc 0.976562, prec 0.0854559, recall 0.863645

Evaluation:
2017-12-09T23:10:26.932154: step 2400, loss 3.0606, acc 0.960087, prec 0.0858865, recall 0.850192

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_128_fold_3/1512878245/checkpoints/model-2400

2017-12-09T23:10:28.234343: step 2401, loss 0.099014, acc 0.976562, prec 0.085882, recall 0.850192
2017-12-09T23:10:28.532458: step 2402, loss 0.219237, acc 0.953125, prec 0.085889, recall 0.850219
2017-12-09T23:10:28.831013: step 2403, loss 0.131443, acc 0.976562, prec 0.0859329, recall 0.850297
2017-12-09T23:10:29.128328: step 2404, loss 0.16596, acc 0.945312, prec 0.0859546, recall 0.85035
2017-12-09T23:10:29.428577: step 2405, loss 0.0502804, acc 0.984375, prec 0.0859677, recall 0.850376
2017-12-09T23:10:29.729403: step 2406, loss 0.160401, acc 0.945312, prec 0.0859894, recall 0.850428
2017-12-09T23:10:30.031787: step 2407, loss 0.310257, acc 0.976562, prec 0.086001, recall 0.850454
2017-12-09T23:10:30.336449: step 2408, loss 0.141238, acc 0.96875, prec 0.0860111, recall 0.85048
2017-12-09T23:10:30.637216: step 2409, loss 0.183469, acc 0.984375, prec 0.0860565, recall 0.850559
2017-12-09T23:10:30.942404: step 2410, loss 0.207584, acc 0.984375, prec 0.0860857, recall 0.850611
2017-12-09T23:10:31.241177: step 2411, loss 0.136728, acc 0.976562, prec 0.0861296, recall 0.850689
2017-12-09T23:10:31.545762: step 2412, loss 0.0662686, acc 0.984375, prec 0.0861265, recall 0.850689
2017-12-09T23:10:31.846894: step 2413, loss 0.270343, acc 0.984375, prec 0.086188, recall 0.850793
2017-12-09T23:10:32.152130: step 2414, loss 0.277235, acc 0.96875, prec 0.0862303, recall 0.850871
2017-12-09T23:10:32.451843: step 2415, loss 0.139856, acc 0.96875, prec 0.0862565, recall 0.850923
2017-12-09T23:10:32.748445: step 2416, loss 0.127932, acc 0.960938, prec 0.0862812, recall 0.850975
2017-12-09T23:10:33.048680: step 2417, loss 0.228775, acc 0.96875, prec 0.0863234, recall 0.851053
2017-12-09T23:10:33.355501: step 2418, loss 0.0731093, acc 0.960938, prec 0.0863481, recall 0.851105
2017-12-09T23:10:33.656962: step 2419, loss 0.0660094, acc 0.96875, prec 0.086342, recall 0.851105
2017-12-09T23:10:33.953967: step 2420, loss 0.1643, acc 0.960938, prec 0.0863666, recall 0.851156
2017-12-09T23:10:34.256615: step 2421, loss 0.188076, acc 0.945312, prec 0.0863559, recall 0.851156
2017-12-09T23:10:34.553831: step 2422, loss 0.167785, acc 0.921875, prec 0.0863407, recall 0.851156
2017-12-09T23:10:34.850684: step 2423, loss 0.116923, acc 0.945312, prec 0.0863462, recall 0.851182
2017-12-09T23:10:35.147135: step 2424, loss 0.0947919, acc 0.976562, prec 0.0863577, recall 0.851208
2017-12-09T23:10:35.455663: step 2425, loss 0.342688, acc 0.921875, prec 0.0863747, recall 0.85126
2017-12-09T23:10:35.757640: step 2426, loss 0.0800359, acc 0.976562, prec 0.0863701, recall 0.85126
2017-12-09T23:10:36.057284: step 2427, loss 0.0999905, acc 0.953125, prec 0.0863771, recall 0.851286
2017-12-09T23:10:36.357970: step 2428, loss 0.105454, acc 0.96875, prec 0.086371, recall 0.851286
2017-12-09T23:10:36.659004: step 2429, loss 0.0878531, acc 0.976562, prec 0.0863664, recall 0.851286
2017-12-09T23:10:36.954738: step 2430, loss 0.070319, acc 0.96875, prec 0.0863765, recall 0.851311
2017-12-09T23:10:37.258263: step 2431, loss 0.162224, acc 0.953125, prec 0.0863995, recall 0.851363
2017-12-09T23:10:37.562044: step 2432, loss 0.252563, acc 0.976562, prec 0.0864593, recall 0.851466
2017-12-09T23:10:37.858154: step 2433, loss 0.506887, acc 0.953125, prec 0.0864985, recall 0.851544
2017-12-09T23:10:38.160281: step 2434, loss 0.11538, acc 0.992188, prec 0.0865452, recall 0.851621
2017-12-09T23:10:38.457299: step 2435, loss 0.144891, acc 0.960938, prec 0.0865537, recall 0.851646
2017-12-09T23:10:38.756082: step 2436, loss 0.14062, acc 0.976562, prec 0.0865491, recall 0.851646
2017-12-09T23:10:39.053446: step 2437, loss 0.0749419, acc 0.96875, prec 0.0865591, recall 0.851672
2017-12-09T23:10:39.353325: step 2438, loss 0.243491, acc 0.976562, prec 0.0866189, recall 0.851775
2017-12-09T23:10:39.653300: step 2439, loss 0.0511169, acc 0.984375, prec 0.0866319, recall 0.851801
2017-12-09T23:10:39.950488: step 2440, loss 0.0458345, acc 0.984375, prec 0.0866449, recall 0.851826
2017-12-09T23:10:40.252833: step 2441, loss 0.120244, acc 0.992188, prec 0.0866917, recall 0.851903
2017-12-09T23:10:40.551429: step 2442, loss 0.209125, acc 0.96875, prec 0.0867499, recall 0.852006
2017-12-09T23:10:40.851635: step 2443, loss 0.0994023, acc 0.96875, prec 0.0867438, recall 0.852006
2017-12-09T23:10:41.149901: step 2444, loss 0.0444825, acc 0.984375, prec 0.0867568, recall 0.852031
2017-12-09T23:10:41.453559: step 2445, loss 0.100147, acc 0.976562, prec 0.0867683, recall 0.852057
2017-12-09T23:10:41.750980: step 2446, loss 0.100864, acc 0.984375, prec 0.0867652, recall 0.852057
2017-12-09T23:10:42.050560: step 2447, loss 0.0747177, acc 0.96875, prec 0.0867591, recall 0.852057
2017-12-09T23:10:42.346010: step 2448, loss 0.0551607, acc 0.992188, prec 0.0868219, recall 0.852159
2017-12-09T23:10:42.642002: step 2449, loss 0.0563937, acc 0.992188, prec 0.0868525, recall 0.85221
2017-12-09T23:10:42.938000: step 2450, loss 0.134544, acc 0.953125, prec 0.0868594, recall 0.852235
2017-12-09T23:10:43.238107: step 2451, loss 0.0488497, acc 1, prec 0.0869076, recall 0.852312
2017-12-09T23:10:43.539602: step 2452, loss 0.146594, acc 0.960938, prec 0.086916, recall 0.852337
2017-12-09T23:10:43.837924: step 2453, loss 0.29507, acc 0.984375, prec 0.086929, recall 0.852363
2017-12-09T23:10:44.137915: step 2454, loss 0.214167, acc 0.984375, prec 0.0869902, recall 0.852465
2017-12-09T23:10:44.441686: step 2455, loss 0.0628794, acc 0.96875, prec 0.0870001, recall 0.85249
2017-12-09T23:10:44.741112: step 2456, loss 0.087715, acc 0.96875, prec 0.086994, recall 0.85249
2017-12-09T23:10:45.038730: step 2457, loss 0.182831, acc 0.945312, prec 0.0870154, recall 0.852541
2017-12-09T23:10:45.339910: step 2458, loss 0.25422, acc 0.921875, prec 0.0870001, recall 0.852541
2017-12-09T23:10:45.642700: step 2459, loss 0.0720759, acc 0.96875, prec 0.0870421, recall 0.852617
2017-12-09T23:10:45.944149: step 2460, loss 0.172729, acc 0.929688, prec 0.0870444, recall 0.852642
2017-12-09T23:10:46.243534: step 2461, loss 0.187589, acc 0.9375, prec 0.0870803, recall 0.852719
2017-12-09T23:10:46.540706: step 2462, loss 0.0775564, acc 0.984375, prec 0.0871253, recall 0.852794
2017-12-09T23:10:46.834416: step 2463, loss 0.0417855, acc 0.992188, prec 0.0871398, recall 0.85282
2017-12-09T23:10:47.140757: step 2464, loss 0.146064, acc 0.984375, prec 0.0871689, recall 0.85287
2017-12-09T23:10:47.440771: step 2465, loss 0.0582644, acc 0.96875, prec 0.0871627, recall 0.85287
2017-12-09T23:10:47.740345: step 2466, loss 0.121391, acc 0.960938, prec 0.0871871, recall 0.852921
2017-12-09T23:10:48.035057: step 2467, loss 0.292543, acc 0.921875, prec 0.0871879, recall 0.852946
2017-12-09T23:10:48.336459: step 2468, loss 0.718068, acc 0.953125, prec 0.0872268, recall 0.853022
2017-12-09T23:10:48.635051: step 2469, loss 0.111873, acc 0.976562, prec 0.0872542, recall 0.853072
2017-12-09T23:10:48.930672: step 2470, loss 0.127245, acc 0.976562, prec 0.0872817, recall 0.853123
2017-12-09T23:10:49.231184: step 2471, loss 0.313367, acc 0.953125, prec 0.0873205, recall 0.853198
2017-12-09T23:10:49.531717: step 2472, loss 0.0880603, acc 0.976562, prec 0.0873159, recall 0.853198
2017-12-09T23:10:49.824758: step 2473, loss 0.0278737, acc 0.992188, prec 0.0873464, recall 0.853249
2017-12-09T23:10:50.125541: step 2474, loss 0.112455, acc 0.976562, prec 0.0873739, recall 0.853299
2017-12-09T23:10:50.437772: step 2475, loss 0.345177, acc 0.984375, prec 0.0874349, recall 0.8534
2017-12-09T23:10:50.744450: step 2476, loss 0.275865, acc 0.953125, prec 0.0874737, recall 0.853475
2017-12-09T23:10:51.043519: step 2477, loss 0.292135, acc 0.96875, prec 0.0875156, recall 0.85355
2017-12-09T23:10:51.340848: step 2478, loss 0.15781, acc 0.976562, prec 0.087591, recall 0.853675
2017-12-09T23:10:51.633509: step 2479, loss 0.0801632, acc 0.96875, prec 0.0875848, recall 0.853675
2017-12-09T23:10:51.932642: step 2480, loss 0.153865, acc 0.960938, prec 0.0876092, recall 0.853725
2017-12-09T23:10:52.231332: step 2481, loss 0.111747, acc 0.945312, prec 0.0876304, recall 0.853775
2017-12-09T23:10:52.527492: step 2482, loss 0.0586557, acc 0.976562, prec 0.0876258, recall 0.853775
2017-12-09T23:10:52.826218: step 2483, loss 0.0706659, acc 0.984375, prec 0.0876707, recall 0.85385
2017-12-09T23:10:53.125494: step 2484, loss 0.129082, acc 0.953125, prec 0.0876775, recall 0.853875
2017-12-09T23:10:53.425183: step 2485, loss 0.127853, acc 0.96875, prec 0.0877033, recall 0.853925
2017-12-09T23:10:53.721332: step 2486, loss 0.217106, acc 0.96875, prec 0.0877611, recall 0.854025
2017-12-09T23:10:54.021603: step 2487, loss 0.125604, acc 0.992188, prec 0.0878075, recall 0.854099
2017-12-09T23:10:54.323535: step 2488, loss 0.0991137, acc 0.96875, prec 0.0878014, recall 0.854099
2017-12-09T23:10:54.617803: step 2489, loss 0.235311, acc 0.929688, prec 0.0878195, recall 0.854149
2017-12-09T23:10:54.800714: step 2490, loss 0.0582946, acc 0.980769, prec 0.0878339, recall 0.854174
Training finished
Starting Experiment - batch_size_256 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 256
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_256_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055

Start training
2017-12-09T23:10:58.869410: step 1, loss 7.12182, acc 0.761719, prec 0.0172414, recall 0.2
2017-12-09T23:10:59.378929: step 2, loss 5.13189, acc 0.605469, prec 0.0126582, recall 0.25
2017-12-09T23:10:59.877192: step 3, loss 5.92454, acc 0.34375, prec 0.00920245, recall 0.3
2017-12-09T23:11:00.389017: step 4, loss 5.4354, acc 0.316406, prec 0.00996016, recall 0.384615
2017-12-09T23:11:00.893514: step 5, loss 4.66282, acc 0.21875, prec 0.0113475, recall 0.5
2017-12-09T23:11:01.394953: step 6, loss 5.30375, acc 0.300781, prec 0.010181, recall 0.5
2017-12-09T23:11:01.896702: step 7, loss 6.02472, acc 0.296875, prec 0.0103383, recall 0.5
2017-12-09T23:11:02.399653: step 8, loss 6.28949, acc 0.347656, prec 0.0105605, recall 0.5
2017-12-09T23:11:02.901526: step 9, loss 14.9426, acc 0.359375, prec 0.0107759, recall 0.454545
2017-12-09T23:11:03.400791: step 10, loss 4.87167, acc 0.382812, prec 0.0103226, recall 0.457143
2017-12-09T23:11:03.900578: step 11, loss 4.08935, acc 0.34375, prec 0.0116144, recall 0.512821
2017-12-09T23:11:04.405997: step 12, loss 3.50253, acc 0.339844, prec 0.0116218, recall 0.536585
2017-12-09T23:11:04.917144: step 13, loss 2.99325, acc 0.40625, prec 0.0107579, recall 0.536585
2017-12-09T23:11:05.427480: step 14, loss 2.4957, acc 0.496094, prec 0.0101196, recall 0.536585
2017-12-09T23:11:05.930267: step 15, loss 3.57159, acc 0.597656, prec 0.0105402, recall 0.533333
2017-12-09T23:11:06.434953: step 16, loss 2.28824, acc 0.671875, prec 0.0110076, recall 0.541667
2017-12-09T23:11:06.942586: step 17, loss 2.9228, acc 0.675781, prec 0.0110429, recall 0.54
2017-12-09T23:11:07.445269: step 18, loss 1.85722, acc 0.714844, prec 0.0111199, recall 0.538462
2017-12-09T23:11:07.947826: step 19, loss 11.5255, acc 0.765625, prec 0.0116505, recall 0.508475
2017-12-09T23:11:08.447259: step 20, loss 1.02066, acc 0.742188, prec 0.0113593, recall 0.508475
2017-12-09T23:11:08.949964: step 21, loss 1.37572, acc 0.652344, prec 0.0113512, recall 0.516667
2017-12-09T23:11:09.452954: step 22, loss 9.70058, acc 0.675781, prec 0.012104, recall 0.478873
2017-12-09T23:11:09.956392: step 23, loss 3.83527, acc 0.617188, prec 0.0120482, recall 0.466667
2017-12-09T23:11:10.458446: step 24, loss 2.65977, acc 0.523438, prec 0.0128713, recall 0.4875
2017-12-09T23:11:10.956957: step 25, loss 5.95289, acc 0.488281, prec 0.0132785, recall 0.5
2017-12-09T23:11:11.466212: step 26, loss 3.52197, acc 0.371094, prec 0.0141184, recall 0.52809
2017-12-09T23:11:11.969916: step 27, loss 3.62065, acc 0.316406, prec 0.0136947, recall 0.533333
2017-12-09T23:11:12.474864: step 28, loss 3.74609, acc 0.324219, prec 0.0143904, recall 0.557895
2017-12-09T23:11:12.974893: step 29, loss 3.61831, acc 0.34375, prec 0.0142746, recall 0.56701
2017-12-09T23:11:13.470376: step 30, loss 3.54548, acc 0.386719, prec 0.0156794, recall 0.6
2017-12-09T23:11:13.972995: step 31, loss 3.05226, acc 0.429688, prec 0.0165468, recall 0.621622
2017-12-09T23:11:14.477148: step 32, loss 8.11858, acc 0.445312, prec 0.0164695, recall 0.612069
2017-12-09T23:11:14.977555: step 33, loss 3.28414, acc 0.46875, prec 0.0164119, recall 0.613445
2017-12-09T23:11:15.472647: step 34, loss 8.64272, acc 0.519531, prec 0.0161961, recall 0.601626
2017-12-09T23:11:15.974594: step 35, loss 4.45122, acc 0.527344, prec 0.016624, recall 0.604651
2017-12-09T23:11:16.480055: step 36, loss 2.96378, acc 0.46875, prec 0.0165631, recall 0.610687
2017-12-09T23:11:16.980501: step 37, loss 2.5841, acc 0.554688, prec 0.01698, recall 0.617647
2017-12-09T23:11:17.479779: step 38, loss 5.92181, acc 0.546875, prec 0.0169826, recall 0.618705
2017-12-09T23:11:17.984097: step 39, loss 2.78077, acc 0.476562, prec 0.016734, recall 0.621429
2017-12-09T23:11:18.486654: step 40, loss 2.10714, acc 0.570312, prec 0.0173128, recall 0.634483
2017-12-09T23:11:18.986689: step 41, loss 2.61396, acc 0.589844, prec 0.0180678, recall 0.644737
2017-12-09T23:11:19.491935: step 42, loss 4.72553, acc 0.636719, prec 0.0179478, recall 0.63871
2017-12-09T23:11:19.997961: step 43, loss 2.3648, acc 0.574219, prec 0.0177778, recall 0.636943
2017-12-09T23:11:20.513014: step 44, loss 3.29681, acc 0.65625, prec 0.0180227, recall 0.639752
2017-12-09T23:11:21.015010: step 45, loss 6.15802, acc 0.574219, prec 0.0181974, recall 0.638554
2017-12-09T23:11:21.525877: step 46, loss 5.36466, acc 0.636719, prec 0.0184184, recall 0.633721
2017-12-09T23:11:22.036572: step 47, loss 2.85501, acc 0.621094, prec 0.0182876, recall 0.632184
2017-12-09T23:11:22.541967: step 48, loss 4.26766, acc 0.621094, prec 0.0184822, recall 0.634831
2017-12-09T23:11:23.050702: step 49, loss 4.1819, acc 0.554688, prec 0.0186226, recall 0.63388
2017-12-09T23:11:23.557955: step 50, loss 2.27874, acc 0.554688, prec 0.0182879, recall 0.63388
2017-12-09T23:11:24.064743: step 51, loss 2.41016, acc 0.515625, prec 0.0180891, recall 0.63587
2017-12-09T23:11:24.566894: step 52, loss 3.11036, acc 0.53125, prec 0.0182094, recall 0.638298
2017-12-09T23:11:25.068735: step 53, loss 3.07674, acc 0.523438, prec 0.0180274, recall 0.636842
2017-12-09T23:11:25.575866: step 54, loss 2.48791, acc 0.503906, prec 0.0181234, recall 0.642487
2017-12-09T23:11:26.104909: step 55, loss 3.43206, acc 0.554688, prec 0.0186755, recall 0.65
2017-12-09T23:11:26.607031: step 56, loss 3.87859, acc 0.574219, prec 0.018529, recall 0.648515
2017-12-09T23:11:27.107306: step 57, loss 3.71267, acc 0.589844, prec 0.0185366, recall 0.645631
2017-12-09T23:11:27.614478: step 58, loss 6.99989, acc 0.605469, prec 0.0184192, recall 0.641148
2017-12-09T23:11:28.127512: step 59, loss 8.59545, acc 0.628906, prec 0.0181892, recall 0.632075
2017-12-09T23:11:28.631831: step 60, loss 1.55586, acc 0.660156, prec 0.0185036, recall 0.638889
2017-12-09T23:11:29.135741: step 61, loss 2.38806, acc 0.582031, prec 0.0183741, recall 0.637615
2017-12-09T23:11:29.637052: step 62, loss 5.68791, acc 0.574219, prec 0.0185065, recall 0.631111
2017-12-09T23:11:30.149681: step 63, loss 6.60316, acc 0.605469, prec 0.0185233, recall 0.628821
2017-12-09T23:11:30.658951: step 64, loss 2.52138, acc 0.542969, prec 0.018746, recall 0.635193
2017-12-09T23:11:31.161035: step 65, loss 2.66565, acc 0.464844, prec 0.0186707, recall 0.638298
2017-12-09T23:11:31.669893: step 66, loss 2.81861, acc 0.476562, prec 0.0186047, recall 0.64135
2017-12-09T23:11:32.178429: step 67, loss 3.06672, acc 0.566406, prec 0.0188315, recall 0.644628
2017-12-09T23:11:32.687500: step 68, loss 2.93489, acc 0.523438, prec 0.0187916, recall 0.647541
2017-12-09T23:11:33.191452: step 69, loss 2.22401, acc 0.613281, prec 0.0190364, recall 0.650602
2017-12-09T23:11:33.700866: step 70, loss 2.2561, acc 0.589844, prec 0.019146, recall 0.654762
2017-12-09T23:11:34.205765: step 71, loss 4.56016, acc 0.609375, prec 0.0191536, recall 0.654902
2017-12-09T23:11:34.705095: step 72, loss 1.64475, acc 0.628906, prec 0.0190584, recall 0.65625
2017-12-09T23:11:35.200544: step 73, loss 1.37797, acc 0.675781, prec 0.0189909, recall 0.657588
2017-12-09T23:11:35.723278: step 74, loss 1.88413, acc 0.660156, prec 0.0193527, recall 0.664122
2017-12-09T23:11:36.228976: step 75, loss 0.912783, acc 0.78125, prec 0.0194497, recall 0.666667
2017-12-09T23:11:36.732305: step 76, loss 4.56851, acc 0.792969, prec 0.0198792, recall 0.667897
2017-12-09T23:11:37.248980: step 77, loss 4.64538, acc 0.78125, prec 0.0198712, recall 0.661818
2017-12-09T23:11:37.764926: step 78, loss 0.829584, acc 0.792969, prec 0.0197568, recall 0.661818
2017-12-09T23:11:38.263979: step 79, loss 8.2701, acc 0.785156, prec 0.0201791, recall 0.65614
2017-12-09T23:11:38.765653: step 80, loss 2.10598, acc 0.753906, prec 0.020465, recall 0.658621
2017-12-09T23:11:39.266916: step 81, loss 4.03583, acc 0.664062, prec 0.0208024, recall 0.659933
2017-12-09T23:11:39.776493: step 82, loss 1.94039, acc 0.59375, prec 0.0207809, recall 0.662207
2017-12-09T23:11:40.279500: step 83, loss 2.27227, acc 0.589844, prec 0.0208593, recall 0.665563
2017-12-09T23:11:40.786604: step 84, loss 4.04318, acc 0.457031, prec 0.0207673, recall 0.663399
2017-12-09T23:11:41.290610: step 85, loss 3.11761, acc 0.5, prec 0.0209953, recall 0.666667
2017-12-09T23:11:41.795696: step 86, loss 3.40261, acc 0.421875, prec 0.0210757, recall 0.670886
2017-12-09T23:11:42.300139: step 87, loss 2.6921, acc 0.476562, prec 0.0211827, recall 0.675
2017-12-09T23:11:42.804638: step 88, loss 4.25847, acc 0.472656, prec 0.0210995, recall 0.67284
2017-12-09T23:11:43.302054: step 89, loss 3.98161, acc 0.5, prec 0.0211241, recall 0.673781
2017-12-09T23:11:43.804783: step 90, loss 2.92945, acc 0.484375, prec 0.0212304, recall 0.677711
2017-12-09T23:11:44.310971: step 91, loss 4.67635, acc 0.539062, prec 0.0214552, recall 0.680473
2017-12-09T23:11:44.814349: step 92, loss 4.93379, acc 0.484375, prec 0.0211982, recall 0.676471
2017-12-09T23:11:45.321017: step 93, loss 2.34571, acc 0.507812, prec 0.0212223, recall 0.6793
2017-12-09T23:11:45.825641: step 94, loss 3.81971, acc 0.578125, prec 0.0215471, recall 0.682857
2017-12-09T23:11:46.339659: step 95, loss 2.31972, acc 0.601562, prec 0.0217003, recall 0.686441
2017-12-09T23:11:46.839540: step 96, loss 1.98304, acc 0.609375, prec 0.0216814, recall 0.688202
2017-12-09T23:11:47.342052: step 97, loss 3.56375, acc 0.632812, prec 0.021762, recall 0.688889
2017-12-09T23:11:47.854920: step 98, loss 1.86523, acc 0.640625, prec 0.0217599, recall 0.688705
2017-12-09T23:11:48.361887: step 99, loss 1.53787, acc 0.703125, prec 0.0219552, recall 0.692098
2017-12-09T23:11:48.863958: step 100, loss 2.89386, acc 0.742188, prec 0.0220847, recall 0.692722
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-100

2017-12-09T23:11:50.392573: step 101, loss 3.9396, acc 0.714844, prec 0.0220362, recall 0.688
2017-12-09T23:11:50.902073: step 102, loss 1.30894, acc 0.703125, prec 0.022226, recall 0.691293
2017-12-09T23:11:51.408839: step 103, loss 5.57787, acc 0.710938, prec 0.0222597, recall 0.685714
2017-12-09T23:11:51.918283: step 104, loss 4.44009, acc 0.722656, prec 0.02295, recall 0.690176
2017-12-09T23:11:52.421316: step 105, loss 2.98006, acc 0.691406, prec 0.0229636, recall 0.69
2017-12-09T23:11:52.925976: step 106, loss 5.15219, acc 0.589844, prec 0.0231695, recall 0.692118
2017-12-09T23:11:53.427051: step 107, loss 3.49158, acc 0.5, prec 0.0230888, recall 0.691932
2017-12-09T23:11:53.931750: step 108, loss 3.87949, acc 0.425781, prec 0.0228171, recall 0.690244
2017-12-09T23:11:54.433641: step 109, loss 3.28176, acc 0.398438, prec 0.0226151, recall 0.690998
2017-12-09T23:11:54.937290: step 110, loss 4.00398, acc 0.460938, prec 0.022448, recall 0.690073
2017-12-09T23:11:55.438439: step 111, loss 3.03353, acc 0.433594, prec 0.0225751, recall 0.69378
2017-12-09T23:11:55.936858: step 112, loss 3.29055, acc 0.425781, prec 0.0225454, recall 0.695962
2017-12-09T23:11:56.440523: step 113, loss 2.86937, acc 0.441406, prec 0.0225232, recall 0.698113
2017-12-09T23:11:56.938338: step 114, loss 2.35053, acc 0.523438, prec 0.0226845, recall 0.701632
2017-12-09T23:11:57.438281: step 115, loss 2.496, acc 0.570312, prec 0.0227918, recall 0.702765
2017-12-09T23:11:57.938313: step 116, loss 1.67554, acc 0.632812, prec 0.0227054, recall 0.703448
2017-12-09T23:11:58.446619: step 117, loss 5.58405, acc 0.683594, prec 0.0226451, recall 0.700913
2017-12-09T23:11:58.949430: step 118, loss 2.04926, acc 0.722656, prec 0.0226005, recall 0.7
2017-12-09T23:11:59.453766: step 119, loss 0.842179, acc 0.761719, prec 0.0224998, recall 0.7
2017-12-09T23:11:59.960758: step 120, loss 0.969885, acc 0.765625, prec 0.0224016, recall 0.7
2017-12-09T23:12:00.476986: step 121, loss 1.50188, acc 0.808594, prec 0.0225362, recall 0.70045
2017-12-09T23:12:00.978683: step 122, loss 3.09099, acc 0.84375, prec 0.0225434, recall 0.699552
2017-12-09T23:12:01.489386: step 123, loss 1.37798, acc 0.8125, prec 0.0227485, recall 0.700665
2017-12-09T23:12:02.000893: step 124, loss 1.6496, acc 0.8125, prec 0.0228121, recall 0.700441
2017-12-09T23:12:02.180082: step 125, loss 0.457324, acc 0.843137, prec 0.022799, recall 0.700441
2017-12-09T23:12:02.694284: step 126, loss 1.70272, acc 0.816406, prec 0.0228637, recall 0.700219
2017-12-09T23:12:03.202632: step 127, loss 0.635742, acc 0.84375, prec 0.0232161, recall 0.704104
2017-12-09T23:12:03.708705: step 128, loss 1.58801, acc 0.835938, prec 0.0231501, recall 0.701075
2017-12-09T23:12:04.212595: step 129, loss 1.1548, acc 0.824219, prec 0.0234237, recall 0.70276
2017-12-09T23:12:04.716207: step 130, loss 1.46971, acc 0.789062, prec 0.023405, recall 0.701903
2017-12-09T23:12:05.220148: step 131, loss 2.51868, acc 0.761719, prec 0.0233064, recall 0.700422
2017-12-09T23:12:05.745159: step 132, loss 3.92142, acc 0.742188, prec 0.0234735, recall 0.701461
2017-12-09T23:12:06.248672: step 133, loss 5.63293, acc 0.734375, prec 0.0237052, recall 0.701646
2017-12-09T23:12:06.754005: step 134, loss 2.04157, acc 0.628906, prec 0.0237536, recall 0.702041
2017-12-09T23:12:07.259678: step 135, loss 4.1669, acc 0.636719, prec 0.0237377, recall 0.701826
2017-12-09T23:12:07.764973: step 136, loss 2.59894, acc 0.554688, prec 0.0238873, recall 0.703407
2017-12-09T23:12:08.268027: step 137, loss 4.67747, acc 0.496094, prec 0.0237469, recall 0.702595
2017-12-09T23:12:08.773393: step 138, loss 2.43213, acc 0.492188, prec 0.0236057, recall 0.703187
2017-12-09T23:12:09.288219: step 139, loss 2.95982, acc 0.507812, prec 0.0236043, recall 0.703557
2017-12-09T23:12:09.786794: step 140, loss 2.45391, acc 0.464844, prec 0.0235201, recall 0.704724
2017-12-09T23:12:10.290483: step 141, loss 2.0611, acc 0.515625, prec 0.0233937, recall 0.705305
2017-12-09T23:12:10.795622: step 142, loss 2.49959, acc 0.5, prec 0.0232633, recall 0.705882
2017-12-09T23:12:11.299033: step 143, loss 1.80221, acc 0.5625, prec 0.0235347, recall 0.709865
2017-12-09T23:12:11.802064: step 144, loss 1.6031, acc 0.617188, prec 0.0235122, recall 0.710983
2017-12-09T23:12:12.303494: step 145, loss 1.48063, acc 0.667969, prec 0.0235093, recall 0.712092
2017-12-09T23:12:12.807301: step 146, loss 1.3032, acc 0.660156, prec 0.0235035, recall 0.713193
2017-12-09T23:12:13.305225: step 147, loss 0.938746, acc 0.777344, prec 0.0237871, recall 0.716446
2017-12-09T23:12:13.807620: step 148, loss 4.82709, acc 0.792969, prec 0.0237127, recall 0.712406
2017-12-09T23:12:14.313278: step 149, loss 1.70583, acc 0.8125, prec 0.023765, recall 0.71215
2017-12-09T23:12:14.815223: step 150, loss 0.653707, acc 0.8125, prec 0.0237547, recall 0.712687
2017-12-09T23:12:15.318871: step 151, loss 2.77328, acc 0.835938, prec 0.0236943, recall 0.711359
2017-12-09T23:12:15.824825: step 152, loss 0.657489, acc 0.824219, prec 0.0238699, recall 0.713494
2017-12-09T23:12:16.329913: step 153, loss 3.32513, acc 0.832031, prec 0.0239329, recall 0.709324
2017-12-09T23:12:16.839625: step 154, loss 2.40182, acc 0.859375, prec 0.024003, recall 0.707804
2017-12-09T23:12:17.343529: step 155, loss 2.73728, acc 0.863281, prec 0.0240157, recall 0.704504
2017-12-09T23:12:17.845283: step 156, loss 0.80365, acc 0.777344, prec 0.0239917, recall 0.705036
2017-12-09T23:12:18.346525: step 157, loss 1.02974, acc 0.742188, prec 0.0240141, recall 0.706093
2017-12-09T23:12:18.850693: step 158, loss 1.36933, acc 0.667969, prec 0.0240679, recall 0.707665
2017-12-09T23:12:19.350083: step 159, loss 1.78633, acc 0.703125, prec 0.0240768, recall 0.707447
2017-12-09T23:12:19.861289: step 160, loss 2.57992, acc 0.679688, prec 0.0241354, recall 0.707747
2017-12-09T23:12:20.368759: step 161, loss 1.89204, acc 0.609375, prec 0.0242243, recall 0.70979
2017-12-09T23:12:20.872258: step 162, loss 3.13865, acc 0.574219, prec 0.0243006, recall 0.710572
2017-12-09T23:12:21.372121: step 163, loss 1.57114, acc 0.597656, prec 0.0245554, recall 0.714041
2017-12-09T23:12:21.873914: step 164, loss 1.51462, acc 0.625, prec 0.0246458, recall 0.715986
2017-12-09T23:12:22.374398: step 165, loss 1.53006, acc 0.621094, prec 0.024677, recall 0.717428
2017-12-09T23:12:22.883006: step 166, loss 2.21565, acc 0.675781, prec 0.0247307, recall 0.716443
2017-12-09T23:12:23.390601: step 167, loss 1.23501, acc 0.679688, prec 0.02467, recall 0.716918
2017-12-09T23:12:23.894087: step 168, loss 2.7307, acc 0.710938, prec 0.0246786, recall 0.716667
2017-12-09T23:12:24.400462: step 169, loss 0.970562, acc 0.734375, prec 0.0246942, recall 0.717608
2017-12-09T23:12:24.904004: step 170, loss 0.949053, acc 0.765625, prec 0.0246653, recall 0.718076
2017-12-09T23:12:25.417880: step 171, loss 2.35221, acc 0.75, prec 0.0246339, recall 0.716172
2017-12-09T23:12:25.935212: step 172, loss 0.831416, acc 0.804688, prec 0.0246746, recall 0.717105
2017-12-09T23:12:26.445365: step 173, loss 2.65094, acc 0.816406, prec 0.0246684, recall 0.714052
2017-12-09T23:12:26.951085: step 174, loss 1.38037, acc 0.789062, prec 0.0247594, recall 0.714286
2017-12-09T23:12:27.458922: step 175, loss 0.871542, acc 0.777344, prec 0.0248991, recall 0.716129
2017-12-09T23:12:27.966791: step 176, loss 0.87338, acc 0.765625, prec 0.0249246, recall 0.717042
2017-12-09T23:12:28.469763: step 177, loss 0.847272, acc 0.792969, prec 0.0251769, recall 0.719745
2017-12-09T23:12:28.970151: step 178, loss 0.865912, acc 0.796875, prec 0.0252124, recall 0.720635
2017-12-09T23:12:29.477894: step 179, loss 1.91489, acc 0.796875, prec 0.025519, recall 0.722571
2017-12-09T23:12:29.995747: step 180, loss 3.55472, acc 0.757812, prec 0.0255944, recall 0.722741
2017-12-09T23:12:30.511776: step 181, loss 2.05012, acc 0.738281, prec 0.0256622, recall 0.72291
2017-12-09T23:12:31.014985: step 182, loss 2.14342, acc 0.785156, prec 0.025693, recall 0.72265
2017-12-09T23:12:31.517142: step 183, loss 1.3936, acc 0.683594, prec 0.0258451, recall 0.724771
2017-12-09T23:12:32.018933: step 184, loss 1.71645, acc 0.734375, prec 0.0259627, recall 0.725341
2017-12-09T23:12:32.527182: step 185, loss 5.17581, acc 0.703125, prec 0.0261208, recall 0.726316
2017-12-09T23:12:33.037391: step 186, loss 3.76857, acc 0.6875, prec 0.0261146, recall 0.726048
2017-12-09T23:12:33.542568: step 187, loss 2.77896, acc 0.65625, prec 0.026045, recall 0.725373
2017-12-09T23:12:34.049784: step 188, loss 2.11725, acc 0.570312, prec 0.0261532, recall 0.726331
2017-12-09T23:12:34.552143: step 189, loss 2.63797, acc 0.597656, prec 0.0263214, recall 0.727672
2017-12-09T23:12:35.060648: step 190, loss 1.87392, acc 0.578125, prec 0.0262742, recall 0.728467
2017-12-09T23:12:35.588841: step 191, loss 1.90873, acc 0.554688, prec 0.0263213, recall 0.730044
2017-12-09T23:12:36.087453: step 192, loss 1.90176, acc 0.59375, prec 0.0263815, recall 0.731602
2017-12-09T23:12:36.590073: step 193, loss 1.76631, acc 0.59375, prec 0.0264411, recall 0.733142
2017-12-09T23:12:37.111931: step 194, loss 1.34893, acc 0.652344, prec 0.0267209, recall 0.73617
2017-12-09T23:12:37.614866: step 195, loss 1.03171, acc 0.71875, prec 0.0268219, recall 0.737659
2017-12-09T23:12:38.120983: step 196, loss 0.918489, acc 0.734375, prec 0.0268779, recall 0.738764
2017-12-09T23:12:38.621050: step 197, loss 1.36358, acc 0.765625, prec 0.0269458, recall 0.738827
2017-12-09T23:12:39.127829: step 198, loss 1.15759, acc 0.746094, prec 0.0270051, recall 0.739917
2017-12-09T23:12:39.636221: step 199, loss 1.73812, acc 0.785156, prec 0.0272267, recall 0.741047
2017-12-09T23:12:40.141996: step 200, loss 2.26926, acc 0.851562, prec 0.0273724, recall 0.74145
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-200

2017-12-09T23:12:41.605133: step 201, loss 1.61929, acc 0.828125, prec 0.0273634, recall 0.739782
2017-12-09T23:12:42.109803: step 202, loss 0.622919, acc 0.847656, prec 0.0274565, recall 0.740841
2017-12-09T23:12:42.618322: step 203, loss 4.48379, acc 0.867188, prec 0.02751, recall 0.739541
2017-12-09T23:12:43.117797: step 204, loss 0.416846, acc 0.871094, prec 0.027562, recall 0.740242
2017-12-09T23:12:43.623765: step 205, loss 1.43623, acc 0.824219, prec 0.0277931, recall 0.741333
2017-12-09T23:12:44.130679: step 206, loss 0.631586, acc 0.816406, prec 0.0279218, recall 0.742706
2017-12-09T23:12:44.631371: step 207, loss 1.55135, acc 0.785156, prec 0.0279932, recall 0.741765
2017-12-09T23:12:45.141078: step 208, loss 0.981998, acc 0.796875, prec 0.028162, recall 0.743455
2017-12-09T23:12:45.641427: step 209, loss 1.84599, acc 0.75, prec 0.0281718, recall 0.742188
2017-12-09T23:12:46.147349: step 210, loss 1.58915, acc 0.738281, prec 0.0281295, recall 0.740597
2017-12-09T23:12:46.655902: step 211, loss 0.809755, acc 0.746094, prec 0.0281828, recall 0.741602
2017-12-09T23:12:47.156116: step 212, loss 1.02786, acc 0.71875, prec 0.0281787, recall 0.742268
2017-12-09T23:12:47.656818: step 213, loss 1.41291, acc 0.632812, prec 0.028239, recall 0.74359
2017-12-09T23:12:48.173037: step 214, loss 2.42694, acc 0.707031, prec 0.028279, recall 0.743622
2017-12-09T23:12:48.679485: step 215, loss 1.20269, acc 0.710938, prec 0.0283657, recall 0.744924
2017-12-09T23:12:49.183550: step 216, loss 1.60299, acc 0.664062, prec 0.0285288, recall 0.746851
2017-12-09T23:12:49.688056: step 217, loss 2.01845, acc 0.707031, prec 0.0284742, recall 0.746231
2017-12-09T23:12:50.193764: step 218, loss 1.96756, acc 0.707031, prec 0.0284664, recall 0.745932
2017-12-09T23:12:50.725333: step 219, loss 2.71531, acc 0.632812, prec 0.0285714, recall 0.746584
2017-12-09T23:12:51.226801: step 220, loss 1.21493, acc 0.648438, prec 0.0287256, recall 0.748459
2017-12-09T23:12:51.728645: step 221, loss 1.15489, acc 0.683594, prec 0.0288906, recall 0.750306
2017-12-09T23:12:52.227413: step 222, loss 1.27249, acc 0.636719, prec 0.0288101, recall 0.750611
2017-12-09T23:12:52.726865: step 223, loss 3.23422, acc 0.714844, prec 0.0288507, recall 0.749696
2017-12-09T23:12:53.227733: step 224, loss 1.12054, acc 0.679688, prec 0.0288309, recall 0.750303
2017-12-09T23:12:53.725057: step 225, loss 1.60254, acc 0.746094, prec 0.0289256, recall 0.750602
2017-12-09T23:12:54.231994: step 226, loss 0.937285, acc 0.734375, prec 0.0289245, recall 0.751202
2017-12-09T23:12:54.737551: step 227, loss 0.762507, acc 0.785156, prec 0.0290303, recall 0.752392
2017-12-09T23:12:55.246774: step 228, loss 1.44063, acc 0.722656, prec 0.0290262, recall 0.752086
2017-12-09T23:12:55.747273: step 229, loss 0.703293, acc 0.804688, prec 0.0290487, recall 0.752675
2017-12-09T23:12:56.251983: step 230, loss 0.658092, acc 0.824219, prec 0.0290778, recall 0.753262
2017-12-09T23:12:56.762225: step 231, loss 2.07462, acc 0.800781, prec 0.029057, recall 0.751773
2017-12-09T23:12:57.268159: step 232, loss 3.66321, acc 0.828125, prec 0.0291355, recall 0.75
2017-12-09T23:12:57.772721: step 233, loss 0.706631, acc 0.835938, prec 0.0292124, recall 0.750877
2017-12-09T23:12:58.286443: step 234, loss 0.465987, acc 0.847656, prec 0.0292047, recall 0.751168
2017-12-09T23:12:58.786930: step 235, loss 4.82939, acc 0.789062, prec 0.0292239, recall 0.75
2017-12-09T23:12:59.292249: step 236, loss 0.823015, acc 0.765625, prec 0.0291885, recall 0.75029
2017-12-09T23:12:59.792354: step 237, loss 0.928553, acc 0.730469, prec 0.0291415, recall 0.75058
2017-12-09T23:13:00.320753: step 238, loss 4.67696, acc 0.699219, prec 0.0293049, recall 0.750575
2017-12-09T23:13:00.820604: step 239, loss 2.87789, acc 0.703125, prec 0.0292947, recall 0.749428
2017-12-09T23:13:01.324168: step 240, loss 1.12351, acc 0.695312, prec 0.0291929, recall 0.749428
2017-12-09T23:13:01.821017: step 241, loss 2.43083, acc 0.636719, prec 0.0293321, recall 0.750284
2017-12-09T23:13:02.322632: step 242, loss 2.00429, acc 0.589844, prec 0.0293689, recall 0.750564
2017-12-09T23:13:02.830435: step 243, loss 2.01135, acc 0.496094, prec 0.0292878, recall 0.751126
2017-12-09T23:13:03.333820: step 244, loss 1.91857, acc 0.535156, prec 0.029178, recall 0.751406
2017-12-09T23:13:03.835570: step 245, loss 1.82558, acc 0.546875, prec 0.029284, recall 0.753073
2017-12-09T23:13:04.340848: step 246, loss 1.84052, acc 0.621094, prec 0.0292871, recall 0.753898
2017-12-09T23:13:04.846706: step 247, loss 1.86685, acc 0.585938, prec 0.0293624, recall 0.75526
2017-12-09T23:13:05.354545: step 248, loss 1.16209, acc 0.640625, prec 0.0293298, recall 0.755801
2017-12-09T23:13:05.866364: step 249, loss 1.41489, acc 0.683594, prec 0.0293125, recall 0.755507
2017-12-09T23:13:06.045083: step 250, loss 1.33522, acc 0.745098, prec 0.0292962, recall 0.755507
2017-12-09T23:13:06.550162: step 251, loss 0.890451, acc 0.753906, prec 0.0294656, recall 0.757112
2017-12-09T23:13:07.055901: step 252, loss 1.85367, acc 0.796875, prec 0.0296092, recall 0.756786
2017-12-09T23:13:07.564812: step 253, loss 0.7112, acc 0.839844, prec 0.02964, recall 0.757313
2017-12-09T23:13:08.070367: step 254, loss 0.50223, acc 0.871094, prec 0.0296397, recall 0.757576
2017-12-09T23:13:08.572097: step 255, loss 0.940608, acc 0.871094, prec 0.0298047, recall 0.758065
2017-12-09T23:13:09.075635: step 256, loss 5.54895, acc 0.878906, prec 0.0298501, recall 0.756959
2017-12-09T23:13:09.577206: step 257, loss 1.35583, acc 0.875, prec 0.029976, recall 0.756383
2017-12-09T23:13:10.084442: step 258, loss 0.779142, acc 0.855469, prec 0.030053, recall 0.756356
2017-12-09T23:13:10.594038: step 259, loss 0.618122, acc 0.878906, prec 0.0301769, recall 0.757384
2017-12-09T23:13:11.096993: step 260, loss 2.62436, acc 0.859375, prec 0.0302967, recall 0.756813
2017-12-09T23:13:11.610211: step 261, loss 0.946142, acc 0.839844, prec 0.0304477, recall 0.758081
2017-12-09T23:13:12.124275: step 262, loss 1.03473, acc 0.730469, prec 0.030441, recall 0.758585
2017-12-09T23:13:12.627093: step 263, loss 1.13524, acc 0.746094, prec 0.03052, recall 0.759585
2017-12-09T23:13:13.129378: step 264, loss 0.973212, acc 0.714844, prec 0.0305081, recall 0.760083
2017-12-09T23:13:13.635988: step 265, loss 1.18978, acc 0.675781, prec 0.0305236, recall 0.760825
2017-12-09T23:13:14.145841: step 266, loss 1.2599, acc 0.679688, prec 0.0305803, recall 0.761807
2017-12-09T23:13:14.646708: step 267, loss 1.27917, acc 0.746094, prec 0.030658, recall 0.762781
2017-12-09T23:13:15.162617: step 268, loss 2.28446, acc 0.6875, prec 0.0307982, recall 0.762677
2017-12-09T23:13:15.672202: step 269, loss 1.00128, acc 0.714844, prec 0.0308647, recall 0.763636
2017-12-09T23:13:16.175889: step 270, loss 1.11096, acc 0.785156, prec 0.0309929, recall 0.764824
2017-12-09T23:13:16.681534: step 271, loss 0.923171, acc 0.761719, prec 0.0309555, recall 0.76506
2017-12-09T23:13:17.188066: step 272, loss 0.857386, acc 0.761719, prec 0.0309575, recall 0.765531
2017-12-09T23:13:17.694221: step 273, loss 0.853369, acc 0.75, prec 0.0309557, recall 0.766
2017-12-09T23:13:18.196638: step 274, loss 2.96538, acc 0.738281, prec 0.0310309, recall 0.765408
2017-12-09T23:13:18.695599: step 275, loss 0.60995, acc 0.800781, prec 0.0310452, recall 0.765873
2017-12-09T23:13:19.195268: step 276, loss 0.617524, acc 0.808594, prec 0.031023, recall 0.766105
2017-12-09T23:13:19.696434: step 277, loss 1.69382, acc 0.816406, prec 0.0310435, recall 0.76581
2017-12-09T23:13:20.204697: step 278, loss 0.580612, acc 0.8125, prec 0.0310226, recall 0.766041
2017-12-09T23:13:20.725229: step 279, loss 0.764357, acc 0.804688, prec 0.0310767, recall 0.766732
2017-12-09T23:13:21.228877: step 280, loss 2.89798, acc 0.832031, prec 0.0311791, recall 0.766895
2017-12-09T23:13:21.745261: step 281, loss 0.326855, acc 0.871094, prec 0.0312537, recall 0.767578
2017-12-09T23:13:22.252344: step 282, loss 0.613235, acc 0.835938, prec 0.031317, recall 0.768257
2017-12-09T23:13:22.753611: step 283, loss 1.81611, acc 0.828125, prec 0.0312649, recall 0.766764
2017-12-09T23:13:23.258070: step 284, loss 2.21132, acc 0.855469, prec 0.0312982, recall 0.765731
2017-12-09T23:13:23.760321: step 285, loss 0.873155, acc 0.832031, prec 0.0314376, recall 0.766121
2017-12-09T23:13:24.264586: step 286, loss 0.751501, acc 0.808594, prec 0.0315296, recall 0.767018
2017-12-09T23:13:24.778092: step 287, loss 1.16625, acc 0.757812, prec 0.0316443, recall 0.767398
2017-12-09T23:13:25.279440: step 288, loss 0.676348, acc 0.789062, prec 0.0316153, recall 0.767619
2017-12-09T23:13:25.779982: step 289, loss 1.15889, acc 0.753906, prec 0.0317647, recall 0.768939
2017-12-09T23:13:26.283073: step 290, loss 0.852097, acc 0.726562, prec 0.0317157, recall 0.769158
2017-12-09T23:13:26.789092: step 291, loss 0.740368, acc 0.757812, prec 0.0316392, recall 0.769158
2017-12-09T23:13:27.289222: step 292, loss 0.658263, acc 0.789062, prec 0.031648, recall 0.769594
2017-12-09T23:13:27.796043: step 293, loss 0.657905, acc 0.824219, prec 0.0316679, recall 0.770028
2017-12-09T23:13:28.302017: step 294, loss 0.961386, acc 0.828125, prec 0.0318762, recall 0.771536
2017-12-09T23:13:28.804511: step 295, loss 0.50134, acc 0.847656, prec 0.031903, recall 0.771963
2017-12-09T23:13:29.310874: step 296, loss 0.475384, acc 0.839844, prec 0.0320392, recall 0.773023
2017-12-09T23:13:29.816048: step 297, loss 0.386584, acc 0.859375, prec 0.0321066, recall 0.773655
2017-12-09T23:13:30.328055: step 298, loss 0.970082, acc 0.859375, prec 0.0321006, recall 0.773148
2017-12-09T23:13:30.839748: step 299, loss 0.960051, acc 0.898438, prec 0.0321813, recall 0.773063
2017-12-09T23:13:31.345486: step 300, loss 1.68851, acc 0.867188, prec 0.0321776, recall 0.77256

Evaluation:
2017-12-09T23:13:36.090113: step 300, loss 2.11412, acc 0.908859, prec 0.0344394, recall 0.7456

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-300

2017-12-09T23:13:37.747000: step 301, loss 1.22981, acc 0.867188, prec 0.0345057, recall 0.74502
2017-12-09T23:13:38.250183: step 302, loss 0.677294, acc 0.898438, prec 0.0346874, recall 0.745642
2017-12-09T23:13:38.751085: step 303, loss 0.658245, acc 0.808594, prec 0.0347314, recall 0.746245
2017-12-09T23:13:39.254758: step 304, loss 0.449832, acc 0.863281, prec 0.0347932, recall 0.746845
2017-12-09T23:13:39.761367: step 305, loss 0.856757, acc 0.828125, prec 0.0348078, recall 0.747244
2017-12-09T23:13:40.270221: step 306, loss 0.900237, acc 0.785156, prec 0.0349144, recall 0.748235
2017-12-09T23:13:40.778322: step 307, loss 0.488318, acc 0.824219, prec 0.034857, recall 0.748235
2017-12-09T23:13:41.277367: step 308, loss 0.621139, acc 0.832031, prec 0.0349431, recall 0.749023
2017-12-09T23:13:41.778727: step 309, loss 0.486388, acc 0.828125, prec 0.0349574, recall 0.749415
2017-12-09T23:13:42.281057: step 310, loss 1.96928, acc 0.824219, prec 0.0349729, recall 0.748638
2017-12-09T23:13:42.785471: step 311, loss 1.96282, acc 0.773438, prec 0.0350769, recall 0.748452
2017-12-09T23:13:43.288908: step 312, loss 0.891129, acc 0.769531, prec 0.035143, recall 0.748651
2017-12-09T23:13:43.783677: step 313, loss 1.19875, acc 0.742188, prec 0.0352334, recall 0.749616
2017-12-09T23:13:44.288626: step 314, loss 1.14471, acc 0.769531, prec 0.0352293, recall 0.749425
2017-12-09T23:13:44.790633: step 315, loss 0.808514, acc 0.761719, prec 0.0352214, recall 0.749809
2017-12-09T23:13:45.296667: step 316, loss 1.47295, acc 0.714844, prec 0.0351651, recall 0.749427
2017-12-09T23:13:45.795312: step 317, loss 1.26105, acc 0.730469, prec 0.0351486, recall 0.749238
2017-12-09T23:13:46.305836: step 318, loss 0.788273, acc 0.761719, prec 0.0352442, recall 0.75019
2017-12-09T23:13:46.814026: step 319, loss 0.966063, acc 0.753906, prec 0.0353368, recall 0.751135
2017-12-09T23:13:47.310383: step 320, loss 1.82766, acc 0.695312, prec 0.0352745, recall 0.750755
2017-12-09T23:13:47.809290: step 321, loss 0.878247, acc 0.757812, prec 0.0352312, recall 0.750943
2017-12-09T23:13:48.326204: step 322, loss 3.1026, acc 0.742188, prec 0.0353207, recall 0.751315
2017-12-09T23:13:48.827754: step 323, loss 0.884097, acc 0.757812, prec 0.0353795, recall 0.75206
2017-12-09T23:13:49.329757: step 324, loss 0.940297, acc 0.742188, prec 0.035467, recall 0.752985
2017-12-09T23:13:49.838733: step 325, loss 0.805159, acc 0.730469, prec 0.0355503, recall 0.753903
2017-12-09T23:13:50.348532: step 326, loss 1.11503, acc 0.785156, prec 0.0355506, recall 0.753709
2017-12-09T23:13:50.856641: step 327, loss 0.687775, acc 0.804688, prec 0.0356569, recall 0.754619
2017-12-09T23:13:51.367822: step 328, loss 1.64259, acc 0.847656, prec 0.035779, recall 0.754412
2017-12-09T23:13:51.876123: step 329, loss 2.61933, acc 0.792969, prec 0.0359156, recall 0.754938
2017-12-09T23:13:52.385362: step 330, loss 1.03461, acc 0.753906, prec 0.0359375, recall 0.755474
2017-12-09T23:13:52.889036: step 331, loss 0.744382, acc 0.742188, prec 0.0359889, recall 0.756186
2017-12-09T23:13:53.400889: step 332, loss 2.50589, acc 0.777344, prec 0.0360525, recall 0.756345
2017-12-09T23:13:53.904864: step 333, loss 0.764294, acc 0.761719, prec 0.0361097, recall 0.75705
2017-12-09T23:13:54.411436: step 334, loss 0.814269, acc 0.730469, prec 0.0360903, recall 0.757401
2017-12-09T23:13:54.911749: step 335, loss 0.760916, acc 0.734375, prec 0.036006, recall 0.757401
2017-12-09T23:13:55.420988: step 336, loss 0.828448, acc 0.753906, prec 0.0361264, recall 0.758447
2017-12-09T23:13:55.920914: step 337, loss 0.614222, acc 0.78125, prec 0.0360902, recall 0.758621
2017-12-09T23:13:56.425827: step 338, loss 0.742611, acc 0.777344, prec 0.0361187, recall 0.75914
2017-12-09T23:13:56.931388: step 339, loss 1.81685, acc 0.816406, prec 0.0360962, recall 0.758226
2017-12-09T23:13:57.436890: step 340, loss 0.596025, acc 0.847656, prec 0.0362777, recall 0.759431
2017-12-09T23:13:57.944404: step 341, loss 0.512313, acc 0.863281, prec 0.0362673, recall 0.759602
2017-12-09T23:13:58.446789: step 342, loss 1.72772, acc 0.824219, prec 0.0363112, recall 0.759574
2017-12-09T23:13:58.953701: step 343, loss 1.16225, acc 0.863281, prec 0.0363686, recall 0.759011
2017-12-09T23:13:59.452638: step 344, loss 0.304855, acc 0.902344, prec 0.036403, recall 0.759351
2017-12-09T23:13:59.954409: step 345, loss 1.21505, acc 0.789062, prec 0.0363704, recall 0.758985
2017-12-09T23:14:00.472299: step 346, loss 0.813855, acc 0.886719, prec 0.0364336, recall 0.75896
2017-12-09T23:14:00.980923: step 347, loss 3.14247, acc 0.851562, prec 0.0365192, recall 0.758572
2017-12-09T23:14:01.486404: step 348, loss 0.55671, acc 0.816406, prec 0.0365263, recall 0.75891
2017-12-09T23:14:01.994068: step 349, loss 1.17433, acc 0.785156, prec 0.0365895, recall 0.759053
2017-12-09T23:14:02.496191: step 350, loss 0.789782, acc 0.773438, prec 0.036712, recall 0.760055
2017-12-09T23:14:02.993547: step 351, loss 0.805776, acc 0.769531, prec 0.0366396, recall 0.760055
2017-12-09T23:14:03.490040: step 352, loss 0.694082, acc 0.761719, prec 0.0366936, recall 0.760719
2017-12-09T23:14:03.995225: step 353, loss 0.883315, acc 0.746094, prec 0.0367104, recall 0.761215
2017-12-09T23:14:04.499415: step 354, loss 0.75937, acc 0.738281, prec 0.0366287, recall 0.761215
2017-12-09T23:14:05.004454: step 355, loss 0.872957, acc 0.726562, prec 0.0366395, recall 0.761708
2017-12-09T23:14:05.522620: step 356, loss 1.03561, acc 0.777344, prec 0.0366354, recall 0.761512
2017-12-09T23:14:06.028847: step 357, loss 0.630992, acc 0.847656, prec 0.036779, recall 0.762491
2017-12-09T23:14:06.537263: step 358, loss 2.65867, acc 0.808594, prec 0.0368173, recall 0.761937
2017-12-09T23:14:07.051311: step 359, loss 4.35359, acc 0.808594, prec 0.036825, recall 0.760707
2017-12-09T23:14:07.555646: step 360, loss 0.675091, acc 0.824219, prec 0.0368655, recall 0.761194
2017-12-09T23:14:08.053648: step 361, loss 0.681239, acc 0.777344, prec 0.0368913, recall 0.761679
2017-12-09T23:14:08.562216: step 362, loss 2.02033, acc 0.777344, prec 0.0368552, recall 0.761325
2017-12-09T23:14:09.079080: step 363, loss 0.887378, acc 0.71875, prec 0.0367685, recall 0.761325
2017-12-09T23:14:09.579034: step 364, loss 1.78847, acc 0.707031, prec 0.0368368, recall 0.761616
2017-12-09T23:14:10.078454: step 365, loss 0.973059, acc 0.757812, prec 0.0369191, recall 0.762416
2017-12-09T23:14:10.581412: step 366, loss 1.21448, acc 0.710938, prec 0.0369866, recall 0.763211
2017-12-09T23:14:11.087063: step 367, loss 1.45723, acc 0.722656, prec 0.0371197, recall 0.764314
2017-12-09T23:14:11.590716: step 368, loss 1.04684, acc 0.753906, prec 0.0371995, recall 0.765096
2017-12-09T23:14:12.092088: step 369, loss 0.897529, acc 0.707031, prec 0.0372337, recall 0.765718
2017-12-09T23:14:12.589885: step 370, loss 0.919262, acc 0.722656, prec 0.0372106, recall 0.766028
2017-12-09T23:14:13.088994: step 371, loss 4.99852, acc 0.734375, prec 0.0372862, recall 0.765789
2017-12-09T23:14:13.596778: step 372, loss 0.913967, acc 0.734375, prec 0.0372359, recall 0.765943
2017-12-09T23:14:14.098127: step 373, loss 0.714797, acc 0.765625, prec 0.0371954, recall 0.766097
2017-12-09T23:14:14.606550: step 374, loss 0.87802, acc 0.726562, prec 0.0371431, recall 0.766251
2017-12-09T23:14:14.785899: step 375, loss 0.85876, acc 0.823529, prec 0.0372244, recall 0.76671
2017-12-09T23:14:15.297878: step 376, loss 0.954112, acc 0.777344, prec 0.0372793, recall 0.76732
2017-12-09T23:14:15.803909: step 377, loss 0.81494, acc 0.800781, prec 0.0373106, recall 0.767776
2017-12-09T23:14:16.308220: step 378, loss 0.716047, acc 0.8125, prec 0.0374062, recall 0.768531
2017-12-09T23:14:16.804799: step 379, loss 0.593924, acc 0.84375, prec 0.0374502, recall 0.768981
2017-12-09T23:14:17.311238: step 380, loss 0.470685, acc 0.855469, prec 0.037528, recall 0.769579
2017-12-09T23:14:17.824042: step 381, loss 2.6328, acc 0.828125, prec 0.0375378, recall 0.76938
2017-12-09T23:14:18.336830: step 382, loss 0.353874, acc 0.902344, prec 0.0375992, recall 0.769826
2017-12-09T23:14:18.843634: step 383, loss 0.339347, acc 0.886719, prec 0.0376557, recall 0.77027
2017-12-09T23:14:19.346441: step 384, loss 0.366932, acc 0.859375, prec 0.0377038, recall 0.770713
2017-12-09T23:14:19.846206: step 385, loss 0.572011, acc 0.886719, prec 0.0377903, recall 0.7713
2017-12-09T23:14:20.357879: step 386, loss 1.07344, acc 0.894531, prec 0.0379406, recall 0.771684
2017-12-09T23:14:20.873400: step 387, loss 0.309567, acc 0.882812, prec 0.0380254, recall 0.772265
2017-12-09T23:14:21.375552: step 388, loss 0.292177, acc 0.914062, prec 0.0381197, recall 0.772843
2017-12-09T23:14:21.893187: step 389, loss 2.63426, acc 0.847656, prec 0.0381357, recall 0.772152
2017-12-09T23:14:22.409298: step 390, loss 0.398653, acc 0.882812, prec 0.0381901, recall 0.772584
2017-12-09T23:14:22.912312: step 391, loss 0.521837, acc 0.886719, prec 0.0382156, recall 0.772871
2017-12-09T23:14:23.416004: step 392, loss 0.421254, acc 0.847656, prec 0.0381991, recall 0.773014
2017-12-09T23:14:23.922530: step 393, loss 0.758604, acc 0.820312, prec 0.0382641, recall 0.773585
2017-12-09T23:14:24.425250: step 394, loss 0.49969, acc 0.832031, prec 0.0382429, recall 0.773727
2017-12-09T23:14:24.933796: step 395, loss 0.702975, acc 0.835938, prec 0.0383124, recall 0.774295
2017-12-09T23:14:25.433937: step 396, loss 0.835589, acc 0.851562, prec 0.0383579, recall 0.774234
2017-12-09T23:14:25.936444: step 397, loss 0.59016, acc 0.863281, prec 0.0384949, recall 0.775078
2017-12-09T23:14:26.439063: step 398, loss 0.37666, acc 0.835938, prec 0.0385637, recall 0.775637
2017-12-09T23:14:26.943397: step 399, loss 0.604424, acc 0.804688, prec 0.0386526, recall 0.776332
2017-12-09T23:14:27.445212: step 400, loss 0.437385, acc 0.84375, prec 0.0386345, recall 0.776471
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-400

2017-12-09T23:14:28.988809: step 401, loss 0.68329, acc 0.855469, prec 0.0386213, recall 0.776129
2017-12-09T23:14:29.498903: step 402, loss 2.61514, acc 0.878906, prec 0.0387347, recall 0.775862
2017-12-09T23:14:30.017199: step 403, loss 0.411821, acc 0.835938, prec 0.0386847, recall 0.775862
2017-12-09T23:14:30.531196: step 404, loss 0.367396, acc 0.863281, prec 0.0387316, recall 0.776275
2017-12-09T23:14:31.035051: step 405, loss 1.50387, acc 0.855469, prec 0.0387478, recall 0.776074
2017-12-09T23:14:31.536602: step 406, loss 0.666854, acc 0.863281, prec 0.0388252, recall 0.776147
2017-12-09T23:14:32.042736: step 407, loss 0.689777, acc 0.816406, prec 0.0388869, recall 0.776693
2017-12-09T23:14:32.555215: step 408, loss 1.09369, acc 0.820312, prec 0.0389507, recall 0.776764
2017-12-09T23:14:33.061112: step 409, loss 0.865925, acc 0.792969, prec 0.0389476, recall 0.776563
2017-12-09T23:14:33.562977: step 410, loss 0.564586, acc 0.789062, prec 0.0390005, recall 0.777105
2017-12-09T23:14:34.065582: step 411, loss 0.601132, acc 0.804688, prec 0.0389997, recall 0.777374
2017-12-09T23:14:34.567279: step 412, loss 0.752626, acc 0.78125, prec 0.03905, recall 0.777912
2017-12-09T23:14:35.078501: step 413, loss 0.897872, acc 0.785156, prec 0.0391303, recall 0.77858
2017-12-09T23:14:35.605070: step 414, loss 0.782162, acc 0.832031, prec 0.0393986, recall 0.780036
2017-12-09T23:14:36.105936: step 415, loss 0.544257, acc 0.832031, prec 0.0394634, recall 0.780561
2017-12-09T23:14:36.606125: step 416, loss 0.503666, acc 0.835938, prec 0.0395291, recall 0.781083
2017-12-09T23:14:37.112985: step 417, loss 0.756636, acc 0.859375, prec 0.0396596, recall 0.781861
2017-12-09T23:14:37.630162: step 418, loss 0.840698, acc 0.84375, prec 0.0397285, recall 0.781915
2017-12-09T23:14:38.132138: step 419, loss 1.36861, acc 0.804688, prec 0.0397577, recall 0.781379
2017-12-09T23:14:38.635210: step 420, loss 0.596469, acc 0.867188, prec 0.039861, recall 0.782021
2017-12-09T23:14:39.137127: step 421, loss 0.67013, acc 0.757812, prec 0.0399307, recall 0.78266
2017-12-09T23:14:39.646833: step 422, loss 0.966773, acc 0.804688, prec 0.0399296, recall 0.782456
2017-12-09T23:14:40.153468: step 423, loss 1.13752, acc 0.832031, prec 0.039994, recall 0.782507
2017-12-09T23:14:40.667025: step 424, loss 1.02296, acc 0.804688, prec 0.0400214, recall 0.782432
2017-12-09T23:14:41.170774: step 425, loss 0.655902, acc 0.761719, prec 0.040063, recall 0.782937
2017-12-09T23:14:41.678025: step 426, loss 0.62869, acc 0.785156, prec 0.0400546, recall 0.783188
2017-12-09T23:14:42.178416: step 427, loss 2.22162, acc 0.765625, prec 0.0400698, recall 0.783112
2017-12-09T23:14:42.683077: step 428, loss 1.59307, acc 0.800781, prec 0.0400957, recall 0.783035
2017-12-09T23:14:43.186156: step 429, loss 0.659145, acc 0.78125, prec 0.0400861, recall 0.783285
2017-12-09T23:14:43.684613: step 430, loss 0.570376, acc 0.808594, prec 0.0401131, recall 0.783659
2017-12-09T23:14:44.185835: step 431, loss 0.58715, acc 0.796875, prec 0.0401082, recall 0.783908
2017-12-09T23:14:44.686637: step 432, loss 0.778085, acc 0.769531, prec 0.0400951, recall 0.784156
2017-12-09T23:14:45.187553: step 433, loss 0.783619, acc 0.808594, prec 0.0402063, recall 0.784897
2017-12-09T23:14:45.690657: step 434, loss 0.553031, acc 0.796875, prec 0.0403136, recall 0.785633
2017-12-09T23:14:46.190605: step 435, loss 0.542697, acc 0.835938, prec 0.0403202, recall 0.785877
2017-12-09T23:14:46.693708: step 436, loss 0.473485, acc 0.828125, prec 0.0403245, recall 0.786121
2017-12-09T23:14:47.196826: step 437, loss 0.953103, acc 0.84375, prec 0.0403625, recall 0.786039
2017-12-09T23:14:47.701238: step 438, loss 0.531727, acc 0.875, prec 0.0404087, recall 0.786402
2017-12-09T23:14:48.200914: step 439, loss 0.31891, acc 0.882812, prec 0.0404293, recall 0.786644
2017-12-09T23:14:48.704674: step 440, loss 1.02485, acc 0.882812, prec 0.040451, recall 0.786441
2017-12-09T23:14:49.211858: step 441, loss 0.340145, acc 0.9375, prec 0.0405715, recall 0.787042
2017-12-09T23:14:49.717173: step 442, loss 0.748102, acc 0.902344, prec 0.0406547, recall 0.787079
2017-12-09T23:14:50.227021: step 443, loss 1.18153, acc 0.84375, prec 0.0406921, recall 0.786996
2017-12-09T23:14:50.748022: step 444, loss 0.365019, acc 0.90625, prec 0.0407472, recall 0.787353
2017-12-09T23:14:51.254815: step 445, loss 0.517163, acc 0.921875, prec 0.0408346, recall 0.787828
2017-12-09T23:14:51.759361: step 446, loss 1.14016, acc 0.863281, prec 0.0408222, recall 0.787507
2017-12-09T23:14:52.268242: step 447, loss 0.443706, acc 0.898438, prec 0.0409301, recall 0.788098
2017-12-09T23:14:52.769240: step 448, loss 0.596266, acc 0.882812, prec 0.0410054, recall 0.788568
2017-12-09T23:14:53.274401: step 449, loss 1.28697, acc 0.878906, prec 0.0410252, recall 0.788366
2017-12-09T23:14:53.780598: step 450, loss 0.553174, acc 0.832031, prec 0.0410848, recall 0.788834
2017-12-09T23:14:54.288890: step 451, loss 0.452057, acc 0.84375, prec 0.0410927, recall 0.789067
2017-12-09T23:14:54.791846: step 452, loss 0.810933, acc 0.8125, prec 0.0411462, recall 0.789532
2017-12-09T23:14:55.306939: step 453, loss 0.617652, acc 0.789062, prec 0.04122, recall 0.79011
2017-12-09T23:14:55.814461: step 454, loss 0.549233, acc 0.824219, prec 0.0412767, recall 0.79057
2017-12-09T23:14:56.314707: step 455, loss 0.677386, acc 0.800781, prec 0.0413261, recall 0.791028
2017-12-09T23:14:56.818588: step 456, loss 0.701538, acc 0.785156, prec 0.041398, recall 0.791598
2017-12-09T23:14:57.326176: step 457, loss 0.661918, acc 0.78125, prec 0.041332, recall 0.791598
2017-12-09T23:14:57.822448: step 458, loss 0.541499, acc 0.796875, prec 0.0413799, recall 0.792052
2017-12-09T23:14:58.324486: step 459, loss 1.11967, acc 0.828125, prec 0.0413838, recall 0.791848
2017-12-09T23:14:58.832214: step 460, loss 1.0531, acc 0.816406, prec 0.0414386, recall 0.79187
2017-12-09T23:14:59.330699: step 461, loss 0.431679, acc 0.863281, prec 0.0414518, recall 0.792095
2017-12-09T23:14:59.832443: step 462, loss 0.376359, acc 0.855469, prec 0.0414355, recall 0.792208
2017-12-09T23:15:00.349630: step 463, loss 0.413033, acc 0.859375, prec 0.0414476, recall 0.792432
2017-12-09T23:15:00.853143: step 464, loss 0.827179, acc 0.90625, prec 0.0415019, recall 0.792341
2017-12-09T23:15:01.360859: step 465, loss 2.60995, acc 0.839844, prec 0.0415938, recall 0.791197
2017-12-09T23:15:01.865927: step 466, loss 0.391708, acc 0.90625, prec 0.0416737, recall 0.791644
2017-12-09T23:15:02.364859: step 467, loss 0.603207, acc 0.828125, prec 0.041703, recall 0.791979
2017-12-09T23:15:02.863690: step 468, loss 0.439672, acc 0.863281, prec 0.0416889, recall 0.79209
2017-12-09T23:15:03.372582: step 469, loss 1.16348, acc 0.816406, prec 0.0417427, recall 0.792111
2017-12-09T23:15:03.876389: step 470, loss 0.656127, acc 0.789062, prec 0.041787, recall 0.792553
2017-12-09T23:15:04.376587: step 471, loss 0.804208, acc 0.785156, prec 0.0417495, recall 0.792663
2017-12-09T23:15:04.880429: step 472, loss 0.622965, acc 0.804688, prec 0.0417447, recall 0.792884
2017-12-09T23:15:05.389682: step 473, loss 0.669037, acc 0.816406, prec 0.0417435, recall 0.793103
2017-12-09T23:15:05.900920: step 474, loss 1.1378, acc 0.796875, prec 0.0418711, recall 0.79345
2017-12-09T23:15:06.403311: step 475, loss 0.833153, acc 0.8125, prec 0.0419485, recall 0.793994
2017-12-09T23:15:06.901655: step 476, loss 0.837028, acc 0.769531, prec 0.0420395, recall 0.794643
2017-12-09T23:15:07.400125: step 477, loss 0.495527, acc 0.855469, prec 0.0420495, recall 0.794858
2017-12-09T23:15:07.903495: step 478, loss 0.564655, acc 0.816406, prec 0.0420743, recall 0.795181
2017-12-09T23:15:08.402975: step 479, loss 1.51841, acc 0.816406, prec 0.0420219, recall 0.794349
2017-12-09T23:15:08.910714: step 480, loss 0.48317, acc 0.859375, prec 0.0420331, recall 0.794564
2017-12-09T23:15:09.416110: step 481, loss 0.432873, acc 0.847656, prec 0.0420143, recall 0.794671
2017-12-09T23:15:09.934912: step 482, loss 1.02536, acc 0.871094, prec 0.0421093, recall 0.794792
2017-12-09T23:15:10.435810: step 483, loss 0.598035, acc 0.851562, prec 0.0421708, recall 0.795218
2017-12-09T23:15:10.939494: step 484, loss 0.354166, acc 0.871094, prec 0.0421853, recall 0.795431
2017-12-09T23:15:11.447066: step 485, loss 0.680662, acc 0.867188, prec 0.0422775, recall 0.795961
2017-12-09T23:15:11.953190: step 486, loss 1.0215, acc 0.847656, prec 0.0423398, recall 0.79556
2017-12-09T23:15:12.459991: step 487, loss 0.604485, acc 0.859375, prec 0.0424557, recall 0.796191
2017-12-09T23:15:12.959286: step 488, loss 0.386194, acc 0.863281, prec 0.0424937, recall 0.796506
2017-12-09T23:15:13.458972: step 489, loss 0.655283, acc 0.851562, prec 0.0425281, recall 0.796819
2017-12-09T23:15:13.961390: step 490, loss 0.471458, acc 0.878906, prec 0.042623, recall 0.797339
2017-12-09T23:15:14.469930: step 491, loss 0.51825, acc 0.84375, prec 0.0426288, recall 0.797546
2017-12-09T23:15:14.978593: step 492, loss 1.37109, acc 0.867188, prec 0.0426426, recall 0.797346
2017-12-09T23:15:15.480408: step 493, loss 0.316203, acc 0.910156, prec 0.0426942, recall 0.797655
2017-12-09T23:15:15.993202: step 494, loss 0.504231, acc 0.851562, prec 0.0427283, recall 0.797964
2017-12-09T23:15:16.503200: step 495, loss 0.355676, acc 0.859375, prec 0.0427646, recall 0.798272
2017-12-09T23:15:16.999721: step 496, loss 0.290443, acc 0.898438, prec 0.0427864, recall 0.798477
2017-12-09T23:15:17.504686: step 497, loss 2.7215, acc 0.863281, prec 0.0428261, recall 0.797975
2017-12-09T23:15:18.018382: step 498, loss 0.341338, acc 0.910156, prec 0.0428253, recall 0.798077
2017-12-09T23:15:18.515534: step 499, loss 0.328807, acc 0.867188, prec 0.0428637, recall 0.798383
2017-12-09T23:15:18.691426: step 500, loss 0.425515, acc 0.882353, prec 0.0428827, recall 0.798485
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-500

2017-12-09T23:15:20.192054: step 501, loss 0.661626, acc 0.863281, prec 0.0429729, recall 0.79859
2017-12-09T23:15:20.725654: step 502, loss 0.557885, acc 0.875, prec 0.0430652, recall 0.799096
2017-12-09T23:15:21.223401: step 503, loss 0.389188, acc 0.878906, prec 0.0430808, recall 0.799298
2017-12-09T23:15:21.723645: step 504, loss 0.508677, acc 0.855469, prec 0.043167, recall 0.7998
2017-12-09T23:15:22.226392: step 505, loss 0.48516, acc 0.878906, prec 0.0433375, recall 0.800598
2017-12-09T23:15:22.730707: step 506, loss 0.352955, acc 0.867188, prec 0.0433751, recall 0.800896
2017-12-09T23:15:23.235791: step 507, loss 1.14149, acc 0.886719, prec 0.0434197, recall 0.800795
2017-12-09T23:15:23.741621: step 508, loss 0.521152, acc 0.867188, prec 0.0435859, recall 0.801583
2017-12-09T23:15:24.240967: step 509, loss 0.643159, acc 0.847656, prec 0.0436944, recall 0.802171
2017-12-09T23:15:24.744405: step 510, loss 2.02723, acc 0.816406, prec 0.043693, recall 0.801576
2017-12-09T23:15:25.257713: step 511, loss 0.376368, acc 0.882812, prec 0.0437347, recall 0.801868
2017-12-09T23:15:25.763526: step 512, loss 0.418666, acc 0.851562, prec 0.0437927, recall 0.802257
2017-12-09T23:15:26.273727: step 513, loss 0.570603, acc 0.800781, prec 0.0437585, recall 0.802354
2017-12-09T23:15:26.773441: step 514, loss 0.708071, acc 0.816406, prec 0.0438058, recall 0.802741
2017-12-09T23:15:27.279803: step 515, loss 0.488069, acc 0.839844, prec 0.0438599, recall 0.803127
2017-12-09T23:15:27.795018: step 516, loss 0.914532, acc 0.828125, prec 0.043937, recall 0.803215
2017-12-09T23:15:28.297841: step 517, loss 0.476147, acc 0.832031, prec 0.0438867, recall 0.803215
2017-12-09T23:15:28.797753: step 518, loss 0.434796, acc 0.847656, prec 0.0438412, recall 0.803215
2017-12-09T23:15:29.296083: step 519, loss 0.399254, acc 0.898438, prec 0.0439379, recall 0.803693
2017-12-09T23:15:29.805682: step 520, loss 0.233314, acc 0.902344, prec 0.0439342, recall 0.803788
2017-12-09T23:15:30.323091: step 521, loss 0.416791, acc 0.898438, prec 0.0440053, recall 0.804169
2017-12-09T23:15:30.829307: step 522, loss 0.506069, acc 0.890625, prec 0.0440487, recall 0.804453
2017-12-09T23:15:31.336329: step 523, loss 0.673453, acc 0.851562, prec 0.0440561, recall 0.804253
2017-12-09T23:15:31.843187: step 524, loss 0.346054, acc 0.90625, prec 0.0440787, recall 0.804442
2017-12-09T23:15:32.345435: step 525, loss 0.422162, acc 0.9375, prec 0.0441106, recall 0.804631
2017-12-09T23:15:32.849753: step 526, loss 0.41387, acc 0.90625, prec 0.0441585, recall 0.804913
2017-12-09T23:15:33.352231: step 527, loss 1.46832, acc 0.925781, prec 0.0441627, recall 0.80462
2017-12-09T23:15:33.859830: step 528, loss 0.524589, acc 0.929688, prec 0.0443195, recall 0.80489
2017-12-09T23:15:34.367513: step 529, loss 0.513394, acc 0.894531, prec 0.0443636, recall 0.80517
2017-12-09T23:15:34.874664: step 530, loss 2.1117, acc 0.925781, prec 0.0444444, recall 0.804773
2017-12-09T23:15:35.378463: step 531, loss 0.496519, acc 0.886719, prec 0.0444608, recall 0.804959
2017-12-09T23:15:35.903209: step 532, loss 1.5767, acc 0.851562, prec 0.0444678, recall 0.804762
2017-12-09T23:15:36.408105: step 533, loss 0.384375, acc 0.871094, prec 0.0444544, recall 0.804855
2017-12-09T23:15:36.919042: step 534, loss 0.592839, acc 0.875, prec 0.0445425, recall 0.805318
2017-12-09T23:15:37.420703: step 535, loss 0.471047, acc 0.820312, prec 0.0445389, recall 0.805503
2017-12-09T23:15:37.930207: step 536, loss 1.24554, acc 0.8125, prec 0.0446854, recall 0.805477
2017-12-09T23:15:38.434416: step 537, loss 0.917559, acc 0.777344, prec 0.0447187, recall 0.805844
2017-12-09T23:15:38.938700: step 538, loss 1.58103, acc 0.773438, prec 0.044752, recall 0.80583
2017-12-09T23:15:39.447598: step 539, loss 0.796951, acc 0.726562, prec 0.0447948, recall 0.806285
2017-12-09T23:15:39.946728: step 540, loss 0.869471, acc 0.746094, prec 0.0447936, recall 0.806557
2017-12-09T23:15:40.448956: step 541, loss 0.738936, acc 0.777344, prec 0.0448017, recall 0.806829
2017-12-09T23:15:40.953416: step 542, loss 0.580873, acc 0.8125, prec 0.0448946, recall 0.807369
2017-12-09T23:15:41.463971: step 543, loss 0.649426, acc 0.765625, prec 0.0449485, recall 0.807818
2017-12-09T23:15:41.972964: step 544, loss 0.511104, acc 0.828125, prec 0.0449714, recall 0.808086
2017-12-09T23:15:42.479555: step 545, loss 0.460441, acc 0.835938, prec 0.0449966, recall 0.808353
2017-12-09T23:15:42.981633: step 546, loss 0.394753, acc 0.855469, prec 0.045003, recall 0.80853
2017-12-09T23:15:43.485237: step 547, loss 0.643481, acc 0.863281, prec 0.0450855, recall 0.808973
2017-12-09T23:15:43.986710: step 548, loss 1.3553, acc 0.894531, prec 0.045081, recall 0.808314
2017-12-09T23:15:44.489871: step 549, loss 0.523473, acc 0.898438, prec 0.0451492, recall 0.808668
2017-12-09T23:15:44.994763: step 550, loss 2.13858, acc 0.867188, prec 0.0452336, recall 0.808736
2017-12-09T23:15:45.509025: step 551, loss 2.86362, acc 0.890625, prec 0.0453284, recall 0.807692
2017-12-09T23:15:46.024469: step 552, loss 0.402797, acc 0.875, prec 0.0453157, recall 0.80778
2017-12-09T23:15:46.536970: step 553, loss 0.585698, acc 0.871094, prec 0.0453508, recall 0.808044
2017-12-09T23:15:47.044512: step 554, loss 0.621163, acc 0.851562, prec 0.0455757, recall 0.809004
2017-12-09T23:15:47.545905: step 555, loss 0.507643, acc 0.808594, prec 0.0456162, recall 0.809351
2017-12-09T23:15:48.053996: step 556, loss 0.997238, acc 0.710938, prec 0.0455787, recall 0.809524
2017-12-09T23:15:48.571178: step 557, loss 0.789878, acc 0.769531, prec 0.0456075, recall 0.809869
2017-12-09T23:15:49.063185: step 558, loss 0.82346, acc 0.738281, prec 0.0456269, recall 0.810212
2017-12-09T23:15:49.565974: step 559, loss 0.734934, acc 0.777344, prec 0.0456578, recall 0.810555
2017-12-09T23:15:50.068030: step 560, loss 0.693869, acc 0.757812, prec 0.045707, recall 0.810981
2017-12-09T23:15:50.600008: step 561, loss 0.685671, acc 0.78125, prec 0.0456664, recall 0.811066
2017-12-09T23:15:51.109821: step 562, loss 0.718455, acc 0.792969, prec 0.0457017, recall 0.811405
2017-12-09T23:15:51.612943: step 563, loss 0.567294, acc 0.8125, prec 0.0457186, recall 0.811659
2017-12-09T23:15:52.116132: step 564, loss 0.947807, acc 0.800781, prec 0.045732, recall 0.811912
2017-12-09T23:15:52.623798: step 565, loss 0.387893, acc 0.886719, prec 0.0457467, recall 0.812081
2017-12-09T23:15:53.130837: step 566, loss 0.355159, acc 0.90625, prec 0.0457911, recall 0.812332
2017-12-09T23:15:53.633632: step 567, loss 0.904588, acc 0.890625, prec 0.0458321, recall 0.812221
2017-12-09T23:15:54.136849: step 568, loss 0.52158, acc 0.910156, prec 0.0459256, recall 0.812639
2017-12-09T23:15:54.643736: step 569, loss 0.251664, acc 0.90625, prec 0.0459938, recall 0.812972
2017-12-09T23:15:55.144782: step 570, loss 0.398082, acc 0.914062, prec 0.0461121, recall 0.813469
2017-12-09T23:15:55.643973: step 571, loss 0.285834, acc 0.902344, prec 0.0461311, recall 0.813634
2017-12-09T23:15:56.151848: step 572, loss 0.263049, acc 0.933594, prec 0.046231, recall 0.814046
2017-12-09T23:15:56.662026: step 573, loss 1.01759, acc 0.925781, prec 0.0462591, recall 0.813492
2017-12-09T23:15:57.171833: step 574, loss 0.28869, acc 0.917969, prec 0.0462348, recall 0.813492
2017-12-09T23:15:57.680120: step 575, loss 0.290461, acc 0.898438, prec 0.0463719, recall 0.814066
2017-12-09T23:15:58.185979: step 576, loss 0.959121, acc 0.921875, prec 0.0463987, recall 0.813515
2017-12-09T23:15:58.692433: step 577, loss 0.309, acc 0.902344, prec 0.0464889, recall 0.813923
2017-12-09T23:15:59.196957: step 578, loss 0.634196, acc 0.925781, prec 0.046468, recall 0.813567
2017-12-09T23:15:59.718018: step 579, loss 0.283628, acc 0.90625, prec 0.0465116, recall 0.813811
2017-12-09T23:16:00.228832: step 580, loss 0.605245, acc 0.898438, prec 0.0465302, recall 0.813618
2017-12-09T23:16:00.728751: step 581, loss 0.373545, acc 0.875, prec 0.046612, recall 0.814024
2017-12-09T23:16:01.236555: step 582, loss 0.768491, acc 0.851562, prec 0.0466391, recall 0.814267
2017-12-09T23:16:01.749486: step 583, loss 0.342745, acc 0.910156, prec 0.0466599, recall 0.814429
2017-12-09T23:16:02.251309: step 584, loss 0.556994, acc 0.875, prec 0.0466702, recall 0.81459
2017-12-09T23:16:02.756357: step 585, loss 0.719131, acc 0.851562, prec 0.0466983, recall 0.814478
2017-12-09T23:16:03.254006: step 586, loss 0.451511, acc 0.839844, prec 0.0466508, recall 0.814478
2017-12-09T23:16:03.758323: step 587, loss 0.646243, acc 0.839844, prec 0.0466743, recall 0.814719
2017-12-09T23:16:04.258295: step 588, loss 0.378058, acc 0.863281, prec 0.046681, recall 0.814879
2017-12-09T23:16:04.774785: step 589, loss 1.59953, acc 0.84375, prec 0.0467068, recall 0.814767
2017-12-09T23:16:05.283213: step 590, loss 0.41266, acc 0.871094, prec 0.0466922, recall 0.814847
2017-12-09T23:16:05.796158: step 591, loss 0.34893, acc 0.871094, prec 0.0467013, recall 0.815006
2017-12-09T23:16:06.298503: step 592, loss 0.450143, acc 0.863281, prec 0.046755, recall 0.815325
2017-12-09T23:16:06.800319: step 593, loss 1.40155, acc 0.820312, prec 0.0467737, recall 0.815213
2017-12-09T23:16:07.302817: step 594, loss 0.431637, acc 0.867188, prec 0.0468519, recall 0.815609
2017-12-09T23:16:07.807991: step 595, loss 0.286072, acc 0.910156, prec 0.0468254, recall 0.815609
2017-12-09T23:16:08.307860: step 596, loss 2.40812, acc 0.875, prec 0.0468131, recall 0.815338
2017-12-09T23:16:08.811199: step 597, loss 0.268961, acc 0.886719, prec 0.0468032, recall 0.815418
2017-12-09T23:16:09.315404: step 598, loss 0.395828, acc 0.878906, prec 0.046791, recall 0.815497
2017-12-09T23:16:09.812848: step 599, loss 0.750494, acc 0.859375, prec 0.0468666, recall 0.815891
2017-12-09T23:16:10.316449: step 600, loss 1.2237, acc 0.886719, prec 0.0469045, recall 0.815778

Evaluation:
2017-12-09T23:16:15.005831: step 600, loss 1.59388, acc 0.889518, prec 0.0480238, recall 0.803906

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-600

2017-12-09T23:16:16.531146: step 601, loss 0.383525, acc 0.84375, prec 0.0480234, recall 0.804062
2017-12-09T23:16:17.039630: step 602, loss 0.264437, acc 0.917969, prec 0.0480221, recall 0.80414
2017-12-09T23:16:17.536415: step 603, loss 0.341225, acc 0.875, prec 0.048076, recall 0.804452
2017-12-09T23:16:18.049877: step 604, loss 0.400083, acc 0.832031, prec 0.0480721, recall 0.804607
2017-12-09T23:16:18.549443: step 605, loss 0.345086, acc 0.886719, prec 0.0481294, recall 0.804917
2017-12-09T23:16:19.058559: step 606, loss 1.07184, acc 0.882812, prec 0.0481414, recall 0.804752
2017-12-09T23:16:19.562140: step 607, loss 2.46071, acc 0.882812, prec 0.0482221, recall 0.804502
2017-12-09T23:16:20.074079: step 608, loss 0.53037, acc 0.859375, prec 0.0483612, recall 0.805118
2017-12-09T23:16:20.599810: step 609, loss 0.452626, acc 0.863281, prec 0.0483886, recall 0.805348
2017-12-09T23:16:21.104476: step 610, loss 0.338358, acc 0.867188, prec 0.0484396, recall 0.805654
2017-12-09T23:16:21.606565: step 611, loss 0.616253, acc 0.859375, prec 0.0484883, recall 0.805958
2017-12-09T23:16:22.121600: step 612, loss 0.527178, acc 0.835938, prec 0.0485075, recall 0.806186
2017-12-09T23:16:22.622884: step 613, loss 0.521358, acc 0.847656, prec 0.0485302, recall 0.806414
2017-12-09T23:16:23.127934: step 614, loss 0.395989, acc 0.871094, prec 0.0485149, recall 0.806489
2017-12-09T23:16:23.641388: step 615, loss 1.49519, acc 0.8125, prec 0.0485731, recall 0.806552
2017-12-09T23:16:24.145611: step 616, loss 0.393758, acc 0.867188, prec 0.0485343, recall 0.806552
2017-12-09T23:16:24.658390: step 617, loss 0.378203, acc 0.847656, prec 0.0485792, recall 0.806854
2017-12-09T23:16:25.160722: step 618, loss 0.347337, acc 0.890625, prec 0.0486142, recall 0.807079
2017-12-09T23:16:25.664201: step 619, loss 0.450308, acc 0.902344, prec 0.0486748, recall 0.807379
2017-12-09T23:16:26.165486: step 620, loss 0.437503, acc 0.902344, prec 0.0487799, recall 0.807826
2017-12-09T23:16:26.669446: step 621, loss 0.411223, acc 0.882812, prec 0.0489014, recall 0.808346
2017-12-09T23:16:27.178044: step 622, loss 0.580537, acc 0.902344, prec 0.0489395, recall 0.808568
2017-12-09T23:16:27.677588: step 623, loss 0.582791, acc 0.902344, prec 0.0489786, recall 0.808478
2017-12-09T23:16:28.183745: step 624, loss 0.349898, acc 0.917969, prec 0.0490212, recall 0.808699
2017-12-09T23:16:28.365312: step 625, loss 0.205357, acc 0.941176, prec 0.0490178, recall 0.808699
2017-12-09T23:16:28.870458: step 626, loss 0.521242, acc 0.921875, prec 0.0490836, recall 0.808993
2017-12-09T23:16:29.369921: step 627, loss 0.210009, acc 0.917969, prec 0.0491039, recall 0.80914
2017-12-09T23:16:29.871337: step 628, loss 0.492515, acc 0.878906, prec 0.049157, recall 0.809433
2017-12-09T23:16:30.386746: step 629, loss 0.201246, acc 0.933594, prec 0.0492482, recall 0.809797
2017-12-09T23:16:30.894326: step 630, loss 0.270943, acc 0.917969, prec 0.0493569, recall 0.810233
2017-12-09T23:16:31.393745: step 631, loss 0.220145, acc 0.914062, prec 0.0493537, recall 0.810305
2017-12-09T23:16:31.895810: step 632, loss 0.626587, acc 0.90625, prec 0.0494157, recall 0.810286
2017-12-09T23:16:32.397919: step 633, loss 0.454336, acc 0.90625, prec 0.0494764, recall 0.810574
2017-12-09T23:16:32.900195: step 634, loss 1.18251, acc 0.917969, prec 0.0495429, recall 0.810247
2017-12-09T23:16:33.406372: step 635, loss 0.63361, acc 0.917969, prec 0.049564, recall 0.810083
2017-12-09T23:16:33.912860: step 636, loss 0.259929, acc 0.902344, prec 0.0496234, recall 0.810371
2017-12-09T23:16:34.415323: step 637, loss 0.426898, acc 0.902344, prec 0.0496607, recall 0.810586
2017-12-09T23:16:34.923144: step 638, loss 0.343442, acc 0.875, prec 0.0496239, recall 0.810586
2017-12-09T23:16:35.433402: step 639, loss 1.04567, acc 0.902344, prec 0.0497282, recall 0.810709
2017-12-09T23:16:35.941249: step 640, loss 0.501868, acc 0.851562, prec 0.0497724, recall 0.810994
2017-12-09T23:16:36.458343: step 641, loss 0.513366, acc 0.824219, prec 0.0497865, recall 0.811207
2017-12-09T23:16:36.957807: step 642, loss 1.42611, acc 0.8125, prec 0.0497325, recall 0.810902
2017-12-09T23:16:37.461525: step 643, loss 0.404585, acc 0.84375, prec 0.0497305, recall 0.811044
2017-12-09T23:16:37.966585: step 644, loss 0.520651, acc 0.800781, prec 0.049694, recall 0.811115
2017-12-09T23:16:38.469252: step 645, loss 0.545547, acc 0.824219, prec 0.0496645, recall 0.811186
2017-12-09T23:16:38.974428: step 646, loss 0.419417, acc 0.851562, prec 0.0497302, recall 0.81154
2017-12-09T23:16:39.477723: step 647, loss 0.58242, acc 0.835938, prec 0.0497913, recall 0.811892
2017-12-09T23:16:39.996295: step 648, loss 0.736778, acc 0.839844, prec 0.0498534, recall 0.812243
2017-12-09T23:16:40.504865: step 649, loss 0.346345, acc 0.894531, prec 0.0499313, recall 0.812593
2017-12-09T23:16:41.006195: step 650, loss 0.529377, acc 0.863281, prec 0.05, recall 0.812942
2017-12-09T23:16:41.510301: step 651, loss 0.386766, acc 0.914062, prec 0.0500834, recall 0.813289
2017-12-09T23:16:42.015814: step 652, loss 0.996739, acc 0.894531, prec 0.0501405, recall 0.813264
2017-12-09T23:16:42.521667: step 653, loss 0.300894, acc 0.882812, prec 0.0501278, recall 0.813333
2017-12-09T23:16:43.025459: step 654, loss 0.235445, acc 0.921875, prec 0.0501266, recall 0.813402
2017-12-09T23:16:43.527788: step 655, loss 0.302889, acc 0.914062, prec 0.0501881, recall 0.813678
2017-12-09T23:16:44.027408: step 656, loss 0.698768, acc 0.925781, prec 0.0503179, recall 0.814159
2017-12-09T23:16:44.533135: step 657, loss 0.478548, acc 0.910156, prec 0.0503564, recall 0.814365
2017-12-09T23:16:45.046105: step 658, loss 0.293613, acc 0.902344, prec 0.0503926, recall 0.81457
2017-12-09T23:16:45.551137: step 659, loss 0.265245, acc 0.90625, prec 0.0504515, recall 0.814842
2017-12-09T23:16:46.056326: step 660, loss 0.258768, acc 0.914062, prec 0.0504695, recall 0.814978
2017-12-09T23:16:46.571071: step 661, loss 0.281713, acc 0.917969, prec 0.0504885, recall 0.815114
2017-12-09T23:16:47.068123: step 662, loss 0.282673, acc 0.898438, prec 0.050545, recall 0.815385
2017-12-09T23:16:47.570110: step 663, loss 0.271355, acc 0.917969, prec 0.0506071, recall 0.815655
2017-12-09T23:16:48.073973: step 664, loss 0.558706, acc 0.929688, prec 0.0508017, recall 0.816327
2017-12-09T23:16:48.589979: step 665, loss 0.180061, acc 0.921875, prec 0.0508217, recall 0.81646
2017-12-09T23:16:49.094543: step 666, loss 0.484118, acc 0.941406, prec 0.050869, recall 0.816661
2017-12-09T23:16:49.601181: step 667, loss 0.242181, acc 0.949219, prec 0.05094, recall 0.816927
2017-12-09T23:16:50.109931: step 668, loss 0.547102, acc 0.914062, prec 0.0510017, recall 0.816896
2017-12-09T23:16:50.636834: step 669, loss 0.253056, acc 0.945312, prec 0.0511144, recall 0.817294
2017-12-09T23:16:51.148237: step 670, loss 0.210547, acc 0.921875, prec 0.0511342, recall 0.817426
2017-12-09T23:16:51.652772: step 671, loss 0.322254, acc 0.941406, prec 0.0511812, recall 0.817624
2017-12-09T23:16:52.166726: step 672, loss 1.10215, acc 0.917969, prec 0.0512009, recall 0.81746
2017-12-09T23:16:52.667733: step 673, loss 0.422868, acc 0.929688, prec 0.0513944, recall 0.818116
2017-12-09T23:16:53.176546: step 674, loss 0.31111, acc 0.929688, prec 0.0514377, recall 0.818312
2017-12-09T23:16:53.685304: step 675, loss 0.332971, acc 0.914062, prec 0.0514978, recall 0.818573
2017-12-09T23:16:54.192341: step 676, loss 0.429789, acc 0.925781, prec 0.0515826, recall 0.818898
2017-12-09T23:16:54.700070: step 677, loss 0.388331, acc 0.90625, prec 0.0516189, recall 0.819092
2017-12-09T23:16:55.200504: step 678, loss 0.873036, acc 0.882812, prec 0.0516694, recall 0.81935
2017-12-09T23:16:55.709006: step 679, loss 0.376395, acc 0.882812, prec 0.0516985, recall 0.819543
2017-12-09T23:16:56.210019: step 680, loss 0.585749, acc 0.867188, prec 0.0516803, recall 0.819608
2017-12-09T23:16:56.718629: step 681, loss 0.494898, acc 0.847656, prec 0.051699, recall 0.819801
2017-12-09T23:16:57.217644: step 682, loss 0.471393, acc 0.84375, prec 0.0516951, recall 0.819929
2017-12-09T23:16:57.721992: step 683, loss 0.34573, acc 0.902344, prec 0.0517725, recall 0.820249
2017-12-09T23:16:58.226884: step 684, loss 0.391116, acc 0.867188, prec 0.0517755, recall 0.820376
2017-12-09T23:16:58.731853: step 685, loss 0.279876, acc 0.90625, prec 0.0518326, recall 0.820631
2017-12-09T23:16:59.232925: step 686, loss 0.370478, acc 0.867188, prec 0.0518356, recall 0.820758
2017-12-09T23:16:59.732631: step 687, loss 0.32631, acc 0.863281, prec 0.0518586, recall 0.820948
2017-12-09T23:17:00.247088: step 688, loss 0.33762, acc 0.917969, prec 0.0519402, recall 0.821265
2017-12-09T23:17:00.756895: step 689, loss 0.483673, acc 0.890625, prec 0.0519924, recall 0.821517
2017-12-09T23:17:01.257219: step 690, loss 1.11201, acc 0.933594, prec 0.0521431, recall 0.82173
2017-12-09T23:17:01.766468: step 691, loss 1.50625, acc 0.875, prec 0.0521716, recall 0.821341
2017-12-09T23:17:02.268698: step 692, loss 0.426323, acc 0.890625, prec 0.0522446, recall 0.821654
2017-12-09T23:17:02.769911: step 693, loss 0.313889, acc 0.894531, prec 0.0522554, recall 0.821779
2017-12-09T23:17:03.272906: step 694, loss 0.684289, acc 0.90625, prec 0.052313, recall 0.821741
2017-12-09T23:17:03.771135: step 695, loss 0.324152, acc 0.882812, prec 0.0523203, recall 0.821865
2017-12-09T23:17:04.280230: step 696, loss 0.560925, acc 0.859375, prec 0.0523416, recall 0.822052
2017-12-09T23:17:04.780575: step 697, loss 0.356534, acc 0.875, prec 0.0523675, recall 0.822238
2017-12-09T23:17:05.286103: step 698, loss 0.362855, acc 0.882812, prec 0.0524798, recall 0.82267
2017-12-09T23:17:05.800939: step 699, loss 0.360205, acc 0.875, prec 0.0524426, recall 0.82267
2017-12-09T23:17:06.299056: step 700, loss 0.471554, acc 0.882812, prec 0.0525337, recall 0.82304
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-700

2017-12-09T23:17:07.893544: step 701, loss 0.56423, acc 0.855469, prec 0.0526584, recall 0.823529
2017-12-09T23:17:08.395018: step 702, loss 0.477176, acc 0.839844, prec 0.0527153, recall 0.823834
2017-12-09T23:17:08.896449: step 703, loss 0.428344, acc 0.882812, prec 0.0527641, recall 0.824077
2017-12-09T23:17:09.394246: step 704, loss 0.301557, acc 0.933594, prec 0.0528698, recall 0.824441
2017-12-09T23:17:09.900738: step 705, loss 0.281954, acc 0.917969, prec 0.0528662, recall 0.824501
2017-12-09T23:17:10.409984: step 706, loss 0.312306, acc 0.941406, prec 0.0529113, recall 0.824682
2017-12-09T23:17:10.923910: step 707, loss 0.315765, acc 0.917969, prec 0.0529703, recall 0.824923
2017-12-09T23:17:11.430428: step 708, loss 0.306877, acc 0.917969, prec 0.0530084, recall 0.825103
2017-12-09T23:17:11.931526: step 709, loss 0.275679, acc 0.949219, prec 0.0531184, recall 0.825462
2017-12-09T23:17:12.441204: step 710, loss 0.239713, acc 0.925781, prec 0.0531587, recall 0.825641
2017-12-09T23:17:12.950906: step 711, loss 0.832057, acc 0.949219, prec 0.0532072, recall 0.825538
2017-12-09T23:17:13.457626: step 712, loss 0.259658, acc 0.953125, prec 0.0532556, recall 0.825716
2017-12-09T23:17:13.964624: step 713, loss 0.175031, acc 0.941406, prec 0.0532589, recall 0.825776
2017-12-09T23:17:14.471979: step 714, loss 1.15885, acc 0.949219, prec 0.0532864, recall 0.825613
2017-12-09T23:17:14.978059: step 715, loss 0.37191, acc 0.925781, prec 0.0533266, recall 0.825791
2017-12-09T23:17:15.481790: step 716, loss 0.368345, acc 0.925781, prec 0.0533875, recall 0.826028
2017-12-09T23:17:15.986074: step 717, loss 0.942775, acc 0.90625, prec 0.0534437, recall 0.825984
2017-12-09T23:17:16.492060: step 718, loss 0.245984, acc 0.902344, prec 0.0534144, recall 0.825984
2017-12-09T23:17:16.986410: step 719, loss 0.251539, acc 0.898438, prec 0.0534047, recall 0.826043
2017-12-09T23:17:17.490459: step 720, loss 0.374133, acc 0.867188, prec 0.0534064, recall 0.826161
2017-12-09T23:17:18.002728: step 721, loss 0.299489, acc 0.882812, prec 0.0534334, recall 0.826337
2017-12-09T23:17:18.509120: step 722, loss 0.613621, acc 0.910156, prec 0.0534906, recall 0.826293
2017-12-09T23:17:19.031126: step 723, loss 0.483257, acc 0.890625, prec 0.0534992, recall 0.82641
2017-12-09T23:17:19.536811: step 724, loss 0.383184, acc 0.898438, prec 0.0535308, recall 0.826586
2017-12-09T23:17:20.047719: step 725, loss 0.275603, acc 0.925781, prec 0.053612, recall 0.826878
2017-12-09T23:17:20.575667: step 726, loss 0.385723, acc 0.882812, prec 0.0536182, recall 0.826994
2017-12-09T23:17:21.076113: step 727, loss 0.222555, acc 0.894531, prec 0.0536485, recall 0.827169
2017-12-09T23:17:21.573050: step 728, loss 0.24499, acc 0.917969, prec 0.0537065, recall 0.827401
2017-12-09T23:17:22.073499: step 729, loss 1.73107, acc 0.910156, prec 0.0537426, recall 0.827297
2017-12-09T23:17:22.577112: step 730, loss 0.346053, acc 0.890625, prec 0.0537716, recall 0.827471
2017-12-09T23:17:23.085300: step 731, loss 0.264623, acc 0.90625, prec 0.0537847, recall 0.827586
2017-12-09T23:17:23.593323: step 732, loss 0.981878, acc 0.90625, prec 0.0538401, recall 0.82754
2017-12-09T23:17:24.096428: step 733, loss 0.300733, acc 0.910156, prec 0.0538749, recall 0.827713
2017-12-09T23:17:24.597415: step 734, loss 0.31141, acc 0.882812, prec 0.0538809, recall 0.827828
2017-12-09T23:17:25.095596: step 735, loss 0.346801, acc 0.878906, prec 0.0539678, recall 0.828172
2017-12-09T23:17:25.599533: step 736, loss 0.389061, acc 0.910156, prec 0.053982, recall 0.828286
2017-12-09T23:17:26.101044: step 737, loss 0.584825, acc 0.902344, prec 0.0540962, recall 0.828685
2017-12-09T23:17:26.613571: step 738, loss 0.353587, acc 0.910156, prec 0.0541307, recall 0.828856
2017-12-09T23:17:27.118558: step 739, loss 0.328682, acc 0.898438, prec 0.0542027, recall 0.829139
2017-12-09T23:17:27.621336: step 740, loss 1.0477, acc 0.882812, prec 0.05423, recall 0.829034
2017-12-09T23:17:28.125433: step 741, loss 0.312525, acc 0.925781, prec 0.0542895, recall 0.82926
2017-12-09T23:17:28.627672: step 742, loss 0.265607, acc 0.917969, prec 0.0543467, recall 0.829485
2017-12-09T23:17:29.132781: step 743, loss 0.297494, acc 0.890625, prec 0.0543342, recall 0.829542
2017-12-09T23:17:29.640415: step 744, loss 0.324896, acc 0.902344, prec 0.0544069, recall 0.829822
2017-12-09T23:17:30.147095: step 745, loss 0.27884, acc 0.894531, prec 0.0543957, recall 0.829878
2017-12-09T23:17:30.662858: step 746, loss 0.348436, acc 0.917969, prec 0.0544729, recall 0.830158
2017-12-09T23:17:31.161778: step 747, loss 0.22034, acc 0.9375, prec 0.0545153, recall 0.830325
2017-12-09T23:17:31.664701: step 748, loss 0.40739, acc 0.925781, prec 0.0545337, recall 0.830436
2017-12-09T23:17:32.166650: step 749, loss 0.391466, acc 0.902344, prec 0.0545654, recall 0.830603
2017-12-09T23:17:32.343561: step 750, loss 0.154812, acc 0.901961, prec 0.0545595, recall 0.830603
2017-12-09T23:17:32.852810: step 751, loss 0.352319, acc 0.945312, prec 0.0546041, recall 0.830769
2017-12-09T23:17:33.357842: step 752, loss 0.263144, acc 0.929688, prec 0.0546643, recall 0.83099
2017-12-09T23:17:33.875105: step 753, loss 0.218365, acc 0.933594, prec 0.0546647, recall 0.831046
2017-12-09T23:17:34.384176: step 754, loss 0.561183, acc 0.941406, prec 0.0546888, recall 0.830885
2017-12-09T23:17:34.888607: step 755, loss 0.475183, acc 0.925781, prec 0.0547477, recall 0.831105
2017-12-09T23:17:35.404286: step 756, loss 0.253616, acc 0.9375, prec 0.0548304, recall 0.83138
2017-12-09T23:17:35.914598: step 757, loss 0.235651, acc 0.902344, prec 0.0548618, recall 0.831545
2017-12-09T23:17:36.415527: step 758, loss 0.219882, acc 0.929688, prec 0.054942, recall 0.831818
2017-12-09T23:17:36.924131: step 759, loss 0.347699, acc 0.929688, prec 0.0549613, recall 0.831927
2017-12-09T23:17:37.425463: step 760, loss 0.185989, acc 0.953125, prec 0.0550484, recall 0.8322
2017-12-09T23:17:37.923990: step 761, loss 0.270982, acc 0.929688, prec 0.0550879, recall 0.832362
2017-12-09T23:17:38.430297: step 762, loss 0.804044, acc 0.953125, prec 0.0551761, recall 0.832364
2017-12-09T23:17:38.938182: step 763, loss 0.206733, acc 0.949219, prec 0.0552416, recall 0.832581
2017-12-09T23:17:39.446043: step 764, loss 0.296653, acc 0.933594, prec 0.0552822, recall 0.832743
2017-12-09T23:17:39.944720: step 765, loss 0.609104, acc 0.914062, prec 0.0552978, recall 0.832582
2017-12-09T23:17:40.440034: step 766, loss 0.28682, acc 0.917969, prec 0.0553335, recall 0.832744
2017-12-09T23:17:40.942565: step 767, loss 0.296982, acc 0.917969, prec 0.0553491, recall 0.832851
2017-12-09T23:17:41.448792: step 768, loss 0.621157, acc 0.894531, prec 0.0553978, recall 0.833066
2017-12-09T23:17:41.953158: step 769, loss 0.365787, acc 0.859375, prec 0.0554157, recall 0.833226
2017-12-09T23:17:42.456856: step 770, loss 0.379729, acc 0.890625, prec 0.0554632, recall 0.83344
2017-12-09T23:17:42.956354: step 771, loss 0.45692, acc 0.898438, prec 0.0555331, recall 0.833706
2017-12-09T23:17:43.463671: step 772, loss 0.643333, acc 0.875, prec 0.0555366, recall 0.833546
2017-12-09T23:17:43.966703: step 773, loss 0.356136, acc 0.90625, prec 0.0556087, recall 0.833812
2017-12-09T23:17:44.473161: step 774, loss 0.414276, acc 0.902344, prec 0.0556996, recall 0.834129
2017-12-09T23:17:44.977532: step 775, loss 0.473147, acc 0.90625, prec 0.0558317, recall 0.834551
2017-12-09T23:17:45.491300: step 776, loss 0.296273, acc 0.90625, prec 0.0558835, recall 0.834761
2017-12-09T23:17:46.000252: step 777, loss 0.374742, acc 0.894531, prec 0.0559316, recall 0.83497
2017-12-09T23:17:46.505678: step 778, loss 0.34831, acc 0.878906, prec 0.0559549, recall 0.835127
2017-12-09T23:17:47.003123: step 779, loss 0.359049, acc 0.902344, prec 0.0559652, recall 0.835231
2017-12-09T23:17:47.504354: step 780, loss 0.560125, acc 0.878906, prec 0.0560285, recall 0.835491
2017-12-09T23:17:48.011207: step 781, loss 0.29644, acc 0.894531, prec 0.0560763, recall 0.835699
2017-12-09T23:17:48.513385: step 782, loss 0.32696, acc 0.886719, prec 0.0560819, recall 0.835802
2017-12-09T23:17:49.011511: step 783, loss 0.242901, acc 0.933594, prec 0.0561615, recall 0.83606
2017-12-09T23:17:49.510117: step 784, loss 0.372772, acc 0.902344, prec 0.0562315, recall 0.836318
2017-12-09T23:17:50.013945: step 785, loss 0.840827, acc 0.925781, prec 0.0562899, recall 0.836261
2017-12-09T23:17:50.543579: step 786, loss 0.258102, acc 0.921875, prec 0.0563258, recall 0.836415
2017-12-09T23:17:51.044144: step 787, loss 0.942171, acc 0.929688, prec 0.0563455, recall 0.836255
2017-12-09T23:17:51.552802: step 788, loss 0.361511, acc 0.933594, prec 0.056385, recall 0.836409
2017-12-09T23:17:52.060206: step 789, loss 0.186195, acc 0.953125, prec 0.0564105, recall 0.836511
2017-12-09T23:17:52.567308: step 790, loss 0.617669, acc 0.925781, prec 0.0564885, recall 0.836505
2017-12-09T23:17:53.070684: step 791, loss 0.308394, acc 0.933594, prec 0.0566272, recall 0.836913
2017-12-09T23:17:53.571682: step 792, loss 0.260029, acc 0.914062, prec 0.0567003, recall 0.837166
2017-12-09T23:17:54.077209: step 793, loss 0.395759, acc 0.933594, prec 0.0567395, recall 0.837318
2017-12-09T23:17:54.582541: step 794, loss 0.477503, acc 0.90625, prec 0.0568299, recall 0.83762
2017-12-09T23:17:55.077443: step 795, loss 0.919183, acc 0.914062, prec 0.0568643, recall 0.837512
2017-12-09T23:17:55.587180: step 796, loss 0.297714, acc 0.898438, prec 0.0568926, recall 0.837662
2017-12-09T23:17:56.091688: step 797, loss 0.395694, acc 0.859375, prec 0.0568695, recall 0.837713
2017-12-09T23:17:56.601670: step 798, loss 0.341909, acc 0.875, prec 0.0568708, recall 0.837813
2017-12-09T23:17:57.108365: step 799, loss 0.434154, acc 0.894531, prec 0.0569177, recall 0.838013
2017-12-09T23:17:57.609230: step 800, loss 0.391567, acc 0.882812, prec 0.0570005, recall 0.838312
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-800

2017-12-09T23:17:59.132110: step 801, loss 0.367585, acc 0.882812, prec 0.0570436, recall 0.838511
2017-12-09T23:17:59.632404: step 802, loss 0.244324, acc 0.914062, prec 0.0570568, recall 0.838611
2017-12-09T23:18:00.141002: step 803, loss 0.330737, acc 0.902344, prec 0.0571255, recall 0.838858
2017-12-09T23:18:00.657134: step 804, loss 0.766058, acc 0.898438, prec 0.0571942, recall 0.838848
2017-12-09T23:18:01.157091: step 805, loss 0.264095, acc 0.894531, prec 0.057221, recall 0.838996
2017-12-09T23:18:01.659770: step 806, loss 0.326905, acc 0.898438, prec 0.0572686, recall 0.839193
2017-12-09T23:18:02.163723: step 807, loss 0.343842, acc 0.90625, prec 0.0572793, recall 0.839291
2017-12-09T23:18:02.673282: step 808, loss 0.22539, acc 0.929688, prec 0.0572971, recall 0.839389
2017-12-09T23:18:03.180341: step 809, loss 0.227252, acc 0.910156, prec 0.0573089, recall 0.839487
2017-12-09T23:18:03.677503: step 810, loss 0.22866, acc 0.921875, prec 0.0573439, recall 0.839634
2017-12-09T23:18:04.194097: step 811, loss 0.147373, acc 0.945312, prec 0.0574253, recall 0.839878
2017-12-09T23:18:04.695547: step 812, loss 0.221444, acc 0.945312, prec 0.0574478, recall 0.839976
2017-12-09T23:18:05.206148: step 813, loss 0.461128, acc 0.949219, prec 0.0575303, recall 0.840219
2017-12-09T23:18:05.728909: step 814, loss 0.495226, acc 0.949219, prec 0.0575931, recall 0.840413
2017-12-09T23:18:06.238245: step 815, loss 0.347551, acc 0.941406, prec 0.0576535, recall 0.840606
2017-12-09T23:18:06.741237: step 816, loss 0.253762, acc 0.949219, prec 0.0577163, recall 0.840799
2017-12-09T23:18:07.245054: step 817, loss 0.12931, acc 0.949219, prec 0.0577007, recall 0.840799
2017-12-09T23:18:07.746462: step 818, loss 0.320883, acc 0.945312, prec 0.0577426, recall 0.840943
2017-12-09T23:18:08.260954: step 819, loss 0.312965, acc 0.945312, prec 0.0578041, recall 0.841136
2017-12-09T23:18:08.769187: step 820, loss 0.372946, acc 0.957031, prec 0.05783, recall 0.841232
2017-12-09T23:18:09.280630: step 821, loss 0.274833, acc 0.941406, prec 0.0579097, recall 0.841471
2017-12-09T23:18:09.785970: step 822, loss 0.449673, acc 0.925781, prec 0.057965, recall 0.841662
2017-12-09T23:18:10.291801: step 823, loss 0.267227, acc 0.925781, prec 0.0580007, recall 0.841805
2017-12-09T23:18:10.798611: step 824, loss 0.492874, acc 0.914062, prec 0.0580133, recall 0.8419
2017-12-09T23:18:11.303567: step 825, loss 0.185042, acc 0.949219, prec 0.0580172, recall 0.841947
2017-12-09T23:18:11.808254: step 826, loss 0.378181, acc 0.917969, prec 0.0580505, recall 0.842089
2017-12-09T23:18:12.314281: step 827, loss 0.32273, acc 0.878906, prec 0.0580133, recall 0.842089
2017-12-09T23:18:12.814191: step 828, loss 0.29624, acc 0.90625, prec 0.0580819, recall 0.842326
2017-12-09T23:18:13.319343: step 829, loss 0.546182, acc 0.902344, prec 0.0580725, recall 0.842121
2017-12-09T23:18:13.819164: step 830, loss 0.661487, acc 0.894531, prec 0.0580608, recall 0.841916
2017-12-09T23:18:14.326622: step 831, loss 0.252138, acc 0.917969, prec 0.0580551, recall 0.841963
2017-12-09T23:18:14.829805: step 832, loss 0.264405, acc 0.90625, prec 0.0580458, recall 0.842011
2017-12-09T23:18:15.339866: step 833, loss 0.315755, acc 0.914062, prec 0.0580777, recall 0.842152
2017-12-09T23:18:15.844670: step 834, loss 0.420305, acc 0.964844, prec 0.058164, recall 0.842388
2017-12-09T23:18:16.349049: step 835, loss 0.302829, acc 0.902344, prec 0.0581728, recall 0.842482
2017-12-09T23:18:16.847334: step 836, loss 0.376207, acc 0.910156, prec 0.0583004, recall 0.842857
2017-12-09T23:18:17.351081: step 837, loss 0.30062, acc 0.886719, prec 0.0583431, recall 0.843044
2017-12-09T23:18:17.856961: step 838, loss 0.273885, acc 0.882812, prec 0.0583265, recall 0.843091
2017-12-09T23:18:18.359244: step 839, loss 0.260912, acc 0.902344, prec 0.0583546, recall 0.84323
2017-12-09T23:18:18.859021: step 840, loss 0.347847, acc 0.894531, prec 0.0583802, recall 0.84337
2017-12-09T23:18:19.361641: step 841, loss 0.262124, acc 0.914062, prec 0.0584312, recall 0.843556
2017-12-09T23:18:19.858324: step 842, loss 0.323144, acc 0.921875, prec 0.0584458, recall 0.843648
2017-12-09T23:18:20.366747: step 843, loss 0.225738, acc 0.945312, prec 0.0585063, recall 0.843833
2017-12-09T23:18:20.882508: step 844, loss 0.603757, acc 0.929688, prec 0.0585631, recall 0.843768
2017-12-09T23:18:21.394814: step 845, loss 0.917818, acc 0.9375, prec 0.0585644, recall 0.843566
2017-12-09T23:18:21.900824: step 846, loss 0.256286, acc 0.933594, prec 0.0586404, recall 0.843796
2017-12-09T23:18:22.407426: step 847, loss 0.278329, acc 0.910156, prec 0.0586513, recall 0.843888
2017-12-09T23:18:22.908380: step 848, loss 0.372729, acc 0.9375, prec 0.0587284, recall 0.844118
2017-12-09T23:18:23.407759: step 849, loss 0.251028, acc 0.925781, prec 0.0587441, recall 0.844209
2017-12-09T23:18:23.910517: step 850, loss 0.16009, acc 0.933594, prec 0.0587622, recall 0.844301
2017-12-09T23:18:24.415332: step 851, loss 0.735269, acc 0.914062, prec 0.0588139, recall 0.844236
2017-12-09T23:18:24.914104: step 852, loss 0.202078, acc 0.929688, prec 0.0588692, recall 0.844418
2017-12-09T23:18:25.417963: step 853, loss 0.269601, acc 0.921875, prec 0.0589989, recall 0.844782
2017-12-09T23:18:25.925292: step 854, loss 0.460214, acc 0.925781, prec 0.0590912, recall 0.845054
2017-12-09T23:18:26.437471: step 855, loss 0.484423, acc 0.875, prec 0.0591677, recall 0.845325
2017-12-09T23:18:26.941236: step 856, loss 0.275902, acc 0.898438, prec 0.0591556, recall 0.84537
2017-12-09T23:18:27.441633: step 857, loss 0.353303, acc 0.890625, prec 0.0592176, recall 0.845595
2017-12-09T23:18:27.940155: step 858, loss 0.235503, acc 0.894531, prec 0.0592234, recall 0.845684
2017-12-09T23:18:28.445326: step 859, loss 0.6578, acc 0.902344, prec 0.0593093, recall 0.845708
2017-12-09T23:18:28.956425: step 860, loss 0.270887, acc 0.902344, prec 0.0593556, recall 0.845886
2017-12-09T23:18:29.455024: step 861, loss 0.521943, acc 0.890625, prec 0.0593792, recall 0.84602
2017-12-09T23:18:29.960794: step 862, loss 0.255929, acc 0.910156, prec 0.0593897, recall 0.846109
2017-12-09T23:18:30.473739: step 863, loss 0.344806, acc 0.894531, prec 0.0593762, recall 0.846154
2017-12-09T23:18:30.976446: step 864, loss 0.886563, acc 0.894531, prec 0.0594415, recall 0.845887
2017-12-09T23:18:31.480833: step 865, loss 0.291677, acc 0.917969, prec 0.0594734, recall 0.846021
2017-12-09T23:18:31.992753: step 866, loss 0.292072, acc 0.929688, prec 0.0595279, recall 0.846198
2017-12-09T23:18:32.497767: step 867, loss 0.363484, acc 0.898438, prec 0.0596108, recall 0.846464
2017-12-09T23:18:32.998595: step 868, loss 0.363909, acc 0.898438, prec 0.0597317, recall 0.846816
2017-12-09T23:18:33.500991: step 869, loss 0.616989, acc 0.863281, prec 0.0597465, recall 0.846948
2017-12-09T23:18:34.001309: step 870, loss 0.31044, acc 0.898438, prec 0.0597911, recall 0.847123
2017-12-09T23:18:34.495690: step 871, loss 0.275827, acc 0.882812, prec 0.0597738, recall 0.847167
2017-12-09T23:18:34.994456: step 872, loss 0.462677, acc 0.933594, prec 0.0598672, recall 0.847429
2017-12-09T23:18:35.512675: step 873, loss 0.410619, acc 0.925781, prec 0.0599011, recall 0.847559
2017-12-09T23:18:36.027556: step 874, loss 0.224461, acc 0.925781, prec 0.0599161, recall 0.847646
2017-12-09T23:18:36.204183: step 875, loss 0.304556, acc 0.862745, prec 0.0599266, recall 0.84769
2017-12-09T23:18:36.707622: step 876, loss 0.155509, acc 0.945312, prec 0.0599665, recall 0.84782
2017-12-09T23:18:37.223048: step 877, loss 0.226372, acc 0.925781, prec 0.0600383, recall 0.848036
2017-12-09T23:18:37.728600: step 878, loss 0.248792, acc 0.941406, prec 0.0601337, recall 0.848295
2017-12-09T23:18:38.242650: step 879, loss 0.215119, acc 0.945312, prec 0.0601735, recall 0.848425
2017-12-09T23:18:38.743673: step 880, loss 0.186471, acc 0.941406, prec 0.0601932, recall 0.848511
2017-12-09T23:18:39.247202: step 881, loss 0.178356, acc 0.925781, prec 0.0602458, recall 0.848682
2017-12-09T23:18:39.741425: step 882, loss 0.129954, acc 0.976562, prec 0.060333, recall 0.848896
2017-12-09T23:18:40.243972: step 883, loss 0.236879, acc 0.960938, prec 0.0603776, recall 0.849025
2017-12-09T23:18:40.749512: step 884, loss 0.615383, acc 0.976562, prec 0.0604471, recall 0.848955
2017-12-09T23:18:41.258772: step 885, loss 0.641416, acc 0.941406, prec 0.0604678, recall 0.848801
2017-12-09T23:18:41.759668: step 886, loss 0.297238, acc 0.957031, prec 0.0605111, recall 0.848929
2017-12-09T23:18:42.258633: step 887, loss 0.566278, acc 0.941406, prec 0.0605683, recall 0.849099
2017-12-09T23:18:42.764949: step 888, loss 0.573584, acc 0.933594, prec 0.0605866, recall 0.848945
2017-12-09T23:18:43.268870: step 889, loss 0.523188, acc 0.929688, prec 0.0606413, recall 0.848876
2017-12-09T23:18:43.768337: step 890, loss 0.22747, acc 0.933594, prec 0.0606395, recall 0.848919
2017-12-09T23:18:44.268880: step 891, loss 0.302265, acc 0.914062, prec 0.0606692, recall 0.849046
2017-12-09T23:18:44.769112: step 892, loss 0.263299, acc 0.894531, prec 0.0606552, recall 0.849088
2017-12-09T23:18:45.273693: step 893, loss 0.277259, acc 0.910156, prec 0.0607402, recall 0.849342
2017-12-09T23:18:45.773651: step 894, loss 0.365935, acc 0.917969, prec 0.0607898, recall 0.84951
2017-12-09T23:18:46.282693: step 895, loss 0.358148, acc 0.871094, prec 0.0608061, recall 0.849637
2017-12-09T23:18:46.782731: step 896, loss 0.354957, acc 0.886719, prec 0.0608647, recall 0.849846
2017-12-09T23:18:47.289859: step 897, loss 0.210095, acc 0.925781, prec 0.0609166, recall 0.850014
2017-12-09T23:18:47.797801: step 898, loss 0.296378, acc 0.929688, prec 0.061026, recall 0.850306
2017-12-09T23:18:48.305447: step 899, loss 0.205221, acc 0.917969, prec 0.0610379, recall 0.850389
2017-12-09T23:18:48.814083: step 900, loss 0.296576, acc 0.917969, prec 0.0611622, recall 0.850721

Evaluation:
2017-12-09T23:18:53.551491: step 900, loss 1.95805, acc 0.92905, prec 0.0619732, recall 0.837049

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-900

2017-12-09T23:18:55.072456: step 901, loss 0.369284, acc 0.890625, prec 0.0619943, recall 0.837178
2017-12-09T23:18:55.575890: step 902, loss 0.207873, acc 0.945312, prec 0.062051, recall 0.837351
2017-12-09T23:18:56.079093: step 903, loss 0.189529, acc 0.9375, prec 0.0621051, recall 0.837523
2017-12-09T23:18:56.584311: step 904, loss 0.525975, acc 0.9375, prec 0.0621236, recall 0.837388
2017-12-09T23:18:57.090365: step 905, loss 0.0917129, acc 0.96875, prec 0.0621139, recall 0.837388
2017-12-09T23:18:57.591669: step 906, loss 0.152976, acc 0.925781, prec 0.0621459, recall 0.837516
2017-12-09T23:18:58.090894: step 907, loss 0.121857, acc 0.941406, prec 0.0621276, recall 0.837516
2017-12-09T23:18:58.587098: step 908, loss 0.499593, acc 0.960938, prec 0.0622441, recall 0.837816
2017-12-09T23:18:59.102225: step 909, loss 0.331505, acc 0.964844, prec 0.0622698, recall 0.837902
2017-12-09T23:18:59.602369: step 910, loss 0.311268, acc 0.964844, prec 0.0623323, recall 0.838073
2017-12-09T23:19:00.111797: step 911, loss 0.363368, acc 0.964844, prec 0.0623764, recall 0.8382
2017-12-09T23:19:00.625791: step 912, loss 0.183408, acc 0.964844, prec 0.0624388, recall 0.838371
2017-12-09T23:19:01.136880: step 913, loss 0.176057, acc 0.941406, prec 0.0624572, recall 0.838455
2017-12-09T23:19:01.638186: step 914, loss 0.26966, acc 0.953125, prec 0.0624792, recall 0.83854
2017-12-09T23:19:02.149351: step 915, loss 0.151224, acc 0.964844, prec 0.0625232, recall 0.838667
2017-12-09T23:19:02.653853: step 916, loss 0.398832, acc 0.929688, prec 0.0625745, recall 0.838836
2017-12-09T23:19:03.159427: step 917, loss 0.186847, acc 0.972656, prec 0.0626576, recall 0.839047
2017-12-09T23:19:03.666706: step 918, loss 0.346224, acc 0.921875, prec 0.0627063, recall 0.839216
2017-12-09T23:19:04.173263: step 919, loss 0.160489, acc 0.9375, prec 0.0627233, recall 0.8393
2017-12-09T23:19:04.676515: step 920, loss 0.505056, acc 0.917969, prec 0.0627172, recall 0.839122
2017-12-09T23:19:05.181764: step 921, loss 0.503054, acc 0.921875, prec 0.0628024, recall 0.839374
2017-12-09T23:19:05.709180: step 922, loss 0.234108, acc 0.917969, prec 0.0628681, recall 0.839583
2017-12-09T23:19:06.207062: step 923, loss 0.248779, acc 0.933594, prec 0.0628838, recall 0.839667
2017-12-09T23:19:06.711242: step 924, loss 0.201213, acc 0.929688, prec 0.0628982, recall 0.83975
2017-12-09T23:19:07.211134: step 925, loss 0.21229, acc 0.921875, prec 0.0629832, recall 0.84
2017-12-09T23:19:07.715181: step 926, loss 0.227459, acc 0.925781, prec 0.0629964, recall 0.840083
2017-12-09T23:19:08.221137: step 927, loss 0.168497, acc 0.941406, prec 0.063051, recall 0.840249
2017-12-09T23:19:08.725098: step 928, loss 0.286346, acc 0.9375, prec 0.0631042, recall 0.840415
2017-12-09T23:19:09.222747: step 929, loss 0.363415, acc 0.90625, prec 0.063093, recall 0.840456
2017-12-09T23:19:09.728035: step 930, loss 0.284484, acc 0.933594, prec 0.0631814, recall 0.840703
2017-12-09T23:19:10.234728: step 931, loss 0.18997, acc 0.957031, prec 0.0632225, recall 0.840827
2017-12-09T23:19:10.751756: step 932, loss 0.147578, acc 0.941406, prec 0.0632223, recall 0.840868
2017-12-09T23:19:11.253927: step 933, loss 0.141193, acc 0.945312, prec 0.0632233, recall 0.840909
2017-12-09T23:19:11.761034: step 934, loss 0.0951522, acc 0.96875, prec 0.0632317, recall 0.84095
2017-12-09T23:19:12.269123: step 935, loss 0.112698, acc 0.96875, prec 0.0632764, recall 0.841073
2017-12-09T23:19:12.773621: step 936, loss 0.18234, acc 0.96875, prec 0.0633938, recall 0.84136
2017-12-09T23:19:13.282721: step 937, loss 0.471414, acc 0.972656, prec 0.0634228, recall 0.841225
2017-12-09T23:19:13.796884: step 938, loss 0.179721, acc 0.980469, prec 0.0634893, recall 0.841388
2017-12-09T23:19:14.300725: step 939, loss 0.311338, acc 0.960938, prec 0.0634951, recall 0.841429
2017-12-09T23:19:14.802509: step 940, loss 0.325645, acc 0.980469, prec 0.0635798, recall 0.841632
2017-12-09T23:19:15.308137: step 941, loss 0.485367, acc 0.957031, prec 0.0636219, recall 0.841538
2017-12-09T23:19:15.814003: step 942, loss 0.162661, acc 0.953125, prec 0.0636616, recall 0.84166
2017-12-09T23:19:16.312803: step 943, loss 0.334751, acc 0.957031, prec 0.0637387, recall 0.841863
2017-12-09T23:19:16.815022: step 944, loss 0.126772, acc 0.96875, prec 0.0637832, recall 0.841984
2017-12-09T23:19:17.322008: step 945, loss 0.480906, acc 0.964844, prec 0.063864, recall 0.841971
2017-12-09T23:19:17.828789: step 946, loss 0.233093, acc 0.914062, prec 0.0638912, recall 0.842092
2017-12-09T23:19:18.335505: step 947, loss 0.3121, acc 0.953125, prec 0.0639125, recall 0.842172
2017-12-09T23:19:18.838503: step 948, loss 0.204759, acc 0.929688, prec 0.0639446, recall 0.842293
2017-12-09T23:19:19.345393: step 949, loss 0.221901, acc 0.929688, prec 0.064049, recall 0.842574
2017-12-09T23:19:19.846957: step 950, loss 0.305773, acc 0.894531, prec 0.0641603, recall 0.842893
2017-12-09T23:19:20.360507: step 951, loss 0.342378, acc 0.902344, prec 0.0642016, recall 0.843053
2017-12-09T23:19:20.871070: step 952, loss 0.29733, acc 0.875, prec 0.0642161, recall 0.843172
2017-12-09T23:19:21.378571: step 953, loss 0.57621, acc 0.898438, prec 0.0642741, recall 0.84337
2017-12-09T23:19:21.885182: step 954, loss 0.441072, acc 0.867188, prec 0.0643582, recall 0.843647
2017-12-09T23:19:22.388441: step 955, loss 0.379756, acc 0.894531, prec 0.0643788, recall 0.843766
2017-12-09T23:19:22.892337: step 956, loss 0.303581, acc 0.894531, prec 0.0644174, recall 0.843923
2017-12-09T23:19:23.399218: step 957, loss 0.249348, acc 0.910156, prec 0.0644969, recall 0.844159
2017-12-09T23:19:23.899643: step 958, loss 0.307034, acc 0.894531, prec 0.0644814, recall 0.844198
2017-12-09T23:19:24.402537: step 959, loss 0.254282, acc 0.929688, prec 0.064513, recall 0.844316
2017-12-09T23:19:24.906338: step 960, loss 0.230529, acc 0.917969, prec 0.0645768, recall 0.844511
2017-12-09T23:19:25.434570: step 961, loss 0.183596, acc 0.933594, prec 0.0645737, recall 0.84455
2017-12-09T23:19:25.935659: step 962, loss 0.175562, acc 0.957031, prec 0.064596, recall 0.844629
2017-12-09T23:19:26.441183: step 963, loss 0.195965, acc 0.941406, prec 0.0646313, recall 0.844745
2017-12-09T23:19:26.950798: step 964, loss 0.238798, acc 0.964844, prec 0.0646919, recall 0.844901
2017-12-09T23:19:27.458575: step 965, loss 0.292699, acc 0.933594, prec 0.0647067, recall 0.844979
2017-12-09T23:19:27.958951: step 966, loss 0.152318, acc 0.964844, prec 0.0647852, recall 0.845173
2017-12-09T23:19:28.460597: step 967, loss 0.111366, acc 0.996094, prec 0.0648019, recall 0.845211
2017-12-09T23:19:28.962716: step 968, loss 0.214827, acc 0.964844, prec 0.0648803, recall 0.845405
2017-12-09T23:19:29.466662: step 969, loss 0.341359, acc 0.972656, prec 0.0649266, recall 0.845309
2017-12-09T23:19:29.979659: step 970, loss 0.170577, acc 0.964844, prec 0.0649512, recall 0.845387
2017-12-09T23:19:30.497720: step 971, loss 0.427704, acc 0.976562, prec 0.0650512, recall 0.845618
2017-12-09T23:19:31.004631: step 972, loss 0.16813, acc 0.953125, prec 0.0651258, recall 0.84581
2017-12-09T23:19:31.507235: step 973, loss 0.245901, acc 0.964844, prec 0.0651862, recall 0.845963
2017-12-09T23:19:32.014455: step 974, loss 0.0712285, acc 0.980469, prec 0.0651799, recall 0.845963
2017-12-09T23:19:32.512435: step 975, loss 1.14403, acc 0.957031, prec 0.0652582, recall 0.845734
2017-12-09T23:19:33.018866: step 976, loss 0.270624, acc 0.945312, prec 0.0653122, recall 0.845887
2017-12-09T23:19:33.524324: step 977, loss 0.216321, acc 0.917969, prec 0.0653754, recall 0.846078
2017-12-09T23:19:34.032257: step 978, loss 0.12426, acc 0.949219, prec 0.0653591, recall 0.846078
2017-12-09T23:19:34.533362: step 979, loss 0.246825, acc 0.917969, prec 0.0654043, recall 0.84623
2017-12-09T23:19:35.039165: step 980, loss 0.414583, acc 0.898438, prec 0.0654432, recall 0.846382
2017-12-09T23:19:35.559148: step 981, loss 0.253667, acc 0.933594, prec 0.0654933, recall 0.846533
2017-12-09T23:19:36.065097: step 982, loss 0.289236, acc 0.890625, prec 0.0655118, recall 0.846647
2017-12-09T23:19:36.567068: step 983, loss 0.313703, acc 0.882812, prec 0.0654922, recall 0.846685
2017-12-09T23:19:37.071183: step 984, loss 0.443247, acc 0.902344, prec 0.0655144, recall 0.846798
2017-12-09T23:19:37.577540: step 985, loss 0.369887, acc 0.84375, prec 0.0655001, recall 0.846873
2017-12-09T23:19:38.077596: step 986, loss 0.328243, acc 0.890625, prec 0.0655363, recall 0.847024
2017-12-09T23:19:38.576899: step 987, loss 0.333685, acc 0.914062, prec 0.0655978, recall 0.847212
2017-12-09T23:19:39.077592: step 988, loss 0.26558, acc 0.898438, prec 0.0656186, recall 0.847324
2017-12-09T23:19:39.579561: step 989, loss 0.21354, acc 0.933594, prec 0.0656507, recall 0.847437
2017-12-09T23:19:40.082140: step 990, loss 0.770119, acc 0.9375, prec 0.0656852, recall 0.847341
2017-12-09T23:19:40.593987: step 991, loss 0.148204, acc 0.953125, prec 0.0657235, recall 0.847453
2017-12-09T23:19:41.099657: step 992, loss 0.362818, acc 0.941406, prec 0.0657935, recall 0.84764
2017-12-09T23:19:41.597870: step 993, loss 0.334542, acc 0.925781, prec 0.0658229, recall 0.847752
2017-12-09T23:19:42.101715: step 994, loss 0.233183, acc 0.945312, prec 0.0659295, recall 0.848012
2017-12-09T23:19:42.607561: step 995, loss 0.215122, acc 0.910156, prec 0.0659539, recall 0.848123
2017-12-09T23:19:43.107748: step 996, loss 0.357628, acc 0.953125, prec 0.0660628, recall 0.848382
2017-12-09T23:19:43.611842: step 997, loss 0.348256, acc 0.9375, prec 0.0661135, recall 0.848529
2017-12-09T23:19:44.114812: step 998, loss 1.6656, acc 0.941406, prec 0.066168, recall 0.848264
2017-12-09T23:19:44.621485: step 999, loss 0.231799, acc 0.941406, prec 0.0662376, recall 0.848448
2017-12-09T23:19:44.799048: step 1000, loss 0.231169, acc 0.862745, prec 0.0662288, recall 0.848448
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-1000

2017-12-09T23:19:46.404299: step 1001, loss 0.2607, acc 0.914062, prec 0.0662366, recall 0.848522
2017-12-09T23:19:46.905594: step 1002, loss 0.332544, acc 0.878906, prec 0.066286, recall 0.848705
2017-12-09T23:19:47.413482: step 1003, loss 0.294041, acc 0.894531, prec 0.0663051, recall 0.848815
2017-12-09T23:19:47.919088: step 1004, loss 0.29034, acc 0.914062, prec 0.0663481, recall 0.848961
2017-12-09T23:19:48.420837: step 1005, loss 0.573782, acc 0.898438, prec 0.0664565, recall 0.849252
2017-12-09T23:19:48.927341: step 1006, loss 0.251573, acc 0.921875, prec 0.0664843, recall 0.849361
2017-12-09T23:19:49.436453: step 1007, loss 0.20986, acc 0.90625, prec 0.0665246, recall 0.849506
2017-12-09T23:19:49.942656: step 1008, loss 0.243174, acc 0.917969, prec 0.0665687, recall 0.849651
2017-12-09T23:19:50.453603: step 1009, loss 0.198377, acc 0.933594, prec 0.0665825, recall 0.849724
2017-12-09T23:19:50.958798: step 1010, loss 0.280502, acc 0.945312, prec 0.0666353, recall 0.849868
2017-12-09T23:19:51.454731: step 1011, loss 0.11601, acc 0.945312, prec 0.0666704, recall 0.849976
2017-12-09T23:19:51.953288: step 1012, loss 0.157192, acc 0.949219, prec 0.0666717, recall 0.850012
2017-12-09T23:19:52.459371: step 1013, loss 0.158439, acc 0.953125, prec 0.0666918, recall 0.850084
2017-12-09T23:19:52.962923: step 1014, loss 0.508725, acc 0.964844, prec 0.0667344, recall 0.849988
2017-12-09T23:19:53.467541: step 1015, loss 0.109285, acc 0.960938, prec 0.0667745, recall 0.850096
2017-12-09T23:19:53.974665: step 1016, loss 0.350452, acc 0.976562, prec 0.0668723, recall 0.850311
2017-12-09T23:19:54.481375: step 1017, loss 0.36424, acc 0.945312, prec 0.0669599, recall 0.850525
2017-12-09T23:19:54.993179: step 1018, loss 0.726615, acc 0.957031, prec 0.0669999, recall 0.850429
2017-12-09T23:19:55.505848: step 1019, loss 0.271442, acc 0.933594, prec 0.0670487, recall 0.850572
2017-12-09T23:19:56.012351: step 1020, loss 0.163826, acc 0.945312, prec 0.0670661, recall 0.850643
2017-12-09T23:19:56.518589: step 1021, loss 0.23034, acc 0.9375, prec 0.067116, recall 0.850785
2017-12-09T23:19:57.020395: step 1022, loss 0.158537, acc 0.949219, prec 0.0671696, recall 0.850927
2017-12-09T23:19:57.530874: step 1023, loss 0.361324, acc 0.941406, prec 0.0672557, recall 0.85114
2017-12-09T23:19:58.041811: step 1024, loss 0.304014, acc 0.945312, prec 0.0673255, recall 0.851316
2017-12-09T23:19:58.548067: step 1025, loss 0.274975, acc 0.910156, prec 0.0673665, recall 0.851457
2017-12-09T23:19:59.050053: step 1026, loss 0.207499, acc 0.933594, prec 0.0674324, recall 0.851633
2017-12-09T23:19:59.558467: step 1027, loss 0.150393, acc 0.941406, prec 0.0674833, recall 0.851773
2017-12-09T23:20:00.063878: step 1028, loss 0.283903, acc 0.921875, prec 0.0675104, recall 0.851878
2017-12-09T23:20:00.573472: step 1029, loss 0.208371, acc 0.957031, prec 0.0675314, recall 0.851948
2017-12-09T23:20:01.085115: step 1030, loss 0.343376, acc 0.921875, prec 0.0675759, recall 0.852088
2017-12-09T23:20:01.597832: step 1031, loss 0.234913, acc 0.925781, prec 0.0676391, recall 0.852262
2017-12-09T23:20:02.098587: step 1032, loss 0.24961, acc 0.921875, prec 0.0676835, recall 0.852401
2017-12-09T23:20:02.608112: step 1033, loss 0.214038, acc 0.9375, prec 0.06782, recall 0.852713
2017-12-09T23:20:03.115131: step 1034, loss 0.151531, acc 0.953125, prec 0.0678048, recall 0.852713
2017-12-09T23:20:03.612513: step 1035, loss 0.244029, acc 0.921875, prec 0.0678491, recall 0.852851
2017-12-09T23:20:04.113239: step 1036, loss 0.398361, acc 0.945312, prec 0.067901, recall 0.852989
2017-12-09T23:20:04.612327: step 1037, loss 0.234546, acc 0.960938, prec 0.0679927, recall 0.853196
2017-12-09T23:20:05.117229: step 1038, loss 0.122254, acc 0.949219, prec 0.0680631, recall 0.853368
2017-12-09T23:20:05.632528: step 1039, loss 0.134189, acc 0.953125, prec 0.0681174, recall 0.853505
2017-12-09T23:20:06.139024: step 1040, loss 0.327929, acc 0.945312, prec 0.0682038, recall 0.85371
2017-12-09T23:20:06.637761: step 1041, loss 0.286923, acc 0.964844, prec 0.0682445, recall 0.853812
2017-12-09T23:20:07.149382: step 1042, loss 0.444428, acc 0.972656, prec 0.0683063, recall 0.853749
2017-12-09T23:20:07.647943: step 1043, loss 0.137134, acc 0.945312, prec 0.0683232, recall 0.853818
2017-12-09T23:20:08.147758: step 1044, loss 0.127532, acc 0.972656, prec 0.0683664, recall 0.85392
2017-12-09T23:20:08.654367: step 1045, loss 0.352181, acc 0.96875, prec 0.0683909, recall 0.853987
2017-12-09T23:20:09.158666: step 1046, loss 0.140414, acc 0.949219, prec 0.0684437, recall 0.854123
2017-12-09T23:20:09.661746: step 1047, loss 0.209409, acc 0.953125, prec 0.0684631, recall 0.854191
2017-12-09T23:20:10.173252: step 1048, loss 0.108324, acc 0.957031, prec 0.0685184, recall 0.854326
2017-12-09T23:20:10.680383: step 1049, loss 0.193397, acc 0.941406, prec 0.0685339, recall 0.854394
2017-12-09T23:20:11.183679: step 1050, loss 0.175921, acc 0.941406, prec 0.0685494, recall 0.854461
2017-12-09T23:20:11.699076: step 1051, loss 0.290495, acc 0.960938, prec 0.0686059, recall 0.854596
2017-12-09T23:20:12.202543: step 1052, loss 0.139351, acc 0.949219, prec 0.0686932, recall 0.854798
2017-12-09T23:20:12.700366: step 1053, loss 0.170277, acc 0.957031, prec 0.0687138, recall 0.854865
2017-12-09T23:20:13.199957: step 1054, loss 0.186087, acc 0.953125, prec 0.0687504, recall 0.854965
2017-12-09T23:20:13.697496: step 1055, loss 0.24106, acc 0.960938, prec 0.0688067, recall 0.855099
2017-12-09T23:20:14.202566: step 1056, loss 0.289026, acc 0.964844, prec 0.0689508, recall 0.855399
2017-12-09T23:20:14.708802: step 1057, loss 0.107233, acc 0.964844, prec 0.0690084, recall 0.855533
2017-12-09T23:20:15.215220: step 1058, loss 0.333931, acc 0.945312, prec 0.0691286, recall 0.855798
2017-12-09T23:20:15.717519: step 1059, loss 0.13111, acc 0.964844, prec 0.0691689, recall 0.855897
2017-12-09T23:20:16.217684: step 1060, loss 0.22083, acc 0.929688, prec 0.0691803, recall 0.855963
2017-12-09T23:20:16.720774: step 1061, loss 0.161473, acc 0.960938, prec 0.0691847, recall 0.855996
2017-12-09T23:20:17.227808: step 1062, loss 0.156426, acc 0.941406, prec 0.0692172, recall 0.856095
2017-12-09T23:20:17.728906: step 1063, loss 0.813676, acc 0.917969, prec 0.0691916, recall 0.855899
2017-12-09T23:20:18.240877: step 1064, loss 0.188982, acc 0.929688, prec 0.069203, recall 0.855965
2017-12-09T23:20:18.745589: step 1065, loss 0.142841, acc 0.945312, prec 0.0692023, recall 0.855998
2017-12-09T23:20:19.252651: step 1066, loss 0.1483, acc 0.945312, prec 0.0692016, recall 0.856031
2017-12-09T23:20:19.758515: step 1067, loss 0.145455, acc 0.964844, prec 0.0693106, recall 0.856261
2017-12-09T23:20:20.258715: step 1068, loss 0.560103, acc 0.9375, prec 0.0693258, recall 0.856132
2017-12-09T23:20:20.779919: step 1069, loss 0.181166, acc 0.941406, prec 0.0693926, recall 0.856296
2017-12-09T23:20:21.278821: step 1070, loss 0.196884, acc 0.957031, prec 0.0694301, recall 0.856394
2017-12-09T23:20:21.784598: step 1071, loss 0.24957, acc 0.945312, prec 0.0694981, recall 0.856557
2017-12-09T23:20:22.281859: step 1072, loss 0.14846, acc 0.949219, prec 0.0695329, recall 0.856655
2017-12-09T23:20:22.786755: step 1073, loss 0.875455, acc 0.941406, prec 0.0695837, recall 0.856591
2017-12-09T23:20:23.292953: step 1074, loss 0.227722, acc 0.933594, prec 0.0695962, recall 0.856656
2017-12-09T23:20:23.801272: step 1075, loss 0.167931, acc 0.9375, prec 0.06961, recall 0.856721
2017-12-09T23:20:24.301375: step 1076, loss 0.238971, acc 0.9375, prec 0.0696409, recall 0.856819
2017-12-09T23:20:24.808169: step 1077, loss 0.279878, acc 0.953125, prec 0.0697113, recall 0.856981
2017-12-09T23:20:25.314919: step 1078, loss 0.189783, acc 0.941406, prec 0.0697263, recall 0.857046
2017-12-09T23:20:25.818939: step 1079, loss 0.231757, acc 0.925781, prec 0.0697704, recall 0.857175
2017-12-09T23:20:26.324338: step 1080, loss 0.206736, acc 0.964844, prec 0.0698103, recall 0.857272
2017-12-09T23:20:26.832449: step 1081, loss 0.200677, acc 0.941406, prec 0.0698424, recall 0.857369
2017-12-09T23:20:27.336065: step 1082, loss 0.191545, acc 0.941406, prec 0.0698402, recall 0.857401
2017-12-09T23:20:27.836381: step 1083, loss 0.631156, acc 0.941406, prec 0.0698565, recall 0.857272
2017-12-09T23:20:28.339663: step 1084, loss 0.224882, acc 0.933594, prec 0.0699373, recall 0.857465
2017-12-09T23:20:28.840427: step 1085, loss 0.266188, acc 0.949219, prec 0.069989, recall 0.857594
2017-12-09T23:20:29.346027: step 1086, loss 0.206889, acc 0.921875, prec 0.0699803, recall 0.857626
2017-12-09T23:20:29.845801: step 1087, loss 0.269727, acc 0.957031, prec 0.0700175, recall 0.857722
2017-12-09T23:20:30.366377: step 1088, loss 0.196532, acc 0.960938, prec 0.0700729, recall 0.85785
2017-12-09T23:20:30.867858: step 1089, loss 0.114814, acc 0.96875, prec 0.0701481, recall 0.858009
2017-12-09T23:20:31.372644: step 1090, loss 0.222711, acc 0.949219, prec 0.0702337, recall 0.858201
2017-12-09T23:20:31.878982: step 1091, loss 0.160439, acc 0.957031, prec 0.0702366, recall 0.858232
2017-12-09T23:20:32.381238: step 1092, loss 0.243617, acc 0.933594, prec 0.0703171, recall 0.858423
2017-12-09T23:20:32.887361: step 1093, loss 0.255148, acc 0.949219, prec 0.0703515, recall 0.858518
2017-12-09T23:20:33.392826: step 1094, loss 0.103796, acc 0.960938, prec 0.0703897, recall 0.858613
2017-12-09T23:20:33.897961: step 1095, loss 0.17523, acc 0.957031, prec 0.0704096, recall 0.858676
2017-12-09T23:20:34.406348: step 1096, loss 0.296798, acc 0.953125, prec 0.0705134, recall 0.858897
2017-12-09T23:20:34.908226: step 1097, loss 0.0939488, acc 0.972656, prec 0.0705214, recall 0.858929
2017-12-09T23:20:35.423983: step 1098, loss 0.118412, acc 0.980469, prec 0.0705831, recall 0.859054
2017-12-09T23:20:35.940638: step 1099, loss 0.0938168, acc 0.96875, prec 0.0706749, recall 0.859243
2017-12-09T23:20:36.443577: step 1100, loss 0.207151, acc 0.96875, prec 0.0707326, recall 0.859368
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-1100

2017-12-09T23:20:37.998340: step 1101, loss 0.164035, acc 0.960938, prec 0.0707537, recall 0.859431
2017-12-09T23:20:38.499087: step 1102, loss 0.451277, acc 0.972656, prec 0.070848, recall 0.859427
2017-12-09T23:20:39.010701: step 1103, loss 0.263869, acc 0.941406, prec 0.0709306, recall 0.859614
2017-12-09T23:20:39.517107: step 1104, loss 0.139991, acc 0.953125, prec 0.070932, recall 0.859645
2017-12-09T23:20:40.022344: step 1105, loss 0.296918, acc 0.949219, prec 0.0709831, recall 0.85977
2017-12-09T23:20:40.529819: step 1106, loss 0.146395, acc 0.960938, prec 0.071055, recall 0.859925
2017-12-09T23:20:41.028550: step 1107, loss 0.135911, acc 0.953125, prec 0.0710395, recall 0.859925
2017-12-09T23:20:41.530844: step 1108, loss 0.0768598, acc 0.96875, prec 0.0710291, recall 0.859925
2017-12-09T23:20:42.034789: step 1109, loss 0.185678, acc 0.949219, prec 0.0710631, recall 0.860018
2017-12-09T23:20:42.532540: step 1110, loss 0.197012, acc 0.960938, prec 0.071118, recall 0.860141
2017-12-09T23:20:43.033904: step 1111, loss 0.258696, acc 0.953125, prec 0.0711364, recall 0.860203
2017-12-09T23:20:43.536628: step 1112, loss 0.194264, acc 0.945312, prec 0.071169, recall 0.860296
2017-12-09T23:20:44.040314: step 1113, loss 0.17292, acc 0.945312, prec 0.0712017, recall 0.860388
2017-12-09T23:20:44.544466: step 1114, loss 0.1776, acc 0.964844, prec 0.0712239, recall 0.86045
2017-12-09T23:20:45.059849: step 1115, loss 0.513964, acc 0.953125, prec 0.0712944, recall 0.860414
2017-12-09T23:20:45.577182: step 1116, loss 0.095886, acc 0.964844, prec 0.0713165, recall 0.860475
2017-12-09T23:20:46.083324: step 1117, loss 0.156901, acc 0.957031, prec 0.0713869, recall 0.860629
2017-12-09T23:20:46.593375: step 1118, loss 0.271165, acc 0.945312, prec 0.0714533, recall 0.860782
2017-12-09T23:20:47.105613: step 1119, loss 0.230296, acc 0.96875, prec 0.0714767, recall 0.860843
2017-12-09T23:20:47.609051: step 1120, loss 0.179673, acc 0.945312, prec 0.0715093, recall 0.860934
2017-12-09T23:20:48.117744: step 1121, loss 0.220436, acc 0.941406, prec 0.0716081, recall 0.861148
2017-12-09T23:20:48.620207: step 1122, loss 0.211971, acc 0.953125, prec 0.071677, recall 0.8613
2017-12-09T23:20:49.130994: step 1123, loss 0.181223, acc 0.941406, prec 0.071725, recall 0.861421
2017-12-09T23:20:49.637301: step 1124, loss 0.124606, acc 0.957031, prec 0.0717613, recall 0.861512
2017-12-09T23:20:49.817086: step 1125, loss 0.220485, acc 0.921569, prec 0.0717561, recall 0.861512
2017-12-09T23:20:50.328570: step 1126, loss 0.20832, acc 0.972656, prec 0.0718482, recall 0.861693
2017-12-09T23:20:50.841416: step 1127, loss 0.128907, acc 0.941406, prec 0.0718455, recall 0.861723
2017-12-09T23:20:51.343177: step 1128, loss 0.193609, acc 0.964844, prec 0.0719181, recall 0.861874
2017-12-09T23:20:51.847176: step 1129, loss 0.0903558, acc 0.957031, prec 0.0719206, recall 0.861904
2017-12-09T23:20:52.348792: step 1130, loss 0.0977475, acc 0.96875, prec 0.0719439, recall 0.861964
2017-12-09T23:20:52.845943: step 1131, loss 0.186398, acc 0.957031, prec 0.0719969, recall 0.862084
2017-12-09T23:20:53.352448: step 1132, loss 0.108889, acc 0.976562, prec 0.0720734, recall 0.862234
2017-12-09T23:20:53.860288: step 1133, loss 0.117397, acc 0.964844, prec 0.0721459, recall 0.862383
2017-12-09T23:20:54.366650: step 1134, loss 0.254958, acc 0.953125, prec 0.0721807, recall 0.862473
2017-12-09T23:20:54.874390: step 1135, loss 0.218738, acc 0.957031, prec 0.0721831, recall 0.862503
2017-12-09T23:20:55.383343: step 1136, loss 0.128625, acc 0.976562, prec 0.0722089, recall 0.862562
2017-12-09T23:20:55.887765: step 1137, loss 0.140348, acc 0.976562, prec 0.0722516, recall 0.862652
2017-12-09T23:20:56.388320: step 1138, loss 0.0398909, acc 0.992188, prec 0.0722658, recall 0.862681
2017-12-09T23:20:56.884784: step 1139, loss 0.592599, acc 0.960938, prec 0.0723381, recall 0.862643
2017-12-09T23:20:57.395560: step 1140, loss 0.0837328, acc 0.984375, prec 0.0723497, recall 0.862673
2017-12-09T23:20:57.902663: step 1141, loss 0.223016, acc 0.964844, prec 0.0723715, recall 0.862732
2017-12-09T23:20:58.406921: step 1142, loss 0.625995, acc 0.957031, prec 0.072392, recall 0.862605
2017-12-09T23:20:58.907523: step 1143, loss 0.147606, acc 0.96875, prec 0.0724656, recall 0.862754
2017-12-09T23:20:59.406566: step 1144, loss 0.140229, acc 0.953125, prec 0.0725003, recall 0.862842
2017-12-09T23:20:59.902066: step 1145, loss 0.108782, acc 0.964844, prec 0.0725725, recall 0.86299
2017-12-09T23:21:00.417469: step 1146, loss 0.207543, acc 0.929688, prec 0.0727335, recall 0.863314
2017-12-09T23:21:00.920833: step 1147, loss 0.214896, acc 0.925781, prec 0.0727589, recall 0.863402
2017-12-09T23:21:01.419547: step 1148, loss 0.162734, acc 0.953125, prec 0.0728437, recall 0.863578
2017-12-09T23:21:01.922875: step 1149, loss 0.223911, acc 0.953125, prec 0.0729118, recall 0.863724
2017-12-09T23:21:02.428229: step 1150, loss 0.197238, acc 0.917969, prec 0.0729344, recall 0.863812
2017-12-09T23:21:02.936922: step 1151, loss 0.162415, acc 0.9375, prec 0.07293, recall 0.863841
2017-12-09T23:21:03.444143: step 1152, loss 0.219264, acc 0.953125, prec 0.0729645, recall 0.863928
2017-12-09T23:21:03.945002: step 1153, loss 0.184455, acc 0.933594, prec 0.0729923, recall 0.864015
2017-12-09T23:21:04.448509: step 1154, loss 0.199588, acc 0.972656, prec 0.0730835, recall 0.86419
2017-12-09T23:21:04.946966: step 1155, loss 0.541466, acc 0.957031, prec 0.0730871, recall 0.864034
2017-12-09T23:21:05.460627: step 1156, loss 0.218897, acc 0.960938, prec 0.0731408, recall 0.86415
2017-12-09T23:21:05.967634: step 1157, loss 0.228574, acc 0.96875, prec 0.0732306, recall 0.864324
2017-12-09T23:21:06.473151: step 1158, loss 0.166021, acc 0.972656, prec 0.0733551, recall 0.864555
2017-12-09T23:21:06.974599: step 1159, loss 0.205509, acc 0.960938, prec 0.0734254, recall 0.864698
2017-12-09T23:21:07.487793: step 1160, loss 0.176538, acc 0.957031, prec 0.0734944, recall 0.864842
2017-12-09T23:21:07.991947: step 1161, loss 0.147215, acc 0.953125, prec 0.0735453, recall 0.864957
2017-12-09T23:21:08.495005: step 1162, loss 0.277771, acc 0.957031, prec 0.0736309, recall 0.865128
2017-12-09T23:21:08.994969: step 1163, loss 0.130336, acc 0.972656, prec 0.0736884, recall 0.865242
2017-12-09T23:21:09.508689: step 1164, loss 0.137032, acc 0.960938, prec 0.0737085, recall 0.865299
2017-12-09T23:21:10.017516: step 1165, loss 0.203954, acc 0.941406, prec 0.0737553, recall 0.865413
2017-12-09T23:21:10.520994: step 1166, loss 0.310729, acc 0.921875, prec 0.0738121, recall 0.865555
2017-12-09T23:21:11.030102: step 1167, loss 0.2804, acc 0.929688, prec 0.0739048, recall 0.865753
2017-12-09T23:21:11.542235: step 1168, loss 0.128065, acc 0.972656, prec 0.0739455, recall 0.865838
2017-12-09T23:21:12.045227: step 1169, loss 0.113723, acc 0.960938, prec 0.0739655, recall 0.865895
2017-12-09T23:21:12.545402: step 1170, loss 0.221786, acc 0.960938, prec 0.0740355, recall 0.866036
2017-12-09T23:21:13.051882: step 1171, loss 0.215745, acc 0.941406, prec 0.0741153, recall 0.866205
2017-12-09T23:21:13.563921: step 1172, loss 0.176507, acc 0.953125, prec 0.0741493, recall 0.866289
2017-12-09T23:21:14.065088: step 1173, loss 0.1073, acc 0.976562, prec 0.0742411, recall 0.866457
2017-12-09T23:21:14.566340: step 1174, loss 0.136765, acc 0.949219, prec 0.074257, recall 0.866513
2017-12-09T23:21:15.073885: step 1175, loss 0.152495, acc 0.957031, prec 0.0742756, recall 0.866569
2017-12-09T23:21:15.576756: step 1176, loss 0.0828397, acc 0.980469, prec 0.0742855, recall 0.866597
2017-12-09T23:21:16.075650: step 1177, loss 0.275798, acc 0.980469, prec 0.0743287, recall 0.866681
2017-12-09T23:21:16.586523: step 1178, loss 0.164963, acc 0.964844, prec 0.0743997, recall 0.86682
2017-12-09T23:21:17.086715: step 1179, loss 0.168425, acc 0.96875, prec 0.0744887, recall 0.866987
2017-12-09T23:21:17.599899: step 1180, loss 0.181687, acc 0.972656, prec 0.0745291, recall 0.86707
2017-12-09T23:21:18.107933: step 1181, loss 0.410872, acc 0.972656, prec 0.0746028, recall 0.867209
2017-12-09T23:21:18.611408: step 1182, loss 0.125168, acc 0.960938, prec 0.0746558, recall 0.867319
2017-12-09T23:21:19.118864: step 1183, loss 0.117113, acc 0.957031, prec 0.0746908, recall 0.867402
2017-12-09T23:21:19.617564: step 1184, loss 1.95237, acc 0.960938, prec 0.0747285, recall 0.867305
2017-12-09T23:21:20.123811: step 1185, loss 0.31897, acc 0.960938, prec 0.0747814, recall 0.867415
2017-12-09T23:21:20.650066: step 1186, loss 0.490969, acc 0.9375, prec 0.074811, recall 0.867317
2017-12-09T23:21:21.155970: step 1187, loss 0.19186, acc 0.933594, prec 0.0748048, recall 0.867345
2017-12-09T23:21:21.657762: step 1188, loss 0.207869, acc 0.921875, prec 0.0747946, recall 0.867372
2017-12-09T23:21:22.163812: step 1189, loss 0.292789, acc 0.898438, prec 0.0748426, recall 0.86751
2017-12-09T23:21:22.681474: step 1190, loss 0.36947, acc 0.871094, prec 0.074848, recall 0.867592
2017-12-09T23:21:23.185239: step 1191, loss 0.308006, acc 0.894531, prec 0.074845, recall 0.867647
2017-12-09T23:21:23.687549: step 1192, loss 0.261027, acc 0.910156, prec 0.0748308, recall 0.867674
2017-12-09T23:21:24.191100: step 1193, loss 0.25244, acc 0.917969, prec 0.0748192, recall 0.867702
2017-12-09T23:21:24.691888: step 1194, loss 0.223015, acc 0.910156, prec 0.0748216, recall 0.867757
2017-12-09T23:21:25.196846: step 1195, loss 0.298436, acc 0.902344, prec 0.0748212, recall 0.867811
2017-12-09T23:21:25.699292: step 1196, loss 0.234075, acc 0.925781, prec 0.0748123, recall 0.867839
2017-12-09T23:21:26.207485: step 1197, loss 0.191062, acc 0.929688, prec 0.0748378, recall 0.867921
2017-12-09T23:21:26.707266: step 1198, loss 0.263245, acc 0.921875, prec 0.0748771, recall 0.86803
2017-12-09T23:21:27.214358: step 1199, loss 0.697269, acc 0.9375, prec 0.0749381, recall 0.868166
2017-12-09T23:21:27.715741: step 1200, loss 0.115652, acc 0.972656, prec 0.0749782, recall 0.868247

Evaluation:
2017-12-09T23:21:32.412501: step 1200, loss 2.61876, acc 0.949807, prec 0.0755563, recall 0.854009

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_0/1512879055/checkpoints/model-1200

2017-12-09T23:21:33.961607: step 1201, loss 0.174082, acc 0.972656, prec 0.0756122, recall 0.854125
2017-12-09T23:21:34.468103: step 1202, loss 0.168818, acc 0.941406, prec 0.0756248, recall 0.854183
2017-12-09T23:21:34.972921: step 1203, loss 0.135825, acc 0.941406, prec 0.0756374, recall 0.854241
2017-12-09T23:21:35.481534: step 1204, loss 0.0721412, acc 0.972656, prec 0.075628, recall 0.854241
2017-12-09T23:21:35.981242: step 1205, loss 0.334005, acc 0.949219, prec 0.0756922, recall 0.854386
2017-12-09T23:21:36.486608: step 1206, loss 0.299818, acc 0.949219, prec 0.07574, recall 0.854502
2017-12-09T23:21:36.984634: step 1207, loss 0.161576, acc 0.933594, prec 0.0757498, recall 0.85456
2017-12-09T23:21:37.487385: step 1208, loss 0.194535, acc 0.957031, prec 0.0757677, recall 0.854618
2017-12-09T23:21:37.990847: step 1209, loss 0.135826, acc 0.96875, prec 0.0758221, recall 0.854733
2017-12-09T23:21:38.497954: step 1210, loss 0.250789, acc 0.972656, prec 0.0759104, recall 0.854906
2017-12-09T23:21:39.002356: step 1211, loss 0.153422, acc 0.96875, prec 0.0759647, recall 0.855021
2017-12-09T23:21:39.504053: step 1212, loss 0.136962, acc 0.964844, prec 0.0760177, recall 0.855136
2017-12-09T23:21:40.019672: step 1213, loss 0.0895083, acc 0.976562, prec 0.0760585, recall 0.855222
2017-12-09T23:21:40.524544: step 1214, loss 0.213354, acc 0.957031, prec 0.0760763, recall 0.855279
2017-12-09T23:21:41.030159: step 1215, loss 0.804226, acc 0.964844, prec 0.0762442, recall 0.855424
2017-12-09T23:21:41.547164: step 1216, loss 0.0607854, acc 0.980469, prec 0.0763187, recall 0.855567
2017-12-09T23:21:42.051381: step 1217, loss 0.16319, acc 0.949219, prec 0.0763013, recall 0.855567
2017-12-09T23:21:42.552817: step 1218, loss 0.225985, acc 0.953125, prec 0.0763825, recall 0.855737
2017-12-09T23:21:43.056326: step 1219, loss 0.135146, acc 0.964844, prec 0.0764029, recall 0.855794
2017-12-09T23:21:43.557692: step 1220, loss 0.221418, acc 0.941406, prec 0.0764477, recall 0.855907
2017-12-09T23:21:44.059864: step 1221, loss 0.253859, acc 0.953125, prec 0.0764478, recall 0.855936
2017-12-09T23:21:44.565509: step 1222, loss 0.336469, acc 0.90625, prec 0.0764966, recall 0.856077
2017-12-09T23:21:45.072551: step 1223, loss 0.525672, acc 0.929688, prec 0.07649, recall 0.855937
2017-12-09T23:21:45.587402: step 1224, loss 0.240079, acc 0.949219, prec 0.0766183, recall 0.856191
2017-12-09T23:21:46.086218: step 1225, loss 0.218908, acc 0.933594, prec 0.076644, recall 0.856276
2017-12-09T23:21:46.588826: step 1226, loss 0.176547, acc 0.921875, prec 0.0766495, recall 0.856332
2017-12-09T23:21:47.095871: step 1227, loss 0.309568, acc 0.917969, prec 0.0767022, recall 0.856472
2017-12-09T23:21:47.594040: step 1228, loss 0.277396, acc 0.929688, prec 0.0767588, recall 0.856613
2017-12-09T23:21:48.097094: step 1229, loss 0.173552, acc 0.953125, prec 0.0768235, recall 0.856753
2017-12-09T23:21:48.594543: step 1230, loss 0.339062, acc 0.933594, prec 0.0768491, recall 0.856836
2017-12-09T23:21:49.098175: step 1231, loss 0.173346, acc 0.941406, prec 0.0768612, recall 0.856892
2017-12-09T23:21:49.600200: step 1232, loss 0.141479, acc 0.976562, prec 0.0769338, recall 0.857032
2017-12-09T23:21:50.106162: step 1233, loss 0.224017, acc 0.953125, prec 0.0769822, recall 0.857143
2017-12-09T23:21:50.622843: step 1234, loss 0.166837, acc 0.9375, prec 0.0769607, recall 0.857143
2017-12-09T23:21:51.124404: step 1235, loss 0.202357, acc 0.957031, prec 0.0769782, recall 0.857198
2017-12-09T23:21:51.634602: step 1236, loss 0.0986636, acc 0.960938, prec 0.0770292, recall 0.857309
2017-12-09T23:21:52.132504: step 1237, loss 0.468845, acc 0.949219, prec 0.0770762, recall 0.85742
2017-12-09T23:21:52.644001: step 1238, loss 0.757192, acc 0.957031, prec 0.0771755, recall 0.857448
2017-12-09T23:21:53.145333: step 1239, loss 0.641151, acc 0.980469, prec 0.0772507, recall 0.85742
2017-12-09T23:21:53.649027: step 1240, loss 0.160005, acc 0.941406, prec 0.0772788, recall 0.857502
2017-12-09T23:21:54.146112: step 1241, loss 0.204419, acc 0.921875, prec 0.0773162, recall 0.857613
2017-12-09T23:21:54.647366: step 1242, loss 0.168158, acc 0.9375, prec 0.0773107, recall 0.85764
2017-12-09T23:21:55.148113: step 1243, loss 0.215111, acc 0.917969, prec 0.0772824, recall 0.85764
2017-12-09T23:21:55.645933: step 1244, loss 0.267441, acc 0.921875, prec 0.0773841, recall 0.85786
2017-12-09T23:21:56.149852: step 1245, loss 0.213633, acc 0.914062, prec 0.0773866, recall 0.857915
2017-12-09T23:21:56.653292: step 1246, loss 0.231335, acc 0.914062, prec 0.0774372, recall 0.858052
2017-12-09T23:21:57.159199: step 1247, loss 0.302066, acc 0.910156, prec 0.0774704, recall 0.858162
2017-12-09T23:21:57.664017: step 1248, loss 0.221598, acc 0.941406, prec 0.0774823, recall 0.858216
2017-12-09T23:21:58.164642: step 1249, loss 0.171293, acc 0.9375, prec 0.0775409, recall 0.858353
2017-12-09T23:21:58.344579: step 1250, loss 0.101139, acc 0.960784, prec 0.0775382, recall 0.858353
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 256
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_256_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719

Start training
2017-12-09T23:22:02.437259: step 1, loss 3.06686, acc 0.539062, prec 0.00854701, recall 0.333333
2017-12-09T23:22:02.941215: step 2, loss 3.68899, acc 0.511719, prec 0.00416667, recall 0.2
2017-12-09T23:22:03.443619: step 3, loss 10.963, acc 0.449219, prec 0.00791557, recall 0.272727
2017-12-09T23:22:03.950572: step 4, loss 4.86739, acc 0.339844, prec 0.00729927, recall 0.307692
2017-12-09T23:22:04.449977: step 5, loss 4.26863, acc 0.257812, prec 0.00810811, recall 0.4
2017-12-09T23:22:04.950006: step 6, loss 5.49371, acc 0.300781, prec 0.00870511, recall 0.421053
2017-12-09T23:22:05.460925: step 7, loss 6.99134, acc 0.292969, prec 0.00727934, recall 0.4
2017-12-09T23:22:05.983289: step 8, loss 2.99416, acc 0.363281, prec 0.00633914, recall 0.4
2017-12-09T23:22:06.483241: step 9, loss 3.774, acc 0.4375, prec 0.00569395, recall 0.380952
2017-12-09T23:22:06.986587: step 10, loss 4.20203, acc 0.589844, prec 0.00859788, recall 0.448276
2017-12-09T23:22:07.489075: step 11, loss 3.31608, acc 0.59375, prec 0.00804954, recall 0.433333
2017-12-09T23:22:07.989985: step 12, loss 3.85601, acc 0.660156, prec 0.00997653, recall 0.472222
2017-12-09T23:22:08.496326: step 13, loss 2.50254, acc 0.695312, prec 0.010101, recall 0.473684
2017-12-09T23:22:09.000195: step 14, loss 7.57741, acc 0.667969, prec 0.011248, recall 0.477273
2017-12-09T23:22:09.514783: step 15, loss 5.23482, acc 0.546875, prec 0.0110943, recall 0.478261
2017-12-09T23:22:10.011399: step 16, loss 5.85445, acc 0.523438, prec 0.0104712, recall 0.44
2017-12-09T23:22:10.513146: step 17, loss 3.48248, acc 0.476562, prec 0.0107335, recall 0.45283
2017-12-09T23:22:11.016400: step 18, loss 3.20988, acc 0.394531, prec 0.0121035, recall 0.5
2017-12-09T23:22:11.519239: step 19, loss 5.1447, acc 0.371094, prec 0.0117325, recall 0.5
2017-12-09T23:22:12.017919: step 20, loss 3.89597, acc 0.277344, prec 0.0113056, recall 0.5
2017-12-09T23:22:12.522447: step 21, loss 3.95413, acc 0.285156, prec 0.0126237, recall 0.544118
2017-12-09T23:22:13.024478: step 22, loss 3.51283, acc 0.320312, prec 0.0125523, recall 0.557143
2017-12-09T23:22:13.527752: step 23, loss 4.10957, acc 0.4375, prec 0.0132145, recall 0.573333
2017-12-09T23:22:14.033755: step 24, loss 2.81892, acc 0.457031, prec 0.0129641, recall 0.578947
2017-12-09T23:22:14.535095: step 25, loss 1.82862, acc 0.5625, prec 0.0133941, recall 0.594937
2017-12-09T23:22:15.041661: step 26, loss 1.80392, acc 0.628906, prec 0.0138619, recall 0.609756
2017-12-09T23:22:15.551118: step 27, loss 3.14003, acc 0.675781, prec 0.0138286, recall 0.593023
2017-12-09T23:22:16.056407: step 28, loss 6.91616, acc 0.734375, prec 0.0138519, recall 0.577778
2017-12-09T23:22:16.563855: step 29, loss 3.86012, acc 0.808594, prec 0.01394, recall 0.569892
2017-12-09T23:22:17.073145: step 30, loss 1.20085, acc 0.710938, prec 0.0136739, recall 0.569892
2017-12-09T23:22:17.571249: step 31, loss 2.88111, acc 0.777344, prec 0.0139878, recall 0.561224
2017-12-09T23:22:18.072516: step 32, loss 3.41231, acc 0.664062, prec 0.0139442, recall 0.54902
2017-12-09T23:22:18.577633: step 33, loss 5.17321, acc 0.652344, prec 0.0148491, recall 0.559633
2017-12-09T23:22:19.076675: step 34, loss 4.43216, acc 0.644531, prec 0.0152381, recall 0.561404
2017-12-09T23:22:19.587042: step 35, loss 3.25792, acc 0.53125, prec 0.0161887, recall 0.57377
2017-12-09T23:22:20.095397: step 36, loss 3.01318, acc 0.472656, prec 0.0168011, recall 0.590551
2017-12-09T23:22:20.617944: step 37, loss 3.05928, acc 0.421875, prec 0.0166883, recall 0.596899
2017-12-09T23:22:21.122236: step 38, loss 3.64757, acc 0.398438, prec 0.0180029, recall 0.623188
2017-12-09T23:22:21.629758: step 39, loss 3.70951, acc 0.339844, prec 0.017785, recall 0.628571
2017-12-09T23:22:22.126641: step 40, loss 3.17381, acc 0.441406, prec 0.0178642, recall 0.636364
2017-12-09T23:22:22.631491: step 41, loss 3.55495, acc 0.429688, prec 0.0181159, recall 0.646259
2017-12-09T23:22:23.134184: step 42, loss 2.5383, acc 0.503906, prec 0.0180532, recall 0.651007
2017-12-09T23:22:23.637936: step 43, loss 5.3565, acc 0.554688, prec 0.0182183, recall 0.653595
2017-12-09T23:22:24.145999: step 44, loss 3.05957, acc 0.617188, prec 0.0182567, recall 0.649682
2017-12-09T23:22:24.653620: step 45, loss 6.09561, acc 0.609375, prec 0.0182905, recall 0.641975
2017-12-09T23:22:25.156141: step 46, loss 6.76667, acc 0.644531, prec 0.0185217, recall 0.636905
2017-12-09T23:22:25.660804: step 47, loss 2.97967, acc 0.652344, prec 0.0187489, recall 0.635838
2017-12-09T23:22:26.162085: step 48, loss 3.66436, acc 0.527344, prec 0.0188648, recall 0.638418
2017-12-09T23:22:26.670198: step 49, loss 4.11996, acc 0.589844, prec 0.0188648, recall 0.638889
2017-12-09T23:22:27.189331: step 50, loss 4.56764, acc 0.496094, prec 0.0189497, recall 0.641304
2017-12-09T23:22:27.699194: step 51, loss 5.93045, acc 0.417969, prec 0.0191283, recall 0.642105
2017-12-09T23:22:28.208538: step 52, loss 3.25468, acc 0.394531, prec 0.0189748, recall 0.645833
2017-12-09T23:22:28.710391: step 53, loss 3.16167, acc 0.429688, prec 0.019294, recall 0.654822
2017-12-09T23:22:29.219234: step 54, loss 3.12175, acc 0.445312, prec 0.0190365, recall 0.656566
2017-12-09T23:22:29.729218: step 55, loss 3.39671, acc 0.441406, prec 0.0193493, recall 0.665025
2017-12-09T23:22:30.241065: step 56, loss 2.96632, acc 0.46875, prec 0.0191172, recall 0.666667
2017-12-09T23:22:30.742542: step 57, loss 6.3702, acc 0.578125, prec 0.0191083, recall 0.663462
2017-12-09T23:22:31.247646: step 58, loss 2.50483, acc 0.597656, prec 0.0193777, recall 0.666667
2017-12-09T23:22:31.755798: step 59, loss 2.01361, acc 0.585938, prec 0.0192334, recall 0.668224
2017-12-09T23:22:32.269445: step 60, loss 5.27416, acc 0.636719, prec 0.0196495, recall 0.672727
2017-12-09T23:22:32.774661: step 61, loss 2.72889, acc 0.695312, prec 0.0198371, recall 0.674107
2017-12-09T23:22:33.276624: step 62, loss 1.80798, acc 0.703125, prec 0.0197711, recall 0.672566
2017-12-09T23:22:33.779557: step 63, loss 1.28924, acc 0.742188, prec 0.019982, recall 0.676856
2017-12-09T23:22:34.283917: step 64, loss 6.95973, acc 0.761719, prec 0.0198337, recall 0.668103
2017-12-09T23:22:34.796197: step 65, loss 9.2438, acc 0.769531, prec 0.0200711, recall 0.658333
2017-12-09T23:22:35.302127: step 66, loss 5.82471, acc 0.710938, prec 0.0200176, recall 0.64898
2017-12-09T23:22:35.822348: step 67, loss 2.44574, acc 0.636719, prec 0.0200323, recall 0.649194
2017-12-09T23:22:36.336922: step 68, loss 3.13814, acc 0.609375, prec 0.0202703, recall 0.652174
2017-12-09T23:22:36.848399: step 69, loss 10.0452, acc 0.535156, prec 0.0204625, recall 0.64751
2017-12-09T23:22:37.359338: step 70, loss 3.22974, acc 0.460938, prec 0.0203596, recall 0.65019
2017-12-09T23:22:37.867550: step 71, loss 4.58968, acc 0.328125, prec 0.0204082, recall 0.655431
2017-12-09T23:22:38.366998: step 72, loss 4.65426, acc 0.257812, prec 0.0201893, recall 0.657993
2017-12-09T23:22:38.868499: step 73, loss 4.68707, acc 0.261719, prec 0.0199821, recall 0.660517
2017-12-09T23:22:39.387701: step 74, loss 3.91792, acc 0.285156, prec 0.0200109, recall 0.665455
2017-12-09T23:22:39.901380: step 75, loss 4.17208, acc 0.3125, prec 0.0199485, recall 0.669065
2017-12-09T23:22:40.412052: step 76, loss 5.19227, acc 0.386719, prec 0.0199304, recall 0.670213
2017-12-09T23:22:40.911997: step 77, loss 2.92032, acc 0.449219, prec 0.0198442, recall 0.670175
2017-12-09T23:22:41.414000: step 78, loss 2.59976, acc 0.472656, prec 0.0197705, recall 0.672474
2017-12-09T23:22:41.918985: step 79, loss 1.86767, acc 0.625, prec 0.020075, recall 0.678082
2017-12-09T23:22:42.422105: step 80, loss 1.83349, acc 0.617188, prec 0.0200743, recall 0.680272
2017-12-09T23:22:42.921321: step 81, loss 6.21414, acc 0.703125, prec 0.0204183, recall 0.676568
2017-12-09T23:22:43.433364: step 82, loss 3.66964, acc 0.703125, prec 0.0202689, recall 0.672131
2017-12-09T23:22:43.941242: step 83, loss 2.11111, acc 0.726562, prec 0.0204202, recall 0.673139
2017-12-09T23:22:44.449840: step 84, loss 1.83121, acc 0.707031, prec 0.0203684, recall 0.672026
2017-12-09T23:22:44.949211: step 85, loss 5.35843, acc 0.742188, prec 0.0204299, recall 0.671975
2017-12-09T23:22:45.447659: step 86, loss 4.62631, acc 0.644531, prec 0.0205374, recall 0.670846
2017-12-09T23:22:45.960870: step 87, loss 3.37829, acc 0.761719, prec 0.0207956, recall 0.670769
2017-12-09T23:22:46.459908: step 88, loss 1.68203, acc 0.679688, prec 0.0206361, recall 0.668712
2017-12-09T23:22:46.961858: step 89, loss 4.20903, acc 0.648438, prec 0.0204637, recall 0.666667
2017-12-09T23:22:47.471385: step 90, loss 1.82464, acc 0.585938, prec 0.0205352, recall 0.669697
2017-12-09T23:22:47.975986: step 91, loss 3.67928, acc 0.597656, prec 0.0207931, recall 0.672619
2017-12-09T23:22:48.477182: step 92, loss 1.74768, acc 0.648438, prec 0.0208903, recall 0.675516
2017-12-09T23:22:48.978841: step 93, loss 4.05877, acc 0.570312, prec 0.0207731, recall 0.674487
2017-12-09T23:22:49.484576: step 94, loss 1.84032, acc 0.582031, prec 0.0205743, recall 0.674487
2017-12-09T23:22:49.983245: step 95, loss 2.09509, acc 0.558594, prec 0.0206286, recall 0.677326
2017-12-09T23:22:50.488033: step 96, loss 1.71489, acc 0.589844, prec 0.0207822, recall 0.681035
2017-12-09T23:22:51.001987: step 97, loss 1.92621, acc 0.601562, prec 0.0208532, recall 0.683761
2017-12-09T23:22:51.508952: step 98, loss 1.61335, acc 0.707031, prec 0.0209718, recall 0.686441
2017-12-09T23:22:52.011884: step 99, loss 1.43686, acc 0.714844, prec 0.0211781, recall 0.688022
2017-12-09T23:22:52.521769: step 100, loss 8.37236, acc 0.683594, prec 0.0214541, recall 0.686648
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-100

2017-12-09T23:22:54.018276: step 101, loss 1.72977, acc 0.710938, prec 0.0213216, recall 0.684783
2017-12-09T23:22:54.521449: step 102, loss 2.31432, acc 0.777344, prec 0.0215506, recall 0.686327
2017-12-09T23:22:55.029262: step 103, loss 2.30097, acc 0.734375, prec 0.0215135, recall 0.683511
2017-12-09T23:22:55.537516: step 104, loss 1.86218, acc 0.703125, prec 0.021542, recall 0.683377
2017-12-09T23:22:56.046966: step 105, loss 5.20865, acc 0.714844, prec 0.0216601, recall 0.680519
2017-12-09T23:22:56.558539: step 106, loss 1.79323, acc 0.601562, prec 0.0217997, recall 0.683805
2017-12-09T23:22:57.069052: step 107, loss 1.67085, acc 0.582031, prec 0.0218486, recall 0.686224
2017-12-09T23:22:57.571084: step 108, loss 3.74028, acc 0.535156, prec 0.0221918, recall 0.69
2017-12-09T23:22:58.085279: step 109, loss 2.53545, acc 0.53125, prec 0.0222151, recall 0.690594
2017-12-09T23:22:58.589282: step 110, loss 2.06319, acc 0.492188, prec 0.0222187, recall 0.692875
2017-12-09T23:22:59.092512: step 111, loss 5.55603, acc 0.527344, prec 0.022243, recall 0.690073
2017-12-09T23:22:59.593894: step 112, loss 2.37232, acc 0.535156, prec 0.0224163, recall 0.69378
2017-12-09T23:23:00.103965: step 113, loss 2.40223, acc 0.515625, prec 0.0225777, recall 0.6974
2017-12-09T23:23:00.613023: step 114, loss 2.39856, acc 0.519531, prec 0.0225894, recall 0.699531
2017-12-09T23:23:01.118491: step 115, loss 2.39626, acc 0.550781, prec 0.0224694, recall 0.698598
2017-12-09T23:23:01.628141: step 116, loss 1.86593, acc 0.566406, prec 0.022502, recall 0.700696
2017-12-09T23:23:02.131353: step 117, loss 2.35904, acc 0.570312, prec 0.022393, recall 0.699769
2017-12-09T23:23:02.632580: step 118, loss 2.73129, acc 0.675781, prec 0.0224018, recall 0.699541
2017-12-09T23:23:03.137704: step 119, loss 6.44058, acc 0.683594, prec 0.0224169, recall 0.696145
2017-12-09T23:23:03.641368: step 120, loss 4.37683, acc 0.691406, prec 0.0223626, recall 0.693694
2017-12-09T23:23:04.147316: step 121, loss 1.18663, acc 0.722656, prec 0.0225303, recall 0.696429
2017-12-09T23:23:04.647099: step 122, loss 2.0249, acc 0.671875, prec 0.0227468, recall 0.698238
2017-12-09T23:23:05.150060: step 123, loss 6.6075, acc 0.699219, prec 0.0228343, recall 0.697168
2017-12-09T23:23:05.665723: step 124, loss 1.64886, acc 0.625, prec 0.0231636, recall 0.701717
2017-12-09T23:23:05.841074: step 125, loss 6.81957, acc 0.588235, prec 0.0231308, recall 0.700214
2017-12-09T23:23:06.347226: step 126, loss 3.33037, acc 0.523438, prec 0.0229361, recall 0.697228
2017-12-09T23:23:06.848376: step 127, loss 2.40152, acc 0.523438, prec 0.0228095, recall 0.697872
2017-12-09T23:23:07.363795: step 128, loss 3.52227, acc 0.433594, prec 0.022854, recall 0.697479
2017-12-09T23:23:07.871459: step 129, loss 2.5449, acc 0.453125, prec 0.0227691, recall 0.698745
2017-12-09T23:23:08.372076: step 130, loss 2.9951, acc 0.476562, prec 0.022695, recall 0.7
2017-12-09T23:23:08.877019: step 131, loss 2.94271, acc 0.410156, prec 0.0231191, recall 0.706122
2017-12-09T23:23:09.381252: step 132, loss 2.78334, acc 0.429688, prec 0.0229604, recall 0.706721
2017-12-09T23:23:09.886444: step 133, loss 2.67682, acc 0.480469, prec 0.0230164, recall 0.709091
2017-12-09T23:23:10.394945: step 134, loss 1.73225, acc 0.578125, prec 0.0231726, recall 0.712
2017-12-09T23:23:10.901634: step 135, loss 1.73939, acc 0.570312, prec 0.0231972, recall 0.713718
2017-12-09T23:23:11.401697: step 136, loss 1.33707, acc 0.652344, prec 0.0233783, recall 0.716535
2017-12-09T23:23:11.914324: step 137, loss 1.24448, acc 0.703125, prec 0.023452, recall 0.7182
2017-12-09T23:23:12.419151: step 138, loss 2.58093, acc 0.707031, prec 0.0235279, recall 0.718447
2017-12-09T23:23:12.918781: step 139, loss 3.28943, acc 0.835938, prec 0.0235921, recall 0.716763
2017-12-09T23:23:13.427543: step 140, loss 0.644706, acc 0.847656, prec 0.0237809, recall 0.718929
2017-12-09T23:23:13.928943: step 141, loss 4.44832, acc 0.839844, prec 0.0237239, recall 0.714829
2017-12-09T23:23:14.430954: step 142, loss 0.465963, acc 0.835938, prec 0.0237226, recall 0.71537
2017-12-09T23:23:14.934188: step 143, loss 2.27768, acc 0.878906, prec 0.023802, recall 0.713748
2017-12-09T23:23:15.446129: step 144, loss 0.710952, acc 0.8125, prec 0.0237916, recall 0.714286
2017-12-09T23:23:15.954402: step 145, loss 2.60616, acc 0.800781, prec 0.0237798, recall 0.71215
2017-12-09T23:23:16.459460: step 146, loss 1.53843, acc 0.777344, prec 0.0237577, recall 0.711359
2017-12-09T23:23:16.972692: step 147, loss 3.40709, acc 0.75, prec 0.0237859, recall 0.711111
2017-12-09T23:23:17.491813: step 148, loss 1.02815, acc 0.730469, prec 0.0239857, recall 0.713761
2017-12-09T23:23:17.994745: step 149, loss 1.40531, acc 0.707031, prec 0.0241149, recall 0.715847
2017-12-09T23:23:18.512781: step 150, loss 1.16894, acc 0.703125, prec 0.0243009, recall 0.718412
2017-12-09T23:23:19.020747: step 151, loss 1.8189, acc 0.671875, prec 0.0247101, recall 0.722913
2017-12-09T23:23:19.520217: step 152, loss 1.1327, acc 0.65625, prec 0.0246377, recall 0.723404
2017-12-09T23:23:20.017362: step 153, loss 1.69093, acc 0.628906, prec 0.0246143, recall 0.724382
2017-12-09T23:23:20.533945: step 154, loss 1.02385, acc 0.675781, prec 0.0247253, recall 0.726316
2017-12-09T23:23:21.048204: step 155, loss 1.60128, acc 0.734375, prec 0.0249167, recall 0.727431
2017-12-09T23:23:21.549197: step 156, loss 7.33099, acc 0.691406, prec 0.0251494, recall 0.72774
2017-12-09T23:23:22.051350: step 157, loss 1.92276, acc 0.65625, prec 0.0252501, recall 0.728353
2017-12-09T23:23:22.553084: step 158, loss 1.62085, acc 0.632812, prec 0.0252268, recall 0.728041
2017-12-09T23:23:23.055389: step 159, loss 1.25215, acc 0.707031, prec 0.0254005, recall 0.730318
2017-12-09T23:23:23.562246: step 160, loss 1.19388, acc 0.679688, prec 0.0253363, recall 0.730769
2017-12-09T23:23:24.080307: step 161, loss 2.11721, acc 0.679688, prec 0.0254428, recall 0.731343
2017-12-09T23:23:24.587339: step 162, loss 1.45712, acc 0.636719, prec 0.0255307, recall 0.733114
2017-12-09T23:23:25.096343: step 163, loss 2.95899, acc 0.695312, prec 0.0254741, recall 0.732348
2017-12-09T23:23:25.606839: step 164, loss 1.03954, acc 0.667969, prec 0.0256279, recall 0.734528
2017-12-09T23:23:26.105574: step 165, loss 1.21764, acc 0.703125, prec 0.0256831, recall 0.735819
2017-12-09T23:23:26.615223: step 166, loss 1.04185, acc 0.710938, prec 0.0256858, recall 0.736672
2017-12-09T23:23:27.112735: step 167, loss 2.55752, acc 0.753906, prec 0.0257058, recall 0.736334
2017-12-09T23:23:27.620703: step 168, loss 0.570702, acc 0.820312, prec 0.0257487, recall 0.737179
2017-12-09T23:23:28.127142: step 169, loss 2.17911, acc 0.804688, prec 0.0259516, recall 0.736926
2017-12-09T23:23:28.628904: step 170, loss 2.4652, acc 0.785156, prec 0.025982, recall 0.736593
2017-12-09T23:23:29.134654: step 171, loss 0.797611, acc 0.824219, prec 0.0260792, recall 0.737834
2017-12-09T23:23:29.648130: step 172, loss 0.595119, acc 0.808594, prec 0.0260624, recall 0.738245
2017-12-09T23:23:30.157934: step 173, loss 0.742295, acc 0.808594, prec 0.0260457, recall 0.738654
2017-12-09T23:23:30.678599: step 174, loss 1.78398, acc 0.757812, prec 0.0259597, recall 0.736349
2017-12-09T23:23:31.190763: step 175, loss 1.58787, acc 0.785156, prec 0.0262033, recall 0.737654
2017-12-09T23:23:31.698462: step 176, loss 3.20596, acc 0.777344, prec 0.0261792, recall 0.734663
2017-12-09T23:23:32.213133: step 177, loss 1.28541, acc 0.6875, prec 0.0261726, recall 0.734351
2017-12-09T23:23:32.720843: step 178, loss 2.1971, acc 0.746094, prec 0.0262402, recall 0.734446
2017-12-09T23:23:33.228762: step 179, loss 1.28857, acc 0.644531, prec 0.0263215, recall 0.736048
2017-12-09T23:23:33.732853: step 180, loss 1.23586, acc 0.652344, prec 0.0264048, recall 0.737631
2017-12-09T23:23:34.236961: step 181, loss 1.66817, acc 0.671875, prec 0.0263917, recall 0.737313
2017-12-09T23:23:34.740980: step 182, loss 1.42628, acc 0.632812, prec 0.0265703, recall 0.739645
2017-12-09T23:23:35.237683: step 183, loss 1.23647, acc 0.699219, prec 0.0267196, recall 0.741557
2017-12-09T23:23:35.757743: step 184, loss 1.45665, acc 0.601562, prec 0.0266274, recall 0.741935
2017-12-09T23:23:36.268461: step 185, loss 1.23823, acc 0.707031, prec 0.0266758, recall 0.743066
2017-12-09T23:23:36.780806: step 186, loss 1.76078, acc 0.726562, prec 0.0267335, recall 0.742029
2017-12-09T23:23:37.291151: step 187, loss 2.92677, acc 0.730469, prec 0.0266937, recall 0.738129
2017-12-09T23:23:37.796114: step 188, loss 1.03998, acc 0.722656, prec 0.0267468, recall 0.739255
2017-12-09T23:23:38.309562: step 189, loss 1.38734, acc 0.71875, prec 0.0269489, recall 0.741477
2017-12-09T23:23:38.818282: step 190, loss 0.956041, acc 0.703125, prec 0.0269937, recall 0.742574
2017-12-09T23:23:39.327117: step 191, loss 1.12357, acc 0.726562, prec 0.0270464, recall 0.743662
2017-12-09T23:23:39.845000: step 192, loss 1.68313, acc 0.707031, prec 0.0272421, recall 0.74477
2017-12-09T23:23:40.344642: step 193, loss 1.04984, acc 0.777344, prec 0.0273116, recall 0.745833
2017-12-09T23:23:40.848709: step 194, loss 2.79695, acc 0.769531, prec 0.0274299, recall 0.745179
2017-12-09T23:23:41.360200: step 195, loss 1.81734, acc 0.773438, prec 0.0274492, recall 0.744856
2017-12-09T23:23:41.867927: step 196, loss 0.886376, acc 0.710938, prec 0.0274938, recall 0.745902
2017-12-09T23:23:42.382455: step 197, loss 3.41797, acc 0.707031, prec 0.0276844, recall 0.746955
2017-12-09T23:23:42.889615: step 198, loss 0.965111, acc 0.773438, prec 0.02775, recall 0.747978
2017-12-09T23:23:43.394076: step 199, loss 1.66708, acc 0.675781, prec 0.0278787, recall 0.748663
2017-12-09T23:23:43.909170: step 200, loss 1.22571, acc 0.671875, prec 0.0279072, recall 0.749667
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-200

2017-12-09T23:23:45.359892: step 201, loss 1.07646, acc 0.699219, prec 0.0279451, recall 0.750663
2017-12-09T23:23:45.865435: step 202, loss 1.89738, acc 0.746094, prec 0.0280006, recall 0.75066
2017-12-09T23:23:46.377467: step 203, loss 0.861144, acc 0.738281, prec 0.0279563, recall 0.750988
2017-12-09T23:23:46.882459: step 204, loss 0.712262, acc 0.796875, prec 0.0279327, recall 0.751316
2017-12-09T23:23:47.387829: step 205, loss 2.42953, acc 0.777344, prec 0.028, recall 0.750327
2017-12-09T23:23:47.888924: step 206, loss 1.06542, acc 0.808594, prec 0.0280292, recall 0.75
2017-12-09T23:23:48.397652: step 207, loss 0.75203, acc 0.800781, prec 0.0281013, recall 0.750973
2017-12-09T23:23:48.911839: step 208, loss 1.31581, acc 0.785156, prec 0.0280749, recall 0.750323
2017-12-09T23:23:49.425696: step 209, loss 1.2294, acc 0.820312, prec 0.0282486, recall 0.750963
2017-12-09T23:23:49.927644: step 210, loss 1.67415, acc 0.734375, prec 0.0282978, recall 0.750958
2017-12-09T23:23:50.433823: step 211, loss 0.755658, acc 0.804688, prec 0.0282765, recall 0.751276
2017-12-09T23:23:50.944474: step 212, loss 3.01722, acc 0.761719, prec 0.028291, recall 0.749049
2017-12-09T23:23:51.447339: step 213, loss 2.3819, acc 0.785156, prec 0.0283109, recall 0.748737
2017-12-09T23:23:51.946998: step 214, loss 3.58186, acc 0.738281, prec 0.0284083, recall 0.74812
2017-12-09T23:23:52.452524: step 215, loss 1.299, acc 0.726562, prec 0.0285443, recall 0.749689
2017-12-09T23:23:52.964506: step 216, loss 1.78659, acc 0.683594, prec 0.0284365, recall 0.748756
2017-12-09T23:23:53.467937: step 217, loss 2.33737, acc 0.617188, prec 0.0284438, recall 0.748762
2017-12-09T23:23:53.978361: step 218, loss 2.4702, acc 0.589844, prec 0.0285327, recall 0.749386
2017-12-09T23:23:54.485803: step 219, loss 3.70563, acc 0.59375, prec 0.028622, recall 0.75
2017-12-09T23:23:54.998733: step 220, loss 2.2004, acc 0.578125, prec 0.0285688, recall 0.750608
2017-12-09T23:23:55.510105: step 221, loss 2.34903, acc 0.488281, prec 0.0285307, recall 0.751515
2017-12-09T23:23:56.009274: step 222, loss 2.15418, acc 0.523438, prec 0.0286825, recall 0.753606
2017-12-09T23:23:56.514879: step 223, loss 1.88117, acc 0.554688, prec 0.0285779, recall 0.753902
2017-12-09T23:23:57.017777: step 224, loss 1.97635, acc 0.542969, prec 0.0285145, recall 0.754491
2017-12-09T23:23:57.524781: step 225, loss 2.9298, acc 0.65625, prec 0.0286217, recall 0.755054
2017-12-09T23:23:58.027220: step 226, loss 1.95952, acc 0.617188, prec 0.0285842, recall 0.754739
2017-12-09T23:23:58.533167: step 227, loss 3.85028, acc 0.648438, prec 0.0287323, recall 0.754695
2017-12-09T23:23:59.041930: step 228, loss 1.23106, acc 0.671875, prec 0.0286681, recall 0.754982
2017-12-09T23:23:59.549807: step 229, loss 0.993735, acc 0.710938, prec 0.0286171, recall 0.755269
2017-12-09T23:24:00.058734: step 230, loss 0.885733, acc 0.765625, prec 0.0285841, recall 0.755556
2017-12-09T23:24:00.581275: step 231, loss 1.66645, acc 0.792969, prec 0.02869, recall 0.755814
2017-12-09T23:24:01.088132: step 232, loss 1.24751, acc 0.75, prec 0.0286959, recall 0.755504
2017-12-09T23:24:01.589399: step 233, loss 1.82226, acc 0.839844, prec 0.0287308, recall 0.755196
2017-12-09T23:24:02.094500: step 234, loss 1.16328, acc 0.777344, prec 0.0289995, recall 0.757437
2017-12-09T23:24:02.603215: step 235, loss 0.546938, acc 0.828125, prec 0.0289861, recall 0.757714
2017-12-09T23:24:03.115496: step 236, loss 1.90358, acc 0.824219, prec 0.0291, recall 0.757955
2017-12-09T23:24:03.627750: step 237, loss 2.60544, acc 0.835938, prec 0.029048, recall 0.757094
2017-12-09T23:24:04.130809: step 238, loss 3.44024, acc 0.785156, prec 0.0290233, recall 0.755656
2017-12-09T23:24:04.634602: step 239, loss 1.08929, acc 0.769531, prec 0.0290345, recall 0.755355
2017-12-09T23:24:05.142458: step 240, loss 1.70884, acc 0.726562, prec 0.0292415, recall 0.756425
2017-12-09T23:24:05.659413: step 241, loss 4.86095, acc 0.722656, prec 0.0292801, recall 0.755556
2017-12-09T23:24:06.164048: step 242, loss 0.989783, acc 0.730469, prec 0.0292767, recall 0.756098
2017-12-09T23:24:06.667999: step 243, loss 1.11261, acc 0.703125, prec 0.0293061, recall 0.756906
2017-12-09T23:24:07.169989: step 244, loss 1.2701, acc 0.714844, prec 0.0294632, recall 0.758507
2017-12-09T23:24:07.678211: step 245, loss 1.31009, acc 0.660156, prec 0.0296428, recall 0.760349
2017-12-09T23:24:08.175492: step 246, loss 1.67976, acc 0.613281, prec 0.0296418, recall 0.761129
2017-12-09T23:24:08.680987: step 247, loss 1.68358, acc 0.710938, prec 0.0296733, recall 0.761081
2017-12-09T23:24:09.182042: step 248, loss 1.33543, acc 0.691406, prec 0.0298601, recall 0.762876
2017-12-09T23:24:09.694720: step 249, loss 1.19509, acc 0.648438, prec 0.0297883, recall 0.76313
2017-12-09T23:24:09.871839: step 250, loss 2.0671, acc 0.666667, prec 0.0298077, recall 0.763383
2017-12-09T23:24:10.385574: step 251, loss 1.11048, acc 0.679688, prec 0.0298675, recall 0.764392
2017-12-09T23:24:10.883755: step 252, loss 0.896979, acc 0.730469, prec 0.0299834, recall 0.765642
2017-12-09T23:24:11.384164: step 253, loss 0.833204, acc 0.714844, prec 0.0300534, recall 0.766631
2017-12-09T23:24:11.892691: step 254, loss 0.853635, acc 0.757812, prec 0.0301366, recall 0.767613
2017-12-09T23:24:12.407953: step 255, loss 2.4997, acc 0.773438, prec 0.0302257, recall 0.767782
2017-12-09T23:24:12.918566: step 256, loss 0.777064, acc 0.789062, prec 0.0303976, recall 0.769231
2017-12-09T23:24:13.418793: step 257, loss 1.1277, acc 0.785156, prec 0.0304881, recall 0.770186
2017-12-09T23:24:13.925167: step 258, loss 1.00053, acc 0.765625, prec 0.0305334, recall 0.770103
2017-12-09T23:24:14.433978: step 259, loss 0.546321, acc 0.828125, prec 0.0305577, recall 0.770576
2017-12-09T23:24:14.943319: step 260, loss 0.510798, acc 0.839844, prec 0.0306251, recall 0.771282
2017-12-09T23:24:15.444268: step 261, loss 3.09209, acc 0.832031, prec 0.0306529, recall 0.770174
2017-12-09T23:24:15.943252: step 262, loss 3.16337, acc 0.851562, prec 0.0307249, recall 0.770092
2017-12-09T23:24:16.454020: step 263, loss 8.64577, acc 0.800781, prec 0.0307057, recall 0.767206
2017-12-09T23:24:16.964235: step 264, loss 1.0198, acc 0.75, prec 0.0308221, recall 0.768379
2017-12-09T23:24:17.475920: step 265, loss 1.3132, acc 0.667969, prec 0.0307946, recall 0.768844
2017-12-09T23:24:17.986330: step 266, loss 1.38226, acc 0.6875, prec 0.0308902, recall 0.77
2017-12-09T23:24:18.492003: step 267, loss 2.07154, acc 0.636719, prec 0.0309315, recall 0.770149
2017-12-09T23:24:18.993526: step 268, loss 1.80371, acc 0.605469, prec 0.0308856, recall 0.769841
2017-12-09T23:24:19.499959: step 269, loss 2.19273, acc 0.574219, prec 0.0308686, recall 0.769763
2017-12-09T23:24:20.002961: step 270, loss 1.38577, acc 0.613281, prec 0.0309392, recall 0.770895
2017-12-09T23:24:20.514004: step 271, loss 1.6962, acc 0.542969, prec 0.0309493, recall 0.771792
2017-12-09T23:24:21.018594: step 272, loss 1.46696, acc 0.597656, prec 0.0309004, recall 0.772238
2017-12-09T23:24:21.527303: step 273, loss 1.41265, acc 0.582031, prec 0.0311113, recall 0.774225
2017-12-09T23:24:22.029549: step 274, loss 1.40832, acc 0.613281, prec 0.0311046, recall 0.774879
2017-12-09T23:24:22.528883: step 275, loss 1.77909, acc 0.714844, prec 0.0311678, recall 0.775
2017-12-09T23:24:23.031632: step 276, loss 0.768377, acc 0.734375, prec 0.0311235, recall 0.775216
2017-12-09T23:24:23.543304: step 277, loss 0.768121, acc 0.757812, prec 0.0311238, recall 0.775647
2017-12-09T23:24:24.048584: step 278, loss 0.575207, acc 0.832031, prec 0.0312584, recall 0.776718
2017-12-09T23:24:24.549425: step 279, loss 2.26123, acc 0.792969, prec 0.0312704, recall 0.776403
2017-12-09T23:24:25.057312: step 280, loss 2.79952, acc 0.839844, prec 0.0312978, recall 0.775355
2017-12-09T23:24:25.561647: step 281, loss 3.45151, acc 0.84375, prec 0.0313276, recall 0.773585
2017-12-09T23:24:26.075546: step 282, loss 0.812017, acc 0.859375, prec 0.0314324, recall 0.774436
2017-12-09T23:24:26.578137: step 283, loss 1.48638, acc 0.789062, prec 0.0316651, recall 0.774674
2017-12-09T23:24:27.081961: step 284, loss 1.00746, acc 0.769531, prec 0.0316309, recall 0.774884
2017-12-09T23:24:27.587788: step 285, loss 0.981879, acc 0.71875, prec 0.0316547, recall 0.77551
2017-12-09T23:24:28.096525: step 286, loss 0.96135, acc 0.726562, prec 0.0316441, recall 0.775926
2017-12-09T23:24:28.608270: step 287, loss 0.93131, acc 0.707031, prec 0.0316277, recall 0.77634
2017-12-09T23:24:29.109873: step 288, loss 1.41938, acc 0.667969, prec 0.0316358, recall 0.776959
2017-12-09T23:24:29.616128: step 289, loss 1.01194, acc 0.695312, prec 0.031616, recall 0.777369
2017-12-09T23:24:30.130097: step 290, loss 0.854852, acc 0.765625, prec 0.0316175, recall 0.777778
2017-12-09T23:24:30.638648: step 291, loss 0.897033, acc 0.742188, prec 0.0316119, recall 0.778185
2017-12-09T23:24:31.140861: step 292, loss 0.815639, acc 0.769531, prec 0.0315786, recall 0.778388
2017-12-09T23:24:31.643435: step 293, loss 0.877833, acc 0.773438, prec 0.031726, recall 0.779599
2017-12-09T23:24:32.149190: step 294, loss 0.805325, acc 0.820312, prec 0.0317079, recall 0.7798
2017-12-09T23:24:32.654849: step 295, loss 0.517879, acc 0.824219, prec 0.0317267, recall 0.7802
2017-12-09T23:24:33.156706: step 296, loss 0.692615, acc 0.867188, prec 0.0317583, recall 0.780598
2017-12-09T23:24:33.675604: step 297, loss 3.04003, acc 0.847656, prec 0.031822, recall 0.779783
2017-12-09T23:24:34.193472: step 298, loss 0.559393, acc 0.878906, prec 0.032035, recall 0.781166
2017-12-09T23:24:34.695295: step 299, loss 0.607947, acc 0.859375, prec 0.0321349, recall 0.781948
2017-12-09T23:24:35.200349: step 300, loss 2.13632, acc 0.824219, prec 0.0321185, recall 0.781445

Evaluation:
2017-12-09T23:24:39.878659: step 300, loss 1.53798, acc 0.895651, prec 0.0340689, recall 0.761006

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-300

2017-12-09T23:24:41.367850: step 301, loss 0.688751, acc 0.898438, prec 0.0341748, recall 0.761159
2017-12-09T23:24:41.867629: step 302, loss 0.447184, acc 0.839844, prec 0.0342612, recall 0.761905
2017-12-09T23:24:42.365610: step 303, loss 0.65405, acc 0.828125, prec 0.0343099, recall 0.762461
2017-12-09T23:24:42.875000: step 304, loss 0.491228, acc 0.859375, prec 0.0343681, recall 0.763015
2017-12-09T23:24:43.383923: step 305, loss 0.965159, acc 0.835938, prec 0.0344526, recall 0.763749
2017-12-09T23:24:43.889218: step 306, loss 1.86726, acc 0.800781, prec 0.0345621, recall 0.763482
2017-12-09T23:24:44.396511: step 307, loss 1.6794, acc 0.8125, prec 0.034676, recall 0.762634
2017-12-09T23:24:44.898400: step 308, loss 0.881147, acc 0.800781, prec 0.03475, recall 0.762776
2017-12-09T23:24:45.397677: step 309, loss 2.10785, acc 0.726562, prec 0.0348688, recall 0.762699
2017-12-09T23:24:45.902487: step 310, loss 1.01725, acc 0.707031, prec 0.0348785, recall 0.763238
2017-12-09T23:24:46.405846: step 311, loss 1.13492, acc 0.617188, prec 0.0348937, recall 0.763952
2017-12-09T23:24:46.913901: step 312, loss 1.29043, acc 0.65625, prec 0.0348546, recall 0.764307
2017-12-09T23:24:47.417245: step 313, loss 1.43909, acc 0.582031, prec 0.034793, recall 0.764662
2017-12-09T23:24:47.920168: step 314, loss 1.19727, acc 0.65625, prec 0.0348532, recall 0.765543
2017-12-09T23:24:48.421354: step 315, loss 1.24655, acc 0.59375, prec 0.0348284, recall 0.766069
2017-12-09T23:24:48.930072: step 316, loss 2.58781, acc 0.660156, prec 0.0348577, recall 0.766195
2017-12-09T23:24:49.441007: step 317, loss 1.03242, acc 0.667969, prec 0.0347902, recall 0.766369
2017-12-09T23:24:49.939702: step 318, loss 0.755539, acc 0.746094, prec 0.0347466, recall 0.766543
2017-12-09T23:24:50.446642: step 319, loss 0.816106, acc 0.710938, prec 0.0346926, recall 0.766716
2017-12-09T23:24:50.955356: step 320, loss 0.858851, acc 0.8125, prec 0.034735, recall 0.766667
2017-12-09T23:24:51.453784: step 321, loss 0.620402, acc 0.8125, prec 0.0347439, recall 0.767012
2017-12-09T23:24:51.957184: step 322, loss 0.529973, acc 0.867188, prec 0.0348335, recall 0.767699
2017-12-09T23:24:52.460926: step 323, loss 0.316031, acc 0.878906, prec 0.0348297, recall 0.76787
2017-12-09T23:24:52.968793: step 324, loss 0.867986, acc 0.894531, prec 0.0348961, recall 0.767818
2017-12-09T23:24:53.484341: step 325, loss 0.476466, acc 0.933594, prec 0.0350052, recall 0.768498
2017-12-09T23:24:53.995760: step 326, loss 2.25054, acc 0.894531, prec 0.0350082, recall 0.767544
2017-12-09T23:24:54.505058: step 327, loss 2.35842, acc 0.898438, prec 0.0351076, recall 0.767662
2017-12-09T23:24:55.019077: step 328, loss 1.93295, acc 0.929688, prec 0.0352495, recall 0.767391
2017-12-09T23:24:55.526988: step 329, loss 3.9234, acc 0.875, prec 0.0353747, recall 0.767123
2017-12-09T23:24:56.030078: step 330, loss 0.498406, acc 0.820312, prec 0.0353847, recall 0.767459
2017-12-09T23:24:56.535424: step 331, loss 1.40331, acc 0.800781, prec 0.0354539, recall 0.767575
2017-12-09T23:24:57.041241: step 332, loss 0.757111, acc 0.75, prec 0.0355384, recall 0.768406
2017-12-09T23:24:57.546551: step 333, loss 0.897224, acc 0.707031, prec 0.0355459, recall 0.768902
2017-12-09T23:24:58.043460: step 334, loss 1.27164, acc 0.621094, prec 0.035496, recall 0.769231
2017-12-09T23:24:58.550360: step 335, loss 1.57224, acc 0.667969, prec 0.0354298, recall 0.768848
2017-12-09T23:24:59.054846: step 336, loss 0.939933, acc 0.6875, prec 0.0354633, recall 0.769504
2017-12-09T23:24:59.560779: step 337, loss 1.39735, acc 0.621094, prec 0.0355084, recall 0.770318
2017-12-09T23:25:00.072142: step 338, loss 1.15791, acc 0.667969, prec 0.035473, recall 0.770642
2017-12-09T23:25:00.590571: step 339, loss 1.21489, acc 0.703125, prec 0.035543, recall 0.770907
2017-12-09T23:25:01.097080: step 340, loss 0.978063, acc 0.695312, prec 0.0355781, recall 0.771549
2017-12-09T23:25:01.604356: step 341, loss 0.745636, acc 0.773438, prec 0.035636, recall 0.772187
2017-12-09T23:25:02.104347: step 342, loss 0.782597, acc 0.753906, prec 0.0355947, recall 0.772346
2017-12-09T23:25:02.618928: step 343, loss 1.83396, acc 0.800781, prec 0.0356615, recall 0.772443
2017-12-09T23:25:03.121763: step 344, loss 2.00831, acc 0.816406, prec 0.0358563, recall 0.773167
2017-12-09T23:25:03.627784: step 345, loss 0.905508, acc 0.855469, prec 0.0359694, recall 0.773416
2017-12-09T23:25:04.131432: step 346, loss 0.465506, acc 0.839844, prec 0.0359839, recall 0.773728
2017-12-09T23:25:04.644224: step 347, loss 0.876991, acc 0.847656, prec 0.0360634, recall 0.773818
2017-12-09T23:25:05.152690: step 348, loss 0.879707, acc 0.847656, prec 0.0360197, recall 0.773288
2017-12-09T23:25:05.678524: step 349, loss 1.13009, acc 0.84375, prec 0.0360068, recall 0.772386
2017-12-09T23:25:06.188073: step 350, loss 0.934007, acc 0.847656, prec 0.0360552, recall 0.772324
2017-12-09T23:25:06.691062: step 351, loss 1.12899, acc 0.832031, prec 0.0361603, recall 0.772573
2017-12-09T23:25:07.195870: step 352, loss 1.24925, acc 0.855469, prec 0.0362107, recall 0.772512
2017-12-09T23:25:07.699780: step 353, loss 1.27443, acc 0.820312, prec 0.0362202, recall 0.772297
2017-12-09T23:25:08.206065: step 354, loss 0.702095, acc 0.742188, prec 0.0361751, recall 0.772451
2017-12-09T23:25:08.713035: step 355, loss 0.734619, acc 0.734375, prec 0.0362495, recall 0.773217
2017-12-09T23:25:09.214548: step 356, loss 1.28762, acc 0.714844, prec 0.0362887, recall 0.773306
2017-12-09T23:25:09.721663: step 357, loss 0.830913, acc 0.757812, prec 0.0363088, recall 0.773762
2017-12-09T23:25:10.217892: step 358, loss 1.23824, acc 0.742188, prec 0.0363557, recall 0.773849
2017-12-09T23:25:10.725024: step 359, loss 0.725556, acc 0.742188, prec 0.0363409, recall 0.774151
2017-12-09T23:25:11.230280: step 360, loss 0.613523, acc 0.796875, prec 0.0364022, recall 0.774751
2017-12-09T23:25:11.733148: step 361, loss 1.18329, acc 0.785156, prec 0.0364611, recall 0.774834
2017-12-09T23:25:12.246492: step 362, loss 0.804214, acc 0.746094, prec 0.0364773, recall 0.775281
2017-12-09T23:25:12.752245: step 363, loss 0.874434, acc 0.765625, prec 0.0364692, recall 0.775578
2017-12-09T23:25:13.256581: step 364, loss 0.863569, acc 0.828125, prec 0.0365389, recall 0.776169
2017-12-09T23:25:13.772707: step 365, loss 1.96499, acc 0.816406, prec 0.0365167, recall 0.775805
2017-12-09T23:25:14.279582: step 366, loss 1.24624, acc 0.761719, prec 0.036568, recall 0.775885
2017-12-09T23:25:14.784641: step 367, loss 0.529316, acc 0.800781, prec 0.0365402, recall 0.776031
2017-12-09T23:25:15.285561: step 368, loss 0.647528, acc 0.78125, prec 0.0365366, recall 0.776324
2017-12-09T23:25:15.794106: step 369, loss 0.595643, acc 0.859375, prec 0.0365554, recall 0.776617
2017-12-09T23:25:16.297074: step 370, loss 0.526359, acc 0.859375, prec 0.0366629, recall 0.777344
2017-12-09T23:25:16.805212: step 371, loss 3.04758, acc 0.839844, prec 0.0367372, recall 0.776913
2017-12-09T23:25:17.316007: step 372, loss 0.604902, acc 0.882812, prec 0.0367625, recall 0.777202
2017-12-09T23:25:17.818150: step 373, loss 0.74017, acc 0.835938, prec 0.0368331, recall 0.777778
2017-12-09T23:25:18.322716: step 374, loss 1.47713, acc 0.796875, prec 0.036864, recall 0.777706
2017-12-09T23:25:18.506854: step 375, loss 0.68503, acc 0.784314, prec 0.0368516, recall 0.777706
2017-12-09T23:25:19.015507: step 376, loss 0.815921, acc 0.78125, prec 0.0368768, recall 0.778135
2017-12-09T23:25:19.520883: step 377, loss 0.611868, acc 0.789062, prec 0.0369041, recall 0.778562
2017-12-09T23:25:20.025998: step 378, loss 0.767307, acc 0.757812, prec 0.0369223, recall 0.778988
2017-12-09T23:25:20.551639: step 379, loss 0.668927, acc 0.785156, prec 0.0369192, recall 0.779271
2017-12-09T23:25:21.055068: step 380, loss 0.569106, acc 0.84375, prec 0.0370202, recall 0.779974
2017-12-09T23:25:21.569570: step 381, loss 0.681254, acc 0.804688, prec 0.0371098, recall 0.780674
2017-12-09T23:25:22.083668: step 382, loss 0.569149, acc 0.839844, prec 0.037151, recall 0.781091
2017-12-09T23:25:22.588970: step 383, loss 0.356874, acc 0.894531, prec 0.0371498, recall 0.78123
2017-12-09T23:25:23.103606: step 384, loss 0.43221, acc 0.863281, prec 0.0371687, recall 0.781507
2017-12-09T23:25:23.610733: step 385, loss 0.660322, acc 0.839844, prec 0.0372676, recall 0.782197
2017-12-09T23:25:24.116464: step 386, loss 0.930366, acc 0.867188, prec 0.0372885, recall 0.781979
2017-12-09T23:25:24.620168: step 387, loss 1.6811, acc 0.875, prec 0.0373694, recall 0.782035
2017-12-09T23:25:25.128088: step 388, loss 0.610751, acc 0.894531, prec 0.0375124, recall 0.782854
2017-12-09T23:25:25.630500: step 389, loss 0.487541, acc 0.894531, prec 0.0376262, recall 0.783531
2017-12-09T23:25:26.140414: step 390, loss 1.68215, acc 0.867188, prec 0.0378205, recall 0.783633
2017-12-09T23:25:26.649466: step 391, loss 0.454233, acc 0.886719, prec 0.0379603, recall 0.784435
2017-12-09T23:25:27.160115: step 392, loss 0.45146, acc 0.839844, prec 0.0379426, recall 0.784568
2017-12-09T23:25:27.668634: step 393, loss 0.339825, acc 0.898438, prec 0.0379705, recall 0.784834
2017-12-09T23:25:28.182053: step 394, loss 1.4952, acc 0.859375, prec 0.0380467, recall 0.784398
2017-12-09T23:25:28.701959: step 395, loss 0.742957, acc 0.808594, prec 0.0380771, recall 0.784795
2017-12-09T23:25:29.204309: step 396, loss 0.801317, acc 0.808594, prec 0.038136, recall 0.785321
2017-12-09T23:25:29.704581: step 397, loss 0.644958, acc 0.808594, prec 0.0381661, recall 0.785714
2017-12-09T23:25:30.233096: step 398, loss 0.715881, acc 0.824219, prec 0.0382862, recall 0.786496
2017-12-09T23:25:30.750668: step 399, loss 1.14187, acc 0.765625, prec 0.0383035, recall 0.786885
2017-12-09T23:25:31.250974: step 400, loss 0.712144, acc 0.789062, prec 0.038356, recall 0.787402
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-400

2017-12-09T23:25:32.676984: step 401, loss 0.814028, acc 0.746094, prec 0.0384242, recall 0.788043
2017-12-09T23:25:33.177526: step 402, loss 0.826122, acc 0.742188, prec 0.0384344, recall 0.788427
2017-12-09T23:25:33.684796: step 403, loss 0.951986, acc 0.777344, prec 0.0385112, recall 0.789062
2017-12-09T23:25:34.188081: step 404, loss 0.591821, acc 0.816406, prec 0.0386553, recall 0.789946
2017-12-09T23:25:34.691003: step 405, loss 0.727658, acc 0.789062, prec 0.0387067, recall 0.790448
2017-12-09T23:25:35.198757: step 406, loss 1.94447, acc 0.824219, prec 0.0387411, recall 0.790351
2017-12-09T23:25:35.716875: step 407, loss 0.881914, acc 0.804688, prec 0.0388248, recall 0.790974
2017-12-09T23:25:36.228262: step 408, loss 0.6278, acc 0.851562, prec 0.0388378, recall 0.791222
2017-12-09T23:25:36.732417: step 409, loss 0.453272, acc 0.839844, prec 0.0387915, recall 0.791222
2017-12-09T23:25:37.234532: step 410, loss 0.555134, acc 0.867188, prec 0.0388369, recall 0.791593
2017-12-09T23:25:37.744664: step 411, loss 1.83455, acc 0.851562, prec 0.0388242, recall 0.79078
2017-12-09T23:25:38.253887: step 412, loss 0.507174, acc 0.851562, prec 0.0388093, recall 0.790904
2017-12-09T23:25:38.762827: step 413, loss 0.725389, acc 0.855469, prec 0.0389069, recall 0.791519
2017-12-09T23:25:39.264897: step 414, loss 0.404784, acc 0.863281, prec 0.0388953, recall 0.791642
2017-12-09T23:25:39.781756: step 415, loss 0.422246, acc 0.859375, prec 0.0389104, recall 0.791887
2017-12-09T23:25:40.292470: step 416, loss 0.407863, acc 0.863281, prec 0.0389266, recall 0.792132
2017-12-09T23:25:40.795432: step 417, loss 1.57304, acc 0.859375, prec 0.0389704, recall 0.792033
2017-12-09T23:25:41.303413: step 418, loss 2.5947, acc 0.910156, prec 0.0389745, recall 0.791228
2017-12-09T23:25:41.812508: step 419, loss 0.794388, acc 0.875, prec 0.038995, recall 0.79101
2017-12-09T23:25:42.314839: step 420, loss 1.36801, acc 0.867188, prec 0.0390686, recall 0.791036
2017-12-09T23:25:42.822089: step 421, loss 0.561294, acc 0.816406, prec 0.0390986, recall 0.7914
2017-12-09T23:25:43.323285: step 422, loss 2.44636, acc 0.808594, prec 0.0390999, recall 0.791183
2017-12-09T23:25:43.829364: step 423, loss 0.842043, acc 0.78125, prec 0.0391747, recall 0.791787
2017-12-09T23:25:44.336149: step 424, loss 0.68529, acc 0.761719, prec 0.0391614, recall 0.792028
2017-12-09T23:25:44.838894: step 425, loss 0.943299, acc 0.71875, prec 0.0391631, recall 0.792388
2017-12-09T23:25:45.343093: step 426, loss 0.791093, acc 0.757812, prec 0.0392307, recall 0.792984
2017-12-09T23:25:45.850978: step 427, loss 0.67137, acc 0.796875, prec 0.0392001, recall 0.793103
2017-12-09T23:25:46.359582: step 428, loss 0.684272, acc 0.796875, prec 0.0391968, recall 0.793341
2017-12-09T23:25:46.863903: step 429, loss 2.49084, acc 0.792969, prec 0.039249, recall 0.792906
2017-12-09T23:25:47.372731: step 430, loss 0.575133, acc 0.835938, prec 0.0393111, recall 0.793379
2017-12-09T23:25:47.881104: step 431, loss 1.35966, acc 0.769531, prec 0.0392738, recall 0.793044
2017-12-09T23:25:48.383747: step 432, loss 0.546284, acc 0.789062, prec 0.0392411, recall 0.793162
2017-12-09T23:25:48.886537: step 433, loss 2.41614, acc 0.792969, prec 0.0392378, recall 0.792947
2017-12-09T23:25:49.387643: step 434, loss 0.52576, acc 0.835938, prec 0.0392455, recall 0.793182
2017-12-09T23:25:49.894999: step 435, loss 1.30898, acc 0.828125, prec 0.039252, recall 0.792967
2017-12-09T23:25:50.407569: step 436, loss 0.783087, acc 0.828125, prec 0.0392586, recall 0.792752
2017-12-09T23:25:50.915355: step 437, loss 0.703441, acc 0.855469, prec 0.0392459, recall 0.792421
2017-12-09T23:25:51.416293: step 438, loss 0.559507, acc 0.792969, prec 0.0393221, recall 0.793006
2017-12-09T23:25:51.918346: step 439, loss 0.543855, acc 0.804688, prec 0.0393208, recall 0.793239
2017-12-09T23:25:52.418083: step 440, loss 0.864501, acc 0.777344, prec 0.0392851, recall 0.793356
2017-12-09T23:25:52.926470: step 441, loss 0.912558, acc 0.828125, prec 0.0392916, recall 0.793142
2017-12-09T23:25:53.429507: step 442, loss 0.920445, acc 0.816406, prec 0.039268, recall 0.792813
2017-12-09T23:25:53.929762: step 443, loss 0.535679, acc 0.835938, prec 0.0394357, recall 0.793739
2017-12-09T23:25:54.441507: step 444, loss 0.554657, acc 0.820312, prec 0.0393854, recall 0.793739
2017-12-09T23:25:54.943688: step 445, loss 0.659709, acc 0.859375, prec 0.0394259, recall 0.794085
2017-12-09T23:25:55.458695: step 446, loss 0.373526, acc 0.847656, prec 0.0394365, recall 0.794314
2017-12-09T23:25:55.966557: step 447, loss 1.06151, acc 0.863281, prec 0.0395588, recall 0.794559
2017-12-09T23:25:56.476571: step 448, loss 0.43376, acc 0.878906, prec 0.039578, recall 0.794786
2017-12-09T23:25:56.989782: step 449, loss 0.550569, acc 0.867188, prec 0.0396998, recall 0.795467
2017-12-09T23:25:57.504311: step 450, loss 0.311926, acc 0.890625, prec 0.0398015, recall 0.796031
2017-12-09T23:25:58.008246: step 451, loss 1.88959, acc 0.835938, prec 0.0398635, recall 0.795604
2017-12-09T23:25:58.514919: step 452, loss 0.372641, acc 0.894531, prec 0.0399659, recall 0.796164
2017-12-09T23:25:59.018955: step 453, loss 0.367585, acc 0.851562, prec 0.0399242, recall 0.796164
2017-12-09T23:25:59.523987: step 454, loss 0.591676, acc 0.855469, prec 0.039989, recall 0.79661
2017-12-09T23:26:00.037301: step 455, loss 2.95831, acc 0.890625, prec 0.0400132, recall 0.795963
2017-12-09T23:26:00.555610: step 456, loss 0.958394, acc 0.859375, prec 0.0401063, recall 0.796085
2017-12-09T23:26:01.061437: step 457, loss 0.530975, acc 0.839844, prec 0.0401138, recall 0.796306
2017-12-09T23:26:01.568638: step 458, loss 0.584681, acc 0.839844, prec 0.0401738, recall 0.796748
2017-12-09T23:26:02.075704: step 459, loss 1.10513, acc 0.792969, prec 0.0402216, recall 0.796757
2017-12-09T23:26:02.577381: step 460, loss 0.559348, acc 0.8125, prec 0.0402997, recall 0.797305
2017-12-09T23:26:03.081126: step 461, loss 1.72493, acc 0.734375, prec 0.0402524, recall 0.796984
2017-12-09T23:26:03.588821: step 462, loss 0.710738, acc 0.746094, prec 0.0402335, recall 0.797203
2017-12-09T23:26:04.086981: step 463, loss 1.04312, acc 0.734375, prec 0.0402894, recall 0.797747
2017-12-09T23:26:04.590123: step 464, loss 0.853191, acc 0.804688, prec 0.0403647, recall 0.798288
2017-12-09T23:26:05.093083: step 465, loss 1.15531, acc 0.703125, prec 0.0403866, recall 0.798292
2017-12-09T23:26:05.615342: step 466, loss 0.915162, acc 0.707031, prec 0.0404601, recall 0.798936
2017-12-09T23:26:06.116509: step 467, loss 0.715058, acc 0.753906, prec 0.0404432, recall 0.79915
2017-12-09T23:26:06.620400: step 468, loss 0.847553, acc 0.757812, prec 0.0405304, recall 0.799788
2017-12-09T23:26:07.123151: step 469, loss 0.557885, acc 0.820312, prec 0.0405318, recall 0.8
2017-12-09T23:26:07.619567: step 470, loss 0.597739, acc 0.832031, prec 0.0405109, recall 0.800106
2017-12-09T23:26:08.122742: step 471, loss 0.530355, acc 0.851562, prec 0.0405723, recall 0.800528
2017-12-09T23:26:08.626723: step 472, loss 1.19523, acc 0.851562, prec 0.0407885, recall 0.801154
2017-12-09T23:26:09.137259: step 473, loss 0.843265, acc 0.832031, prec 0.0407684, recall 0.800839
2017-12-09T23:26:09.641836: step 474, loss 1.41901, acc 0.855469, prec 0.0409594, recall 0.801356
2017-12-09T23:26:10.150580: step 475, loss 0.66947, acc 0.855469, prec 0.0410467, recall 0.801872
2017-12-09T23:26:10.655798: step 476, loss 0.502239, acc 0.847656, prec 0.0410806, recall 0.802181
2017-12-09T23:26:11.160117: step 477, loss 0.595826, acc 0.796875, prec 0.0410493, recall 0.802283
2017-12-09T23:26:11.672018: step 478, loss 1.18087, acc 0.851562, prec 0.0410356, recall 0.801554
2017-12-09T23:26:12.178824: step 479, loss 0.416929, acc 0.835938, prec 0.0411423, recall 0.802169
2017-12-09T23:26:12.684274: step 480, loss 0.78836, acc 0.851562, prec 0.0411782, recall 0.802062
2017-12-09T23:26:13.179741: step 481, loss 0.719356, acc 0.8125, prec 0.0411766, recall 0.802266
2017-12-09T23:26:13.685947: step 482, loss 0.659581, acc 0.761719, prec 0.0412368, recall 0.802773
2017-12-09T23:26:14.191307: step 483, loss 0.475794, acc 0.863281, prec 0.0413251, recall 0.803279
2017-12-09T23:26:14.699463: step 484, loss 0.7087, acc 0.757812, prec 0.0413334, recall 0.803581
2017-12-09T23:26:15.202328: step 485, loss 0.774817, acc 0.785156, prec 0.0414248, recall 0.804182
2017-12-09T23:26:15.705069: step 486, loss 0.672899, acc 0.832031, prec 0.0415792, recall 0.804977
2017-12-09T23:26:16.220655: step 487, loss 1.41526, acc 0.816406, prec 0.0416296, recall 0.804965
2017-12-09T23:26:16.731539: step 488, loss 0.730664, acc 0.820312, prec 0.0416797, recall 0.805359
2017-12-09T23:26:17.235553: step 489, loss 0.606738, acc 0.792969, prec 0.0416971, recall 0.805654
2017-12-09T23:26:17.742533: step 490, loss 0.483274, acc 0.835938, prec 0.0416765, recall 0.805752
2017-12-09T23:26:18.241812: step 491, loss 0.378704, acc 0.863281, prec 0.0416884, recall 0.805948
2017-12-09T23:26:18.745697: step 492, loss 0.446432, acc 0.894531, prec 0.0418089, recall 0.806533
2017-12-09T23:26:19.250597: step 493, loss 0.272743, acc 0.886719, prec 0.0418521, recall 0.806824
2017-12-09T23:26:19.756476: step 494, loss 0.414916, acc 0.898438, prec 0.0418986, recall 0.807114
2017-12-09T23:26:20.268319: step 495, loss 0.676047, acc 0.902344, prec 0.0419958, recall 0.807596
2017-12-09T23:26:20.792932: step 496, loss 0.328306, acc 0.894531, prec 0.042041, recall 0.807884
2017-12-09T23:26:21.302774: step 497, loss 1.00671, acc 0.914062, prec 0.0420938, recall 0.807367
2017-12-09T23:26:21.809094: step 498, loss 2.00006, acc 0.902344, prec 0.0421681, recall 0.806948
2017-12-09T23:26:22.310638: step 499, loss 0.382282, acc 0.894531, prec 0.0422378, recall 0.80733
2017-12-09T23:26:22.490398: step 500, loss 0.511593, acc 0.862745, prec 0.0422302, recall 0.80733
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-500

2017-12-09T23:26:24.090106: step 501, loss 1.34109, acc 0.875, prec 0.0422459, recall 0.807122
2017-12-09T23:26:24.599932: step 502, loss 0.59424, acc 0.867188, prec 0.0423078, recall 0.807502
2017-12-09T23:26:25.102488: step 503, loss 0.631804, acc 0.859375, prec 0.0424416, recall 0.808165
2017-12-09T23:26:25.607760: step 504, loss 0.492432, acc 0.847656, prec 0.0425224, recall 0.808636
2017-12-09T23:26:26.107361: step 505, loss 0.744292, acc 0.839844, prec 0.042528, recall 0.808427
2017-12-09T23:26:26.626633: step 506, loss 0.474082, acc 0.824219, prec 0.0425033, recall 0.808521
2017-12-09T23:26:27.129822: step 507, loss 0.407197, acc 0.84375, prec 0.0425335, recall 0.808802
2017-12-09T23:26:27.635127: step 508, loss 0.813452, acc 0.828125, prec 0.0425603, recall 0.808687
2017-12-09T23:26:28.140737: step 509, loss 0.550081, acc 0.832031, prec 0.0426116, recall 0.80906
2017-12-09T23:26:28.645144: step 510, loss 0.382987, acc 0.859375, prec 0.0425968, recall 0.809153
2017-12-09T23:26:29.148046: step 511, loss 0.652512, acc 0.828125, prec 0.0427204, recall 0.809801
2017-12-09T23:26:29.650404: step 512, loss 0.508261, acc 0.792969, prec 0.0427359, recall 0.810078
2017-12-09T23:26:30.156799: step 513, loss 0.389675, acc 0.871094, prec 0.0427732, recall 0.810353
2017-12-09T23:26:30.674001: step 514, loss 0.776666, acc 0.855469, prec 0.0428072, recall 0.810237
2017-12-09T23:26:31.183456: step 515, loss 0.432748, acc 0.859375, prec 0.0428411, recall 0.810511
2017-12-09T23:26:31.680711: step 516, loss 1.30181, acc 0.902344, prec 0.0428881, recall 0.810395
2017-12-09T23:26:32.199785: step 517, loss 0.500429, acc 0.886719, prec 0.0429538, recall 0.810759
2017-12-09T23:26:32.701999: step 518, loss 1.17778, acc 0.890625, prec 0.043046, recall 0.810824
2017-12-09T23:26:33.204132: step 519, loss 0.295372, acc 0.878906, prec 0.0430607, recall 0.811005
2017-12-09T23:26:33.710713: step 520, loss 0.452285, acc 0.859375, prec 0.0431185, recall 0.811366
2017-12-09T23:26:34.208480: step 521, loss 0.383398, acc 0.851562, prec 0.043174, recall 0.811725
2017-12-09T23:26:34.714622: step 522, loss 0.364087, acc 0.882812, prec 0.0432382, recall 0.812084
2017-12-09T23:26:35.213471: step 523, loss 0.322495, acc 0.882812, prec 0.0433022, recall 0.812441
2017-12-09T23:26:35.729118: step 524, loss 0.413373, acc 0.863281, prec 0.0433123, recall 0.812619
2017-12-09T23:26:36.225378: step 525, loss 0.307811, acc 0.910156, prec 0.0433354, recall 0.812796
2017-12-09T23:26:36.730132: step 526, loss 0.264779, acc 0.914062, prec 0.0433597, recall 0.812973
2017-12-09T23:26:37.228321: step 527, loss 0.279579, acc 0.910156, prec 0.0433828, recall 0.81315
2017-12-09T23:26:37.727870: step 528, loss 0.377652, acc 0.910156, prec 0.0434059, recall 0.813327
2017-12-09T23:26:38.230610: step 529, loss 0.392849, acc 0.894531, prec 0.0435692, recall 0.81403
2017-12-09T23:26:38.734414: step 530, loss 1.2259, acc 0.90625, prec 0.0436403, recall 0.813997
2017-12-09T23:26:39.242877: step 531, loss 0.229163, acc 0.925781, prec 0.0437398, recall 0.814433
2017-12-09T23:26:39.748775: step 532, loss 0.31286, acc 0.878906, prec 0.0437778, recall 0.814694
2017-12-09T23:26:40.255938: step 533, loss 0.336728, acc 0.910156, prec 0.0438486, recall 0.81504
2017-12-09T23:26:40.761572: step 534, loss 2.00511, acc 0.90625, prec 0.0438484, recall 0.814366
2017-12-09T23:26:41.269408: step 535, loss 0.344067, acc 0.90625, prec 0.043918, recall 0.814711
2017-12-09T23:26:41.773199: step 536, loss 0.28127, acc 0.882812, prec 0.0439089, recall 0.814798
2017-12-09T23:26:42.286227: step 537, loss 0.352022, acc 0.894531, prec 0.043975, recall 0.815142
2017-12-09T23:26:42.791895: step 538, loss 0.762097, acc 0.84375, prec 0.0441235, recall 0.815449
2017-12-09T23:26:43.297615: step 539, loss 0.322873, acc 0.921875, prec 0.0441254, recall 0.815534
2017-12-09T23:26:43.795860: step 540, loss 0.422194, acc 0.890625, prec 0.0443095, recall 0.816298
2017-12-09T23:26:44.299659: step 541, loss 0.830508, acc 0.8125, prec 0.0443052, recall 0.816092
2017-12-09T23:26:44.813223: step 542, loss 0.524312, acc 0.855469, prec 0.0442882, recall 0.816176
2017-12-09T23:26:45.318299: step 543, loss 0.645252, acc 0.882812, prec 0.04428, recall 0.815886
2017-12-09T23:26:45.823061: step 544, loss 0.482039, acc 0.882812, prec 0.0443183, recall 0.816139
2017-12-09T23:26:46.327620: step 545, loss 0.746508, acc 0.855469, prec 0.0444439, recall 0.816728
2017-12-09T23:26:46.839884: step 546, loss 0.765065, acc 0.839844, prec 0.0444709, recall 0.816606
2017-12-09T23:26:47.342360: step 547, loss 0.844392, acc 0.820312, prec 0.044515, recall 0.81694
2017-12-09T23:26:47.850639: step 548, loss 0.371001, acc 0.871094, prec 0.044526, recall 0.817106
2017-12-09T23:26:48.353547: step 549, loss 0.769925, acc 0.828125, prec 0.0445484, recall 0.817356
2017-12-09T23:26:48.852600: step 550, loss 1.10537, acc 0.839844, prec 0.0446462, recall 0.817482
2017-12-09T23:26:49.357683: step 551, loss 0.54298, acc 0.816406, prec 0.0446651, recall 0.81773
2017-12-09T23:26:49.860081: step 552, loss 0.578057, acc 0.84375, prec 0.0446682, recall 0.817894
2017-12-09T23:26:50.365381: step 553, loss 0.336859, acc 0.898438, prec 0.0447102, recall 0.818141
2017-12-09T23:26:50.885308: step 554, loss 1.15246, acc 0.878906, prec 0.0448419, recall 0.818345
2017-12-09T23:26:51.390012: step 555, loss 0.506087, acc 0.828125, prec 0.0448169, recall 0.818427
2017-12-09T23:26:51.888627: step 556, loss 0.45496, acc 0.828125, prec 0.0447919, recall 0.818509
2017-12-09T23:26:52.398272: step 557, loss 0.415134, acc 0.839844, prec 0.0448172, recall 0.818753
2017-12-09T23:26:52.909690: step 558, loss 1.28225, acc 0.804688, prec 0.044857, recall 0.818711
2017-12-09T23:26:53.412399: step 559, loss 0.541654, acc 0.878906, prec 0.0449868, recall 0.819277
2017-12-09T23:26:53.921813: step 560, loss 0.582338, acc 0.828125, prec 0.0450084, recall 0.819519
2017-12-09T23:26:54.424855: step 561, loss 0.328327, acc 0.84375, prec 0.0450111, recall 0.819679
2017-12-09T23:26:54.926898: step 562, loss 1.03559, acc 0.828125, prec 0.0450105, recall 0.819475
2017-12-09T23:26:55.425937: step 563, loss 0.38232, acc 0.859375, prec 0.0450409, recall 0.819716
2017-12-09T23:26:55.929541: step 564, loss 0.523198, acc 0.878906, prec 0.0451465, recall 0.820195
2017-12-09T23:26:56.431025: step 565, loss 0.426497, acc 0.917969, prec 0.045263, recall 0.820671
2017-12-09T23:26:56.940467: step 566, loss 0.476435, acc 0.882812, prec 0.0453229, recall 0.820988
2017-12-09T23:26:57.445768: step 567, loss 0.710926, acc 0.863281, prec 0.045354, recall 0.821224
2017-12-09T23:26:57.951514: step 568, loss 0.328233, acc 0.886719, prec 0.0453916, recall 0.82146
2017-12-09T23:26:58.456632: step 569, loss 0.630373, acc 0.875, prec 0.045449, recall 0.821773
2017-12-09T23:26:58.961971: step 570, loss 1.24902, acc 0.867188, prec 0.0455284, recall 0.821804
2017-12-09T23:26:59.464004: step 571, loss 0.441511, acc 0.859375, prec 0.0455581, recall 0.822038
2017-12-09T23:26:59.960380: step 572, loss 0.74887, acc 0.855469, prec 0.0456559, recall 0.822503
2017-12-09T23:27:00.473939: step 573, loss 0.501528, acc 0.863281, prec 0.0457096, recall 0.822812
2017-12-09T23:27:00.977431: step 574, loss 0.6159, acc 0.886719, prec 0.0458159, recall 0.823274
2017-12-09T23:27:01.483968: step 575, loss 0.463647, acc 0.90625, prec 0.0459276, recall 0.823733
2017-12-09T23:27:01.991941: step 576, loss 0.388158, acc 0.878906, prec 0.0459853, recall 0.824038
2017-12-09T23:27:02.501637: step 577, loss 1.20449, acc 0.84375, prec 0.0459881, recall 0.823834
2017-12-09T23:27:03.001965: step 578, loss 0.570889, acc 0.867188, prec 0.0460194, recall 0.824062
2017-12-09T23:27:03.504384: step 579, loss 0.313345, acc 0.882812, prec 0.046055, recall 0.824289
2017-12-09T23:27:04.012859: step 580, loss 0.549208, acc 0.832031, prec 0.0460991, recall 0.824592
2017-12-09T23:27:04.526307: step 581, loss 0.448114, acc 0.871094, prec 0.0461084, recall 0.824742
2017-12-09T23:27:05.025070: step 582, loss 0.429436, acc 0.863281, prec 0.0461612, recall 0.825043
2017-12-09T23:27:05.542025: step 583, loss 0.560531, acc 0.851562, prec 0.0461878, recall 0.825268
2017-12-09T23:27:06.045185: step 584, loss 0.597265, acc 0.875, prec 0.0462666, recall 0.825641
2017-12-09T23:27:06.543252: step 585, loss 0.396755, acc 0.875, prec 0.0462996, recall 0.825864
2017-12-09T23:27:07.039858: step 586, loss 0.466427, acc 0.875, prec 0.0463326, recall 0.826087
2017-12-09T23:27:07.545542: step 587, loss 0.425886, acc 0.878906, prec 0.0464122, recall 0.826457
2017-12-09T23:27:08.043344: step 588, loss 0.488071, acc 0.878906, prec 0.0464689, recall 0.826752
2017-12-09T23:27:08.548225: step 589, loss 0.268356, acc 0.894531, prec 0.0464617, recall 0.826825
2017-12-09T23:27:09.053058: step 590, loss 1.47115, acc 0.925781, prec 0.0465327, recall 0.826768
2017-12-09T23:27:09.564433: step 591, loss 0.377445, acc 0.917969, prec 0.0466003, recall 0.827061
2017-12-09T23:27:10.077004: step 592, loss 0.321441, acc 0.921875, prec 0.0466462, recall 0.82728
2017-12-09T23:27:10.585682: step 593, loss 0.790745, acc 0.859375, prec 0.0466527, recall 0.827077
2017-12-09T23:27:11.086799: step 594, loss 1.23285, acc 0.878906, prec 0.0468007, recall 0.827311
2017-12-09T23:27:11.598108: step 595, loss 0.185003, acc 0.9375, prec 0.0468962, recall 0.827673
2017-12-09T23:27:12.103108: step 596, loss 0.366447, acc 0.863281, prec 0.0469251, recall 0.827889
2017-12-09T23:27:12.604761: step 597, loss 0.348508, acc 0.882812, prec 0.0469369, recall 0.828033
2017-12-09T23:27:13.103409: step 598, loss 0.32939, acc 0.894531, prec 0.0469746, recall 0.828249
2017-12-09T23:27:13.608610: step 599, loss 0.608681, acc 0.902344, prec 0.0470371, recall 0.828536
2017-12-09T23:27:14.114586: step 600, loss 0.411849, acc 0.886719, prec 0.0470725, recall 0.82875

Evaluation:
2017-12-09T23:27:18.789291: step 600, loss 1.27018, acc 0.893764, prec 0.0482055, recall 0.820855

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-600

2017-12-09T23:27:20.299431: step 601, loss 0.387023, acc 0.867188, prec 0.0481897, recall 0.820925
2017-12-09T23:27:20.818247: step 602, loss 0.377322, acc 0.871094, prec 0.0482188, recall 0.821135
2017-12-09T23:27:21.319597: step 603, loss 0.287177, acc 0.871094, prec 0.0482478, recall 0.821345
2017-12-09T23:27:21.818454: step 604, loss 0.50817, acc 0.894531, prec 0.0482835, recall 0.821554
2017-12-09T23:27:22.317046: step 605, loss 1.04704, acc 0.890625, prec 0.0483201, recall 0.821122
2017-12-09T23:27:22.829641: step 606, loss 2.14261, acc 0.863281, prec 0.0483479, recall 0.821012
2017-12-09T23:27:23.333741: step 607, loss 0.415578, acc 0.84375, prec 0.0483254, recall 0.821081
2017-12-09T23:27:23.840511: step 608, loss 0.549527, acc 0.855469, prec 0.0483281, recall 0.82122
2017-12-09T23:27:24.343896: step 609, loss 0.595086, acc 0.84375, prec 0.0484361, recall 0.821705
2017-12-09T23:27:24.859217: step 610, loss 0.353175, acc 0.851562, prec 0.0485027, recall 0.82205
2017-12-09T23:27:25.372843: step 611, loss 0.438635, acc 0.867188, prec 0.0484868, recall 0.822119
2017-12-09T23:27:25.876677: step 612, loss 0.343699, acc 0.855469, prec 0.0484459, recall 0.822119
2017-12-09T23:27:26.381995: step 613, loss 0.381276, acc 0.878906, prec 0.048455, recall 0.822257
2017-12-09T23:27:26.886325: step 614, loss 0.590571, acc 0.871094, prec 0.0485053, recall 0.822531
2017-12-09T23:27:27.397693: step 615, loss 0.38108, acc 0.894531, prec 0.0486053, recall 0.822941
2017-12-09T23:27:27.905487: step 616, loss 0.356338, acc 0.90625, prec 0.0487085, recall 0.823349
2017-12-09T23:27:28.410238: step 617, loss 0.510986, acc 0.925781, prec 0.0487317, recall 0.823168
2017-12-09T23:27:28.913426: step 618, loss 0.779878, acc 0.925781, prec 0.0487982, recall 0.823124
2017-12-09T23:27:29.411941: step 619, loss 0.3122, acc 0.921875, prec 0.0489055, recall 0.823529
2017-12-09T23:27:29.924707: step 620, loss 0.403329, acc 0.929688, prec 0.0489934, recall 0.823866
2017-12-09T23:27:30.449549: step 621, loss 0.356594, acc 0.90625, prec 0.0490314, recall 0.824067
2017-12-09T23:27:30.952101: step 622, loss 3.50513, acc 0.90625, prec 0.0490727, recall 0.823328
2017-12-09T23:27:31.469957: step 623, loss 0.350502, acc 0.890625, prec 0.0491061, recall 0.823529
2017-12-09T23:27:31.976549: step 624, loss 0.445779, acc 0.875, prec 0.0491136, recall 0.823663
2017-12-09T23:27:32.156598: step 625, loss 0.392111, acc 0.921569, prec 0.0491092, recall 0.823663
2017-12-09T23:27:32.669440: step 626, loss 0.496035, acc 0.832031, prec 0.0491474, recall 0.82393
2017-12-09T23:27:33.174343: step 627, loss 0.476373, acc 0.832031, prec 0.0491641, recall 0.82413
2017-12-09T23:27:33.675391: step 628, loss 0.453025, acc 0.835938, prec 0.0492461, recall 0.824528
2017-12-09T23:27:34.174304: step 629, loss 0.587591, acc 0.8125, prec 0.0492785, recall 0.824793
2017-12-09T23:27:34.677518: step 630, loss 0.603511, acc 0.792969, prec 0.0492411, recall 0.824859
2017-12-09T23:27:35.192566: step 631, loss 0.810797, acc 0.820312, prec 0.0492543, recall 0.825056
2017-12-09T23:27:35.711648: step 632, loss 0.562093, acc 0.914062, prec 0.0492727, recall 0.825188
2017-12-09T23:27:36.216340: step 633, loss 0.490246, acc 0.847656, prec 0.0493149, recall 0.82545
2017-12-09T23:27:36.724998: step 634, loss 0.435844, acc 0.871094, prec 0.0492997, recall 0.825516
2017-12-09T23:27:37.234345: step 635, loss 0.431296, acc 0.859375, prec 0.0493238, recall 0.825712
2017-12-09T23:27:37.736135: step 636, loss 0.765997, acc 0.894531, prec 0.049359, recall 0.825599
2017-12-09T23:27:38.238154: step 637, loss 0.364814, acc 0.890625, prec 0.0493706, recall 0.825729
2017-12-09T23:27:38.744102: step 638, loss 0.813157, acc 0.902344, prec 0.0494715, recall 0.825811
2017-12-09T23:27:39.265381: step 639, loss 0.320478, acc 0.902344, prec 0.0495501, recall 0.826136
2017-12-09T23:27:39.776983: step 640, loss 0.442908, acc 0.910156, prec 0.0495883, recall 0.826329
2017-12-09T23:27:40.284857: step 641, loss 0.71871, acc 0.921875, prec 0.0496096, recall 0.826152
2017-12-09T23:27:40.790853: step 642, loss 0.393506, acc 0.917969, prec 0.04965, recall 0.826345
2017-12-09T23:27:41.297327: step 643, loss 0.45954, acc 0.914062, prec 0.0497103, recall 0.826602
2017-12-09T23:27:41.800260: step 644, loss 0.798198, acc 0.910156, prec 0.0497495, recall 0.826489
2017-12-09T23:27:42.308875: step 645, loss 0.518292, acc 0.914062, prec 0.0498309, recall 0.826809
2017-12-09T23:27:42.815471: step 646, loss 0.815892, acc 0.90625, prec 0.0498476, recall 0.826632
2017-12-09T23:27:43.325797: step 647, loss 0.658616, acc 0.878906, prec 0.0499611, recall 0.827079
2017-12-09T23:27:43.829467: step 648, loss 0.411874, acc 0.863281, prec 0.0499856, recall 0.827269
2017-12-09T23:27:44.339586: step 649, loss 0.368084, acc 0.851562, prec 0.0499856, recall 0.827396
2017-12-09T23:27:44.844001: step 650, loss 0.399679, acc 0.859375, prec 0.0499878, recall 0.827523
2017-12-09T23:27:45.345407: step 651, loss 0.420584, acc 0.882812, prec 0.0499756, recall 0.827586
2017-12-09T23:27:45.847971: step 652, loss 0.763099, acc 0.824219, prec 0.0500321, recall 0.827599
2017-12-09T23:27:46.356704: step 653, loss 0.449477, acc 0.835938, prec 0.0499856, recall 0.827599
2017-12-09T23:27:46.865733: step 654, loss 0.431759, acc 0.851562, prec 0.0500066, recall 0.827788
2017-12-09T23:27:47.360659: step 655, loss 0.934087, acc 0.8125, prec 0.0500596, recall 0.8278
2017-12-09T23:27:47.875037: step 656, loss 0.32139, acc 0.851562, prec 0.0500805, recall 0.827988
2017-12-09T23:27:48.378351: step 657, loss 0.329797, acc 0.882812, prec 0.0501729, recall 0.828364
2017-12-09T23:27:48.881821: step 658, loss 0.315639, acc 0.890625, prec 0.0501838, recall 0.828488
2017-12-09T23:27:49.395903: step 659, loss 0.21324, acc 0.9375, prec 0.0502706, recall 0.828799
2017-12-09T23:27:49.898819: step 660, loss 0.273645, acc 0.898438, prec 0.0502836, recall 0.828924
2017-12-09T23:27:50.400771: step 661, loss 0.662611, acc 0.921875, prec 0.050345, recall 0.829171
2017-12-09T23:27:50.917873: step 662, loss 0.208997, acc 0.933594, prec 0.0503471, recall 0.829233
2017-12-09T23:27:51.423432: step 663, loss 0.318648, acc 0.890625, prec 0.0503578, recall 0.829356
2017-12-09T23:27:51.930973: step 664, loss 0.200235, acc 0.933594, prec 0.0503807, recall 0.82948
2017-12-09T23:27:52.458840: step 665, loss 0.197554, acc 0.933594, prec 0.0504244, recall 0.829664
2017-12-09T23:27:52.963226: step 666, loss 0.161196, acc 0.949219, prec 0.0504309, recall 0.829726
2017-12-09T23:27:53.480058: step 667, loss 0.428625, acc 0.933594, prec 0.0505161, recall 0.830032
2017-12-09T23:27:53.984655: step 668, loss 0.437931, acc 0.9375, prec 0.0506024, recall 0.830338
2017-12-09T23:27:54.487414: step 669, loss 0.530393, acc 0.949219, prec 0.0506504, recall 0.830521
2017-12-09T23:27:55.002332: step 670, loss 0.503147, acc 0.9375, prec 0.0506753, recall 0.830344
2017-12-09T23:27:55.513946: step 671, loss 0.17208, acc 0.953125, prec 0.0507243, recall 0.830527
2017-12-09T23:27:56.018447: step 672, loss 0.963125, acc 0.9375, prec 0.0508323, recall 0.830593
2017-12-09T23:27:56.529942: step 673, loss 0.667225, acc 0.933594, prec 0.0508352, recall 0.830357
2017-12-09T23:27:57.034682: step 674, loss 0.193047, acc 0.929688, prec 0.0508567, recall 0.830478
2017-12-09T23:27:57.545311: step 675, loss 0.549622, acc 0.90625, prec 0.0509337, recall 0.83078
2017-12-09T23:27:58.047908: step 676, loss 0.5255, acc 0.882812, prec 0.0510246, recall 0.831141
2017-12-09T23:27:58.549577: step 677, loss 0.394834, acc 0.882812, prec 0.051074, recall 0.831381
2017-12-09T23:27:59.058366: step 678, loss 0.479971, acc 0.816406, prec 0.0511458, recall 0.831739
2017-12-09T23:27:59.559565: step 679, loss 0.454907, acc 0.886719, prec 0.0511961, recall 0.831977
2017-12-09T23:28:00.076636: step 680, loss 0.475523, acc 0.832031, prec 0.0512308, recall 0.832215
2017-12-09T23:28:00.588726: step 681, loss 0.508877, acc 0.835938, prec 0.0512458, recall 0.832392
2017-12-09T23:28:01.091339: step 682, loss 0.559306, acc 0.84375, prec 0.0513043, recall 0.832688
2017-12-09T23:28:01.593669: step 683, loss 0.583225, acc 0.804688, prec 0.0512898, recall 0.832805
2017-12-09T23:28:02.095659: step 684, loss 0.423452, acc 0.894531, prec 0.0513215, recall 0.832982
2017-12-09T23:28:02.600270: step 685, loss 0.590125, acc 0.847656, prec 0.0513603, recall 0.833216
2017-12-09T23:28:03.110286: step 686, loss 0.517638, acc 0.886719, prec 0.0514307, recall 0.833509
2017-12-09T23:28:03.624066: step 687, loss 0.327628, acc 0.898438, prec 0.0515043, recall 0.8338
2017-12-09T23:28:04.127730: step 688, loss 0.765519, acc 0.835938, prec 0.0515406, recall 0.833741
2017-12-09T23:28:04.636206: step 689, loss 0.851303, acc 0.910156, prec 0.0515775, recall 0.833624
2017-12-09T23:28:05.141939: step 690, loss 0.256537, acc 0.910156, prec 0.0515929, recall 0.83374
2017-12-09T23:28:05.663108: step 691, loss 0.28982, acc 0.910156, prec 0.0516286, recall 0.833914
2017-12-09T23:28:06.162883: step 692, loss 0.291957, acc 0.917969, prec 0.0517483, recall 0.834317
2017-12-09T23:28:06.667965: step 693, loss 0.373959, acc 0.898438, prec 0.051801, recall 0.834547
2017-12-09T23:28:07.171858: step 694, loss 0.297967, acc 0.898438, prec 0.0518536, recall 0.834777
2017-12-09T23:28:07.680119: step 695, loss 0.535351, acc 0.925781, prec 0.0519355, recall 0.834774
2017-12-09T23:28:08.184312: step 696, loss 0.305094, acc 0.914062, prec 0.0519924, recall 0.835002
2017-12-09T23:28:08.692833: step 697, loss 0.743679, acc 0.929688, prec 0.0519938, recall 0.834771
2017-12-09T23:28:09.200321: step 698, loss 0.712018, acc 0.925781, prec 0.0520337, recall 0.834941
2017-12-09T23:28:09.704746: step 699, loss 1.36223, acc 0.882812, prec 0.0521233, recall 0.834995
2017-12-09T23:28:10.215215: step 700, loss 0.304005, acc 0.890625, prec 0.0521734, recall 0.835221
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-700

2017-12-09T23:28:11.597320: step 701, loss 0.633078, acc 0.878906, prec 0.0522606, recall 0.83556
2017-12-09T23:28:12.099985: step 702, loss 0.760991, acc 0.824219, prec 0.0523319, recall 0.835897
2017-12-09T23:28:12.610218: step 703, loss 0.946053, acc 0.875, prec 0.0523986, recall 0.835892
2017-12-09T23:28:13.121652: step 704, loss 0.474549, acc 0.816406, prec 0.0524269, recall 0.836116
2017-12-09T23:28:13.629845: step 705, loss 0.595305, acc 0.757812, prec 0.0524385, recall 0.836339
2017-12-09T23:28:14.140476: step 706, loss 0.575361, acc 0.839844, prec 0.0525542, recall 0.836783
2017-12-09T23:28:14.645660: step 707, loss 0.651924, acc 0.792969, prec 0.0525756, recall 0.837004
2017-12-09T23:28:15.149656: step 708, loss 0.592567, acc 0.808594, prec 0.0526014, recall 0.837225
2017-12-09T23:28:15.659520: step 709, loss 0.544325, acc 0.792969, prec 0.0526025, recall 0.83739
2017-12-09T23:28:16.161657: step 710, loss 0.615131, acc 0.808594, prec 0.0526483, recall 0.837665
2017-12-09T23:28:16.668434: step 711, loss 0.45674, acc 0.851562, prec 0.052626, recall 0.837719
2017-12-09T23:28:17.180582: step 712, loss 0.67096, acc 0.832031, prec 0.0527185, recall 0.838102
2017-12-09T23:28:17.687102: step 713, loss 0.448299, acc 0.875, prec 0.0527229, recall 0.838211
2017-12-09T23:28:18.193135: step 714, loss 0.400208, acc 0.925781, prec 0.0527818, recall 0.838428
2017-12-09T23:28:18.696485: step 715, loss 0.458233, acc 0.902344, prec 0.052794, recall 0.838536
2017-12-09T23:28:19.195234: step 716, loss 0.523163, acc 0.921875, prec 0.0528517, recall 0.838753
2017-12-09T23:28:19.704311: step 717, loss 0.234712, acc 0.941406, prec 0.052855, recall 0.838807
2017-12-09T23:28:20.209207: step 718, loss 0.420336, acc 0.9375, prec 0.0528771, recall 0.838915
2017-12-09T23:28:20.731029: step 719, loss 0.300211, acc 0.90625, prec 0.0529702, recall 0.839238
2017-12-09T23:28:21.233763: step 720, loss 0.229413, acc 0.9375, prec 0.0530123, recall 0.839399
2017-12-09T23:28:21.742389: step 721, loss 0.347453, acc 0.933594, prec 0.0530731, recall 0.839613
2017-12-09T23:28:22.249615: step 722, loss 0.184476, acc 0.957031, prec 0.0530807, recall 0.839667
2017-12-09T23:28:22.753499: step 723, loss 0.284728, acc 0.941406, prec 0.0531039, recall 0.839773
2017-12-09T23:28:23.266432: step 724, loss 0.498234, acc 0.925781, prec 0.0531436, recall 0.839654
2017-12-09T23:28:23.774259: step 725, loss 0.782799, acc 0.957031, prec 0.0531523, recall 0.839428
2017-12-09T23:28:24.275227: step 726, loss 0.132651, acc 0.945312, prec 0.0531964, recall 0.839588
2017-12-09T23:28:24.780146: step 727, loss 0.339669, acc 0.953125, prec 0.0533224, recall 0.83996
2017-12-09T23:28:25.286071: step 728, loss 0.326413, acc 0.910156, prec 0.0533364, recall 0.840066
2017-12-09T23:28:25.790450: step 729, loss 0.21596, acc 0.9375, prec 0.053418, recall 0.840331
2017-12-09T23:28:26.293238: step 730, loss 1.45779, acc 0.90625, prec 0.0534728, recall 0.839987
2017-12-09T23:28:26.800924: step 731, loss 0.267775, acc 0.90625, prec 0.0535055, recall 0.840145
2017-12-09T23:28:27.314315: step 732, loss 0.264228, acc 0.925781, prec 0.0535636, recall 0.840356
2017-12-09T23:28:27.815808: step 733, loss 0.272802, acc 0.921875, prec 0.0536205, recall 0.840565
2017-12-09T23:28:28.314073: step 734, loss 0.369226, acc 0.871094, prec 0.0536429, recall 0.840723
2017-12-09T23:28:28.820367: step 735, loss 0.511157, acc 0.855469, prec 0.0536806, recall 0.840931
2017-12-09T23:28:29.334244: step 736, loss 0.548036, acc 0.898438, prec 0.0537108, recall 0.841088
2017-12-09T23:28:29.838219: step 737, loss 0.368176, acc 0.894531, prec 0.0537794, recall 0.841348
2017-12-09T23:28:30.365936: step 738, loss 0.408342, acc 0.882812, prec 0.0538643, recall 0.841659
2017-12-09T23:28:30.873635: step 739, loss 0.391878, acc 0.855469, prec 0.0539412, recall 0.841968
2017-12-09T23:28:31.381560: step 740, loss 0.53091, acc 0.859375, prec 0.0539796, recall 0.842174
2017-12-09T23:28:31.884973: step 741, loss 0.342067, acc 0.882812, prec 0.0539853, recall 0.842276
2017-12-09T23:28:32.392802: step 742, loss 0.241317, acc 0.898438, prec 0.0540349, recall 0.842481
2017-12-09T23:28:32.896262: step 743, loss 0.410444, acc 0.902344, prec 0.0541053, recall 0.842737
2017-12-09T23:28:33.402080: step 744, loss 0.323797, acc 0.886719, prec 0.0541316, recall 0.84289
2017-12-09T23:28:33.906587: step 745, loss 0.45787, acc 0.894531, prec 0.0541406, recall 0.842991
2017-12-09T23:28:34.408651: step 746, loss 0.394306, acc 0.894531, prec 0.0541692, recall 0.843144
2017-12-09T23:28:34.920314: step 747, loss 0.936515, acc 0.910156, prec 0.054223, recall 0.843074
2017-12-09T23:28:35.429988: step 748, loss 0.345303, acc 0.925781, prec 0.0542605, recall 0.843226
2017-12-09T23:28:35.939237: step 749, loss 0.706915, acc 0.917969, prec 0.0542969, recall 0.843106
2017-12-09T23:28:36.122181: step 750, loss 0.161272, acc 0.941176, prec 0.0542935, recall 0.843106
2017-12-09T23:28:36.635242: step 751, loss 0.15567, acc 0.953125, prec 0.0543388, recall 0.843257
2017-12-09T23:28:37.140275: step 752, loss 0.266136, acc 0.921875, prec 0.0543359, recall 0.843308
2017-12-09T23:28:37.645371: step 753, loss 0.402364, acc 0.941406, prec 0.054417, recall 0.843559
2017-12-09T23:28:38.149977: step 754, loss 0.334947, acc 0.921875, prec 0.0544336, recall 0.84366
2017-12-09T23:28:38.657542: step 755, loss 0.311284, acc 0.933594, prec 0.0545319, recall 0.84396
2017-12-09T23:28:39.164369: step 756, loss 0.255506, acc 0.914062, prec 0.0545462, recall 0.84406
2017-12-09T23:28:39.670509: step 757, loss 0.334017, acc 0.929688, prec 0.0545846, recall 0.84421
2017-12-09T23:28:40.173958: step 758, loss 0.391102, acc 0.902344, prec 0.0546736, recall 0.844508
2017-12-09T23:28:40.680895: step 759, loss 0.789496, acc 0.929688, prec 0.054713, recall 0.844388
2017-12-09T23:28:41.192309: step 760, loss 0.185798, acc 0.945312, prec 0.0547362, recall 0.844487
2017-12-09T23:28:41.693490: step 761, loss 0.30041, acc 0.929688, prec 0.0547744, recall 0.844635
2017-12-09T23:28:42.191259: step 762, loss 0.273272, acc 0.902344, prec 0.0547852, recall 0.844734
2017-12-09T23:28:42.699126: step 763, loss 0.463268, acc 0.914062, prec 0.0548578, recall 0.844981
2017-12-09T23:28:43.200496: step 764, loss 0.349848, acc 0.90625, prec 0.0548696, recall 0.845079
2017-12-09T23:28:43.710056: step 765, loss 0.218689, acc 0.917969, prec 0.0548848, recall 0.845178
2017-12-09T23:28:44.214972: step 766, loss 0.488066, acc 0.945312, prec 0.0549285, recall 0.845057
2017-12-09T23:28:44.720037: step 767, loss 0.295584, acc 0.917969, prec 0.0549826, recall 0.845253
2017-12-09T23:28:45.230430: step 768, loss 0.429, acc 0.921875, prec 0.0550766, recall 0.845546
2017-12-09T23:28:45.738904: step 769, loss 0.445185, acc 0.921875, prec 0.0551511, recall 0.84579
2017-12-09T23:28:46.241142: step 770, loss 0.572553, acc 0.921875, prec 0.0552073, recall 0.845718
2017-12-09T23:28:46.749413: step 771, loss 0.28237, acc 0.90625, prec 0.0552189, recall 0.845815
2017-12-09T23:28:47.257155: step 772, loss 0.332624, acc 0.910156, prec 0.0552704, recall 0.846009
2017-12-09T23:28:47.760105: step 773, loss 0.62878, acc 0.929688, prec 0.0552899, recall 0.84584
2017-12-09T23:28:48.255748: step 774, loss 0.213914, acc 0.933594, prec 0.0553288, recall 0.845985
2017-12-09T23:28:48.757194: step 775, loss 0.303295, acc 0.878906, prec 0.055313, recall 0.846033
2017-12-09T23:28:49.269597: step 776, loss 0.809819, acc 0.921875, prec 0.0554076, recall 0.846058
2017-12-09T23:28:49.782924: step 777, loss 0.416512, acc 0.898438, prec 0.0554941, recall 0.846346
2017-12-09T23:28:50.289622: step 778, loss 0.532449, acc 0.90625, prec 0.0555828, recall 0.846633
2017-12-09T23:28:50.808087: step 779, loss 0.636213, acc 0.871094, prec 0.0556044, recall 0.846513
2017-12-09T23:28:51.319175: step 780, loss 0.494969, acc 0.851562, prec 0.0556384, recall 0.846704
2017-12-09T23:28:51.819157: step 781, loss 0.398632, acc 0.867188, prec 0.0556576, recall 0.846847
2017-12-09T23:28:52.331345: step 782, loss 0.372881, acc 0.863281, prec 0.055695, recall 0.847037
2017-12-09T23:28:52.835651: step 783, loss 0.430455, acc 0.859375, prec 0.0556541, recall 0.847037
2017-12-09T23:28:53.332255: step 784, loss 0.357762, acc 0.886719, prec 0.0556597, recall 0.847132
2017-12-09T23:28:53.833842: step 785, loss 0.3369, acc 0.871094, prec 0.0556607, recall 0.847227
2017-12-09T23:28:54.336818: step 786, loss 0.387519, acc 0.859375, prec 0.0556776, recall 0.847368
2017-12-09T23:28:54.836425: step 787, loss 0.282405, acc 0.894531, prec 0.0557238, recall 0.847557
2017-12-09T23:28:55.335508: step 788, loss 0.350774, acc 0.882812, prec 0.0557282, recall 0.847651
2017-12-09T23:28:55.832082: step 789, loss 0.177759, acc 0.9375, prec 0.0557485, recall 0.847746
2017-12-09T23:28:56.330285: step 790, loss 0.666207, acc 0.925781, prec 0.0557665, recall 0.847578
2017-12-09T23:28:56.836509: step 791, loss 0.18412, acc 0.945312, prec 0.0558081, recall 0.847719
2017-12-09T23:28:57.344428: step 792, loss 0.113434, acc 0.972656, prec 0.0558385, recall 0.847813
2017-12-09T23:28:57.848493: step 793, loss 0.450121, acc 0.957031, prec 0.0558655, recall 0.847645
2017-12-09T23:28:58.356568: step 794, loss 0.111707, acc 0.957031, prec 0.0558722, recall 0.847692
2017-12-09T23:28:58.858064: step 795, loss 0.100464, acc 0.972656, prec 0.0558834, recall 0.847739
2017-12-09T23:28:59.359252: step 796, loss 0.941999, acc 0.957031, prec 0.0559295, recall 0.847619
2017-12-09T23:28:59.863265: step 797, loss 0.537602, acc 0.960938, prec 0.0559958, recall 0.847546
2017-12-09T23:29:00.384798: step 798, loss 0.393092, acc 0.960938, prec 0.0561374, recall 0.847919
2017-12-09T23:29:00.892331: step 799, loss 0.447694, acc 0.957031, prec 0.0562396, recall 0.848198
2017-12-09T23:29:01.394117: step 800, loss 1.55974, acc 0.949219, prec 0.0562844, recall 0.847819
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-800

2017-12-09T23:29:02.833014: step 801, loss 0.175615, acc 0.925781, prec 0.0562819, recall 0.847866
2017-12-09T23:29:03.336098: step 802, loss 0.300959, acc 0.890625, prec 0.0563073, recall 0.848005
2017-12-09T23:29:03.839798: step 803, loss 0.337566, acc 0.886719, prec 0.0563315, recall 0.848144
2017-12-09T23:29:04.342024: step 804, loss 0.418728, acc 0.855469, prec 0.0563275, recall 0.848236
2017-12-09T23:29:04.847360: step 805, loss 0.572903, acc 0.851562, prec 0.0563605, recall 0.84842
2017-12-09T23:29:05.351345: step 806, loss 0.437179, acc 0.832031, prec 0.0563877, recall 0.848604
2017-12-09T23:29:05.864343: step 807, loss 0.624418, acc 0.796875, prec 0.0564047, recall 0.848788
2017-12-09T23:29:06.374636: step 808, loss 0.468413, acc 0.816406, prec 0.0564652, recall 0.849062
2017-12-09T23:29:06.877920: step 809, loss 0.481341, acc 0.894531, prec 0.0566243, recall 0.849517
2017-12-09T23:29:07.379011: step 810, loss 0.607581, acc 0.839844, prec 0.0567102, recall 0.849835
2017-12-09T23:29:07.885864: step 811, loss 0.423588, acc 0.867188, prec 0.0567094, recall 0.849925
2017-12-09T23:29:08.393027: step 812, loss 0.466157, acc 0.847656, prec 0.0567407, recall 0.850105
2017-12-09T23:29:08.898885: step 813, loss 0.463386, acc 0.875, prec 0.0567988, recall 0.85033
2017-12-09T23:29:09.398799: step 814, loss 0.451818, acc 0.902344, prec 0.0568459, recall 0.850509
2017-12-09T23:29:09.911110: step 815, loss 0.243727, acc 0.914062, prec 0.0569342, recall 0.850778
2017-12-09T23:29:10.411524: step 816, loss 0.27002, acc 0.90625, prec 0.0569446, recall 0.850867
2017-12-09T23:29:10.913307: step 817, loss 0.294916, acc 0.898438, prec 0.0570092, recall 0.851089
2017-12-09T23:29:11.418608: step 818, loss 0.288246, acc 0.917969, prec 0.057023, recall 0.851178
2017-12-09T23:29:11.923989: step 819, loss 0.574074, acc 0.925781, prec 0.0570778, recall 0.851102
2017-12-09T23:29:12.427314: step 820, loss 0.28596, acc 0.941406, prec 0.0570795, recall 0.851146
2017-12-09T23:29:12.926162: step 821, loss 0.295855, acc 0.957031, prec 0.0571423, recall 0.851323
2017-12-09T23:29:13.432359: step 822, loss 0.411442, acc 0.949219, prec 0.0571463, recall 0.851367
2017-12-09T23:29:13.932192: step 823, loss 0.28119, acc 0.921875, prec 0.0571987, recall 0.851544
2017-12-09T23:29:14.435091: step 824, loss 0.203531, acc 0.941406, prec 0.057238, recall 0.851676
2017-12-09T23:29:14.943509: step 825, loss 0.156889, acc 0.949219, prec 0.0572607, recall 0.851764
2017-12-09T23:29:15.450191: step 826, loss 1.98599, acc 0.933594, prec 0.0573176, recall 0.851687
2017-12-09T23:29:15.961306: step 827, loss 0.150894, acc 0.957031, prec 0.0573614, recall 0.851819
2017-12-09T23:29:16.468081: step 828, loss 1.33596, acc 0.941406, prec 0.0573454, recall 0.851567
2017-12-09T23:29:16.967832: step 829, loss 0.326158, acc 0.941406, prec 0.0574784, recall 0.851917
2017-12-09T23:29:17.469974: step 830, loss 0.391772, acc 0.90625, prec 0.0575072, recall 0.852048
2017-12-09T23:29:17.972235: step 831, loss 0.644121, acc 0.90625, prec 0.0575746, recall 0.852015
2017-12-09T23:29:18.485004: step 832, loss 0.309926, acc 0.917969, prec 0.0576442, recall 0.852233
2017-12-09T23:29:18.989117: step 833, loss 0.730077, acc 0.886719, prec 0.0577244, recall 0.852243
2017-12-09T23:29:19.492166: step 834, loss 0.589582, acc 0.851562, prec 0.0577369, recall 0.852373
2017-12-09T23:29:19.991928: step 835, loss 0.29225, acc 0.894531, prec 0.0578181, recall 0.852632
2017-12-09T23:29:20.505650: step 836, loss 0.362118, acc 0.867188, prec 0.0578165, recall 0.852718
2017-12-09T23:29:21.023432: step 837, loss 0.456451, acc 0.824219, prec 0.0577837, recall 0.852761
2017-12-09T23:29:21.531060: step 838, loss 0.41508, acc 0.84375, prec 0.0577752, recall 0.852847
2017-12-09T23:29:22.031429: step 839, loss 0.56429, acc 0.828125, prec 0.0578181, recall 0.853061
2017-12-09T23:29:22.534836: step 840, loss 0.427539, acc 0.832031, prec 0.0578062, recall 0.853147
2017-12-09T23:29:23.037429: step 841, loss 0.407868, acc 0.867188, prec 0.0578232, recall 0.853275
2017-12-09T23:29:23.547200: step 842, loss 0.373788, acc 0.914062, prec 0.0578724, recall 0.853446
2017-12-09T23:29:24.050151: step 843, loss 0.398024, acc 0.882812, prec 0.0579125, recall 0.853616
2017-12-09T23:29:24.551674: step 844, loss 0.319827, acc 0.902344, prec 0.0579396, recall 0.853743
2017-12-09T23:29:25.050632: step 845, loss 0.618658, acc 0.90625, prec 0.0580061, recall 0.853708
2017-12-09T23:29:25.553473: step 846, loss 0.218466, acc 0.917969, prec 0.0580192, recall 0.853793
2017-12-09T23:29:26.057483: step 847, loss 0.692398, acc 0.882812, prec 0.0579861, recall 0.853546
2017-12-09T23:29:26.566210: step 848, loss 0.223305, acc 0.9375, prec 0.0580419, recall 0.853715
2017-12-09T23:29:27.076758: step 849, loss 0.535352, acc 0.972656, prec 0.0580721, recall 0.853553
2017-12-09T23:29:27.580247: step 850, loss 0.331063, acc 0.929688, prec 0.0581256, recall 0.853722
2017-12-09T23:29:28.085064: step 851, loss 0.416758, acc 0.96875, prec 0.0582459, recall 0.854017
2017-12-09T23:29:28.589261: step 852, loss 0.191513, acc 0.949219, prec 0.058305, recall 0.854185
2017-12-09T23:29:29.093564: step 853, loss 0.254614, acc 0.941406, prec 0.0583618, recall 0.854352
2017-12-09T23:29:29.595818: step 854, loss 0.484584, acc 0.925781, prec 0.0584324, recall 0.854561
2017-12-09T23:29:30.107776: step 855, loss 0.258328, acc 0.925781, prec 0.0585768, recall 0.854936
2017-12-09T23:29:30.620283: step 856, loss 0.196598, acc 0.921875, prec 0.0585722, recall 0.854977
2017-12-09T23:29:31.125609: step 857, loss 0.293394, acc 0.90625, prec 0.0586, recall 0.855101
2017-12-09T23:29:31.622186: step 858, loss 0.28939, acc 0.933594, prec 0.0586358, recall 0.855226
2017-12-09T23:29:32.130926: step 859, loss 0.313217, acc 0.910156, prec 0.0586831, recall 0.855391
2017-12-09T23:29:32.638720: step 860, loss 0.184549, acc 0.941406, prec 0.0587211, recall 0.855514
2017-12-09T23:29:33.152022: step 861, loss 0.555742, acc 0.914062, prec 0.0587522, recall 0.855394
2017-12-09T23:29:33.651877: step 862, loss 0.419172, acc 0.933594, prec 0.0587879, recall 0.855518
2017-12-09T23:29:34.153065: step 863, loss 0.947122, acc 0.960938, prec 0.0588511, recall 0.855439
2017-12-09T23:29:34.659782: step 864, loss 0.242543, acc 0.898438, prec 0.0588764, recall 0.855562
2017-12-09T23:29:35.168669: step 865, loss 0.772685, acc 0.902344, prec 0.0589223, recall 0.855483
2017-12-09T23:29:35.690118: step 866, loss 0.317754, acc 0.914062, prec 0.0589888, recall 0.855688
2017-12-09T23:29:36.195138: step 867, loss 0.313869, acc 0.902344, prec 0.0590517, recall 0.855891
2017-12-09T23:29:36.699231: step 868, loss 0.253959, acc 0.910156, prec 0.0590803, recall 0.856014
2017-12-09T23:29:37.218961: step 869, loss 0.248335, acc 0.925781, prec 0.0591684, recall 0.856257
2017-12-09T23:29:37.719436: step 870, loss 0.527564, acc 0.890625, prec 0.059246, recall 0.8565
2017-12-09T23:29:38.229442: step 871, loss 0.389023, acc 0.914062, prec 0.0592938, recall 0.856661
2017-12-09T23:29:38.734928: step 872, loss 0.26606, acc 0.894531, prec 0.0593359, recall 0.856822
2017-12-09T23:29:39.232132: step 873, loss 0.332076, acc 0.898438, prec 0.0594156, recall 0.857063
2017-12-09T23:29:39.731666: step 874, loss 0.649943, acc 0.882812, prec 0.0594186, recall 0.856903
2017-12-09T23:29:39.914475: step 875, loss 0.112838, acc 0.960784, prec 0.0594163, recall 0.856903
2017-12-09T23:29:40.419762: step 876, loss 0.640202, acc 0.902344, prec 0.0594252, recall 0.856743
2017-12-09T23:29:40.918150: step 877, loss 0.210107, acc 0.9375, prec 0.059425, recall 0.856783
2017-12-09T23:29:41.425535: step 878, loss 0.289103, acc 0.941406, prec 0.0595171, recall 0.857023
2017-12-09T23:29:41.927538: step 879, loss 0.278036, acc 0.917969, prec 0.059584, recall 0.857223
2017-12-09T23:29:42.432783: step 880, loss 0.304463, acc 0.910156, prec 0.0596304, recall 0.857382
2017-12-09T23:29:42.942722: step 881, loss 0.266497, acc 0.890625, prec 0.0596709, recall 0.85754
2017-12-09T23:29:43.457291: step 882, loss 0.340784, acc 0.921875, prec 0.0597569, recall 0.857778
2017-12-09T23:29:43.958109: step 883, loss 0.306731, acc 0.925781, prec 0.0597895, recall 0.857896
2017-12-09T23:29:44.464682: step 884, loss 0.170902, acc 0.917969, prec 0.0597834, recall 0.857936
2017-12-09T23:29:44.967463: step 885, loss 0.494171, acc 0.941406, prec 0.0598388, recall 0.858093
2017-12-09T23:29:45.483645: step 886, loss 0.239512, acc 0.9375, prec 0.059893, recall 0.85825
2017-12-09T23:29:45.994789: step 887, loss 0.579705, acc 0.933594, prec 0.0599641, recall 0.858446
2017-12-09T23:29:46.506273: step 888, loss 0.466317, acc 0.929688, prec 0.0600521, recall 0.858681
2017-12-09T23:29:47.009112: step 889, loss 0.272072, acc 0.910156, prec 0.060098, recall 0.858837
2017-12-09T23:29:47.515113: step 890, loss 0.227185, acc 0.910156, prec 0.0601439, recall 0.858992
2017-12-09T23:29:48.026501: step 891, loss 0.24555, acc 0.902344, prec 0.0601873, recall 0.859147
2017-12-09T23:29:48.533521: step 892, loss 0.261513, acc 0.925781, prec 0.0602558, recall 0.859341
2017-12-09T23:29:49.037980: step 893, loss 0.323661, acc 0.929688, prec 0.0603254, recall 0.859534
2017-12-09T23:29:49.545288: step 894, loss 0.2076, acc 0.914062, prec 0.0603722, recall 0.859688
2017-12-09T23:29:50.046542: step 895, loss 0.240232, acc 0.925781, prec 0.0604224, recall 0.859841
2017-12-09T23:29:50.564086: step 896, loss 0.221948, acc 0.933594, prec 0.060493, recall 0.860033
2017-12-09T23:29:51.071394: step 897, loss 0.203481, acc 0.925781, prec 0.0605251, recall 0.860147
2017-12-09T23:29:51.577020: step 898, loss 0.37879, acc 0.902344, prec 0.0606043, recall 0.860376
2017-12-09T23:29:52.084056: step 899, loss 0.843842, acc 0.933594, prec 0.0606759, recall 0.860332
2017-12-09T23:29:52.590182: step 900, loss 0.161979, acc 0.941406, prec 0.0606945, recall 0.860408

Evaluation:
2017-12-09T23:29:57.200537: step 900, loss 1.95226, acc 0.947448, prec 0.0614613, recall 0.845792

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-900

2017-12-09T23:29:58.658746: step 901, loss 0.349947, acc 0.941406, prec 0.0615507, recall 0.846033
2017-12-09T23:29:59.163782: step 902, loss 0.234584, acc 0.941406, prec 0.0616044, recall 0.846194
2017-12-09T23:29:59.671336: step 903, loss 0.191297, acc 0.941406, prec 0.0617115, recall 0.846474
2017-12-09T23:30:00.180248: step 904, loss 0.556721, acc 0.929688, prec 0.0617628, recall 0.846414
2017-12-09T23:30:00.694248: step 905, loss 0.373791, acc 0.945312, prec 0.061782, recall 0.846493
2017-12-09T23:30:01.207888: step 906, loss 0.45983, acc 0.925781, prec 0.0618486, recall 0.846693
2017-12-09T23:30:01.712080: step 907, loss 0.281312, acc 0.941406, prec 0.0618844, recall 0.846812
2017-12-09T23:30:02.210056: step 908, loss 0.203393, acc 0.929688, prec 0.0619343, recall 0.84697
2017-12-09T23:30:02.713827: step 909, loss 0.185887, acc 0.9375, prec 0.0619688, recall 0.847089
2017-12-09T23:30:03.219036: step 910, loss 0.258094, acc 0.933594, prec 0.0620554, recall 0.847326
2017-12-09T23:30:03.723610: step 911, loss 0.245796, acc 0.917969, prec 0.0620662, recall 0.847405
2017-12-09T23:30:04.226991: step 912, loss 0.22029, acc 0.9375, prec 0.0621006, recall 0.847523
2017-12-09T23:30:04.732511: step 913, loss 0.160268, acc 0.953125, prec 0.062122, recall 0.847602
2017-12-09T23:30:05.231729: step 914, loss 0.195862, acc 0.929688, prec 0.0621186, recall 0.847641
2017-12-09T23:30:05.751543: step 915, loss 0.895132, acc 0.921875, prec 0.0621495, recall 0.847541
2017-12-09T23:30:06.264258: step 916, loss 0.806458, acc 0.953125, prec 0.0621897, recall 0.84744
2017-12-09T23:30:06.769562: step 917, loss 0.297219, acc 0.949219, prec 0.0622629, recall 0.847636
2017-12-09T23:30:07.275517: step 918, loss 0.174415, acc 0.929688, prec 0.0622771, recall 0.847714
2017-12-09T23:30:07.774799: step 919, loss 0.278845, acc 0.898438, prec 0.062282, recall 0.847793
2017-12-09T23:30:08.275390: step 920, loss 0.174318, acc 0.933594, prec 0.0622974, recall 0.847871
2017-12-09T23:30:08.777228: step 921, loss 0.27204, acc 0.890625, prec 0.0623175, recall 0.847988
2017-12-09T23:30:09.280326: step 922, loss 0.237512, acc 0.917969, prec 0.0623635, recall 0.848143
2017-12-09T23:30:09.789296: step 923, loss 0.332089, acc 0.902344, prec 0.0624047, recall 0.848299
2017-12-09T23:30:10.289686: step 924, loss 0.43666, acc 0.886719, prec 0.0624589, recall 0.848493
2017-12-09T23:30:10.792916: step 925, loss 0.14529, acc 0.949219, prec 0.0624788, recall 0.84857
2017-12-09T23:30:11.288325: step 926, loss 0.205512, acc 0.914062, prec 0.0624883, recall 0.848647
2017-12-09T23:30:11.793922: step 927, loss 0.460474, acc 0.9375, prec 0.0625235, recall 0.848547
2017-12-09T23:30:12.300179: step 928, loss 0.278385, acc 0.941406, prec 0.0626115, recall 0.848778
2017-12-09T23:30:12.803929: step 929, loss 0.534231, acc 0.960938, prec 0.0627065, recall 0.848793
2017-12-09T23:30:13.317776: step 930, loss 0.910714, acc 0.929688, prec 0.0627393, recall 0.848693
2017-12-09T23:30:13.832364: step 931, loss 0.307414, acc 0.917969, prec 0.0628201, recall 0.848923
2017-12-09T23:30:14.340941: step 932, loss 0.435805, acc 0.929688, prec 0.0628516, recall 0.849037
2017-12-09T23:30:14.845259: step 933, loss 0.191693, acc 0.9375, prec 0.0629381, recall 0.849267
2017-12-09T23:30:15.349106: step 934, loss 0.732331, acc 0.917969, prec 0.0629672, recall 0.849166
2017-12-09T23:30:15.852966: step 935, loss 0.409805, acc 0.886719, prec 0.0630383, recall 0.849395
2017-12-09T23:30:16.354513: step 936, loss 0.291974, acc 0.933594, prec 0.0630708, recall 0.849508
2017-12-09T23:30:16.855919: step 937, loss 0.258941, acc 0.882812, prec 0.0630354, recall 0.849508
2017-12-09T23:30:17.366149: step 938, loss 0.274121, acc 0.914062, prec 0.0630445, recall 0.849584
2017-12-09T23:30:17.866598: step 939, loss 0.245036, acc 0.910156, prec 0.0630349, recall 0.849622
2017-12-09T23:30:18.374978: step 940, loss 0.24649, acc 0.921875, prec 0.0630639, recall 0.849736
2017-12-09T23:30:18.874654: step 941, loss 0.311064, acc 0.890625, prec 0.0630484, recall 0.849774
2017-12-09T23:30:19.375435: step 942, loss 0.318653, acc 0.875, prec 0.0630632, recall 0.849887
2017-12-09T23:30:19.881444: step 943, loss 0.330992, acc 0.902344, prec 0.0631387, recall 0.850113
2017-12-09T23:30:20.389172: step 944, loss 0.233804, acc 0.945312, prec 0.063192, recall 0.850263
2017-12-09T23:30:20.906975: step 945, loss 0.204626, acc 0.925781, prec 0.0632744, recall 0.850488
2017-12-09T23:30:21.414526: step 946, loss 0.232757, acc 0.9375, prec 0.0632904, recall 0.850563
2017-12-09T23:30:21.915962: step 947, loss 0.386948, acc 0.945312, prec 0.0633263, recall 0.850675
2017-12-09T23:30:22.422164: step 948, loss 0.536478, acc 0.941406, prec 0.0633783, recall 0.850825
2017-12-09T23:30:22.926302: step 949, loss 0.283494, acc 0.941406, prec 0.0634652, recall 0.851048
2017-12-09T23:30:23.428312: step 950, loss 1.15307, acc 0.945312, prec 0.0635381, recall 0.850809
2017-12-09T23:30:23.932363: step 951, loss 0.156596, acc 0.960938, prec 0.0635611, recall 0.850884
2017-12-09T23:30:24.446028: step 952, loss 0.226385, acc 0.914062, prec 0.0635526, recall 0.850921
2017-12-09T23:30:24.948770: step 953, loss 0.336906, acc 0.894531, prec 0.0635381, recall 0.850958
2017-12-09T23:30:25.445800: step 954, loss 0.272832, acc 0.902344, prec 0.0635433, recall 0.851032
2017-12-09T23:30:25.951590: step 955, loss 0.198695, acc 0.929688, prec 0.0635743, recall 0.851143
2017-12-09T23:30:26.452577: step 956, loss 0.196725, acc 0.914062, prec 0.0635657, recall 0.85118
2017-12-09T23:30:26.959103: step 957, loss 0.279487, acc 0.894531, prec 0.0635686, recall 0.851254
2017-12-09T23:30:27.459323: step 958, loss 0.123174, acc 0.960938, prec 0.0636263, recall 0.851402
2017-12-09T23:30:27.968656: step 959, loss 0.387876, acc 0.894531, prec 0.0637679, recall 0.851769
2017-12-09T23:30:28.466132: step 960, loss 0.224344, acc 0.945312, prec 0.0638207, recall 0.851916
2017-12-09T23:30:28.973066: step 961, loss 0.224527, acc 0.941406, prec 0.063959, recall 0.852245
2017-12-09T23:30:29.479683: step 962, loss 0.286651, acc 0.945312, prec 0.0639944, recall 0.852354
2017-12-09T23:30:29.982982: step 963, loss 0.249334, acc 0.945312, prec 0.0640297, recall 0.852463
2017-12-09T23:30:30.504994: step 964, loss 0.199736, acc 0.960938, prec 0.0640872, recall 0.852608
2017-12-09T23:30:31.007204: step 965, loss 0.516015, acc 0.960938, prec 0.0641445, recall 0.852753
2017-12-09T23:30:31.507903: step 966, loss 0.11268, acc 0.964844, prec 0.0641685, recall 0.852826
2017-12-09T23:30:32.018520: step 967, loss 0.131441, acc 0.949219, prec 0.0641703, recall 0.852862
2017-12-09T23:30:32.520165: step 968, loss 0.170872, acc 0.945312, prec 0.0642229, recall 0.853006
2017-12-09T23:30:33.027229: step 969, loss 0.0979706, acc 0.964844, prec 0.0642295, recall 0.853042
2017-12-09T23:30:33.528386: step 970, loss 0.745432, acc 0.941406, prec 0.0642475, recall 0.852905
2017-12-09T23:30:34.029617: step 971, loss 0.128549, acc 0.964844, prec 0.0643059, recall 0.853049
2017-12-09T23:30:34.533729: step 972, loss 0.264037, acc 0.9375, prec 0.0643387, recall 0.853157
2017-12-09T23:30:35.040909: step 973, loss 0.164712, acc 0.96875, prec 0.0643983, recall 0.853301
2017-12-09T23:30:35.558317: step 974, loss 0.305417, acc 0.925781, prec 0.0644102, recall 0.853372
2017-12-09T23:30:36.061935: step 975, loss 0.172859, acc 0.9375, prec 0.0644085, recall 0.853408
2017-12-09T23:30:36.570363: step 976, loss 0.166139, acc 0.949219, prec 0.0644275, recall 0.85348
2017-12-09T23:30:37.076245: step 977, loss 0.198335, acc 0.929688, prec 0.0644406, recall 0.853551
2017-12-09T23:30:37.581337: step 978, loss 0.478694, acc 0.933594, prec 0.0644894, recall 0.853694
2017-12-09T23:30:38.085012: step 979, loss 0.443569, acc 0.949219, prec 0.0645429, recall 0.853837
2017-12-09T23:30:38.589251: step 980, loss 0.270293, acc 0.917969, prec 0.0645524, recall 0.853908
2017-12-09T23:30:39.098464: step 981, loss 0.453608, acc 0.921875, prec 0.0646146, recall 0.854086
2017-12-09T23:30:39.608203: step 982, loss 0.355645, acc 0.921875, prec 0.0647285, recall 0.854369
2017-12-09T23:30:40.128019: step 983, loss 0.322779, acc 0.902344, prec 0.0647331, recall 0.85444
2017-12-09T23:30:40.642428: step 984, loss 0.210194, acc 0.929688, prec 0.0647289, recall 0.854475
2017-12-09T23:30:41.146776: step 985, loss 0.270981, acc 0.90625, prec 0.0647519, recall 0.854581
2017-12-09T23:30:41.648645: step 986, loss 0.333569, acc 0.921875, prec 0.064814, recall 0.854757
2017-12-09T23:30:42.147609: step 987, loss 0.171285, acc 0.941406, prec 0.0648476, recall 0.854862
2017-12-09T23:30:42.645138: step 988, loss 0.310068, acc 0.917969, prec 0.0649084, recall 0.855037
2017-12-09T23:30:43.141776: step 989, loss 0.247933, acc 0.949219, prec 0.0649443, recall 0.855142
2017-12-09T23:30:43.656325: step 990, loss 0.421254, acc 0.9375, prec 0.0650281, recall 0.855352
2017-12-09T23:30:44.161562: step 991, loss 0.257285, acc 0.921875, prec 0.0650557, recall 0.855457
2017-12-09T23:30:44.667947: step 992, loss 0.223597, acc 0.925781, prec 0.0651187, recall 0.85563
2017-12-09T23:30:45.174830: step 993, loss 0.195921, acc 0.9375, prec 0.0652194, recall 0.855873
2017-12-09T23:30:45.682983: step 994, loss 0.195664, acc 0.941406, prec 0.0652528, recall 0.855977
2017-12-09T23:30:46.190327: step 995, loss 0.244128, acc 0.941406, prec 0.0652862, recall 0.856081
2017-12-09T23:30:46.694114: step 996, loss 0.347411, acc 0.949219, prec 0.0653561, recall 0.856253
2017-12-09T23:30:47.208912: step 997, loss 0.289196, acc 0.941406, prec 0.0654407, recall 0.856459
2017-12-09T23:30:47.712124: step 998, loss 0.181736, acc 0.933594, prec 0.0654716, recall 0.856562
2017-12-09T23:30:48.219865: step 999, loss 0.309613, acc 0.949219, prec 0.0655414, recall 0.856734
2017-12-09T23:30:48.404024: step 1000, loss 3.24625, acc 0.941176, prec 0.065539, recall 0.856529
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-1000

2017-12-09T23:30:49.883075: step 1001, loss 0.139524, acc 0.949219, prec 0.0655747, recall 0.856632
2017-12-09T23:30:50.384745: step 1002, loss 0.220178, acc 0.925781, prec 0.0656713, recall 0.856871
2017-12-09T23:30:50.892971: step 1003, loss 0.289385, acc 0.917969, prec 0.0656632, recall 0.856905
2017-12-09T23:30:51.398635: step 1004, loss 0.461774, acc 0.847656, prec 0.0656676, recall 0.857007
2017-12-09T23:30:51.899072: step 1005, loss 0.443635, acc 0.855469, prec 0.0656234, recall 0.857007
2017-12-09T23:30:52.397939: step 1006, loss 0.556956, acc 0.820312, prec 0.0656024, recall 0.857075
2017-12-09T23:30:52.902942: step 1007, loss 0.490789, acc 0.832031, prec 0.0656531, recall 0.857279
2017-12-09T23:30:53.411603: step 1008, loss 0.668088, acc 0.796875, prec 0.065608, recall 0.857312
2017-12-09T23:30:53.920121: step 1009, loss 0.365939, acc 0.878906, prec 0.065622, recall 0.857414
2017-12-09T23:30:54.431359: step 1010, loss 0.44321, acc 0.894531, prec 0.0655898, recall 0.857414
2017-12-09T23:30:54.938741: step 1011, loss 0.336539, acc 0.882812, prec 0.0656219, recall 0.857549
2017-12-09T23:30:55.443341: step 1012, loss 1.06412, acc 0.898438, prec 0.0657277, recall 0.857616
2017-12-09T23:30:55.945480: step 1013, loss 0.20175, acc 0.941406, prec 0.0657606, recall 0.857717
2017-12-09T23:30:56.441382: step 1014, loss 0.476084, acc 0.894531, prec 0.06583, recall 0.857918
2017-12-09T23:30:56.951480: step 1015, loss 0.475322, acc 0.910156, prec 0.0659378, recall 0.858186
2017-12-09T23:30:57.462597: step 1016, loss 0.201203, acc 0.921875, prec 0.0659816, recall 0.85832
2017-12-09T23:30:57.963732: step 1017, loss 1.38423, acc 0.902344, prec 0.0660374, recall 0.858284
2017-12-09T23:30:58.471403: step 1018, loss 0.21217, acc 0.914062, prec 0.0660449, recall 0.858351
2017-12-09T23:30:58.975320: step 1019, loss 0.528328, acc 0.890625, prec 0.0661296, recall 0.858584
2017-12-09T23:30:59.481196: step 1020, loss 0.241344, acc 0.929688, prec 0.0661418, recall 0.85865
2017-12-09T23:30:59.983797: step 1021, loss 0.286219, acc 0.925781, prec 0.0661528, recall 0.858716
2017-12-09T23:31:00.512848: step 1022, loss 0.244244, acc 0.945312, prec 0.0661698, recall 0.858782
2017-12-09T23:31:01.019781: step 1023, loss 0.165689, acc 0.960938, prec 0.0661916, recall 0.858848
2017-12-09T23:31:01.527191: step 1024, loss 0.216096, acc 0.9375, prec 0.066223, recall 0.858947
2017-12-09T23:31:02.032945: step 1025, loss 0.241651, acc 0.933594, prec 0.0662532, recall 0.859046
2017-12-09T23:31:02.539854: step 1026, loss 0.239276, acc 0.910156, prec 0.0662426, recall 0.859079
2017-12-09T23:31:03.052614: step 1027, loss 0.229638, acc 0.929688, prec 0.0662547, recall 0.859145
2017-12-09T23:31:03.558657: step 1028, loss 0.666582, acc 0.917969, prec 0.0662309, recall 0.858944
2017-12-09T23:31:04.074593: step 1029, loss 0.227204, acc 0.933594, prec 0.0662947, recall 0.859109
2017-12-09T23:31:04.578478: step 1030, loss 0.218522, acc 0.941406, prec 0.0663608, recall 0.859273
2017-12-09T23:31:05.094841: step 1031, loss 0.604798, acc 0.945312, prec 0.0664616, recall 0.859502
2017-12-09T23:31:05.609992: step 1032, loss 0.141119, acc 0.957031, prec 0.066482, recall 0.859568
2017-12-09T23:31:06.116575: step 1033, loss 0.125301, acc 0.945312, prec 0.0665324, recall 0.859698
2017-12-09T23:31:06.621212: step 1034, loss 0.177095, acc 0.953125, prec 0.0665684, recall 0.859796
2017-12-09T23:31:07.126369: step 1035, loss 0.158946, acc 0.945312, prec 0.0665684, recall 0.859828
2017-12-09T23:31:07.629384: step 1036, loss 0.226147, acc 0.949219, prec 0.06662, recall 0.859958
2017-12-09T23:31:08.133764: step 1037, loss 0.195012, acc 0.972656, prec 0.0666619, recall 0.860056
2017-12-09T23:31:08.646897: step 1038, loss 0.271927, acc 0.949219, prec 0.0667301, recall 0.860218
2017-12-09T23:31:09.146823: step 1039, loss 0.20764, acc 0.949219, prec 0.066748, recall 0.860282
2017-12-09T23:31:09.644792: step 1040, loss 0.183242, acc 0.949219, prec 0.0667827, recall 0.860379
2017-12-09T23:31:10.149215: step 1041, loss 0.426088, acc 0.96875, prec 0.0668401, recall 0.860508
2017-12-09T23:31:10.654067: step 1042, loss 0.3582, acc 0.9375, prec 0.0668544, recall 0.860572
2017-12-09T23:31:11.161315: step 1043, loss 0.159317, acc 0.9375, prec 0.0668519, recall 0.860605
2017-12-09T23:31:11.663747: step 1044, loss 0.17056, acc 0.957031, prec 0.0669056, recall 0.860733
2017-12-09T23:31:12.170043: step 1045, loss 0.448405, acc 0.960938, prec 0.0669605, recall 0.860862
2017-12-09T23:31:12.679499: step 1046, loss 0.26795, acc 0.925781, prec 0.0669712, recall 0.860926
2017-12-09T23:31:13.183652: step 1047, loss 0.160095, acc 0.953125, prec 0.0669902, recall 0.86099
2017-12-09T23:31:13.684944: step 1048, loss 0.269761, acc 0.9375, prec 0.0670211, recall 0.861086
2017-12-09T23:31:14.187252: step 1049, loss 0.570152, acc 0.9375, prec 0.0671021, recall 0.861277
2017-12-09T23:31:14.687254: step 1050, loss 0.37617, acc 0.921875, prec 0.0671281, recall 0.861373
2017-12-09T23:31:15.188601: step 1051, loss 0.566738, acc 0.933594, prec 0.0671911, recall 0.861531
2017-12-09T23:31:15.697135: step 1052, loss 0.312649, acc 0.914062, prec 0.0672147, recall 0.861627
2017-12-09T23:31:16.199932: step 1053, loss 0.554798, acc 0.9375, prec 0.0673122, recall 0.861848
2017-12-09T23:31:16.703716: step 1054, loss 0.443604, acc 0.882812, prec 0.0673594, recall 0.862006
2017-12-09T23:31:17.197564: step 1055, loss 0.356267, acc 0.859375, prec 0.0673827, recall 0.862132
2017-12-09T23:31:17.705022: step 1056, loss 0.40648, acc 0.835938, prec 0.0673821, recall 0.862226
2017-12-09T23:31:18.206220: step 1057, loss 0.41506, acc 0.878906, prec 0.0673947, recall 0.86232
2017-12-09T23:31:18.710499: step 1058, loss 0.455533, acc 0.84375, prec 0.0674629, recall 0.86254
2017-12-09T23:31:19.216036: step 1059, loss 0.347549, acc 0.886719, prec 0.0675277, recall 0.862727
2017-12-09T23:31:19.713868: step 1060, loss 0.43418, acc 0.84375, prec 0.067546, recall 0.862852
2017-12-09T23:31:20.217558: step 1061, loss 0.375168, acc 0.886719, prec 0.0676271, recall 0.86307
2017-12-09T23:31:20.738629: step 1062, loss 0.328755, acc 0.914062, prec 0.0676503, recall 0.863163
2017-12-09T23:31:21.236179: step 1063, loss 0.294783, acc 0.90625, prec 0.0676877, recall 0.863287
2017-12-09T23:31:21.740917: step 1064, loss 0.195873, acc 0.929688, prec 0.0677323, recall 0.86341
2017-12-09T23:31:22.240051: step 1065, loss 0.166093, acc 0.9375, prec 0.0677461, recall 0.863472
2017-12-09T23:31:22.737640: step 1066, loss 0.192404, acc 0.9375, prec 0.0678095, recall 0.863626
2017-12-09T23:31:23.248189: step 1067, loss 0.394459, acc 0.925781, prec 0.0678693, recall 0.86378
2017-12-09T23:31:23.755567: step 1068, loss 0.174188, acc 0.960938, prec 0.0679398, recall 0.863933
2017-12-09T23:31:24.256468: step 1069, loss 0.148925, acc 0.949219, prec 0.0679407, recall 0.863964
2017-12-09T23:31:24.762547: step 1070, loss 0.116166, acc 0.972656, prec 0.0679983, recall 0.864086
2017-12-09T23:31:25.273100: step 1071, loss 0.528778, acc 0.976562, prec 0.0680253, recall 0.863953
2017-12-09T23:31:25.774190: step 1072, loss 0.859623, acc 0.96875, prec 0.0680828, recall 0.863881
2017-12-09T23:31:26.283873: step 1073, loss 0.953075, acc 0.964844, prec 0.0681062, recall 0.863749
2017-12-09T23:31:26.793902: step 1074, loss 0.107375, acc 0.972656, prec 0.0681472, recall 0.86384
2017-12-09T23:31:27.291024: step 1075, loss 0.181654, acc 0.9375, prec 0.0681444, recall 0.863871
2017-12-09T23:31:27.793918: step 1076, loss 0.49537, acc 0.925781, prec 0.0681874, recall 0.863993
2017-12-09T23:31:28.300344: step 1077, loss 0.249879, acc 0.929688, prec 0.0681987, recall 0.864054
2017-12-09T23:31:28.811346: step 1078, loss 0.178508, acc 0.953125, prec 0.0682336, recall 0.864145
2017-12-09T23:31:29.320962: step 1079, loss 0.184603, acc 0.925781, prec 0.0682601, recall 0.864236
2017-12-09T23:31:29.817220: step 1080, loss 0.766898, acc 0.917969, prec 0.0683183, recall 0.864195
2017-12-09T23:31:30.339374: step 1081, loss 0.528908, acc 0.941406, prec 0.0684482, recall 0.864467
2017-12-09T23:31:30.847786: step 1082, loss 0.370533, acc 0.917969, prec 0.0684886, recall 0.864588
2017-12-09T23:31:31.350558: step 1083, loss 0.33486, acc 0.902344, prec 0.0685405, recall 0.864739
2017-12-09T23:31:31.861096: step 1084, loss 0.358951, acc 0.894531, prec 0.0685571, recall 0.864829
2017-12-09T23:31:32.361703: step 1085, loss 0.376482, acc 0.871094, prec 0.0685993, recall 0.864979
2017-12-09T23:31:32.870346: step 1086, loss 0.335724, acc 0.886719, prec 0.0686299, recall 0.865099
2017-12-09T23:31:33.374105: step 1087, loss 0.459958, acc 0.882812, prec 0.068692, recall 0.865278
2017-12-09T23:31:33.885545: step 1088, loss 0.296149, acc 0.875, prec 0.0686861, recall 0.865338
2017-12-09T23:31:34.389194: step 1089, loss 0.39881, acc 0.871094, prec 0.0686626, recall 0.865368
2017-12-09T23:31:34.892845: step 1090, loss 0.291634, acc 0.894531, prec 0.06863, recall 0.865368
2017-12-09T23:31:35.397253: step 1091, loss 0.5062, acc 0.949219, prec 0.0687125, recall 0.865546
2017-12-09T23:31:35.912107: step 1092, loss 0.231931, acc 0.90625, prec 0.0687652, recall 0.865695
2017-12-09T23:31:36.424114: step 1093, loss 0.203272, acc 0.953125, prec 0.0688161, recall 0.865813
2017-12-09T23:31:36.924004: step 1094, loss 0.216212, acc 0.960938, prec 0.068853, recall 0.865902
2017-12-09T23:31:37.435378: step 1095, loss 0.16209, acc 0.9375, prec 0.0688337, recall 0.865902
2017-12-09T23:31:37.940699: step 1096, loss 1.09592, acc 0.945312, prec 0.068867, recall 0.8658
2017-12-09T23:31:38.444009: step 1097, loss 0.250598, acc 0.957031, prec 0.0689516, recall 0.865977
2017-12-09T23:31:38.942565: step 1098, loss 0.656932, acc 0.933594, prec 0.0690778, recall 0.866242
2017-12-09T23:31:39.449828: step 1099, loss 0.174997, acc 0.980469, prec 0.0691859, recall 0.866447
2017-12-09T23:31:39.967396: step 1100, loss 0.224265, acc 0.921875, prec 0.0691943, recall 0.866506
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-1100

2017-12-09T23:31:41.550304: step 1101, loss 0.396187, acc 0.917969, prec 0.069234, recall 0.866623
2017-12-09T23:31:42.057583: step 1102, loss 0.473628, acc 0.910156, prec 0.0692875, recall 0.866769
2017-12-09T23:31:42.559957: step 1103, loss 0.267464, acc 0.902344, prec 0.0692898, recall 0.866827
2017-12-09T23:31:43.071865: step 1104, loss 0.270243, acc 0.914062, prec 0.0694258, recall 0.867118
2017-12-09T23:31:43.572118: step 1105, loss 0.316889, acc 0.902344, prec 0.0694442, recall 0.867205
2017-12-09T23:31:44.081054: step 1106, loss 0.252816, acc 0.945312, prec 0.0694922, recall 0.86732
2017-12-09T23:31:44.594093: step 1107, loss 0.248038, acc 0.941406, prec 0.0695065, recall 0.867378
2017-12-09T23:31:45.098336: step 1108, loss 0.134546, acc 0.964844, prec 0.069528, recall 0.867436
2017-12-09T23:31:45.612248: step 1109, loss 0.299769, acc 0.917969, prec 0.0695837, recall 0.86758
2017-12-09T23:31:46.126806: step 1110, loss 0.374177, acc 0.925781, prec 0.0696418, recall 0.867724
2017-12-09T23:31:46.628965: step 1111, loss 0.356662, acc 0.949219, prec 0.0697233, recall 0.867896
2017-12-09T23:31:47.142519: step 1112, loss 0.756881, acc 0.960938, prec 0.0697448, recall 0.867765
2017-12-09T23:31:47.652622: step 1113, loss 0.123634, acc 0.957031, prec 0.0698124, recall 0.867908
2017-12-09T23:31:48.156807: step 1114, loss 0.284031, acc 0.964844, prec 0.0698663, recall 0.868023
2017-12-09T23:31:48.658616: step 1115, loss 0.123538, acc 0.972656, prec 0.0699387, recall 0.868165
2017-12-09T23:31:49.173988: step 1116, loss 0.18227, acc 0.9375, prec 0.0699354, recall 0.868194
2017-12-09T23:31:49.678465: step 1117, loss 0.225982, acc 0.929688, prec 0.0699459, recall 0.868251
2017-12-09T23:31:50.181705: step 1118, loss 0.439062, acc 0.921875, prec 0.0699863, recall 0.868364
2017-12-09T23:31:50.701580: step 1119, loss 0.20902, acc 0.925781, prec 0.0700278, recall 0.868478
2017-12-09T23:31:51.202037: step 1120, loss 0.558964, acc 0.9375, prec 0.0700742, recall 0.868404
2017-12-09T23:31:51.708043: step 1121, loss 0.156542, acc 0.964844, prec 0.0701117, recall 0.868489
2017-12-09T23:31:52.214453: step 1122, loss 0.147576, acc 0.953125, prec 0.0701294, recall 0.868546
2017-12-09T23:31:52.716476: step 1123, loss 0.284094, acc 0.941406, prec 0.0701596, recall 0.86863
2017-12-09T23:31:53.223470: step 1124, loss 0.224904, acc 0.941406, prec 0.070222, recall 0.868771
2017-12-09T23:31:53.405321: step 1125, loss 0.429232, acc 0.921569, prec 0.0702172, recall 0.868771
2017-12-09T23:31:53.914695: step 1126, loss 0.310393, acc 0.9375, prec 0.0702783, recall 0.868912
2017-12-09T23:31:54.414489: step 1127, loss 0.234943, acc 0.945312, prec 0.0703258, recall 0.869025
2017-12-09T23:31:54.914270: step 1128, loss 0.22725, acc 0.957031, prec 0.0703446, recall 0.869081
2017-12-09T23:31:55.419698: step 1129, loss 0.307614, acc 0.910156, prec 0.0703649, recall 0.869165
2017-12-09T23:31:55.925329: step 1130, loss 0.138931, acc 0.949219, prec 0.0703813, recall 0.869221
2017-12-09T23:31:56.427785: step 1131, loss 0.143693, acc 0.933594, prec 0.0703767, recall 0.869249
2017-12-09T23:31:56.932134: step 1132, loss 0.277243, acc 0.964844, prec 0.0704462, recall 0.869389
2017-12-09T23:31:57.437348: step 1133, loss 0.120533, acc 0.953125, prec 0.0704638, recall 0.869444
2017-12-09T23:31:57.941734: step 1134, loss 0.196342, acc 0.949219, prec 0.0705123, recall 0.869556
2017-12-09T23:31:58.453595: step 1135, loss 0.12155, acc 0.957031, prec 0.0705632, recall 0.869667
2017-12-09T23:31:58.959471: step 1136, loss 0.102311, acc 0.960938, prec 0.0705992, recall 0.869751
2017-12-09T23:31:59.458934: step 1137, loss 0.365829, acc 0.960938, prec 0.0706353, recall 0.869834
2017-12-09T23:31:59.964699: step 1138, loss 0.214635, acc 0.960938, prec 0.0707355, recall 0.870028
2017-12-09T23:32:00.476172: step 1139, loss 0.0811767, acc 0.976562, prec 0.0707443, recall 0.870055
2017-12-09T23:32:00.980765: step 1140, loss 0.319833, acc 0.953125, prec 0.0707939, recall 0.870166
2017-12-09T23:32:01.491022: step 1141, loss 0.148362, acc 0.960938, prec 0.070894, recall 0.870359
2017-12-09T23:32:01.988551: step 1142, loss 0.162534, acc 0.917969, prec 0.0709165, recall 0.870441
2017-12-09T23:32:02.496197: step 1143, loss 0.163419, acc 0.972656, prec 0.07094, recall 0.870496
2017-12-09T23:32:02.999202: step 1144, loss 0.124922, acc 0.960938, prec 0.0709759, recall 0.870578
2017-12-09T23:32:03.505710: step 1145, loss 0.237195, acc 0.960938, prec 0.0710919, recall 0.870797
2017-12-09T23:32:04.010628: step 1146, loss 0.0854677, acc 0.984375, prec 0.0711191, recall 0.870852
2017-12-09T23:32:04.520495: step 1147, loss 0.184827, acc 0.957031, prec 0.0711216, recall 0.870879
2017-12-09T23:32:05.027327: step 1148, loss 0.185528, acc 0.960938, prec 0.0711735, recall 0.870988
2017-12-09T23:32:05.539838: step 1149, loss 0.166104, acc 0.941406, prec 0.0711871, recall 0.871043
2017-12-09T23:32:06.048616: step 1150, loss 0.456204, acc 0.960938, prec 0.0713029, recall 0.87126
2017-12-09T23:32:06.554625: step 1151, loss 0.622902, acc 0.941406, prec 0.0713017, recall 0.871104
2017-12-09T23:32:07.059456: step 1152, loss 0.162277, acc 0.96875, prec 0.0713559, recall 0.871212
2017-12-09T23:32:07.563519: step 1153, loss 0.185827, acc 0.9375, prec 0.0714003, recall 0.87132
2017-12-09T23:32:08.080784: step 1154, loss 0.377535, acc 0.933594, prec 0.0714593, recall 0.871456
2017-12-09T23:32:08.582972: step 1155, loss 0.126648, acc 0.964844, prec 0.0714962, recall 0.871537
2017-12-09T23:32:09.091909: step 1156, loss 0.739255, acc 0.949219, prec 0.0715134, recall 0.871408
2017-12-09T23:32:09.594749: step 1157, loss 0.198218, acc 0.949219, prec 0.0715933, recall 0.871569
2017-12-09T23:32:10.103791: step 1158, loss 0.199048, acc 0.960938, prec 0.0717088, recall 0.871784
2017-12-09T23:32:10.611929: step 1159, loss 0.190643, acc 0.945312, prec 0.0717713, recall 0.871918
2017-12-09T23:32:11.110259: step 1160, loss 0.186192, acc 0.9375, prec 0.0717675, recall 0.871945
2017-12-09T23:32:11.614715: step 1161, loss 0.254617, acc 0.882812, prec 0.0718262, recall 0.872105
2017-12-09T23:32:12.129538: step 1162, loss 0.407372, acc 0.945312, prec 0.0718727, recall 0.872212
2017-12-09T23:32:12.634835: step 1163, loss 0.346224, acc 0.914062, prec 0.0718934, recall 0.872292
2017-12-09T23:32:13.138124: step 1164, loss 0.319076, acc 0.917969, prec 0.071979, recall 0.872478
2017-12-09T23:32:13.642333: step 1165, loss 0.242085, acc 0.925781, prec 0.0720192, recall 0.872584
2017-12-09T23:32:14.152601: step 1166, loss 0.149815, acc 0.9375, prec 0.0719995, recall 0.872584
2017-12-09T23:32:14.649324: step 1167, loss 0.190013, acc 0.914062, prec 0.0720518, recall 0.872716
2017-12-09T23:32:15.151062: step 1168, loss 0.369894, acc 0.902344, prec 0.0720687, recall 0.872795
2017-12-09T23:32:15.659048: step 1169, loss 0.16963, acc 0.953125, prec 0.0721492, recall 0.872953
2017-12-09T23:32:16.155443: step 1170, loss 0.220692, acc 0.945312, prec 0.0721955, recall 0.873059
2017-12-09T23:32:16.663337: step 1171, loss 0.346797, acc 0.941406, prec 0.0722405, recall 0.873164
2017-12-09T23:32:17.173112: step 1172, loss 0.238681, acc 0.9375, prec 0.0722683, recall 0.873242
2017-12-09T23:32:17.670441: step 1173, loss 0.118318, acc 0.957031, prec 0.0722706, recall 0.873269
2017-12-09T23:32:18.170716: step 1174, loss 0.158835, acc 0.96875, prec 0.0723559, recall 0.873426
2017-12-09T23:32:18.671937: step 1175, loss 0.450915, acc 0.953125, prec 0.0723899, recall 0.873324
2017-12-09T23:32:19.183293: step 1176, loss 0.200823, acc 0.972656, prec 0.0724447, recall 0.873428
2017-12-09T23:32:19.695104: step 1177, loss 0.263014, acc 0.945312, prec 0.0725066, recall 0.873558
2017-12-09T23:32:20.200872: step 1178, loss 0.241409, acc 0.925781, prec 0.0725623, recall 0.873689
2017-12-09T23:32:20.727036: step 1179, loss 0.266132, acc 0.964844, prec 0.0726304, recall 0.873818
2017-12-09T23:32:21.228576: step 1180, loss 0.156451, acc 0.941406, prec 0.0727068, recall 0.873974
2017-12-09T23:32:21.742863: step 1181, loss 0.100988, acc 0.972656, prec 0.0727614, recall 0.874077
2017-12-09T23:32:22.255626: step 1182, loss 0.525579, acc 0.925781, prec 0.0728169, recall 0.874206
2017-12-09T23:32:22.758071: step 1183, loss 0.180385, acc 0.9375, prec 0.0728761, recall 0.874335
2017-12-09T23:32:23.256153: step 1184, loss 0.0925049, acc 0.96875, prec 0.0729136, recall 0.874412
2017-12-09T23:32:23.765432: step 1185, loss 0.179433, acc 0.9375, prec 0.0729728, recall 0.87454
2017-12-09T23:32:24.267700: step 1186, loss 0.457324, acc 0.902344, prec 0.0730207, recall 0.874668
2017-12-09T23:32:24.770230: step 1187, loss 0.178729, acc 0.945312, prec 0.0730822, recall 0.874796
2017-12-09T23:32:25.276693: step 1188, loss 0.212971, acc 0.949219, prec 0.0731134, recall 0.874873
2017-12-09T23:32:25.782379: step 1189, loss 0.108207, acc 0.964844, prec 0.0731338, recall 0.874924
2017-12-09T23:32:26.295567: step 1190, loss 0.141879, acc 0.941406, prec 0.0731151, recall 0.874924
2017-12-09T23:32:26.790782: step 1191, loss 0.292127, acc 0.929688, prec 0.0731716, recall 0.875051
2017-12-09T23:32:27.290868: step 1192, loss 0.575476, acc 0.949219, prec 0.0732039, recall 0.874949
2017-12-09T23:32:27.791613: step 1193, loss 0.422494, acc 0.9375, prec 0.0732471, recall 0.875051
2017-12-09T23:32:28.304132: step 1194, loss 0.314124, acc 0.976562, prec 0.0733184, recall 0.875178
2017-12-09T23:32:28.812601: step 1195, loss 0.841154, acc 0.949219, prec 0.0733664, recall 0.875101
2017-12-09T23:32:29.311005: step 1196, loss 0.418816, acc 0.929688, prec 0.0734227, recall 0.875228
2017-12-09T23:32:29.811378: step 1197, loss 0.223507, acc 0.917969, prec 0.0734123, recall 0.875253
2017-12-09T23:32:30.328583: step 1198, loss 0.290064, acc 0.910156, prec 0.0734151, recall 0.875304
2017-12-09T23:32:30.832590: step 1199, loss 0.275169, acc 0.886719, prec 0.0734261, recall 0.875379
2017-12-09T23:32:31.335396: step 1200, loss 0.297264, acc 0.914062, prec 0.0734458, recall 0.875455

Evaluation:
2017-12-09T23:32:36.065853: step 1200, loss 1.49779, acc 0.913294, prec 0.0738429, recall 0.867962

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_1/1512879719/checkpoints/model-1200

2017-12-09T23:32:37.502477: step 1201, loss 0.424801, acc 0.890625, prec 0.0738393, recall 0.868013
2017-12-09T23:32:38.004697: step 1202, loss 0.36303, acc 0.902344, prec 0.0739012, recall 0.868168
2017-12-09T23:32:38.511089: step 1203, loss 0.270945, acc 0.917969, prec 0.0739525, recall 0.868297
2017-12-09T23:32:39.016975: step 1204, loss 0.370308, acc 0.910156, prec 0.0740013, recall 0.868426
2017-12-09T23:32:39.524853: step 1205, loss 1.24684, acc 0.933594, prec 0.0739815, recall 0.868256
2017-12-09T23:32:40.026576: step 1206, loss 0.235447, acc 0.921875, prec 0.074034, recall 0.868385
2017-12-09T23:32:40.528824: step 1207, loss 0.224938, acc 0.921875, prec 0.074071, recall 0.868488
2017-12-09T23:32:41.029758: step 1208, loss 0.230499, acc 0.921875, prec 0.0740772, recall 0.868539
2017-12-09T23:32:41.532124: step 1209, loss 0.278982, acc 0.933594, prec 0.0741486, recall 0.868693
2017-12-09T23:32:42.038414: step 1210, loss 0.289629, acc 0.941406, prec 0.0742378, recall 0.868872
2017-12-09T23:32:42.544592: step 1211, loss 0.866608, acc 0.9375, prec 0.0742347, recall 0.868728
2017-12-09T23:32:43.054836: step 1212, loss 0.606502, acc 0.933594, prec 0.0742611, recall 0.868636
2017-12-09T23:32:43.566519: step 1213, loss 0.259811, acc 0.917969, prec 0.0743121, recall 0.868763
2017-12-09T23:32:44.068875: step 1214, loss 0.282147, acc 0.921875, prec 0.0743335, recall 0.86884
2017-12-09T23:32:44.574262: step 1215, loss 0.218878, acc 0.941406, prec 0.0743457, recall 0.868891
2017-12-09T23:32:45.079191: step 1216, loss 0.323272, acc 0.914062, prec 0.0743647, recall 0.868967
2017-12-09T23:32:45.586591: step 1217, loss 0.243436, acc 0.925781, prec 0.0744026, recall 0.869068
2017-12-09T23:32:46.089792: step 1218, loss 0.138515, acc 0.960938, prec 0.0744363, recall 0.869144
2017-12-09T23:32:46.600740: step 1219, loss 0.236108, acc 0.941406, prec 0.0744485, recall 0.869195
2017-12-09T23:32:47.109645: step 1220, loss 0.183116, acc 0.941406, prec 0.0744453, recall 0.86922
2017-12-09T23:32:47.612676: step 1221, loss 0.204475, acc 0.941406, prec 0.0744575, recall 0.869271
2017-12-09T23:32:48.121063: step 1222, loss 0.196523, acc 0.925781, prec 0.0744647, recall 0.869321
2017-12-09T23:32:48.629529: step 1223, loss 0.203298, acc 0.941406, prec 0.0744616, recall 0.869347
2017-12-09T23:32:49.136613: step 1224, loss 0.184059, acc 0.933594, prec 0.0744866, recall 0.869422
2017-12-09T23:32:49.640894: step 1225, loss 0.122085, acc 0.964844, prec 0.0745061, recall 0.869473
2017-12-09T23:32:50.142972: step 1226, loss 0.293466, acc 0.984375, prec 0.074593, recall 0.869624
2017-12-09T23:32:50.664434: step 1227, loss 0.0933353, acc 0.964844, prec 0.0746126, recall 0.869674
2017-12-09T23:32:51.165988: step 1228, loss 0.18336, acc 0.953125, prec 0.074613, recall 0.869699
2017-12-09T23:32:51.671240: step 1229, loss 0.256841, acc 0.964844, prec 0.0746478, recall 0.869775
2017-12-09T23:32:52.175019: step 1230, loss 1.39015, acc 0.96875, prec 0.0746851, recall 0.869682
2017-12-09T23:32:52.683095: step 1231, loss 0.0758244, acc 0.972656, prec 0.0746765, recall 0.869682
2017-12-09T23:32:53.181742: step 1232, loss 0.505471, acc 0.96875, prec 0.0747736, recall 0.869858
2017-12-09T23:32:53.691897: step 1233, loss 0.237807, acc 0.921875, prec 0.07481, recall 0.869958
2017-12-09T23:32:54.195825: step 1234, loss 0.17971, acc 0.953125, prec 0.0748716, recall 0.870082
2017-12-09T23:32:54.696785: step 1235, loss 0.217508, acc 0.953125, prec 0.0749179, recall 0.870182
2017-12-09T23:32:55.194950: step 1236, loss 0.122495, acc 0.964844, prec 0.0749831, recall 0.870306
2017-12-09T23:32:55.693981: step 1237, loss 0.176876, acc 0.921875, prec 0.0750194, recall 0.870406
2017-12-09T23:32:56.198720: step 1238, loss 0.241787, acc 0.914062, prec 0.0750379, recall 0.87048
2017-12-09T23:32:56.697032: step 1239, loss 0.358255, acc 0.9375, prec 0.0750791, recall 0.870579
2017-12-09T23:32:57.205673: step 1240, loss 0.228536, acc 0.933594, prec 0.0751191, recall 0.870678
2017-12-09T23:32:57.709842: step 1241, loss 0.276899, acc 0.925781, prec 0.0751413, recall 0.870752
2017-12-09T23:32:58.220132: step 1242, loss 0.290594, acc 0.945312, prec 0.0751696, recall 0.870826
2017-12-09T23:32:58.721810: step 1243, loss 0.338695, acc 0.945312, prec 0.0752589, recall 0.870999
2017-12-09T23:32:59.231804: step 1244, loss 0.393197, acc 0.953125, prec 0.0753506, recall 0.87117
2017-12-09T23:32:59.731728: step 1245, loss 0.215407, acc 0.921875, prec 0.0753562, recall 0.871219
2017-12-09T23:33:00.242953: step 1246, loss 0.232317, acc 0.945312, prec 0.0754149, recall 0.871342
2017-12-09T23:33:00.754709: step 1247, loss 0.243283, acc 0.929688, prec 0.075423, recall 0.871391
2017-12-09T23:33:01.254066: step 1248, loss 0.287593, acc 0.9375, prec 0.0754943, recall 0.871537
2017-12-09T23:33:01.759288: step 1249, loss 0.72838, acc 0.929688, prec 0.0755188, recall 0.871445
2017-12-09T23:33:01.941014: step 1250, loss 0.502561, acc 0.882353, prec 0.0755114, recall 0.871445
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 256
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_256_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382

Start training
2017-12-09T23:33:06.034581: step 1, loss 2.4495, acc 0.441406, prec 0, recall 0
2017-12-09T23:33:06.537796: step 2, loss 4.19678, acc 0.84375, prec 0, recall 0
2017-12-09T23:33:07.040346: step 3, loss 18.5467, acc 0.847656, prec 0, recall 0
2017-12-09T23:33:07.548939: step 4, loss 2.64743, acc 0.828125, prec 0.00387597, recall 0.1
2017-12-09T23:33:08.057562: step 5, loss 4.99259, acc 0.648438, prec 0.00578035, recall 0.142857
2017-12-09T23:33:08.558876: step 6, loss 6.83608, acc 0.464844, prec 0.00826446, recall 0.235294
2017-12-09T23:33:09.061173: step 7, loss 6.08392, acc 0.425781, prec 0.00952381, recall 0.272727
2017-12-09T23:33:09.571872: step 8, loss 4.70595, acc 0.324219, prec 0.0148515, recall 0.413793
2017-12-09T23:33:10.076474: step 9, loss 6.65286, acc 0.15625, prec 0.0146199, recall 0.454545
2017-12-09T23:33:10.587453: step 10, loss 5.55407, acc 0.167969, prec 0.0129032, recall 0.470588
2017-12-09T23:33:11.090713: step 11, loss 4.8916, acc 0.210938, prec 0.0131488, recall 0.513514
2017-12-09T23:33:11.600125: step 12, loss 4.6582, acc 0.226562, prec 0.0139648, recall 0.560976
2017-12-09T23:33:12.121551: step 13, loss 3.80894, acc 0.332031, prec 0.0142779, recall 0.590909
2017-12-09T23:33:12.620019: step 14, loss 3.49521, acc 0.457031, prec 0.0157841, recall 0.62
2017-12-09T23:33:13.121886: step 15, loss 2.92134, acc 0.496094, prec 0.0157593, recall 0.622642
2017-12-09T23:33:13.633718: step 16, loss 7.78219, acc 0.589844, prec 0.0150342, recall 0.578947
2017-12-09T23:33:14.144565: step 17, loss 2.09827, acc 0.671875, prec 0.0153509, recall 0.583333
2017-12-09T23:33:14.650373: step 18, loss 1.26147, acc 0.683594, prec 0.0156581, recall 0.596774
2017-12-09T23:33:15.158966: step 19, loss 2.09356, acc 0.738281, prec 0.0160428, recall 0.6
2017-12-09T23:33:15.662850: step 20, loss 1.091, acc 0.730469, prec 0.0159936, recall 0.606061
2017-12-09T23:33:16.172478: step 21, loss 6.41654, acc 0.832031, prec 0.0161354, recall 0.577465
2017-12-09T23:33:16.681605: step 22, loss 3.3695, acc 0.722656, prec 0.0160858, recall 0.567568
2017-12-09T23:33:17.187764: step 23, loss 4.46462, acc 0.726562, prec 0.0160448, recall 0.558442
2017-12-09T23:33:17.687080: step 24, loss 2.3987, acc 0.679688, prec 0.0166425, recall 0.567901
2017-12-09T23:33:18.198345: step 25, loss 1.4299, acc 0.65625, prec 0.0171629, recall 0.583333
2017-12-09T23:33:18.707048: step 26, loss 1.25107, acc 0.675781, prec 0.0170126, recall 0.588235
2017-12-09T23:33:19.219189: step 27, loss 4.03985, acc 0.65625, prec 0.0178277, recall 0.593407
2017-12-09T23:33:19.723987: step 28, loss 4.04114, acc 0.53125, prec 0.0177778, recall 0.595745
2017-12-09T23:33:20.230217: step 29, loss 2.38381, acc 0.621094, prec 0.0175547, recall 0.59375
2017-12-09T23:33:20.754346: step 30, loss 2.59662, acc 0.558594, prec 0.0178465, recall 0.6
2017-12-09T23:33:21.261733: step 31, loss 7.94234, acc 0.5625, prec 0.0175691, recall 0.586538
2017-12-09T23:33:21.778559: step 32, loss 3.59145, acc 0.5, prec 0.0174951, recall 0.588785
2017-12-09T23:33:22.291500: step 33, loss 4.12525, acc 0.464844, prec 0.0179144, recall 0.59292
2017-12-09T23:33:22.800161: step 34, loss 4.6028, acc 0.484375, prec 0.0180692, recall 0.598291
2017-12-09T23:33:23.308670: step 35, loss 3.01337, acc 0.4375, prec 0.0179104, recall 0.605042
2017-12-09T23:33:23.804344: step 36, loss 4.60769, acc 0.425781, prec 0.0179899, recall 0.609756
2017-12-09T23:33:24.306157: step 37, loss 3.5176, acc 0.480469, prec 0.0181227, recall 0.614173
2017-12-09T23:33:24.805182: step 38, loss 3.42999, acc 0.453125, prec 0.0182186, recall 0.618321
2017-12-09T23:33:25.311869: step 39, loss 3.16571, acc 0.453125, prec 0.0178766, recall 0.621212
2017-12-09T23:33:25.809581: step 40, loss 2.33201, acc 0.519531, prec 0.0184518, recall 0.635036
2017-12-09T23:33:26.315355: step 41, loss 3.81159, acc 0.597656, prec 0.0182648, recall 0.633094
2017-12-09T23:33:26.818368: step 42, loss 2.54126, acc 0.667969, prec 0.0183524, recall 0.633803
2017-12-09T23:33:27.332076: step 43, loss 1.70166, acc 0.667969, prec 0.0184332, recall 0.638889
2017-12-09T23:33:27.841434: step 44, loss 3.43558, acc 0.707031, prec 0.0185551, recall 0.635135
2017-12-09T23:33:28.343981: step 45, loss 1.00058, acc 0.75, prec 0.0188973, recall 0.642384
2017-12-09T23:33:28.854142: step 46, loss 6.73181, acc 0.785156, prec 0.018897, recall 0.632258
2017-12-09T23:33:29.360018: step 47, loss 0.886447, acc 0.792969, prec 0.0187059, recall 0.632258
2017-12-09T23:33:29.866013: step 48, loss 5.28098, acc 0.785156, prec 0.0190746, recall 0.63125
2017-12-09T23:33:30.382332: step 49, loss 4.89247, acc 0.707031, prec 0.0190015, recall 0.621951
2017-12-09T23:33:30.887289: step 50, loss 7.88935, acc 0.699219, prec 0.0191001, recall 0.619048
2017-12-09T23:33:31.388063: step 51, loss 3.34293, acc 0.628906, prec 0.019137, recall 0.612717
2017-12-09T23:33:31.891999: step 52, loss 3.69803, acc 0.605469, prec 0.0191455, recall 0.613636
2017-12-09T23:33:32.396550: step 53, loss 6.64094, acc 0.515625, prec 0.0190807, recall 0.611111
2017-12-09T23:33:32.896118: step 54, loss 3.79295, acc 0.464844, prec 0.0193057, recall 0.616216
2017-12-09T23:33:33.395464: step 55, loss 3.47146, acc 0.394531, prec 0.0194591, recall 0.624339
2017-12-09T23:33:33.904507: step 56, loss 3.39906, acc 0.351562, prec 0.0190981, recall 0.626316
2017-12-09T23:33:34.405835: step 57, loss 3.83833, acc 0.316406, prec 0.0188826, recall 0.630208
2017-12-09T23:33:34.916883: step 58, loss 3.35885, acc 0.378906, prec 0.0185749, recall 0.632124
2017-12-09T23:33:35.418840: step 59, loss 2.97093, acc 0.394531, prec 0.0185846, recall 0.637755
2017-12-09T23:33:35.930396: step 60, loss 3.14861, acc 0.484375, prec 0.0185158, recall 0.638191
2017-12-09T23:33:36.434858: step 61, loss 1.91259, acc 0.570312, prec 0.018646, recall 0.643564
2017-12-09T23:33:36.956916: step 62, loss 2.02801, acc 0.605469, prec 0.0190732, recall 0.652174
2017-12-09T23:33:37.462313: step 63, loss 6.38651, acc 0.710938, prec 0.0195695, recall 0.651163
2017-12-09T23:33:37.969739: step 64, loss 4.63382, acc 0.628906, prec 0.0195889, recall 0.648402
2017-12-09T23:33:38.469592: step 65, loss 9.88822, acc 0.699219, prec 0.0195275, recall 0.638393
2017-12-09T23:33:38.981546: step 66, loss 4.22586, acc 0.695312, prec 0.0199865, recall 0.643478
2017-12-09T23:33:39.486776: step 67, loss 5.30137, acc 0.597656, prec 0.0201092, recall 0.642553
2017-12-09T23:33:40.001157: step 68, loss 3.37105, acc 0.558594, prec 0.0201994, recall 0.644351
2017-12-09T23:33:40.507122: step 69, loss 2.57283, acc 0.496094, prec 0.0202424, recall 0.64876
2017-12-09T23:33:41.014455: step 70, loss 3.84869, acc 0.515625, prec 0.0204237, recall 0.651822
2017-12-09T23:33:41.511026: step 71, loss 3.00407, acc 0.445312, prec 0.0203065, recall 0.654619
2017-12-09T23:33:42.008664: step 72, loss 3.31648, acc 0.402344, prec 0.020645, recall 0.662745
2017-12-09T23:33:42.511505: step 73, loss 2.71785, acc 0.429688, prec 0.0206359, recall 0.666667
2017-12-09T23:33:43.011291: step 74, loss 2.50078, acc 0.449219, prec 0.0204082, recall 0.667954
2017-12-09T23:33:43.519053: step 75, loss 2.25211, acc 0.574219, prec 0.0207194, recall 0.674242
2017-12-09T23:33:44.022710: step 76, loss 2.39686, acc 0.542969, prec 0.0211154, recall 0.681481
2017-12-09T23:33:44.537915: step 77, loss 4.00874, acc 0.664062, prec 0.0216898, recall 0.68705
2017-12-09T23:33:45.042998: step 78, loss 3.86069, acc 0.6875, prec 0.0218272, recall 0.687943
2017-12-09T23:33:45.552328: step 79, loss 4.62867, acc 0.710938, prec 0.0217634, recall 0.681818
2017-12-09T23:33:46.052297: step 80, loss 2.93716, acc 0.707031, prec 0.0220206, recall 0.681507
2017-12-09T23:33:46.557761: step 81, loss 5.70201, acc 0.707031, prec 0.0222783, recall 0.676667
2017-12-09T23:33:47.065947: step 82, loss 1.26226, acc 0.671875, prec 0.0225, recall 0.680921
2017-12-09T23:33:47.576559: step 83, loss 3.04413, acc 0.597656, prec 0.0225685, recall 0.681818
2017-12-09T23:33:48.079808: step 84, loss 4.35746, acc 0.539062, prec 0.0227056, recall 0.681529
2017-12-09T23:33:48.591515: step 85, loss 2.09852, acc 0.558594, prec 0.0226415, recall 0.683544
2017-12-09T23:33:49.102029: step 86, loss 2.77596, acc 0.511719, prec 0.0226544, recall 0.684375
2017-12-09T23:33:49.603261: step 87, loss 3.40482, acc 0.488281, prec 0.0227528, recall 0.686154
2017-12-09T23:33:50.103204: step 88, loss 2.50527, acc 0.492188, prec 0.0227502, recall 0.689024
2017-12-09T23:33:50.622789: step 89, loss 2.27675, acc 0.519531, prec 0.0227634, recall 0.691843
2017-12-09T23:33:51.120920: step 90, loss 2.9279, acc 0.539062, prec 0.0230777, recall 0.695266
2017-12-09T23:33:51.631646: step 91, loss 1.8875, acc 0.558594, prec 0.0230142, recall 0.697059
2017-12-09T23:33:52.141138: step 92, loss 2.29908, acc 0.617188, prec 0.0230814, recall 0.697674
2017-12-09T23:33:52.654110: step 93, loss 1.59588, acc 0.648438, prec 0.0230696, recall 0.699422
2017-12-09T23:33:53.165994: step 94, loss 1.35232, acc 0.640625, prec 0.0234227, recall 0.704545
2017-12-09T23:33:53.667059: step 95, loss 3.30006, acc 0.742188, prec 0.0233715, recall 0.70339
2017-12-09T23:33:54.169746: step 96, loss 2.78405, acc 0.667969, prec 0.0237364, recall 0.70442
2017-12-09T23:33:54.683887: step 97, loss 1.98034, acc 0.796875, prec 0.0240763, recall 0.706522
2017-12-09T23:33:55.190405: step 98, loss 4.28564, acc 0.734375, prec 0.0242915, recall 0.704
2017-12-09T23:33:55.699923: step 99, loss 1.6351, acc 0.703125, prec 0.0243925, recall 0.704485
2017-12-09T23:33:56.213349: step 100, loss 1.779, acc 0.742188, prec 0.0244257, recall 0.704188
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-100

2017-12-09T23:33:57.738510: step 101, loss 1.42686, acc 0.695312, prec 0.0242539, recall 0.704188
2017-12-09T23:33:58.240381: step 102, loss 6.79713, acc 0.625, prec 0.0242289, recall 0.698454
2017-12-09T23:33:58.751161: step 103, loss 3.53418, acc 0.597656, prec 0.024185, recall 0.696429
2017-12-09T23:33:59.269590: step 104, loss 4.11654, acc 0.515625, prec 0.0240119, recall 0.693671
2017-12-09T23:33:59.778370: step 105, loss 2.31526, acc 0.511719, prec 0.0240055, recall 0.69598
2017-12-09T23:34:00.286750: step 106, loss 2.8187, acc 0.417969, prec 0.02395, recall 0.698254
2017-12-09T23:34:00.791101: step 107, loss 3.00575, acc 0.410156, prec 0.0238095, recall 0.699752
2017-12-09T23:34:01.305659: step 108, loss 2.7712, acc 0.484375, prec 0.0237916, recall 0.70197
2017-12-09T23:34:01.813852: step 109, loss 3.18689, acc 0.417969, prec 0.0239822, recall 0.706311
2017-12-09T23:34:02.322002: step 110, loss 3.20697, acc 0.492188, prec 0.0241278, recall 0.708134
2017-12-09T23:34:02.823463: step 111, loss 2.21348, acc 0.53125, prec 0.0242879, recall 0.711584
2017-12-09T23:34:03.327621: step 112, loss 1.79373, acc 0.554688, prec 0.0242226, recall 0.712941
2017-12-09T23:34:03.828604: step 113, loss 1.97482, acc 0.582031, prec 0.0240171, recall 0.712941
2017-12-09T23:34:04.332008: step 114, loss 1.15209, acc 0.71875, prec 0.0239578, recall 0.713615
2017-12-09T23:34:04.841610: step 115, loss 5.54101, acc 0.703125, prec 0.0238972, recall 0.709302
2017-12-09T23:34:05.350405: step 116, loss 3.11773, acc 0.738281, prec 0.0240785, recall 0.710345
2017-12-09T23:34:05.876857: step 117, loss 1.18392, acc 0.742188, prec 0.0241067, recall 0.71167
2017-12-09T23:34:06.381497: step 118, loss 2.26808, acc 0.789062, prec 0.0243093, recall 0.71267
2017-12-09T23:34:06.887163: step 119, loss 2.40665, acc 0.785156, prec 0.0244333, recall 0.713004
2017-12-09T23:34:07.393352: step 120, loss 2.5964, acc 0.808594, prec 0.0244201, recall 0.710468
2017-12-09T23:34:07.900773: step 121, loss 1.75433, acc 0.78125, prec 0.0244683, recall 0.708609
2017-12-09T23:34:08.412901: step 122, loss 5.31406, acc 0.738281, prec 0.0245734, recall 0.704348
2017-12-09T23:34:08.917293: step 123, loss 2.72744, acc 0.726562, prec 0.0244454, recall 0.70282
2017-12-09T23:34:09.421009: step 124, loss 2.23306, acc 0.6875, prec 0.0247395, recall 0.705128
2017-12-09T23:34:09.601554: step 125, loss 1.83703, acc 0.711538, prec 0.0248577, recall 0.706383
2017-12-09T23:34:10.111779: step 126, loss 3.61507, acc 0.566406, prec 0.0251633, recall 0.707724
2017-12-09T23:34:10.617377: step 127, loss 2.36482, acc 0.511719, prec 0.0254337, recall 0.711934
2017-12-09T23:34:11.120795: step 128, loss 2.49016, acc 0.476562, prec 0.0255403, recall 0.714868
2017-12-09T23:34:11.633758: step 129, loss 3.21243, acc 0.429688, prec 0.0253438, recall 0.713996
2017-12-09T23:34:12.134117: step 130, loss 2.90829, acc 0.410156, prec 0.0253489, recall 0.716298
2017-12-09T23:34:12.642621: step 131, loss 3.1138, acc 0.394531, prec 0.0252781, recall 0.718
2017-12-09T23:34:13.142299: step 132, loss 3.70936, acc 0.445312, prec 0.0251673, recall 0.71627
2017-12-09T23:34:13.646937: step 133, loss 2.19022, acc 0.515625, prec 0.0250864, recall 0.717391
2017-12-09T23:34:14.147466: step 134, loss 2.23251, acc 0.5, prec 0.0248664, recall 0.717391
2017-12-09T23:34:14.648247: step 135, loss 2.07702, acc 0.492188, prec 0.0248456, recall 0.719057
2017-12-09T23:34:15.165472: step 136, loss 1.77619, acc 0.617188, prec 0.0249444, recall 0.721248
2017-12-09T23:34:15.670188: step 137, loss 1.64238, acc 0.703125, prec 0.0252749, recall 0.725
2017-12-09T23:34:16.171142: step 138, loss 1.2398, acc 0.734375, prec 0.0254203, recall 0.727099
2017-12-09T23:34:16.674641: step 139, loss 6.28825, acc 0.769531, prec 0.0253922, recall 0.722117
2017-12-09T23:34:17.184939: step 140, loss 1.44811, acc 0.777344, prec 0.0254916, recall 0.722326
2017-12-09T23:34:17.687577: step 141, loss 2.55518, acc 0.792969, prec 0.0255328, recall 0.722015
2017-12-09T23:34:18.190515: step 142, loss 3.50591, acc 0.722656, prec 0.0254187, recall 0.717996
2017-12-09T23:34:18.703601: step 143, loss 2.50247, acc 0.722656, prec 0.0255572, recall 0.71875
2017-12-09T23:34:19.214750: step 144, loss 3.52305, acc 0.765625, prec 0.0256527, recall 0.716364
2017-12-09T23:34:19.732457: step 145, loss 1.40078, acc 0.707031, prec 0.0257174, recall 0.717902
2017-12-09T23:34:20.232176: step 146, loss 1.9139, acc 0.621094, prec 0.0255585, recall 0.716606
2017-12-09T23:34:20.762315: step 147, loss 2.02946, acc 0.609375, prec 0.025582, recall 0.718133
2017-12-09T23:34:21.267182: step 148, loss 1.9017, acc 0.601562, prec 0.0256638, recall 0.720143
2017-12-09T23:34:21.771709: step 149, loss 1.83007, acc 0.550781, prec 0.0257235, recall 0.722124
2017-12-09T23:34:22.277140: step 150, loss 1.96193, acc 0.570312, prec 0.0256683, recall 0.723104
2017-12-09T23:34:22.785901: step 151, loss 1.84635, acc 0.585938, prec 0.0256809, recall 0.724561
2017-12-09T23:34:23.286957: step 152, loss 1.52321, acc 0.628906, prec 0.0257108, recall 0.726003
2017-12-09T23:34:23.791322: step 153, loss 4.20054, acc 0.625, prec 0.0256205, recall 0.725217
2017-12-09T23:34:24.297555: step 154, loss 1.4142, acc 0.667969, prec 0.0257852, recall 0.727586
2017-12-09T23:34:24.802308: step 155, loss 4.55395, acc 0.71875, prec 0.0257939, recall 0.726027
2017-12-09T23:34:25.316347: step 156, loss 1.18061, acc 0.6875, prec 0.025728, recall 0.726496
2017-12-09T23:34:25.820067: step 157, loss 1.05147, acc 0.757812, prec 0.0258667, recall 0.728353
2017-12-09T23:34:26.328562: step 158, loss 0.903534, acc 0.726562, prec 0.0258165, recall 0.728814
2017-12-09T23:34:26.836174: step 159, loss 0.901946, acc 0.78125, prec 0.0258466, recall 0.72973
2017-12-09T23:34:27.341514: step 160, loss 2.42861, acc 0.757812, prec 0.0257526, recall 0.728499
2017-12-09T23:34:27.848406: step 161, loss 2.45637, acc 0.78125, prec 0.0257842, recall 0.728188
2017-12-09T23:34:28.364369: step 162, loss 3.38865, acc 0.792969, prec 0.0258217, recall 0.726667
2017-12-09T23:34:28.873274: step 163, loss 0.793479, acc 0.804688, prec 0.0259181, recall 0.728027
2017-12-09T23:34:29.376850: step 164, loss 0.845264, acc 0.847656, prec 0.0261453, recall 0.730263
2017-12-09T23:34:29.883125: step 165, loss 1.59287, acc 0.78125, prec 0.0261752, recall 0.729951
2017-12-09T23:34:30.401695: step 166, loss 0.80253, acc 0.777344, prec 0.0263158, recall 0.731707
2017-12-09T23:34:30.905779: step 167, loss 1.72083, acc 0.792969, prec 0.0264063, recall 0.731826
2017-12-09T23:34:31.407103: step 168, loss 0.922381, acc 0.726562, prec 0.0263555, recall 0.732258
2017-12-09T23:34:31.916359: step 169, loss 0.909475, acc 0.757812, prec 0.0263736, recall 0.733119
2017-12-09T23:34:32.431759: step 170, loss 1.7955, acc 0.78125, prec 0.0265145, recall 0.733652
2017-12-09T23:34:32.938753: step 171, loss 1.57859, acc 0.757812, prec 0.0265893, recall 0.733756
2017-12-09T23:34:33.447700: step 172, loss 4.46133, acc 0.722656, prec 0.0267056, recall 0.734277
2017-12-09T23:34:33.959495: step 173, loss 1.17279, acc 0.757812, prec 0.0267776, recall 0.735524
2017-12-09T23:34:34.466753: step 174, loss 1.12388, acc 0.726562, prec 0.0268369, recall 0.73676
2017-12-09T23:34:34.972269: step 175, loss 1.27246, acc 0.683594, prec 0.026879, recall 0.737984
2017-12-09T23:34:35.483805: step 176, loss 1.35581, acc 0.734375, prec 0.0269966, recall 0.738462
2017-12-09T23:34:35.993673: step 177, loss 2.19374, acc 0.707031, prec 0.0271027, recall 0.738931
2017-12-09T23:34:36.500771: step 178, loss 1.40583, acc 0.679688, prec 0.0272499, recall 0.740909
2017-12-09T23:34:37.007557: step 179, loss 1.10572, acc 0.707031, prec 0.0271365, recall 0.740909
2017-12-09T23:34:37.511921: step 180, loss 1.37118, acc 0.703125, prec 0.027024, recall 0.739788
2017-12-09T23:34:38.012780: step 181, loss 0.936001, acc 0.738281, prec 0.0270851, recall 0.740964
2017-12-09T23:34:38.516338: step 182, loss 3.55505, acc 0.800781, prec 0.027122, recall 0.737313
2017-12-09T23:34:39.021824: step 183, loss 3.10803, acc 0.785156, prec 0.0272016, recall 0.737389
2017-12-09T23:34:39.537608: step 184, loss 3.07926, acc 0.734375, prec 0.0273674, recall 0.738235
2017-12-09T23:34:40.044055: step 185, loss 1.16092, acc 0.738281, prec 0.0274791, recall 0.739766
2017-12-09T23:34:40.543368: step 186, loss 1.2457, acc 0.671875, prec 0.0274595, recall 0.740525
2017-12-09T23:34:41.052831: step 187, loss 1.69113, acc 0.691406, prec 0.0275535, recall 0.740955
2017-12-09T23:34:41.561666: step 188, loss 1.39733, acc 0.632812, prec 0.0274669, recall 0.741329
2017-12-09T23:34:42.064287: step 189, loss 1.08713, acc 0.679688, prec 0.0273987, recall 0.741703
2017-12-09T23:34:42.571678: step 190, loss 4.043, acc 0.707031, prec 0.0274474, recall 0.740688
2017-12-09T23:34:43.076484: step 191, loss 2.30494, acc 0.683594, prec 0.027537, recall 0.74111
2017-12-09T23:34:43.583881: step 192, loss 3.72078, acc 0.675781, prec 0.027522, recall 0.739745
2017-12-09T23:34:44.087799: step 193, loss 1.31776, acc 0.683594, prec 0.0275071, recall 0.74048
2017-12-09T23:34:44.584750: step 194, loss 1.46575, acc 0.660156, prec 0.0274837, recall 0.74121
2017-12-09T23:34:45.088748: step 195, loss 1.28326, acc 0.699219, prec 0.0276263, recall 0.743017
2017-12-09T23:34:45.591734: step 196, loss 1.25355, acc 0.644531, prec 0.0275466, recall 0.743375
2017-12-09T23:34:46.096271: step 197, loss 1.9062, acc 0.667969, prec 0.0276777, recall 0.744122
2017-12-09T23:34:46.600871: step 198, loss 1.3489, acc 0.636719, prec 0.0275957, recall 0.744475
2017-12-09T23:34:47.103815: step 199, loss 1.01721, acc 0.738281, prec 0.0276502, recall 0.74553
2017-12-09T23:34:47.606216: step 200, loss 1.4504, acc 0.765625, prec 0.0277651, recall 0.745902
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-200

2017-12-09T23:34:49.098062: step 201, loss 0.677731, acc 0.820312, prec 0.0278975, recall 0.747283
2017-12-09T23:34:49.600207: step 202, loss 0.834561, acc 0.816406, prec 0.0279787, recall 0.748309
2017-12-09T23:34:50.107034: step 203, loss 2.95535, acc 0.851562, prec 0.0280261, recall 0.746972
2017-12-09T23:34:50.635834: step 204, loss 0.612451, acc 0.828125, prec 0.0281108, recall 0.747989
2017-12-09T23:34:51.146779: step 205, loss 1.98858, acc 0.824219, prec 0.0284408, recall 0.748677
2017-12-09T23:34:51.656305: step 206, loss 2.87075, acc 0.761719, prec 0.0286001, recall 0.748362
2017-12-09T23:34:52.165799: step 207, loss 0.913677, acc 0.792969, prec 0.0286214, recall 0.74902
2017-12-09T23:34:52.673459: step 208, loss 1.41195, acc 0.746094, prec 0.0287236, recall 0.749351
2017-12-09T23:34:53.180887: step 209, loss 2.10393, acc 0.714844, prec 0.0288619, recall 0.75
2017-12-09T23:34:53.696302: step 210, loss 2.40738, acc 0.707031, prec 0.0289003, recall 0.75
2017-12-09T23:34:54.205203: step 211, loss 1.20157, acc 0.667969, prec 0.028875, recall 0.750639
2017-12-09T23:34:54.704190: step 212, loss 1.42384, acc 0.648438, prec 0.0289855, recall 0.752224
2017-12-09T23:34:55.207578: step 213, loss 4.48954, acc 0.714844, prec 0.0291693, recall 0.752201
2017-12-09T23:34:55.710581: step 214, loss 2.19418, acc 0.71875, prec 0.0292102, recall 0.75219
2017-12-09T23:34:56.225858: step 215, loss 1.14003, acc 0.683594, prec 0.0293306, recall 0.753731
2017-12-09T23:34:56.733403: step 216, loss 1.84481, acc 0.628906, prec 0.0293834, recall 0.754951
2017-12-09T23:34:57.242608: step 217, loss 2.09136, acc 0.625, prec 0.0292961, recall 0.754321
2017-12-09T23:34:57.752281: step 218, loss 1.95657, acc 0.617188, prec 0.0292068, recall 0.753695
2017-12-09T23:34:58.255558: step 219, loss 1.5333, acc 0.605469, prec 0.0291128, recall 0.753998
2017-12-09T23:34:58.768620: step 220, loss 1.35767, acc 0.667969, prec 0.0291795, recall 0.755202
2017-12-09T23:34:59.269784: step 221, loss 1.32151, acc 0.644531, prec 0.0292373, recall 0.756395
2017-12-09T23:34:59.772666: step 222, loss 1.86167, acc 0.652344, prec 0.0293442, recall 0.756953
2017-12-09T23:35:00.290042: step 223, loss 1.17942, acc 0.679688, prec 0.0293225, recall 0.757539
2017-12-09T23:35:00.800986: step 224, loss 3.11639, acc 0.675781, prec 0.0294829, recall 0.757467
2017-12-09T23:35:01.304989: step 225, loss 0.778626, acc 0.789062, prec 0.0296341, recall 0.758907
2017-12-09T23:35:01.807539: step 226, loss 1.05083, acc 0.796875, prec 0.0297424, recall 0.760047
2017-12-09T23:35:02.312503: step 227, loss 2.51512, acc 0.800781, prec 0.029899, recall 0.759672
2017-12-09T23:35:02.819619: step 228, loss 1.32956, acc 0.789062, prec 0.02996, recall 0.759627
2017-12-09T23:35:03.326300: step 229, loss 1.87767, acc 0.820312, prec 0.0300762, recall 0.759861
2017-12-09T23:35:03.828598: step 230, loss 0.76549, acc 0.761719, prec 0.030081, recall 0.760417
2017-12-09T23:35:04.335780: step 231, loss 1.0317, acc 0.710938, prec 0.0300237, recall 0.760694
2017-12-09T23:35:04.844078: step 232, loss 0.860783, acc 0.792969, prec 0.0301279, recall 0.761795
2017-12-09T23:35:05.347443: step 233, loss 2.64257, acc 0.726562, prec 0.0302109, recall 0.761143
2017-12-09T23:35:05.879352: step 234, loss 3.33915, acc 0.730469, prec 0.030251, recall 0.760227
2017-12-09T23:35:06.384189: step 235, loss 1.45643, acc 0.78125, prec 0.0302634, recall 0.759909
2017-12-09T23:35:06.891653: step 236, loss 1.25964, acc 0.683594, prec 0.030284, recall 0.760722
2017-12-09T23:35:07.397632: step 237, loss 1.13621, acc 0.707031, prec 0.0302691, recall 0.761261
2017-12-09T23:35:07.904568: step 238, loss 1.00592, acc 0.699219, prec 0.0302949, recall 0.762065
2017-12-09T23:35:08.415041: step 239, loss 1.53417, acc 0.660156, prec 0.0303515, recall 0.762277
2017-12-09T23:35:08.916335: step 240, loss 1.41661, acc 0.699219, prec 0.030464, recall 0.762749
2017-12-09T23:35:09.426084: step 241, loss 0.787742, acc 0.726562, prec 0.0305839, recall 0.764057
2017-12-09T23:35:09.930750: step 242, loss 1.02732, acc 0.679688, prec 0.0306015, recall 0.764835
2017-12-09T23:35:10.442369: step 243, loss 0.940902, acc 0.738281, prec 0.0307241, recall 0.76612
2017-12-09T23:35:10.945436: step 244, loss 1.21958, acc 0.769531, prec 0.030899, recall 0.767644
2017-12-09T23:35:11.451606: step 245, loss 0.59117, acc 0.824219, prec 0.0308806, recall 0.767896
2017-12-09T23:35:11.965078: step 246, loss 6.40468, acc 0.835938, prec 0.0309139, recall 0.765086
2017-12-09T23:35:12.475901: step 247, loss 2.29768, acc 0.753906, prec 0.0309149, recall 0.764769
2017-12-09T23:35:12.983062: step 248, loss 1.50422, acc 0.753906, prec 0.0309997, recall 0.764957
2017-12-09T23:35:13.493719: step 249, loss 0.861561, acc 0.757812, prec 0.0310004, recall 0.765458
2017-12-09T23:35:13.676036: step 250, loss 1.12148, acc 0.788462, prec 0.0310693, recall 0.765957
2017-12-09T23:35:14.193358: step 251, loss 1.11736, acc 0.710938, prec 0.0310538, recall 0.766454
2017-12-09T23:35:14.704217: step 252, loss 0.791097, acc 0.730469, prec 0.0311281, recall 0.767442
2017-12-09T23:35:15.207580: step 253, loss 1.23518, acc 0.769531, prec 0.0312166, recall 0.767613
2017-12-09T23:35:15.712157: step 254, loss 0.971222, acc 0.761719, prec 0.0313832, recall 0.76907
2017-12-09T23:35:16.218051: step 255, loss 0.581126, acc 0.8125, prec 0.0314016, recall 0.769552
2017-12-09T23:35:16.721851: step 256, loss 0.792532, acc 0.796875, prec 0.0314556, recall 0.77027
2017-12-09T23:35:17.223582: step 257, loss 0.752647, acc 0.796875, prec 0.0314684, recall 0.770747
2017-12-09T23:35:17.728657: step 258, loss 0.545616, acc 0.851562, prec 0.0316635, recall 0.772165
2017-12-09T23:35:18.234308: step 259, loss 0.822564, acc 0.792969, prec 0.0317969, recall 0.773333
2017-12-09T23:35:18.736956: step 260, loss 2.82005, acc 0.84375, prec 0.0318276, recall 0.772217
2017-12-09T23:35:19.250478: step 261, loss 0.703497, acc 0.816406, prec 0.0318867, recall 0.772912
2017-12-09T23:35:19.757451: step 262, loss 0.538913, acc 0.84375, prec 0.031955, recall 0.773604
2017-12-09T23:35:20.260620: step 263, loss 0.366758, acc 0.851562, prec 0.0319447, recall 0.773834
2017-12-09T23:35:20.780780: step 264, loss 0.35047, acc 0.882812, prec 0.0320261, recall 0.77452
2017-12-09T23:35:21.290861: step 265, loss 0.792332, acc 0.878906, prec 0.0320668, recall 0.774194
2017-12-09T23:35:21.793029: step 266, loss 0.312107, acc 0.890625, prec 0.0321101, recall 0.774648
2017-12-09T23:35:22.294369: step 267, loss 1.78899, acc 0.878906, prec 0.0322312, recall 0.774775
2017-12-09T23:35:22.798395: step 268, loss 1.01725, acc 0.921875, prec 0.0322862, recall 0.774451
2017-12-09T23:35:23.304019: step 269, loss 0.511219, acc 0.859375, prec 0.0323586, recall 0.775124
2017-12-09T23:35:23.807806: step 270, loss 1.03259, acc 0.878906, prec 0.0323986, recall 0.774802
2017-12-09T23:35:24.318412: step 271, loss 0.322333, acc 0.898438, prec 0.0324439, recall 0.775248
2017-12-09T23:35:24.822269: step 272, loss 0.683398, acc 0.867188, prec 0.0325984, recall 0.776355
2017-12-09T23:35:25.333120: step 273, loss 0.628281, acc 0.847656, prec 0.0327056, recall 0.777233
2017-12-09T23:35:25.838369: step 274, loss 0.412531, acc 0.855469, prec 0.0326956, recall 0.777451
2017-12-09T23:35:26.344023: step 275, loss 0.359593, acc 0.882812, prec 0.0328146, recall 0.77832
2017-12-09T23:35:26.846100: step 276, loss 1.57466, acc 0.890625, prec 0.0329386, recall 0.77767
2017-12-09T23:35:27.346525: step 277, loss 1.79791, acc 0.847656, prec 0.0330076, recall 0.776812
2017-12-09T23:35:27.849654: step 278, loss 1.12726, acc 0.84375, prec 0.0332323, recall 0.777565
2017-12-09T23:35:28.354067: step 279, loss 3.22232, acc 0.777344, prec 0.0332761, recall 0.776718
2017-12-09T23:35:28.861416: step 280, loss 1.11526, acc 0.734375, prec 0.0333415, recall 0.777567
2017-12-09T23:35:29.367740: step 281, loss 1.37809, acc 0.640625, prec 0.0334524, recall 0.778828
2017-12-09T23:35:29.864482: step 282, loss 1.1304, acc 0.699219, prec 0.0335046, recall 0.779661
2017-12-09T23:35:30.373237: step 283, loss 1.17364, acc 0.644531, prec 0.0333817, recall 0.779661
2017-12-09T23:35:30.872745: step 284, loss 1.25938, acc 0.640625, prec 0.0333748, recall 0.780282
2017-12-09T23:35:31.375937: step 285, loss 1.55602, acc 0.628906, prec 0.03352, recall 0.780988
2017-12-09T23:35:31.878175: step 286, loss 1.43966, acc 0.625, prec 0.0335073, recall 0.781599
2017-12-09T23:35:32.375232: step 287, loss 1.43483, acc 0.664062, prec 0.033508, recall 0.782206
2017-12-09T23:35:32.881952: step 288, loss 1.49952, acc 0.671875, prec 0.0335496, recall 0.78301
2017-12-09T23:35:33.389488: step 289, loss 1.38968, acc 0.695312, prec 0.0336751, recall 0.784206
2017-12-09T23:35:33.898150: step 290, loss 1.0838, acc 0.785156, prec 0.0339076, recall 0.785064
2017-12-09T23:35:34.401049: step 291, loss 1.11181, acc 0.722656, prec 0.0340027, recall 0.786038
2017-12-09T23:35:34.908367: step 292, loss 0.948287, acc 0.78125, prec 0.0340038, recall 0.786425
2017-12-09T23:35:35.413103: step 293, loss 0.7406, acc 0.800781, prec 0.0340492, recall 0.787004
2017-12-09T23:35:35.939423: step 294, loss 1.78475, acc 0.792969, prec 0.0341684, recall 0.787253
2017-12-09T23:35:36.450353: step 295, loss 0.907808, acc 0.820312, prec 0.0342213, recall 0.78712
2017-12-09T23:35:36.952452: step 296, loss 1.04408, acc 0.796875, prec 0.034191, recall 0.786607
2017-12-09T23:35:37.465704: step 297, loss 0.53498, acc 0.855469, prec 0.0343665, recall 0.787744
2017-12-09T23:35:37.973490: step 298, loss 2.32638, acc 0.851562, prec 0.0344294, recall 0.787611
2017-12-09T23:35:38.486965: step 299, loss 0.52097, acc 0.84375, prec 0.0345254, recall 0.78836
2017-12-09T23:35:38.989251: step 300, loss 2.73239, acc 0.789062, prec 0.0346063, recall 0.787029

Evaluation:
2017-12-09T23:35:43.662054: step 300, loss 1.3978, acc 0.823457, prec 0.0359397, recall 0.777347

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-300

2017-12-09T23:35:45.075864: step 301, loss 0.629995, acc 0.765625, prec 0.0358971, recall 0.777519
2017-12-09T23:35:45.577624: step 302, loss 0.603194, acc 0.789062, prec 0.0358623, recall 0.777692
2017-12-09T23:35:46.083959: step 303, loss 1.0382, acc 0.792969, prec 0.0361051, recall 0.778632
2017-12-09T23:35:46.593351: step 304, loss 0.788774, acc 0.730469, prec 0.036085, recall 0.778972
2017-12-09T23:35:47.103683: step 305, loss 1.09651, acc 0.730469, prec 0.0360993, recall 0.779479
2017-12-09T23:35:47.615096: step 306, loss 1.1975, acc 0.753906, prec 0.0360883, recall 0.779221
2017-12-09T23:35:48.115176: step 307, loss 0.877758, acc 0.726562, prec 0.0361012, recall 0.779726
2017-12-09T23:35:48.621253: step 308, loss 0.644989, acc 0.808594, prec 0.0361068, recall 0.780061
2017-12-09T23:35:49.137384: step 309, loss 1.32039, acc 0.78125, prec 0.0362403, recall 0.780469
2017-12-09T23:35:49.638889: step 310, loss 3.09016, acc 0.820312, prec 0.0362845, recall 0.780377
2017-12-09T23:35:50.146416: step 311, loss 1.33184, acc 0.746094, prec 0.0363382, recall 0.780451
2017-12-09T23:35:50.663819: step 312, loss 0.695466, acc 0.765625, prec 0.036363, recall 0.780945
2017-12-09T23:35:51.168025: step 313, loss 0.78478, acc 0.753906, prec 0.0363503, recall 0.781273
2017-12-09T23:35:51.678335: step 314, loss 1.86906, acc 0.761719, prec 0.036308, recall 0.780853
2017-12-09T23:35:52.193638: step 315, loss 0.932831, acc 0.757812, prec 0.0363636, recall 0.781506
2017-12-09T23:35:52.704343: step 316, loss 3.94319, acc 0.851562, prec 0.0364852, recall 0.781157
2017-12-09T23:35:53.212930: step 317, loss 0.766319, acc 0.84375, prec 0.0366681, recall 0.782288
2017-12-09T23:35:53.725986: step 318, loss 0.64253, acc 0.816406, prec 0.0367083, recall 0.782769
2017-12-09T23:35:54.228294: step 319, loss 1.88757, acc 0.757812, prec 0.0366976, recall 0.782513
2017-12-09T23:35:54.746970: step 320, loss 0.777162, acc 0.796875, prec 0.0367313, recall 0.782991
2017-12-09T23:35:55.256532: step 321, loss 0.686856, acc 0.773438, prec 0.0367574, recall 0.783467
2017-12-09T23:35:55.761311: step 322, loss 0.741448, acc 0.75, prec 0.0367758, recall 0.783942
2017-12-09T23:35:56.264608: step 323, loss 0.84885, acc 0.765625, prec 0.036832, recall 0.784571
2017-12-09T23:35:56.769301: step 324, loss 0.940378, acc 0.769531, prec 0.0368893, recall 0.785196
2017-12-09T23:35:57.273841: step 325, loss 0.683494, acc 0.867188, prec 0.0369122, recall 0.785507
2017-12-09T23:35:57.777785: step 326, loss 1.23242, acc 0.804688, prec 0.0369816, recall 0.78556
2017-12-09T23:35:58.286976: step 327, loss 0.985049, acc 0.777344, prec 0.0370408, recall 0.786177
2017-12-09T23:35:58.788742: step 328, loss 0.475535, acc 0.851562, prec 0.0370257, recall 0.786331
2017-12-09T23:35:59.286986: step 329, loss 2.04477, acc 0.867188, prec 0.0370496, recall 0.786073
2017-12-09T23:35:59.785265: step 330, loss 1.28894, acc 0.8125, prec 0.0371209, recall 0.786123
2017-12-09T23:36:00.302461: step 331, loss 0.603047, acc 0.824219, prec 0.0371619, recall 0.786581
2017-12-09T23:36:00.804794: step 332, loss 1.51246, acc 0.800781, prec 0.0372614, recall 0.78678
2017-12-09T23:36:01.311207: step 333, loss 1.1978, acc 0.761719, prec 0.0372834, recall 0.786676
2017-12-09T23:36:01.809902: step 334, loss 0.862127, acc 0.769531, prec 0.0372419, recall 0.786827
2017-12-09T23:36:02.305347: step 335, loss 1.55019, acc 0.792969, prec 0.0372105, recall 0.785866
2017-12-09T23:36:02.806015: step 336, loss 0.683128, acc 0.789062, prec 0.0372077, recall 0.786168
2017-12-09T23:36:03.316486: step 337, loss 0.830392, acc 0.765625, prec 0.0372617, recall 0.78677
2017-12-09T23:36:03.818742: step 338, loss 0.860942, acc 0.691406, prec 0.0371638, recall 0.78677
2017-12-09T23:36:04.325425: step 339, loss 0.808446, acc 0.757812, prec 0.037311, recall 0.787815
2017-12-09T23:36:04.829109: step 340, loss 0.776098, acc 0.785156, prec 0.0373386, recall 0.78826
2017-12-09T23:36:05.336880: step 341, loss 0.61856, acc 0.820312, prec 0.0373773, recall 0.788703
2017-12-09T23:36:05.853932: step 342, loss 1.08507, acc 0.835938, prec 0.0374538, recall 0.788742
2017-12-09T23:36:06.357663: step 343, loss 1.28551, acc 0.820312, prec 0.0374312, recall 0.787795
2017-12-09T23:36:06.859325: step 344, loss 0.554709, acc 0.800781, prec 0.0374634, recall 0.788235
2017-12-09T23:36:07.354611: step 345, loss 2.5641, acc 0.84375, prec 0.0375103, recall 0.78813
2017-12-09T23:36:07.856404: step 346, loss 1.17954, acc 0.859375, prec 0.0375619, recall 0.788025
2017-12-09T23:36:08.365374: step 347, loss 0.354191, acc 0.882812, prec 0.037525, recall 0.788025
2017-12-09T23:36:08.869153: step 348, loss 0.920746, acc 0.839844, prec 0.0375401, recall 0.787234
2017-12-09T23:36:09.385647: step 349, loss 0.73185, acc 0.828125, prec 0.0375502, recall 0.786986
2017-12-09T23:36:09.896580: step 350, loss 0.580863, acc 0.828125, prec 0.037622, recall 0.787568
2017-12-09T23:36:10.401261: step 351, loss 0.921137, acc 0.804688, prec 0.0376547, recall 0.788003
2017-12-09T23:36:10.908175: step 352, loss 0.636177, acc 0.8125, prec 0.0376585, recall 0.788291
2017-12-09T23:36:11.418401: step 353, loss 2.42564, acc 0.804688, prec 0.0376936, recall 0.787653
2017-12-09T23:36:11.925610: step 354, loss 0.559559, acc 0.800781, prec 0.0376937, recall 0.78794
2017-12-09T23:36:12.431907: step 355, loss 0.822599, acc 0.804688, prec 0.0377572, recall 0.788514
2017-12-09T23:36:12.939937: step 356, loss 1.62835, acc 0.785156, prec 0.0378157, recall 0.788552
2017-12-09T23:36:13.452055: step 357, loss 2.33911, acc 0.761719, prec 0.0378368, recall 0.787919
2017-12-09T23:36:13.957371: step 358, loss 1.45143, acc 0.761719, prec 0.0378566, recall 0.787818
2017-12-09T23:36:14.460452: step 359, loss 1.8403, acc 0.707031, prec 0.0379519, recall 0.788141
2017-12-09T23:36:14.967271: step 360, loss 1.06742, acc 0.683594, prec 0.0380075, recall 0.788845
2017-12-09T23:36:15.471538: step 361, loss 1.00543, acc 0.695312, prec 0.0380357, recall 0.789404
2017-12-09T23:36:15.975768: step 362, loss 1.17021, acc 0.675781, prec 0.0379964, recall 0.789683
2017-12-09T23:36:16.475742: step 363, loss 1.62599, acc 0.628906, prec 0.0379136, recall 0.7893
2017-12-09T23:36:16.982148: step 364, loss 1.50766, acc 0.660156, prec 0.0379615, recall 0.789993
2017-12-09T23:36:17.487079: step 365, loss 1.25508, acc 0.648438, prec 0.0379751, recall 0.790545
2017-12-09T23:36:17.984194: step 366, loss 1.2283, acc 0.664062, prec 0.0379934, recall 0.791094
2017-12-09T23:36:18.491317: step 367, loss 1.28242, acc 0.6875, prec 0.0379886, recall 0.791503
2017-12-09T23:36:18.998780: step 368, loss 0.862432, acc 0.730469, prec 0.0380571, recall 0.792182
2017-12-09T23:36:19.502866: step 369, loss 0.740938, acc 0.773438, prec 0.0380482, recall 0.792453
2017-12-09T23:36:20.009791: step 370, loss 0.730111, acc 0.824219, prec 0.0380848, recall 0.792857
2017-12-09T23:36:20.522876: step 371, loss 0.821617, acc 0.800781, prec 0.0381741, recall 0.793527
2017-12-09T23:36:21.041442: step 372, loss 1.7498, acc 0.851562, prec 0.0382498, recall 0.793548
2017-12-09T23:36:21.546694: step 373, loss 1.99099, acc 0.847656, prec 0.0382656, recall 0.792793
2017-12-09T23:36:22.052961: step 374, loss 0.498409, acc 0.847656, prec 0.038279, recall 0.793059
2017-12-09T23:36:22.232759: step 375, loss 3.90793, acc 0.846154, prec 0.0383005, recall 0.792683
2017-12-09T23:36:22.749398: step 376, loss 0.696702, acc 0.816406, prec 0.0383043, recall 0.792949
2017-12-09T23:36:23.260338: step 377, loss 0.588421, acc 0.839844, prec 0.0383747, recall 0.793478
2017-12-09T23:36:23.770351: step 378, loss 0.590572, acc 0.808594, prec 0.0384651, recall 0.794136
2017-12-09T23:36:24.269647: step 379, loss 0.953296, acc 0.746094, prec 0.0385658, recall 0.794921
2017-12-09T23:36:24.774213: step 380, loss 0.900927, acc 0.753906, prec 0.0385798, recall 0.79531
2017-12-09T23:36:25.274856: step 381, loss 1.60215, acc 0.730469, prec 0.0386468, recall 0.795455
2017-12-09T23:36:25.785888: step 382, loss 0.867196, acc 0.742188, prec 0.0385981, recall 0.795584
2017-12-09T23:36:26.289282: step 383, loss 0.944426, acc 0.726562, prec 0.038633, recall 0.796098
2017-12-09T23:36:26.798376: step 384, loss 0.872396, acc 0.730469, prec 0.0387569, recall 0.796992
2017-12-09T23:36:27.304132: step 385, loss 0.871101, acc 0.703125, prec 0.038755, recall 0.797373
2017-12-09T23:36:27.806845: step 386, loss 0.749415, acc 0.769531, prec 0.0388023, recall 0.797879
2017-12-09T23:36:28.311710: step 387, loss 0.610386, acc 0.800781, prec 0.0388005, recall 0.798131
2017-12-09T23:36:28.819513: step 388, loss 0.741973, acc 0.832031, prec 0.0388954, recall 0.798758
2017-12-09T23:36:29.322164: step 389, loss 0.853314, acc 0.785156, prec 0.0389179, recall 0.799132
2017-12-09T23:36:29.828380: step 390, loss 0.54092, acc 0.820312, prec 0.0389509, recall 0.799505
2017-12-09T23:36:30.359400: step 391, loss 0.487846, acc 0.859375, prec 0.0390244, recall 0.8
2017-12-09T23:36:30.869101: step 392, loss 0.580229, acc 0.902344, prec 0.0391396, recall 0.800615
2017-12-09T23:36:31.379571: step 393, loss 0.371418, acc 0.902344, prec 0.0392257, recall 0.801105
2017-12-09T23:36:31.886969: step 394, loss 0.625057, acc 0.910156, prec 0.0392286, recall 0.800736
2017-12-09T23:36:32.399343: step 395, loss 3.62604, acc 0.875, prec 0.0392545, recall 0.798535
2017-12-09T23:36:32.911503: step 396, loss 1.39453, acc 0.914062, prec 0.0393462, recall 0.798054
2017-12-09T23:36:33.420460: step 397, loss 0.865856, acc 0.796875, prec 0.0394575, recall 0.798788
2017-12-09T23:36:33.932573: step 398, loss 0.607022, acc 0.789062, prec 0.0394513, recall 0.799031
2017-12-09T23:36:34.430444: step 399, loss 1.22009, acc 0.78125, prec 0.0395585, recall 0.799277
2017-12-09T23:36:34.936900: step 400, loss 0.924978, acc 0.75, prec 0.0396261, recall 0.79988
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-400

2017-12-09T23:36:36.326096: step 401, loss 0.883944, acc 0.703125, prec 0.0396792, recall 0.800479
2017-12-09T23:36:36.828724: step 402, loss 0.89893, acc 0.730469, prec 0.039655, recall 0.800718
2017-12-09T23:36:37.331660: step 403, loss 1.17093, acc 0.695312, prec 0.0396204, recall 0.800956
2017-12-09T23:36:37.840053: step 404, loss 1.17814, acc 0.671875, prec 0.0396638, recall 0.801549
2017-12-09T23:36:38.344051: step 405, loss 0.844363, acc 0.71875, prec 0.039608, recall 0.801668
2017-12-09T23:36:38.843185: step 406, loss 1.01381, acc 0.726562, prec 0.0396958, recall 0.802374
2017-12-09T23:36:39.346395: step 407, loss 0.890854, acc 0.734375, prec 0.0398418, recall 0.803308
2017-12-09T23:36:39.854475: step 408, loss 0.54325, acc 0.816406, prec 0.0398713, recall 0.803656
2017-12-09T23:36:40.365010: step 409, loss 0.964385, acc 0.757812, prec 0.0399112, recall 0.804118
2017-12-09T23:36:40.876433: step 410, loss 0.534838, acc 0.835938, prec 0.0399184, recall 0.804348
2017-12-09T23:36:41.389046: step 411, loss 0.633655, acc 0.867188, prec 0.0400746, recall 0.805149
2017-12-09T23:36:41.899020: step 412, loss 0.497018, acc 0.871094, prec 0.0401199, recall 0.805491
2017-12-09T23:36:42.407158: step 413, loss 0.275523, acc 0.882812, prec 0.0401965, recall 0.805944
2017-12-09T23:36:42.922642: step 414, loss 0.300663, acc 0.933594, prec 0.0402324, recall 0.80617
2017-12-09T23:36:43.428112: step 415, loss 0.292944, acc 0.917969, prec 0.0402636, recall 0.806395
2017-12-09T23:36:43.942613: step 416, loss 0.698634, acc 0.9375, prec 0.0403018, recall 0.806152
2017-12-09T23:36:44.450409: step 417, loss 0.420988, acc 0.929688, prec 0.0403364, recall 0.806377
2017-12-09T23:36:44.961150: step 418, loss 3.62558, acc 0.925781, prec 0.0403999, recall 0.80578
2017-12-09T23:36:45.477613: step 419, loss 1.7317, acc 0.929688, prec 0.0405468, recall 0.805987
2017-12-09T23:36:45.983733: step 420, loss 0.368899, acc 0.910156, prec 0.0406031, recall 0.806322
2017-12-09T23:36:46.495321: step 421, loss 0.731931, acc 0.890625, prec 0.0407644, recall 0.807098
2017-12-09T23:36:47.004526: step 422, loss 0.370486, acc 0.855469, prec 0.0407762, recall 0.807318
2017-12-09T23:36:47.518319: step 423, loss 0.723626, acc 0.878906, prec 0.0409058, recall 0.807977
2017-12-09T23:36:48.021723: step 424, loss 0.383916, acc 0.867188, prec 0.0408934, recall 0.808087
2017-12-09T23:36:48.534312: step 425, loss 0.857913, acc 0.792969, prec 0.0409702, recall 0.808173
2017-12-09T23:36:49.039708: step 426, loss 0.62365, acc 0.804688, prec 0.0409389, recall 0.808281
2017-12-09T23:36:49.547345: step 427, loss 0.615256, acc 0.785156, prec 0.0409018, recall 0.80839
2017-12-09T23:36:50.062594: step 428, loss 0.793358, acc 0.71875, prec 0.040845, recall 0.808499
2017-12-09T23:36:50.576615: step 429, loss 0.916015, acc 0.726562, prec 0.0408729, recall 0.808932
2017-12-09T23:36:51.077885: step 430, loss 0.589934, acc 0.777344, prec 0.0409979, recall 0.809685
2017-12-09T23:36:51.584243: step 431, loss 0.6366, acc 0.820312, prec 0.0410261, recall 0.810006
2017-12-09T23:36:52.087367: step 432, loss 1.2286, acc 0.792969, prec 0.0412652, recall 0.81072
2017-12-09T23:36:52.596295: step 433, loss 0.613579, acc 0.867188, prec 0.0414431, recall 0.811562
2017-12-09T23:36:53.097054: step 434, loss 1.03764, acc 0.816406, prec 0.0414978, recall 0.81153
2017-12-09T23:36:53.598798: step 435, loss 0.397204, acc 0.839844, prec 0.0415581, recall 0.811947
2017-12-09T23:36:54.111308: step 436, loss 1.25238, acc 0.839844, prec 0.0415394, recall 0.811154
2017-12-09T23:36:54.618096: step 437, loss 0.481331, acc 0.808594, prec 0.0415631, recall 0.811466
2017-12-09T23:36:55.120266: step 438, loss 0.404218, acc 0.875, prec 0.0415797, recall 0.811674
2017-12-09T23:36:55.627979: step 439, loss 0.469224, acc 0.851562, prec 0.0416702, recall 0.812191
2017-12-09T23:36:56.130483: step 440, loss 0.360816, acc 0.886719, prec 0.0416901, recall 0.812397
2017-12-09T23:36:56.635270: step 441, loss 0.438759, acc 0.855469, prec 0.0417276, recall 0.812705
2017-12-09T23:36:57.143917: step 442, loss 1.82469, acc 0.851562, prec 0.0417661, recall 0.812125
2017-12-09T23:36:57.645440: step 443, loss 0.608445, acc 0.863281, prec 0.0418327, recall 0.812534
2017-12-09T23:36:58.149319: step 444, loss 1.25386, acc 0.878906, prec 0.0419855, recall 0.812805
2017-12-09T23:36:58.654449: step 445, loss 0.781353, acc 0.867188, prec 0.0421344, recall 0.813074
2017-12-09T23:36:59.164149: step 446, loss 0.905908, acc 0.84375, prec 0.0421421, recall 0.812837
2017-12-09T23:36:59.667362: step 447, loss 0.582333, acc 0.863281, prec 0.0422079, recall 0.81324
2017-12-09T23:37:00.178836: step 448, loss 1.09389, acc 0.835938, prec 0.0423199, recall 0.813405
2017-12-09T23:37:00.694488: step 449, loss 0.55287, acc 0.832031, prec 0.0422692, recall 0.813405
2017-12-09T23:37:01.199344: step 450, loss 0.745698, acc 0.730469, prec 0.0422946, recall 0.813804
2017-12-09T23:37:01.703379: step 451, loss 0.682269, acc 0.769531, prec 0.0423051, recall 0.814103
2017-12-09T23:37:02.203602: step 452, loss 0.698378, acc 0.769531, prec 0.0423156, recall 0.8144
2017-12-09T23:37:02.721284: step 453, loss 0.536244, acc 0.816406, prec 0.042393, recall 0.814894
2017-12-09T23:37:03.228147: step 454, loss 1.26508, acc 0.789062, prec 0.0424103, recall 0.814756
2017-12-09T23:37:03.734639: step 455, loss 0.734036, acc 0.765625, prec 0.042393, recall 0.814952
2017-12-09T23:37:04.238524: step 456, loss 2.7521, acc 0.851562, prec 0.0424553, recall 0.814913
2017-12-09T23:37:04.742980: step 457, loss 1.06434, acc 0.867188, prec 0.0424694, recall 0.814678
2017-12-09T23:37:05.249215: step 458, loss 0.545348, acc 0.820312, prec 0.0424684, recall 0.814873
2017-12-09T23:37:05.784650: step 459, loss 0.600144, acc 0.8125, prec 0.0425964, recall 0.815554
2017-12-09T23:37:06.289543: step 460, loss 0.415777, acc 0.847656, prec 0.0426821, recall 0.816038
2017-12-09T23:37:06.794135: step 461, loss 0.436022, acc 0.859375, prec 0.0426662, recall 0.816134
2017-12-09T23:37:07.300452: step 462, loss 1.34198, acc 0.875, prec 0.0426562, recall 0.815803
2017-12-09T23:37:07.805895: step 463, loss 0.4254, acc 0.851562, prec 0.0426643, recall 0.815996
2017-12-09T23:37:08.318730: step 464, loss 2.30642, acc 0.882812, prec 0.0426828, recall 0.815762
2017-12-09T23:37:08.824799: step 465, loss 0.367511, acc 0.882812, prec 0.0426739, recall 0.815858
2017-12-09T23:37:09.332836: step 466, loss 0.449974, acc 0.910156, prec 0.0427516, recall 0.816242
2017-12-09T23:37:09.840302: step 467, loss 0.378509, acc 0.871094, prec 0.0427392, recall 0.816337
2017-12-09T23:37:10.343523: step 468, loss 1.04582, acc 0.875, prec 0.0428334, recall 0.81639
2017-12-09T23:37:10.846195: step 469, loss 0.548567, acc 0.851562, prec 0.0428672, recall 0.816675
2017-12-09T23:37:11.355227: step 470, loss 0.852244, acc 0.875, prec 0.0429599, recall 0.817149
2017-12-09T23:37:11.864541: step 471, loss 2.39859, acc 0.875, prec 0.0429769, recall 0.816495
2017-12-09T23:37:12.367883: step 472, loss 0.727735, acc 0.863281, prec 0.042988, recall 0.816684
2017-12-09T23:37:12.869053: step 473, loss 0.981659, acc 0.816406, prec 0.0430381, recall 0.816641
2017-12-09T23:37:13.374310: step 474, loss 0.638234, acc 0.816406, prec 0.0430869, recall 0.817017
2017-12-09T23:37:13.884892: step 475, loss 0.832891, acc 0.738281, prec 0.0430348, recall 0.817111
2017-12-09T23:37:14.394103: step 476, loss 1.39175, acc 0.738281, prec 0.0430872, recall 0.81716
2017-12-09T23:37:14.899743: step 477, loss 0.739941, acc 0.765625, prec 0.0430177, recall 0.81716
2017-12-09T23:37:15.403408: step 478, loss 0.897719, acc 0.699219, prec 0.0430572, recall 0.817626
2017-12-09T23:37:15.907298: step 479, loss 0.931704, acc 0.703125, prec 0.0430977, recall 0.818089
2017-12-09T23:37:16.413450: step 480, loss 0.968082, acc 0.6875, prec 0.0430567, recall 0.818274
2017-12-09T23:37:16.915711: step 481, loss 0.740682, acc 0.746094, prec 0.0430076, recall 0.818366
2017-12-09T23:37:17.421594: step 482, loss 0.752979, acc 0.742188, prec 0.0430085, recall 0.818642
2017-12-09T23:37:17.930035: step 483, loss 0.423113, acc 0.839844, prec 0.0430633, recall 0.819009
2017-12-09T23:37:18.430050: step 484, loss 0.415194, acc 0.886719, prec 0.0430556, recall 0.819101
2017-12-09T23:37:18.940489: step 485, loss 0.22176, acc 0.910156, prec 0.0431055, recall 0.819374
2017-12-09T23:37:19.447448: step 486, loss 1.11034, acc 0.894531, prec 0.0431519, recall 0.819235
2017-12-09T23:37:19.971310: step 487, loss 0.289223, acc 0.914062, prec 0.0431775, recall 0.819417
2017-12-09T23:37:20.486165: step 488, loss 1.60068, acc 0.925781, prec 0.043309, recall 0.819549
2017-12-09T23:37:20.994117: step 489, loss 1.08582, acc 0.914062, prec 0.0434621, recall 0.81977
2017-12-09T23:37:21.505656: step 490, loss 0.330301, acc 0.90625, prec 0.0435105, recall 0.82004
2017-12-09T23:37:22.007070: step 491, loss 2.1, acc 0.925781, prec 0.0435426, recall 0.818996
2017-12-09T23:37:22.513251: step 492, loss 0.449714, acc 0.867188, prec 0.0435541, recall 0.819175
2017-12-09T23:37:23.017675: step 493, loss 0.367309, acc 0.882812, prec 0.0435953, recall 0.819444
2017-12-09T23:37:23.524317: step 494, loss 1.02287, acc 0.800781, prec 0.0435631, recall 0.819128
2017-12-09T23:37:24.023744: step 495, loss 0.70105, acc 0.792969, prec 0.0435275, recall 0.819217
2017-12-09T23:37:24.527606: step 496, loss 0.684185, acc 0.804688, prec 0.0435205, recall 0.819396
2017-12-09T23:37:25.035305: step 497, loss 0.619008, acc 0.816406, prec 0.0435924, recall 0.819842
2017-12-09T23:37:25.552591: step 498, loss 0.73523, acc 0.773438, prec 0.0435512, recall 0.819931
2017-12-09T23:37:26.052257: step 499, loss 0.475262, acc 0.828125, prec 0.0435261, recall 0.82002
2017-12-09T23:37:26.231153: step 500, loss 0.44841, acc 0.769231, prec 0.0435124, recall 0.82002
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-500

2017-12-09T23:37:27.740537: step 501, loss 1.05012, acc 0.792969, prec 0.0435782, recall 0.820059
2017-12-09T23:37:28.247138: step 502, loss 0.558511, acc 0.804688, prec 0.0436212, recall 0.820412
2017-12-09T23:37:28.754234: step 503, loss 0.496866, acc 0.835938, prec 0.0436731, recall 0.820764
2017-12-09T23:37:29.257869: step 504, loss 0.319588, acc 0.882812, prec 0.0436639, recall 0.820852
2017-12-09T23:37:29.769399: step 505, loss 0.568935, acc 0.878906, prec 0.043778, recall 0.821376
2017-12-09T23:37:30.287702: step 506, loss 0.303284, acc 0.898438, prec 0.0437732, recall 0.821463
2017-12-09T23:37:30.797693: step 507, loss 0.387733, acc 0.90625, prec 0.0438205, recall 0.821724
2017-12-09T23:37:31.297009: step 508, loss 0.666133, acc 0.910156, prec 0.0438936, recall 0.822071
2017-12-09T23:37:31.798731: step 509, loss 0.255033, acc 0.917969, prec 0.0438945, recall 0.822157
2017-12-09T23:37:32.302842: step 510, loss 1.35403, acc 0.925781, prec 0.0439731, recall 0.822104
2017-12-09T23:37:32.806463: step 511, loss 0.451449, acc 0.925781, prec 0.0440506, recall 0.822448
2017-12-09T23:37:33.318287: step 512, loss 0.718931, acc 0.925781, prec 0.0441043, recall 0.822308
2017-12-09T23:37:33.821350: step 513, loss 1.20789, acc 0.890625, prec 0.0440994, recall 0.821601
2017-12-09T23:37:34.322692: step 514, loss 1.54792, acc 0.882812, prec 0.0442405, recall 0.821411
2017-12-09T23:37:34.827891: step 515, loss 1.32822, acc 0.863281, prec 0.0442521, recall 0.820795
2017-12-09T23:37:35.336472: step 516, loss 0.537504, acc 0.839844, prec 0.044304, recall 0.821138
2017-12-09T23:37:35.858377: step 517, loss 1.09515, acc 0.796875, prec 0.0443689, recall 0.821173
2017-12-09T23:37:36.363762: step 518, loss 0.764282, acc 0.769531, prec 0.0443753, recall 0.821429
2017-12-09T23:37:36.870008: step 519, loss 0.889034, acc 0.75, prec 0.0444496, recall 0.821937
2017-12-09T23:37:37.375532: step 520, loss 0.797853, acc 0.757812, prec 0.0445994, recall 0.822695
2017-12-09T23:37:37.874568: step 521, loss 0.893048, acc 0.757812, prec 0.0446509, recall 0.823113
2017-12-09T23:37:38.385371: step 522, loss 1.23681, acc 0.644531, prec 0.0446691, recall 0.823529
2017-12-09T23:37:38.888511: step 523, loss 0.960143, acc 0.695312, prec 0.0446777, recall 0.823861
2017-12-09T23:37:39.395696: step 524, loss 0.971728, acc 0.691406, prec 0.0445879, recall 0.823861
2017-12-09T23:37:39.896921: step 525, loss 0.747469, acc 0.746094, prec 0.0445144, recall 0.823861
2017-12-09T23:37:40.405994: step 526, loss 1.75913, acc 0.777344, prec 0.0444996, recall 0.82364
2017-12-09T23:37:40.907854: step 527, loss 0.485599, acc 0.871094, prec 0.0445834, recall 0.824052
2017-12-09T23:37:41.415194: step 528, loss 0.464234, acc 0.839844, prec 0.0445855, recall 0.824217
2017-12-09T23:37:41.921908: step 529, loss 1.06768, acc 0.8125, prec 0.044605, recall 0.824078
2017-12-09T23:37:42.423829: step 530, loss 0.40996, acc 0.847656, prec 0.0447057, recall 0.82457
2017-12-09T23:37:42.922701: step 531, loss 0.356827, acc 0.878906, prec 0.0447189, recall 0.824733
2017-12-09T23:37:43.422722: step 532, loss 0.426343, acc 0.847656, prec 0.0447472, recall 0.824977
2017-12-09T23:37:43.925454: step 533, loss 0.302341, acc 0.917969, prec 0.0448197, recall 0.825301
2017-12-09T23:37:44.428130: step 534, loss 1.02488, acc 0.886719, prec 0.0449311, recall 0.825786
2017-12-09T23:37:44.933729: step 535, loss 0.26403, acc 0.898438, prec 0.0449497, recall 0.825946
2017-12-09T23:37:45.441972: step 536, loss 1.12541, acc 0.910156, prec 0.0450459, recall 0.825587
2017-12-09T23:37:45.950673: step 537, loss 0.294659, acc 0.941406, prec 0.0451488, recall 0.825987
2017-12-09T23:37:46.452230: step 538, loss 1.55408, acc 0.878906, prec 0.0452118, recall 0.825549
2017-12-09T23:37:46.962905: step 539, loss 0.287953, acc 0.914062, prec 0.0452347, recall 0.825709
2017-12-09T23:37:47.464909: step 540, loss 0.488994, acc 0.878906, prec 0.0452952, recall 0.826027
2017-12-09T23:37:47.971882: step 541, loss 0.291627, acc 0.902344, prec 0.0452669, recall 0.826027
2017-12-09T23:37:48.482810: step 542, loss 0.961999, acc 0.875, prec 0.0453511, recall 0.826047
2017-12-09T23:37:48.982069: step 543, loss 0.370685, acc 0.878906, prec 0.0453399, recall 0.826127
2017-12-09T23:37:49.487946: step 544, loss 0.41755, acc 0.859375, prec 0.0453706, recall 0.826364
2017-12-09T23:37:49.993039: step 545, loss 0.676847, acc 0.851562, prec 0.045399, recall 0.8266
2017-12-09T23:37:50.504322: step 546, loss 0.913065, acc 0.816406, prec 0.0455598, recall 0.827306
2017-12-09T23:37:51.020779: step 547, loss 0.648092, acc 0.863281, prec 0.0455925, recall 0.827166
2017-12-09T23:37:51.528140: step 548, loss 0.627175, acc 0.792969, prec 0.0455325, recall 0.827166
2017-12-09T23:37:52.029606: step 549, loss 0.654366, acc 0.804688, prec 0.0455233, recall 0.827322
2017-12-09T23:37:52.527306: step 550, loss 0.461886, acc 0.867188, prec 0.0456269, recall 0.827788
2017-12-09T23:37:53.027311: step 551, loss 0.627754, acc 0.839844, prec 0.0457696, recall 0.828405
2017-12-09T23:37:53.535477: step 552, loss 0.359229, acc 0.847656, prec 0.0457726, recall 0.828559
2017-12-09T23:37:54.041696: step 553, loss 0.315259, acc 0.890625, prec 0.0457881, recall 0.828712
2017-12-09T23:37:54.550927: step 554, loss 0.571581, acc 0.824219, prec 0.0457373, recall 0.828712
2017-12-09T23:37:55.063285: step 555, loss 1.15749, acc 0.851562, prec 0.0458602, recall 0.828877
2017-12-09T23:37:55.572380: step 556, loss 0.301382, acc 0.886719, prec 0.0459685, recall 0.829333
2017-12-09T23:37:56.087111: step 557, loss 0.398231, acc 0.894531, prec 0.0460084, recall 0.829561
2017-12-09T23:37:56.598192: step 558, loss 0.410459, acc 0.882812, prec 0.0460449, recall 0.829787
2017-12-09T23:37:57.108853: step 559, loss 0.479739, acc 0.867188, prec 0.0461236, recall 0.830164
2017-12-09T23:37:57.618228: step 560, loss 0.35759, acc 0.878906, prec 0.0461353, recall 0.830314
2017-12-09T23:37:58.121339: step 561, loss 0.622171, acc 0.859375, prec 0.046235, recall 0.830762
2017-12-09T23:37:58.632007: step 562, loss 0.993308, acc 0.863281, prec 0.0462432, recall 0.830546
2017-12-09T23:37:59.136077: step 563, loss 0.857653, acc 0.914062, prec 0.0463818, recall 0.831066
2017-12-09T23:37:59.641087: step 564, loss 0.933891, acc 0.910156, prec 0.0464735, recall 0.831072
2017-12-09T23:38:00.153054: step 565, loss 0.573718, acc 0.855469, prec 0.0465713, recall 0.831515
2017-12-09T23:38:00.671290: step 566, loss 1.25701, acc 0.855469, prec 0.0465781, recall 0.830937
2017-12-09T23:38:01.179354: step 567, loss 0.411906, acc 0.886719, prec 0.0466382, recall 0.831231
2017-12-09T23:38:01.676238: step 568, loss 0.758824, acc 0.84375, prec 0.0467089, recall 0.831597
2017-12-09T23:38:02.183506: step 569, loss 0.584657, acc 0.800781, prec 0.0466973, recall 0.831743
2017-12-09T23:38:02.701304: step 570, loss 0.572763, acc 0.84375, prec 0.0467447, recall 0.832035
2017-12-09T23:38:03.201081: step 571, loss 1.66128, acc 0.785156, prec 0.046776, recall 0.831965
2017-12-09T23:38:03.712661: step 572, loss 0.505886, acc 0.839844, prec 0.0468219, recall 0.832255
2017-12-09T23:38:04.207979: step 573, loss 0.984069, acc 0.800781, prec 0.0468576, recall 0.832186
2017-12-09T23:38:04.715604: step 574, loss 0.587135, acc 0.792969, prec 0.0468436, recall 0.83233
2017-12-09T23:38:05.212132: step 575, loss 0.55287, acc 0.796875, prec 0.0468999, recall 0.83269
2017-12-09T23:38:05.736282: step 576, loss 0.730917, acc 0.769531, prec 0.0469711, recall 0.833119
2017-12-09T23:38:06.236034: step 577, loss 0.650269, acc 0.800781, prec 0.0469364, recall 0.833191
2017-12-09T23:38:06.734015: step 578, loss 0.617108, acc 0.839844, prec 0.0470277, recall 0.833618
2017-12-09T23:38:07.236911: step 579, loss 0.907464, acc 0.761719, prec 0.0470732, recall 0.833972
2017-12-09T23:38:07.744353: step 580, loss 1.4084, acc 0.882812, prec 0.047132, recall 0.8339
2017-12-09T23:38:08.245998: step 581, loss 0.354917, acc 0.847656, prec 0.0471107, recall 0.83397
2017-12-09T23:38:08.752352: step 582, loss 0.320022, acc 0.882812, prec 0.0471226, recall 0.834111
2017-12-09T23:38:09.255300: step 583, loss 0.457317, acc 0.855469, prec 0.0472177, recall 0.834532
2017-12-09T23:38:09.761504: step 584, loss 1.98702, acc 0.867188, prec 0.047226, recall 0.83432
2017-12-09T23:38:10.265227: step 585, loss 0.308862, acc 0.90625, prec 0.0472445, recall 0.834459
2017-12-09T23:38:10.778840: step 586, loss 0.483103, acc 0.90625, prec 0.0472401, recall 0.834529
2017-12-09T23:38:11.280153: step 587, loss 0.544375, acc 0.878906, prec 0.0472962, recall 0.834808
2017-12-09T23:38:11.786301: step 588, loss 0.185413, acc 0.929688, prec 0.0472986, recall 0.834878
2017-12-09T23:38:12.292100: step 589, loss 0.297439, acc 0.941406, prec 0.0473726, recall 0.835156
2017-12-09T23:38:12.801478: step 590, loss 0.285551, acc 0.921875, prec 0.0474181, recall 0.835363
2017-12-09T23:38:13.306477: step 591, loss 0.256981, acc 0.964844, prec 0.0474533, recall 0.835501
2017-12-09T23:38:13.808386: step 592, loss 0.394687, acc 0.929688, prec 0.0475691, recall 0.835915
2017-12-09T23:38:14.319055: step 593, loss 0.269537, acc 0.941406, prec 0.0476202, recall 0.83612
2017-12-09T23:38:14.826464: step 594, loss 0.276872, acc 0.921875, prec 0.0476655, recall 0.836326
2017-12-09T23:38:15.339309: step 595, loss 0.241385, acc 0.957031, prec 0.0476757, recall 0.836394
2017-12-09T23:38:15.842544: step 596, loss 0.715488, acc 0.914062, prec 0.0476519, recall 0.836045
2017-12-09T23:38:16.349025: step 597, loss 0.32683, acc 0.921875, prec 0.0476745, recall 0.836182
2017-12-09T23:38:16.856812: step 598, loss 0.40445, acc 0.898438, prec 0.0477355, recall 0.836454
2017-12-09T23:38:17.368503: step 599, loss 0.339743, acc 0.925781, prec 0.0478044, recall 0.836726
2017-12-09T23:38:17.879049: step 600, loss 0.723483, acc 0.941406, prec 0.0478337, recall 0.836515

Evaluation:
2017-12-09T23:38:22.601425: step 600, loss 1.878, acc 0.928383, prec 0.0487964, recall 0.818608

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-600

2017-12-09T23:38:24.181958: step 601, loss 0.292619, acc 0.90625, prec 0.0488134, recall 0.81875
2017-12-09T23:38:24.694460: step 602, loss 1.92035, acc 0.902344, prec 0.0488759, recall 0.818394
2017-12-09T23:38:25.209676: step 603, loss 0.810113, acc 0.878906, prec 0.0489966, recall 0.81857
2017-12-09T23:38:25.722200: step 604, loss 0.756241, acc 0.882812, prec 0.0490741, recall 0.818605
2017-12-09T23:38:26.220962: step 605, loss 0.713529, acc 0.839844, prec 0.0490936, recall 0.818815
2017-12-09T23:38:26.739585: step 606, loss 0.579151, acc 0.820312, prec 0.0491074, recall 0.819026
2017-12-09T23:38:27.246539: step 607, loss 0.590472, acc 0.789062, prec 0.049112, recall 0.819235
2017-12-09T23:38:27.750904: step 608, loss 0.706459, acc 0.753906, prec 0.0490624, recall 0.819305
2017-12-09T23:38:28.260546: step 609, loss 0.631656, acc 0.765625, prec 0.0491043, recall 0.819653
2017-12-09T23:38:28.760031: step 610, loss 0.769544, acc 0.765625, prec 0.0491459, recall 0.82
2017-12-09T23:38:29.261773: step 611, loss 0.664741, acc 0.804688, prec 0.0492426, recall 0.820483
2017-12-09T23:38:29.761121: step 612, loss 0.5003, acc 0.796875, prec 0.0492056, recall 0.820552
2017-12-09T23:38:30.279439: step 613, loss 1.07733, acc 0.773438, prec 0.0492285, recall 0.820513
2017-12-09T23:38:30.791461: step 614, loss 0.538874, acc 0.796875, prec 0.0491916, recall 0.820581
2017-12-09T23:38:31.294859: step 615, loss 0.456541, acc 0.855469, prec 0.0491499, recall 0.820581
2017-12-09T23:38:31.798242: step 616, loss 0.435221, acc 0.867188, prec 0.049177, recall 0.820787
2017-12-09T23:38:32.301382: step 617, loss 0.435221, acc 0.851562, prec 0.0491777, recall 0.820924
2017-12-09T23:38:32.797221: step 618, loss 0.278138, acc 0.917969, prec 0.0491758, recall 0.820992
2017-12-09T23:38:33.310162: step 619, loss 0.565279, acc 0.917969, prec 0.0492391, recall 0.821265
2017-12-09T23:38:33.824524: step 620, loss 1.18269, acc 0.890625, prec 0.0492739, recall 0.821157
2017-12-09T23:38:34.340073: step 621, loss 1.48085, acc 0.902344, prec 0.0492914, recall 0.820669
2017-12-09T23:38:34.854211: step 622, loss 0.425122, acc 0.890625, prec 0.0493684, recall 0.821009
2017-12-09T23:38:35.361541: step 623, loss 0.739958, acc 0.890625, prec 0.0494885, recall 0.821483
2017-12-09T23:38:35.884584: step 624, loss 0.405109, acc 0.894531, prec 0.0495013, recall 0.821618
2017-12-09T23:38:36.066455: step 625, loss 0.178746, acc 0.942308, prec 0.049498, recall 0.821618
2017-12-09T23:38:36.576924: step 626, loss 0.464916, acc 0.855469, prec 0.0494995, recall 0.821752
2017-12-09T23:38:37.084668: step 627, loss 0.317912, acc 0.890625, prec 0.0495545, recall 0.822021
2017-12-09T23:38:37.596454: step 628, loss 0.554414, acc 0.84375, prec 0.0495958, recall 0.822289
2017-12-09T23:38:38.098887: step 629, loss 0.478108, acc 0.886719, prec 0.0496925, recall 0.82269
2017-12-09T23:38:38.607048: step 630, loss 0.440036, acc 0.882812, prec 0.0497234, recall 0.822889
2017-12-09T23:38:39.107992: step 631, loss 0.928413, acc 0.882812, prec 0.0497553, recall 0.82278
2017-12-09T23:38:39.617089: step 632, loss 0.601744, acc 0.792969, prec 0.0498031, recall 0.823111
2017-12-09T23:38:40.118788: step 633, loss 0.453621, acc 0.890625, prec 0.0498575, recall 0.823376
2017-12-09T23:38:40.630834: step 634, loss 0.470001, acc 0.84375, prec 0.0498984, recall 0.823639
2017-12-09T23:38:41.130365: step 635, loss 0.382227, acc 0.847656, prec 0.0498759, recall 0.823705
2017-12-09T23:38:41.637166: step 636, loss 0.499546, acc 0.890625, prec 0.05008, recall 0.824425
2017-12-09T23:38:42.147911: step 637, loss 0.33443, acc 0.851562, prec 0.05008, recall 0.824555
2017-12-09T23:38:42.650185: step 638, loss 0.446681, acc 0.894531, prec 0.0501992, recall 0.825009
2017-12-09T23:38:43.159632: step 639, loss 0.280739, acc 0.882812, prec 0.0502508, recall 0.825268
2017-12-09T23:38:43.665918: step 640, loss 0.302532, acc 0.882812, prec 0.0502596, recall 0.825397
2017-12-09T23:38:44.172350: step 641, loss 0.179779, acc 0.925781, prec 0.0503022, recall 0.82559
2017-12-09T23:38:44.680403: step 642, loss 0.259816, acc 0.925781, prec 0.0503234, recall 0.825719
2017-12-09T23:38:45.189637: step 643, loss 0.180146, acc 0.925781, prec 0.0503658, recall 0.825911
2017-12-09T23:38:45.695738: step 644, loss 0.276367, acc 0.9375, prec 0.0503904, recall 0.826039
2017-12-09T23:38:46.203194: step 645, loss 0.15772, acc 0.949219, prec 0.0504396, recall 0.826231
2017-12-09T23:38:46.707768: step 646, loss 0.326178, acc 0.929688, prec 0.0504618, recall 0.826358
2017-12-09T23:38:47.221256: step 647, loss 0.137317, acc 0.953125, prec 0.0504695, recall 0.826422
2017-12-09T23:38:47.731230: step 648, loss 0.149037, acc 0.960938, prec 0.0505433, recall 0.826676
2017-12-09T23:38:48.242780: step 649, loss 0.0699223, acc 0.988281, prec 0.050625, recall 0.82693
2017-12-09T23:38:48.748738: step 650, loss 0.475209, acc 0.972656, prec 0.0506607, recall 0.826754
2017-12-09T23:38:49.261802: step 651, loss 0.338865, acc 0.960938, prec 0.0507344, recall 0.827007
2017-12-09T23:38:49.780178: step 652, loss 1.83777, acc 0.976562, prec 0.0508148, recall 0.826657
2017-12-09T23:38:50.291489: step 653, loss 0.515173, acc 0.960938, prec 0.0508683, recall 0.826545
2017-12-09T23:38:50.813687: step 654, loss 0.220623, acc 0.941406, prec 0.0508725, recall 0.826608
2017-12-09T23:38:51.327455: step 655, loss 0.0978257, acc 0.964844, prec 0.0509047, recall 0.826734
2017-12-09T23:38:51.846817: step 656, loss 0.659751, acc 0.9375, prec 0.0509725, recall 0.826686
2017-12-09T23:38:52.361472: step 657, loss 0.462635, acc 0.929688, prec 0.0510792, recall 0.827062
2017-12-09T23:38:52.870879: step 658, loss 0.577736, acc 0.914062, prec 0.0510765, recall 0.826826
2017-12-09T23:38:53.377412: step 659, loss 0.243859, acc 0.917969, prec 0.0511372, recall 0.827076
2017-12-09T23:38:53.888564: step 660, loss 1.15955, acc 0.878906, prec 0.0511454, recall 0.826902
2017-12-09T23:38:54.389752: step 661, loss 0.457806, acc 0.863281, prec 0.0512535, recall 0.827338
2017-12-09T23:38:54.892040: step 662, loss 0.503585, acc 0.835938, prec 0.0512689, recall 0.827524
2017-12-09T23:38:55.393632: step 663, loss 0.574037, acc 0.84375, prec 0.0513921, recall 0.828019
2017-12-09T23:38:55.902162: step 664, loss 0.502, acc 0.820312, prec 0.0513817, recall 0.828142
2017-12-09T23:38:56.405182: step 665, loss 0.563086, acc 0.792969, prec 0.0514055, recall 0.828388
2017-12-09T23:38:56.918416: step 666, loss 0.582681, acc 0.792969, prec 0.0514502, recall 0.828694
2017-12-09T23:38:57.423280: step 667, loss 0.530374, acc 0.808594, prec 0.0514784, recall 0.828938
2017-12-09T23:38:57.935638: step 668, loss 0.886955, acc 0.84375, prec 0.0515389, recall 0.828947
2017-12-09T23:38:58.439979: step 669, loss 0.792885, acc 0.839844, prec 0.051576, recall 0.82919
2017-12-09T23:38:58.944632: step 670, loss 0.408531, acc 0.875, prec 0.0516233, recall 0.829433
2017-12-09T23:38:59.442104: step 671, loss 0.428999, acc 0.839844, prec 0.0516394, recall 0.829614
2017-12-09T23:38:59.945215: step 672, loss 0.494684, acc 0.84375, prec 0.0516774, recall 0.829855
2017-12-09T23:39:00.463059: step 673, loss 0.37643, acc 0.871094, prec 0.0517442, recall 0.830155
2017-12-09T23:39:00.964641: step 674, loss 0.396517, acc 0.84375, prec 0.0517613, recall 0.830335
2017-12-09T23:39:01.471826: step 675, loss 0.672747, acc 0.90625, prec 0.0517768, recall 0.830162
2017-12-09T23:39:01.977840: step 676, loss 0.436829, acc 0.933594, prec 0.0518616, recall 0.830461
2017-12-09T23:39:02.483550: step 677, loss 0.347837, acc 0.945312, prec 0.0519289, recall 0.830699
2017-12-09T23:39:02.980189: step 678, loss 0.242249, acc 0.910156, prec 0.0519443, recall 0.830818
2017-12-09T23:39:03.478740: step 679, loss 0.333174, acc 0.910156, prec 0.0519805, recall 0.830996
2017-12-09T23:39:03.993495: step 680, loss 0.502797, acc 0.914062, prec 0.0521009, recall 0.83141
2017-12-09T23:39:04.496300: step 681, loss 0.278658, acc 0.925781, prec 0.0522038, recall 0.831763
2017-12-09T23:39:05.002383: step 682, loss 0.656467, acc 0.875, prec 0.0522306, recall 0.831649
2017-12-09T23:39:05.512690: step 683, loss 0.231169, acc 0.90625, prec 0.0522447, recall 0.831766
2017-12-09T23:39:06.028310: step 684, loss 0.354557, acc 0.914062, prec 0.0523853, recall 0.832233
2017-12-09T23:39:06.533737: step 685, loss 0.987442, acc 0.898438, prec 0.0523567, recall 0.831944
2017-12-09T23:39:07.036937: step 686, loss 0.548199, acc 0.929688, prec 0.052358, recall 0.831714
2017-12-09T23:39:07.543038: step 687, loss 0.310106, acc 0.894531, prec 0.0523685, recall 0.831831
2017-12-09T23:39:08.041062: step 688, loss 0.268454, acc 0.886719, prec 0.0523767, recall 0.831947
2017-12-09T23:39:08.550774: step 689, loss 0.693679, acc 0.914062, prec 0.052518, recall 0.832124
2017-12-09T23:39:09.055747: step 690, loss 0.413599, acc 0.894531, prec 0.0526316, recall 0.832529
2017-12-09T23:39:09.570850: step 691, loss 0.359281, acc 0.890625, prec 0.0527026, recall 0.832817
2017-12-09T23:39:10.075757: step 692, loss 0.419038, acc 0.855469, prec 0.0526808, recall 0.832875
2017-12-09T23:39:10.582250: step 693, loss 0.502228, acc 0.898438, prec 0.0528363, recall 0.83339
2017-12-09T23:39:11.084594: step 694, loss 0.318408, acc 0.894531, prec 0.0529288, recall 0.833732
2017-12-09T23:39:11.587102: step 695, loss 0.255587, acc 0.882812, prec 0.0529354, recall 0.833846
2017-12-09T23:39:12.089683: step 696, loss 0.504868, acc 0.847656, prec 0.0530344, recall 0.834243
2017-12-09T23:39:12.591278: step 697, loss 0.215001, acc 0.921875, prec 0.0530935, recall 0.834469
2017-12-09T23:39:13.097137: step 698, loss 0.356718, acc 0.894531, prec 0.0531035, recall 0.834581
2017-12-09T23:39:13.603482: step 699, loss 0.43224, acc 0.898438, prec 0.0531146, recall 0.834694
2017-12-09T23:39:14.107277: step 700, loss 0.748294, acc 0.910156, prec 0.0531303, recall 0.834523
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-700

2017-12-09T23:39:15.614955: step 701, loss 0.573839, acc 0.902344, prec 0.0532653, recall 0.834971
2017-12-09T23:39:16.126436: step 702, loss 0.196337, acc 0.949219, prec 0.0533117, recall 0.835139
2017-12-09T23:39:16.632906: step 703, loss 0.525695, acc 0.894531, prec 0.053342, recall 0.835306
2017-12-09T23:39:17.138706: step 704, loss 0.213486, acc 0.898438, prec 0.0533529, recall 0.835417
2017-12-09T23:39:17.646276: step 705, loss 0.300378, acc 0.898438, prec 0.0533434, recall 0.835473
2017-12-09T23:39:18.147616: step 706, loss 0.301619, acc 0.910156, prec 0.0533986, recall 0.835695
2017-12-09T23:39:18.657008: step 707, loss 0.24609, acc 0.917969, prec 0.0534356, recall 0.835861
2017-12-09T23:39:19.165240: step 708, loss 0.59693, acc 0.882812, prec 0.053443, recall 0.83569
2017-12-09T23:39:19.677117: step 709, loss 0.54363, acc 0.914062, prec 0.0534992, recall 0.835911
2017-12-09T23:39:20.186392: step 710, loss 0.250359, acc 0.914062, prec 0.0534942, recall 0.835966
2017-12-09T23:39:20.706584: step 711, loss 0.252286, acc 0.925781, prec 0.0535131, recall 0.836077
2017-12-09T23:39:21.206050: step 712, loss 0.283734, acc 0.925781, prec 0.0535522, recall 0.836242
2017-12-09T23:39:21.704083: step 713, loss 0.370075, acc 0.914062, prec 0.0535879, recall 0.836406
2017-12-09T23:39:22.208288: step 714, loss 0.135145, acc 0.953125, prec 0.0536148, recall 0.836516
2017-12-09T23:39:22.711846: step 715, loss 0.422709, acc 0.894531, prec 0.0536649, recall 0.836735
2017-12-09T23:39:23.222047: step 716, loss 0.827586, acc 0.90625, prec 0.0537196, recall 0.836673
2017-12-09T23:39:23.724941: step 717, loss 0.279573, acc 0.917969, prec 0.053736, recall 0.836782
2017-12-09T23:39:24.241154: step 718, loss 0.333808, acc 0.90625, prec 0.0538705, recall 0.837217
2017-12-09T23:39:24.752953: step 719, loss 0.273191, acc 0.949219, prec 0.0539366, recall 0.837434
2017-12-09T23:39:25.264567: step 720, loss 0.292395, acc 0.9375, prec 0.0540193, recall 0.837703
2017-12-09T23:39:25.773189: step 721, loss 0.628127, acc 0.910156, prec 0.0541142, recall 0.838026
2017-12-09T23:39:26.280090: step 722, loss 0.231379, acc 0.917969, prec 0.0541505, recall 0.838187
2017-12-09T23:39:26.783003: step 723, loss 0.568585, acc 0.894531, prec 0.0542203, recall 0.838454
2017-12-09T23:39:27.285324: step 724, loss 0.28045, acc 0.90625, prec 0.0542531, recall 0.838614
2017-12-09T23:39:27.790635: step 725, loss 0.276995, acc 0.910156, prec 0.0543072, recall 0.838827
2017-12-09T23:39:28.287364: step 726, loss 0.290471, acc 0.90625, prec 0.0543198, recall 0.838933
2017-12-09T23:39:28.790822: step 727, loss 0.313975, acc 0.886719, prec 0.0543265, recall 0.839039
2017-12-09T23:39:29.296031: step 728, loss 0.460128, acc 0.894531, prec 0.0543356, recall 0.839145
2017-12-09T23:39:29.812930: step 729, loss 0.274235, acc 0.910156, prec 0.0544297, recall 0.839462
2017-12-09T23:39:30.327498: step 730, loss 0.533656, acc 0.921875, prec 0.0545072, recall 0.839725
2017-12-09T23:39:30.850126: step 731, loss 0.275851, acc 0.921875, prec 0.0545242, recall 0.83983
2017-12-09T23:39:31.359190: step 732, loss 0.334366, acc 0.953125, prec 0.0545706, recall 0.839987
2017-12-09T23:39:31.864774: step 733, loss 0.206887, acc 0.921875, prec 0.0545474, recall 0.839987
2017-12-09T23:39:32.372402: step 734, loss 0.716403, acc 0.917969, prec 0.0545644, recall 0.839817
2017-12-09T23:39:32.871133: step 735, loss 0.26786, acc 0.894531, prec 0.0545532, recall 0.839869
2017-12-09T23:39:33.378453: step 736, loss 0.256993, acc 0.917969, prec 0.0546091, recall 0.840078
2017-12-09T23:39:33.884767: step 737, loss 0.866817, acc 0.917969, prec 0.0547263, recall 0.840169
2017-12-09T23:39:34.389221: step 738, loss 0.349742, acc 0.902344, prec 0.0547173, recall 0.840221
2017-12-09T23:39:34.889986: step 739, loss 0.24137, acc 0.921875, prec 0.0547542, recall 0.840377
2017-12-09T23:39:35.400995: step 740, loss 0.595723, acc 0.886719, prec 0.0547618, recall 0.840208
2017-12-09T23:39:35.933143: step 741, loss 0.469498, acc 0.894531, prec 0.0548305, recall 0.840467
2017-12-09T23:39:36.446710: step 742, loss 0.436823, acc 0.871094, prec 0.0548521, recall 0.840622
2017-12-09T23:39:36.957013: step 743, loss 1.09967, acc 0.921875, prec 0.0548912, recall 0.840233
2017-12-09T23:39:37.461097: step 744, loss 0.236968, acc 0.910156, prec 0.0549643, recall 0.840491
2017-12-09T23:39:37.970740: step 745, loss 0.556296, acc 0.925781, prec 0.0550819, recall 0.840851
2017-12-09T23:39:38.483447: step 746, loss 0.237725, acc 0.910156, prec 0.0550751, recall 0.840902
2017-12-09T23:39:38.987327: step 747, loss 0.60077, acc 0.871094, prec 0.0551563, recall 0.841209
2017-12-09T23:39:39.493558: step 748, loss 0.343748, acc 0.898438, prec 0.0551858, recall 0.841362
2017-12-09T23:39:39.999037: step 749, loss 0.381476, acc 0.878906, prec 0.0551895, recall 0.841463
2017-12-09T23:39:40.176608: step 750, loss 0.12534, acc 0.980769, prec 0.0551884, recall 0.841463
2017-12-09T23:39:40.694647: step 751, loss 0.294377, acc 0.894531, prec 0.0552365, recall 0.841667
2017-12-09T23:39:41.198595: step 752, loss 0.344349, acc 0.871094, prec 0.0552975, recall 0.84192
2017-12-09T23:39:41.699557: step 753, loss 0.36976, acc 0.890625, prec 0.0553444, recall 0.842122
2017-12-09T23:39:42.204942: step 754, loss 0.251128, acc 0.917969, prec 0.0553398, recall 0.842173
2017-12-09T23:39:42.706992: step 755, loss 0.27777, acc 0.914062, prec 0.0553539, recall 0.842273
2017-12-09T23:39:43.208394: step 756, loss 0.360042, acc 0.945312, prec 0.0554565, recall 0.842575
2017-12-09T23:39:43.716987: step 757, loss 0.409958, acc 0.960938, prec 0.0555241, recall 0.842775
2017-12-09T23:39:44.217919: step 758, loss 0.15667, acc 0.945312, prec 0.0555474, recall 0.842875
2017-12-09T23:39:44.720090: step 759, loss 0.802298, acc 0.941406, prec 0.0556103, recall 0.842807
2017-12-09T23:39:45.231035: step 760, loss 0.315578, acc 0.925781, prec 0.055687, recall 0.843056
2017-12-09T23:39:45.735384: step 761, loss 0.218793, acc 0.921875, prec 0.0557428, recall 0.843255
2017-12-09T23:39:46.242701: step 762, loss 0.397384, acc 0.914062, prec 0.0557961, recall 0.843454
2017-12-09T23:39:46.751743: step 763, loss 0.470477, acc 0.90625, prec 0.0558866, recall 0.84375
2017-12-09T23:39:47.260714: step 764, loss 0.225364, acc 0.941406, prec 0.0558888, recall 0.843799
2017-12-09T23:39:47.772217: step 765, loss 0.269882, acc 0.914062, prec 0.055942, recall 0.843996
2017-12-09T23:39:48.283033: step 766, loss 0.865851, acc 0.921875, prec 0.0559592, recall 0.843829
2017-12-09T23:39:48.801604: step 767, loss 0.337253, acc 0.917969, prec 0.0560332, recall 0.844074
2017-12-09T23:39:49.311343: step 768, loss 1.41343, acc 0.917969, prec 0.0560689, recall 0.843956
2017-12-09T23:39:49.820769: step 769, loss 0.443432, acc 0.878906, prec 0.0560917, recall 0.844103
2017-12-09T23:39:50.337606: step 770, loss 0.427626, acc 0.863281, prec 0.0560901, recall 0.844201
2017-12-09T23:39:50.855128: step 771, loss 0.360583, acc 0.898438, prec 0.0560991, recall 0.844298
2017-12-09T23:39:51.356892: step 772, loss 0.329011, acc 0.882812, prec 0.0560837, recall 0.844347
2017-12-09T23:39:51.859789: step 773, loss 0.402645, acc 0.859375, prec 0.0561202, recall 0.844542
2017-12-09T23:39:52.367126: step 774, loss 0.506318, acc 0.867188, prec 0.056159, recall 0.844736
2017-12-09T23:39:52.876613: step 775, loss 0.525189, acc 0.871094, prec 0.0562381, recall 0.845026
2017-12-09T23:39:53.378590: step 776, loss 0.693718, acc 0.84375, prec 0.0562697, recall 0.84522
2017-12-09T23:39:53.885471: step 777, loss 0.318075, acc 0.878906, prec 0.0562727, recall 0.845316
2017-12-09T23:39:54.391691: step 778, loss 0.465762, acc 0.882812, prec 0.0563354, recall 0.845556
2017-12-09T23:39:54.900112: step 779, loss 0.398219, acc 0.917969, prec 0.0564476, recall 0.845891
2017-12-09T23:39:55.407298: step 780, loss 0.343869, acc 0.886719, prec 0.0564723, recall 0.846035
2017-12-09T23:39:55.904738: step 781, loss 0.366117, acc 0.878906, prec 0.0564751, recall 0.84613
2017-12-09T23:39:56.410164: step 782, loss 0.290247, acc 0.914062, prec 0.0564884, recall 0.846225
2017-12-09T23:39:56.910521: step 783, loss 0.237499, acc 0.960938, prec 0.0565547, recall 0.846415
2017-12-09T23:39:57.412676: step 784, loss 0.179843, acc 0.933594, prec 0.0565738, recall 0.84651
2017-12-09T23:39:57.918663: step 785, loss 0.579781, acc 0.910156, prec 0.056587, recall 0.846344
2017-12-09T23:39:58.425849: step 786, loss 0.344828, acc 0.921875, prec 0.0566804, recall 0.846628
2017-12-09T23:39:58.932664: step 787, loss 0.180962, acc 0.945312, prec 0.0566641, recall 0.846628
2017-12-09T23:39:59.437166: step 788, loss 0.214844, acc 0.96875, prec 0.0566936, recall 0.846722
2017-12-09T23:39:59.940480: step 789, loss 0.135303, acc 0.960938, prec 0.0567014, recall 0.846769
2017-12-09T23:40:00.454975: step 790, loss 0.182879, acc 0.949219, prec 0.0568027, recall 0.847052
2017-12-09T23:40:00.959337: step 791, loss 1.10517, acc 0.972656, prec 0.0569328, recall 0.846861
2017-12-09T23:40:01.467122: step 792, loss 0.362033, acc 0.9375, prec 0.0569529, recall 0.846954
2017-12-09T23:40:01.972851: step 793, loss 0.856759, acc 0.949219, prec 0.0570747, recall 0.847023
2017-12-09T23:40:02.478158: step 794, loss 0.148063, acc 0.941406, prec 0.0570571, recall 0.847023
2017-12-09T23:40:02.980738: step 795, loss 0.186761, acc 0.957031, prec 0.0571023, recall 0.847163
2017-12-09T23:40:03.482258: step 796, loss 0.575225, acc 0.910156, prec 0.0571335, recall 0.847303
2017-12-09T23:40:03.984509: step 797, loss 0.340438, acc 0.929688, prec 0.0571898, recall 0.847489
2017-12-09T23:40:04.485392: step 798, loss 0.473248, acc 0.929688, prec 0.0572461, recall 0.847674
2017-12-09T23:40:04.992626: step 799, loss 0.310798, acc 0.898438, prec 0.0572929, recall 0.847859
2017-12-09T23:40:05.495101: step 800, loss 0.259778, acc 0.875, prec 0.0573713, recall 0.848136
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-800

2017-12-09T23:40:06.965307: step 801, loss 0.375595, acc 0.863281, prec 0.0574268, recall 0.848366
2017-12-09T23:40:07.462739: step 802, loss 0.37367, acc 0.871094, prec 0.0574844, recall 0.848595
2017-12-09T23:40:07.968580: step 803, loss 0.414711, acc 0.859375, prec 0.0575192, recall 0.848778
2017-12-09T23:40:08.468698: step 804, loss 0.518217, acc 0.824219, prec 0.0575049, recall 0.848869
2017-12-09T23:40:08.969761: step 805, loss 0.365985, acc 0.886719, prec 0.0575863, recall 0.849142
2017-12-09T23:40:09.480325: step 806, loss 0.417177, acc 0.859375, prec 0.0576786, recall 0.849459
2017-12-09T23:40:09.986076: step 807, loss 0.244703, acc 0.898438, prec 0.0576672, recall 0.849504
2017-12-09T23:40:10.493205: step 808, loss 0.2989, acc 0.890625, prec 0.0576919, recall 0.84964
2017-12-09T23:40:10.999690: step 809, loss 0.225346, acc 0.929688, prec 0.0576708, recall 0.84964
2017-12-09T23:40:11.496929: step 810, loss 0.372496, acc 0.933594, prec 0.0577275, recall 0.84982
2017-12-09T23:40:12.012520: step 811, loss 0.239271, acc 0.949219, prec 0.0578465, recall 0.850135
2017-12-09T23:40:12.535582: step 812, loss 0.210773, acc 0.921875, prec 0.0578422, recall 0.850179
2017-12-09T23:40:13.039217: step 813, loss 0.326417, acc 0.945312, prec 0.0579023, recall 0.850358
2017-12-09T23:40:13.549522: step 814, loss 1.36607, acc 0.933594, prec 0.057923, recall 0.84994
2017-12-09T23:40:14.052431: step 815, loss 0.7663, acc 0.960938, prec 0.0579315, recall 0.849732
2017-12-09T23:40:14.556968: step 816, loss 0.180922, acc 0.949219, prec 0.0579928, recall 0.849911
2017-12-09T23:40:15.066092: step 817, loss 0.152391, acc 0.949219, prec 0.0579966, recall 0.849955
2017-12-09T23:40:15.565326: step 818, loss 0.639679, acc 0.949219, prec 0.0581535, recall 0.850356
2017-12-09T23:40:16.071063: step 819, loss 0.294359, acc 0.921875, prec 0.0581872, recall 0.850489
2017-12-09T23:40:16.574330: step 820, loss 0.348751, acc 0.917969, prec 0.0582579, recall 0.850711
2017-12-09T23:40:17.080131: step 821, loss 0.48309, acc 0.921875, prec 0.0582916, recall 0.850843
2017-12-09T23:40:17.591820: step 822, loss 0.229282, acc 0.921875, prec 0.0583252, recall 0.850976
2017-12-09T23:40:18.088180: step 823, loss 0.277714, acc 0.894531, prec 0.0583505, recall 0.851108
2017-12-09T23:40:18.592435: step 824, loss 0.498129, acc 0.886719, prec 0.0583735, recall 0.85124
2017-12-09T23:40:19.094784: step 825, loss 0.285916, acc 0.914062, prec 0.0583665, recall 0.851284
2017-12-09T23:40:19.601798: step 826, loss 0.262469, acc 0.898438, prec 0.0583739, recall 0.851371
2017-12-09T23:40:20.107564: step 827, loss 0.324887, acc 0.925781, prec 0.0584847, recall 0.851677
2017-12-09T23:40:20.625872: step 828, loss 1.16126, acc 0.890625, prec 0.0584909, recall 0.851514
2017-12-09T23:40:21.130978: step 829, loss 0.315893, acc 0.886719, prec 0.0584566, recall 0.851514
2017-12-09T23:40:21.649769: step 830, loss 0.317748, acc 0.871094, prec 0.0584747, recall 0.851645
2017-12-09T23:40:22.148461: step 831, loss 0.376895, acc 0.847656, prec 0.0585046, recall 0.851819
2017-12-09T23:40:22.650221: step 832, loss 0.434594, acc 0.859375, prec 0.0585381, recall 0.851993
2017-12-09T23:40:23.152710: step 833, loss 0.465052, acc 0.921875, prec 0.058704, recall 0.852425
2017-12-09T23:40:23.649918: step 834, loss 0.218208, acc 0.925781, prec 0.0587194, recall 0.852512
2017-12-09T23:40:24.153859: step 835, loss 0.206888, acc 0.933594, prec 0.0587372, recall 0.852598
2017-12-09T23:40:24.663779: step 836, loss 0.213071, acc 0.925781, prec 0.0588283, recall 0.852856
2017-12-09T23:40:25.175345: step 837, loss 0.678309, acc 0.917969, prec 0.0588992, recall 0.852821
2017-12-09T23:40:25.684671: step 838, loss 0.292899, acc 0.917969, prec 0.0589877, recall 0.853078
2017-12-09T23:40:26.185114: step 839, loss 0.213591, acc 0.945312, prec 0.0590467, recall 0.853248
2017-12-09T23:40:26.689135: step 840, loss 0.254928, acc 0.929688, prec 0.0591008, recall 0.853418
2017-12-09T23:40:27.193193: step 841, loss 0.353138, acc 0.910156, prec 0.0591868, recall 0.853673
2017-12-09T23:40:27.702156: step 842, loss 0.482559, acc 0.925781, prec 0.0592585, recall 0.853884
2017-12-09T23:40:28.211335: step 843, loss 0.56543, acc 0.945312, prec 0.0592808, recall 0.853722
2017-12-09T23:40:28.716787: step 844, loss 0.297872, acc 0.925781, prec 0.0592959, recall 0.853806
2017-12-09T23:40:29.216825: step 845, loss 0.309067, acc 0.917969, prec 0.0593463, recall 0.853975
2017-12-09T23:40:29.723386: step 846, loss 0.296925, acc 0.933594, prec 0.0593826, recall 0.854101
2017-12-09T23:40:30.244141: step 847, loss 0.217996, acc 0.914062, prec 0.0593752, recall 0.854143
2017-12-09T23:40:30.748929: step 848, loss 0.232974, acc 0.921875, prec 0.0593891, recall 0.854227
2017-12-09T23:40:31.256328: step 849, loss 0.227487, acc 0.925781, prec 0.0594042, recall 0.85431
2017-12-09T23:40:31.765097: step 850, loss 0.133602, acc 0.945312, prec 0.0594251, recall 0.854394
2017-12-09T23:40:32.266263: step 851, loss 0.281536, acc 0.90625, prec 0.0594718, recall 0.854561
2017-12-09T23:40:32.776666: step 852, loss 0.268196, acc 0.933594, prec 0.0595454, recall 0.854769
2017-12-09T23:40:33.278271: step 853, loss 0.442025, acc 0.941406, prec 0.0596214, recall 0.854977
2017-12-09T23:40:33.784592: step 854, loss 0.216203, acc 0.941406, prec 0.0596411, recall 0.85506
2017-12-09T23:40:34.286385: step 855, loss 0.273809, acc 0.949219, prec 0.0597568, recall 0.855349
2017-12-09T23:40:34.792117: step 856, loss 0.267999, acc 0.949219, prec 0.0598538, recall 0.855597
2017-12-09T23:40:35.299583: step 857, loss 0.158279, acc 0.925781, prec 0.0598873, recall 0.85572
2017-12-09T23:40:35.828833: step 858, loss 0.333096, acc 0.933594, prec 0.0599793, recall 0.855966
2017-12-09T23:40:36.333034: step 859, loss 0.537815, acc 0.941406, prec 0.0600175, recall 0.856089
2017-12-09T23:40:36.836184: step 860, loss 0.186309, acc 0.933594, prec 0.060072, recall 0.856252
2017-12-09T23:40:37.346482: step 861, loss 0.234125, acc 0.921875, prec 0.0601229, recall 0.856415
2017-12-09T23:40:37.850696: step 862, loss 0.345259, acc 0.917969, prec 0.0602098, recall 0.856658
2017-12-09T23:40:38.353525: step 863, loss 0.618873, acc 0.949219, prec 0.0603063, recall 0.856901
2017-12-09T23:40:38.864980: step 864, loss 0.16729, acc 0.925781, prec 0.0603209, recall 0.856982
2017-12-09T23:40:39.365562: step 865, loss 0.196798, acc 0.941406, prec 0.0603775, recall 0.857143
2017-12-09T23:40:39.871073: step 866, loss 0.346986, acc 0.925781, prec 0.060448, recall 0.857344
2017-12-09T23:40:40.372693: step 867, loss 0.299233, acc 0.902344, prec 0.0604552, recall 0.857424
2017-12-09T23:40:40.878666: step 868, loss 0.374439, acc 0.929688, prec 0.0605082, recall 0.857584
2017-12-09T23:40:41.381989: step 869, loss 0.552251, acc 0.898438, prec 0.0605514, recall 0.857744
2017-12-09T23:40:41.887818: step 870, loss 0.396926, acc 0.929688, prec 0.0606229, recall 0.857943
2017-12-09T23:40:42.390365: step 871, loss 0.414748, acc 0.902344, prec 0.0606672, recall 0.858102
2017-12-09T23:40:42.900041: step 872, loss 0.259249, acc 0.902344, prec 0.0607487, recall 0.85834
2017-12-09T23:40:43.394143: step 873, loss 0.347298, acc 0.898438, prec 0.0607546, recall 0.858419
2017-12-09T23:40:43.893032: step 874, loss 0.331592, acc 0.882812, prec 0.0607928, recall 0.858577
2017-12-09T23:40:44.076708: step 875, loss 0.193487, acc 0.961538, prec 0.0608089, recall 0.858617
2017-12-09T23:40:44.583639: step 876, loss 0.293738, acc 0.90625, prec 0.0608358, recall 0.858735
2017-12-09T23:40:45.088337: step 877, loss 0.344008, acc 0.914062, prec 0.0608835, recall 0.858892
2017-12-09T23:40:45.597573: step 878, loss 0.20931, acc 0.933594, prec 0.0609001, recall 0.858971
2017-12-09T23:40:46.095664: step 879, loss 0.229332, acc 0.917969, prec 0.0609489, recall 0.859128
2017-12-09T23:40:46.611121: step 880, loss 0.227231, acc 0.917969, prec 0.0609422, recall 0.859167
2017-12-09T23:40:47.110759: step 881, loss 0.351149, acc 0.910156, prec 0.0610625, recall 0.859479
2017-12-09T23:40:47.614827: step 882, loss 0.27075, acc 0.9375, prec 0.0611172, recall 0.859635
2017-12-09T23:40:48.119504: step 883, loss 0.316137, acc 0.964844, prec 0.0613096, recall 0.860061
2017-12-09T23:40:48.623010: step 884, loss 0.328153, acc 0.929688, prec 0.0613064, recall 0.860099
2017-12-09T23:40:49.129995: step 885, loss 0.244954, acc 0.921875, prec 0.0613745, recall 0.860292
2017-12-09T23:40:49.632935: step 886, loss 0.159798, acc 0.953125, prec 0.0614338, recall 0.860446
2017-12-09T23:40:50.142780: step 887, loss 0.419477, acc 0.957031, prec 0.0614955, recall 0.860363
2017-12-09T23:40:50.661949: step 888, loss 0.175992, acc 0.929688, prec 0.061566, recall 0.860555
2017-12-09T23:40:51.165209: step 889, loss 0.401014, acc 0.929688, prec 0.0616179, recall 0.860707
2017-12-09T23:40:51.671035: step 890, loss 0.442293, acc 0.9375, prec 0.0617274, recall 0.860974
2017-12-09T23:40:52.179010: step 891, loss 0.1761, acc 0.953125, prec 0.0617681, recall 0.861088
2017-12-09T23:40:52.680912: step 892, loss 0.385297, acc 0.953125, prec 0.0618456, recall 0.861278
2017-12-09T23:40:53.186437: step 893, loss 0.293384, acc 0.929688, prec 0.0619157, recall 0.861467
2017-12-09T23:40:53.691815: step 894, loss 0.981404, acc 0.882812, prec 0.0619173, recall 0.861308
2017-12-09T23:40:54.197147: step 895, loss 0.224584, acc 0.902344, prec 0.0619421, recall 0.861421
2017-12-09T23:40:54.710101: step 896, loss 0.182334, acc 0.933594, prec 0.0619398, recall 0.861459
2017-12-09T23:40:55.213199: step 897, loss 0.213579, acc 0.945312, prec 0.0619962, recall 0.86161
2017-12-09T23:40:55.719825: step 898, loss 0.91547, acc 0.910156, prec 0.0620063, recall 0.861451
2017-12-09T23:40:56.229471: step 899, loss 0.250893, acc 0.914062, prec 0.0620163, recall 0.861526
2017-12-09T23:40:56.733953: step 900, loss 0.169088, acc 0.9375, prec 0.0620335, recall 0.861601

Evaluation:
2017-12-09T23:41:01.439131: step 900, loss 1.69635, acc 0.924325, prec 0.062601, recall 0.849204

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-900

2017-12-09T23:41:02.950565: step 901, loss 0.685055, acc 0.925781, prec 0.0626153, recall 0.849062
2017-12-09T23:41:03.456502: step 902, loss 0.197602, acc 0.921875, prec 0.0626453, recall 0.849179
2017-12-09T23:41:03.957645: step 903, loss 0.517663, acc 0.886719, prec 0.0626824, recall 0.849336
2017-12-09T23:41:04.471308: step 904, loss 0.323783, acc 0.878906, prec 0.0626631, recall 0.849376
2017-12-09T23:41:04.974801: step 905, loss 0.333941, acc 0.921875, prec 0.062729, recall 0.849571
2017-12-09T23:41:05.484166: step 906, loss 0.430153, acc 0.902344, prec 0.0627708, recall 0.849728
2017-12-09T23:41:05.996049: step 907, loss 0.211685, acc 0.921875, prec 0.0628186, recall 0.849883
2017-12-09T23:41:06.505259: step 908, loss 0.42742, acc 0.871094, prec 0.0628507, recall 0.850039
2017-12-09T23:41:07.008983: step 909, loss 0.181964, acc 0.921875, prec 0.0628625, recall 0.850116
2017-12-09T23:41:07.513112: step 910, loss 0.378188, acc 0.9375, prec 0.0628971, recall 0.850233
2017-12-09T23:41:08.015770: step 911, loss 0.190717, acc 0.921875, prec 0.0628909, recall 0.850272
2017-12-09T23:41:08.524428: step 912, loss 0.115078, acc 0.957031, prec 0.0629135, recall 0.850349
2017-12-09T23:41:09.031177: step 913, loss 0.189981, acc 0.929688, prec 0.0629277, recall 0.850426
2017-12-09T23:41:09.538121: step 914, loss 0.234749, acc 0.941406, prec 0.0629455, recall 0.850504
2017-12-09T23:41:10.040261: step 915, loss 0.636723, acc 0.949219, prec 0.0629669, recall 0.850361
2017-12-09T23:41:10.550429: step 916, loss 0.280277, acc 0.941406, prec 0.0630562, recall 0.850592
2017-12-09T23:41:11.056079: step 917, loss 0.200231, acc 0.921875, prec 0.0630679, recall 0.850669
2017-12-09T23:41:11.554780: step 918, loss 0.379423, acc 0.945312, prec 0.0631225, recall 0.850823
2017-12-09T23:41:12.062484: step 919, loss 0.33326, acc 0.925781, prec 0.0631533, recall 0.850938
2017-12-09T23:41:12.563981: step 920, loss 0.385152, acc 0.957031, prec 0.0632472, recall 0.851168
2017-12-09T23:41:13.070652: step 921, loss 0.25332, acc 0.949219, prec 0.0633387, recall 0.851396
2017-12-09T23:41:13.579513: step 922, loss 0.56268, acc 0.953125, prec 0.0634491, recall 0.851662
2017-12-09T23:41:14.083034: step 923, loss 0.410425, acc 0.925781, prec 0.0634797, recall 0.851776
2017-12-09T23:41:14.594904: step 924, loss 0.216491, acc 0.953125, prec 0.0635187, recall 0.85189
2017-12-09T23:41:15.101906: step 925, loss 1.00627, acc 0.945312, prec 0.0636455, recall 0.851975
2017-12-09T23:41:15.606709: step 926, loss 0.312683, acc 0.914062, prec 0.0636723, recall 0.852088
2017-12-09T23:41:16.109971: step 927, loss 0.235477, acc 0.945312, prec 0.06378, recall 0.852351
2017-12-09T23:41:16.612497: step 928, loss 0.279606, acc 0.910156, prec 0.0637877, recall 0.852426
2017-12-09T23:41:17.118428: step 929, loss 0.328397, acc 0.902344, prec 0.0638108, recall 0.852538
2017-12-09T23:41:17.632405: step 930, loss 0.288463, acc 0.898438, prec 0.0638326, recall 0.85265
2017-12-09T23:41:18.131220: step 931, loss 0.289613, acc 0.898438, prec 0.0638189, recall 0.852688
2017-12-09T23:41:18.634624: step 932, loss 0.341746, acc 0.898438, prec 0.0638762, recall 0.852874
2017-12-09T23:41:19.144153: step 933, loss 0.215643, acc 0.910156, prec 0.0638838, recall 0.852949
2017-12-09T23:41:19.644751: step 934, loss 0.211508, acc 0.914062, prec 0.0639459, recall 0.853134
2017-12-09T23:41:20.146335: step 935, loss 0.336545, acc 0.90625, prec 0.0639345, recall 0.853172
2017-12-09T23:41:20.661269: step 936, loss 0.188147, acc 0.933594, prec 0.0639494, recall 0.853246
2017-12-09T23:41:21.173036: step 937, loss 0.174121, acc 0.925781, prec 0.0639618, recall 0.85332
2017-12-09T23:41:21.678351: step 938, loss 1.44298, acc 0.9375, prec 0.063998, recall 0.853001
2017-12-09T23:41:22.184476: step 939, loss 0.128667, acc 0.9375, prec 0.0640672, recall 0.853186
2017-12-09T23:41:22.688832: step 940, loss 0.441167, acc 0.914062, prec 0.0640936, recall 0.853296
2017-12-09T23:41:23.193089: step 941, loss 0.291282, acc 0.933594, prec 0.0641084, recall 0.85337
2017-12-09T23:41:23.705409: step 942, loss 0.459507, acc 0.929688, prec 0.064228, recall 0.853665
2017-12-09T23:41:24.215098: step 943, loss 0.317822, acc 0.929688, prec 0.0642591, recall 0.853775
2017-12-09T23:41:24.725530: step 944, loss 0.410465, acc 0.921875, prec 0.0643408, recall 0.853994
2017-12-09T23:41:25.229705: step 945, loss 0.245208, acc 0.910156, prec 0.0643659, recall 0.854104
2017-12-09T23:41:25.742345: step 946, loss 0.406731, acc 0.945312, prec 0.0644371, recall 0.854286
2017-12-09T23:41:26.256205: step 947, loss 0.374554, acc 0.914062, prec 0.0644985, recall 0.854468
2017-12-09T23:41:26.761776: step 948, loss 0.306971, acc 0.898438, prec 0.0645022, recall 0.854541
2017-12-09T23:41:27.265729: step 949, loss 0.255264, acc 0.921875, prec 0.0645131, recall 0.854613
2017-12-09T23:41:27.764564: step 950, loss 0.267348, acc 0.875, prec 0.0645095, recall 0.854686
2017-12-09T23:41:28.262399: step 951, loss 0.304526, acc 0.890625, prec 0.0645458, recall 0.854831
2017-12-09T23:41:28.768250: step 952, loss 0.456655, acc 0.910156, prec 0.0645707, recall 0.854939
2017-12-09T23:41:29.271429: step 953, loss 0.37457, acc 0.945312, prec 0.0646591, recall 0.855155
2017-12-09T23:41:29.780070: step 954, loss 0.276518, acc 0.921875, prec 0.0647227, recall 0.855335
2017-12-09T23:41:30.300633: step 955, loss 0.199381, acc 0.9375, prec 0.0648261, recall 0.855586
2017-12-09T23:41:30.819015: step 956, loss 0.276501, acc 0.9375, prec 0.0648944, recall 0.855764
2017-12-09T23:41:31.324371: step 957, loss 0.298847, acc 0.925781, prec 0.0649414, recall 0.855907
2017-12-09T23:41:31.835823: step 958, loss 0.394023, acc 0.933594, prec 0.0650609, recall 0.856191
2017-12-09T23:41:32.355363: step 959, loss 0.34117, acc 0.933594, prec 0.0650752, recall 0.856262
2017-12-09T23:41:32.860988: step 960, loss 0.325132, acc 0.925781, prec 0.0651571, recall 0.856475
2017-12-09T23:41:33.365777: step 961, loss 0.301337, acc 0.894531, prec 0.0651592, recall 0.856545
2017-12-09T23:41:33.872881: step 962, loss 0.1741, acc 0.957031, prec 0.0651983, recall 0.856651
2017-12-09T23:41:34.377262: step 963, loss 0.230316, acc 0.929688, prec 0.0651938, recall 0.856686
2017-12-09T23:41:34.888430: step 964, loss 0.221192, acc 0.933594, prec 0.0652255, recall 0.856792
2017-12-09T23:41:35.394843: step 965, loss 0.266769, acc 0.910156, prec 0.0652499, recall 0.856897
2017-12-09T23:41:35.911619: step 966, loss 0.25867, acc 0.933594, prec 0.065299, recall 0.857038
2017-12-09T23:41:36.423306: step 967, loss 0.206513, acc 0.925781, prec 0.0653282, recall 0.857143
2017-12-09T23:41:36.935375: step 968, loss 0.231154, acc 0.957031, prec 0.0653497, recall 0.857213
2017-12-09T23:41:37.439648: step 969, loss 0.253366, acc 0.9375, prec 0.0654, recall 0.857353
2017-12-09T23:41:37.945943: step 970, loss 0.38148, acc 0.945312, prec 0.065505, recall 0.857596
2017-12-09T23:41:38.459455: step 971, loss 0.157114, acc 0.957031, prec 0.0655438, recall 0.857701
2017-12-09T23:41:38.964101: step 972, loss 0.403907, acc 0.949219, prec 0.0655976, recall 0.85784
2017-12-09T23:41:39.468729: step 973, loss 0.5905, acc 0.945312, prec 0.0655991, recall 0.857665
2017-12-09T23:41:39.981004: step 974, loss 3.00255, acc 0.898438, prec 0.0656929, recall 0.857282
2017-12-09T23:41:40.488938: step 975, loss 0.162226, acc 0.949219, prec 0.0657466, recall 0.85742
2017-12-09T23:41:41.002829: step 976, loss 0.252664, acc 0.90625, prec 0.0658042, recall 0.857593
2017-12-09T23:41:41.511303: step 977, loss 0.488342, acc 0.941406, prec 0.0658044, recall 0.85742
2017-12-09T23:41:42.020223: step 978, loss 0.400485, acc 0.902344, prec 0.0658607, recall 0.857593
2017-12-09T23:41:42.518071: step 979, loss 0.292649, acc 0.890625, prec 0.0658611, recall 0.857662
2017-12-09T23:41:43.020846: step 980, loss 0.351651, acc 0.867188, prec 0.0658369, recall 0.857696
2017-12-09T23:41:43.524755: step 981, loss 0.459303, acc 0.875, prec 0.0658845, recall 0.857868
2017-12-09T23:41:44.036666: step 982, loss 0.406432, acc 0.832031, prec 0.0659012, recall 0.858005
2017-12-09T23:41:44.543294: step 983, loss 0.419918, acc 0.824219, prec 0.0659328, recall 0.858177
2017-12-09T23:41:45.049529: step 984, loss 0.330647, acc 0.898438, prec 0.0659357, recall 0.858245
2017-12-09T23:41:45.555385: step 985, loss 0.273584, acc 0.898438, prec 0.0659212, recall 0.858279
2017-12-09T23:41:46.067545: step 986, loss 0.284214, acc 0.890625, prec 0.0659044, recall 0.858313
2017-12-09T23:41:46.570251: step 987, loss 0.258807, acc 0.910156, prec 0.0659109, recall 0.858382
2017-12-09T23:41:47.069435: step 988, loss 0.253941, acc 0.910156, prec 0.0659347, recall 0.858484
2017-12-09T23:41:47.583092: step 989, loss 0.208412, acc 0.917969, prec 0.0659436, recall 0.858552
2017-12-09T23:41:48.090619: step 990, loss 0.24379, acc 0.953125, prec 0.0660153, recall 0.858722
2017-12-09T23:41:48.602348: step 991, loss 0.114246, acc 0.960938, prec 0.0660548, recall 0.858824
2017-12-09T23:41:49.112106: step 992, loss 1.28688, acc 0.964844, prec 0.066114, recall 0.858753
2017-12-09T23:41:49.621415: step 993, loss 0.150493, acc 0.972656, prec 0.0661917, recall 0.858922
2017-12-09T23:41:50.130734: step 994, loss 0.207867, acc 0.9375, prec 0.0662238, recall 0.859023
2017-12-09T23:41:50.655902: step 995, loss 0.301832, acc 0.960938, prec 0.0662633, recall 0.859125
2017-12-09T23:41:51.163299: step 996, loss 0.267392, acc 0.957031, prec 0.0663015, recall 0.859226
2017-12-09T23:41:51.665574: step 997, loss 0.395673, acc 0.972656, prec 0.0663458, recall 0.859121
2017-12-09T23:41:52.173930: step 998, loss 0.688726, acc 0.9375, prec 0.0664651, recall 0.859185
2017-12-09T23:41:52.690805: step 999, loss 0.173859, acc 0.945312, prec 0.0665684, recall 0.85942
2017-12-09T23:41:52.873047: step 1000, loss 0.169236, acc 0.961538, prec 0.0665659, recall 0.85942
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-1000

2017-12-09T23:41:54.350623: step 1001, loss 0.211254, acc 0.964844, prec 0.0666065, recall 0.85952
2017-12-09T23:41:54.861379: step 1002, loss 0.193408, acc 0.964844, prec 0.0666642, recall 0.859653
2017-12-09T23:41:55.374415: step 1003, loss 0.419964, acc 0.949219, prec 0.0667513, recall 0.859853
2017-12-09T23:41:55.888689: step 1004, loss 0.211586, acc 0.941406, prec 0.0668016, recall 0.859986
2017-12-09T23:41:56.398260: step 1005, loss 0.200956, acc 0.917969, prec 0.0668445, recall 0.860118
2017-12-09T23:41:56.902759: step 1006, loss 0.181061, acc 0.914062, prec 0.0668518, recall 0.860185
2017-12-09T23:41:57.409528: step 1007, loss 0.316073, acc 0.886719, prec 0.0668676, recall 0.860284
2017-12-09T23:41:57.917743: step 1008, loss 0.181871, acc 0.933594, prec 0.0668981, recall 0.860383
2017-12-09T23:41:58.421768: step 1009, loss 0.211811, acc 0.933594, prec 0.0669458, recall 0.860515
2017-12-09T23:41:58.928574: step 1010, loss 0.172512, acc 0.945312, prec 0.0670142, recall 0.860679
2017-12-09T23:41:59.433905: step 1011, loss 0.212095, acc 0.941406, prec 0.0670129, recall 0.860712
2017-12-09T23:41:59.944802: step 1012, loss 0.140411, acc 0.960938, prec 0.0670348, recall 0.860777
2017-12-09T23:42:00.464005: step 1013, loss 0.170945, acc 0.941406, prec 0.0670506, recall 0.860843
2017-12-09T23:42:00.972330: step 1014, loss 0.199841, acc 0.925781, prec 0.0670785, recall 0.860941
2017-12-09T23:42:01.474978: step 1015, loss 0.238399, acc 0.972656, prec 0.0671554, recall 0.861105
2017-12-09T23:42:01.979403: step 1016, loss 0.588051, acc 0.9375, prec 0.0671882, recall 0.861
2017-12-09T23:42:02.489401: step 1017, loss 0.203361, acc 0.957031, prec 0.0672089, recall 0.861066
2017-12-09T23:42:02.991017: step 1018, loss 0.191832, acc 0.933594, prec 0.0672563, recall 0.861196
2017-12-09T23:42:03.493809: step 1019, loss 0.123772, acc 0.964844, prec 0.0672623, recall 0.861228
2017-12-09T23:42:04.002465: step 1020, loss 0.333578, acc 0.960938, prec 0.0672841, recall 0.861293
2017-12-09T23:42:04.509559: step 1021, loss 0.292884, acc 0.949219, prec 0.0673705, recall 0.861488
2017-12-09T23:42:05.012094: step 1022, loss 0.0783409, acc 0.96875, prec 0.0673777, recall 0.86152
2017-12-09T23:42:05.520645: step 1023, loss 0.237548, acc 0.945312, prec 0.0673775, recall 0.861553
2017-12-09T23:42:06.028193: step 1024, loss 0.165323, acc 0.957031, prec 0.0674492, recall 0.861715
2017-12-09T23:42:06.534945: step 1025, loss 0.202472, acc 0.964844, prec 0.0674893, recall 0.861811
2017-12-09T23:42:07.040288: step 1026, loss 0.263111, acc 0.933594, prec 0.0675365, recall 0.86194
2017-12-09T23:42:07.546507: step 1027, loss 0.165636, acc 0.953125, prec 0.0675728, recall 0.862037
2017-12-09T23:42:08.047677: step 1028, loss 0.166866, acc 0.957031, prec 0.0675932, recall 0.862101
2017-12-09T23:42:08.556140: step 1029, loss 0.203063, acc 0.945312, prec 0.067627, recall 0.862197
2017-12-09T23:42:09.055663: step 1030, loss 0.149463, acc 0.949219, prec 0.067662, recall 0.862294
2017-12-09T23:42:09.565618: step 1031, loss 0.170691, acc 0.949219, prec 0.0677481, recall 0.862485
2017-12-09T23:42:10.073186: step 1032, loss 0.14848, acc 0.964844, prec 0.067805, recall 0.862613
2017-12-09T23:42:10.577891: step 1033, loss 0.134168, acc 0.960938, prec 0.0678606, recall 0.862741
2017-12-09T23:42:11.084910: step 1034, loss 0.335916, acc 0.945312, prec 0.0678943, recall 0.862836
2017-12-09T23:42:11.599531: step 1035, loss 0.641758, acc 0.953125, prec 0.0679486, recall 0.862763
2017-12-09T23:42:12.102809: step 1036, loss 0.289561, acc 0.953125, prec 0.0679677, recall 0.862827
2017-12-09T23:42:12.609825: step 1037, loss 0.275332, acc 0.957031, prec 0.068005, recall 0.862922
2017-12-09T23:42:13.120950: step 1038, loss 0.194419, acc 0.9375, prec 0.0680531, recall 0.863048
2017-12-09T23:42:13.620752: step 1039, loss 0.271679, acc 0.921875, prec 0.0680623, recall 0.863112
2017-12-09T23:42:14.132040: step 1040, loss 0.17951, acc 0.949219, prec 0.0681309, recall 0.86327
2017-12-09T23:42:14.641271: step 1041, loss 0.377332, acc 0.949219, prec 0.0681657, recall 0.863364
2017-12-09T23:42:15.150386: step 1042, loss 0.247836, acc 0.929688, prec 0.0682281, recall 0.863521
2017-12-09T23:42:15.647083: step 1043, loss 0.272927, acc 0.953125, prec 0.068281, recall 0.863647
2017-12-09T23:42:16.154537: step 1044, loss 0.234487, acc 0.910156, prec 0.0683032, recall 0.863741
2017-12-09T23:42:16.659371: step 1045, loss 0.220857, acc 0.925781, prec 0.0683304, recall 0.863835
2017-12-09T23:42:17.167731: step 1046, loss 0.241066, acc 0.929688, prec 0.0683588, recall 0.863928
2017-12-09T23:42:17.666720: step 1047, loss 0.198358, acc 0.921875, prec 0.0683848, recall 0.864022
2017-12-09T23:42:18.176167: step 1048, loss 0.309509, acc 0.894531, prec 0.068402, recall 0.864115
2017-12-09T23:42:18.678262: step 1049, loss 0.200562, acc 0.941406, prec 0.0684678, recall 0.864271
2017-12-09T23:42:19.187848: step 1050, loss 0.247571, acc 0.941406, prec 0.0685505, recall 0.864457
2017-12-09T23:42:19.693501: step 1051, loss 0.153011, acc 0.945312, prec 0.06855, recall 0.864488
2017-12-09T23:42:20.198890: step 1052, loss 0.132016, acc 0.957031, prec 0.0685701, recall 0.86455
2017-12-09T23:42:20.722037: step 1053, loss 0.151147, acc 0.9375, prec 0.068584, recall 0.864612
2017-12-09T23:42:21.224451: step 1054, loss 0.25836, acc 0.933594, prec 0.0686303, recall 0.864735
2017-12-09T23:42:21.730649: step 1055, loss 0.192621, acc 0.960938, prec 0.0686516, recall 0.864797
2017-12-09T23:42:22.241636: step 1056, loss 0.240116, acc 0.941406, prec 0.0686835, recall 0.86489
2017-12-09T23:42:22.750214: step 1057, loss 0.32789, acc 0.953125, prec 0.0687697, recall 0.865074
2017-12-09T23:42:23.260481: step 1058, loss 0.545376, acc 0.964844, prec 0.0688595, recall 0.865258
2017-12-09T23:42:23.768699: step 1059, loss 0.164962, acc 0.941406, prec 0.0688913, recall 0.86535
2017-12-09T23:42:24.277486: step 1060, loss 0.215177, acc 0.953125, prec 0.0689269, recall 0.865441
2017-12-09T23:42:24.784819: step 1061, loss 0.134594, acc 0.960938, prec 0.0689481, recall 0.865502
2017-12-09T23:42:25.290921: step 1062, loss 0.60897, acc 0.933594, prec 0.069029, recall 0.865489
2017-12-09T23:42:25.795501: step 1063, loss 0.297383, acc 0.953125, prec 0.069199, recall 0.865823
2017-12-09T23:42:26.305286: step 1064, loss 0.167449, acc 0.953125, prec 0.069268, recall 0.865975
2017-12-09T23:42:26.812141: step 1065, loss 0.207769, acc 0.933594, prec 0.0692635, recall 0.866005
2017-12-09T23:42:27.321071: step 1066, loss 0.247072, acc 0.933594, prec 0.0692926, recall 0.866096
2017-12-09T23:42:27.834308: step 1067, loss 0.25318, acc 0.910156, prec 0.0692807, recall 0.866126
2017-12-09T23:42:28.335742: step 1068, loss 0.521646, acc 0.898438, prec 0.0693656, recall 0.866337
2017-12-09T23:42:28.842559: step 1069, loss 0.232425, acc 0.921875, prec 0.0693742, recall 0.866397
2017-12-09T23:42:29.351813: step 1070, loss 0.360765, acc 0.921875, prec 0.0694162, recall 0.866517
2017-12-09T23:42:29.860427: step 1071, loss 0.172545, acc 0.941406, prec 0.0694644, recall 0.866637
2017-12-09T23:42:30.376698: step 1072, loss 0.169754, acc 0.957031, prec 0.0694842, recall 0.866697
2017-12-09T23:42:30.892721: step 1073, loss 0.192114, acc 0.933594, prec 0.0695299, recall 0.866816
2017-12-09T23:42:31.396230: step 1074, loss 0.36147, acc 0.910156, prec 0.069568, recall 0.866935
2017-12-09T23:42:31.898665: step 1075, loss 0.154785, acc 0.941406, prec 0.0695827, recall 0.866995
2017-12-09T23:42:32.399943: step 1076, loss 0.248176, acc 0.925781, prec 0.0695757, recall 0.867025
2017-12-09T23:42:32.910159: step 1077, loss 0.131284, acc 0.945312, prec 0.0696417, recall 0.867174
2017-12-09T23:42:33.429649: step 1078, loss 0.141643, acc 0.945312, prec 0.0696576, recall 0.867233
2017-12-09T23:42:33.934225: step 1079, loss 0.154276, acc 0.960938, prec 0.0696618, recall 0.867263
2017-12-09T23:42:34.435515: step 1080, loss 0.25855, acc 0.953125, prec 0.0697136, recall 0.867381
2017-12-09T23:42:34.940483: step 1081, loss 0.281632, acc 0.949219, prec 0.0697808, recall 0.867529
2017-12-09T23:42:35.450092: step 1082, loss 0.438451, acc 0.949219, prec 0.0697825, recall 0.867365
2017-12-09T23:42:35.971719: step 1083, loss 0.165106, acc 0.964844, prec 0.0698546, recall 0.867513
2017-12-09T23:42:36.485858: step 1084, loss 0.328787, acc 0.941406, prec 0.0699358, recall 0.86769
2017-12-09T23:42:36.989685: step 1085, loss 0.289833, acc 0.964844, prec 0.0700245, recall 0.867866
2017-12-09T23:42:37.499726: step 1086, loss 0.0860345, acc 0.96875, prec 0.0700478, recall 0.867925
2017-12-09T23:42:38.002438: step 1087, loss 0.179496, acc 0.964844, prec 0.0701365, recall 0.8681
2017-12-09T23:42:38.511475: step 1088, loss 0.137038, acc 0.949219, prec 0.0701701, recall 0.868188
2017-12-09T23:42:39.015405: step 1089, loss 0.360318, acc 0.921875, prec 0.0702115, recall 0.868305
2017-12-09T23:42:39.521817: step 1090, loss 0.137167, acc 0.945312, prec 0.0702272, recall 0.868363
2017-12-09T23:42:40.026488: step 1091, loss 0.234678, acc 0.953125, prec 0.0703119, recall 0.868537
2017-12-09T23:42:40.531661: step 1092, loss 0.112473, acc 0.953125, prec 0.0703135, recall 0.868566
2017-12-09T23:42:41.037067: step 1093, loss 0.179691, acc 0.949219, prec 0.070347, recall 0.868653
2017-12-09T23:42:41.540496: step 1094, loss 0.186661, acc 0.945312, prec 0.0704457, recall 0.868856
2017-12-09T23:42:42.048251: step 1095, loss 0.175326, acc 0.941406, prec 0.0705098, recall 0.869
2017-12-09T23:42:42.564510: step 1096, loss 0.9042, acc 0.921875, prec 0.0705357, recall 0.868896
2017-12-09T23:42:43.077009: step 1097, loss 0.116674, acc 0.960938, prec 0.0705563, recall 0.868953
2017-12-09T23:42:43.576897: step 1098, loss 0.209202, acc 0.953125, prec 0.0706076, recall 0.869069
2017-12-09T23:42:44.077808: step 1099, loss 0.171533, acc 0.960938, prec 0.070711, recall 0.86927
2017-12-09T23:42:44.581235: step 1100, loss 0.153899, acc 0.953125, prec 0.0707125, recall 0.869298
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-1100

2017-12-09T23:42:46.125857: step 1101, loss 0.283498, acc 0.949219, prec 0.0707789, recall 0.869441
2017-12-09T23:42:46.632471: step 1102, loss 0.131832, acc 0.960938, prec 0.0707995, recall 0.869499
2017-12-09T23:42:47.138618: step 1103, loss 0.191886, acc 0.9375, prec 0.0708455, recall 0.869613
2017-12-09T23:42:47.645430: step 1104, loss 1.09745, acc 0.9375, prec 0.0708775, recall 0.869318
2017-12-09T23:42:48.154323: step 1105, loss 0.152767, acc 0.949219, prec 0.0709604, recall 0.869489
2017-12-09T23:42:48.655036: step 1106, loss 0.371704, acc 0.929688, prec 0.0710203, recall 0.869632
2017-12-09T23:42:49.165587: step 1107, loss 0.201966, acc 0.9375, prec 0.0710332, recall 0.869688
2017-12-09T23:42:49.667844: step 1108, loss 0.38532, acc 0.910156, prec 0.0711033, recall 0.869859
2017-12-09T23:42:50.169935: step 1109, loss 0.373148, acc 0.882812, prec 0.0710984, recall 0.869915
2017-12-09T23:42:50.686356: step 1110, loss 0.280981, acc 0.921875, prec 0.0711557, recall 0.870057
2017-12-09T23:42:51.184440: step 1111, loss 0.327605, acc 0.925781, prec 0.0712471, recall 0.870254
2017-12-09T23:42:51.692142: step 1112, loss 0.287694, acc 0.925781, prec 0.0713056, recall 0.870394
2017-12-09T23:42:52.202771: step 1113, loss 0.335271, acc 0.90625, prec 0.0713246, recall 0.870479
2017-12-09T23:42:52.704666: step 1114, loss 0.341249, acc 0.875, prec 0.0713336, recall 0.870563
2017-12-09T23:42:53.209093: step 1115, loss 0.353904, acc 0.894531, prec 0.0714311, recall 0.870786
2017-12-09T23:42:53.717336: step 1116, loss 0.354415, acc 0.90625, prec 0.0714994, recall 0.870954
2017-12-09T23:42:54.228018: step 1117, loss 0.220067, acc 0.941406, prec 0.0715133, recall 0.871009
2017-12-09T23:42:54.740489: step 1118, loss 0.237607, acc 0.925781, prec 0.0715386, recall 0.871093
2017-12-09T23:42:55.248460: step 1119, loss 0.439443, acc 0.929688, prec 0.0716801, recall 0.87137
2017-12-09T23:42:55.759544: step 1120, loss 0.270489, acc 0.9375, prec 0.0717255, recall 0.871481
2017-12-09T23:42:56.271665: step 1121, loss 0.204909, acc 0.945312, prec 0.0717734, recall 0.871591
2017-12-09T23:42:56.778120: step 1122, loss 0.715364, acc 0.945312, prec 0.0718061, recall 0.871487
2017-12-09T23:42:57.286123: step 1123, loss 0.208035, acc 0.925781, prec 0.0718968, recall 0.87168
2017-12-09T23:42:57.794465: step 1124, loss 0.348119, acc 0.921875, prec 0.0719698, recall 0.871844
2017-12-09T23:42:57.976466: step 1125, loss 0.164234, acc 0.903846, prec 0.0719634, recall 0.871844
2017-12-09T23:42:58.492773: step 1126, loss 0.250251, acc 0.941406, prec 0.0719771, recall 0.871899
2017-12-09T23:42:59.000561: step 1127, loss 0.211768, acc 0.929688, prec 0.0720361, recall 0.872036
2017-12-09T23:42:59.504614: step 1128, loss 0.255512, acc 0.96875, prec 0.0721569, recall 0.872254
2017-12-09T23:43:00.017545: step 1129, loss 0.257713, acc 0.933594, prec 0.0722171, recall 0.87239
2017-12-09T23:43:00.532554: step 1130, loss 0.207789, acc 0.941406, prec 0.0722471, recall 0.872472
2017-12-09T23:43:01.035446: step 1131, loss 0.257696, acc 0.957031, prec 0.0723476, recall 0.872662
2017-12-09T23:43:01.545689: step 1132, loss 0.136019, acc 0.949219, prec 0.0724127, recall 0.872797
2017-12-09T23:43:02.055130: step 1133, loss 0.0948368, acc 0.960938, prec 0.072449, recall 0.872878
2017-12-09T23:43:02.560949: step 1134, loss 0.148196, acc 0.964844, prec 0.0724865, recall 0.872959
2017-12-09T23:43:03.065764: step 1135, loss 0.120108, acc 0.949219, prec 0.0725026, recall 0.873012
2017-12-09T23:43:03.575966: step 1136, loss 0.122503, acc 0.960938, prec 0.0725224, recall 0.873066
2017-12-09T23:43:04.078847: step 1137, loss 0.389598, acc 0.929688, prec 0.0725974, recall 0.873228
2017-12-09T23:43:04.584230: step 1138, loss 0.162673, acc 0.96875, prec 0.0726198, recall 0.873281
2017-12-09T23:43:05.089450: step 1139, loss 0.234907, acc 0.957031, prec 0.0727199, recall 0.873469
2017-12-09T23:43:05.606361: step 1140, loss 0.21368, acc 0.964844, prec 0.0728551, recall 0.873709
2017-12-09T23:43:06.120250: step 1141, loss 0.198593, acc 0.949219, prec 0.0728711, recall 0.873762
2017-12-09T23:43:06.626005: step 1142, loss 0.138342, acc 0.960938, prec 0.0728583, recall 0.873762
2017-12-09T23:43:07.133138: step 1143, loss 0.23275, acc 0.960938, prec 0.0729432, recall 0.873921
2017-12-09T23:43:07.638513: step 1144, loss 0.0972316, acc 0.960938, prec 0.0729467, recall 0.873948
2017-12-09T23:43:08.135224: step 1145, loss 0.158401, acc 0.972656, prec 0.0729866, recall 0.874027
2017-12-09T23:43:08.639921: step 1146, loss 0.160394, acc 0.964844, prec 0.0730238, recall 0.874107
2017-12-09T23:43:09.151898: step 1147, loss 0.153508, acc 0.957031, prec 0.0730748, recall 0.874213
2017-12-09T23:43:09.659152: step 1148, loss 0.244458, acc 0.980469, prec 0.0731172, recall 0.874292
2017-12-09T23:43:10.170467: step 1149, loss 0.165993, acc 0.964844, prec 0.0732033, recall 0.87445
2017-12-09T23:43:10.682712: step 1150, loss 0.171781, acc 0.960938, prec 0.0732229, recall 0.874502
2017-12-09T23:43:11.193336: step 1151, loss 0.108591, acc 0.953125, prec 0.07324, recall 0.874555
2017-12-09T23:43:11.702980: step 1152, loss 0.0993257, acc 0.972656, prec 0.0733285, recall 0.874712
2017-12-09T23:43:12.208148: step 1153, loss 0.212883, acc 0.957031, prec 0.0733794, recall 0.874817
2017-12-09T23:43:12.721871: step 1154, loss 0.303528, acc 0.964844, prec 0.0734328, recall 0.874922
2017-12-09T23:43:13.221039: step 1155, loss 0.113636, acc 0.96875, prec 0.0734874, recall 0.875026
2017-12-09T23:43:13.723435: step 1156, loss 0.665679, acc 0.964844, prec 0.073542, recall 0.874948
2017-12-09T23:43:14.227044: step 1157, loss 0.339444, acc 0.945312, prec 0.0735727, recall 0.875026
2017-12-09T23:43:14.727130: step 1158, loss 0.121349, acc 0.964844, prec 0.0735611, recall 0.875026
2017-12-09T23:43:15.233334: step 1159, loss 0.198553, acc 0.921875, prec 0.0735678, recall 0.875078
2017-12-09T23:43:15.735392: step 1160, loss 0.110425, acc 0.964844, prec 0.073621, recall 0.875182
2017-12-09T23:43:16.236945: step 1161, loss 0.20412, acc 0.9375, prec 0.0736328, recall 0.875234
2017-12-09T23:43:16.740985: step 1162, loss 0.244661, acc 0.933594, prec 0.0736919, recall 0.875364
2017-12-09T23:43:17.242937: step 1163, loss 0.170888, acc 0.957031, prec 0.073694, recall 0.875389
2017-12-09T23:43:17.742752: step 1164, loss 0.223908, acc 0.941406, prec 0.073788, recall 0.87557
2017-12-09T23:43:18.245242: step 1165, loss 0.159804, acc 0.925781, prec 0.0738282, recall 0.875673
2017-12-09T23:43:18.746851: step 1166, loss 0.133477, acc 0.953125, prec 0.0738451, recall 0.875725
2017-12-09T23:43:19.257753: step 1167, loss 0.1966, acc 0.949219, prec 0.0738768, recall 0.875802
2017-12-09T23:43:19.762383: step 1168, loss 0.171383, acc 0.929688, prec 0.0738859, recall 0.875853
2017-12-09T23:43:20.271902: step 1169, loss 0.153499, acc 0.953125, prec 0.073919, recall 0.875931
2017-12-09T23:43:20.794694: step 1170, loss 0.166047, acc 0.945312, prec 0.0739817, recall 0.876059
2017-12-09T23:43:21.300964: step 1171, loss 0.38287, acc 0.957031, prec 0.0740321, recall 0.876161
2017-12-09T23:43:21.811343: step 1172, loss 0.330838, acc 0.980469, prec 0.0741548, recall 0.876365
2017-12-09T23:43:22.319981: step 1173, loss 0.200135, acc 0.960938, prec 0.0742064, recall 0.876467
2017-12-09T23:43:22.820292: step 1174, loss 0.231255, acc 0.964844, prec 0.0742271, recall 0.876518
2017-12-09T23:43:23.324960: step 1175, loss 0.695907, acc 0.972656, prec 0.0742677, recall 0.876414
2017-12-09T23:43:23.831715: step 1176, loss 0.169407, acc 0.945312, prec 0.074298, recall 0.87649
2017-12-09T23:43:24.350813: step 1177, loss 0.129223, acc 0.957031, prec 0.0743482, recall 0.876591
2017-12-09T23:43:24.856952: step 1178, loss 0.214207, acc 0.949219, prec 0.0744442, recall 0.876769
2017-12-09T23:43:25.369472: step 1179, loss 0.331135, acc 0.941406, prec 0.0745214, recall 0.87692
2017-12-09T23:43:25.870218: step 1180, loss 0.182468, acc 0.953125, prec 0.0745381, recall 0.87697
2017-12-09T23:43:26.373902: step 1181, loss 0.186356, acc 0.9375, prec 0.0745495, recall 0.877021
2017-12-09T23:43:26.881725: step 1182, loss 0.211706, acc 0.941406, prec 0.0745944, recall 0.877121
2017-12-09T23:43:27.383614: step 1183, loss 0.25943, acc 0.917969, prec 0.0746476, recall 0.877247
2017-12-09T23:43:27.890989: step 1184, loss 0.246283, acc 0.925781, prec 0.074623, recall 0.877247
2017-12-09T23:43:28.400992: step 1185, loss 0.29228, acc 0.949219, prec 0.0747186, recall 0.877422
2017-12-09T23:43:28.910325: step 1186, loss 0.187994, acc 0.929688, prec 0.0747274, recall 0.877472
2017-12-09T23:43:29.425509: step 1187, loss 0.197704, acc 0.9375, prec 0.074803, recall 0.877622
2017-12-09T23:43:29.947758: step 1188, loss 0.135291, acc 0.957031, prec 0.074853, recall 0.877721
2017-12-09T23:43:30.473997: step 1189, loss 0.25336, acc 0.914062, prec 0.0748886, recall 0.877821
2017-12-09T23:43:30.981673: step 1190, loss 0.11795, acc 0.960938, prec 0.0749558, recall 0.877945
2017-12-09T23:43:31.488013: step 1191, loss 0.1673, acc 0.949219, prec 0.075003, recall 0.878044
2017-12-09T23:43:31.986929: step 1192, loss 0.0777061, acc 0.988281, prec 0.0750152, recall 0.878069
2017-12-09T23:43:32.500871: step 1193, loss 0.137501, acc 0.960938, prec 0.0750182, recall 0.878093
2017-12-09T23:43:33.012879: step 1194, loss 0.361364, acc 0.964844, prec 0.0750706, recall 0.878192
2017-12-09T23:43:33.521277: step 1195, loss 0.0971077, acc 0.980469, prec 0.0750801, recall 0.878217
2017-12-09T23:43:34.023305: step 1196, loss 0.357854, acc 0.988281, prec 0.0751563, recall 0.87834
2017-12-09T23:43:34.524806: step 1197, loss 0.581764, acc 0.960938, prec 0.0752247, recall 0.878285
2017-12-09T23:43:35.033333: step 1198, loss 0.240935, acc 0.960938, prec 0.0752597, recall 0.878359
2017-12-09T23:43:35.555633: step 1199, loss 0.180261, acc 0.957031, prec 0.0753414, recall 0.878507
2017-12-09T23:43:36.059802: step 1200, loss 0.36081, acc 0.964844, prec 0.0753937, recall 0.878605

Evaluation:
2017-12-09T23:43:40.760610: step 1200, loss 2.29575, acc 0.948198, prec 0.0758217, recall 0.86587

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_2/1512880382/checkpoints/model-1200

2017-12-09T23:43:42.254800: step 1201, loss 0.226802, acc 0.949219, prec 0.0758841, recall 0.866002
2017-12-09T23:43:42.764560: step 1202, loss 0.234002, acc 0.945312, prec 0.0759609, recall 0.866159
2017-12-09T23:43:43.275730: step 1203, loss 0.168119, acc 0.941406, prec 0.0759572, recall 0.866185
2017-12-09T23:43:43.780314: step 1204, loss 0.195737, acc 0.949219, prec 0.0759877, recall 0.866263
2017-12-09T23:43:44.289032: step 1205, loss 0.185422, acc 0.929688, prec 0.075996, recall 0.866315
2017-12-09T23:43:44.789554: step 1206, loss 0.219855, acc 0.933594, prec 0.0760687, recall 0.866472
2017-12-09T23:43:45.297286: step 1207, loss 0.290984, acc 0.917969, prec 0.0761046, recall 0.866576
2017-12-09T23:43:45.802358: step 1208, loss 0.206103, acc 0.917969, prec 0.0761089, recall 0.866628
2017-12-09T23:43:46.311679: step 1209, loss 0.231487, acc 0.941406, prec 0.076121, recall 0.86668
2017-12-09T23:43:46.814236: step 1210, loss 0.232183, acc 0.9375, prec 0.0761633, recall 0.866783
2017-12-09T23:43:47.329179: step 1211, loss 0.1959, acc 0.914062, prec 0.0761662, recall 0.866835
2017-12-09T23:43:47.834675: step 1212, loss 0.104043, acc 0.972656, prec 0.0762045, recall 0.866913
2017-12-09T23:43:48.339321: step 1213, loss 0.213368, acc 0.945312, prec 0.0762336, recall 0.86699
2017-12-09T23:43:48.839689: step 1214, loss 0.346486, acc 0.921875, prec 0.0763179, recall 0.867171
2017-12-09T23:43:49.348318: step 1215, loss 0.156976, acc 0.964844, prec 0.076385, recall 0.867299
2017-12-09T23:43:49.855403: step 1216, loss 0.12768, acc 0.960938, prec 0.0764192, recall 0.867377
2017-12-09T23:43:50.359846: step 1217, loss 0.233541, acc 0.953125, prec 0.0764666, recall 0.867479
2017-12-09T23:43:50.883523: step 1218, loss 0.220984, acc 0.953125, prec 0.0764824, recall 0.86753
2017-12-09T23:43:51.380968: step 1219, loss 0.132537, acc 0.964844, prec 0.0765651, recall 0.867684
2017-12-09T23:43:51.884742: step 1220, loss 0.180894, acc 0.984375, prec 0.0766071, recall 0.867761
2017-12-09T23:43:52.394274: step 1221, loss 0.065309, acc 0.984375, prec 0.0766176, recall 0.867786
2017-12-09T23:43:52.900425: step 1222, loss 0.438105, acc 0.964844, prec 0.076716, recall 0.867965
2017-12-09T23:43:53.405636: step 1223, loss 0.644316, acc 0.972656, prec 0.0767711, recall 0.867899
2017-12-09T23:43:53.911241: step 1224, loss 0.0935743, acc 0.972656, prec 0.0768091, recall 0.867975
2017-12-09T23:43:54.421026: step 1225, loss 0.189139, acc 0.9375, prec 0.0768039, recall 0.868001
2017-12-09T23:43:54.919388: step 1226, loss 0.173235, acc 0.941406, prec 0.0768314, recall 0.868077
2017-12-09T23:43:55.421517: step 1227, loss 0.152633, acc 0.945312, prec 0.076876, recall 0.868178
2017-12-09T23:43:55.923836: step 1228, loss 0.28302, acc 0.957031, prec 0.0769244, recall 0.86828
2017-12-09T23:43:56.434275: step 1229, loss 0.184068, acc 0.945312, prec 0.0769375, recall 0.86833
2017-12-09T23:43:56.936908: step 1230, loss 0.274238, acc 0.941406, prec 0.0769649, recall 0.868406
2017-12-09T23:43:57.439611: step 1231, loss 0.193137, acc 0.933594, prec 0.0770054, recall 0.868507
2017-12-09T23:43:57.946884: step 1232, loss 0.181482, acc 0.953125, prec 0.0770838, recall 0.868658
2017-12-09T23:43:58.463544: step 1233, loss 0.297309, acc 0.953125, prec 0.0770995, recall 0.868708
2017-12-09T23:43:58.976071: step 1234, loss 0.148151, acc 0.949219, prec 0.0771138, recall 0.868758
2017-12-09T23:43:59.480497: step 1235, loss 0.170704, acc 0.933594, prec 0.0771229, recall 0.868809
2017-12-09T23:43:59.989946: step 1236, loss 0.187264, acc 0.957031, prec 0.0771711, recall 0.868909
2017-12-09T23:44:00.509769: step 1237, loss 0.154882, acc 0.945312, prec 0.0771841, recall 0.868959
2017-12-09T23:44:01.014300: step 1238, loss 0.242098, acc 0.949219, prec 0.0772767, recall 0.869134
2017-12-09T23:44:01.520865: step 1239, loss 0.193725, acc 0.949219, prec 0.0773535, recall 0.869284
2017-12-09T23:44:02.024111: step 1240, loss 0.233547, acc 0.964844, prec 0.0774199, recall 0.869408
2017-12-09T23:44:02.538691: step 1241, loss 0.183025, acc 0.945312, prec 0.0774953, recall 0.869557
2017-12-09T23:44:03.042448: step 1242, loss 0.217375, acc 0.957031, prec 0.0775278, recall 0.869631
2017-12-09T23:44:03.544692: step 1243, loss 0.110718, acc 0.960938, prec 0.0775615, recall 0.869706
2017-12-09T23:44:04.054931: step 1244, loss 0.214286, acc 0.949219, prec 0.0776069, recall 0.869805
2017-12-09T23:44:04.559523: step 1245, loss 0.218533, acc 0.96875, prec 0.0777057, recall 0.869977
2017-12-09T23:44:05.067482: step 1246, loss 0.138555, acc 0.949219, prec 0.0777042, recall 0.870002
2017-12-09T23:44:05.579410: step 1247, loss 0.17295, acc 0.953125, prec 0.0777509, recall 0.8701
2017-12-09T23:44:06.080442: step 1248, loss 0.136554, acc 0.976562, prec 0.0778054, recall 0.870199
2017-12-09T23:44:06.591693: step 1249, loss 0.197335, acc 0.941406, prec 0.0778793, recall 0.870346
2017-12-09T23:44:06.776626: step 1250, loss 0.162921, acc 0.903846, prec 0.0778883, recall 0.87037
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 256
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR batch_size_256_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047

Start training
2017-12-09T23:44:10.845715: step 1, loss 4.26598, acc 0.347656, prec 0.0118343, recall 1
2017-12-09T23:44:11.347315: step 2, loss 19.3559, acc 0.597656, prec 0.011194, recall 0.375
2017-12-09T23:44:11.848492: step 3, loss 8.82889, acc 0.621094, prec 0.010989, recall 0.363636
2017-12-09T23:44:12.351812: step 4, loss 3.49237, acc 0.550781, prec 0.014553, recall 0.466667
2017-12-09T23:44:12.852934: step 5, loss 5.10623, acc 0.503906, prec 0.0148026, recall 0.473684
2017-12-09T23:44:13.358366: step 6, loss 3.90431, acc 0.417969, prec 0.0119205, recall 0.428571
2017-12-09T23:44:13.873341: step 7, loss 4.534, acc 0.414062, prec 0.0121413, recall 0.458333
2017-12-09T23:44:14.377553: step 8, loss 6.5699, acc 0.28125, prec 0.0110193, recall 0.444444
2017-12-09T23:44:14.889913: step 9, loss 3.21105, acc 0.355469, prec 0.0103586, recall 0.464286
2017-12-09T23:44:15.391246: step 10, loss 4.00245, acc 0.402344, prec 0.0127479, recall 0.529412
2017-12-09T23:44:15.891676: step 11, loss 3.29915, acc 0.410156, prec 0.0140396, recall 0.578947
2017-12-09T23:44:16.396358: step 12, loss 3.81185, acc 0.472656, prec 0.0141011, recall 0.571429
2017-12-09T23:44:16.897303: step 13, loss 4.78537, acc 0.527344, prec 0.0137137, recall 0.568182
2017-12-09T23:44:17.402130: step 14, loss 5.58843, acc 0.597656, prec 0.0140187, recall 0.5625
2017-12-09T23:44:17.906129: step 15, loss 1.85938, acc 0.601562, prec 0.0142857, recall 0.58
2017-12-09T23:44:18.414025: step 16, loss 4.96311, acc 0.671875, prec 0.0142045, recall 0.555556
2017-12-09T23:44:18.915259: step 17, loss 2.29076, acc 0.617188, prec 0.0153638, recall 0.576271
2017-12-09T23:44:19.419147: step 18, loss 1.7464, acc 0.609375, prec 0.0159758, recall 0.596774
2017-12-09T23:44:19.923366: step 19, loss 4.53486, acc 0.636719, prec 0.0161826, recall 0.6
2017-12-09T23:44:20.428573: step 20, loss 6.40779, acc 0.664062, prec 0.0180072, recall 0.608108
2017-12-09T23:44:20.943552: step 21, loss 3.17374, acc 0.621094, prec 0.0184758, recall 0.615385
2017-12-09T23:44:21.447981: step 22, loss 2.30926, acc 0.535156, prec 0.018028, recall 0.620253
2017-12-09T23:44:21.945427: step 23, loss 2.13934, acc 0.550781, prec 0.0186817, recall 0.638554
2017-12-09T23:44:22.446587: step 24, loss 2.29889, acc 0.496094, prec 0.0182002, recall 0.642857
2017-12-09T23:44:22.948112: step 25, loss 4.11153, acc 0.527344, prec 0.0181288, recall 0.643678
2017-12-09T23:44:23.455225: step 26, loss 1.97771, acc 0.621094, prec 0.0185011, recall 0.655556
2017-12-09T23:44:23.957493: step 27, loss 2.50798, acc 0.636719, prec 0.0188852, recall 0.652632
2017-12-09T23:44:24.462808: step 28, loss 2.49872, acc 0.648438, prec 0.0192593, recall 0.656566
2017-12-09T23:44:24.968783: step 29, loss 1.38718, acc 0.6875, prec 0.0196645, recall 0.666667
2017-12-09T23:44:25.466397: step 30, loss 3.13181, acc 0.722656, prec 0.0201076, recall 0.669811
2017-12-09T23:44:25.970517: step 31, loss 1.96832, acc 0.699219, prec 0.0202272, recall 0.669725
2017-12-09T23:44:26.472395: step 32, loss 2.82271, acc 0.730469, prec 0.0201251, recall 0.660714
2017-12-09T23:44:26.972283: step 33, loss 1.76037, acc 0.652344, prec 0.0201699, recall 0.666667
2017-12-09T23:44:27.470144: step 34, loss 9.52367, acc 0.734375, prec 0.0200835, recall 0.652542
2017-12-09T23:44:27.972903: step 35, loss 6.86844, acc 0.757812, prec 0.020272, recall 0.652893
2017-12-09T23:44:28.475096: step 36, loss 8.44021, acc 0.65625, prec 0.0203364, recall 0.637795
2017-12-09T23:44:28.978114: step 37, loss 1.90476, acc 0.574219, prec 0.020991, recall 0.651515
2017-12-09T23:44:29.483263: step 38, loss 4.09551, acc 0.570312, prec 0.0211452, recall 0.654412
2017-12-09T23:44:29.990807: step 39, loss 3.63678, acc 0.535156, prec 0.0212569, recall 0.647887
2017-12-09T23:44:30.507192: step 40, loss 2.81543, acc 0.445312, prec 0.0208007, recall 0.65035
2017-12-09T23:44:31.007248: step 41, loss 3.31659, acc 0.421875, prec 0.020982, recall 0.659864
2017-12-09T23:44:31.517117: step 42, loss 4.10992, acc 0.375, prec 0.0211032, recall 0.664474
2017-12-09T23:44:32.019241: step 43, loss 3.45214, acc 0.398438, prec 0.0218314, recall 0.679245
2017-12-09T23:44:32.519360: step 44, loss 3.44359, acc 0.410156, prec 0.0217604, recall 0.685185
2017-12-09T23:44:33.025931: step 45, loss 3.58928, acc 0.484375, prec 0.0223368, recall 0.692308
2017-12-09T23:44:33.531085: step 46, loss 2.19805, acc 0.535156, prec 0.0225704, recall 0.699422
2017-12-09T23:44:34.034903: step 47, loss 5.20421, acc 0.515625, prec 0.0224207, recall 0.698864
2017-12-09T23:44:34.543266: step 48, loss 6.28243, acc 0.65625, prec 0.0224296, recall 0.690608
2017-12-09T23:44:35.055376: step 49, loss 4.77296, acc 0.675781, prec 0.0227996, recall 0.68984
2017-12-09T23:44:35.556258: step 50, loss 6.66733, acc 0.65625, prec 0.0226244, recall 0.687831
2017-12-09T23:44:36.064314: step 51, loss 5.58682, acc 0.664062, prec 0.0228013, recall 0.685567
2017-12-09T23:44:36.572482: step 52, loss 3.35969, acc 0.691406, prec 0.0229963, recall 0.686869
2017-12-09T23:44:37.076426: step 53, loss 2.74463, acc 0.585938, prec 0.023082, recall 0.688119
2017-12-09T23:44:37.583237: step 54, loss 3.12279, acc 0.625, prec 0.0228833, recall 0.686275
2017-12-09T23:44:38.084054: step 55, loss 3.86417, acc 0.535156, prec 0.022607, recall 0.684466
2017-12-09T23:44:38.584753: step 56, loss 3.11217, acc 0.492188, prec 0.022763, recall 0.687204
2017-12-09T23:44:39.095387: step 57, loss 3.4208, acc 0.539062, prec 0.0225031, recall 0.685446
2017-12-09T23:44:39.596033: step 58, loss 2.85472, acc 0.496094, prec 0.0225076, recall 0.689815
2017-12-09T23:44:40.106012: step 59, loss 2.15897, acc 0.5625, prec 0.0225687, recall 0.694064
2017-12-09T23:44:40.609786: step 60, loss 2.41659, acc 0.511719, prec 0.0232964, recall 0.704846
2017-12-09T23:44:41.110014: step 61, loss 3.2276, acc 0.570312, prec 0.0234957, recall 0.703863
2017-12-09T23:44:41.613649: step 62, loss 2.86699, acc 0.628906, prec 0.0236009, recall 0.701681
2017-12-09T23:44:42.122145: step 63, loss 2.17751, acc 0.601562, prec 0.0236769, recall 0.702479
2017-12-09T23:44:42.622399: step 64, loss 4.47905, acc 0.640625, prec 0.0233838, recall 0.696721
2017-12-09T23:44:43.126027: step 65, loss 3.7561, acc 0.628906, prec 0.0232179, recall 0.695122
2017-12-09T23:44:43.631283: step 66, loss 2.63889, acc 0.621094, prec 0.023181, recall 0.694779
2017-12-09T23:44:44.141593: step 67, loss 2.52534, acc 0.699219, prec 0.02308, recall 0.690476
2017-12-09T23:44:44.654149: step 68, loss 1.364, acc 0.707031, prec 0.0231092, recall 0.692913
2017-12-09T23:44:45.161573: step 69, loss 1.64375, acc 0.621094, prec 0.0229453, recall 0.694118
2017-12-09T23:44:45.662854: step 70, loss 1.39537, acc 0.726562, prec 0.02299, recall 0.696498
2017-12-09T23:44:46.166433: step 71, loss 2.62871, acc 0.683594, prec 0.0232558, recall 0.695817
2017-12-09T23:44:46.681484: step 72, loss 4.8, acc 0.703125, prec 0.0232822, recall 0.695489
2017-12-09T23:44:47.188251: step 73, loss 1.77747, acc 0.730469, prec 0.0234502, recall 0.696296
2017-12-09T23:44:47.695156: step 74, loss 2.78098, acc 0.691406, prec 0.0238301, recall 0.696751
2017-12-09T23:44:48.203296: step 75, loss 1.84862, acc 0.683594, prec 0.023955, recall 0.697509
2017-12-09T23:44:48.717117: step 76, loss 4.22866, acc 0.699219, prec 0.024092, recall 0.695804
2017-12-09T23:44:49.222094: step 77, loss 2.09304, acc 0.597656, prec 0.0241482, recall 0.696552
2017-12-09T23:44:49.729258: step 78, loss 2.97898, acc 0.546875, prec 0.024166, recall 0.697279
2017-12-09T23:44:50.236647: step 79, loss 2.38669, acc 0.546875, prec 0.0242939, recall 0.701342
2017-12-09T23:44:50.761374: step 80, loss 2.46693, acc 0.480469, prec 0.0241474, recall 0.703333
2017-12-09T23:44:51.265585: step 81, loss 5.85584, acc 0.53125, prec 0.0241562, recall 0.701639
2017-12-09T23:44:51.777381: step 82, loss 3.17606, acc 0.523438, prec 0.0242654, recall 0.703226
2017-12-09T23:44:52.281055: step 83, loss 2.4092, acc 0.523438, prec 0.024583, recall 0.708861
2017-12-09T23:44:52.776819: step 84, loss 2.68087, acc 0.492188, prec 0.0247648, recall 0.713396
2017-12-09T23:44:53.291253: step 85, loss 3.44083, acc 0.566406, prec 0.024789, recall 0.711656
2017-12-09T23:44:53.801931: step 86, loss 2.02086, acc 0.589844, prec 0.0247201, recall 0.713415
2017-12-09T23:44:54.301964: step 87, loss 4.30609, acc 0.605469, prec 0.0247675, recall 0.713855
2017-12-09T23:44:54.812484: step 88, loss 1.7184, acc 0.640625, prec 0.0248344, recall 0.716418
2017-12-09T23:44:55.314799: step 89, loss 1.31769, acc 0.722656, prec 0.0252541, recall 0.721408
2017-12-09T23:44:55.818738: step 90, loss 1.54171, acc 0.65625, prec 0.0254246, recall 0.724638
2017-12-09T23:44:56.330976: step 91, loss 1.98459, acc 0.664062, prec 0.0256979, recall 0.726496
2017-12-09T23:44:56.840025: step 92, loss 0.694744, acc 0.789062, prec 0.0256564, recall 0.727273
2017-12-09T23:44:57.341882: step 93, loss 1.39212, acc 0.777344, prec 0.0257075, recall 0.726761
2017-12-09T23:44:57.843004: step 94, loss 4.02484, acc 0.835938, prec 0.0258929, recall 0.727019
2017-12-09T23:44:58.349364: step 95, loss 6.20387, acc 0.75, prec 0.0258357, recall 0.71978
2017-12-09T23:44:58.853471: step 96, loss 2.41862, acc 0.804688, prec 0.0258071, recall 0.718579
2017-12-09T23:44:59.365981: step 97, loss 0.768371, acc 0.792969, prec 0.0257687, recall 0.719346
2017-12-09T23:44:59.871711: step 98, loss 0.873731, acc 0.769531, prec 0.0257157, recall 0.720109
2017-12-09T23:45:00.390107: step 99, loss 4.57467, acc 0.761719, prec 0.0257573, recall 0.717742
2017-12-09T23:45:00.896777: step 100, loss 4.55026, acc 0.710938, prec 0.0256754, recall 0.712766
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-100

2017-12-09T23:45:02.346730: step 101, loss 1.30962, acc 0.683594, prec 0.025663, recall 0.714286
2017-12-09T23:45:02.849246: step 102, loss 2.26544, acc 0.597656, prec 0.0256917, recall 0.71466
2017-12-09T23:45:03.352135: step 103, loss 2.00152, acc 0.566406, prec 0.0256076, recall 0.716146
2017-12-09T23:45:03.859098: step 104, loss 4.6732, acc 0.53125, prec 0.0256859, recall 0.717224
2017-12-09T23:45:04.364840: step 105, loss 2.17921, acc 0.496094, prec 0.0254731, recall 0.717949
2017-12-09T23:45:04.870412: step 106, loss 2.03343, acc 0.566406, prec 0.025394, recall 0.719388
2017-12-09T23:45:05.375459: step 107, loss 2.18971, acc 0.507812, prec 0.0252826, recall 0.720812
2017-12-09T23:45:05.894365: step 108, loss 5.34148, acc 0.605469, prec 0.0254918, recall 0.720698
2017-12-09T23:45:06.396536: step 109, loss 4.17765, acc 0.625, prec 0.0256231, recall 0.719902
2017-12-09T23:45:06.900590: step 110, loss 1.51816, acc 0.628906, prec 0.02575, recall 0.722628
2017-12-09T23:45:07.400873: step 111, loss 3.24156, acc 0.628906, prec 0.0258791, recall 0.721823
2017-12-09T23:45:07.901539: step 112, loss 2.64713, acc 0.554688, prec 0.0258789, recall 0.72209
2017-12-09T23:45:08.403473: step 113, loss 1.5168, acc 0.582031, prec 0.0258919, recall 0.724057
2017-12-09T23:45:08.905922: step 114, loss 2.46863, acc 0.617188, prec 0.0259262, recall 0.724299
2017-12-09T23:45:09.402596: step 115, loss 1.66497, acc 0.632812, prec 0.0258878, recall 0.723898
2017-12-09T23:45:09.919161: step 116, loss 1.30686, acc 0.667969, prec 0.0257868, recall 0.724537
2017-12-09T23:45:10.425243: step 117, loss 1.36109, acc 0.65625, prec 0.0259995, recall 0.727689
2017-12-09T23:45:10.925232: step 118, loss 1.54761, acc 0.726562, prec 0.0261704, recall 0.728507
2017-12-09T23:45:11.436792: step 119, loss 0.834775, acc 0.78125, prec 0.0262094, recall 0.72973
2017-12-09T23:45:11.944486: step 120, loss 0.990206, acc 0.726562, prec 0.0262184, recall 0.730942
2017-12-09T23:45:12.453370: step 121, loss 2.98476, acc 0.792969, prec 0.0261113, recall 0.727679
2017-12-09T23:45:12.964996: step 122, loss 2.2209, acc 0.804688, prec 0.0262423, recall 0.727876
2017-12-09T23:45:13.467207: step 123, loss 0.769032, acc 0.847656, prec 0.026548, recall 0.730853
2017-12-09T23:45:13.975608: step 124, loss 6.3386, acc 0.824219, prec 0.0266139, recall 0.727273
2017-12-09T23:45:14.157625: step 125, loss 1.92049, acc 0.692308, prec 0.0266572, recall 0.727862
2017-12-09T23:45:14.667972: step 126, loss 0.892727, acc 0.75, prec 0.0267527, recall 0.729614
2017-12-09T23:45:15.173444: step 127, loss 2.26823, acc 0.726562, prec 0.0266844, recall 0.728632
2017-12-09T23:45:15.689832: step 128, loss 1.21793, acc 0.703125, prec 0.0269808, recall 0.732068
2017-12-09T23:45:16.194968: step 129, loss 1.50575, acc 0.652344, prec 0.0270959, recall 0.73431
2017-12-09T23:45:16.698015: step 130, loss 1.29523, acc 0.691406, prec 0.0272302, recall 0.736515
2017-12-09T23:45:17.195446: step 131, loss 2.40783, acc 0.675781, prec 0.0274307, recall 0.737705
2017-12-09T23:45:17.691874: step 132, loss 1.59644, acc 0.648438, prec 0.0273195, recall 0.736735
2017-12-09T23:45:18.196192: step 133, loss 1.06065, acc 0.71875, prec 0.0273911, recall 0.738337
2017-12-09T23:45:18.700033: step 134, loss 1.19789, acc 0.699219, prec 0.0274516, recall 0.739919
2017-12-09T23:45:19.200924: step 135, loss 1.57342, acc 0.726562, prec 0.0276724, recall 0.741036
2017-12-09T23:45:19.698326: step 136, loss 2.87513, acc 0.726562, prec 0.027675, recall 0.740594
2017-12-09T23:45:20.195109: step 137, loss 4.5403, acc 0.71875, prec 0.0278187, recall 0.739726
2017-12-09T23:45:20.714609: step 138, loss 8.28533, acc 0.707031, prec 0.0277473, recall 0.733075
2017-12-09T23:45:21.221027: step 139, loss 3.03713, acc 0.660156, prec 0.0277859, recall 0.733205
2017-12-09T23:45:21.733910: step 140, loss 1.60496, acc 0.589844, prec 0.0276454, recall 0.733716
2017-12-09T23:45:22.240063: step 141, loss 1.77856, acc 0.585938, prec 0.0277837, recall 0.736243
2017-12-09T23:45:22.737187: step 142, loss 1.93567, acc 0.527344, prec 0.0278211, recall 0.73823
2017-12-09T23:45:23.238183: step 143, loss 2.42182, acc 0.554688, prec 0.0278052, recall 0.738318
2017-12-09T23:45:23.742998: step 144, loss 2.03537, acc 0.515625, prec 0.0277681, recall 0.739777
2017-12-09T23:45:24.238635: step 145, loss 1.85329, acc 0.550781, prec 0.0276817, recall 0.740741
2017-12-09T23:45:24.739602: step 146, loss 1.74588, acc 0.582031, prec 0.0276786, recall 0.742173
2017-12-09T23:45:25.240645: step 147, loss 1.68357, acc 0.589844, prec 0.0277456, recall 0.744058
2017-12-09T23:45:25.739284: step 148, loss 1.3622, acc 0.664062, prec 0.0277815, recall 0.745455
2017-12-09T23:45:26.245746: step 149, loss 2.41854, acc 0.730469, prec 0.0278508, recall 0.745487
2017-12-09T23:45:26.746779: step 150, loss 1.09083, acc 0.75, prec 0.027927, recall 0.746858
2017-12-09T23:45:27.258973: step 151, loss 1.0349, acc 0.785156, prec 0.0279561, recall 0.746429
2017-12-09T23:45:27.763215: step 152, loss 0.700734, acc 0.796875, prec 0.0279888, recall 0.747331
2017-12-09T23:45:28.268712: step 153, loss 1.50584, acc 0.816406, prec 0.028097, recall 0.74735
2017-12-09T23:45:28.769558: step 154, loss 0.53578, acc 0.871094, prec 0.0282931, recall 0.749123
2017-12-09T23:45:29.274838: step 155, loss 1.25565, acc 0.875, prec 0.0284298, recall 0.747826
2017-12-09T23:45:29.782998: step 156, loss 0.260413, acc 0.910156, prec 0.0283866, recall 0.747826
2017-12-09T23:45:30.302699: step 157, loss 1.22453, acc 0.859375, prec 0.0283851, recall 0.746967
2017-12-09T23:45:30.817126: step 158, loss 0.74442, acc 0.859375, prec 0.0285733, recall 0.748709
2017-12-09T23:45:31.326633: step 159, loss 5.14964, acc 0.90625, prec 0.0285958, recall 0.746575
2017-12-09T23:45:31.834345: step 160, loss 0.390399, acc 0.871094, prec 0.0285976, recall 0.747009
2017-12-09T23:45:32.339677: step 161, loss 2.30344, acc 0.800781, prec 0.0285062, recall 0.744463
2017-12-09T23:45:32.848981: step 162, loss 0.837011, acc 0.835938, prec 0.0284934, recall 0.743633
2017-12-09T23:45:33.361269: step 163, loss 0.797182, acc 0.773438, prec 0.0286381, recall 0.745363
2017-12-09T23:45:33.866290: step 164, loss 0.958797, acc 0.765625, prec 0.0287779, recall 0.747069
2017-12-09T23:45:34.380432: step 165, loss 0.94242, acc 0.714844, prec 0.0287054, recall 0.747492
2017-12-09T23:45:34.884513: step 166, loss 0.792118, acc 0.753906, prec 0.0287139, recall 0.748333
2017-12-09T23:45:35.382870: step 167, loss 1.04289, acc 0.707031, prec 0.0286387, recall 0.748752
2017-12-09T23:45:35.903258: step 168, loss 0.88852, acc 0.753906, prec 0.0287706, recall 0.750413
2017-12-09T23:45:36.408050: step 169, loss 0.727431, acc 0.8125, prec 0.0289288, recall 0.752053
2017-12-09T23:45:36.911239: step 170, loss 3.81495, acc 0.855469, prec 0.0289873, recall 0.750408
2017-12-09T23:45:37.422708: step 171, loss 2.17459, acc 0.835938, prec 0.0291567, recall 0.750809
2017-12-09T23:45:37.930998: step 172, loss 0.654392, acc 0.773438, prec 0.0291724, recall 0.751613
2017-12-09T23:45:38.426743: step 173, loss 1.01194, acc 0.808594, prec 0.0295072, recall 0.754386
2017-12-09T23:45:38.926951: step 174, loss 0.80121, acc 0.792969, prec 0.0297116, recall 0.756329
2017-12-09T23:45:39.422651: step 175, loss 1.44769, acc 0.746094, prec 0.0298942, recall 0.757053
2017-12-09T23:45:39.938657: step 176, loss 3.7928, acc 0.746094, prec 0.0300789, recall 0.755418
2017-12-09T23:45:40.452232: step 177, loss 1.1425, acc 0.746094, prec 0.0302565, recall 0.757296
2017-12-09T23:45:40.958070: step 178, loss 1.89404, acc 0.726562, prec 0.030366, recall 0.757622
2017-12-09T23:45:41.469178: step 179, loss 1.16124, acc 0.675781, prec 0.0302128, recall 0.757622
2017-12-09T23:45:41.968820: step 180, loss 1.31606, acc 0.652344, prec 0.0302261, recall 0.758725
2017-12-09T23:45:42.469944: step 181, loss 2.16915, acc 0.753906, prec 0.0302884, recall 0.758673
2017-12-09T23:45:42.972960: step 182, loss 2.37391, acc 0.753906, prec 0.0304666, recall 0.759342
2017-12-09T23:45:43.482039: step 183, loss 3.63351, acc 0.644531, prec 0.0307078, recall 0.760709
2017-12-09T23:45:43.994091: step 184, loss 1.59748, acc 0.675781, prec 0.0309015, recall 0.762811
2017-12-09T23:45:44.491050: step 185, loss 2.76494, acc 0.671875, prec 0.030752, recall 0.760584
2017-12-09T23:45:44.992597: step 186, loss 3.82076, acc 0.675781, prec 0.0307747, recall 0.760522
2017-12-09T23:45:45.503071: step 187, loss 1.58106, acc 0.59375, prec 0.0308141, recall 0.761905
2017-12-09T23:45:46.006654: step 188, loss 1.92181, acc 0.546875, prec 0.030944, recall 0.763949
2017-12-09T23:45:46.507145: step 189, loss 1.61532, acc 0.613281, prec 0.0308791, recall 0.764622
2017-12-09T23:45:47.008843: step 190, loss 1.49196, acc 0.605469, prec 0.030867, recall 0.765625
2017-12-09T23:45:47.505230: step 191, loss 1.57239, acc 0.625, prec 0.0308638, recall 0.76662
2017-12-09T23:45:48.014445: step 192, loss 1.16601, acc 0.675781, prec 0.0309383, recall 0.767932
2017-12-09T23:45:48.517793: step 193, loss 1.79442, acc 0.703125, prec 0.0310279, recall 0.767085
2017-12-09T23:45:49.040098: step 194, loss 0.773812, acc 0.742188, prec 0.0311306, recall 0.768377
2017-12-09T23:45:49.554585: step 195, loss 2.76732, acc 0.757812, prec 0.0311362, recall 0.76584
2017-12-09T23:45:50.059974: step 196, loss 2.6994, acc 0.75, prec 0.0311907, recall 0.764706
2017-12-09T23:45:50.573556: step 197, loss 2.15127, acc 0.773438, prec 0.0314689, recall 0.7659
2017-12-09T23:45:51.079497: step 198, loss 0.906567, acc 0.734375, prec 0.0313504, recall 0.7659
2017-12-09T23:45:51.582573: step 199, loss 1.06679, acc 0.757812, prec 0.0314035, recall 0.766846
2017-12-09T23:45:52.091723: step 200, loss 1.10003, acc 0.722656, prec 0.0314407, recall 0.767785
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-200

2017-12-09T23:45:53.584683: step 201, loss 1.11831, acc 0.703125, prec 0.0313629, recall 0.768097
2017-12-09T23:45:54.082426: step 202, loss 1.37113, acc 0.714844, prec 0.0316076, recall 0.770252
2017-12-09T23:45:54.583896: step 203, loss 0.939392, acc 0.773438, prec 0.0316659, recall 0.771164
2017-12-09T23:45:55.081646: step 204, loss 0.730963, acc 0.8125, prec 0.0318458, recall 0.772668
2017-12-09T23:45:55.587867: step 205, loss 1.74186, acc 0.824219, prec 0.031927, recall 0.772549
2017-12-09T23:45:56.092951: step 206, loss 1.17402, acc 0.820312, prec 0.0320582, recall 0.772727
2017-12-09T23:45:56.602077: step 207, loss 0.726521, acc 0.835938, prec 0.0322459, recall 0.774194
2017-12-09T23:45:57.101725: step 208, loss 2.47059, acc 0.769531, prec 0.0323548, recall 0.773367
2017-12-09T23:45:57.602853: step 209, loss 0.896231, acc 0.78125, prec 0.0324131, recall 0.774235
2017-12-09T23:45:58.117589: step 210, loss 3.612, acc 0.820312, prec 0.0323887, recall 0.772554
2017-12-09T23:45:58.626350: step 211, loss 2.19952, acc 0.734375, prec 0.0323248, recall 0.771863
2017-12-09T23:45:59.134848: step 212, loss 1.12905, acc 0.785156, prec 0.0324868, recall 0.7733
2017-12-09T23:45:59.633519: step 213, loss 1.02423, acc 0.691406, prec 0.0325045, recall 0.774153
2017-12-09T23:46:00.143493: step 214, loss 1.03906, acc 0.761719, prec 0.0325528, recall 0.775
2017-12-09T23:46:00.658516: step 215, loss 0.900218, acc 0.761719, prec 0.0328032, recall 0.776952
2017-12-09T23:46:01.161811: step 216, loss 1.64307, acc 0.710938, prec 0.0327288, recall 0.776267
2017-12-09T23:46:01.658734: step 217, loss 2.23555, acc 0.707031, prec 0.0326533, recall 0.775586
2017-12-09T23:46:02.158982: step 218, loss 1.29126, acc 0.734375, prec 0.0327386, recall 0.776687
2017-12-09T23:46:02.660031: step 219, loss 1.2265, acc 0.734375, prec 0.0327751, recall 0.776557
2017-12-09T23:46:03.160525: step 220, loss 1.08808, acc 0.730469, prec 0.0327086, recall 0.776829
2017-12-09T23:46:03.665065: step 221, loss 0.845226, acc 0.761719, prec 0.0327055, recall 0.777372
2017-12-09T23:46:04.171646: step 222, loss 2.03684, acc 0.75, prec 0.0326004, recall 0.776428
2017-12-09T23:46:04.676905: step 223, loss 0.818208, acc 0.777344, prec 0.0326043, recall 0.77697
2017-12-09T23:46:05.188225: step 224, loss 3.14626, acc 0.820312, prec 0.0326787, recall 0.775904
2017-12-09T23:46:05.715391: step 225, loss 2.51039, acc 0.808594, prec 0.0326973, recall 0.77551
2017-12-09T23:46:06.234213: step 226, loss 6.93435, acc 0.765625, prec 0.0327513, recall 0.772619
2017-12-09T23:46:06.744334: step 227, loss 1.50546, acc 0.707031, prec 0.0328726, recall 0.77305
2017-12-09T23:46:07.249118: step 228, loss 1.34818, acc 0.675781, prec 0.032978, recall 0.774383
2017-12-09T23:46:07.751979: step 229, loss 2.30225, acc 0.644531, prec 0.0329265, recall 0.774005
2017-12-09T23:46:08.255987: step 230, loss 1.70697, acc 0.574219, prec 0.0330362, recall 0.775581
2017-12-09T23:46:08.749807: step 231, loss 1.60535, acc 0.5625, prec 0.0329492, recall 0.776102
2017-12-09T23:46:09.247702: step 232, loss 2.76061, acc 0.53125, prec 0.0329466, recall 0.77624
2017-12-09T23:46:09.746768: step 233, loss 1.64203, acc 0.546875, prec 0.0329488, recall 0.777268
2017-12-09T23:46:10.248121: step 234, loss 2.25744, acc 0.585938, prec 0.0329686, recall 0.777397
2017-12-09T23:46:10.757069: step 235, loss 1.56739, acc 0.601562, prec 0.0329464, recall 0.778157
2017-12-09T23:46:11.263026: step 236, loss 1.42529, acc 0.707031, prec 0.0330598, recall 0.779412
2017-12-09T23:46:11.772842: step 237, loss 1.54692, acc 0.695312, prec 0.0330306, recall 0.77903
2017-12-09T23:46:12.279689: step 238, loss 1.00163, acc 0.738281, prec 0.0330634, recall 0.779775
2017-12-09T23:46:12.781754: step 239, loss 1.35052, acc 0.726562, prec 0.0330928, recall 0.779642
2017-12-09T23:46:13.285535: step 240, loss 0.85953, acc 0.75, prec 0.0329925, recall 0.779642
2017-12-09T23:46:13.785985: step 241, loss 1.20767, acc 0.777344, prec 0.032951, recall 0.779018
2017-12-09T23:46:14.295058: step 242, loss 0.950674, acc 0.835938, prec 0.0329329, recall 0.778396
2017-12-09T23:46:14.804183: step 243, loss 0.507876, acc 0.828125, prec 0.0329102, recall 0.778643
2017-12-09T23:46:15.306898: step 244, loss 1.86003, acc 0.851562, prec 0.0330346, recall 0.778761
2017-12-09T23:46:15.812383: step 245, loss 1.00468, acc 0.878906, prec 0.0330335, recall 0.778146
2017-12-09T23:46:16.320668: step 246, loss 0.549647, acc 0.867188, prec 0.0331166, recall 0.778878
2017-12-09T23:46:16.827099: step 247, loss 1.88617, acc 0.847656, prec 0.033103, recall 0.778266
2017-12-09T23:46:17.335212: step 248, loss 3.20735, acc 0.886719, prec 0.0331966, recall 0.777293
2017-12-09T23:46:17.841556: step 249, loss 4.11809, acc 0.835938, prec 0.0333163, recall 0.775731
2017-12-09T23:46:18.023343: step 250, loss 4.59281, acc 0.769231, prec 0.0333891, recall 0.775378
2017-12-09T23:46:18.532330: step 251, loss 2.79415, acc 0.640625, prec 0.0333827, recall 0.775269
2017-12-09T23:46:19.046248: step 252, loss 1.88414, acc 0.5625, prec 0.0334331, recall 0.776471
2017-12-09T23:46:19.551641: step 253, loss 2.1359, acc 0.507812, prec 0.033373, recall 0.777185
2017-12-09T23:46:20.054559: step 254, loss 3.00002, acc 0.371094, prec 0.0332167, recall 0.77766
2017-12-09T23:46:20.559510: step 255, loss 3.2352, acc 0.382812, prec 0.0331544, recall 0.778602
2017-12-09T23:46:21.069749: step 256, loss 3.42646, acc 0.300781, prec 0.0329321, recall 0.778836
2017-12-09T23:46:21.577358: step 257, loss 2.79843, acc 0.4375, prec 0.0328933, recall 0.779768
2017-12-09T23:46:22.079389: step 258, loss 2.66525, acc 0.4375, prec 0.0328976, recall 0.780922
2017-12-09T23:46:22.583335: step 259, loss 2.14238, acc 0.496094, prec 0.0328812, recall 0.781837
2017-12-09T23:46:23.085727: step 260, loss 2.18623, acc 0.5625, prec 0.0328485, recall 0.781705
2017-12-09T23:46:23.588832: step 261, loss 1.65029, acc 0.609375, prec 0.0328318, recall 0.782383
2017-12-09T23:46:24.084919: step 262, loss 1.5229, acc 0.648438, prec 0.0327457, recall 0.782609
2017-12-09T23:46:24.588880: step 263, loss 1.38745, acc 0.738281, prec 0.0327777, recall 0.782474
2017-12-09T23:46:25.092952: step 264, loss 0.559313, acc 0.8125, prec 0.0327932, recall 0.782922
2017-12-09T23:46:25.593353: step 265, loss 0.893785, acc 0.792969, prec 0.0328863, recall 0.783009
2017-12-09T23:46:26.102018: step 266, loss 0.591187, acc 0.859375, prec 0.03296, recall 0.783673
2017-12-09T23:46:26.617836: step 267, loss 3.12967, acc 0.882812, prec 0.0330061, recall 0.780933
2017-12-09T23:46:27.130668: step 268, loss 1.09174, acc 0.917969, prec 0.0330607, recall 0.780586
2017-12-09T23:46:27.633148: step 269, loss 0.592366, acc 0.875, prec 0.0331395, recall 0.78125
2017-12-09T23:46:28.139653: step 270, loss 0.316693, acc 0.894531, prec 0.0331426, recall 0.78147
2017-12-09T23:46:28.651156: step 271, loss 1.58952, acc 0.855469, prec 0.0332154, recall 0.781344
2017-12-09T23:46:29.164761: step 272, loss 1.68442, acc 0.882812, prec 0.0333816, recall 0.780877
2017-12-09T23:46:29.668392: step 273, loss 2.16459, acc 0.878906, prec 0.0335473, recall 0.779644
2017-12-09T23:46:30.189580: step 274, loss 1.81718, acc 0.828125, prec 0.0336092, recall 0.779528
2017-12-09T23:46:30.701015: step 275, loss 0.720848, acc 0.785156, prec 0.0337355, recall 0.780607
2017-12-09T23:46:31.197818: step 276, loss 1.15949, acc 0.722656, prec 0.0337975, recall 0.781463
2017-12-09T23:46:31.696343: step 277, loss 0.956341, acc 0.726562, prec 0.0339418, recall 0.782735
2017-12-09T23:46:32.200634: step 278, loss 1.04188, acc 0.699219, prec 0.0341966, recall 0.784615
2017-12-09T23:46:32.708725: step 279, loss 1.1958, acc 0.683594, prec 0.0342826, recall 0.785646
2017-12-09T23:46:33.215812: step 280, loss 1.58043, acc 0.671875, prec 0.0342446, recall 0.785305
2017-12-09T23:46:33.714689: step 281, loss 1.69878, acc 0.542969, prec 0.0341187, recall 0.78551
2017-12-09T23:46:34.220375: step 282, loss 1.37662, acc 0.625, prec 0.0341429, recall 0.786325
2017-12-09T23:46:34.722499: step 283, loss 1.18343, acc 0.667969, prec 0.034103, recall 0.78673
2017-12-09T23:46:35.226903: step 284, loss 1.03105, acc 0.730469, prec 0.0341254, recall 0.787335
2017-12-09T23:46:35.752734: step 285, loss 0.669552, acc 0.769531, prec 0.0342404, recall 0.788335
2017-12-09T23:46:36.260611: step 286, loss 0.623747, acc 0.808594, prec 0.0342114, recall 0.788534
2017-12-09T23:46:36.768120: step 287, loss 1.29378, acc 0.792969, prec 0.0342962, recall 0.788587
2017-12-09T23:46:37.278221: step 288, loss 0.668848, acc 0.835938, prec 0.0343945, recall 0.789376
2017-12-09T23:46:37.775557: step 289, loss 0.674846, acc 0.867188, prec 0.0344646, recall 0.789963
2017-12-09T23:46:38.278646: step 290, loss 2.10599, acc 0.882812, prec 0.0344646, recall 0.788693
2017-12-09T23:46:38.777494: step 291, loss 2.60343, acc 0.898438, prec 0.0344688, recall 0.788159
2017-12-09T23:46:39.283687: step 292, loss 2.28515, acc 0.898438, prec 0.034551, recall 0.788018
2017-12-09T23:46:39.789339: step 293, loss 0.438029, acc 0.910156, prec 0.0346748, recall 0.788797
2017-12-09T23:46:40.295608: step 294, loss 1.12232, acc 0.839844, prec 0.0348134, recall 0.789041
2017-12-09T23:46:40.802187: step 295, loss 1.00012, acc 0.84375, prec 0.0350306, recall 0.789665
2017-12-09T23:46:41.306821: step 296, loss 1.45404, acc 0.796875, prec 0.0350363, recall 0.789331
2017-12-09T23:46:41.814171: step 297, loss 1.1408, acc 0.789062, prec 0.0351924, recall 0.790468
2017-12-09T23:46:42.317125: step 298, loss 0.626954, acc 0.769531, prec 0.035225, recall 0.791031
2017-12-09T23:46:42.821709: step 299, loss 2.03161, acc 0.753906, prec 0.0353302, recall 0.791258
2017-12-09T23:46:43.325020: step 300, loss 1.31869, acc 0.773438, prec 0.0353652, recall 0.791111

Evaluation:
2017-12-09T23:46:48.009550: step 300, loss 1.28468, acc 0.752689, prec 0.0365032, recall 0.795313

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-300

2017-12-09T23:46:49.516443: step 301, loss 0.894108, acc 0.742188, prec 0.036417, recall 0.795313
2017-12-09T23:46:50.020715: step 302, loss 0.864917, acc 0.722656, prec 0.0363935, recall 0.795632
2017-12-09T23:46:50.535057: step 303, loss 1.10669, acc 0.734375, prec 0.0364426, recall 0.796268
2017-12-09T23:46:51.042618: step 304, loss 1.10377, acc 0.769531, prec 0.0365715, recall 0.797214
2017-12-09T23:46:51.546944: step 305, loss 1.01708, acc 0.71875, prec 0.0366147, recall 0.79784
2017-12-09T23:46:52.044665: step 306, loss 1.16162, acc 0.6875, prec 0.0367494, recall 0.798926
2017-12-09T23:46:52.543330: step 307, loss 0.983043, acc 0.714844, prec 0.0368584, recall 0.799847
2017-12-09T23:46:53.048528: step 308, loss 0.784397, acc 0.769531, prec 0.0368497, recall 0.800153
2017-12-09T23:46:53.546820: step 309, loss 0.831661, acc 0.792969, prec 0.03695, recall 0.800912
2017-12-09T23:46:54.051696: step 310, loss 3.97489, acc 0.757812, prec 0.0369736, recall 0.800151
2017-12-09T23:46:54.557257: step 311, loss 0.797071, acc 0.828125, prec 0.0369517, recall 0.799698
2017-12-09T23:46:55.059960: step 312, loss 0.842016, acc 0.738281, prec 0.0369325, recall 0.8
2017-12-09T23:46:55.566650: step 313, loss 0.583089, acc 0.835938, prec 0.0370126, recall 0.800602
2017-12-09T23:46:56.066838: step 314, loss 1.46211, acc 0.785156, prec 0.0369766, recall 0.80015
2017-12-09T23:46:56.571161: step 315, loss 1.23201, acc 0.785156, prec 0.0370409, recall 0.80015
2017-12-09T23:46:57.076156: step 316, loss 3.58761, acc 0.800781, prec 0.0371459, recall 0.799107
2017-12-09T23:46:57.580102: step 317, loss 0.791521, acc 0.777344, prec 0.0372058, recall 0.799703
2017-12-09T23:46:58.084278: step 318, loss 0.858232, acc 0.75, prec 0.0371238, recall 0.799703
2017-12-09T23:46:58.585336: step 319, loss 0.879076, acc 0.722656, prec 0.0371655, recall 0.800296
2017-12-09T23:46:59.087722: step 320, loss 0.971431, acc 0.710938, prec 0.0372692, recall 0.801178
2017-12-09T23:46:59.594827: step 321, loss 0.87305, acc 0.726562, prec 0.0374103, recall 0.802198
2017-12-09T23:47:00.112977: step 322, loss 1.31313, acc 0.710938, prec 0.0373172, recall 0.801611
2017-12-09T23:47:00.630280: step 323, loss 0.919694, acc 0.726562, prec 0.0373593, recall 0.80219
2017-12-09T23:47:01.129598: step 324, loss 1.19202, acc 0.765625, prec 0.0374152, recall 0.802182
2017-12-09T23:47:01.631365: step 325, loss 0.868622, acc 0.769531, prec 0.0373731, recall 0.802326
2017-12-09T23:47:02.136664: step 326, loss 1.07359, acc 0.816406, prec 0.0373475, recall 0.801887
2017-12-09T23:47:02.643601: step 327, loss 0.789522, acc 0.824219, prec 0.0373882, recall 0.802317
2017-12-09T23:47:03.146631: step 328, loss 0.699518, acc 0.824219, prec 0.0374613, recall 0.802888
2017-12-09T23:47:03.649190: step 329, loss 0.935917, acc 0.855469, prec 0.0375454, recall 0.802878
2017-12-09T23:47:04.154459: step 330, loss 0.69065, acc 0.859375, prec 0.037597, recall 0.803302
2017-12-09T23:47:04.658272: step 331, loss 0.739629, acc 0.832031, prec 0.0376074, recall 0.803584
2017-12-09T23:47:05.164317: step 332, loss 0.664187, acc 0.847656, prec 0.0377194, recall 0.804286
2017-12-09T23:47:05.688900: step 333, loss 0.903363, acc 0.824219, prec 0.0377283, recall 0.803991
2017-12-09T23:47:06.193634: step 334, loss 1.17982, acc 0.839844, prec 0.0377743, recall 0.803838
2017-12-09T23:47:06.700104: step 335, loss 0.553468, acc 0.855469, prec 0.037824, recall 0.804255
2017-12-09T23:47:07.204062: step 336, loss 0.415355, acc 0.863281, prec 0.037876, recall 0.804671
2017-12-09T23:47:07.701275: step 337, loss 1.36775, acc 0.832031, prec 0.0379191, recall 0.804517
2017-12-09T23:47:08.212247: step 338, loss 0.517769, acc 0.832031, prec 0.0379289, recall 0.804792
2017-12-09T23:47:08.714740: step 339, loss 2.53627, acc 0.839844, prec 0.0379424, recall 0.804501
2017-12-09T23:47:09.214615: step 340, loss 1.17017, acc 0.847656, prec 0.0379583, recall 0.804211
2017-12-09T23:47:09.712619: step 341, loss 0.51583, acc 0.820312, prec 0.0380278, recall 0.804759
2017-12-09T23:47:10.223416: step 342, loss 1.29792, acc 0.804688, prec 0.0380298, recall 0.804469
2017-12-09T23:47:10.723107: step 343, loss 0.759786, acc 0.765625, prec 0.0380181, recall 0.804742
2017-12-09T23:47:11.233125: step 344, loss 2.08194, acc 0.816406, prec 0.0380238, recall 0.804454
2017-12-09T23:47:11.735839: step 345, loss 0.990748, acc 0.746094, prec 0.0379743, recall 0.80459
2017-12-09T23:47:12.233021: step 346, loss 0.841724, acc 0.742188, prec 0.0379237, recall 0.804726
2017-12-09T23:47:12.741213: step 347, loss 0.677177, acc 0.777344, prec 0.0379159, recall 0.804997
2017-12-09T23:47:13.237875: step 348, loss 0.667535, acc 0.792969, prec 0.0379759, recall 0.805536
2017-12-09T23:47:13.737944: step 349, loss 1.31105, acc 0.800781, prec 0.0379454, recall 0.805114
2017-12-09T23:47:14.250088: step 350, loss 1.14998, acc 0.800781, prec 0.0379463, recall 0.804828
2017-12-09T23:47:14.758502: step 351, loss 0.574672, acc 0.800781, prec 0.0379771, recall 0.805231
2017-12-09T23:47:15.263043: step 352, loss 1.14224, acc 0.828125, prec 0.0379866, recall 0.804945
2017-12-09T23:47:15.768588: step 353, loss 0.825621, acc 0.839844, prec 0.0380931, recall 0.805062
2017-12-09T23:47:16.273637: step 354, loss 0.461807, acc 0.828125, prec 0.03807, recall 0.805195
2017-12-09T23:47:16.778427: step 355, loss 0.482472, acc 0.828125, prec 0.0380159, recall 0.805195
2017-12-09T23:47:17.280093: step 356, loss 1.60029, acc 0.875, prec 0.038071, recall 0.805044
2017-12-09T23:47:17.781267: step 357, loss 2.41564, acc 0.859375, prec 0.0381532, recall 0.804481
2017-12-09T23:47:18.287878: step 358, loss 0.9247, acc 0.855469, prec 0.0382018, recall 0.804333
2017-12-09T23:47:18.793820: step 359, loss 0.816171, acc 0.8125, prec 0.0382059, recall 0.804054
2017-12-09T23:47:19.306566: step 360, loss 1.05765, acc 0.828125, prec 0.0382458, recall 0.803908
2017-12-09T23:47:19.811891: step 361, loss 1.70406, acc 0.804688, prec 0.0383089, recall 0.803895
2017-12-09T23:47:20.312930: step 362, loss 4.63944, acc 0.765625, prec 0.0383289, recall 0.803751
2017-12-09T23:47:20.831729: step 363, loss 0.71026, acc 0.789062, prec 0.0383242, recall 0.804013
2017-12-09T23:47:21.328558: step 364, loss 2.72184, acc 0.710938, prec 0.0383894, recall 0.803595
2017-12-09T23:47:21.837347: step 365, loss 0.98518, acc 0.699219, prec 0.0384176, recall 0.804117
2017-12-09T23:47:22.341716: step 366, loss 1.19493, acc 0.644531, prec 0.0384287, recall 0.804636
2017-12-09T23:47:22.850323: step 367, loss 1.64012, acc 0.652344, prec 0.0383827, recall 0.804362
2017-12-09T23:47:23.349812: step 368, loss 1.50823, acc 0.667969, prec 0.0384615, recall 0.805135
2017-12-09T23:47:23.854579: step 369, loss 1.20556, acc 0.621094, prec 0.0384049, recall 0.805391
2017-12-09T23:47:24.358880: step 370, loss 1.4606, acc 0.601562, prec 0.0383426, recall 0.805647
2017-12-09T23:47:24.860621: step 371, loss 1.21102, acc 0.6875, prec 0.0383668, recall 0.806156
2017-12-09T23:47:25.364480: step 372, loss 0.927793, acc 0.679688, prec 0.0383288, recall 0.806409
2017-12-09T23:47:25.870932: step 373, loss 1.21502, acc 0.707031, prec 0.0384782, recall 0.807417
2017-12-09T23:47:26.377054: step 374, loss 1.04836, acc 0.78125, prec 0.0386198, recall 0.80829
2017-12-09T23:47:26.556318: step 375, loss 0.407224, acc 0.865385, prec 0.0386115, recall 0.80829
2017-12-09T23:47:27.067940: step 376, loss 0.632464, acc 0.804688, prec 0.0387003, recall 0.808909
2017-12-09T23:47:27.566812: step 377, loss 0.495027, acc 0.882812, prec 0.0387831, recall 0.809401
2017-12-09T23:47:28.072825: step 378, loss 0.416555, acc 0.863281, prec 0.038919, recall 0.810135
2017-12-09T23:47:28.572801: step 379, loss 1.37524, acc 0.894531, prec 0.0389766, recall 0.809981
2017-12-09T23:47:29.078124: step 380, loss 1.08865, acc 0.894531, prec 0.0390046, recall 0.809706
2017-12-09T23:47:29.584012: step 381, loss 1.026, acc 0.882812, prec 0.0390584, recall 0.809554
2017-12-09T23:47:30.097583: step 382, loss 0.57402, acc 0.917969, prec 0.0391513, recall 0.810038
2017-12-09T23:47:30.613053: step 383, loss 0.962743, acc 0.871094, prec 0.0391423, recall 0.809645
2017-12-09T23:47:31.113147: step 384, loss 0.423889, acc 0.910156, prec 0.0391736, recall 0.809886
2017-12-09T23:47:31.622625: step 385, loss 0.8844, acc 0.917969, prec 0.0393556, recall 0.810214
2017-12-09T23:47:32.121825: step 386, loss 1.9271, acc 0.890625, prec 0.0395876, recall 0.810777
2017-12-09T23:47:32.625738: step 387, loss 0.634166, acc 0.871094, prec 0.0396357, recall 0.811132
2017-12-09T23:47:33.124932: step 388, loss 0.937715, acc 0.804688, prec 0.0396644, recall 0.810979
2017-12-09T23:47:33.628778: step 389, loss 0.88264, acc 0.835938, prec 0.0397319, recall 0.810945
2017-12-09T23:47:34.132369: step 390, loss 0.586986, acc 0.800781, prec 0.0397871, recall 0.811414
2017-12-09T23:47:34.627665: step 391, loss 0.706586, acc 0.773438, prec 0.0397753, recall 0.811648
2017-12-09T23:47:35.126658: step 392, loss 0.808925, acc 0.757812, prec 0.0398461, recall 0.81223
2017-12-09T23:47:35.644202: step 393, loss 0.855501, acc 0.742188, prec 0.0398827, recall 0.812693
2017-12-09T23:47:36.147416: step 394, loss 0.618332, acc 0.773438, prec 0.0399288, recall 0.813153
2017-12-09T23:47:36.650004: step 395, loss 1.50913, acc 0.742188, prec 0.0399663, recall 0.813113
2017-12-09T23:47:37.148036: step 396, loss 1.78872, acc 0.765625, prec 0.039982, recall 0.812958
2017-12-09T23:47:37.654684: step 397, loss 1.08049, acc 0.796875, prec 0.0400936, recall 0.813147
2017-12-09T23:47:38.158018: step 398, loss 1.14839, acc 0.746094, prec 0.0400168, recall 0.812652
2017-12-09T23:47:38.660128: step 399, loss 0.564466, acc 0.828125, prec 0.040079, recall 0.813107
2017-12-09T23:47:39.154050: step 400, loss 0.654683, acc 0.796875, prec 0.0400454, recall 0.81322
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-400

2017-12-09T23:47:40.628375: step 401, loss 0.553147, acc 0.824219, prec 0.0401062, recall 0.813672
2017-12-09T23:47:41.132867: step 402, loss 0.87341, acc 0.820312, prec 0.0402798, recall 0.81457
2017-12-09T23:47:41.634018: step 403, loss 0.570013, acc 0.859375, prec 0.0403223, recall 0.814904
2017-12-09T23:47:42.140162: step 404, loss 0.539306, acc 0.832031, prec 0.0404133, recall 0.815458
2017-12-09T23:47:42.642543: step 405, loss 0.608574, acc 0.867188, prec 0.0405149, recall 0.81601
2017-12-09T23:47:43.151668: step 406, loss 0.439805, acc 0.863281, prec 0.0405582, recall 0.816339
2017-12-09T23:47:43.662411: step 407, loss 0.870591, acc 0.855469, prec 0.0405433, recall 0.815962
2017-12-09T23:47:44.162297: step 408, loss 1.90084, acc 0.898438, prec 0.0406292, recall 0.814947
2017-12-09T23:47:44.666416: step 409, loss 0.324771, acc 0.886719, prec 0.0405944, recall 0.814947
2017-12-09T23:47:45.165090: step 410, loss 0.527983, acc 0.867188, prec 0.0407236, recall 0.815603
2017-12-09T23:47:45.669043: step 411, loss 1.89964, acc 0.863281, prec 0.0407971, recall 0.815077
2017-12-09T23:47:46.172922: step 412, loss 0.54526, acc 0.832031, prec 0.0408301, recall 0.815403
2017-12-09T23:47:46.674723: step 413, loss 2.02116, acc 0.820312, prec 0.0409183, recall 0.814988
2017-12-09T23:47:47.182436: step 414, loss 0.696844, acc 0.820312, prec 0.0409475, recall 0.815313
2017-12-09T23:47:47.688599: step 415, loss 0.847861, acc 0.792969, prec 0.0409682, recall 0.815636
2017-12-09T23:47:48.191686: step 416, loss 1.31218, acc 0.734375, prec 0.0409721, recall 0.815483
2017-12-09T23:47:48.691149: step 417, loss 1.64276, acc 0.675781, prec 0.040986, recall 0.815438
2017-12-09T23:47:49.191807: step 418, loss 1.17839, acc 0.6875, prec 0.0410301, recall 0.815972
2017-12-09T23:47:49.700450: step 419, loss 0.906576, acc 0.667969, prec 0.0409567, recall 0.816079
2017-12-09T23:47:50.207900: step 420, loss 0.780368, acc 0.71875, prec 0.0409269, recall 0.816291
2017-12-09T23:47:50.717859: step 421, loss 0.947257, acc 0.738281, prec 0.0409585, recall 0.816715
2017-12-09T23:47:51.217440: step 422, loss 0.853955, acc 0.738281, prec 0.0408793, recall 0.816715
2017-12-09T23:47:51.724886: step 423, loss 0.691028, acc 0.785156, prec 0.0409251, recall 0.817136
2017-12-09T23:47:52.228915: step 424, loss 2.01489, acc 0.828125, prec 0.0409308, recall 0.816409
2017-12-09T23:47:52.730312: step 425, loss 0.556076, acc 0.8125, prec 0.0410121, recall 0.816934
2017-12-09T23:47:53.232502: step 426, loss 0.477475, acc 0.847656, prec 0.0411313, recall 0.81756
2017-12-09T23:47:53.736666: step 427, loss 0.458753, acc 0.839844, prec 0.0411653, recall 0.817871
2017-12-09T23:47:54.238507: step 428, loss 0.534032, acc 0.828125, prec 0.0411684, recall 0.818078
2017-12-09T23:47:54.744407: step 429, loss 0.43602, acc 0.863281, prec 0.0412094, recall 0.818388
2017-12-09T23:47:55.251259: step 430, loss 0.587635, acc 0.859375, prec 0.0412218, recall 0.818594
2017-12-09T23:47:55.750984: step 431, loss 0.6232, acc 0.933594, prec 0.0413659, recall 0.819209
2017-12-09T23:47:56.259957: step 432, loss 1.72009, acc 0.878906, prec 0.041441, recall 0.818694
2017-12-09T23:47:56.763886: step 433, loss 0.445725, acc 0.902344, prec 0.0415753, recall 0.819304
2017-12-09T23:47:57.275675: step 434, loss 0.8509, acc 0.933594, prec 0.0416655, recall 0.81925
2017-12-09T23:47:57.778076: step 435, loss 1.00426, acc 0.898438, prec 0.0416904, recall 0.818994
2017-12-09T23:47:58.288050: step 436, loss 0.816617, acc 0.875, prec 0.0417625, recall 0.818942
2017-12-09T23:47:58.792889: step 437, loss 0.500732, acc 0.859375, prec 0.0418287, recall 0.819344
2017-12-09T23:47:59.299606: step 438, loss 0.401176, acc 0.847656, prec 0.0418096, recall 0.819444
2017-12-09T23:47:59.806108: step 439, loss 1.25584, acc 0.867188, prec 0.0418519, recall 0.81929
2017-12-09T23:48:00.321120: step 440, loss 0.502494, acc 0.878906, prec 0.0418965, recall 0.819591
2017-12-09T23:48:00.834600: step 441, loss 0.899653, acc 0.820312, prec 0.0418703, recall 0.819237
2017-12-09T23:48:01.334037: step 442, loss 0.788677, acc 0.765625, prec 0.0418265, recall 0.819337
2017-12-09T23:48:01.850441: step 443, loss 0.898203, acc 0.761719, prec 0.0418895, recall 0.819835
2017-12-09T23:48:02.352287: step 444, loss 0.657872, acc 0.785156, prec 0.0418517, recall 0.819934
2017-12-09T23:48:02.860340: step 445, loss 0.660982, acc 0.78125, prec 0.0419204, recall 0.820428
2017-12-09T23:48:03.366472: step 446, loss 0.529071, acc 0.800781, prec 0.0420752, recall 0.821214
2017-12-09T23:48:03.872370: step 447, loss 0.989492, acc 0.785156, prec 0.0420116, recall 0.820765
2017-12-09T23:48:04.378033: step 448, loss 0.590932, acc 0.820312, prec 0.0420112, recall 0.820961
2017-12-09T23:48:04.881615: step 449, loss 0.495608, acc 0.84375, prec 0.0420445, recall 0.821253
2017-12-09T23:48:05.381845: step 450, loss 0.916775, acc 0.859375, prec 0.0420836, recall 0.821098
2017-12-09T23:48:05.902462: step 451, loss 0.53749, acc 0.871094, prec 0.0421516, recall 0.821487
2017-12-09T23:48:06.408584: step 452, loss 0.598035, acc 0.84375, prec 0.042158, recall 0.82168
2017-12-09T23:48:06.914835: step 453, loss 0.406808, acc 0.859375, prec 0.0422222, recall 0.822066
2017-12-09T23:48:07.420436: step 454, loss 1.6431, acc 0.894531, prec 0.0422727, recall 0.821467
2017-12-09T23:48:07.929544: step 455, loss 0.429896, acc 0.839844, prec 0.0423043, recall 0.821756
2017-12-09T23:48:08.431185: step 456, loss 0.513903, acc 0.867188, prec 0.042344, recall 0.822043
2017-12-09T23:48:08.947137: step 457, loss 1.12011, acc 0.847656, prec 0.0423272, recall 0.821256
2017-12-09T23:48:09.448924: step 458, loss 0.711846, acc 0.875, prec 0.0424221, recall 0.821734
2017-12-09T23:48:09.952765: step 459, loss 0.456699, acc 0.800781, prec 0.0424681, recall 0.822115
2017-12-09T23:48:10.458401: step 460, loss 1.50784, acc 0.792969, prec 0.0425391, recall 0.822151
2017-12-09T23:48:10.965756: step 461, loss 1.09255, acc 0.8125, prec 0.0426158, recall 0.822187
2017-12-09T23:48:11.463088: step 462, loss 0.571921, acc 0.78125, prec 0.0426555, recall 0.822564
2017-12-09T23:48:11.966008: step 463, loss 0.599475, acc 0.832031, prec 0.0428152, recall 0.823312
2017-12-09T23:48:12.474797: step 464, loss 0.686916, acc 0.804688, prec 0.0427828, recall 0.823405
2017-12-09T23:48:12.973997: step 465, loss 0.477316, acc 0.824219, prec 0.0427825, recall 0.823591
2017-12-09T23:48:13.483631: step 466, loss 0.466096, acc 0.832031, prec 0.0427845, recall 0.823777
2017-12-09T23:48:13.986733: step 467, loss 0.517724, acc 0.824219, prec 0.0427581, recall 0.82387
2017-12-09T23:48:14.492327: step 468, loss 0.789087, acc 0.808594, prec 0.0427793, recall 0.824147
2017-12-09T23:48:15.001200: step 469, loss 0.766983, acc 0.878906, prec 0.0427965, recall 0.823899
2017-12-09T23:48:15.501966: step 470, loss 0.56291, acc 0.871094, prec 0.0428622, recall 0.824268
2017-12-09T23:48:16.013848: step 471, loss 1.62367, acc 0.835938, prec 0.0429704, recall 0.824388
2017-12-09T23:48:16.522891: step 472, loss 0.457603, acc 0.863281, prec 0.0430854, recall 0.824935
2017-12-09T23:48:17.028913: step 473, loss 0.527286, acc 0.84375, prec 0.0431165, recall 0.825207
2017-12-09T23:48:17.527362: step 474, loss 0.569459, acc 0.894531, prec 0.043138, recall 0.824961
2017-12-09T23:48:18.031215: step 475, loss 1.67528, acc 0.851562, prec 0.0431219, recall 0.824199
2017-12-09T23:48:18.539977: step 476, loss 0.521698, acc 0.875, prec 0.0431622, recall 0.824471
2017-12-09T23:48:19.040994: step 477, loss 0.384991, acc 0.878906, prec 0.0432035, recall 0.824742
2017-12-09T23:48:19.551035: step 478, loss 0.449595, acc 0.851562, prec 0.0431851, recall 0.824833
2017-12-09T23:48:20.053884: step 479, loss 0.917939, acc 0.824219, prec 0.0431854, recall 0.824588
2017-12-09T23:48:20.563442: step 480, loss 0.837714, acc 0.808594, prec 0.0431811, recall 0.824345
2017-12-09T23:48:21.064516: step 481, loss 0.845165, acc 0.847656, prec 0.0432142, recall 0.824193
2017-12-09T23:48:21.576526: step 482, loss 0.674669, acc 0.839844, prec 0.0433207, recall 0.824732
2017-12-09T23:48:22.087145: step 483, loss 0.773502, acc 0.789062, prec 0.0434119, recall 0.825267
2017-12-09T23:48:22.590391: step 484, loss 0.481644, acc 0.832031, prec 0.0434899, recall 0.825711
2017-12-09T23:48:23.103239: step 485, loss 0.806028, acc 0.835938, prec 0.0435178, recall 0.825977
2017-12-09T23:48:23.605250: step 486, loss 0.511162, acc 0.808594, prec 0.0434864, recall 0.826065
2017-12-09T23:48:24.107971: step 487, loss 0.617103, acc 0.800781, prec 0.0434528, recall 0.826153
2017-12-09T23:48:24.613544: step 488, loss 0.630965, acc 0.804688, prec 0.0434968, recall 0.826505
2017-12-09T23:48:25.120363: step 489, loss 0.507488, acc 0.8125, prec 0.0434413, recall 0.826505
2017-12-09T23:48:25.626086: step 490, loss 1.09264, acc 0.796875, prec 0.0434332, recall 0.826263
2017-12-09T23:48:26.133076: step 491, loss 0.506511, acc 0.871094, prec 0.0434713, recall 0.826525
2017-12-09T23:48:26.638430: step 492, loss 0.319122, acc 0.84375, prec 0.0434506, recall 0.826613
2017-12-09T23:48:27.141118: step 493, loss 0.259375, acc 0.90625, prec 0.0434737, recall 0.826788
2017-12-09T23:48:27.639749: step 494, loss 0.341153, acc 0.917969, prec 0.0435001, recall 0.826962
2017-12-09T23:48:28.147674: step 495, loss 0.281244, acc 0.90625, prec 0.0434978, recall 0.827049
2017-12-09T23:48:28.649399: step 496, loss 0.524291, acc 0.921875, prec 0.0435518, recall 0.826894
2017-12-09T23:48:29.150098: step 497, loss 1.13267, acc 0.929688, prec 0.0436344, recall 0.826413
2017-12-09T23:48:29.658406: step 498, loss 0.609406, acc 0.921875, prec 0.0436378, recall 0.826087
2017-12-09T23:48:30.166360: step 499, loss 1.30739, acc 0.898438, prec 0.0437352, recall 0.826109
2017-12-09T23:48:30.353224: step 500, loss 0.295941, acc 0.903846, prec 0.0437294, recall 0.826109
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-500

2017-12-09T23:48:31.885680: step 501, loss 0.222214, acc 0.941406, prec 0.0437877, recall 0.826368
2017-12-09T23:48:32.397473: step 502, loss 0.716889, acc 0.90625, prec 0.0438368, recall 0.826216
2017-12-09T23:48:32.906828: step 503, loss 0.434929, acc 0.875, prec 0.0438753, recall 0.826475
2017-12-09T23:48:33.413949: step 504, loss 0.385957, acc 0.851562, prec 0.0439321, recall 0.826818
2017-12-09T23:48:33.921161: step 505, loss 0.425842, acc 0.882812, prec 0.044023, recall 0.827246
2017-12-09T23:48:34.430474: step 506, loss 0.323919, acc 0.878906, prec 0.0440625, recall 0.827501
2017-12-09T23:48:34.934504: step 507, loss 0.381503, acc 0.878906, prec 0.0441269, recall 0.827841
2017-12-09T23:48:35.435215: step 508, loss 0.393217, acc 0.847656, prec 0.0440818, recall 0.827841
2017-12-09T23:48:35.956245: step 509, loss 0.427825, acc 0.871094, prec 0.0440938, recall 0.82801
2017-12-09T23:48:36.452289: step 510, loss 0.374391, acc 0.851562, prec 0.0441, recall 0.828179
2017-12-09T23:48:36.954469: step 511, loss 0.328194, acc 0.890625, prec 0.0441426, recall 0.828431
2017-12-09T23:48:37.457664: step 512, loss 1.11994, acc 0.882812, prec 0.044209, recall 0.828362
2017-12-09T23:48:37.967823: step 513, loss 0.247083, acc 0.910156, prec 0.0442323, recall 0.82853
2017-12-09T23:48:38.466592: step 514, loss 0.230235, acc 0.925781, prec 0.0443101, recall 0.828864
2017-12-09T23:48:38.966642: step 515, loss 0.221587, acc 0.9375, prec 0.0443912, recall 0.829197
2017-12-09T23:48:39.472735: step 516, loss 0.254508, acc 0.914062, prec 0.0444653, recall 0.829529
2017-12-09T23:48:39.986626: step 517, loss 0.295144, acc 0.925781, prec 0.0445676, recall 0.829942
2017-12-09T23:48:40.487753: step 518, loss 0.225196, acc 0.910156, prec 0.0446403, recall 0.830271
2017-12-09T23:48:40.991325: step 519, loss 0.661134, acc 0.929688, prec 0.0447447, recall 0.83028
2017-12-09T23:48:41.491173: step 520, loss 1.01143, acc 0.9375, prec 0.0447521, recall 0.829961
2017-12-09T23:48:42.001701: step 521, loss 1.4923, acc 0.910156, prec 0.0448269, recall 0.829491
2017-12-09T23:48:42.511019: step 522, loss 0.31053, acc 0.914062, prec 0.0449004, recall 0.829818
2017-12-09T23:48:43.011449: step 523, loss 0.331828, acc 0.894531, prec 0.0449185, recall 0.829981
2017-12-09T23:48:43.512608: step 524, loss 0.488558, acc 0.882812, prec 0.0449083, recall 0.830062
2017-12-09T23:48:44.012339: step 525, loss 0.837792, acc 0.90625, prec 0.0451523, recall 0.830952
2017-12-09T23:48:44.511107: step 526, loss 0.513576, acc 0.871094, prec 0.0451631, recall 0.831113
2017-12-09T23:48:45.015929: step 527, loss 0.507827, acc 0.8125, prec 0.0452058, recall 0.831434
2017-12-09T23:48:45.517074: step 528, loss 0.518684, acc 0.855469, prec 0.0452119, recall 0.831594
2017-12-09T23:48:46.017773: step 529, loss 0.5628, acc 0.816406, prec 0.0452063, recall 0.831754
2017-12-09T23:48:46.519628: step 530, loss 1.37689, acc 0.808594, prec 0.0451996, recall 0.831519
2017-12-09T23:48:47.024110: step 531, loss 0.847168, acc 0.804688, prec 0.0452163, recall 0.831365
2017-12-09T23:48:47.526232: step 532, loss 0.757505, acc 0.789062, prec 0.0452027, recall 0.831524
2017-12-09T23:48:48.040211: step 533, loss 0.502908, acc 0.820312, prec 0.0452228, recall 0.831762
2017-12-09T23:48:48.549699: step 534, loss 0.40349, acc 0.855469, prec 0.0452533, recall 0.832
2017-12-09T23:48:49.053286: step 535, loss 0.364687, acc 0.851562, prec 0.0452581, recall 0.832158
2017-12-09T23:48:49.558405: step 536, loss 0.563555, acc 0.824219, prec 0.0453036, recall 0.832473
2017-12-09T23:48:50.065353: step 537, loss 1.66932, acc 0.886719, prec 0.0454418, recall 0.832632
2017-12-09T23:48:50.577659: step 538, loss 0.411441, acc 0.878906, prec 0.0455275, recall 0.833022
2017-12-09T23:48:51.089038: step 539, loss 0.232241, acc 0.910156, prec 0.0455252, recall 0.8331
2017-12-09T23:48:51.590425: step 540, loss 0.277543, acc 0.914062, prec 0.0455726, recall 0.833333
2017-12-09T23:48:52.104348: step 541, loss 0.264805, acc 0.917969, prec 0.0455725, recall 0.833411
2017-12-09T23:48:52.602689: step 542, loss 1.08691, acc 0.894531, prec 0.0455909, recall 0.833178
2017-12-09T23:48:53.110812: step 543, loss 0.319038, acc 0.910156, prec 0.0456613, recall 0.833488
2017-12-09T23:48:53.616068: step 544, loss 0.220766, acc 0.929688, prec 0.0456647, recall 0.833565
2017-12-09T23:48:54.115953: step 545, loss 0.616109, acc 0.914062, prec 0.0456645, recall 0.833256
2017-12-09T23:48:54.632995: step 546, loss 1.44116, acc 0.917969, prec 0.0457152, recall 0.832717
2017-12-09T23:48:55.135228: step 547, loss 0.232592, acc 0.902344, prec 0.0457829, recall 0.833026
2017-12-09T23:48:55.644082: step 548, loss 0.867127, acc 0.890625, prec 0.0458483, recall 0.83295
2017-12-09T23:48:56.146777: step 549, loss 0.450817, acc 0.910156, prec 0.0459424, recall 0.833333
2017-12-09T23:48:56.649471: step 550, loss 0.849621, acc 0.859375, prec 0.0460465, recall 0.83341
2017-12-09T23:48:57.155187: step 551, loss 0.580554, acc 0.816406, prec 0.0460641, recall 0.833638
2017-12-09T23:48:57.657615: step 552, loss 0.407433, acc 0.867188, prec 0.046193, recall 0.834169
2017-12-09T23:48:58.163786: step 553, loss 0.602243, acc 0.832031, prec 0.0463112, recall 0.834696
2017-12-09T23:48:58.672139: step 554, loss 0.476391, acc 0.835938, prec 0.0463103, recall 0.834846
2017-12-09T23:48:59.172188: step 555, loss 0.632764, acc 0.804688, prec 0.046324, recall 0.83507
2017-12-09T23:48:59.676172: step 556, loss 1.0685, acc 0.800781, prec 0.0463137, recall 0.834842
2017-12-09T23:49:00.187547: step 557, loss 0.615474, acc 0.816406, prec 0.0463787, recall 0.835214
2017-12-09T23:49:00.704337: step 558, loss 0.605093, acc 0.824219, prec 0.0464697, recall 0.83566
2017-12-09T23:49:01.202246: step 559, loss 1.14677, acc 0.816406, prec 0.046607, recall 0.835874
2017-12-09T23:49:01.707350: step 560, loss 0.376179, acc 0.871094, prec 0.0466162, recall 0.836021
2017-12-09T23:49:02.213020: step 561, loss 0.526588, acc 0.835938, prec 0.0466863, recall 0.836388
2017-12-09T23:49:02.713390: step 562, loss 0.473542, acc 0.828125, prec 0.0468014, recall 0.836898
2017-12-09T23:49:03.221132: step 563, loss 0.805926, acc 0.863281, prec 0.0468567, recall 0.836816
2017-12-09T23:49:03.721814: step 564, loss 0.391028, acc 0.820312, prec 0.0468505, recall 0.836961
2017-12-09T23:49:04.220075: step 565, loss 0.40664, acc 0.886719, prec 0.0469115, recall 0.837251
2017-12-09T23:49:04.725780: step 566, loss 0.423203, acc 0.863281, prec 0.0469654, recall 0.837539
2017-12-09T23:49:05.228415: step 567, loss 0.963606, acc 0.933594, prec 0.0470886, recall 0.837599
2017-12-09T23:49:05.758157: step 568, loss 0.255675, acc 0.917969, prec 0.0471113, recall 0.837743
2017-12-09T23:49:06.260310: step 569, loss 0.247671, acc 0.925781, prec 0.04716, recall 0.837957
2017-12-09T23:49:06.766623: step 570, loss 0.588682, acc 0.886719, prec 0.0472441, recall 0.838313
2017-12-09T23:49:07.266392: step 571, loss 1.00911, acc 0.90625, prec 0.0472408, recall 0.838016
2017-12-09T23:49:07.768363: step 572, loss 0.843121, acc 0.910156, prec 0.0472858, recall 0.837862
2017-12-09T23:49:08.272976: step 573, loss 0.543326, acc 0.886719, prec 0.0473461, recall 0.838145
2017-12-09T23:49:08.778786: step 574, loss 0.364943, acc 0.863281, prec 0.0473287, recall 0.838216
2017-12-09T23:49:09.279006: step 575, loss 0.636936, acc 0.855469, prec 0.0474029, recall 0.838569
2017-12-09T23:49:09.779405: step 576, loss 0.679804, acc 0.882812, prec 0.0474618, recall 0.83885
2017-12-09T23:49:10.288317: step 577, loss 0.369965, acc 0.863281, prec 0.0474678, recall 0.83899
2017-12-09T23:49:10.790373: step 578, loss 0.479382, acc 0.855469, prec 0.0475652, recall 0.83941
2017-12-09T23:49:11.292286: step 579, loss 0.407718, acc 0.863281, prec 0.0476413, recall 0.839757
2017-12-09T23:49:11.798566: step 580, loss 0.382347, acc 0.863281, prec 0.0476003, recall 0.839757
2017-12-09T23:49:12.305508: step 581, loss 0.480031, acc 0.898438, prec 0.0476868, recall 0.840104
2017-12-09T23:49:12.814175: step 582, loss 0.991107, acc 0.867188, prec 0.0477182, recall 0.839948
2017-12-09T23:49:13.319641: step 583, loss 0.379592, acc 0.910156, prec 0.0477847, recall 0.840224
2017-12-09T23:49:13.825032: step 584, loss 0.327991, acc 0.875, prec 0.0477938, recall 0.840361
2017-12-09T23:49:14.324273: step 585, loss 0.402554, acc 0.894531, prec 0.0478554, recall 0.840636
2017-12-09T23:49:14.830180: step 586, loss 0.610835, acc 0.847656, prec 0.0479261, recall 0.840977
2017-12-09T23:49:15.332224: step 587, loss 0.297472, acc 0.882812, prec 0.0479608, recall 0.841182
2017-12-09T23:49:15.841928: step 588, loss 1.4431, acc 0.886719, prec 0.0479512, recall 0.84089
2017-12-09T23:49:16.344557: step 589, loss 0.658977, acc 0.871094, prec 0.0479823, recall 0.841094
2017-12-09T23:49:16.844552: step 590, loss 0.507679, acc 0.914062, prec 0.0480725, recall 0.841432
2017-12-09T23:49:17.355501: step 591, loss 0.432325, acc 0.878906, prec 0.0480825, recall 0.841567
2017-12-09T23:49:17.859400: step 592, loss 0.361124, acc 0.855469, prec 0.0481319, recall 0.841837
2017-12-09T23:49:18.359012: step 593, loss 0.414439, acc 0.863281, prec 0.0482296, recall 0.842239
2017-12-09T23:49:18.859043: step 594, loss 0.411484, acc 0.867188, prec 0.0482129, recall 0.842306
2017-12-09T23:49:19.366159: step 595, loss 0.575122, acc 0.886719, prec 0.0482713, recall 0.842573
2017-12-09T23:49:19.875154: step 596, loss 0.227975, acc 0.945312, prec 0.048278, recall 0.84264
2017-12-09T23:49:20.376559: step 597, loss 0.488855, acc 0.886719, prec 0.0483824, recall 0.843038
2017-12-09T23:49:20.899066: step 598, loss 0.532838, acc 0.902344, prec 0.0484913, recall 0.843434
2017-12-09T23:49:21.398740: step 599, loss 1.50425, acc 0.910156, prec 0.0485817, recall 0.843055
2017-12-09T23:49:21.902829: step 600, loss 0.298139, acc 0.894531, prec 0.048619, recall 0.843252

Evaluation:
2017-12-09T23:49:26.573862: step 600, loss 1.65479, acc 0.90017, prec 0.0495409, recall 0.82802

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-600

2017-12-09T23:49:28.119596: step 601, loss 0.568597, acc 0.894531, prec 0.0495765, recall 0.828223
2017-12-09T23:49:28.622450: step 602, loss 0.365557, acc 0.871094, prec 0.0496497, recall 0.82856
2017-12-09T23:49:29.125631: step 603, loss 0.428072, acc 0.859375, prec 0.0496301, recall 0.828627
2017-12-09T23:49:29.625860: step 604, loss 0.352604, acc 0.847656, prec 0.0496292, recall 0.828762
2017-12-09T23:49:30.132900: step 605, loss 0.483028, acc 0.847656, prec 0.0496507, recall 0.828963
2017-12-09T23:49:30.646567: step 606, loss 0.302363, acc 0.890625, prec 0.0496627, recall 0.829097
2017-12-09T23:49:31.151266: step 607, loss 0.45717, acc 0.875, prec 0.0496922, recall 0.829297
2017-12-09T23:49:31.655120: step 608, loss 0.36879, acc 0.882812, prec 0.0497685, recall 0.82963
2017-12-09T23:49:32.166953: step 609, loss 1.03751, acc 0.90625, prec 0.0498305, recall 0.829572
2017-12-09T23:49:32.672242: step 610, loss 0.47267, acc 0.851562, prec 0.0498751, recall 0.829837
2017-12-09T23:49:33.169412: step 611, loss 0.50948, acc 0.863281, prec 0.0499673, recall 0.830233
2017-12-09T23:49:33.676073: step 612, loss 0.309006, acc 0.898438, prec 0.0500256, recall 0.830495
2017-12-09T23:49:34.178820: step 613, loss 0.286675, acc 0.902344, prec 0.0500408, recall 0.830626
2017-12-09T23:49:34.681305: step 614, loss 0.971964, acc 0.867188, prec 0.0500908, recall 0.830567
2017-12-09T23:49:35.184385: step 615, loss 0.284167, acc 0.941406, prec 0.0501175, recall 0.830698
2017-12-09T23:49:35.709258: step 616, loss 0.940866, acc 0.878906, prec 0.0501488, recall 0.830574
2017-12-09T23:49:36.219779: step 617, loss 0.462001, acc 0.875, prec 0.0501556, recall 0.830704
2017-12-09T23:49:36.722281: step 618, loss 0.258584, acc 0.910156, prec 0.050195, recall 0.830899
2017-12-09T23:49:37.226769: step 619, loss 0.274271, acc 0.90625, prec 0.0502332, recall 0.831094
2017-12-09T23:49:37.724127: step 620, loss 0.468899, acc 0.878906, prec 0.0502631, recall 0.831288
2017-12-09T23:49:38.223793: step 621, loss 0.321475, acc 0.914062, prec 0.0503255, recall 0.831547
2017-12-09T23:49:38.727815: step 622, loss 0.788957, acc 0.894531, prec 0.0503832, recall 0.831486
2017-12-09T23:49:39.234760: step 623, loss 0.445702, acc 0.859375, prec 0.0504071, recall 0.831679
2017-12-09T23:49:39.736023: step 624, loss 0.388313, acc 0.894531, prec 0.0504635, recall 0.831936
2017-12-09T23:49:39.922090: step 625, loss 0.893494, acc 0.942308, prec 0.0504819, recall 0.832
2017-12-09T23:49:40.428175: step 626, loss 0.419979, acc 0.875, prec 0.0505104, recall 0.832192
2017-12-09T23:49:40.937353: step 627, loss 0.369175, acc 0.875, prec 0.0506046, recall 0.832574
2017-12-09T23:49:41.442265: step 628, loss 0.515558, acc 0.890625, prec 0.0507251, recall 0.833018
2017-12-09T23:49:41.949320: step 629, loss 0.743056, acc 0.820312, prec 0.0507151, recall 0.833144
2017-12-09T23:49:42.451949: step 630, loss 0.315544, acc 0.871094, prec 0.0507858, recall 0.833459
2017-12-09T23:49:42.957334: step 631, loss 0.391264, acc 0.878906, prec 0.0508151, recall 0.833648
2017-12-09T23:49:43.460907: step 632, loss 0.459383, acc 0.847656, prec 0.0508786, recall 0.833961
2017-12-09T23:49:43.962846: step 633, loss 0.526914, acc 0.871094, prec 0.050949, recall 0.834273
2017-12-09T23:49:44.461934: step 634, loss 0.426452, acc 0.871094, prec 0.0509757, recall 0.834459
2017-12-09T23:49:44.960395: step 635, loss 0.695978, acc 0.878906, prec 0.0510059, recall 0.834333
2017-12-09T23:49:45.467169: step 636, loss 0.326358, acc 0.921875, prec 0.0510695, recall 0.834581
2017-12-09T23:49:45.976259: step 637, loss 0.392336, acc 0.890625, prec 0.0511236, recall 0.834828
2017-12-09T23:49:46.479001: step 638, loss 0.347959, acc 0.925781, prec 0.0511231, recall 0.83489
2017-12-09T23:49:46.975747: step 639, loss 0.261666, acc 0.914062, prec 0.0511842, recall 0.835136
2017-12-09T23:49:47.482209: step 640, loss 0.333505, acc 0.875, prec 0.0512118, recall 0.83532
2017-12-09T23:49:47.990662: step 641, loss 0.958234, acc 0.894531, prec 0.0512897, recall 0.835316
2017-12-09T23:49:48.493987: step 642, loss 0.298935, acc 0.898438, prec 0.0513025, recall 0.835438
2017-12-09T23:49:48.996096: step 643, loss 0.218831, acc 0.910156, prec 0.0513189, recall 0.835561
2017-12-09T23:49:49.503904: step 644, loss 0.856313, acc 0.894531, prec 0.0513101, recall 0.835312
2017-12-09T23:49:50.009643: step 645, loss 0.501598, acc 0.929688, prec 0.0514619, recall 0.835799
2017-12-09T23:49:50.515074: step 646, loss 0.318816, acc 0.917969, prec 0.051502, recall 0.835981
2017-12-09T23:49:51.020593: step 647, loss 0.229192, acc 0.914062, prec 0.0515194, recall 0.836102
2017-12-09T23:49:51.523134: step 648, loss 1.01816, acc 0.921875, prec 0.0515619, recall 0.835975
2017-12-09T23:49:52.032221: step 649, loss 0.34744, acc 0.914062, prec 0.0516007, recall 0.836156
2017-12-09T23:49:52.535895: step 650, loss 0.382904, acc 0.914062, prec 0.0516611, recall 0.836397
2017-12-09T23:49:53.050937: step 651, loss 0.504528, acc 0.902344, prec 0.0517394, recall 0.836697
2017-12-09T23:49:53.555156: step 652, loss 0.387726, acc 0.882812, prec 0.0518117, recall 0.836996
2017-12-09T23:49:54.065354: step 653, loss 0.396064, acc 0.878906, prec 0.0518183, recall 0.837116
2017-12-09T23:49:54.566343: step 654, loss 0.33796, acc 0.867188, prec 0.0518213, recall 0.837235
2017-12-09T23:49:55.061687: step 655, loss 0.337358, acc 0.886719, prec 0.0518517, recall 0.837413
2017-12-09T23:49:55.560754: step 656, loss 0.573035, acc 0.878906, prec 0.0519225, recall 0.83771
2017-12-09T23:49:56.073610: step 657, loss 0.463476, acc 0.867188, prec 0.0519469, recall 0.837887
2017-12-09T23:49:56.575619: step 658, loss 0.35096, acc 0.882812, prec 0.0519973, recall 0.838123
2017-12-09T23:49:57.080977: step 659, loss 0.23347, acc 0.910156, prec 0.0519917, recall 0.838182
2017-12-09T23:49:57.578978: step 660, loss 0.339731, acc 0.914062, prec 0.0520728, recall 0.838476
2017-12-09T23:49:58.079857: step 661, loss 0.489082, acc 0.925781, prec 0.0521798, recall 0.838523
2017-12-09T23:49:58.583685: step 662, loss 0.270946, acc 0.9375, prec 0.0522464, recall 0.838756
2017-12-09T23:49:59.096768: step 663, loss 0.267469, acc 0.941406, prec 0.0523141, recall 0.838989
2017-12-09T23:49:59.600453: step 664, loss 0.328354, acc 0.945312, prec 0.0523829, recall 0.839221
2017-12-09T23:50:00.104967: step 665, loss 0.64032, acc 0.929688, prec 0.0524055, recall 0.839035
2017-12-09T23:50:00.621097: step 666, loss 1.39731, acc 0.941406, prec 0.0524328, recall 0.838547
2017-12-09T23:50:01.132377: step 667, loss 0.501081, acc 0.90625, prec 0.0524684, recall 0.838721
2017-12-09T23:50:01.632149: step 668, loss 0.446038, acc 0.925781, prec 0.0524885, recall 0.838837
2017-12-09T23:50:02.140690: step 669, loss 0.366999, acc 0.871094, prec 0.052556, recall 0.839126
2017-12-09T23:50:02.640789: step 670, loss 0.410186, acc 0.867188, prec 0.0525797, recall 0.839298
2017-12-09T23:50:03.149622: step 671, loss 0.403217, acc 0.886719, prec 0.0525879, recall 0.839413
2017-12-09T23:50:03.663152: step 672, loss 1.02282, acc 0.847656, prec 0.0526493, recall 0.8394
2017-12-09T23:50:04.171743: step 673, loss 0.406465, acc 0.859375, prec 0.0527552, recall 0.839801
2017-12-09T23:50:04.675148: step 674, loss 0.490249, acc 0.847656, prec 0.0527727, recall 0.839972
2017-12-09T23:50:05.175831: step 675, loss 0.40817, acc 0.851562, prec 0.0527914, recall 0.840142
2017-12-09T23:50:05.686955: step 676, loss 0.473019, acc 0.855469, prec 0.0528534, recall 0.840426
2017-12-09T23:50:06.188497: step 677, loss 0.481841, acc 0.835938, prec 0.0528884, recall 0.840652
2017-12-09T23:50:06.697479: step 678, loss 0.41152, acc 0.832031, prec 0.052901, recall 0.840821
2017-12-09T23:50:07.195915: step 679, loss 0.389583, acc 0.875, prec 0.0529476, recall 0.841046
2017-12-09T23:50:07.700338: step 680, loss 0.483573, acc 0.925781, prec 0.0529673, recall 0.841158
2017-12-09T23:50:08.200219: step 681, loss 0.638958, acc 0.910156, prec 0.0529625, recall 0.840917
2017-12-09T23:50:08.704720: step 682, loss 0.28461, acc 0.917969, prec 0.0530009, recall 0.841085
2017-12-09T23:50:09.203754: step 683, loss 1.29685, acc 0.921875, prec 0.0530626, recall 0.841013
2017-12-09T23:50:09.710198: step 684, loss 0.191172, acc 0.933594, prec 0.0530846, recall 0.841125
2017-12-09T23:50:10.212053: step 685, loss 0.255116, acc 0.941406, prec 0.0531089, recall 0.841236
2017-12-09T23:50:10.713870: step 686, loss 0.275689, acc 0.925781, prec 0.0531705, recall 0.841459
2017-12-09T23:50:11.216566: step 687, loss 0.804081, acc 0.886719, prec 0.0531585, recall 0.84122
2017-12-09T23:50:11.730415: step 688, loss 0.417069, acc 0.925781, prec 0.05322, recall 0.841442
2017-12-09T23:50:12.230785: step 689, loss 0.360303, acc 0.914062, prec 0.0532779, recall 0.841664
2017-12-09T23:50:12.736598: step 690, loss 0.709217, acc 0.914062, prec 0.053295, recall 0.84148
2017-12-09T23:50:13.237704: step 691, loss 0.262532, acc 0.914062, prec 0.0533319, recall 0.841646
2017-12-09T23:50:13.740256: step 692, loss 0.564242, acc 0.890625, prec 0.053487, recall 0.842142
2017-12-09T23:50:14.250157: step 693, loss 0.345189, acc 0.882812, prec 0.0534725, recall 0.842197
2017-12-09T23:50:14.750706: step 694, loss 0.39476, acc 0.90625, prec 0.053486, recall 0.842306
2017-12-09T23:50:15.250392: step 695, loss 0.249835, acc 0.910156, prec 0.0535006, recall 0.842416
2017-12-09T23:50:15.754334: step 696, loss 0.549511, acc 0.886719, prec 0.0535706, recall 0.842689
2017-12-09T23:50:16.265636: step 697, loss 0.39794, acc 0.847656, prec 0.0536288, recall 0.842961
2017-12-09T23:50:16.775097: step 698, loss 0.402571, acc 0.925781, prec 0.0536688, recall 0.843124
2017-12-09T23:50:17.279113: step 699, loss 0.316156, acc 0.882812, prec 0.053675, recall 0.843232
2017-12-09T23:50:17.782360: step 700, loss 0.25887, acc 0.902344, prec 0.0536456, recall 0.843232
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-700

2017-12-09T23:50:19.221357: step 701, loss 0.397116, acc 0.863281, prec 0.0536874, recall 0.843448
2017-12-09T23:50:19.722848: step 702, loss 0.536337, acc 0.910156, prec 0.0537849, recall 0.843772
2017-12-09T23:50:20.230569: step 703, loss 1.45457, acc 0.90625, prec 0.053903, recall 0.843857
2017-12-09T23:50:20.749221: step 704, loss 0.611929, acc 0.902344, prec 0.0539771, recall 0.844125
2017-12-09T23:50:21.252387: step 705, loss 0.336812, acc 0.914062, prec 0.0539925, recall 0.844231
2017-12-09T23:50:21.751427: step 706, loss 0.30566, acc 0.898438, prec 0.0540653, recall 0.844498
2017-12-09T23:50:22.257334: step 707, loss 0.206357, acc 0.929688, prec 0.0541061, recall 0.844657
2017-12-09T23:50:22.762943: step 708, loss 0.339585, acc 0.882812, prec 0.0540913, recall 0.84471
2017-12-09T23:50:23.271658: step 709, loss 0.24529, acc 0.925781, prec 0.0541928, recall 0.845027
2017-12-09T23:50:23.774962: step 710, loss 0.214888, acc 0.910156, prec 0.0542275, recall 0.845185
2017-12-09T23:50:24.277917: step 711, loss 0.353529, acc 0.921875, prec 0.0542658, recall 0.845343
2017-12-09T23:50:24.791494: step 712, loss 0.272817, acc 0.90625, prec 0.0543405, recall 0.845606
2017-12-09T23:50:25.298200: step 713, loss 0.190331, acc 0.921875, prec 0.054358, recall 0.84571
2017-12-09T23:50:25.797906: step 714, loss 0.345243, acc 0.90625, prec 0.0543502, recall 0.845763
2017-12-09T23:50:26.300288: step 715, loss 0.141414, acc 0.949219, prec 0.0543966, recall 0.845919
2017-12-09T23:50:26.802094: step 716, loss 0.354211, acc 0.917969, prec 0.0544335, recall 0.846076
2017-12-09T23:50:27.310261: step 717, loss 0.210259, acc 0.929688, prec 0.0544739, recall 0.846232
2017-12-09T23:50:27.815240: step 718, loss 0.178339, acc 0.933594, prec 0.0544948, recall 0.846336
2017-12-09T23:50:28.320217: step 719, loss 0.297455, acc 0.957031, prec 0.054564, recall 0.846543
2017-12-09T23:50:28.832401: step 720, loss 0.663205, acc 0.945312, prec 0.0545897, recall 0.846361
2017-12-09T23:50:29.337670: step 721, loss 0.219824, acc 0.960938, prec 0.0546805, recall 0.84662
2017-12-09T23:50:29.849080: step 722, loss 0.131764, acc 0.945312, prec 0.054705, recall 0.846723
2017-12-09T23:50:30.364287: step 723, loss 0.670523, acc 0.960938, prec 0.0547752, recall 0.846928
2017-12-09T23:50:30.869960: step 724, loss 0.373166, acc 0.929688, prec 0.0547743, recall 0.84698
2017-12-09T23:50:31.372330: step 725, loss 0.682772, acc 0.957031, prec 0.0548855, recall 0.847004
2017-12-09T23:50:31.873937: step 726, loss 0.495884, acc 0.933594, prec 0.0549677, recall 0.847259
2017-12-09T23:50:32.376209: step 727, loss 0.376065, acc 0.921875, prec 0.0549848, recall 0.847361
2017-12-09T23:50:32.876416: step 728, loss 0.530774, acc 0.871094, prec 0.0550274, recall 0.847565
2017-12-09T23:50:33.380963: step 729, loss 0.482792, acc 0.882812, prec 0.0550939, recall 0.847819
2017-12-09T23:50:33.881931: step 730, loss 0.322656, acc 0.894531, prec 0.0551026, recall 0.84792
2017-12-09T23:50:34.384438: step 731, loss 0.723887, acc 0.835938, prec 0.0551547, recall 0.848173
2017-12-09T23:50:34.890703: step 732, loss 0.44742, acc 0.839844, prec 0.0551671, recall 0.848324
2017-12-09T23:50:35.402367: step 733, loss 0.459927, acc 0.871094, prec 0.0551685, recall 0.848425
2017-12-09T23:50:35.918786: step 734, loss 0.323899, acc 0.902344, prec 0.0553221, recall 0.848876
2017-12-09T23:50:36.427917: step 735, loss 0.411687, acc 0.84375, prec 0.0552948, recall 0.848926
2017-12-09T23:50:36.931539: step 736, loss 0.337456, acc 0.882812, prec 0.0552997, recall 0.849025
2017-12-09T23:50:37.437231: step 737, loss 0.414388, acc 0.886719, prec 0.0553262, recall 0.849175
2017-12-09T23:50:37.943995: step 738, loss 0.629061, acc 0.878906, prec 0.0554111, recall 0.849473
2017-12-09T23:50:38.449515: step 739, loss 0.559417, acc 0.890625, prec 0.0554398, recall 0.849342
2017-12-09T23:50:38.948196: step 740, loss 0.269948, acc 0.886719, prec 0.0554662, recall 0.849491
2017-12-09T23:50:39.455953: step 741, loss 1.10111, acc 0.902344, prec 0.0555996, recall 0.849607
2017-12-09T23:50:39.953718: step 742, loss 0.769627, acc 0.910156, prec 0.0556544, recall 0.849526
2017-12-09T23:50:40.455050: step 743, loss 0.464314, acc 0.894531, prec 0.0557031, recall 0.849722
2017-12-09T23:50:40.962605: step 744, loss 0.348234, acc 0.890625, prec 0.0557505, recall 0.849918
2017-12-09T23:50:41.466731: step 745, loss 0.378444, acc 0.898438, prec 0.0557801, recall 0.850065
2017-12-09T23:50:41.969297: step 746, loss 0.354687, acc 0.914062, prec 0.0558952, recall 0.850407
2017-12-09T23:50:42.468245: step 747, loss 0.607684, acc 0.871094, prec 0.0558973, recall 0.850227
2017-12-09T23:50:42.966175: step 748, loss 0.251733, acc 0.90625, prec 0.0559291, recall 0.850373
2017-12-09T23:50:43.466030: step 749, loss 0.258432, acc 0.90625, prec 0.0560213, recall 0.850664
2017-12-09T23:50:43.643182: step 750, loss 0.306975, acc 0.923077, prec 0.0560367, recall 0.850712
2017-12-09T23:50:44.148989: step 751, loss 0.431333, acc 0.878906, prec 0.0561003, recall 0.850954
2017-12-09T23:50:44.654043: step 752, loss 0.29537, acc 0.894531, prec 0.0561082, recall 0.85105
2017-12-09T23:50:45.153364: step 753, loss 0.28789, acc 0.914062, prec 0.0561422, recall 0.851194
2017-12-09T23:50:45.658965: step 754, loss 0.297691, acc 0.902344, prec 0.0561726, recall 0.851338
2017-12-09T23:50:46.158972: step 755, loss 0.34454, acc 0.910156, prec 0.0562455, recall 0.851578
2017-12-09T23:50:46.667994: step 756, loss 0.454915, acc 0.941406, prec 0.0562488, recall 0.851351
2017-12-09T23:50:47.165194: step 757, loss 0.138448, acc 0.949219, prec 0.0563736, recall 0.851685
2017-12-09T23:50:47.666162: step 758, loss 0.972268, acc 0.925781, prec 0.0564134, recall 0.851282
2017-12-09T23:50:48.175641: step 759, loss 0.195849, acc 0.949219, prec 0.0564579, recall 0.851425
2017-12-09T23:50:48.682974: step 760, loss 0.241934, acc 0.929688, prec 0.0565365, recall 0.851662
2017-12-09T23:50:49.190698: step 761, loss 0.219135, acc 0.929688, prec 0.056635, recall 0.851946
2017-12-09T23:50:49.688369: step 762, loss 0.475624, acc 0.890625, prec 0.0566414, recall 0.852041
2017-12-09T23:50:50.198038: step 763, loss 0.550305, acc 0.925781, prec 0.0567385, recall 0.852323
2017-12-09T23:50:50.710912: step 764, loss 0.220529, acc 0.941406, prec 0.0567804, recall 0.852464
2017-12-09T23:50:51.217087: step 765, loss 0.321818, acc 0.914062, prec 0.0568737, recall 0.852745
2017-12-09T23:50:51.725611: step 766, loss 0.40451, acc 0.917969, prec 0.0569083, recall 0.852885
2017-12-09T23:50:52.228357: step 767, loss 0.274395, acc 0.910156, prec 0.0569604, recall 0.853072
2017-12-09T23:50:52.731815: step 768, loss 0.235542, acc 0.933594, prec 0.0570197, recall 0.853257
2017-12-09T23:50:53.233572: step 769, loss 0.218173, acc 0.933594, prec 0.0570789, recall 0.853443
2017-12-09T23:50:53.740131: step 770, loss 0.390986, acc 0.882812, prec 0.0570825, recall 0.853535
2017-12-09T23:50:54.246471: step 771, loss 0.325701, acc 0.929688, prec 0.0571603, recall 0.853766
2017-12-09T23:50:54.755834: step 772, loss 0.247167, acc 0.90625, prec 0.0572109, recall 0.85395
2017-12-09T23:50:55.260323: step 773, loss 0.347198, acc 0.914062, prec 0.0572043, recall 0.853996
2017-12-09T23:50:55.765351: step 774, loss 0.529947, acc 0.894531, prec 0.0572325, recall 0.853866
2017-12-09T23:50:56.273404: step 775, loss 0.373727, acc 0.933594, prec 0.0572716, recall 0.854003
2017-12-09T23:50:56.775097: step 776, loss 0.316842, acc 0.945312, prec 0.0573936, recall 0.854323
2017-12-09T23:50:57.282910: step 777, loss 0.315944, acc 0.933594, prec 0.0574722, recall 0.854551
2017-12-09T23:50:57.778609: step 778, loss 0.272733, acc 0.933594, prec 0.0574715, recall 0.854597
2017-12-09T23:50:58.285585: step 779, loss 1.59224, acc 0.90625, prec 0.0575428, recall 0.854557
2017-12-09T23:50:58.792481: step 780, loss 0.317937, acc 0.941406, prec 0.057584, recall 0.854693
2017-12-09T23:50:59.294761: step 781, loss 0.592115, acc 0.917969, prec 0.0576588, recall 0.854653
2017-12-09T23:50:59.802209: step 782, loss 0.244024, acc 0.902344, prec 0.0577076, recall 0.854834
2017-12-09T23:51:00.317654: step 783, loss 0.32073, acc 0.875, prec 0.0577282, recall 0.854969
2017-12-09T23:51:00.823639: step 784, loss 0.28309, acc 0.917969, prec 0.0577818, recall 0.855149
2017-12-09T23:51:01.326678: step 785, loss 0.393973, acc 0.890625, prec 0.057886, recall 0.855463
2017-12-09T23:51:01.825774: step 786, loss 0.285791, acc 0.886719, prec 0.0578903, recall 0.855552
2017-12-09T23:51:02.331878: step 787, loss 0.329333, acc 0.878906, prec 0.0579513, recall 0.855775
2017-12-09T23:51:02.826581: step 788, loss 0.526032, acc 0.855469, prec 0.0580049, recall 0.855998
2017-12-09T23:51:03.329056: step 789, loss 0.360249, acc 0.890625, prec 0.0580497, recall 0.856175
2017-12-09T23:51:03.828191: step 790, loss 0.820768, acc 0.886719, prec 0.0580747, recall 0.856044
2017-12-09T23:51:04.334364: step 791, loss 0.396444, acc 0.878906, prec 0.0580961, recall 0.856177
2017-12-09T23:51:04.834929: step 792, loss 0.229005, acc 0.914062, prec 0.0581088, recall 0.856265
2017-12-09T23:51:05.336750: step 793, loss 0.292679, acc 0.90625, prec 0.0580993, recall 0.856309
2017-12-09T23:51:05.864883: step 794, loss 0.21294, acc 0.921875, prec 0.0581928, recall 0.856574
2017-12-09T23:51:06.375937: step 795, loss 0.294526, acc 0.917969, prec 0.0582654, recall 0.856793
2017-12-09T23:51:06.880570: step 796, loss 0.802261, acc 0.914062, prec 0.0582791, recall 0.856619
2017-12-09T23:51:07.384707: step 797, loss 0.385872, acc 0.921875, prec 0.0583136, recall 0.85675
2017-12-09T23:51:07.884778: step 798, loss 0.646776, acc 0.921875, prec 0.0583493, recall 0.85662
2017-12-09T23:51:08.393181: step 799, loss 0.217697, acc 0.921875, prec 0.0583641, recall 0.856707
2017-12-09T23:51:08.899209: step 800, loss 0.320603, acc 0.898438, prec 0.0583913, recall 0.856838
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-800

2017-12-09T23:51:10.290193: step 801, loss 0.325153, acc 0.929688, prec 0.0584281, recall 0.856969
2017-12-09T23:51:10.792511: step 802, loss 0.185088, acc 0.949219, prec 0.0584514, recall 0.857056
2017-12-09T23:51:11.291487: step 803, loss 0.407609, acc 0.894531, prec 0.0584772, recall 0.857186
2017-12-09T23:51:11.796319: step 804, loss 0.313747, acc 0.90625, prec 0.0585067, recall 0.857316
2017-12-09T23:51:12.293614: step 805, loss 0.432665, acc 0.9375, prec 0.0585848, recall 0.857533
2017-12-09T23:51:12.788923: step 806, loss 0.355876, acc 0.914062, prec 0.0586165, recall 0.857662
2017-12-09T23:51:13.296572: step 807, loss 0.331106, acc 0.914062, prec 0.0586872, recall 0.857877
2017-12-09T23:51:13.794824: step 808, loss 0.306381, acc 0.902344, prec 0.0587348, recall 0.858049
2017-12-09T23:51:14.300525: step 809, loss 0.200951, acc 0.933594, prec 0.058753, recall 0.858135
2017-12-09T23:51:14.805523: step 810, loss 0.253206, acc 0.929688, prec 0.0587895, recall 0.858263
2017-12-09T23:51:15.309681: step 811, loss 0.215634, acc 0.945312, prec 0.0589086, recall 0.858562
2017-12-09T23:51:15.808741: step 812, loss 0.242842, acc 0.910156, prec 0.0589389, recall 0.858689
2017-12-09T23:51:16.318671: step 813, loss 0.233015, acc 0.957031, prec 0.0590225, recall 0.858901
2017-12-09T23:51:16.817649: step 814, loss 0.201277, acc 0.929688, prec 0.0590006, recall 0.858901
2017-12-09T23:51:17.325557: step 815, loss 0.159544, acc 0.960938, prec 0.0590467, recall 0.859028
2017-12-09T23:51:17.833487: step 816, loss 0.333777, acc 0.925781, prec 0.0591593, recall 0.859324
2017-12-09T23:51:18.339311: step 817, loss 0.370662, acc 0.941406, prec 0.0591798, recall 0.859408
2017-12-09T23:51:18.846076: step 818, loss 0.243021, acc 0.9375, prec 0.0592571, recall 0.859618
2017-12-09T23:51:19.351008: step 819, loss 0.701796, acc 0.953125, prec 0.0593405, recall 0.859571
2017-12-09T23:51:19.860829: step 820, loss 0.287004, acc 0.941406, prec 0.0594383, recall 0.859821
2017-12-09T23:51:20.364953: step 821, loss 0.321959, acc 0.980469, prec 0.0595096, recall 0.859988
2017-12-09T23:51:20.892789: step 822, loss 0.268102, acc 0.976562, prec 0.0595216, recall 0.86003
2017-12-09T23:51:21.406315: step 823, loss 0.171124, acc 0.96875, prec 0.0595505, recall 0.860113
2017-12-09T23:51:21.907540: step 824, loss 0.0952695, acc 0.972656, prec 0.0595806, recall 0.860196
2017-12-09T23:51:22.406391: step 825, loss 0.192525, acc 0.964844, prec 0.0596662, recall 0.860403
2017-12-09T23:51:22.918765: step 826, loss 0.165308, acc 0.960938, prec 0.0596926, recall 0.860486
2017-12-09T23:51:23.420995: step 827, loss 0.215122, acc 0.964844, prec 0.0597202, recall 0.860568
2017-12-09T23:51:23.929751: step 828, loss 0.401345, acc 0.9375, prec 0.0597585, recall 0.860692
2017-12-09T23:51:24.436143: step 829, loss 0.51687, acc 0.929688, prec 0.0598908, recall 0.861021
2017-12-09T23:51:24.946296: step 830, loss 0.204469, acc 0.9375, prec 0.059929, recall 0.861144
2017-12-09T23:51:25.455552: step 831, loss 0.383211, acc 0.933594, prec 0.0599467, recall 0.861226
2017-12-09T23:51:25.969179: step 832, loss 0.201694, acc 0.929688, prec 0.0600209, recall 0.86143
2017-12-09T23:51:26.471924: step 833, loss 0.270447, acc 0.9375, prec 0.0600398, recall 0.861511
2017-12-09T23:51:26.976035: step 834, loss 0.28656, acc 0.910156, prec 0.0600307, recall 0.861552
2017-12-09T23:51:27.479412: step 835, loss 0.464775, acc 0.882812, prec 0.0600901, recall 0.861755
2017-12-09T23:51:27.994359: step 836, loss 0.221179, acc 0.953125, prec 0.0601522, recall 0.861917
2017-12-09T23:51:28.508099: step 837, loss 0.261811, acc 0.9375, prec 0.0602479, recall 0.86216
2017-12-09T23:51:29.010665: step 838, loss 0.31289, acc 0.917969, prec 0.0602796, recall 0.862281
2017-12-09T23:51:29.510130: step 839, loss 0.206534, acc 0.9375, prec 0.0603175, recall 0.862401
2017-12-09T23:51:30.023835: step 840, loss 0.25217, acc 0.902344, prec 0.0602867, recall 0.862401
2017-12-09T23:51:30.538650: step 841, loss 0.197146, acc 0.921875, prec 0.0603388, recall 0.862562
2017-12-09T23:51:31.047314: step 842, loss 0.347243, acc 0.921875, prec 0.0604292, recall 0.862802
2017-12-09T23:51:31.551550: step 843, loss 0.93833, acc 0.914062, prec 0.0604417, recall 0.862631
2017-12-09T23:51:32.059021: step 844, loss 0.539947, acc 0.898438, prec 0.0604863, recall 0.862791
2017-12-09T23:51:32.559946: step 845, loss 0.269647, acc 0.925781, prec 0.0605011, recall 0.86287
2017-12-09T23:51:33.063947: step 846, loss 0.258344, acc 0.90625, prec 0.0605481, recall 0.86303
2017-12-09T23:51:33.564555: step 847, loss 0.244192, acc 0.921875, prec 0.0605999, recall 0.863188
2017-12-09T23:51:34.066620: step 848, loss 0.273521, acc 0.902344, prec 0.0606455, recall 0.863347
2017-12-09T23:51:34.566370: step 849, loss 0.355664, acc 0.929688, prec 0.0606615, recall 0.863426
2017-12-09T23:51:35.077900: step 850, loss 0.196096, acc 0.945312, prec 0.0607206, recall 0.863584
2017-12-09T23:51:35.585296: step 851, loss 0.274676, acc 0.925781, prec 0.0608307, recall 0.863859
2017-12-09T23:51:36.085473: step 852, loss 0.360664, acc 0.898438, prec 0.0608749, recall 0.864016
2017-12-09T23:51:36.590413: step 853, loss 0.475192, acc 0.945312, prec 0.0610481, recall 0.864407
2017-12-09T23:51:37.103375: step 854, loss 0.413795, acc 0.941406, prec 0.0611247, recall 0.864601
2017-12-09T23:51:37.603189: step 855, loss 0.669276, acc 0.902344, prec 0.0611521, recall 0.86447
2017-12-09T23:51:38.104411: step 856, loss 0.288848, acc 0.890625, prec 0.0611174, recall 0.86447
2017-12-09T23:51:38.615322: step 857, loss 0.271826, acc 0.9375, prec 0.0611356, recall 0.864548
2017-12-09T23:51:39.116471: step 858, loss 0.232553, acc 0.917969, prec 0.0611857, recall 0.864703
2017-12-09T23:51:39.628303: step 859, loss 0.195043, acc 0.949219, prec 0.0612076, recall 0.86478
2017-12-09T23:51:40.134994: step 860, loss 0.185447, acc 0.945312, prec 0.0611902, recall 0.86478
2017-12-09T23:51:40.638899: step 861, loss 0.249338, acc 0.9375, prec 0.0612274, recall 0.864896
2017-12-09T23:51:41.137512: step 862, loss 0.259719, acc 0.929688, prec 0.0612051, recall 0.864896
2017-12-09T23:51:41.640086: step 863, loss 0.571202, acc 0.9375, prec 0.0612055, recall 0.864687
2017-12-09T23:51:42.144037: step 864, loss 0.281387, acc 0.90625, prec 0.0612327, recall 0.864803
2017-12-09T23:51:42.652890: step 865, loss 0.631116, acc 0.917969, prec 0.0613028, recall 0.864749
2017-12-09T23:51:43.157000: step 866, loss 0.211665, acc 0.910156, prec 0.0613311, recall 0.864865
2017-12-09T23:51:43.663642: step 867, loss 0.267118, acc 0.933594, prec 0.0614237, recall 0.865095
2017-12-09T23:51:44.170584: step 868, loss 0.265903, acc 0.914062, prec 0.061491, recall 0.865286
2017-12-09T23:51:44.671671: step 869, loss 0.168287, acc 0.9375, prec 0.0615468, recall 0.865439
2017-12-09T23:51:45.183257: step 870, loss 0.388278, acc 0.933594, prec 0.0616014, recall 0.865591
2017-12-09T23:51:45.692171: step 871, loss 0.274495, acc 0.917969, prec 0.061632, recall 0.865705
2017-12-09T23:51:46.199630: step 872, loss 0.734629, acc 0.941406, prec 0.0616524, recall 0.865537
2017-12-09T23:51:46.704885: step 873, loss 0.578587, acc 0.929688, prec 0.0617257, recall 0.865482
2017-12-09T23:51:47.207001: step 874, loss 0.349388, acc 0.894531, prec 0.0617487, recall 0.865596
2017-12-09T23:51:47.393389: step 875, loss 0.692195, acc 0.865385, prec 0.0617778, recall 0.865672
2017-12-09T23:51:47.911152: step 876, loss 0.201517, acc 0.921875, prec 0.0617718, recall 0.865709
2017-12-09T23:51:48.412030: step 877, loss 0.261441, acc 0.921875, prec 0.0618977, recall 0.866011
2017-12-09T23:51:48.916449: step 878, loss 0.438467, acc 0.910156, prec 0.0619632, recall 0.866199
2017-12-09T23:51:49.422766: step 879, loss 0.343196, acc 0.878906, prec 0.062, recall 0.866349
2017-12-09T23:51:49.937179: step 880, loss 0.272955, acc 0.925781, prec 0.0620328, recall 0.866461
2017-12-09T23:51:50.447634: step 881, loss 0.362194, acc 0.894531, prec 0.0621119, recall 0.866685
2017-12-09T23:51:50.968223: step 882, loss 0.32419, acc 0.910156, prec 0.0621021, recall 0.866723
2017-12-09T23:51:51.478823: step 883, loss 0.485172, acc 0.851562, prec 0.0620737, recall 0.86676
2017-12-09T23:51:51.981867: step 884, loss 0.317214, acc 0.90625, prec 0.0621564, recall 0.866983
2017-12-09T23:51:52.490182: step 885, loss 0.184454, acc 0.941406, prec 0.0622877, recall 0.867279
2017-12-09T23:51:52.996240: step 886, loss 0.298033, acc 0.917969, prec 0.0623552, recall 0.867463
2017-12-09T23:51:53.506096: step 887, loss 0.249351, acc 0.941406, prec 0.0624488, recall 0.867684
2017-12-09T23:51:54.009538: step 888, loss 0.225621, acc 0.933594, prec 0.0624838, recall 0.867794
2017-12-09T23:51:54.521869: step 889, loss 0.643322, acc 0.929688, prec 0.0625561, recall 0.867737
2017-12-09T23:51:55.028127: step 890, loss 0.196855, acc 0.945312, prec 0.0626134, recall 0.867883
2017-12-09T23:51:55.530487: step 891, loss 0.171469, acc 0.96875, prec 0.0626595, recall 0.867992
2017-12-09T23:51:56.033889: step 892, loss 0.710457, acc 0.953125, prec 0.0627018, recall 0.867862
2017-12-09T23:51:56.542150: step 893, loss 0.20757, acc 0.953125, prec 0.0627989, recall 0.86808
2017-12-09T23:51:57.045970: step 894, loss 0.16937, acc 0.933594, prec 0.0628149, recall 0.868153
2017-12-09T23:51:57.545052: step 895, loss 0.182869, acc 0.96875, prec 0.0628982, recall 0.868334
2017-12-09T23:51:58.046623: step 896, loss 0.179274, acc 0.957031, prec 0.0629217, recall 0.868407
2017-12-09T23:51:58.548276: step 897, loss 0.240581, acc 0.941406, prec 0.0629776, recall 0.868551
2017-12-09T23:51:59.055859: step 898, loss 0.203753, acc 0.945312, prec 0.0630346, recall 0.868695
2017-12-09T23:51:59.554911: step 899, loss 0.26189, acc 0.9375, prec 0.0631077, recall 0.868875
2017-12-09T23:52:00.067135: step 900, loss 0.359881, acc 0.921875, prec 0.0631198, recall 0.868947

Evaluation:
2017-12-09T23:52:04.718409: step 900, loss 2.15014, acc 0.932818, prec 0.0636809, recall 0.852756

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-900

2017-12-09T23:52:06.260864: step 901, loss 0.586535, acc 0.9375, prec 0.0637906, recall 0.852803
2017-12-09T23:52:06.768712: step 902, loss 0.424818, acc 0.921875, prec 0.0638756, recall 0.853033
2017-12-09T23:52:07.272057: step 903, loss 0.231013, acc 0.921875, prec 0.0638873, recall 0.85311
2017-12-09T23:52:07.770625: step 904, loss 0.245023, acc 0.917969, prec 0.0638976, recall 0.853187
2017-12-09T23:52:08.271852: step 905, loss 0.17412, acc 0.921875, prec 0.0638726, recall 0.853187
2017-12-09T23:52:08.778315: step 906, loss 0.298125, acc 0.925781, prec 0.0639221, recall 0.85334
2017-12-09T23:52:09.282308: step 907, loss 0.214256, acc 0.929688, prec 0.0639179, recall 0.853379
2017-12-09T23:52:09.792226: step 908, loss 0.225172, acc 0.933594, prec 0.0639698, recall 0.853531
2017-12-09T23:52:10.288818: step 909, loss 0.195057, acc 0.933594, prec 0.0640217, recall 0.853684
2017-12-09T23:52:10.789024: step 910, loss 0.398494, acc 0.941406, prec 0.0641126, recall 0.853912
2017-12-09T23:52:11.290621: step 911, loss 0.391416, acc 0.953125, prec 0.0642071, recall 0.85414
2017-12-09T23:52:11.794425: step 912, loss 0.174263, acc 0.941406, prec 0.0642431, recall 0.854253
2017-12-09T23:52:12.295578: step 913, loss 0.139603, acc 0.957031, prec 0.064284, recall 0.854366
2017-12-09T23:52:12.793848: step 914, loss 0.129413, acc 0.957031, prec 0.0643432, recall 0.854517
2017-12-09T23:52:13.294373: step 915, loss 0.242922, acc 0.933594, prec 0.0643948, recall 0.854668
2017-12-09T23:52:13.799974: step 916, loss 0.479094, acc 0.933594, prec 0.0644646, recall 0.854855
2017-12-09T23:52:14.302867: step 917, loss 0.275246, acc 0.945312, prec 0.0645745, recall 0.855117
2017-12-09T23:52:14.807316: step 918, loss 0.173648, acc 0.960938, prec 0.0646166, recall 0.855229
2017-12-09T23:52:15.307629: step 919, loss 0.21506, acc 0.957031, prec 0.0647301, recall 0.85549
2017-12-09T23:52:15.818856: step 920, loss 0.408138, acc 0.921875, prec 0.0647777, recall 0.855638
2017-12-09T23:52:16.322846: step 921, loss 0.157185, acc 0.945312, prec 0.0647965, recall 0.855712
2017-12-09T23:52:16.822891: step 922, loss 0.173817, acc 0.933594, prec 0.0648296, recall 0.855824
2017-12-09T23:52:17.329498: step 923, loss 0.376568, acc 0.953125, prec 0.0648508, recall 0.855897
2017-12-09T23:52:17.830749: step 924, loss 0.262191, acc 0.925781, prec 0.0648813, recall 0.856008
2017-12-09T23:52:18.333749: step 925, loss 0.62831, acc 0.9375, prec 0.0649714, recall 0.85601
2017-12-09T23:52:18.841505: step 926, loss 0.309128, acc 0.902344, prec 0.0649943, recall 0.856121
2017-12-09T23:52:19.344375: step 927, loss 0.180227, acc 0.949219, prec 0.0650142, recall 0.856194
2017-12-09T23:52:19.844557: step 928, loss 0.318589, acc 0.941406, prec 0.0650859, recall 0.856378
2017-12-09T23:52:20.346941: step 929, loss 0.297275, acc 0.917969, prec 0.0651138, recall 0.856487
2017-12-09T23:52:20.857192: step 930, loss 0.255631, acc 0.910156, prec 0.0651391, recall 0.856597
2017-12-09T23:52:21.355895: step 931, loss 0.287847, acc 0.957031, prec 0.0651795, recall 0.856707
2017-12-09T23:52:21.859635: step 932, loss 0.246806, acc 0.925781, prec 0.065246, recall 0.856889
2017-12-09T23:52:22.364804: step 933, loss 0.143366, acc 0.960938, prec 0.0652515, recall 0.856925
2017-12-09T23:52:22.877066: step 934, loss 0.207831, acc 0.957031, prec 0.0652737, recall 0.856998
2017-12-09T23:52:23.382130: step 935, loss 0.202466, acc 0.945312, prec 0.0652922, recall 0.85707
2017-12-09T23:52:23.882626: step 936, loss 0.222722, acc 0.949219, prec 0.0654023, recall 0.857324
2017-12-09T23:52:24.382357: step 937, loss 0.156257, acc 0.953125, prec 0.0654413, recall 0.857432
2017-12-09T23:52:24.890456: step 938, loss 0.199645, acc 0.925781, prec 0.0654173, recall 0.857432
2017-12-09T23:52:25.393158: step 939, loss 0.291276, acc 0.9375, prec 0.0654693, recall 0.857577
2017-12-09T23:52:25.891269: step 940, loss 0.256244, acc 0.980469, prec 0.0655532, recall 0.857756
2017-12-09T23:52:26.402466: step 941, loss 0.241291, acc 0.957031, prec 0.0655754, recall 0.857828
2017-12-09T23:52:26.904460: step 942, loss 0.241231, acc 0.957031, prec 0.0656877, recall 0.858079
2017-12-09T23:52:27.405785: step 943, loss 0.176211, acc 0.964844, prec 0.0657123, recall 0.858151
2017-12-09T23:52:27.908545: step 944, loss 1.31149, acc 0.953125, prec 0.0657357, recall 0.85779
2017-12-09T23:52:28.415639: step 945, loss 0.143979, acc 0.957031, prec 0.0657758, recall 0.857897
2017-12-09T23:52:28.918455: step 946, loss 0.184883, acc 0.949219, prec 0.0658133, recall 0.858005
2017-12-09T23:52:29.417048: step 947, loss 0.737412, acc 0.964844, prec 0.0658572, recall 0.857896
2017-12-09T23:52:29.917372: step 948, loss 0.248229, acc 0.9375, prec 0.0659269, recall 0.858074
2017-12-09T23:52:30.449620: step 949, loss 0.168014, acc 0.921875, prec 0.0659375, recall 0.858145
2017-12-09T23:52:30.948953: step 950, loss 0.463211, acc 0.886719, prec 0.0659186, recall 0.858181
2017-12-09T23:52:31.453808: step 951, loss 0.457836, acc 0.894531, prec 0.0659742, recall 0.858358
2017-12-09T23:52:31.962213: step 952, loss 0.308005, acc 0.890625, prec 0.0659746, recall 0.858429
2017-12-09T23:52:32.458945: step 953, loss 0.336112, acc 0.902344, prec 0.0659968, recall 0.858535
2017-12-09T23:52:32.969442: step 954, loss 0.251842, acc 0.90625, prec 0.0660381, recall 0.858677
2017-12-09T23:52:33.473112: step 955, loss 0.469013, acc 0.878906, prec 0.0661422, recall 0.858958
2017-12-09T23:52:33.981856: step 956, loss 0.284409, acc 0.902344, prec 0.0661463, recall 0.859029
2017-12-09T23:52:34.498124: step 957, loss 0.397547, acc 0.855469, prec 0.0662247, recall 0.859274
2017-12-09T23:52:35.010928: step 958, loss 0.233121, acc 0.925781, prec 0.0662363, recall 0.859344
2017-12-09T23:52:35.520183: step 959, loss 0.235059, acc 0.921875, prec 0.0662467, recall 0.859414
2017-12-09T23:52:36.037889: step 960, loss 0.527879, acc 0.929688, prec 0.066349, recall 0.859658
2017-12-09T23:52:36.549515: step 961, loss 0.290972, acc 0.894531, prec 0.0663326, recall 0.859693
2017-12-09T23:52:37.052874: step 962, loss 0.646204, acc 0.914062, prec 0.0664118, recall 0.859901
2017-12-09T23:52:37.555007: step 963, loss 0.12302, acc 0.953125, prec 0.0664144, recall 0.859936
2017-12-09T23:52:38.057386: step 964, loss 0.657348, acc 0.921875, prec 0.0663903, recall 0.859723
2017-12-09T23:52:38.562912: step 965, loss 0.345591, acc 0.933594, prec 0.0664579, recall 0.859896
2017-12-09T23:52:39.072012: step 966, loss 0.283852, acc 0.886719, prec 0.0664567, recall 0.859965
2017-12-09T23:52:39.580365: step 967, loss 0.277972, acc 0.925781, prec 0.0665039, recall 0.860104
2017-12-09T23:52:40.097336: step 968, loss 0.354531, acc 0.894531, prec 0.0665586, recall 0.860276
2017-12-09T23:52:40.602096: step 969, loss 0.195106, acc 0.914062, prec 0.0665663, recall 0.860345
2017-12-09T23:52:41.108659: step 970, loss 0.172469, acc 0.9375, prec 0.0665816, recall 0.860414
2017-12-09T23:52:41.611599: step 971, loss 0.215541, acc 0.933594, prec 0.0665956, recall 0.860482
2017-12-09T23:52:42.114944: step 972, loss 0.325705, acc 0.953125, prec 0.0667225, recall 0.860756
2017-12-09T23:52:42.616213: step 973, loss 0.145012, acc 0.953125, prec 0.0667606, recall 0.860859
2017-12-09T23:52:43.120665: step 974, loss 0.358753, acc 0.945312, prec 0.0667783, recall 0.860927
2017-12-09T23:52:43.626872: step 975, loss 0.175966, acc 0.957031, prec 0.0667821, recall 0.860961
2017-12-09T23:52:44.130542: step 976, loss 0.285791, acc 0.949219, prec 0.0668898, recall 0.861199
2017-12-09T23:52:44.638248: step 977, loss 0.0918132, acc 0.980469, prec 0.0669189, recall 0.861267
2017-12-09T23:52:45.141056: step 978, loss 0.884708, acc 0.945312, prec 0.0669378, recall 0.861125
2017-12-09T23:52:45.648530: step 979, loss 0.130332, acc 0.960938, prec 0.0669606, recall 0.861193
2017-12-09T23:52:46.157255: step 980, loss 0.397402, acc 0.964844, prec 0.0670377, recall 0.861362
2017-12-09T23:52:46.669498: step 981, loss 0.126099, acc 0.964844, prec 0.0670794, recall 0.861463
2017-12-09T23:52:47.168239: step 982, loss 0.124061, acc 0.949219, prec 0.0670983, recall 0.861531
2017-12-09T23:52:47.674538: step 983, loss 0.444665, acc 0.953125, prec 0.0671715, recall 0.8617
2017-12-09T23:52:48.178596: step 984, loss 0.167215, acc 0.9375, prec 0.0672043, recall 0.8618
2017-12-09T23:52:48.685624: step 985, loss 0.252143, acc 0.917969, prec 0.0672659, recall 0.861968
2017-12-09T23:52:49.192352: step 986, loss 0.157441, acc 0.933594, prec 0.0672619, recall 0.862002
2017-12-09T23:52:49.693047: step 987, loss 0.326297, acc 0.933594, prec 0.0673286, recall 0.862169
2017-12-09T23:52:50.192972: step 988, loss 0.334452, acc 0.925781, prec 0.0674104, recall 0.86237
2017-12-09T23:52:50.708857: step 989, loss 0.221592, acc 0.9375, prec 0.0674606, recall 0.862503
2017-12-09T23:52:51.208051: step 990, loss 0.161553, acc 0.9375, prec 0.0675108, recall 0.862636
2017-12-09T23:52:51.711795: step 991, loss 0.247691, acc 0.917969, prec 0.0675545, recall 0.862769
2017-12-09T23:52:52.216622: step 992, loss 0.751519, acc 0.933594, prec 0.067587, recall 0.86266
2017-12-09T23:52:52.726730: step 993, loss 0.214206, acc 0.933594, prec 0.0675653, recall 0.86266
2017-12-09T23:52:53.230403: step 994, loss 0.173398, acc 0.957031, prec 0.0676041, recall 0.862759
2017-12-09T23:52:53.741560: step 995, loss 0.188786, acc 0.933594, prec 0.0676176, recall 0.862825
2017-12-09T23:52:54.250073: step 996, loss 0.130608, acc 0.925781, prec 0.0676286, recall 0.862892
2017-12-09T23:52:54.749333: step 997, loss 0.252424, acc 0.945312, prec 0.0676987, recall 0.863057
2017-12-09T23:52:55.253087: step 998, loss 0.189465, acc 0.941406, prec 0.0677499, recall 0.863188
2017-12-09T23:52:55.749275: step 999, loss 0.371376, acc 0.9375, prec 0.0678701, recall 0.863451
2017-12-09T23:52:55.928349: step 1000, loss 0.235568, acc 0.942308, prec 0.0679015, recall 0.863516
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-1000

2017-12-09T23:52:57.482786: step 1001, loss 0.12768, acc 0.957031, prec 0.0679225, recall 0.863582
2017-12-09T23:52:57.988033: step 1002, loss 0.203708, acc 0.941406, prec 0.0679033, recall 0.863582
2017-12-09T23:52:58.492855: step 1003, loss 0.247592, acc 0.972656, prec 0.0679646, recall 0.863713
2017-12-09T23:52:59.000807: step 1004, loss 0.234745, acc 0.957031, prec 0.0680032, recall 0.86381
2017-12-09T23:52:59.517221: step 1005, loss 0.181471, acc 0.941406, prec 0.0680191, recall 0.863876
2017-12-09T23:53:00.023113: step 1006, loss 0.214859, acc 0.964844, prec 0.0681305, recall 0.864103
2017-12-09T23:53:00.544617: step 1007, loss 0.824675, acc 0.957031, prec 0.0681703, recall 0.863994
2017-12-09T23:53:01.051160: step 1008, loss 0.069844, acc 0.980469, prec 0.0681814, recall 0.864027
2017-12-09T23:53:01.556649: step 1009, loss 0.154404, acc 0.945312, prec 0.0681985, recall 0.864092
2017-12-09T23:53:02.070587: step 1010, loss 0.1063, acc 0.980469, prec 0.0682271, recall 0.864156
2017-12-09T23:53:02.578312: step 1011, loss 0.0818879, acc 0.976562, prec 0.068272, recall 0.864253
2017-12-09T23:53:03.086804: step 1012, loss 0.183943, acc 0.953125, prec 0.0682741, recall 0.864286
2017-12-09T23:53:03.588919: step 1013, loss 0.199412, acc 0.949219, prec 0.06831, recall 0.864383
2017-12-09T23:53:04.088692: step 1014, loss 0.257697, acc 0.933594, prec 0.0683757, recall 0.864544
2017-12-09T23:53:04.599694: step 1015, loss 0.126748, acc 0.953125, prec 0.0684128, recall 0.86464
2017-12-09T23:53:05.103451: step 1016, loss 0.124074, acc 0.949219, prec 0.0684136, recall 0.864672
2017-12-09T23:53:05.613033: step 1017, loss 0.201462, acc 0.960938, prec 0.0685058, recall 0.864865
2017-12-09T23:53:06.132943: step 1018, loss 0.0906146, acc 0.984375, prec 0.0685706, recall 0.864993
2017-12-09T23:53:06.633089: step 1019, loss 0.540063, acc 0.9375, prec 0.0687073, recall 0.86528
2017-12-09T23:53:07.150424: step 1020, loss 0.121287, acc 0.964844, prec 0.0687307, recall 0.865344
2017-12-09T23:53:07.652796: step 1021, loss 0.141821, acc 0.957031, prec 0.0687689, recall 0.865439
2017-12-09T23:53:08.157008: step 1022, loss 0.160359, acc 0.921875, prec 0.068778, recall 0.865503
2017-12-09T23:53:08.665974: step 1023, loss 0.175713, acc 0.960938, prec 0.0688175, recall 0.865598
2017-12-09T23:53:09.169059: step 1024, loss 0.236167, acc 0.96875, prec 0.0689119, recall 0.865788
2017-12-09T23:53:09.666754: step 1025, loss 0.219676, acc 0.953125, prec 0.0689313, recall 0.865851
2017-12-09T23:53:10.165332: step 1026, loss 0.206171, acc 0.9375, prec 0.0689804, recall 0.865977
2017-12-09T23:53:10.662061: step 1027, loss 0.123616, acc 0.945312, prec 0.0690146, recall 0.866071
2017-12-09T23:53:11.163431: step 1028, loss 0.198699, acc 0.964844, prec 0.0690378, recall 0.866134
2017-12-09T23:53:11.674178: step 1029, loss 0.309338, acc 0.960938, prec 0.0691294, recall 0.866323
2017-12-09T23:53:12.184228: step 1030, loss 0.133133, acc 0.949219, prec 0.0691474, recall 0.866385
2017-12-09T23:53:12.686489: step 1031, loss 0.131334, acc 0.960938, prec 0.0691867, recall 0.866479
2017-12-09T23:53:13.193643: step 1032, loss 0.162479, acc 0.96875, prec 0.069246, recall 0.866604
2017-12-09T23:53:13.702597: step 1033, loss 0.100465, acc 0.976562, prec 0.0692731, recall 0.866667
2017-12-09T23:53:14.208062: step 1034, loss 0.218193, acc 0.964844, prec 0.0693136, recall 0.86676
2017-12-09T23:53:14.715502: step 1035, loss 0.375472, acc 0.976562, prec 0.0693928, recall 0.866916
2017-12-09T23:53:15.223998: step 1036, loss 0.160613, acc 0.980469, prec 0.069508, recall 0.867133
2017-12-09T23:53:15.732357: step 1037, loss 0.154803, acc 0.949219, prec 0.0695259, recall 0.867195
2017-12-09T23:53:16.232376: step 1038, loss 1.39728, acc 0.957031, prec 0.0695477, recall 0.867055
2017-12-09T23:53:16.751571: step 1039, loss 0.311214, acc 0.964844, prec 0.0695881, recall 0.867148
2017-12-09T23:53:17.253404: step 1040, loss 0.234096, acc 0.957031, prec 0.0696259, recall 0.86724
2017-12-09T23:53:17.756042: step 1041, loss 0.408435, acc 0.925781, prec 0.069688, recall 0.867394
2017-12-09T23:53:18.266654: step 1042, loss 0.368643, acc 0.921875, prec 0.0696967, recall 0.867456
2017-12-09T23:53:18.768531: step 1043, loss 0.402419, acc 0.921875, prec 0.0698095, recall 0.867702
2017-12-09T23:53:19.267625: step 1044, loss 0.341681, acc 0.882812, prec 0.0698571, recall 0.867855
2017-12-09T23:53:19.775976: step 1045, loss 0.366953, acc 0.894531, prec 0.0699605, recall 0.868099
2017-12-09T23:53:20.281477: step 1046, loss 0.433025, acc 0.890625, prec 0.0700106, recall 0.868251
2017-12-09T23:53:20.801275: step 1047, loss 0.328854, acc 0.890625, prec 0.0700433, recall 0.868372
2017-12-09T23:53:21.300465: step 1048, loss 0.386753, acc 0.84375, prec 0.0700258, recall 0.868433
2017-12-09T23:53:21.809248: step 1049, loss 0.261335, acc 0.90625, prec 0.0700119, recall 0.868464
2017-12-09T23:53:22.318452: step 1050, loss 0.326607, acc 0.882812, prec 0.0700419, recall 0.868585
2017-12-09T23:53:22.825467: step 1051, loss 0.323501, acc 0.902344, prec 0.070113, recall 0.868766
2017-12-09T23:53:23.328579: step 1052, loss 0.241279, acc 0.929688, prec 0.070193, recall 0.868947
2017-12-09T23:53:23.831689: step 1053, loss 0.269515, acc 0.925781, prec 0.0702372, recall 0.869067
2017-12-09T23:53:24.333163: step 1054, loss 0.298784, acc 0.921875, prec 0.0703145, recall 0.869247
2017-12-09T23:53:24.842627: step 1055, loss 0.293486, acc 0.941406, prec 0.0703983, recall 0.869426
2017-12-09T23:53:25.348527: step 1056, loss 0.127567, acc 0.949219, prec 0.0704158, recall 0.869486
2017-12-09T23:53:25.850304: step 1057, loss 0.174879, acc 0.949219, prec 0.0704504, recall 0.869575
2017-12-09T23:53:26.353435: step 1058, loss 0.60915, acc 0.917969, prec 0.0704587, recall 0.869436
2017-12-09T23:53:26.861998: step 1059, loss 0.137013, acc 0.953125, prec 0.0705462, recall 0.869615
2017-12-09T23:53:27.373543: step 1060, loss 0.140428, acc 0.96875, prec 0.0706389, recall 0.869793
2017-12-09T23:53:27.876554: step 1061, loss 0.117909, acc 0.964844, prec 0.0706443, recall 0.869823
2017-12-09T23:53:28.380936: step 1062, loss 0.332499, acc 0.976562, prec 0.0707396, recall 0.87
2017-12-09T23:53:28.889485: step 1063, loss 0.249766, acc 0.976562, prec 0.0707832, recall 0.870089
2017-12-09T23:53:29.394144: step 1064, loss 0.652749, acc 0.964844, prec 0.0708071, recall 0.86995
2017-12-09T23:53:29.897130: step 1065, loss 0.17638, acc 0.964844, prec 0.0708468, recall 0.870039
2017-12-09T23:53:30.415148: step 1066, loss 0.104602, acc 0.972656, prec 0.0709234, recall 0.870186
2017-12-09T23:53:30.916769: step 1067, loss 0.218671, acc 0.964844, prec 0.0709459, recall 0.870245
2017-12-09T23:53:31.422928: step 1068, loss 0.352704, acc 0.972656, prec 0.0710225, recall 0.870391
2017-12-09T23:53:31.928961: step 1069, loss 0.20763, acc 0.929688, prec 0.0710161, recall 0.870421
2017-12-09T23:53:32.433796: step 1070, loss 0.201875, acc 0.945312, prec 0.0710491, recall 0.870508
2017-12-09T23:53:32.940038: step 1071, loss 0.134619, acc 0.949219, prec 0.0711178, recall 0.870655
2017-12-09T23:53:33.439604: step 1072, loss 0.22702, acc 0.921875, prec 0.0711258, recall 0.870713
2017-12-09T23:53:33.944389: step 1073, loss 0.183141, acc 0.933594, prec 0.0711206, recall 0.870742
2017-12-09T23:53:34.449006: step 1074, loss 0.158911, acc 0.941406, prec 0.0711694, recall 0.870859
2017-12-09T23:53:34.952906: step 1075, loss 0.224704, acc 0.9375, prec 0.0712852, recall 0.871091
2017-12-09T23:53:35.468541: step 1076, loss 0.310689, acc 0.933594, prec 0.0713142, recall 0.871178
2017-12-09T23:53:35.986288: step 1077, loss 0.255612, acc 0.921875, prec 0.0713392, recall 0.871265
2017-12-09T23:53:36.488699: step 1078, loss 0.179736, acc 0.957031, prec 0.0713931, recall 0.87138
2017-12-09T23:53:37.000654: step 1079, loss 0.161775, acc 0.960938, prec 0.0714995, recall 0.871582
2017-12-09T23:53:37.510656: step 1080, loss 0.201378, acc 0.960938, prec 0.0715375, recall 0.871669
2017-12-09T23:53:38.013983: step 1081, loss 0.654878, acc 0.953125, prec 0.0715743, recall 0.87156
2017-12-09T23:53:38.520854: step 1082, loss 0.208117, acc 0.933594, prec 0.0716201, recall 0.871674
2017-12-09T23:53:39.021189: step 1083, loss 0.701332, acc 0.960938, prec 0.0716594, recall 0.871566
2017-12-09T23:53:39.525864: step 1084, loss 0.149908, acc 0.953125, prec 0.0716948, recall 0.871652
2017-12-09T23:53:40.024487: step 1085, loss 0.242364, acc 0.921875, prec 0.0717196, recall 0.871738
2017-12-09T23:53:40.529431: step 1086, loss 0.318092, acc 0.945312, prec 0.0717352, recall 0.871795
2017-12-09T23:53:41.036333: step 1087, loss 0.204812, acc 0.917969, prec 0.0718097, recall 0.871966
2017-12-09T23:53:41.539928: step 1088, loss 0.16396, acc 0.945312, prec 0.0718594, recall 0.87208
2017-12-09T23:53:42.043118: step 1089, loss 0.329998, acc 0.929688, prec 0.0719377, recall 0.872251
2017-12-09T23:53:42.544781: step 1090, loss 0.168038, acc 0.9375, prec 0.0719336, recall 0.872279
2017-12-09T23:53:43.049265: step 1091, loss 0.146727, acc 0.945312, prec 0.0719322, recall 0.872307
2017-12-09T23:53:43.550638: step 1092, loss 0.187948, acc 0.925781, prec 0.0719411, recall 0.872364
2017-12-09T23:53:44.065318: step 1093, loss 0.200558, acc 0.949219, prec 0.0720259, recall 0.872534
2017-12-09T23:53:44.565122: step 1094, loss 0.1816, acc 0.960938, prec 0.0720806, recall 0.872647
2017-12-09T23:53:45.070792: step 1095, loss 0.249324, acc 0.953125, prec 0.0721327, recall 0.872759
2017-12-09T23:53:45.574441: step 1096, loss 0.314426, acc 0.960938, prec 0.0721534, recall 0.872816
2017-12-09T23:53:46.075627: step 1097, loss 0.142881, acc 0.96875, prec 0.0722616, recall 0.873012
2017-12-09T23:53:46.586533: step 1098, loss 0.140311, acc 0.972656, prec 0.0722863, recall 0.873068
2017-12-09T23:53:47.095577: step 1099, loss 1.21554, acc 0.949219, prec 0.0723721, recall 0.873044
2017-12-09T23:53:47.602658: step 1100, loss 0.0745793, acc 0.976562, prec 0.0723981, recall 0.8731
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-1100

2017-12-09T23:53:49.230049: step 1101, loss 0.21298, acc 0.957031, prec 0.0724683, recall 0.873239
2017-12-09T23:53:49.739284: step 1102, loss 0.19481, acc 0.949219, prec 0.0725357, recall 0.873379
2017-12-09T23:53:50.249050: step 1103, loss 0.150228, acc 0.96875, prec 0.0726098, recall 0.873518
2017-12-09T23:53:50.776145: step 1104, loss 0.214551, acc 0.929688, prec 0.0726029, recall 0.873546
2017-12-09T23:53:51.280935: step 1105, loss 0.175184, acc 0.949219, prec 0.0726533, recall 0.873657
2017-12-09T23:53:51.783481: step 1106, loss 0.244541, acc 0.933594, prec 0.0726646, recall 0.873712
2017-12-09T23:53:52.282015: step 1107, loss 0.511335, acc 0.945312, prec 0.0727475, recall 0.873878
2017-12-09T23:53:52.785734: step 1108, loss 0.222965, acc 0.929688, prec 0.0727743, recall 0.873961
2017-12-09T23:53:53.292433: step 1109, loss 0.193205, acc 0.941406, prec 0.0727882, recall 0.874016
2017-12-09T23:53:53.795447: step 1110, loss 0.285801, acc 0.9375, prec 0.0728683, recall 0.874181
2017-12-09T23:53:54.299457: step 1111, loss 0.238404, acc 0.929688, prec 0.0728782, recall 0.874236
2017-12-09T23:53:54.799934: step 1112, loss 0.195347, acc 0.964844, prec 0.0729337, recall 0.874346
2017-12-09T23:53:55.302739: step 1113, loss 0.634251, acc 0.949219, prec 0.073019, recall 0.874319
2017-12-09T23:53:55.810847: step 1114, loss 0.224979, acc 0.941406, prec 0.0730665, recall 0.874429
2017-12-09T23:53:56.311179: step 1115, loss 0.307833, acc 0.921875, prec 0.073141, recall 0.874592
2017-12-09T23:53:56.811068: step 1116, loss 0.160456, acc 0.945312, prec 0.0731729, recall 0.874674
2017-12-09T23:53:57.314667: step 1117, loss 0.176048, acc 0.957031, prec 0.0732257, recall 0.874783
2017-12-09T23:53:57.818760: step 1118, loss 0.173657, acc 0.949219, prec 0.073242, recall 0.874837
2017-12-09T23:53:58.327040: step 1119, loss 0.376249, acc 0.972656, prec 0.0733337, recall 0.875
2017-12-09T23:53:58.831837: step 1120, loss 0.232665, acc 0.914062, prec 0.0733044, recall 0.875
2017-12-09T23:53:59.340319: step 1121, loss 0.276858, acc 0.945312, prec 0.0733699, recall 0.875135
2017-12-09T23:53:59.852294: step 1122, loss 0.149567, acc 0.957031, prec 0.0734056, recall 0.875216
2017-12-09T23:54:00.369792: step 1123, loss 0.123799, acc 0.953125, prec 0.0734065, recall 0.875243
2017-12-09T23:54:00.868185: step 1124, loss 0.403293, acc 0.945312, prec 0.0734886, recall 0.875405
2017-12-09T23:54:01.047733: step 1125, loss 0.810289, acc 0.884615, prec 0.0734974, recall 0.875432
2017-12-09T23:54:01.561673: step 1126, loss 0.177952, acc 0.929688, prec 0.0735574, recall 0.875566
2017-12-09T23:54:02.077115: step 1127, loss 0.212124, acc 0.933594, prec 0.0735851, recall 0.875647
2017-12-09T23:54:02.581042: step 1128, loss 0.214122, acc 0.925781, prec 0.0736436, recall 0.87578
2017-12-09T23:54:03.078843: step 1129, loss 0.136159, acc 0.949219, prec 0.0736598, recall 0.875834
2017-12-09T23:54:03.574994: step 1130, loss 0.331168, acc 0.914062, prec 0.0736975, recall 0.875941
2017-12-09T23:54:04.079601: step 1131, loss 0.399019, acc 0.894531, prec 0.0738123, recall 0.87618
2017-12-09T23:54:04.580313: step 1132, loss 0.216787, acc 0.917969, prec 0.0738177, recall 0.876233
2017-12-09T23:54:05.086848: step 1133, loss 0.156026, acc 0.964844, prec 0.0738392, recall 0.876286
2017-12-09T23:54:05.596540: step 1134, loss 0.241668, acc 0.945312, prec 0.0738874, recall 0.876392
2017-12-09T23:54:06.121804: step 1135, loss 0.133124, acc 0.964844, prec 0.0739256, recall 0.876472
2017-12-09T23:54:06.624813: step 1136, loss 0.217752, acc 0.949219, prec 0.0739751, recall 0.876578
2017-12-09T23:54:07.127466: step 1137, loss 0.198734, acc 0.945312, prec 0.07404, recall 0.876709
2017-12-09T23:54:07.634222: step 1138, loss 0.229308, acc 0.953125, prec 0.0740741, recall 0.876788
2017-12-09T23:54:08.138895: step 1139, loss 0.245452, acc 0.953125, prec 0.0741582, recall 0.876946
2017-12-09T23:54:08.641949: step 1140, loss 0.128638, acc 0.953125, prec 0.0741422, recall 0.876946
2017-12-09T23:54:09.146263: step 1141, loss 0.220221, acc 0.933594, prec 0.0741528, recall 0.876998
2017-12-09T23:54:09.651192: step 1142, loss 0.319468, acc 0.957031, prec 0.0742382, recall 0.877156
2017-12-09T23:54:10.150380: step 1143, loss 0.108114, acc 0.960938, prec 0.0742582, recall 0.877208
2017-12-09T23:54:10.659729: step 1144, loss 0.140848, acc 0.949219, prec 0.0742575, recall 0.877234
2017-12-09T23:54:11.161096: step 1145, loss 0.392742, acc 0.960938, prec 0.0743442, recall 0.877391
2017-12-09T23:54:11.666400: step 1146, loss 0.164642, acc 0.941406, prec 0.0744074, recall 0.877521
2017-12-09T23:54:12.165442: step 1147, loss 0.0824474, acc 0.976562, prec 0.0744327, recall 0.877573
2017-12-09T23:54:12.668461: step 1148, loss 0.0483188, acc 0.980469, prec 0.0744426, recall 0.877599
2017-12-09T23:54:13.180666: step 1149, loss 0.35714, acc 0.964844, prec 0.0745138, recall 0.877728
2017-12-09T23:54:13.691727: step 1150, loss 0.116227, acc 0.972656, prec 0.0746043, recall 0.877884
2017-12-09T23:54:14.198787: step 1151, loss 0.241076, acc 0.957031, prec 0.0747227, recall 0.87809
2017-12-09T23:54:14.703457: step 1152, loss 0.214234, acc 0.945312, prec 0.0747704, recall 0.878193
2017-12-09T23:54:15.198641: step 1153, loss 0.492764, acc 0.964844, prec 0.0748428, recall 0.878136
2017-12-09T23:54:15.706516: step 1154, loss 0.263378, acc 0.964844, prec 0.0748805, recall 0.878213
2017-12-09T23:54:16.221310: step 1155, loss 0.113272, acc 0.96875, prec 0.0749528, recall 0.878341
2017-12-09T23:54:16.719465: step 1156, loss 0.228216, acc 0.933594, prec 0.0749964, recall 0.878444
2017-12-09T23:54:17.216934: step 1157, loss 0.114583, acc 0.957031, prec 0.0749982, recall 0.878469
2017-12-09T23:54:17.726230: step 1158, loss 0.332979, acc 0.945312, prec 0.0750292, recall 0.878546
2017-12-09T23:54:18.228916: step 1159, loss 0.237177, acc 0.914062, prec 0.0750327, recall 0.878597
2017-12-09T23:54:18.735832: step 1160, loss 0.12311, acc 0.96875, prec 0.0750551, recall 0.878648
2017-12-09T23:54:19.238695: step 1161, loss 0.148675, acc 0.953125, prec 0.0751385, recall 0.878801
2017-12-09T23:54:19.740132: step 1162, loss 0.496016, acc 0.914062, prec 0.0752249, recall 0.878978
2017-12-09T23:54:20.244816: step 1163, loss 0.387931, acc 0.941406, prec 0.0753537, recall 0.879206
2017-12-09T23:54:20.762494: step 1164, loss 0.216085, acc 0.929688, prec 0.0753626, recall 0.879256
2017-12-09T23:54:21.262963: step 1165, loss 0.41117, acc 0.925781, prec 0.0754197, recall 0.879382
2017-12-09T23:54:21.763974: step 1166, loss 0.24343, acc 0.941406, prec 0.0754656, recall 0.879483
2017-12-09T23:54:22.263018: step 1167, loss 0.232196, acc 0.925781, prec 0.0754896, recall 0.879558
2017-12-09T23:54:22.767367: step 1168, loss 0.159029, acc 0.9375, prec 0.0755506, recall 0.879684
2017-12-09T23:54:23.270434: step 1169, loss 0.137023, acc 0.953125, prec 0.075551, recall 0.879709
2017-12-09T23:54:23.775801: step 1170, loss 0.242269, acc 0.925781, prec 0.0755583, recall 0.879759
2017-12-09T23:54:24.282275: step 1171, loss 0.114312, acc 0.957031, prec 0.07556, recall 0.879784
2017-12-09T23:54:24.782437: step 1172, loss 0.221428, acc 0.953125, prec 0.0756098, recall 0.879884
2017-12-09T23:54:25.290181: step 1173, loss 0.137221, acc 0.964844, prec 0.0756637, recall 0.879983
2017-12-09T23:54:25.796606: step 1174, loss 0.125721, acc 0.960938, prec 0.0757162, recall 0.880083
2017-12-09T23:54:26.303464: step 1175, loss 0.146475, acc 0.957031, prec 0.0757343, recall 0.880133
2017-12-09T23:54:26.801469: step 1176, loss 0.0912427, acc 0.980469, prec 0.0757606, recall 0.880182
2017-12-09T23:54:27.300786: step 1177, loss 0.0714838, acc 0.96875, prec 0.0758157, recall 0.880282
2017-12-09T23:54:27.810997: step 1178, loss 0.0724586, acc 0.984375, prec 0.0758268, recall 0.880306
2017-12-09T23:54:28.317450: step 1179, loss 0.199192, acc 0.972656, prec 0.0758832, recall 0.880406
2017-12-09T23:54:28.823846: step 1180, loss 0.173174, acc 0.945312, prec 0.0759302, recall 0.880504
2017-12-09T23:54:29.322703: step 1181, loss 0.423624, acc 0.960938, prec 0.0759839, recall 0.880421
2017-12-09T23:54:29.831205: step 1182, loss 0.998816, acc 0.964844, prec 0.0760719, recall 0.880388
2017-12-09T23:54:30.350506: step 1183, loss 0.29, acc 0.957031, prec 0.0761063, recall 0.880462
2017-12-09T23:54:30.860249: step 1184, loss 0.435258, acc 0.953125, prec 0.0761888, recall 0.880609
2017-12-09T23:54:31.371616: step 1185, loss 0.155127, acc 0.957031, prec 0.0762232, recall 0.880683
2017-12-09T23:54:31.871085: step 1186, loss 0.790644, acc 0.949219, prec 0.0762563, recall 0.880576
2017-12-09T23:54:32.383069: step 1187, loss 0.223443, acc 0.933594, prec 0.0762825, recall 0.880649
2017-12-09T23:54:32.883105: step 1188, loss 0.248551, acc 0.902344, prec 0.0762979, recall 0.880723
2017-12-09T23:54:33.394011: step 1189, loss 0.326301, acc 0.875, prec 0.0762873, recall 0.880772
2017-12-09T23:54:33.897011: step 1190, loss 0.266873, acc 0.9375, prec 0.0763477, recall 0.880894
2017-12-09T23:54:34.400449: step 1191, loss 0.328655, acc 0.898438, prec 0.0764272, recall 0.881064
2017-12-09T23:54:34.903377: step 1192, loss 0.317329, acc 0.917969, prec 0.0764479, recall 0.881137
2017-12-09T23:54:35.403769: step 1193, loss 0.21303, acc 0.9375, prec 0.076459, recall 0.881186
2017-12-09T23:54:35.920030: step 1194, loss 0.319534, acc 0.875, prec 0.0764647, recall 0.881259
2017-12-09T23:54:36.430509: step 1195, loss 0.203059, acc 0.910156, prec 0.0764336, recall 0.881259
2017-12-09T23:54:36.932323: step 1196, loss 0.221511, acc 0.917969, prec 0.0764379, recall 0.881307
2017-12-09T23:54:37.439965: step 1197, loss 0.212236, acc 0.914062, prec 0.0764735, recall 0.881404
2017-12-09T23:54:37.945178: step 1198, loss 0.207349, acc 0.945312, prec 0.0766344, recall 0.88167
2017-12-09T23:54:38.455310: step 1199, loss 0.307057, acc 0.925781, prec 0.076674, recall 0.881766
2017-12-09T23:54:38.963923: step 1200, loss 0.156872, acc 0.945312, prec 0.076704, recall 0.881839

Evaluation:
2017-12-09T23:54:43.668423: step 1200, loss 2.36187, acc 0.942348, prec 0.0771735, recall 0.869282

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/batch_size_256_fold_3/1512881047/checkpoints/model-1200

2017-12-09T23:54:45.235947: step 1201, loss 0.164655, acc 0.949219, prec 0.0772528, recall 0.869437
2017-12-09T23:54:45.744357: step 1202, loss 0.377169, acc 0.945312, prec 0.0773469, recall 0.869617
2017-12-09T23:54:46.249460: step 1203, loss 0.201468, acc 0.96875, prec 0.0773845, recall 0.869693
2017-12-09T23:54:46.753401: step 1204, loss 0.204846, acc 0.933594, prec 0.077426, recall 0.869796
2017-12-09T23:54:47.258671: step 1205, loss 0.686168, acc 0.964844, prec 0.0775281, recall 0.869804
2017-12-09T23:54:47.769382: step 1206, loss 0.146339, acc 0.964844, prec 0.077532, recall 0.869829
2017-12-09T23:54:48.278700: step 1207, loss 0.31014, acc 0.957031, prec 0.0775815, recall 0.869931
2017-12-09T23:54:48.784016: step 1208, loss 0.256231, acc 0.949219, prec 0.0776123, recall 0.870008
2017-12-09T23:54:49.280768: step 1209, loss 0.64941, acc 0.988281, prec 0.0777062, recall 0.86999
2017-12-09T23:54:49.795271: step 1210, loss 0.404659, acc 0.941406, prec 0.0777663, recall 0.870117
2017-12-09T23:54:50.300304: step 1211, loss 0.238307, acc 0.90625, prec 0.077782, recall 0.870193
2017-12-09T23:54:50.830689: step 1212, loss 0.184533, acc 0.910156, prec 0.0778152, recall 0.870295
2017-12-09T23:54:51.338778: step 1213, loss 0.362307, acc 0.921875, prec 0.0778202, recall 0.870345
2017-12-09T23:54:51.839821: step 1214, loss 0.219706, acc 0.941406, prec 0.0778159, recall 0.87037
2017-12-09T23:54:52.342386: step 1215, loss 0.182388, acc 0.910156, prec 0.0778169, recall 0.870421
2017-12-09T23:54:52.843747: step 1216, loss 0.231396, acc 0.933594, prec 0.077842, recall 0.870497
2017-12-09T23:54:53.354018: step 1217, loss 0.26903, acc 0.933594, prec 0.0778992, recall 0.870623
2017-12-09T23:54:53.864091: step 1218, loss 0.3378, acc 0.910156, prec 0.0779804, recall 0.870799
2017-12-09T23:54:54.371211: step 1219, loss 0.231967, acc 0.925781, prec 0.0779706, recall 0.870824
2017-12-09T23:54:54.867155: step 1220, loss 0.307093, acc 0.921875, prec 0.0780557, recall 0.870999
2017-12-09T23:54:55.373309: step 1221, loss 0.297969, acc 0.90625, prec 0.0781033, recall 0.871124
2017-12-09T23:54:55.878587: step 1222, loss 0.216789, acc 0.914062, prec 0.0781215, recall 0.871199
2017-12-09T23:54:56.381316: step 1223, loss 0.272711, acc 0.878906, prec 0.0780954, recall 0.871224
2017-12-09T23:54:56.889103: step 1224, loss 0.137178, acc 0.960938, prec 0.0780979, recall 0.871249
2017-12-09T23:54:57.402025: step 1225, loss 0.192304, acc 0.933594, prec 0.0781708, recall 0.871398
2017-12-09T23:54:57.907156: step 1226, loss 0.167616, acc 0.945312, prec 0.0781678, recall 0.871423
2017-12-09T23:54:58.410087: step 1227, loss 0.0701652, acc 0.980469, prec 0.078193, recall 0.871473
2017-12-09T23:54:58.919067: step 1228, loss 0.11065, acc 0.964844, prec 0.0782288, recall 0.871547
2017-12-09T23:54:59.417379: step 1229, loss 0.588317, acc 0.980469, prec 0.0782873, recall 0.871478
2017-12-09T23:54:59.925944: step 1230, loss 0.09692, acc 0.984375, prec 0.0783297, recall 0.871553
2017-12-09T23:55:00.453627: step 1231, loss 0.0807931, acc 0.976562, prec 0.0783536, recall 0.871602
2017-12-09T23:55:00.956564: step 1232, loss 0.0647801, acc 0.96875, prec 0.0783906, recall 0.871676
2017-12-09T23:55:01.456411: step 1233, loss 0.333757, acc 0.960938, prec 0.0784249, recall 0.87175
2017-12-09T23:55:01.961453: step 1234, loss 0.182285, acc 0.988281, prec 0.0784847, recall 0.871849
2017-12-09T23:55:02.469233: step 1235, loss 0.193512, acc 0.988281, prec 0.0785604, recall 0.871972
2017-12-09T23:55:02.969765: step 1236, loss 0.0780594, acc 0.984375, prec 0.0786029, recall 0.872046
2017-12-09T23:55:03.480272: step 1237, loss 0.0946454, acc 0.976562, prec 0.0786266, recall 0.872095
2017-12-09T23:55:03.987497: step 1238, loss 0.484307, acc 0.976562, prec 0.0786822, recall 0.872193
2017-12-09T23:55:04.489530: step 1239, loss 0.144577, acc 0.976562, prec 0.0787378, recall 0.872291
2017-12-09T23:55:04.997172: step 1240, loss 0.0231988, acc 1, prec 0.0787857, recall 0.872365
2017-12-09T23:55:05.509467: step 1241, loss 0.288252, acc 0.9375, prec 0.0788595, recall 0.872512
2017-12-09T23:55:06.017330: step 1242, loss 0.159393, acc 0.964844, prec 0.0788791, recall 0.87256
2017-12-09T23:55:06.523720: step 1243, loss 0.222644, acc 0.964844, prec 0.0789305, recall 0.872658
2017-12-09T23:55:07.031807: step 1244, loss 0.162055, acc 0.929688, prec 0.0789537, recall 0.872731
2017-12-09T23:55:07.538227: step 1245, loss 0.124377, acc 0.957031, prec 0.0789865, recall 0.872804
2017-12-09T23:55:08.041356: step 1246, loss 0.226623, acc 0.914062, prec 0.079036, recall 0.872925
2017-12-09T23:55:08.546215: step 1247, loss 0.197476, acc 0.945312, prec 0.0790646, recall 0.872998
2017-12-09T23:55:09.046387: step 1248, loss 0.197136, acc 0.957031, prec 0.0791132, recall 0.873094
2017-12-09T23:55:09.551783: step 1249, loss 0.153383, acc 0.945312, prec 0.0791259, recall 0.873143
2017-12-09T23:55:09.729385: step 1250, loss 0.151483, acc 0.942308, prec 0.0791218, recall 0.873143
Training finished
